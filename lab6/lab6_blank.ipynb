{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9199231",
   "metadata": {},
   "source": [
    "**Лабораторный практикум по курсу «Распознавание диктора», Университет ИТМО, 2025**\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dd7db",
   "metadata": {},
   "source": [
    "**Лабораторная работа №6. Обучение блока построения дикторских моделей на основе трансформерных претрейнов**\n",
    "\n",
    "**Цель работы:** изучение процедуры обучения блока построения дикторских моделей на основе трансформерных претрейнов.\n",
    "\n",
    "**Краткое описание:** в рамках настоящей лабораторной работы предлагается изучить и реализовать схему обучения нейросетевого блока построения дикторских моделей на основе Whisper tiny трансформерного претрейна. Процедуры обучения и тестирования предлагается выполнить по отношению к задаче идентификации на закрытом множестве и верификации на открытом множестве, соответственно. Тестирование полученной системы предполагает использование доли правильных ответов (accuracy) в рамках идентификации диктора на закрытом множестве, а также равновероятной ошибки (EER, equal error rate) в рамках верификации диктора на открытом множестве в качестве целевых метрик оценки качества.\n",
    "\n",
    "**Данные:** в качестве данных для выполнения лабораторной работы предлагается использовать базу [VoxCeleb1](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html).\n",
    "\n",
    "**Содержание лабораторной работы**\n",
    "\n",
    "1. Подготовка данных для обучения и тестирования блока построения дикторских моделей.\t\t\t\t\t\t\t\n",
    "\n",
    "2. Обучение параметров блока построения дикторских моделей, основанного на использовании «размороженного» Whisper tiny трансформерного претрейна.\n",
    "\n",
    "3. Проведение дополнительных экспериментов по использованию трансформерных архитектур в задаче распознавания диктора."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f862cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython extension to reload modules before executing user code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import of modules\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common import download_dataset, concatenate, extract_dataset, part_extract, download_protocol, split_musan\n",
    "from common import test_dataset_loader as test_dataset_loader_verification\n",
    "from common import extract_features, compute_scores\n",
    "from common import get_eer\n",
    "from exercises_blank import train_dataset_loader, test_dataset_loader, train_network, test_network, tar_imp_hists\n",
    "from exercises_blank import WhisperEncoder, TDNNSimple, StatPoolLayer, MaxoutSegmentLevel, SSLDownstream, MainModel\n",
    "from whisper_fbanks import ExtractWhisperFbanks80\n",
    "from LossFunction import AAMSoftmaxLoss\n",
    "from Optimizer import SGDOptimizer, AdamOptimizer\n",
    "from Scheduler import OneCycleLRScheduler\n",
    "from load_save_pth import saveParameters, loadParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3baf71",
   "metadata": {},
   "source": [
    "**1. Подготовка данных для обучения и тестирования детектора речевой активности**\n",
    "\n",
    "В ходе выполнения лабораторной работы необходимы данные для проведения процедуры обучения и процедуры тестирования нейросетевого блока генерации дикторских моделей. Возьмём в качестве этих данных звукозаписи, сохраненные в формат *wav*, из корпуса [VoxCeleb1 dev set и VoxCeleb1 test set](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html). Данные корпуса содержат 148,642 и 4,874 звукозаписи (частота дискретизации равна 16кГц) с общим количеством дикторов женского и мужского пола равным 1,251, разговаривающих преимущественно на английском языке. В рамках настоящей лабораторной работы данные из корпуса VoxCeleb1 dev set  будут использоваться при проведения процедур обучения и валидации блока построения дикторских моделей в задаче идентификации на закрытом множестве. Данные из корпуса VoxCeleb1 test set будут использоваться для проведения верификационного теста на открытом множестве из 40 дикторов, которые отсутствовали в корпусе VoxCeleb1 dev set.\n",
    "\n",
    "В рамках настоящего пункта требуется выполнить следующее:\n",
    "\n",
    "1. Загрузить и распаковать звуковые wav-файлы из корпуса VoxCeleb1 dev set и VoxCeleb1 test set.\n",
    "\n",
    "2. Загрузить идентификационный протокол, позволяющий разделить данные из корпуса VoxCeleb1 dev set на обучающее и валидацилонное множества.\n",
    "\n",
    "3. Загрузить верификационный протокол, связанный с корпусом VoxCeleb1 test set.\n",
    "\n",
    "4. Загрузить и распаковать данные из корпусов [SLR17 (MUSAN)](https://openslr.org/17/) and [SLR28 (RIR noises)](https://openslr.org/28/), с использованием которых можно расширить обучающее множество данных через запуск процедур аугментации.\n",
    "\n",
    "![Рисунок 1](https://analyticsindiamag.com/wp-content/uploads/2020/12/image.png \"VoxCeleb. Крупномасштабная аудиовизуальная база данных человеческой речи.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd7ecdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum successful vox1_test_wav.zip.\n",
      "Checksum successful vox1_dev_wav_partaa.\n",
      "Checksum successful vox1_dev_wav_partab.\n",
      "Checksum successful vox1_dev_wav_partac.\n",
      "Checksum successful vox1_dev_wav_partad.\n"
     ]
    }
   ],
   "source": [
    "# Download VoxCeleb1 (test set)\n",
    "with open('../data/lists/datasets.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "download_dataset(lines, user='voxceleb1902', password='nx0bl2v2', save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0fd3c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum successful vox1_dev_wav.zip.\n"
     ]
    }
   ],
   "source": [
    "# Concatenate archives for VoxCeleb1 dev set\n",
    "with open('../data/lists/concat_arch.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "concatenate(lines, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ed3a9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting of ../data/vox1_dev_wav.zip is successful.\n"
     ]
    }
   ],
   "source": [
    "# Extract VoxCeleb1 dev set\n",
    "extract_dataset(save_path='../data/voxceleb1_dev', fname='../data/vox1_dev_wav.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "875ec4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File veri_test2.txt is downloaded.\n",
      "File iden_split.txt is downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download VoxCeleb1 identification protocol and VoxCeleb1-O cleaned protocol\n",
    "with open('../data/lists/protocols.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "download_protocol(lines, save_path='../data/voxceleb1_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7474573e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksum successful musan.tar.gz.\n",
      "Checksum successful rirs_noises.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download SLR17 (MUSAN) and SLR28 (RIR noises) datasets for train data augmentation\n",
    "with open('../data/lists/augment_datasets.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "download_dataset(lines, user=None, password=None, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c59215b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting of ../data/musan.tar.gz is successful.\n",
      "Extracting ../data/rirs_noises.zip\n"
     ]
    }
   ],
   "source": [
    "# Extract SLR17 (MUSAN)\n",
    "extract_dataset(save_path='../data', fname='../data/musan.tar.gz')\n",
    "\n",
    "# Extract SLR28 (RIR noises)\n",
    "part_extract(save_path='../data', fname='../data/rirs_noises.zip', target=['RIRS_NOISES/simulated_rirs/mediumroom', 'RIRS_NOISES/simulated_rirs/smallroom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "974abe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ../data/musan/noise/free-sound/noise-free-sound-0536.wav\n",
      "1 ../data/musan/noise/free-sound/noise-free-sound-0306.wav\n",
      "2 ../data/musan/noise/free-sound/noise-free-sound-0115.wav\n",
      "3 ../data/musan/noise/free-sound/noise-free-sound-0566.wav\n",
      "4 ../data/musan/noise/free-sound/noise-free-sound-0152.wav\n",
      "5 ../data/musan/noise/free-sound/noise-free-sound-0527.wav\n",
      "6 ../data/musan/noise/free-sound/noise-free-sound-0302.wav\n",
      "7 ../data/musan/noise/free-sound/noise-free-sound-0461.wav\n",
      "8 ../data/musan/noise/free-sound/noise-free-sound-0141.wav\n",
      "9 ../data/musan/noise/free-sound/noise-free-sound-0565.wav\n",
      "10 ../data/musan/noise/free-sound/noise-free-sound-0744.wav\n",
      "11 ../data/musan/noise/free-sound/noise-free-sound-0336.wav\n",
      "12 ../data/musan/noise/free-sound/noise-free-sound-0038.wav\n",
      "13 ../data/musan/noise/free-sound/noise-free-sound-0727.wav\n",
      "14 ../data/musan/noise/free-sound/noise-free-sound-0445.wav\n",
      "15 ../data/musan/noise/free-sound/noise-free-sound-0762.wav\n",
      "16 ../data/musan/noise/free-sound/noise-free-sound-0738.wav\n",
      "17 ../data/musan/noise/free-sound/noise-free-sound-0830.wav\n",
      "18 ../data/musan/noise/free-sound/noise-free-sound-0378.wav\n",
      "19 ../data/musan/noise/free-sound/noise-free-sound-0489.wav\n",
      "20 ../data/musan/noise/free-sound/noise-free-sound-0488.wav\n",
      "21 ../data/musan/noise/free-sound/noise-free-sound-0806.wav\n",
      "22 ../data/musan/noise/free-sound/noise-free-sound-0734.wav\n",
      "23 ../data/musan/noise/free-sound/noise-free-sound-0612.wav\n",
      "24 ../data/musan/noise/free-sound/noise-free-sound-0212.wav\n",
      "25 ../data/musan/noise/free-sound/noise-free-sound-0567.wav\n",
      "26 ../data/musan/noise/free-sound/noise-free-sound-0512.wav\n",
      "27 ../data/musan/noise/free-sound/noise-free-sound-0502.wav\n",
      "28 ../data/musan/noise/free-sound/noise-free-sound-0259.wav\n",
      "29 ../data/musan/noise/free-sound/noise-free-sound-0507.wav\n",
      "30 ../data/musan/noise/free-sound/noise-free-sound-0162.wav\n",
      "31 ../data/musan/noise/free-sound/noise-free-sound-0526.wav\n",
      "32 ../data/musan/noise/free-sound/noise-free-sound-0759.wav\n",
      "33 ../data/musan/noise/free-sound/noise-free-sound-0578.wav\n",
      "34 ../data/musan/noise/free-sound/noise-free-sound-0260.wav\n",
      "35 ../data/musan/noise/free-sound/noise-free-sound-0314.wav\n",
      "36 ../data/musan/noise/free-sound/noise-free-sound-0722.wav\n",
      "37 ../data/musan/noise/free-sound/noise-free-sound-0838.wav\n",
      "38 ../data/musan/noise/free-sound/noise-free-sound-0813.wav\n",
      "39 ../data/musan/noise/free-sound/noise-free-sound-0534.wav\n",
      "40 ../data/musan/noise/free-sound/noise-free-sound-0828.wav\n",
      "41 ../data/musan/noise/free-sound/noise-free-sound-0428.wav\n",
      "42 ../data/musan/noise/free-sound/noise-free-sound-0777.wav\n",
      "43 ../data/musan/noise/free-sound/noise-free-sound-0551.wav\n",
      "44 ../data/musan/noise/free-sound/noise-free-sound-0214.wav\n",
      "45 ../data/musan/noise/free-sound/noise-free-sound-0718.wav\n",
      "46 ../data/musan/noise/free-sound/noise-free-sound-0272.wav\n",
      "47 ../data/musan/noise/free-sound/noise-free-sound-0237.wav\n",
      "48 ../data/musan/noise/free-sound/noise-free-sound-0531.wav\n",
      "49 ../data/musan/noise/free-sound/noise-free-sound-0063.wav\n",
      "50 ../data/musan/noise/free-sound/noise-free-sound-0370.wav\n",
      "51 ../data/musan/noise/free-sound/noise-free-sound-0426.wav\n",
      "52 ../data/musan/noise/free-sound/noise-free-sound-0569.wav\n",
      "53 ../data/musan/noise/free-sound/noise-free-sound-0354.wav\n",
      "54 ../data/musan/noise/free-sound/noise-free-sound-0689.wav\n",
      "55 ../data/musan/noise/free-sound/noise-free-sound-0013.wav\n",
      "56 ../data/musan/noise/free-sound/noise-free-sound-0169.wav\n",
      "57 ../data/musan/noise/free-sound/noise-free-sound-0810.wav\n",
      "58 ../data/musan/noise/free-sound/noise-free-sound-0060.wav\n",
      "59 ../data/musan/noise/free-sound/noise-free-sound-0596.wav\n",
      "60 ../data/musan/noise/free-sound/noise-free-sound-0658.wav\n",
      "61 ../data/musan/noise/free-sound/noise-free-sound-0128.wav\n",
      "62 ../data/musan/noise/free-sound/noise-free-sound-0574.wav\n",
      "63 ../data/musan/noise/free-sound/noise-free-sound-0562.wav\n",
      "64 ../data/musan/noise/free-sound/noise-free-sound-0463.wav\n",
      "65 ../data/musan/noise/free-sound/noise-free-sound-0119.wav\n",
      "66 ../data/musan/noise/free-sound/noise-free-sound-0542.wav\n",
      "67 ../data/musan/noise/free-sound/noise-free-sound-0514.wav\n",
      "68 ../data/musan/noise/free-sound/noise-free-sound-0265.wav\n",
      "69 ../data/musan/noise/free-sound/noise-free-sound-0407.wav\n",
      "70 ../data/musan/noise/free-sound/noise-free-sound-0017.wav\n",
      "71 ../data/musan/noise/free-sound/noise-free-sound-0646.wav\n",
      "72 ../data/musan/noise/free-sound/noise-free-sound-0653.wav\n",
      "73 ../data/musan/noise/free-sound/noise-free-sound-0329.wav\n",
      "74 ../data/musan/noise/free-sound/noise-free-sound-0641.wav\n",
      "75 ../data/musan/noise/free-sound/noise-free-sound-0647.wav\n",
      "76 ../data/musan/noise/free-sound/noise-free-sound-0078.wav\n",
      "77 ../data/musan/noise/free-sound/noise-free-sound-0783.wav\n",
      "78 ../data/musan/noise/free-sound/noise-free-sound-0827.wav\n",
      "79 ../data/musan/noise/free-sound/noise-free-sound-0711.wav\n",
      "80 ../data/musan/noise/free-sound/noise-free-sound-0075.wav\n",
      "81 ../data/musan/noise/free-sound/noise-free-sound-0006.wav\n",
      "82 ../data/musan/noise/free-sound/noise-free-sound-0422.wav\n",
      "83 ../data/musan/noise/free-sound/noise-free-sound-0252.wav\n",
      "84 ../data/musan/noise/free-sound/noise-free-sound-0343.wav\n",
      "85 ../data/musan/noise/free-sound/noise-free-sound-0360.wav\n",
      "86 ../data/musan/noise/free-sound/noise-free-sound-0120.wav\n",
      "87 ../data/musan/noise/free-sound/noise-free-sound-0447.wav\n",
      "88 ../data/musan/noise/free-sound/noise-free-sound-0363.wav\n",
      "89 ../data/musan/noise/free-sound/noise-free-sound-0494.wav\n",
      "90 ../data/musan/noise/free-sound/noise-free-sound-0153.wav\n",
      "91 ../data/musan/noise/free-sound/noise-free-sound-0509.wav\n",
      "92 ../data/musan/noise/free-sound/noise-free-sound-0634.wav\n",
      "93 ../data/musan/noise/free-sound/noise-free-sound-0440.wav\n",
      "94 ../data/musan/noise/free-sound/noise-free-sound-0587.wav\n",
      "95 ../data/musan/noise/free-sound/noise-free-sound-0829.wav\n",
      "96 ../data/musan/noise/free-sound/noise-free-sound-0288.wav\n",
      "97 ../data/musan/noise/free-sound/noise-free-sound-0547.wav\n",
      "98 ../data/musan/noise/free-sound/noise-free-sound-0601.wav\n",
      "99 ../data/musan/noise/free-sound/noise-free-sound-0457.wav\n",
      "100 ../data/musan/noise/free-sound/noise-free-sound-0761.wav\n",
      "101 ../data/musan/noise/free-sound/noise-free-sound-0220.wav\n",
      "102 ../data/musan/noise/free-sound/noise-free-sound-0310.wav\n",
      "103 ../data/musan/noise/free-sound/noise-free-sound-0775.wav\n",
      "104 ../data/musan/noise/free-sound/noise-free-sound-0667.wav\n",
      "105 ../data/musan/noise/free-sound/noise-free-sound-0486.wav\n",
      "106 ../data/musan/noise/free-sound/noise-free-sound-0184.wav\n",
      "107 ../data/musan/noise/free-sound/noise-free-sound-0456.wav\n",
      "108 ../data/musan/noise/free-sound/noise-free-sound-0537.wav\n",
      "109 ../data/musan/noise/free-sound/noise-free-sound-0417.wav\n",
      "110 ../data/musan/noise/free-sound/noise-free-sound-0369.wav\n",
      "111 ../data/musan/noise/free-sound/noise-free-sound-0281.wav\n",
      "112 ../data/musan/noise/free-sound/noise-free-sound-0316.wav\n",
      "113 ../data/musan/noise/free-sound/noise-free-sound-0385.wav\n",
      "114 ../data/musan/noise/free-sound/noise-free-sound-0195.wav\n",
      "115 ../data/musan/noise/free-sound/noise-free-sound-0040.wav\n",
      "116 ../data/musan/noise/free-sound/noise-free-sound-0737.wav\n",
      "117 ../data/musan/noise/free-sound/noise-free-sound-0419.wav\n",
      "118 ../data/musan/noise/free-sound/noise-free-sound-0464.wav\n",
      "119 ../data/musan/noise/free-sound/noise-free-sound-0712.wav\n",
      "120 ../data/musan/noise/free-sound/noise-free-sound-0677.wav\n",
      "121 ../data/musan/noise/free-sound/noise-free-sound-0010.wav\n",
      "122 ../data/musan/noise/free-sound/noise-free-sound-0716.wav\n",
      "123 ../data/musan/noise/free-sound/noise-free-sound-0576.wav\n",
      "124 ../data/musan/noise/free-sound/noise-free-sound-0014.wav\n",
      "125 ../data/musan/noise/free-sound/noise-free-sound-0543.wav\n",
      "126 ../data/musan/noise/free-sound/noise-free-sound-0222.wav\n",
      "127 ../data/musan/noise/free-sound/noise-free-sound-0118.wav\n",
      "128 ../data/musan/noise/free-sound/noise-free-sound-0411.wav\n",
      "129 ../data/musan/noise/free-sound/noise-free-sound-0839.wav\n",
      "130 ../data/musan/noise/free-sound/noise-free-sound-0037.wav\n",
      "131 ../data/musan/noise/free-sound/noise-free-sound-0379.wav\n",
      "132 ../data/musan/noise/free-sound/noise-free-sound-0725.wav\n",
      "133 ../data/musan/noise/free-sound/noise-free-sound-0190.wav\n",
      "134 ../data/musan/noise/free-sound/noise-free-sound-0300.wav\n",
      "135 ../data/musan/noise/free-sound/noise-free-sound-0669.wav\n",
      "136 ../data/musan/noise/free-sound/noise-free-sound-0776.wav\n",
      "137 ../data/musan/noise/free-sound/noise-free-sound-0632.wav\n",
      "138 ../data/musan/noise/free-sound/noise-free-sound-0053.wav\n",
      "139 ../data/musan/noise/free-sound/noise-free-sound-0747.wav\n",
      "140 ../data/musan/noise/free-sound/noise-free-sound-0409.wav\n",
      "141 ../data/musan/noise/free-sound/noise-free-sound-0403.wav\n",
      "142 ../data/musan/noise/free-sound/noise-free-sound-0766.wav\n",
      "143 ../data/musan/noise/free-sound/noise-free-sound-0145.wav\n",
      "144 ../data/musan/noise/free-sound/noise-free-sound-0155.wav\n",
      "145 ../data/musan/noise/free-sound/noise-free-sound-0671.wav\n",
      "146 ../data/musan/noise/free-sound/noise-free-sound-0033.wav\n",
      "147 ../data/musan/noise/free-sound/noise-free-sound-0337.wav\n",
      "148 ../data/musan/noise/free-sound/noise-free-sound-0100.wav\n",
      "149 ../data/musan/noise/free-sound/noise-free-sound-0098.wav\n",
      "150 ../data/musan/noise/free-sound/noise-free-sound-0774.wav\n",
      "151 ../data/musan/noise/free-sound/noise-free-sound-0593.wav\n",
      "152 ../data/musan/noise/free-sound/noise-free-sound-0437.wav\n",
      "153 ../data/musan/noise/free-sound/noise-free-sound-0561.wav\n",
      "154 ../data/musan/noise/free-sound/noise-free-sound-0088.wav\n",
      "155 ../data/musan/noise/free-sound/noise-free-sound-0525.wav\n",
      "156 ../data/musan/noise/free-sound/noise-free-sound-0122.wav\n",
      "157 ../data/musan/noise/free-sound/noise-free-sound-0111.wav\n",
      "158 ../data/musan/noise/free-sound/noise-free-sound-0058.wav\n",
      "159 ../data/musan/noise/free-sound/noise-free-sound-0104.wav\n",
      "160 ../data/musan/noise/free-sound/noise-free-sound-0459.wav\n",
      "161 ../data/musan/noise/free-sound/noise-free-sound-0585.wav\n",
      "162 ../data/musan/noise/free-sound/noise-free-sound-0420.wav\n",
      "163 ../data/musan/noise/free-sound/noise-free-sound-0724.wav\n",
      "164 ../data/musan/noise/free-sound/noise-free-sound-0740.wav\n",
      "165 ../data/musan/noise/free-sound/noise-free-sound-0572.wav\n",
      "166 ../data/musan/noise/free-sound/noise-free-sound-0500.wav\n",
      "167 ../data/musan/noise/free-sound/noise-free-sound-0499.wav\n",
      "168 ../data/musan/noise/free-sound/noise-free-sound-0818.wav\n",
      "169 ../data/musan/noise/free-sound/noise-free-sound-0583.wav\n",
      "170 ../data/musan/noise/free-sound/noise-free-sound-0786.wav\n",
      "171 ../data/musan/noise/free-sound/noise-free-sound-0441.wav\n",
      "172 ../data/musan/noise/free-sound/noise-free-sound-0127.wav\n",
      "173 ../data/musan/noise/free-sound/noise-free-sound-0248.wav\n",
      "174 ../data/musan/noise/free-sound/noise-free-sound-0505.wav\n",
      "175 ../data/musan/noise/free-sound/noise-free-sound-0003.wav\n",
      "176 ../data/musan/noise/free-sound/noise-free-sound-0278.wav\n",
      "177 ../data/musan/noise/free-sound/noise-free-sound-0798.wav\n",
      "178 ../data/musan/noise/free-sound/noise-free-sound-0055.wav\n",
      "179 ../data/musan/noise/free-sound/noise-free-sound-0387.wav\n",
      "180 ../data/musan/noise/free-sound/noise-free-sound-0423.wav\n",
      "181 ../data/musan/noise/free-sound/noise-free-sound-0751.wav\n",
      "182 ../data/musan/noise/free-sound/noise-free-sound-0710.wav\n",
      "183 ../data/musan/noise/free-sound/noise-free-sound-0619.wav\n",
      "184 ../data/musan/noise/free-sound/noise-free-sound-0756.wav\n",
      "185 ../data/musan/noise/free-sound/noise-free-sound-0170.wav\n",
      "186 ../data/musan/noise/free-sound/noise-free-sound-0824.wav\n",
      "187 ../data/musan/noise/free-sound/noise-free-sound-0604.wav\n",
      "188 ../data/musan/noise/free-sound/noise-free-sound-0299.wav\n",
      "189 ../data/musan/noise/free-sound/noise-free-sound-0386.wav\n",
      "190 ../data/musan/noise/free-sound/noise-free-sound-0504.wav\n",
      "191 ../data/musan/noise/free-sound/noise-free-sound-0451.wav\n",
      "192 ../data/musan/noise/free-sound/noise-free-sound-0608.wav\n",
      "193 ../data/musan/noise/free-sound/noise-free-sound-0292.wav\n",
      "194 ../data/musan/noise/free-sound/noise-free-sound-0415.wav\n",
      "195 ../data/musan/noise/free-sound/noise-free-sound-0620.wav\n",
      "196 ../data/musan/noise/free-sound/noise-free-sound-0807.wav\n",
      "197 ../data/musan/noise/free-sound/noise-free-sound-0276.wav\n",
      "198 ../data/musan/noise/free-sound/noise-free-sound-0702.wav\n",
      "199 ../data/musan/noise/free-sound/noise-free-sound-0819.wav\n",
      "200 ../data/musan/noise/free-sound/noise-free-sound-0621.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 ../data/musan/noise/free-sound/noise-free-sound-0059.wav\n",
      "202 ../data/musan/noise/free-sound/noise-free-sound-0290.wav\n",
      "203 ../data/musan/noise/free-sound/noise-free-sound-0396.wav\n",
      "204 ../data/musan/noise/free-sound/noise-free-sound-0760.wav\n",
      "205 ../data/musan/noise/free-sound/noise-free-sound-0528.wav\n",
      "206 ../data/musan/noise/free-sound/noise-free-sound-0688.wav\n",
      "207 ../data/musan/noise/free-sound/noise-free-sound-0021.wav\n",
      "208 ../data/musan/noise/free-sound/noise-free-sound-0784.wav\n",
      "209 ../data/musan/noise/free-sound/noise-free-sound-0755.wav\n",
      "210 ../data/musan/noise/free-sound/noise-free-sound-0049.wav\n",
      "211 ../data/musan/noise/free-sound/noise-free-sound-0795.wav\n",
      "212 ../data/musan/noise/free-sound/noise-free-sound-0635.wav\n",
      "213 ../data/musan/noise/free-sound/noise-free-sound-0430.wav\n",
      "214 ../data/musan/noise/free-sound/noise-free-sound-0358.wav\n",
      "215 ../data/musan/noise/free-sound/noise-free-sound-0477.wav\n",
      "216 ../data/musan/noise/free-sound/noise-free-sound-0563.wav\n",
      "217 ../data/musan/noise/free-sound/noise-free-sound-0814.wav\n",
      "218 ../data/musan/noise/free-sound/noise-free-sound-0045.wav\n",
      "219 ../data/musan/noise/free-sound/noise-free-sound-0616.wav\n",
      "220 ../data/musan/noise/free-sound/noise-free-sound-0081.wav\n",
      "221 ../data/musan/noise/free-sound/noise-free-sound-0644.wav\n",
      "222 ../data/musan/noise/free-sound/noise-free-sound-0200.wav\n",
      "223 ../data/musan/noise/free-sound/noise-free-sound-0579.wav\n",
      "224 ../data/musan/noise/free-sound/noise-free-sound-0347.wav\n",
      "225 ../data/musan/noise/free-sound/noise-free-sound-0685.wav\n",
      "226 ../data/musan/noise/free-sound/noise-free-sound-0344.wav\n",
      "227 ../data/musan/noise/free-sound/noise-free-sound-0433.wav\n",
      "228 ../data/musan/noise/free-sound/noise-free-sound-0203.wav\n",
      "229 ../data/musan/noise/free-sound/noise-free-sound-0249.wav\n",
      "230 ../data/musan/noise/free-sound/noise-free-sound-0558.wav\n",
      "231 ../data/musan/noise/free-sound/noise-free-sound-0592.wav\n",
      "232 ../data/musan/noise/free-sound/noise-free-sound-0312.wav\n",
      "233 ../data/musan/noise/free-sound/noise-free-sound-0025.wav\n",
      "234 ../data/musan/noise/free-sound/noise-free-sound-0668.wav\n",
      "235 ../data/musan/noise/free-sound/noise-free-sound-0147.wav\n",
      "236 ../data/musan/noise/free-sound/noise-free-sound-0189.wav\n",
      "237 ../data/musan/noise/free-sound/noise-free-sound-0655.wav\n",
      "238 ../data/musan/noise/free-sound/noise-free-sound-0287.wav\n",
      "239 ../data/musan/noise/free-sound/noise-free-sound-0325.wav\n",
      "240 ../data/musan/noise/free-sound/noise-free-sound-0615.wav\n",
      "241 ../data/musan/noise/free-sound/noise-free-sound-0696.wav\n",
      "242 ../data/musan/noise/free-sound/noise-free-sound-0728.wav\n",
      "243 ../data/musan/noise/free-sound/noise-free-sound-0418.wav\n",
      "244 ../data/musan/noise/free-sound/noise-free-sound-0446.wav\n",
      "245 ../data/musan/noise/free-sound/noise-free-sound-0649.wav\n",
      "246 ../data/musan/noise/free-sound/noise-free-sound-0052.wav\n",
      "247 ../data/musan/noise/free-sound/noise-free-sound-0731.wav\n",
      "248 ../data/musan/noise/free-sound/noise-free-sound-0317.wav\n",
      "249 ../data/musan/noise/free-sound/noise-free-sound-0191.wav\n",
      "250 ../data/musan/noise/free-sound/noise-free-sound-0707.wav\n",
      "251 ../data/musan/noise/free-sound/noise-free-sound-0255.wav\n",
      "252 ../data/musan/noise/free-sound/noise-free-sound-0144.wav\n",
      "253 ../data/musan/noise/free-sound/noise-free-sound-0773.wav\n",
      "254 ../data/musan/noise/free-sound/noise-free-sound-0206.wav\n",
      "255 ../data/musan/noise/free-sound/noise-free-sound-0124.wav\n",
      "256 ../data/musan/noise/free-sound/noise-free-sound-0684.wav\n",
      "257 ../data/musan/noise/free-sound/noise-free-sound-0057.wav\n",
      "258 ../data/musan/noise/free-sound/noise-free-sound-0246.wav\n",
      "259 ../data/musan/noise/free-sound/noise-free-sound-0622.wav\n",
      "260 ../data/musan/noise/free-sound/noise-free-sound-0201.wav\n",
      "261 ../data/musan/noise/free-sound/noise-free-sound-0721.wav\n",
      "262 ../data/musan/noise/free-sound/noise-free-sound-0822.wav\n",
      "263 ../data/musan/noise/free-sound/noise-free-sound-0695.wav\n",
      "264 ../data/musan/noise/free-sound/noise-free-sound-0532.wav\n",
      "265 ../data/musan/noise/free-sound/noise-free-sound-0496.wav\n",
      "266 ../data/musan/noise/free-sound/noise-free-sound-0163.wav\n",
      "267 ../data/musan/noise/free-sound/noise-free-sound-0636.wav\n",
      "268 ../data/musan/noise/free-sound/noise-free-sound-0606.wav\n",
      "269 ../data/musan/noise/free-sound/noise-free-sound-0270.wav\n",
      "270 ../data/musan/noise/free-sound/noise-free-sound-0004.wav\n",
      "271 ../data/musan/noise/free-sound/noise-free-sound-0674.wav\n",
      "272 ../data/musan/noise/free-sound/noise-free-sound-0591.wav\n",
      "273 ../data/musan/noise/free-sound/noise-free-sound-0023.wav\n",
      "274 ../data/musan/noise/free-sound/noise-free-sound-0121.wav\n",
      "275 ../data/musan/noise/free-sound/noise-free-sound-0367.wav\n",
      "276 ../data/musan/noise/free-sound/noise-free-sound-0069.wav\n",
      "277 ../data/musan/noise/free-sound/noise-free-sound-0242.wav\n",
      "278 ../data/musan/noise/free-sound/noise-free-sound-0130.wav\n",
      "279 ../data/musan/noise/free-sound/noise-free-sound-0800.wav\n",
      "280 ../data/musan/noise/free-sound/noise-free-sound-0016.wav\n",
      "281 ../data/musan/noise/free-sound/noise-free-sound-0666.wav\n",
      "282 ../data/musan/noise/free-sound/noise-free-sound-0253.wav\n",
      "283 ../data/musan/noise/free-sound/noise-free-sound-0054.wav\n",
      "284 ../data/musan/noise/free-sound/noise-free-sound-0247.wav\n",
      "285 ../data/musan/noise/free-sound/noise-free-sound-0624.wav\n",
      "286 ../data/musan/noise/free-sound/noise-free-sound-0012.wav\n",
      "287 ../data/musan/noise/free-sound/noise-free-sound-0254.wav\n",
      "288 ../data/musan/noise/free-sound/noise-free-sound-0109.wav\n",
      "289 ../data/musan/noise/free-sound/noise-free-sound-0802.wav\n",
      "290 ../data/musan/noise/free-sound/noise-free-sound-0675.wav\n",
      "291 ../data/musan/noise/free-sound/noise-free-sound-0083.wav\n",
      "292 ../data/musan/noise/free-sound/noise-free-sound-0181.wav\n",
      "293 ../data/musan/noise/free-sound/noise-free-sound-0687.wav\n",
      "294 ../data/musan/noise/free-sound/noise-free-sound-0487.wav\n",
      "295 ../data/musan/noise/free-sound/noise-free-sound-0603.wav\n",
      "296 ../data/musan/noise/free-sound/noise-free-sound-0743.wav\n",
      "297 ../data/musan/noise/free-sound/noise-free-sound-0048.wav\n",
      "298 ../data/musan/noise/free-sound/noise-free-sound-0581.wav\n",
      "299 ../data/musan/noise/free-sound/noise-free-sound-0125.wav\n",
      "300 ../data/musan/noise/free-sound/noise-free-sound-0600.wav\n",
      "301 ../data/musan/noise/free-sound/noise-free-sound-0266.wav\n",
      "302 ../data/musan/noise/free-sound/noise-free-sound-0068.wav\n",
      "303 ../data/musan/noise/free-sound/noise-free-sound-0803.wav\n",
      "304 ../data/musan/noise/free-sound/noise-free-sound-0087.wav\n",
      "305 ../data/musan/noise/free-sound/noise-free-sound-0805.wav\n",
      "306 ../data/musan/noise/free-sound/noise-free-sound-0841.wav\n",
      "307 ../data/musan/noise/free-sound/noise-free-sound-0267.wav\n",
      "308 ../data/musan/noise/free-sound/noise-free-sound-0732.wav\n",
      "309 ../data/musan/noise/free-sound/noise-free-sound-0196.wav\n",
      "310 ../data/musan/noise/free-sound/noise-free-sound-0134.wav\n",
      "311 ../data/musan/noise/free-sound/noise-free-sound-0400.wav\n",
      "312 ../data/musan/noise/free-sound/noise-free-sound-0495.wav\n",
      "313 ../data/musan/noise/free-sound/noise-free-sound-0492.wav\n",
      "314 ../data/musan/noise/free-sound/noise-free-sound-0605.wav\n",
      "315 ../data/musan/noise/free-sound/noise-free-sound-0717.wav\n",
      "316 ../data/musan/noise/free-sound/noise-free-sound-0094.wav\n",
      "317 ../data/musan/noise/free-sound/noise-free-sound-0434.wav\n",
      "318 ../data/musan/noise/free-sound/noise-free-sound-0311.wav\n",
      "319 ../data/musan/noise/free-sound/noise-free-sound-0295.wav\n",
      "320 ../data/musan/noise/free-sound/noise-free-sound-0318.wav\n",
      "321 ../data/musan/noise/free-sound/noise-free-sound-0452.wav\n",
      "322 ../data/musan/noise/free-sound/noise-free-sound-0618.wav\n",
      "323 ../data/musan/noise/free-sound/noise-free-sound-0413.wav\n",
      "324 ../data/musan/noise/free-sound/noise-free-sound-0471.wav\n",
      "325 ../data/musan/noise/free-sound/noise-free-sound-0796.wav\n",
      "326 ../data/musan/noise/free-sound/noise-free-sound-0092.wav\n",
      "327 ../data/musan/noise/free-sound/noise-free-sound-0703.wav\n",
      "328 ../data/musan/noise/free-sound/noise-free-sound-0811.wav\n",
      "329 ../data/musan/noise/free-sound/noise-free-sound-0808.wav\n",
      "330 ../data/musan/noise/free-sound/noise-free-sound-0590.wav\n",
      "331 ../data/musan/noise/free-sound/noise-free-sound-0157.wav\n",
      "332 ../data/musan/noise/free-sound/noise-free-sound-0402.wav\n",
      "333 ../data/musan/noise/free-sound/noise-free-sound-0140.wav\n",
      "334 ../data/musan/noise/free-sound/noise-free-sound-0763.wav\n",
      "335 ../data/musan/noise/free-sound/noise-free-sound-0323.wav\n",
      "336 ../data/musan/noise/free-sound/noise-free-sound-0209.wav\n",
      "337 ../data/musan/noise/free-sound/noise-free-sound-0729.wav\n",
      "338 ../data/musan/noise/free-sound/noise-free-sound-0085.wav\n",
      "339 ../data/musan/noise/free-sound/noise-free-sound-0375.wav\n",
      "340 ../data/musan/noise/free-sound/noise-free-sound-0042.wav\n",
      "341 ../data/musan/noise/free-sound/noise-free-sound-0559.wav\n",
      "342 ../data/musan/noise/free-sound/noise-free-sound-0414.wav\n",
      "343 ../data/musan/noise/free-sound/noise-free-sound-0374.wav\n",
      "344 ../data/musan/noise/free-sound/noise-free-sound-0066.wav\n",
      "345 ../data/musan/noise/free-sound/noise-free-sound-0368.wav\n",
      "346 ../data/musan/noise/free-sound/noise-free-sound-0629.wav\n",
      "347 ../data/musan/noise/free-sound/noise-free-sound-0303.wav\n",
      "348 ../data/musan/noise/free-sound/noise-free-sound-0146.wav\n",
      "349 ../data/musan/noise/free-sound/noise-free-sound-0333.wav\n",
      "350 ../data/musan/noise/free-sound/noise-free-sound-0283.wav\n",
      "351 ../data/musan/noise/free-sound/noise-free-sound-0091.wav\n",
      "352 ../data/musan/noise/free-sound/noise-free-sound-0485.wav\n",
      "353 ../data/musan/noise/free-sound/noise-free-sound-0271.wav\n",
      "354 ../data/musan/noise/free-sound/noise-free-sound-0297.wav\n",
      "355 ../data/musan/noise/free-sound/noise-free-sound-0309.wav\n",
      "356 ../data/musan/noise/free-sound/noise-free-sound-0167.wav\n",
      "357 ../data/musan/noise/free-sound/noise-free-sound-0213.wav\n",
      "358 ../data/musan/noise/free-sound/noise-free-sound-0365.wav\n",
      "359 ../data/musan/noise/free-sound/noise-free-sound-0665.wav\n",
      "360 ../data/musan/noise/free-sound/noise-free-sound-0321.wav\n",
      "361 ../data/musan/noise/free-sound/noise-free-sound-0557.wav\n",
      "362 ../data/musan/noise/free-sound/noise-free-sound-0720.wav\n",
      "363 ../data/musan/noise/free-sound/noise-free-sound-0335.wav\n",
      "364 ../data/musan/noise/free-sound/noise-free-sound-0250.wav\n",
      "365 ../data/musan/noise/free-sound/noise-free-sound-0518.wav\n",
      "366 ../data/musan/noise/free-sound/noise-free-sound-0334.wav\n",
      "367 ../data/musan/noise/free-sound/noise-free-sound-0216.wav\n",
      "368 ../data/musan/noise/free-sound/noise-free-sound-0171.wav\n",
      "369 ../data/musan/noise/free-sound/noise-free-sound-0498.wav\n",
      "370 ../data/musan/noise/free-sound/noise-free-sound-0617.wav\n",
      "371 ../data/musan/noise/free-sound/noise-free-sound-0510.wav\n",
      "372 ../data/musan/noise/free-sound/noise-free-sound-0296.wav\n",
      "373 ../data/musan/noise/free-sound/noise-free-sound-0584.wav\n",
      "374 ../data/musan/noise/free-sound/noise-free-sound-0436.wav\n",
      "375 ../data/musan/noise/free-sound/noise-free-sound-0764.wav\n",
      "376 ../data/musan/noise/free-sound/noise-free-sound-0826.wav\n",
      "377 ../data/musan/noise/free-sound/noise-free-sound-0533.wav\n",
      "378 ../data/musan/noise/free-sound/noise-free-sound-0793.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "379 ../data/musan/noise/free-sound/noise-free-sound-0524.wav\n",
      "380 ../data/musan/noise/free-sound/noise-free-sound-0791.wav\n",
      "381 ../data/musan/noise/free-sound/noise-free-sound-0454.wav\n",
      "382 ../data/musan/noise/free-sound/noise-free-sound-0397.wav\n",
      "383 ../data/musan/noise/free-sound/noise-free-sound-0226.wav\n",
      "384 ../data/musan/noise/free-sound/noise-free-sound-0186.wav\n",
      "385 ../data/musan/noise/free-sound/noise-free-sound-0096.wav\n",
      "386 ../data/musan/noise/free-sound/noise-free-sound-0765.wav\n",
      "387 ../data/musan/noise/free-sound/noise-free-sound-0008.wav\n",
      "388 ../data/musan/noise/free-sound/noise-free-sound-0801.wav\n",
      "389 ../data/musan/noise/free-sound/noise-free-sound-0207.wav\n",
      "390 ../data/musan/noise/free-sound/noise-free-sound-0790.wav\n",
      "391 ../data/musan/noise/free-sound/noise-free-sound-0837.wav\n",
      "392 ../data/musan/noise/free-sound/noise-free-sound-0706.wav\n",
      "393 ../data/musan/noise/free-sound/noise-free-sound-0554.wav\n",
      "394 ../data/musan/noise/free-sound/noise-free-sound-0112.wav\n",
      "395 ../data/musan/noise/free-sound/noise-free-sound-0107.wav\n",
      "396 ../data/musan/noise/free-sound/noise-free-sound-0101.wav\n",
      "397 ../data/musan/noise/free-sound/noise-free-sound-0405.wav\n",
      "398 ../data/musan/noise/free-sound/noise-free-sound-0398.wav\n",
      "399 ../data/musan/noise/free-sound/noise-free-sound-0834.wav\n",
      "400 ../data/musan/noise/free-sound/noise-free-sound-0395.wav\n",
      "401 ../data/musan/noise/free-sound/noise-free-sound-0645.wav\n",
      "402 ../data/musan/noise/free-sound/noise-free-sound-0491.wav\n",
      "403 ../data/musan/noise/free-sound/noise-free-sound-0546.wav\n",
      "404 ../data/musan/noise/free-sound/noise-free-sound-0429.wav\n",
      "405 ../data/musan/noise/free-sound/noise-free-sound-0289.wav\n",
      "406 ../data/musan/noise/free-sound/noise-free-sound-0382.wav\n",
      "407 ../data/musan/noise/free-sound/noise-free-sound-0227.wav\n",
      "408 ../data/musan/noise/free-sound/noise-free-sound-0779.wav\n",
      "409 ../data/musan/noise/free-sound/noise-free-sound-0258.wav\n",
      "410 ../data/musan/noise/free-sound/noise-free-sound-0568.wav\n",
      "411 ../data/musan/noise/free-sound/noise-free-sound-0357.wav\n",
      "412 ../data/musan/noise/free-sound/noise-free-sound-0320.wav\n",
      "413 ../data/musan/noise/free-sound/noise-free-sound-0117.wav\n",
      "414 ../data/musan/noise/free-sound/noise-free-sound-0432.wav\n",
      "415 ../data/musan/noise/free-sound/noise-free-sound-0412.wav\n",
      "416 ../data/musan/noise/free-sound/noise-free-sound-0298.wav\n",
      "417 ../data/musan/noise/free-sound/noise-free-sound-0633.wav\n",
      "418 ../data/musan/noise/free-sound/noise-free-sound-0545.wav\n",
      "419 ../data/musan/noise/free-sound/noise-free-sound-0005.wav\n",
      "420 ../data/musan/noise/free-sound/noise-free-sound-0478.wav\n",
      "421 ../data/musan/noise/free-sound/noise-free-sound-0516.wav\n",
      "422 ../data/musan/noise/free-sound/noise-free-sound-0598.wav\n",
      "423 ../data/musan/noise/free-sound/noise-free-sound-0610.wav\n",
      "424 ../data/musan/noise/free-sound/noise-free-sound-0614.wav\n",
      "425 ../data/musan/noise/free-sound/noise-free-sound-0123.wav\n",
      "426 ../data/musan/noise/free-sound/noise-free-sound-0394.wav\n",
      "427 ../data/musan/noise/free-sound/noise-free-sound-0767.wav\n",
      "428 ../data/musan/noise/free-sound/noise-free-sound-0210.wav\n",
      "429 ../data/musan/noise/free-sound/noise-free-sound-0346.wav\n",
      "430 ../data/musan/noise/free-sound/noise-free-sound-0204.wav\n",
      "431 ../data/musan/noise/free-sound/noise-free-sound-0229.wav\n",
      "432 ../data/musan/noise/free-sound/noise-free-sound-0797.wav\n",
      "433 ../data/musan/noise/free-sound/noise-free-sound-0680.wav\n",
      "434 ../data/musan/noise/free-sound/noise-free-sound-0823.wav\n",
      "435 ../data/musan/noise/free-sound/noise-free-sound-0686.wav\n",
      "436 ../data/musan/noise/free-sound/noise-free-sound-0821.wav\n",
      "437 ../data/musan/noise/free-sound/noise-free-sound-0781.wav\n",
      "438 ../data/musan/noise/free-sound/noise-free-sound-0693.wav\n",
      "439 ../data/musan/noise/free-sound/noise-free-sound-0535.wav\n",
      "440 ../data/musan/noise/free-sound/noise-free-sound-0682.wav\n",
      "441 ../data/musan/noise/free-sound/noise-free-sound-0168.wav\n",
      "442 ../data/musan/noise/free-sound/noise-free-sound-0245.wav\n",
      "443 ../data/musan/noise/free-sound/noise-free-sound-0079.wav\n",
      "444 ../data/musan/noise/free-sound/noise-free-sound-0670.wav\n",
      "445 ../data/musan/noise/free-sound/noise-free-sound-0714.wav\n",
      "446 ../data/musan/noise/free-sound/noise-free-sound-0215.wav\n",
      "447 ../data/musan/noise/free-sound/noise-free-sound-0031.wav\n",
      "448 ../data/musan/noise/free-sound/noise-free-sound-0733.wav\n",
      "449 ../data/musan/noise/free-sound/noise-free-sound-0753.wav\n",
      "450 ../data/musan/noise/free-sound/noise-free-sound-0657.wav\n",
      "451 ../data/musan/noise/free-sound/noise-free-sound-0570.wav\n",
      "452 ../data/musan/noise/free-sound/noise-free-sound-0076.wav\n",
      "453 ../data/musan/noise/free-sound/noise-free-sound-0771.wav\n",
      "454 ../data/musan/noise/free-sound/noise-free-sound-0093.wav\n",
      "455 ../data/musan/noise/free-sound/noise-free-sound-0541.wav\n",
      "456 ../data/musan/noise/free-sound/noise-free-sound-0453.wav\n",
      "457 ../data/musan/noise/free-sound/noise-free-sound-0564.wav\n",
      "458 ../data/musan/noise/free-sound/noise-free-sound-0466.wav\n",
      "459 ../data/musan/noise/free-sound/noise-free-sound-0611.wav\n",
      "460 ../data/musan/noise/free-sound/noise-free-sound-0458.wav\n",
      "461 ../data/musan/noise/free-sound/noise-free-sound-0416.wav\n",
      "462 ../data/musan/noise/free-sound/noise-free-sound-0787.wav\n",
      "463 ../data/musan/noise/free-sound/noise-free-sound-0178.wav\n",
      "464 ../data/musan/noise/free-sound/noise-free-sound-0540.wav\n",
      "465 ../data/musan/noise/free-sound/noise-free-sound-0694.wav\n",
      "466 ../data/musan/noise/free-sound/noise-free-sound-0388.wav\n",
      "467 ../data/musan/noise/free-sound/noise-free-sound-0338.wav\n",
      "468 ../data/musan/noise/free-sound/noise-free-sound-0460.wav\n",
      "469 ../data/musan/noise/free-sound/noise-free-sound-0061.wav\n",
      "470 ../data/musan/noise/free-sound/noise-free-sound-0175.wav\n",
      "471 ../data/musan/noise/free-sound/noise-free-sound-0815.wav\n",
      "472 ../data/musan/noise/free-sound/noise-free-sound-0631.wav\n",
      "473 ../data/musan/noise/free-sound/noise-free-sound-0450.wav\n",
      "474 ../data/musan/noise/free-sound/noise-free-sound-0659.wav\n",
      "475 ../data/musan/noise/free-sound/noise-free-sound-0089.wav\n",
      "476 ../data/musan/noise/free-sound/noise-free-sound-0427.wav\n",
      "477 ../data/musan/noise/free-sound/noise-free-sound-0331.wav\n",
      "478 ../data/musan/noise/free-sound/noise-free-sound-0701.wav\n",
      "479 ../data/musan/noise/free-sound/noise-free-sound-0328.wav\n",
      "480 ../data/musan/noise/free-sound/noise-free-sound-0654.wav\n",
      "481 ../data/musan/noise/free-sound/noise-free-sound-0176.wav\n",
      "482 ../data/musan/noise/free-sound/noise-free-sound-0135.wav\n",
      "483 ../data/musan/noise/free-sound/noise-free-sound-0697.wav\n",
      "484 ../data/musan/noise/free-sound/noise-free-sound-0745.wav\n",
      "485 ../data/musan/noise/free-sound/noise-free-sound-0353.wav\n",
      "486 ../data/musan/noise/free-sound/noise-free-sound-0056.wav\n",
      "487 ../data/musan/noise/free-sound/noise-free-sound-0035.wav\n",
      "488 ../data/musan/noise/free-sound/noise-free-sound-0511.wav\n",
      "489 ../data/musan/noise/free-sound/noise-free-sound-0544.wav\n",
      "490 ../data/musan/noise/free-sound/noise-free-sound-0002.wav\n",
      "491 ../data/musan/noise/free-sound/noise-free-sound-0438.wav\n",
      "492 ../data/musan/noise/free-sound/noise-free-sound-0393.wav\n",
      "493 ../data/musan/noise/free-sound/noise-free-sound-0589.wav\n",
      "494 ../data/musan/noise/free-sound/noise-free-sound-0011.wav\n",
      "495 ../data/musan/noise/free-sound/noise-free-sound-0573.wav\n",
      "496 ../data/musan/noise/free-sound/noise-free-sound-0269.wav\n",
      "497 ../data/musan/noise/free-sound/noise-free-sound-0497.wav\n",
      "498 ../data/musan/noise/free-sound/noise-free-sound-0835.wav\n",
      "499 ../data/musan/noise/free-sound/noise-free-sound-0185.wav\n",
      "500 ../data/musan/noise/free-sound/noise-free-sound-0062.wav\n",
      "501 ../data/musan/noise/free-sound/noise-free-sound-0692.wav\n",
      "502 ../data/musan/noise/free-sound/noise-free-sound-0286.wav\n",
      "503 ../data/musan/noise/free-sound/noise-free-sound-0741.wav\n",
      "504 ../data/musan/noise/free-sound/noise-free-sound-0110.wav\n",
      "505 ../data/musan/noise/free-sound/noise-free-sound-0030.wav\n",
      "506 ../data/musan/noise/free-sound/noise-free-sound-0825.wav\n",
      "507 ../data/musan/noise/free-sound/noise-free-sound-0424.wav\n",
      "508 ../data/musan/noise/free-sound/noise-free-sound-0332.wav\n",
      "509 ../data/musan/noise/free-sound/noise-free-sound-0219.wav\n",
      "510 ../data/musan/noise/free-sound/noise-free-sound-0503.wav\n",
      "511 ../data/musan/noise/free-sound/noise-free-sound-0594.wav\n",
      "512 ../data/musan/noise/free-sound/noise-free-sound-0772.wav\n",
      "513 ../data/musan/noise/free-sound/noise-free-sound-0182.wav\n",
      "514 ../data/musan/noise/free-sound/noise-free-sound-0308.wav\n",
      "515 ../data/musan/noise/free-sound/noise-free-sound-0384.wav\n",
      "516 ../data/musan/noise/free-sound/noise-free-sound-0626.wav\n",
      "517 ../data/musan/noise/free-sound/noise-free-sound-0231.wav\n",
      "518 ../data/musan/noise/free-sound/noise-free-sound-0391.wav\n",
      "519 ../data/musan/noise/free-sound/noise-free-sound-0816.wav\n",
      "520 ../data/musan/noise/free-sound/noise-free-sound-0575.wav\n",
      "521 ../data/musan/noise/free-sound/noise-free-sound-0481.wav\n",
      "522 ../data/musan/noise/free-sound/noise-free-sound-0263.wav\n",
      "523 ../data/musan/noise/free-sound/noise-free-sound-0735.wav\n",
      "524 ../data/musan/noise/free-sound/noise-free-sound-0361.wav\n",
      "525 ../data/musan/noise/free-sound/noise-free-sound-0313.wav\n",
      "526 ../data/musan/noise/free-sound/noise-free-sound-0651.wav\n",
      "527 ../data/musan/noise/free-sound/noise-free-sound-0051.wav\n",
      "528 ../data/musan/noise/free-sound/noise-free-sound-0074.wav\n",
      "529 ../data/musan/noise/free-sound/noise-free-sound-0640.wav\n",
      "530 ../data/musan/noise/free-sound/noise-free-sound-0156.wav\n",
      "531 ../data/musan/noise/free-sound/noise-free-sound-0809.wav\n",
      "532 ../data/musan/noise/free-sound/noise-free-sound-0623.wav\n",
      "533 ../data/musan/noise/free-sound/noise-free-sound-0022.wav\n",
      "534 ../data/musan/noise/free-sound/noise-free-sound-0114.wav\n",
      "535 ../data/musan/noise/free-sound/noise-free-sound-0001.wav\n",
      "536 ../data/musan/noise/free-sound/noise-free-sound-0469.wav\n",
      "537 ../data/musan/noise/free-sound/noise-free-sound-0307.wav\n",
      "538 ../data/musan/noise/free-sound/noise-free-sound-0371.wav\n",
      "539 ../data/musan/noise/free-sound/noise-free-sound-0362.wav\n",
      "540 ../data/musan/noise/free-sound/noise-free-sound-0538.wav\n",
      "541 ../data/musan/noise/free-sound/noise-free-sound-0150.wav\n",
      "542 ../data/musan/noise/free-sound/noise-free-sound-0293.wav\n",
      "543 ../data/musan/noise/free-sound/noise-free-sound-0470.wav\n",
      "544 ../data/musan/noise/free-sound/noise-free-sound-0521.wav\n",
      "545 ../data/musan/noise/free-sound/noise-free-sound-0159.wav\n",
      "546 ../data/musan/noise/free-sound/noise-free-sound-0241.wav\n",
      "547 ../data/musan/noise/free-sound/noise-free-sound-0046.wav\n",
      "548 ../data/musan/noise/free-sound/noise-free-sound-0026.wav\n",
      "549 ../data/musan/noise/free-sound/noise-free-sound-0280.wav\n",
      "550 ../data/musan/noise/free-sound/noise-free-sound-0268.wav\n",
      "551 ../data/musan/noise/free-sound/noise-free-sound-0072.wav\n",
      "552 ../data/musan/noise/free-sound/noise-free-sound-0439.wav\n",
      "553 ../data/musan/noise/free-sound/noise-free-sound-0174.wav\n",
      "554 ../data/musan/noise/free-sound/noise-free-sound-0070.wav\n",
      "555 ../data/musan/noise/free-sound/noise-free-sound-0106.wav\n",
      "556 ../data/musan/noise/free-sound/noise-free-sound-0515.wav\n",
      "557 ../data/musan/noise/free-sound/noise-free-sound-0698.wav\n",
      "558 ../data/musan/noise/free-sound/noise-free-sound-0324.wav\n",
      "559 ../data/musan/noise/free-sound/noise-free-sound-0389.wav\n",
      "560 ../data/musan/noise/free-sound/noise-free-sound-0804.wav\n",
      "561 ../data/musan/noise/free-sound/noise-free-sound-0390.wav\n",
      "562 ../data/musan/noise/free-sound/noise-free-sound-0476.wav\n",
      "563 ../data/musan/noise/free-sound/noise-free-sound-0350.wav\n",
      "564 ../data/musan/noise/free-sound/noise-free-sound-0065.wav\n",
      "565 ../data/musan/noise/free-sound/noise-free-sound-0700.wav\n",
      "566 ../data/musan/noise/free-sound/noise-free-sound-0355.wav\n",
      "567 ../data/musan/noise/free-sound/noise-free-sound-0506.wav\n",
      "568 ../data/musan/noise/free-sound/noise-free-sound-0232.wav\n",
      "569 ../data/musan/noise/free-sound/noise-free-sound-0484.wav\n",
      "570 ../data/musan/noise/free-sound/noise-free-sound-0244.wav\n",
      "571 ../data/musan/noise/free-sound/noise-free-sound-0474.wav\n",
      "572 ../data/musan/noise/free-sound/noise-free-sound-0676.wav\n",
      "573 ../data/musan/noise/free-sound/noise-free-sound-0552.wav\n",
      "574 ../data/musan/noise/free-sound/noise-free-sound-0768.wav\n",
      "575 ../data/musan/noise/free-sound/noise-free-sound-0642.wav\n",
      "576 ../data/musan/noise/free-sound/noise-free-sound-0294.wav\n",
      "577 ../data/musan/noise/free-sound/noise-free-sound-0342.wav\n",
      "578 ../data/musan/noise/free-sound/noise-free-sound-0556.wav\n",
      "579 ../data/musan/noise/free-sound/noise-free-sound-0084.wav\n",
      "580 ../data/musan/noise/free-sound/noise-free-sound-0664.wav\n",
      "581 ../data/musan/noise/free-sound/noise-free-sound-0399.wav\n",
      "582 ../data/musan/noise/free-sound/noise-free-sound-0726.wav\n",
      "583 ../data/musan/noise/free-sound/noise-free-sound-0549.wav\n",
      "584 ../data/musan/noise/free-sound/noise-free-sound-0243.wav\n",
      "585 ../data/musan/noise/free-sound/noise-free-sound-0345.wav\n",
      "586 ../data/musan/noise/free-sound/noise-free-sound-0501.wav\n",
      "587 ../data/musan/noise/free-sound/noise-free-sound-0126.wav\n",
      "588 ../data/musan/noise/free-sound/noise-free-sound-0757.wav\n",
      "589 ../data/musan/noise/free-sound/noise-free-sound-0239.wav\n",
      "590 ../data/musan/noise/free-sound/noise-free-sound-0221.wav\n",
      "591 ../data/musan/noise/free-sound/noise-free-sound-0482.wav\n",
      "592 ../data/musan/noise/free-sound/noise-free-sound-0652.wav\n",
      "593 ../data/musan/noise/free-sound/noise-free-sound-0172.wav\n",
      "594 ../data/musan/noise/free-sound/noise-free-sound-0007.wav\n",
      "595 ../data/musan/noise/free-sound/noise-free-sound-0274.wav\n",
      "596 ../data/musan/noise/free-sound/noise-free-sound-0719.wav\n",
      "597 ../data/musan/noise/free-sound/noise-free-sound-0319.wav\n",
      "598 ../data/musan/noise/free-sound/noise-free-sound-0812.wav\n",
      "599 ../data/musan/noise/free-sound/noise-free-sound-0199.wav\n",
      "600 ../data/musan/noise/free-sound/noise-free-sound-0782.wav\n",
      "601 ../data/musan/noise/free-sound/noise-free-sound-0392.wav\n",
      "602 ../data/musan/noise/free-sound/noise-free-sound-0142.wav\n",
      "603 ../data/musan/noise/free-sound/noise-free-sound-0188.wav\n",
      "604 ../data/musan/noise/free-sound/noise-free-sound-0832.wav\n",
      "605 ../data/musan/noise/free-sound/noise-free-sound-0736.wav\n",
      "606 ../data/musan/noise/free-sound/noise-free-sound-0198.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607 ../data/musan/noise/free-sound/noise-free-sound-0225.wav\n",
      "608 ../data/musan/noise/free-sound/noise-free-sound-0285.wav\n",
      "609 ../data/musan/noise/free-sound/noise-free-sound-0628.wav\n",
      "610 ../data/musan/noise/free-sound/noise-free-sound-0234.wav\n",
      "611 ../data/musan/noise/free-sound/noise-free-sound-0067.wav\n",
      "612 ../data/musan/noise/free-sound/noise-free-sound-0555.wav\n",
      "613 ../data/musan/noise/free-sound/noise-free-sound-0517.wav\n",
      "614 ../data/musan/noise/free-sound/noise-free-sound-0132.wav\n",
      "615 ../data/musan/noise/free-sound/noise-free-sound-0264.wav\n",
      "616 ../data/musan/noise/free-sound/noise-free-sound-0401.wav\n",
      "617 ../data/musan/noise/free-sound/noise-free-sound-0792.wav\n",
      "618 ../data/musan/noise/free-sound/noise-free-sound-0425.wav\n",
      "619 ../data/musan/noise/free-sound/noise-free-sound-0224.wav\n",
      "620 ../data/musan/noise/free-sound/noise-free-sound-0820.wav\n",
      "621 ../data/musan/noise/free-sound/noise-free-sound-0009.wav\n",
      "622 ../data/musan/noise/free-sound/noise-free-sound-0377.wav\n",
      "623 ../data/musan/noise/free-sound/noise-free-sound-0149.wav\n",
      "624 ../data/musan/noise/free-sound/noise-free-sound-0479.wav\n",
      "625 ../data/musan/noise/free-sound/noise-free-sound-0077.wav\n",
      "626 ../data/musan/noise/free-sound/noise-free-sound-0588.wav\n",
      "627 ../data/musan/noise/free-sound/noise-free-sound-0282.wav\n",
      "628 ../data/musan/noise/free-sound/noise-free-sound-0340.wav\n",
      "629 ../data/musan/noise/free-sound/noise-free-sound-0752.wav\n",
      "630 ../data/musan/noise/free-sound/noise-free-sound-0708.wav\n",
      "631 ../data/musan/noise/free-sound/noise-free-sound-0709.wav\n",
      "632 ../data/musan/noise/free-sound/noise-free-sound-0197.wav\n",
      "633 ../data/musan/noise/free-sound/noise-free-sound-0435.wav\n",
      "634 ../data/musan/noise/free-sound/noise-free-sound-0550.wav\n",
      "635 ../data/musan/noise/free-sound/noise-free-sound-0609.wav\n",
      "636 ../data/musan/noise/free-sound/noise-free-sound-0627.wav\n",
      "637 ../data/musan/noise/free-sound/noise-free-sound-0443.wav\n",
      "638 ../data/musan/noise/free-sound/noise-free-sound-0639.wav\n",
      "639 ../data/musan/noise/free-sound/noise-free-sound-0715.wav\n",
      "640 ../data/musan/noise/free-sound/noise-free-sound-0279.wav\n",
      "641 ../data/musan/noise/free-sound/noise-free-sound-0148.wav\n",
      "642 ../data/musan/noise/free-sound/noise-free-sound-0408.wav\n",
      "643 ../data/musan/noise/free-sound/noise-free-sound-0187.wav\n",
      "644 ../data/musan/noise/free-sound/noise-free-sound-0406.wav\n",
      "645 ../data/musan/noise/free-sound/noise-free-sound-0780.wav\n",
      "646 ../data/musan/noise/free-sound/noise-free-sound-0136.wav\n",
      "647 ../data/musan/noise/free-sound/noise-free-sound-0108.wav\n",
      "648 ../data/musan/noise/free-sound/noise-free-sound-0359.wav\n",
      "649 ../data/musan/noise/free-sound/noise-free-sound-0522.wav\n",
      "650 ../data/musan/noise/free-sound/noise-free-sound-0831.wav\n",
      "651 ../data/musan/noise/free-sound/noise-free-sound-0000.wav\n",
      "652 ../data/musan/noise/free-sound/noise-free-sound-0284.wav\n",
      "653 ../data/musan/noise/free-sound/noise-free-sound-0179.wav\n",
      "654 ../data/musan/noise/free-sound/noise-free-sound-0480.wav\n",
      "655 ../data/musan/noise/free-sound/noise-free-sound-0672.wav\n",
      "656 ../data/musan/noise/free-sound/noise-free-sound-0444.wav\n",
      "657 ../data/musan/noise/free-sound/noise-free-sound-0291.wav\n",
      "658 ../data/musan/noise/free-sound/noise-free-sound-0205.wav\n",
      "659 ../data/musan/noise/free-sound/noise-free-sound-0352.wav\n",
      "660 ../data/musan/noise/free-sound/noise-free-sound-0165.wav\n",
      "661 ../data/musan/noise/free-sound/noise-free-sound-0770.wav\n",
      "662 ../data/musan/noise/free-sound/noise-free-sound-0713.wav\n",
      "663 ../data/musan/noise/free-sound/noise-free-sound-0137.wav\n",
      "664 ../data/musan/noise/free-sound/noise-free-sound-0233.wav\n",
      "665 ../data/musan/noise/free-sound/noise-free-sound-0166.wav\n",
      "666 ../data/musan/noise/free-sound/noise-free-sound-0462.wav\n",
      "667 ../data/musan/noise/free-sound/noise-free-sound-0690.wav\n",
      "668 ../data/musan/noise/free-sound/noise-free-sound-0691.wav\n",
      "669 ../data/musan/noise/free-sound/noise-free-sound-0341.wav\n",
      "670 ../data/musan/noise/free-sound/noise-free-sound-0154.wav\n",
      "671 ../data/musan/noise/free-sound/noise-free-sound-0656.wav\n",
      "672 ../data/musan/noise/free-sound/noise-free-sound-0421.wav\n",
      "673 ../data/musan/noise/free-sound/noise-free-sound-0472.wav\n",
      "674 ../data/musan/noise/free-sound/noise-free-sound-0465.wav\n",
      "675 ../data/musan/noise/free-sound/noise-free-sound-0158.wav\n",
      "676 ../data/musan/noise/free-sound/noise-free-sound-0788.wav\n",
      "677 ../data/musan/noise/free-sound/noise-free-sound-0508.wav\n",
      "678 ../data/musan/noise/free-sound/noise-free-sound-0086.wav\n",
      "679 ../data/musan/noise/free-sound/noise-free-sound-0530.wav\n",
      "680 ../data/musan/noise/free-sound/noise-free-sound-0840.wav\n",
      "681 ../data/musan/noise/free-sound/noise-free-sound-0257.wav\n",
      "682 ../data/musan/noise/free-sound/noise-free-sound-0366.wav\n",
      "683 ../data/musan/noise/free-sound/noise-free-sound-0322.wav\n",
      "684 ../data/musan/noise/free-sound/noise-free-sound-0475.wav\n",
      "685 ../data/musan/noise/free-sound/noise-free-sound-0661.wav\n",
      "686 ../data/musan/noise/free-sound/noise-free-sound-0681.wav\n",
      "687 ../data/musan/noise/free-sound/noise-free-sound-0238.wav\n",
      "688 ../data/musan/noise/free-sound/noise-free-sound-0778.wav\n",
      "689 ../data/musan/noise/free-sound/noise-free-sound-0467.wav\n",
      "690 ../data/musan/noise/free-sound/noise-free-sound-0080.wav\n",
      "691 ../data/musan/noise/free-sound/noise-free-sound-0448.wav\n",
      "692 ../data/musan/noise/free-sound/noise-free-sound-0103.wav\n",
      "693 ../data/musan/noise/free-sound/noise-free-sound-0746.wav\n",
      "694 ../data/musan/noise/free-sound/noise-free-sound-0431.wav\n",
      "695 ../data/musan/noise/free-sound/noise-free-sound-0643.wav\n",
      "696 ../data/musan/noise/free-sound/noise-free-sound-0240.wav\n",
      "697 ../data/musan/noise/free-sound/noise-free-sound-0625.wav\n",
      "698 ../data/musan/noise/free-sound/noise-free-sound-0523.wav\n",
      "699 ../data/musan/noise/free-sound/noise-free-sound-0305.wav\n",
      "700 ../data/musan/noise/free-sound/noise-free-sound-0599.wav\n",
      "701 ../data/musan/noise/free-sound/noise-free-sound-0705.wav\n",
      "702 ../data/musan/noise/free-sound/noise-free-sound-0683.wav\n",
      "703 ../data/musan/noise/free-sound/noise-free-sound-0177.wav\n",
      "704 ../data/musan/noise/free-sound/noise-free-sound-0571.wav\n",
      "705 ../data/musan/noise/free-sound/noise-free-sound-0758.wav\n",
      "706 ../data/musan/noise/free-sound/noise-free-sound-0520.wav\n",
      "707 ../data/musan/noise/free-sound/noise-free-sound-0513.wav\n",
      "708 ../data/musan/noise/free-sound/noise-free-sound-0769.wav\n",
      "709 ../data/musan/noise/free-sound/noise-free-sound-0129.wav\n",
      "710 ../data/musan/noise/free-sound/noise-free-sound-0817.wav\n",
      "711 ../data/musan/noise/free-sound/noise-free-sound-0650.wav\n",
      "712 ../data/musan/noise/free-sound/noise-free-sound-0194.wav\n",
      "713 ../data/musan/noise/free-sound/noise-free-sound-0519.wav\n",
      "714 ../data/musan/noise/free-sound/noise-free-sound-0273.wav\n",
      "715 ../data/musan/noise/free-sound/noise-free-sound-0339.wav\n",
      "716 ../data/musan/noise/free-sound/noise-free-sound-0192.wav\n",
      "717 ../data/musan/noise/free-sound/noise-free-sound-0607.wav\n",
      "718 ../data/musan/noise/free-sound/noise-free-sound-0372.wav\n",
      "719 ../data/musan/noise/free-sound/noise-free-sound-0211.wav\n",
      "720 ../data/musan/noise/free-sound/noise-free-sound-0580.wav\n",
      "721 ../data/musan/noise/free-sound/noise-free-sound-0133.wav\n",
      "722 ../data/musan/noise/free-sound/noise-free-sound-0032.wav\n",
      "723 ../data/musan/noise/free-sound/noise-free-sound-0277.wav\n",
      "724 ../data/musan/noise/free-sound/noise-free-sound-0164.wav\n",
      "725 ../data/musan/noise/free-sound/noise-free-sound-0113.wav\n",
      "726 ../data/musan/noise/free-sound/noise-free-sound-0586.wav\n",
      "727 ../data/musan/noise/free-sound/noise-free-sound-0473.wav\n",
      "728 ../data/musan/noise/free-sound/noise-free-sound-0356.wav\n",
      "729 ../data/musan/noise/free-sound/noise-free-sound-0833.wav\n",
      "730 ../data/musan/noise/free-sound/noise-free-sound-0028.wav\n",
      "731 ../data/musan/noise/free-sound/noise-free-sound-0073.wav\n",
      "732 ../data/musan/noise/free-sound/noise-free-sound-0662.wav\n",
      "733 ../data/musan/noise/free-sound/noise-free-sound-0223.wav\n",
      "734 ../data/musan/noise/free-sound/noise-free-sound-0330.wav\n",
      "735 ../data/musan/noise/free-sound/noise-free-sound-0071.wav\n",
      "736 ../data/musan/noise/free-sound/noise-free-sound-0723.wav\n",
      "737 ../data/musan/noise/free-sound/noise-free-sound-0376.wav\n",
      "738 ../data/musan/noise/free-sound/noise-free-sound-0730.wav\n",
      "739 ../data/musan/noise/free-sound/noise-free-sound-0327.wav\n",
      "740 ../data/musan/noise/free-sound/noise-free-sound-0261.wav\n",
      "741 ../data/musan/noise/free-sound/noise-free-sound-0097.wav\n",
      "742 ../data/musan/noise/free-sound/noise-free-sound-0582.wav\n",
      "743 ../data/musan/noise/free-sound/noise-free-sound-0442.wav\n",
      "744 ../data/musan/noise/free-sound/noise-free-sound-0064.wav\n",
      "745 ../data/musan/noise/free-sound/noise-free-sound-0036.wav\n",
      "746 ../data/musan/noise/free-sound/noise-free-sound-0131.wav\n",
      "747 ../data/musan/noise/free-sound/noise-free-sound-0251.wav\n",
      "748 ../data/musan/noise/free-sound/noise-free-sound-0613.wav\n",
      "749 ../data/musan/noise/free-sound/noise-free-sound-0351.wav\n",
      "750 ../data/musan/noise/free-sound/noise-free-sound-0679.wav\n",
      "751 ../data/musan/noise/free-sound/noise-free-sound-0794.wav\n",
      "752 ../data/musan/noise/free-sound/noise-free-sound-0217.wav\n",
      "753 ../data/musan/noise/free-sound/noise-free-sound-0749.wav\n",
      "754 ../data/musan/noise/free-sound/noise-free-sound-0047.wav\n",
      "755 ../data/musan/noise/free-sound/noise-free-sound-0020.wav\n",
      "756 ../data/musan/noise/free-sound/noise-free-sound-0597.wav\n",
      "757 ../data/musan/noise/free-sound/noise-free-sound-0799.wav\n",
      "758 ../data/musan/noise/free-sound/noise-free-sound-0648.wav\n",
      "759 ../data/musan/noise/free-sound/noise-free-sound-0304.wav\n",
      "760 ../data/musan/noise/free-sound/noise-free-sound-0529.wav\n",
      "761 ../data/musan/noise/free-sound/noise-free-sound-0082.wav\n",
      "762 ../data/musan/noise/free-sound/noise-free-sound-0090.wav\n",
      "763 ../data/musan/noise/free-sound/noise-free-sound-0493.wav\n",
      "764 ../data/musan/noise/free-sound/noise-free-sound-0018.wav\n",
      "765 ../data/musan/noise/free-sound/noise-free-sound-0256.wav\n",
      "766 ../data/musan/noise/free-sound/noise-free-sound-0468.wav\n",
      "767 ../data/musan/noise/free-sound/noise-free-sound-0383.wav\n",
      "768 ../data/musan/noise/free-sound/noise-free-sound-0673.wav\n",
      "769 ../data/musan/noise/free-sound/noise-free-sound-0553.wav\n",
      "770 ../data/musan/noise/free-sound/noise-free-sound-0041.wav\n",
      "771 ../data/musan/noise/free-sound/noise-free-sound-0034.wav\n",
      "772 ../data/musan/noise/free-sound/noise-free-sound-0183.wav\n",
      "773 ../data/musan/noise/free-sound/noise-free-sound-0275.wav\n",
      "774 ../data/musan/noise/free-sound/noise-free-sound-0015.wav\n",
      "775 ../data/musan/noise/free-sound/noise-free-sound-0663.wav\n",
      "776 ../data/musan/noise/free-sound/noise-free-sound-0836.wav\n",
      "777 ../data/musan/noise/free-sound/noise-free-sound-0102.wav\n",
      "778 ../data/musan/noise/free-sound/noise-free-sound-0748.wav\n",
      "779 ../data/musan/noise/free-sound/noise-free-sound-0019.wav\n",
      "780 ../data/musan/noise/free-sound/noise-free-sound-0349.wav\n",
      "781 ../data/musan/noise/free-sound/noise-free-sound-0050.wav\n",
      "782 ../data/musan/noise/free-sound/noise-free-sound-0455.wav\n",
      "783 ../data/musan/noise/free-sound/noise-free-sound-0173.wav\n",
      "784 ../data/musan/noise/free-sound/noise-free-sound-0577.wav\n",
      "785 ../data/musan/noise/free-sound/noise-free-sound-0660.wav\n",
      "786 ../data/musan/noise/free-sound/noise-free-sound-0602.wav\n",
      "787 ../data/musan/noise/free-sound/noise-free-sound-0151.wav\n",
      "788 ../data/musan/noise/free-sound/noise-free-sound-0143.wav\n",
      "789 ../data/musan/noise/free-sound/noise-free-sound-0742.wav\n",
      "790 ../data/musan/noise/free-sound/noise-free-sound-0704.wav\n",
      "791 ../data/musan/noise/free-sound/noise-free-sound-0380.wav\n",
      "792 ../data/musan/noise/free-sound/noise-free-sound-0678.wav\n",
      "793 ../data/musan/noise/free-sound/noise-free-sound-0218.wav\n",
      "794 ../data/musan/noise/free-sound/noise-free-sound-0739.wav\n",
      "795 ../data/musan/noise/free-sound/noise-free-sound-0160.wav\n",
      "796 ../data/musan/noise/free-sound/noise-free-sound-0139.wav\n",
      "797 ../data/musan/noise/free-sound/noise-free-sound-0539.wav\n",
      "798 ../data/musan/noise/free-sound/noise-free-sound-0373.wav\n",
      "799 ../data/musan/noise/free-sound/noise-free-sound-0044.wav\n",
      "800 ../data/musan/noise/free-sound/noise-free-sound-0301.wav\n",
      "801 ../data/musan/noise/free-sound/noise-free-sound-0105.wav\n",
      "802 ../data/musan/noise/free-sound/noise-free-sound-0099.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "803 ../data/musan/noise/free-sound/noise-free-sound-0560.wav\n",
      "804 ../data/musan/noise/free-sound/noise-free-sound-0595.wav\n",
      "805 ../data/musan/noise/free-sound/noise-free-sound-0235.wav\n",
      "806 ../data/musan/noise/free-sound/noise-free-sound-0039.wav\n",
      "807 ../data/musan/noise/free-sound/noise-free-sound-0637.wav\n",
      "808 ../data/musan/noise/free-sound/noise-free-sound-0490.wav\n",
      "809 ../data/musan/noise/free-sound/noise-free-sound-0202.wav\n",
      "810 ../data/musan/noise/free-sound/noise-free-sound-0449.wav\n",
      "811 ../data/musan/noise/free-sound/noise-free-sound-0228.wav\n",
      "812 ../data/musan/noise/free-sound/noise-free-sound-0381.wav\n",
      "813 ../data/musan/noise/free-sound/noise-free-sound-0024.wav\n",
      "814 ../data/musan/noise/free-sound/noise-free-sound-0180.wav\n",
      "815 ../data/musan/noise/free-sound/noise-free-sound-0548.wav\n",
      "816 ../data/musan/noise/free-sound/noise-free-sound-0095.wav\n",
      "817 ../data/musan/noise/free-sound/noise-free-sound-0638.wav\n",
      "818 ../data/musan/noise/free-sound/noise-free-sound-0027.wav\n",
      "819 ../data/musan/noise/free-sound/noise-free-sound-0404.wav\n",
      "820 ../data/musan/noise/free-sound/noise-free-sound-0483.wav\n",
      "821 ../data/musan/noise/free-sound/noise-free-sound-0785.wav\n",
      "822 ../data/musan/noise/free-sound/noise-free-sound-0842.wav\n",
      "823 ../data/musan/noise/free-sound/noise-free-sound-0236.wav\n",
      "824 ../data/musan/noise/free-sound/noise-free-sound-0630.wav\n",
      "825 ../data/musan/noise/free-sound/noise-free-sound-0161.wav\n",
      "826 ../data/musan/noise/free-sound/noise-free-sound-0364.wav\n",
      "827 ../data/musan/noise/free-sound/noise-free-sound-0315.wav\n",
      "828 ../data/musan/noise/free-sound/noise-free-sound-0208.wav\n",
      "829 ../data/musan/noise/free-sound/noise-free-sound-0750.wav\n",
      "830 ../data/musan/noise/free-sound/noise-free-sound-0699.wav\n",
      "831 ../data/musan/noise/free-sound/noise-free-sound-0262.wav\n",
      "832 ../data/musan/noise/free-sound/noise-free-sound-0029.wav\n",
      "833 ../data/musan/noise/free-sound/noise-free-sound-0410.wav\n",
      "834 ../data/musan/noise/free-sound/noise-free-sound-0789.wav\n",
      "835 ../data/musan/noise/free-sound/noise-free-sound-0043.wav\n",
      "836 ../data/musan/noise/free-sound/noise-free-sound-0326.wav\n",
      "837 ../data/musan/noise/free-sound/noise-free-sound-0754.wav\n",
      "838 ../data/musan/noise/free-sound/noise-free-sound-0230.wav\n",
      "839 ../data/musan/noise/free-sound/noise-free-sound-0138.wav\n",
      "840 ../data/musan/noise/free-sound/noise-free-sound-0193.wav\n",
      "841 ../data/musan/noise/free-sound/noise-free-sound-0348.wav\n",
      "842 ../data/musan/noise/free-sound/noise-free-sound-0116.wav\n",
      "843 ../data/musan/noise/sound-bible/noise-sound-bible-0063.wav\n",
      "844 ../data/musan/noise/sound-bible/noise-sound-bible-0042.wav\n",
      "845 ../data/musan/noise/sound-bible/noise-sound-bible-0077.wav\n",
      "846 ../data/musan/noise/sound-bible/noise-sound-bible-0005.wav\n",
      "847 ../data/musan/noise/sound-bible/noise-sound-bible-0067.wav\n",
      "848 ../data/musan/noise/sound-bible/noise-sound-bible-0019.wav\n",
      "849 ../data/musan/noise/sound-bible/noise-sound-bible-0061.wav\n",
      "850 ../data/musan/noise/sound-bible/noise-sound-bible-0058.wav\n",
      "851 ../data/musan/noise/sound-bible/noise-sound-bible-0000.wav\n",
      "852 ../data/musan/noise/sound-bible/noise-sound-bible-0018.wav\n",
      "853 ../data/musan/noise/sound-bible/noise-sound-bible-0029.wav\n",
      "854 ../data/musan/noise/sound-bible/noise-sound-bible-0051.wav\n",
      "855 ../data/musan/noise/sound-bible/noise-sound-bible-0073.wav\n",
      "856 ../data/musan/noise/sound-bible/noise-sound-bible-0001.wav\n",
      "857 ../data/musan/noise/sound-bible/noise-sound-bible-0070.wav\n",
      "858 ../data/musan/noise/sound-bible/noise-sound-bible-0028.wav\n",
      "859 ../data/musan/noise/sound-bible/noise-sound-bible-0080.wav\n",
      "860 ../data/musan/noise/sound-bible/noise-sound-bible-0038.wav\n",
      "861 ../data/musan/noise/sound-bible/noise-sound-bible-0057.wav\n",
      "862 ../data/musan/noise/sound-bible/noise-sound-bible-0083.wav\n",
      "863 ../data/musan/noise/sound-bible/noise-sound-bible-0048.wav\n",
      "864 ../data/musan/noise/sound-bible/noise-sound-bible-0059.wav\n",
      "865 ../data/musan/noise/sound-bible/noise-sound-bible-0017.wav\n",
      "866 ../data/musan/noise/sound-bible/noise-sound-bible-0031.wav\n",
      "867 ../data/musan/noise/sound-bible/noise-sound-bible-0072.wav\n",
      "868 ../data/musan/noise/sound-bible/noise-sound-bible-0047.wav\n",
      "869 ../data/musan/noise/sound-bible/noise-sound-bible-0066.wav\n",
      "870 ../data/musan/noise/sound-bible/noise-sound-bible-0026.wav\n",
      "871 ../data/musan/noise/sound-bible/noise-sound-bible-0053.wav\n",
      "872 ../data/musan/noise/sound-bible/noise-sound-bible-0011.wav\n",
      "873 ../data/musan/noise/sound-bible/noise-sound-bible-0004.wav\n",
      "874 ../data/musan/noise/sound-bible/noise-sound-bible-0036.wav\n",
      "875 ../data/musan/noise/sound-bible/noise-sound-bible-0044.wav\n",
      "876 ../data/musan/noise/sound-bible/noise-sound-bible-0023.wav\n",
      "877 ../data/musan/noise/sound-bible/noise-sound-bible-0065.wav\n",
      "878 ../data/musan/noise/sound-bible/noise-sound-bible-0052.wav\n",
      "879 ../data/musan/noise/sound-bible/noise-sound-bible-0024.wav\n",
      "880 ../data/musan/noise/sound-bible/noise-sound-bible-0076.wav\n",
      "881 ../data/musan/noise/sound-bible/noise-sound-bible-0010.wav\n",
      "882 ../data/musan/noise/sound-bible/noise-sound-bible-0041.wav\n",
      "883 ../data/musan/noise/sound-bible/noise-sound-bible-0002.wav\n",
      "884 ../data/musan/noise/sound-bible/noise-sound-bible-0075.wav\n",
      "885 ../data/musan/noise/sound-bible/noise-sound-bible-0033.wav\n",
      "886 ../data/musan/noise/sound-bible/noise-sound-bible-0020.wav\n",
      "887 ../data/musan/noise/sound-bible/noise-sound-bible-0040.wav\n",
      "888 ../data/musan/noise/sound-bible/noise-sound-bible-0022.wav\n",
      "889 ../data/musan/noise/sound-bible/noise-sound-bible-0014.wav\n",
      "890 ../data/musan/noise/sound-bible/noise-sound-bible-0039.wav\n",
      "891 ../data/musan/noise/sound-bible/noise-sound-bible-0071.wav\n",
      "892 ../data/musan/noise/sound-bible/noise-sound-bible-0032.wav\n",
      "893 ../data/musan/noise/sound-bible/noise-sound-bible-0016.wav\n",
      "894 ../data/musan/noise/sound-bible/noise-sound-bible-0060.wav\n",
      "895 ../data/musan/noise/sound-bible/noise-sound-bible-0055.wav\n",
      "896 ../data/musan/noise/sound-bible/noise-sound-bible-0012.wav\n",
      "897 ../data/musan/noise/sound-bible/noise-sound-bible-0035.wav\n",
      "898 ../data/musan/noise/sound-bible/noise-sound-bible-0008.wav\n",
      "899 ../data/musan/noise/sound-bible/noise-sound-bible-0034.wav\n",
      "900 ../data/musan/noise/sound-bible/noise-sound-bible-0085.wav\n",
      "901 ../data/musan/noise/sound-bible/noise-sound-bible-0006.wav\n",
      "902 ../data/musan/noise/sound-bible/noise-sound-bible-0062.wav\n",
      "903 ../data/musan/noise/sound-bible/noise-sound-bible-0046.wav\n",
      "904 ../data/musan/noise/sound-bible/noise-sound-bible-0049.wav\n",
      "905 ../data/musan/noise/sound-bible/noise-sound-bible-0021.wav\n",
      "906 ../data/musan/noise/sound-bible/noise-sound-bible-0064.wav\n",
      "907 ../data/musan/noise/sound-bible/noise-sound-bible-0013.wav\n",
      "908 ../data/musan/noise/sound-bible/noise-sound-bible-0045.wav\n",
      "909 ../data/musan/noise/sound-bible/noise-sound-bible-0007.wav\n",
      "910 ../data/musan/noise/sound-bible/noise-sound-bible-0030.wav\n",
      "911 ../data/musan/noise/sound-bible/noise-sound-bible-0069.wav\n",
      "912 ../data/musan/noise/sound-bible/noise-sound-bible-0084.wav\n",
      "913 ../data/musan/noise/sound-bible/noise-sound-bible-0025.wav\n",
      "914 ../data/musan/noise/sound-bible/noise-sound-bible-0068.wav\n",
      "915 ../data/musan/noise/sound-bible/noise-sound-bible-0078.wav\n",
      "916 ../data/musan/noise/sound-bible/noise-sound-bible-0027.wav\n",
      "917 ../data/musan/noise/sound-bible/noise-sound-bible-0081.wav\n",
      "918 ../data/musan/noise/sound-bible/noise-sound-bible-0079.wav\n",
      "919 ../data/musan/noise/sound-bible/noise-sound-bible-0009.wav\n",
      "920 ../data/musan/noise/sound-bible/noise-sound-bible-0086.wav\n",
      "921 ../data/musan/noise/sound-bible/noise-sound-bible-0015.wav\n",
      "922 ../data/musan/noise/sound-bible/noise-sound-bible-0043.wav\n",
      "923 ../data/musan/noise/sound-bible/noise-sound-bible-0074.wav\n",
      "924 ../data/musan/noise/sound-bible/noise-sound-bible-0037.wav\n",
      "925 ../data/musan/noise/sound-bible/noise-sound-bible-0003.wav\n",
      "926 ../data/musan/noise/sound-bible/noise-sound-bible-0050.wav\n",
      "927 ../data/musan/noise/sound-bible/noise-sound-bible-0056.wav\n",
      "928 ../data/musan/noise/sound-bible/noise-sound-bible-0082.wav\n",
      "929 ../data/musan/noise/sound-bible/noise-sound-bible-0054.wav\n",
      "930 ../data/musan/music/fma/music-fma-0023.wav\n",
      "931 ../data/musan/music/fma/music-fma-0098.wav\n",
      "932 ../data/musan/music/fma/music-fma-0007.wav\n",
      "933 ../data/musan/music/fma/music-fma-0100.wav\n",
      "934 ../data/musan/music/fma/music-fma-0038.wav\n",
      "935 ../data/musan/music/fma/music-fma-0063.wav\n",
      "936 ../data/musan/music/fma/music-fma-0059.wav\n",
      "937 ../data/musan/music/fma/music-fma-0060.wav\n",
      "938 ../data/musan/music/fma/music-fma-0031.wav\n",
      "939 ../data/musan/music/fma/music-fma-0070.wav\n",
      "940 ../data/musan/music/fma/music-fma-0080.wav\n",
      "941 ../data/musan/music/fma/music-fma-0004.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "942 ../data/musan/music/fma/music-fma-0041.wav\n",
      "943 ../data/musan/music/fma/music-fma-0104.wav\n",
      "944 ../data/musan/music/fma/music-fma-0071.wav\n",
      "945 ../data/musan/music/fma/music-fma-0108.wav\n",
      "946 ../data/musan/music/fma/music-fma-0027.wav\n",
      "947 ../data/musan/music/fma/music-fma-0044.wav\n",
      "948 ../data/musan/music/fma/music-fma-0020.wav\n",
      "949 ../data/musan/music/fma/music-fma-0110.wav\n",
      "950 ../data/musan/music/fma/music-fma-0079.wav\n",
      "951 ../data/musan/music/fma/music-fma-0033.wav\n",
      "952 ../data/musan/music/fma/music-fma-0123.wav\n",
      "953 ../data/musan/music/fma/music-fma-0009.wav\n",
      "954 ../data/musan/music/fma/music-fma-0077.wav\n",
      "955 ../data/musan/music/fma/music-fma-0006.wav\n",
      "956 ../data/musan/music/fma/music-fma-0081.wav\n",
      "957 ../data/musan/music/fma/music-fma-0126.wav\n",
      "958 ../data/musan/music/fma/music-fma-0065.wav\n",
      "959 ../data/musan/music/fma/music-fma-0019.wav\n",
      "960 ../data/musan/music/fma/music-fma-0092.wav\n",
      "961 ../data/musan/music/fma/music-fma-0002.wav\n",
      "962 ../data/musan/music/fma/music-fma-0018.wav\n",
      "963 ../data/musan/music/fma/music-fma-0089.wav\n",
      "964 ../data/musan/music/fma/music-fma-0086.wav\n",
      "965 ../data/musan/music/fma/music-fma-0087.wav\n",
      "966 ../data/musan/music/fma/music-fma-0026.wav\n",
      "967 ../data/musan/music/fma/music-fma-0064.wav\n",
      "968 ../data/musan/music/fma/music-fma-0035.wav\n",
      "969 ../data/musan/music/fma/music-fma-0003.wav\n",
      "970 ../data/musan/music/fma/music-fma-0029.wav\n",
      "971 ../data/musan/music/fma/music-fma-0013.wav\n",
      "972 ../data/musan/music/fma/music-fma-0037.wav\n",
      "973 ../data/musan/music/fma/music-fma-0119.wav\n",
      "974 ../data/musan/music/fma/music-fma-0046.wav\n",
      "975 ../data/musan/music/fma/music-fma-0093.wav\n",
      "976 ../data/musan/music/fma/music-fma-0047.wav\n",
      "977 ../data/musan/music/fma/music-fma-0127.wav\n",
      "978 ../data/musan/music/fma/music-fma-0094.wav\n",
      "979 ../data/musan/music/fma/music-fma-0022.wav\n",
      "980 ../data/musan/music/fma/music-fma-0015.wav\n",
      "981 ../data/musan/music/fma/music-fma-0000.wav\n",
      "982 ../data/musan/music/fma/music-fma-0056.wav\n",
      "983 ../data/musan/music/fma/music-fma-0042.wav\n",
      "984 ../data/musan/music/fma/music-fma-0122.wav\n",
      "985 ../data/musan/music/fma/music-fma-0068.wav\n",
      "986 ../data/musan/music/fma/music-fma-0096.wav\n",
      "987 ../data/musan/music/fma/music-fma-0088.wav\n",
      "988 ../data/musan/music/fma/music-fma-0099.wav\n",
      "989 ../data/musan/music/fma/music-fma-0076.wav\n",
      "990 ../data/musan/music/fma/music-fma-0066.wav\n",
      "991 ../data/musan/music/fma/music-fma-0005.wav\n",
      "992 ../data/musan/music/fma/music-fma-0045.wav\n",
      "993 ../data/musan/music/fma/music-fma-0011.wav\n",
      "994 ../data/musan/music/fma/music-fma-0008.wav\n",
      "995 ../data/musan/music/fma/music-fma-0028.wav\n",
      "996 ../data/musan/music/fma/music-fma-0039.wav\n",
      "997 ../data/musan/music/fma/music-fma-0091.wav\n",
      "998 ../data/musan/music/fma/music-fma-0090.wav\n",
      "999 ../data/musan/music/fma/music-fma-0125.wav\n",
      "1000 ../data/musan/music/fma/music-fma-0043.wav\n",
      "1001 ../data/musan/music/fma/music-fma-0014.wav\n",
      "1002 ../data/musan/music/fma/music-fma-0114.wav\n",
      "1003 ../data/musan/music/fma/music-fma-0102.wav\n",
      "1004 ../data/musan/music/fma/music-fma-0072.wav\n",
      "1005 ../data/musan/music/fma/music-fma-0024.wav\n",
      "1006 ../data/musan/music/fma/music-fma-0111.wav\n",
      "1007 ../data/musan/music/fma/music-fma-0121.wav\n",
      "1008 ../data/musan/music/fma/music-fma-0097.wav\n",
      "1009 ../data/musan/music/fma/music-fma-0001.wav\n",
      "1010 ../data/musan/music/fma/music-fma-0116.wav\n",
      "1011 ../data/musan/music/fma/music-fma-0115.wav\n",
      "1012 ../data/musan/music/fma/music-fma-0040.wav\n",
      "1013 ../data/musan/music/fma/music-fma-0055.wav\n",
      "1014 ../data/musan/music/fma/music-fma-0058.wav\n",
      "1015 ../data/musan/music/fma/music-fma-0083.wav\n",
      "1016 ../data/musan/music/fma/music-fma-0073.wav\n",
      "1017 ../data/musan/music/fma/music-fma-0105.wav\n",
      "1018 ../data/musan/music/fma/music-fma-0062.wav\n",
      "1019 ../data/musan/music/fma/music-fma-0103.wav\n",
      "1020 ../data/musan/music/fma/music-fma-0032.wav\n",
      "1021 ../data/musan/music/fma/music-fma-0048.wav\n",
      "1022 ../data/musan/music/fma/music-fma-0118.wav\n",
      "1023 ../data/musan/music/fma/music-fma-0061.wav\n",
      "1024 ../data/musan/music/fma/music-fma-0112.wav\n",
      "1025 ../data/musan/music/fma/music-fma-0052.wav\n",
      "1026 ../data/musan/music/fma/music-fma-0034.wav\n",
      "1027 ../data/musan/music/fma/music-fma-0117.wav\n",
      "1028 ../data/musan/music/fma/music-fma-0074.wav\n",
      "1029 ../data/musan/music/fma/music-fma-0016.wav\n",
      "1030 ../data/musan/music/fma/music-fma-0095.wav\n",
      "1031 ../data/musan/music/fma/music-fma-0010.wav\n",
      "1032 ../data/musan/music/fma/music-fma-0054.wav\n",
      "1033 ../data/musan/music/fma/music-fma-0057.wav\n",
      "1034 ../data/musan/music/fma/music-fma-0075.wav\n",
      "1035 ../data/musan/music/fma/music-fma-0113.wav\n",
      "1036 ../data/musan/music/fma/music-fma-0053.wav\n",
      "1037 ../data/musan/music/fma/music-fma-0069.wav\n",
      "1038 ../data/musan/music/fma/music-fma-0036.wav\n",
      "1039 ../data/musan/music/fma/music-fma-0082.wav\n",
      "1040 ../data/musan/music/fma/music-fma-0085.wav\n",
      "1041 ../data/musan/music/fma/music-fma-0049.wav\n",
      "1042 ../data/musan/music/fma/music-fma-0107.wav\n",
      "1043 ../data/musan/music/fma/music-fma-0050.wav\n",
      "1044 ../data/musan/music/fma/music-fma-0101.wav\n",
      "1045 ../data/musan/music/fma/music-fma-0109.wav\n",
      "1046 ../data/musan/music/fma/music-fma-0124.wav\n",
      "1047 ../data/musan/music/fma/music-fma-0021.wav\n",
      "1048 ../data/musan/music/fma/music-fma-0017.wav\n",
      "1049 ../data/musan/music/fma/music-fma-0030.wav\n",
      "1050 ../data/musan/music/fma/music-fma-0078.wav\n",
      "1051 ../data/musan/music/fma/music-fma-0120.wav\n",
      "1052 ../data/musan/music/fma/music-fma-0067.wav\n",
      "1053 ../data/musan/music/fma/music-fma-0106.wav\n",
      "1054 ../data/musan/music/fma/music-fma-0025.wav\n",
      "1055 ../data/musan/music/fma/music-fma-0012.wav\n",
      "1056 ../data/musan/music/fma/music-fma-0084.wav\n",
      "1057 ../data/musan/music/fma/music-fma-0051.wav\n",
      "1058 ../data/musan/music/hd-classical/music-hd-0045.wav\n",
      "1059 ../data/musan/music/hd-classical/music-hd-0066.wav\n",
      "1060 ../data/musan/music/hd-classical/music-hd-0055.wav\n",
      "1061 ../data/musan/music/hd-classical/music-hd-0048.wav\n",
      "1062 ../data/musan/music/hd-classical/music-hd-0009.wav\n",
      "1063 ../data/musan/music/hd-classical/music-hd-0069.wav\n",
      "1064 ../data/musan/music/hd-classical/music-hd-0005.wav\n",
      "1065 ../data/musan/music/hd-classical/music-hd-0038.wav\n",
      "1066 ../data/musan/music/hd-classical/music-hd-0034.wav\n",
      "1067 ../data/musan/music/hd-classical/music-hd-0053.wav\n",
      "1068 ../data/musan/music/hd-classical/music-hd-0049.wav\n",
      "1069 ../data/musan/music/hd-classical/music-hd-0036.wav\n",
      "1070 ../data/musan/music/hd-classical/music-hd-0027.wav\n",
      "1071 ../data/musan/music/hd-classical/music-hd-0011.wav\n",
      "1072 ../data/musan/music/hd-classical/music-hd-0002.wav\n",
      "1073 ../data/musan/music/hd-classical/music-hd-0017.wav\n",
      "1074 ../data/musan/music/hd-classical/music-hd-0044.wav\n",
      "1075 ../data/musan/music/hd-classical/music-hd-0023.wav\n",
      "1076 ../data/musan/music/hd-classical/music-hd-0039.wav\n",
      "1077 ../data/musan/music/hd-classical/music-hd-0057.wav\n",
      "1078 ../data/musan/music/hd-classical/music-hd-0064.wav\n",
      "1079 ../data/musan/music/hd-classical/music-hd-0007.wav\n",
      "1080 ../data/musan/music/hd-classical/music-hd-0031.wav\n",
      "1081 ../data/musan/music/hd-classical/music-hd-0050.wav\n",
      "1082 ../data/musan/music/hd-classical/music-hd-0008.wav\n",
      "1083 ../data/musan/music/hd-classical/music-hd-0018.wav\n",
      "1084 ../data/musan/music/hd-classical/music-hd-0003.wav\n",
      "1085 ../data/musan/music/hd-classical/music-hd-0033.wav\n",
      "1086 ../data/musan/music/hd-classical/music-hd-0029.wav\n",
      "1087 ../data/musan/music/hd-classical/music-hd-0006.wav\n",
      "1088 ../data/musan/music/hd-classical/music-hd-0042.wav\n",
      "1089 ../data/musan/music/hd-classical/music-hd-0026.wav\n",
      "1090 ../data/musan/music/hd-classical/music-hd-0072.wav\n",
      "1091 ../data/musan/music/hd-classical/music-hd-0051.wav\n",
      "1092 ../data/musan/music/hd-classical/music-hd-0019.wav\n",
      "1093 ../data/musan/music/hd-classical/music-hd-0014.wav\n",
      "1094 ../data/musan/music/hd-classical/music-hd-0004.wav\n",
      "1095 ../data/musan/music/hd-classical/music-hd-0043.wav\n",
      "1096 ../data/musan/music/hd-classical/music-hd-0058.wav\n",
      "1097 ../data/musan/music/hd-classical/music-hd-0030.wav\n",
      "1098 ../data/musan/music/hd-classical/music-hd-0012.wav\n",
      "1099 ../data/musan/music/hd-classical/music-hd-0010.wav\n",
      "1100 ../data/musan/music/hd-classical/music-hd-0016.wav\n",
      "1101 ../data/musan/music/hd-classical/music-hd-0062.wav\n",
      "1102 ../data/musan/music/hd-classical/music-hd-0037.wav\n",
      "1103 ../data/musan/music/hd-classical/music-hd-0035.wav\n",
      "1104 ../data/musan/music/hd-classical/music-hd-0070.wav\n",
      "1105 ../data/musan/music/hd-classical/music-hd-0001.wav\n",
      "1106 ../data/musan/music/hd-classical/music-hd-0025.wav\n",
      "1107 ../data/musan/music/hd-classical/music-hd-0056.wav\n",
      "1108 ../data/musan/music/hd-classical/music-hd-0013.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109 ../data/musan/music/hd-classical/music-hd-0040.wav\n",
      "1110 ../data/musan/music/hd-classical/music-hd-0041.wav\n",
      "1111 ../data/musan/music/hd-classical/music-hd-0047.wav\n",
      "1112 ../data/musan/music/hd-classical/music-hd-0000.wav\n",
      "1113 ../data/musan/music/hd-classical/music-hd-0063.wav\n",
      "1114 ../data/musan/music/hd-classical/music-hd-0046.wav\n",
      "1115 ../data/musan/music/hd-classical/music-hd-0021.wav\n",
      "1116 ../data/musan/music/hd-classical/music-hd-0024.wav\n",
      "1117 ../data/musan/music/hd-classical/music-hd-0060.wav\n",
      "1118 ../data/musan/music/hd-classical/music-hd-0068.wav\n",
      "1119 ../data/musan/music/hd-classical/music-hd-0028.wav\n",
      "1120 ../data/musan/music/hd-classical/music-hd-0067.wav\n",
      "1121 ../data/musan/music/hd-classical/music-hd-0052.wav\n",
      "1122 ../data/musan/music/hd-classical/music-hd-0015.wav\n",
      "1123 ../data/musan/music/hd-classical/music-hd-0020.wav\n",
      "1124 ../data/musan/music/hd-classical/music-hd-0071.wav\n",
      "1125 ../data/musan/music/hd-classical/music-hd-0054.wav\n",
      "1126 ../data/musan/music/hd-classical/music-hd-0032.wav\n",
      "1127 ../data/musan/music/hd-classical/music-hd-0022.wav\n",
      "1128 ../data/musan/music/hd-classical/music-hd-0061.wav\n",
      "1129 ../data/musan/music/hd-classical/music-hd-0065.wav\n",
      "1130 ../data/musan/music/hd-classical/music-hd-0073.wav\n",
      "1131 ../data/musan/music/hd-classical/music-hd-0059.wav\n",
      "1132 ../data/musan/music/hd-classical/music-hd-0074.wav\n",
      "1133 ../data/musan/music/jamendo/music-jamendo-0209.wav\n",
      "1134 ../data/musan/music/jamendo/music-jamendo-0025.wav\n",
      "1135 ../data/musan/music/jamendo/music-jamendo-0073.wav\n",
      "1136 ../data/musan/music/jamendo/music-jamendo-0169.wav\n",
      "1137 ../data/musan/music/jamendo/music-jamendo-0036.wav\n",
      "1138 ../data/musan/music/jamendo/music-jamendo-0147.wav\n",
      "1139 ../data/musan/music/jamendo/music-jamendo-0057.wav\n",
      "1140 ../data/musan/music/jamendo/music-jamendo-0094.wav\n",
      "1141 ../data/musan/music/jamendo/music-jamendo-0069.wav\n",
      "1142 ../data/musan/music/jamendo/music-jamendo-0174.wav\n",
      "1143 ../data/musan/music/jamendo/music-jamendo-0034.wav\n",
      "1144 ../data/musan/music/jamendo/music-jamendo-0190.wav\n",
      "1145 ../data/musan/music/jamendo/music-jamendo-0091.wav\n",
      "1146 ../data/musan/music/jamendo/music-jamendo-0166.wav\n",
      "1147 ../data/musan/music/jamendo/music-jamendo-0083.wav\n",
      "1148 ../data/musan/music/jamendo/music-jamendo-0124.wav\n",
      "1149 ../data/musan/music/jamendo/music-jamendo-0028.wav\n",
      "1150 ../data/musan/music/jamendo/music-jamendo-0008.wav\n",
      "1151 ../data/musan/music/jamendo/music-jamendo-0084.wav\n",
      "1152 ../data/musan/music/jamendo/music-jamendo-0029.wav\n",
      "1153 ../data/musan/music/jamendo/music-jamendo-0159.wav\n",
      "1154 ../data/musan/music/jamendo/music-jamendo-0132.wav\n",
      "1155 ../data/musan/music/jamendo/music-jamendo-0187.wav\n",
      "1156 ../data/musan/music/jamendo/music-jamendo-0058.wav\n",
      "1157 ../data/musan/music/jamendo/music-jamendo-0093.wav\n",
      "1158 ../data/musan/music/jamendo/music-jamendo-0072.wav\n",
      "1159 ../data/musan/music/jamendo/music-jamendo-0052.wav\n",
      "1160 ../data/musan/music/jamendo/music-jamendo-0183.wav\n",
      "1161 ../data/musan/music/jamendo/music-jamendo-0154.wav\n",
      "1162 ../data/musan/music/jamendo/music-jamendo-0074.wav\n",
      "1163 ../data/musan/music/jamendo/music-jamendo-0126.wav\n",
      "1164 ../data/musan/music/jamendo/music-jamendo-0001.wav\n",
      "1165 ../data/musan/music/jamendo/music-jamendo-0077.wav\n",
      "1166 ../data/musan/music/jamendo/music-jamendo-0202.wav\n",
      "1167 ../data/musan/music/jamendo/music-jamendo-0129.wav\n",
      "1168 ../data/musan/music/jamendo/music-jamendo-0164.wav\n",
      "1169 ../data/musan/music/jamendo/music-jamendo-0206.wav\n",
      "1170 ../data/musan/music/jamendo/music-jamendo-0055.wav\n",
      "1171 ../data/musan/music/jamendo/music-jamendo-0213.wav\n",
      "1172 ../data/musan/music/jamendo/music-jamendo-0038.wav\n",
      "1173 ../data/musan/music/jamendo/music-jamendo-0086.wav\n",
      "1174 ../data/musan/music/jamendo/music-jamendo-0085.wav\n",
      "1175 ../data/musan/music/jamendo/music-jamendo-0096.wav\n",
      "1176 ../data/musan/music/jamendo/music-jamendo-0125.wav\n",
      "1177 ../data/musan/music/jamendo/music-jamendo-0143.wav\n",
      "1178 ../data/musan/music/jamendo/music-jamendo-0003.wav\n",
      "1179 ../data/musan/music/jamendo/music-jamendo-0104.wav\n",
      "1180 ../data/musan/music/jamendo/music-jamendo-0140.wav\n",
      "1181 ../data/musan/music/jamendo/music-jamendo-0131.wav\n",
      "1182 ../data/musan/music/jamendo/music-jamendo-0046.wav\n",
      "1183 ../data/musan/music/jamendo/music-jamendo-0107.wav\n",
      "1184 ../data/musan/music/jamendo/music-jamendo-0127.wav\n",
      "1185 ../data/musan/music/jamendo/music-jamendo-0053.wav\n",
      "1186 ../data/musan/music/jamendo/music-jamendo-0079.wav\n",
      "1187 ../data/musan/music/jamendo/music-jamendo-0103.wav\n",
      "1188 ../data/musan/music/jamendo/music-jamendo-0173.wav\n",
      "1189 ../data/musan/music/jamendo/music-jamendo-0106.wav\n",
      "1190 ../data/musan/music/jamendo/music-jamendo-0172.wav\n",
      "1191 ../data/musan/music/jamendo/music-jamendo-0142.wav\n",
      "1192 ../data/musan/music/jamendo/music-jamendo-0037.wav\n",
      "1193 ../data/musan/music/jamendo/music-jamendo-0080.wav\n",
      "1194 ../data/musan/music/jamendo/music-jamendo-0102.wav\n",
      "1195 ../data/musan/music/jamendo/music-jamendo-0087.wav\n",
      "1196 ../data/musan/music/jamendo/music-jamendo-0111.wav\n",
      "1197 ../data/musan/music/jamendo/music-jamendo-0175.wav\n",
      "1198 ../data/musan/music/jamendo/music-jamendo-0192.wav\n",
      "1199 ../data/musan/music/jamendo/music-jamendo-0076.wav\n",
      "1200 ../data/musan/music/jamendo/music-jamendo-0089.wav\n",
      "1201 ../data/musan/music/jamendo/music-jamendo-0054.wav\n",
      "1202 ../data/musan/music/jamendo/music-jamendo-0005.wav\n",
      "1203 ../data/musan/music/jamendo/music-jamendo-0121.wav\n",
      "1204 ../data/musan/music/jamendo/music-jamendo-0194.wav\n",
      "1205 ../data/musan/music/jamendo/music-jamendo-0162.wav\n",
      "1206 ../data/musan/music/jamendo/music-jamendo-0022.wav\n",
      "1207 ../data/musan/music/jamendo/music-jamendo-0150.wav\n",
      "1208 ../data/musan/music/jamendo/music-jamendo-0167.wav\n",
      "1209 ../data/musan/music/jamendo/music-jamendo-0155.wav\n",
      "1210 ../data/musan/music/jamendo/music-jamendo-0067.wav\n",
      "1211 ../data/musan/music/jamendo/music-jamendo-0033.wav\n",
      "1212 ../data/musan/music/jamendo/music-jamendo-0021.wav\n",
      "1213 ../data/musan/music/jamendo/music-jamendo-0197.wav\n",
      "1214 ../data/musan/music/jamendo/music-jamendo-0026.wav\n",
      "1215 ../data/musan/music/jamendo/music-jamendo-0019.wav\n",
      "1216 ../data/musan/music/jamendo/music-jamendo-0016.wav\n",
      "1217 ../data/musan/music/jamendo/music-jamendo-0007.wav\n",
      "1218 ../data/musan/music/jamendo/music-jamendo-0059.wav\n",
      "1219 ../data/musan/music/jamendo/music-jamendo-0149.wav\n",
      "1220 ../data/musan/music/jamendo/music-jamendo-0119.wav\n",
      "1221 ../data/musan/music/jamendo/music-jamendo-0047.wav\n",
      "1222 ../data/musan/music/jamendo/music-jamendo-0182.wav\n",
      "1223 ../data/musan/music/jamendo/music-jamendo-0207.wav\n",
      "1224 ../data/musan/music/jamendo/music-jamendo-0048.wav\n",
      "1225 ../data/musan/music/jamendo/music-jamendo-0201.wav\n",
      "1226 ../data/musan/music/jamendo/music-jamendo-0039.wav\n",
      "1227 ../data/musan/music/jamendo/music-jamendo-0181.wav\n",
      "1228 ../data/musan/music/jamendo/music-jamendo-0018.wav\n",
      "1229 ../data/musan/music/jamendo/music-jamendo-0193.wav\n",
      "1230 ../data/musan/music/jamendo/music-jamendo-0049.wav\n",
      "1231 ../data/musan/music/jamendo/music-jamendo-0035.wav\n",
      "1232 ../data/musan/music/jamendo/music-jamendo-0203.wav\n",
      "1233 ../data/musan/music/jamendo/music-jamendo-0128.wav\n",
      "1234 ../data/musan/music/jamendo/music-jamendo-0170.wav\n",
      "1235 ../data/musan/music/jamendo/music-jamendo-0168.wav\n",
      "1236 ../data/musan/music/jamendo/music-jamendo-0198.wav\n",
      "1237 ../data/musan/music/jamendo/music-jamendo-0030.wav\n",
      "1238 ../data/musan/music/jamendo/music-jamendo-0211.wav\n",
      "1239 ../data/musan/music/jamendo/music-jamendo-0185.wav\n",
      "1240 ../data/musan/music/jamendo/music-jamendo-0081.wav\n",
      "1241 ../data/musan/music/jamendo/music-jamendo-0088.wav\n",
      "1242 ../data/musan/music/jamendo/music-jamendo-0108.wav\n",
      "1243 ../data/musan/music/jamendo/music-jamendo-0012.wav\n",
      "1244 ../data/musan/music/jamendo/music-jamendo-0212.wav\n",
      "1245 ../data/musan/music/jamendo/music-jamendo-0144.wav\n",
      "1246 ../data/musan/music/jamendo/music-jamendo-0138.wav\n",
      "1247 ../data/musan/music/jamendo/music-jamendo-0210.wav\n",
      "1248 ../data/musan/music/jamendo/music-jamendo-0014.wav\n",
      "1249 ../data/musan/music/jamendo/music-jamendo-0171.wav\n",
      "1250 ../data/musan/music/jamendo/music-jamendo-0160.wav\n",
      "1251 ../data/musan/music/jamendo/music-jamendo-0116.wav\n",
      "1252 ../data/musan/music/jamendo/music-jamendo-0045.wav\n",
      "1253 ../data/musan/music/jamendo/music-jamendo-0098.wav\n",
      "1254 ../data/musan/music/jamendo/music-jamendo-0118.wav\n",
      "1255 ../data/musan/music/jamendo/music-jamendo-0044.wav\n",
      "1256 ../data/musan/music/jamendo/music-jamendo-0101.wav\n",
      "1257 ../data/musan/music/jamendo/music-jamendo-0139.wav\n",
      "1258 ../data/musan/music/jamendo/music-jamendo-0152.wav\n",
      "1259 ../data/musan/music/jamendo/music-jamendo-0097.wav\n",
      "1260 ../data/musan/music/jamendo/music-jamendo-0146.wav\n",
      "1261 ../data/musan/music/jamendo/music-jamendo-0065.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1262 ../data/musan/music/jamendo/music-jamendo-0178.wav\n",
      "1263 ../data/musan/music/jamendo/music-jamendo-0082.wav\n",
      "1264 ../data/musan/music/jamendo/music-jamendo-0156.wav\n",
      "1265 ../data/musan/music/jamendo/music-jamendo-0180.wav\n",
      "1266 ../data/musan/music/jamendo/music-jamendo-0117.wav\n",
      "1267 ../data/musan/music/jamendo/music-jamendo-0042.wav\n",
      "1268 ../data/musan/music/jamendo/music-jamendo-0120.wav\n",
      "1269 ../data/musan/music/jamendo/music-jamendo-0216.wav\n",
      "1270 ../data/musan/music/jamendo/music-jamendo-0063.wav\n",
      "1271 ../data/musan/music/jamendo/music-jamendo-0123.wav\n",
      "1272 ../data/musan/music/jamendo/music-jamendo-0068.wav\n",
      "1273 ../data/musan/music/jamendo/music-jamendo-0130.wav\n",
      "1274 ../data/musan/music/jamendo/music-jamendo-0196.wav\n",
      "1275 ../data/musan/music/jamendo/music-jamendo-0092.wav\n",
      "1276 ../data/musan/music/jamendo/music-jamendo-0090.wav\n",
      "1277 ../data/musan/music/jamendo/music-jamendo-0060.wav\n",
      "1278 ../data/musan/music/jamendo/music-jamendo-0000.wav\n",
      "1279 ../data/musan/music/jamendo/music-jamendo-0004.wav\n",
      "1280 ../data/musan/music/jamendo/music-jamendo-0050.wav\n",
      "1281 ../data/musan/music/jamendo/music-jamendo-0070.wav\n",
      "1282 ../data/musan/music/jamendo/music-jamendo-0056.wav\n",
      "1283 ../data/musan/music/jamendo/music-jamendo-0013.wav\n",
      "1284 ../data/musan/music/jamendo/music-jamendo-0010.wav\n",
      "1285 ../data/musan/music/jamendo/music-jamendo-0148.wav\n",
      "1286 ../data/musan/music/jamendo/music-jamendo-0188.wav\n",
      "1287 ../data/musan/music/jamendo/music-jamendo-0015.wav\n",
      "1288 ../data/musan/music/jamendo/music-jamendo-0062.wav\n",
      "1289 ../data/musan/music/jamendo/music-jamendo-0020.wav\n",
      "1290 ../data/musan/music/jamendo/music-jamendo-0184.wav\n",
      "1291 ../data/musan/music/jamendo/music-jamendo-0023.wav\n",
      "1292 ../data/musan/music/jamendo/music-jamendo-0002.wav\n",
      "1293 ../data/musan/music/jamendo/music-jamendo-0043.wav\n",
      "1294 ../data/musan/music/jamendo/music-jamendo-0061.wav\n",
      "1295 ../data/musan/music/jamendo/music-jamendo-0032.wav\n",
      "1296 ../data/musan/music/jamendo/music-jamendo-0112.wav\n",
      "1297 ../data/musan/music/jamendo/music-jamendo-0199.wav\n",
      "1298 ../data/musan/music/jamendo/music-jamendo-0137.wav\n",
      "1299 ../data/musan/music/jamendo/music-jamendo-0109.wav\n",
      "1300 ../data/musan/music/jamendo/music-jamendo-0163.wav\n",
      "1301 ../data/musan/music/jamendo/music-jamendo-0064.wav\n",
      "1302 ../data/musan/music/jamendo/music-jamendo-0114.wav\n",
      "1303 ../data/musan/music/jamendo/music-jamendo-0017.wav\n",
      "1304 ../data/musan/music/jamendo/music-jamendo-0177.wav\n",
      "1305 ../data/musan/music/jamendo/music-jamendo-0208.wav\n",
      "1306 ../data/musan/music/jamendo/music-jamendo-0099.wav\n",
      "1307 ../data/musan/music/jamendo/music-jamendo-0011.wav\n",
      "1308 ../data/musan/music/jamendo/music-jamendo-0157.wav\n",
      "1309 ../data/musan/music/jamendo/music-jamendo-0031.wav\n",
      "1310 ../data/musan/music/jamendo/music-jamendo-0161.wav\n",
      "1311 ../data/musan/music/jamendo/music-jamendo-0095.wav\n",
      "1312 ../data/musan/music/jamendo/music-jamendo-0191.wav\n",
      "1313 ../data/musan/music/jamendo/music-jamendo-0153.wav\n",
      "1314 ../data/musan/music/jamendo/music-jamendo-0066.wav\n",
      "1315 ../data/musan/music/jamendo/music-jamendo-0024.wav\n",
      "1316 ../data/musan/music/jamendo/music-jamendo-0133.wav\n",
      "1317 ../data/musan/music/jamendo/music-jamendo-0051.wav\n",
      "1318 ../data/musan/music/jamendo/music-jamendo-0179.wav\n",
      "1319 ../data/musan/music/jamendo/music-jamendo-0110.wav\n",
      "1320 ../data/musan/music/jamendo/music-jamendo-0204.wav\n",
      "1321 ../data/musan/music/jamendo/music-jamendo-0145.wav\n",
      "1322 ../data/musan/music/jamendo/music-jamendo-0009.wav\n",
      "1323 ../data/musan/music/jamendo/music-jamendo-0115.wav\n",
      "1324 ../data/musan/music/jamendo/music-jamendo-0186.wav\n",
      "1325 ../data/musan/music/jamendo/music-jamendo-0105.wav\n",
      "1326 ../data/musan/music/jamendo/music-jamendo-0165.wav\n",
      "1327 ../data/musan/music/jamendo/music-jamendo-0075.wav\n",
      "1328 ../data/musan/music/jamendo/music-jamendo-0215.wav\n",
      "1329 ../data/musan/music/jamendo/music-jamendo-0135.wav\n",
      "1330 ../data/musan/music/jamendo/music-jamendo-0151.wav\n",
      "1331 ../data/musan/music/jamendo/music-jamendo-0071.wav\n",
      "1332 ../data/musan/music/jamendo/music-jamendo-0195.wav\n",
      "1333 ../data/musan/music/jamendo/music-jamendo-0078.wav\n",
      "1334 ../data/musan/music/jamendo/music-jamendo-0189.wav\n",
      "1335 ../data/musan/music/jamendo/music-jamendo-0006.wav\n",
      "1336 ../data/musan/music/jamendo/music-jamendo-0141.wav\n",
      "1337 ../data/musan/music/jamendo/music-jamendo-0041.wav\n",
      "1338 ../data/musan/music/jamendo/music-jamendo-0176.wav\n",
      "1339 ../data/musan/music/jamendo/music-jamendo-0200.wav\n",
      "1340 ../data/musan/music/jamendo/music-jamendo-0122.wav\n",
      "1341 ../data/musan/music/jamendo/music-jamendo-0113.wav\n",
      "1342 ../data/musan/music/jamendo/music-jamendo-0136.wav\n",
      "1343 ../data/musan/music/jamendo/music-jamendo-0027.wav\n",
      "1344 ../data/musan/music/jamendo/music-jamendo-0134.wav\n",
      "1345 ../data/musan/music/jamendo/music-jamendo-0158.wav\n",
      "1346 ../data/musan/music/jamendo/music-jamendo-0205.wav\n",
      "1347 ../data/musan/music/jamendo/music-jamendo-0214.wav\n",
      "1348 ../data/musan/music/jamendo/music-jamendo-0040.wav\n",
      "1349 ../data/musan/music/jamendo/music-jamendo-0100.wav\n",
      "1350 ../data/musan/music/rfm/music-rfm-0093.wav\n",
      "1351 ../data/musan/music/rfm/music-rfm-0075.wav\n",
      "1352 ../data/musan/music/rfm/music-rfm-0143.wav\n",
      "1353 ../data/musan/music/rfm/music-rfm-0031.wav\n",
      "1354 ../data/musan/music/rfm/music-rfm-0001.wav\n",
      "1355 ../data/musan/music/rfm/music-rfm-0066.wav\n",
      "1356 ../data/musan/music/rfm/music-rfm-0146.wav\n",
      "1357 ../data/musan/music/rfm/music-rfm-0046.wav\n",
      "1358 ../data/musan/music/rfm/music-rfm-0077.wav\n",
      "1359 ../data/musan/music/rfm/music-rfm-0116.wav\n",
      "1360 ../data/musan/music/rfm/music-rfm-0056.wav\n",
      "1361 ../data/musan/music/rfm/music-rfm-0013.wav\n",
      "1362 ../data/musan/music/rfm/music-rfm-0102.wav\n",
      "1363 ../data/musan/music/rfm/music-rfm-0070.wav\n",
      "1364 ../data/musan/music/rfm/music-rfm-0117.wav\n",
      "1365 ../data/musan/music/rfm/music-rfm-0122.wav\n",
      "1366 ../data/musan/music/rfm/music-rfm-0068.wav\n",
      "1367 ../data/musan/music/rfm/music-rfm-0019.wav\n",
      "1368 ../data/musan/music/rfm/music-rfm-0009.wav\n",
      "1369 ../data/musan/music/rfm/music-rfm-0110.wav\n",
      "1370 ../data/musan/music/rfm/music-rfm-0003.wav\n",
      "1371 ../data/musan/music/rfm/music-rfm-0021.wav\n",
      "1372 ../data/musan/music/rfm/music-rfm-0040.wav\n",
      "1373 ../data/musan/music/rfm/music-rfm-0120.wav\n",
      "1374 ../data/musan/music/rfm/music-rfm-0132.wav\n",
      "1375 ../data/musan/music/rfm/music-rfm-0089.wav\n",
      "1376 ../data/musan/music/rfm/music-rfm-0078.wav\n",
      "1377 ../data/musan/music/rfm/music-rfm-0033.wav\n",
      "1378 ../data/musan/music/rfm/music-rfm-0126.wav\n",
      "1379 ../data/musan/music/rfm/music-rfm-0105.wav\n",
      "1380 ../data/musan/music/rfm/music-rfm-0062.wav\n",
      "1381 ../data/musan/music/rfm/music-rfm-0012.wav\n",
      "1382 ../data/musan/music/rfm/music-rfm-0030.wav\n",
      "1383 ../data/musan/music/rfm/music-rfm-0006.wav\n",
      "1384 ../data/musan/music/rfm/music-rfm-0025.wav\n",
      "1385 ../data/musan/music/rfm/music-rfm-0081.wav\n",
      "1386 ../data/musan/music/rfm/music-rfm-0026.wav\n",
      "1387 ../data/musan/music/rfm/music-rfm-0084.wav\n",
      "1388 ../data/musan/music/rfm/music-rfm-0008.wav\n",
      "1389 ../data/musan/music/rfm/music-rfm-0107.wav\n",
      "1390 ../data/musan/music/rfm/music-rfm-0032.wav\n",
      "1391 ../data/musan/music/rfm/music-rfm-0097.wav\n",
      "1392 ../data/musan/music/rfm/music-rfm-0065.wav\n",
      "1393 ../data/musan/music/rfm/music-rfm-0111.wav\n",
      "1394 ../data/musan/music/rfm/music-rfm-0129.wav\n",
      "1395 ../data/musan/music/rfm/music-rfm-0052.wav\n",
      "1396 ../data/musan/music/rfm/music-rfm-0113.wav\n",
      "1397 ../data/musan/music/rfm/music-rfm-0014.wav\n",
      "1398 ../data/musan/music/rfm/music-rfm-0054.wav\n",
      "1399 ../data/musan/music/rfm/music-rfm-0004.wav\n",
      "1400 ../data/musan/music/rfm/music-rfm-0115.wav\n",
      "1401 ../data/musan/music/rfm/music-rfm-0112.wav\n",
      "1402 ../data/musan/music/rfm/music-rfm-0067.wav\n",
      "1403 ../data/musan/music/rfm/music-rfm-0103.wav\n",
      "1404 ../data/musan/music/rfm/music-rfm-0109.wav\n",
      "1405 ../data/musan/music/rfm/music-rfm-0100.wav\n",
      "1406 ../data/musan/music/rfm/music-rfm-0072.wav\n",
      "1407 ../data/musan/music/rfm/music-rfm-0087.wav\n",
      "1408 ../data/musan/music/rfm/music-rfm-0053.wav\n",
      "1409 ../data/musan/music/rfm/music-rfm-0074.wav\n",
      "1410 ../data/musan/music/rfm/music-rfm-0133.wav\n",
      "1411 ../data/musan/music/rfm/music-rfm-0134.wav\n",
      "1412 ../data/musan/music/rfm/music-rfm-0123.wav\n",
      "1413 ../data/musan/music/rfm/music-rfm-0098.wav\n",
      "1414 ../data/musan/music/rfm/music-rfm-0086.wav\n",
      "1415 ../data/musan/music/rfm/music-rfm-0017.wav\n",
      "1416 ../data/musan/music/rfm/music-rfm-0082.wav\n",
      "1417 ../data/musan/music/rfm/music-rfm-0002.wav\n",
      "1418 ../data/musan/music/rfm/music-rfm-0043.wav\n",
      "1419 ../data/musan/music/rfm/music-rfm-0042.wav\n",
      "1420 ../data/musan/music/rfm/music-rfm-0007.wav\n",
      "1421 ../data/musan/music/rfm/music-rfm-0060.wav\n",
      "1422 ../data/musan/music/rfm/music-rfm-0091.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1423 ../data/musan/music/rfm/music-rfm-0080.wav\n",
      "1424 ../data/musan/music/rfm/music-rfm-0005.wav\n",
      "1425 ../data/musan/music/rfm/music-rfm-0045.wav\n",
      "1426 ../data/musan/music/rfm/music-rfm-0000.wav\n",
      "1427 ../data/musan/music/rfm/music-rfm-0114.wav\n",
      "1428 ../data/musan/music/rfm/music-rfm-0136.wav\n",
      "1429 ../data/musan/music/rfm/music-rfm-0118.wav\n",
      "1430 ../data/musan/music/rfm/music-rfm-0124.wav\n",
      "1431 ../data/musan/music/rfm/music-rfm-0018.wav\n",
      "1432 ../data/musan/music/rfm/music-rfm-0048.wav\n",
      "1433 ../data/musan/music/rfm/music-rfm-0108.wav\n",
      "1434 ../data/musan/music/rfm/music-rfm-0028.wav\n",
      "1435 ../data/musan/music/rfm/music-rfm-0055.wav\n",
      "1436 ../data/musan/music/rfm/music-rfm-0015.wav\n",
      "1437 ../data/musan/music/rfm/music-rfm-0010.wav\n",
      "1438 ../data/musan/music/rfm/music-rfm-0023.wav\n",
      "1439 ../data/musan/music/rfm/music-rfm-0144.wav\n",
      "1440 ../data/musan/music/rfm/music-rfm-0039.wav\n",
      "1441 ../data/musan/music/rfm/music-rfm-0063.wav\n",
      "1442 ../data/musan/music/rfm/music-rfm-0047.wav\n",
      "1443 ../data/musan/music/rfm/music-rfm-0034.wav\n",
      "1444 ../data/musan/music/rfm/music-rfm-0083.wav\n",
      "1445 ../data/musan/music/rfm/music-rfm-0095.wav\n",
      "1446 ../data/musan/music/rfm/music-rfm-0128.wav\n",
      "1447 ../data/musan/music/rfm/music-rfm-0011.wav\n",
      "1448 ../data/musan/music/rfm/music-rfm-0049.wav\n",
      "1449 ../data/musan/music/rfm/music-rfm-0138.wav\n",
      "1450 ../data/musan/music/rfm/music-rfm-0141.wav\n",
      "1451 ../data/musan/music/rfm/music-rfm-0037.wav\n",
      "1452 ../data/musan/music/rfm/music-rfm-0035.wav\n",
      "1453 ../data/musan/music/rfm/music-rfm-0130.wav\n",
      "1454 ../data/musan/music/rfm/music-rfm-0073.wav\n",
      "1455 ../data/musan/music/rfm/music-rfm-0140.wav\n",
      "1456 ../data/musan/music/rfm/music-rfm-0079.wav\n",
      "1457 ../data/musan/music/rfm/music-rfm-0131.wav\n",
      "1458 ../data/musan/music/rfm/music-rfm-0061.wav\n",
      "1459 ../data/musan/music/rfm/music-rfm-0104.wav\n",
      "1460 ../data/musan/music/rfm/music-rfm-0057.wav\n",
      "1461 ../data/musan/music/rfm/music-rfm-0071.wav\n",
      "1462 ../data/musan/music/rfm/music-rfm-0069.wav\n",
      "1463 ../data/musan/music/rfm/music-rfm-0106.wav\n",
      "1464 ../data/musan/music/rfm/music-rfm-0027.wav\n",
      "1465 ../data/musan/music/rfm/music-rfm-0064.wav\n",
      "1466 ../data/musan/music/rfm/music-rfm-0145.wav\n",
      "1467 ../data/musan/music/rfm/music-rfm-0022.wav\n",
      "1468 ../data/musan/music/rfm/music-rfm-0142.wav\n",
      "1469 ../data/musan/music/rfm/music-rfm-0051.wav\n",
      "1470 ../data/musan/music/rfm/music-rfm-0059.wav\n",
      "1471 ../data/musan/music/rfm/music-rfm-0029.wav\n",
      "1472 ../data/musan/music/rfm/music-rfm-0036.wav\n",
      "1473 ../data/musan/music/rfm/music-rfm-0119.wav\n",
      "1474 ../data/musan/music/rfm/music-rfm-0092.wav\n",
      "1475 ../data/musan/music/rfm/music-rfm-0139.wav\n",
      "1476 ../data/musan/music/rfm/music-rfm-0121.wav\n",
      "1477 ../data/musan/music/rfm/music-rfm-0099.wav\n",
      "1478 ../data/musan/music/rfm/music-rfm-0094.wav\n",
      "1479 ../data/musan/music/rfm/music-rfm-0050.wav\n",
      "1480 ../data/musan/music/rfm/music-rfm-0041.wav\n",
      "1481 ../data/musan/music/rfm/music-rfm-0090.wav\n",
      "1482 ../data/musan/music/rfm/music-rfm-0101.wav\n",
      "1483 ../data/musan/music/rfm/music-rfm-0038.wav\n",
      "1484 ../data/musan/music/rfm/music-rfm-0096.wav\n",
      "1485 ../data/musan/music/rfm/music-rfm-0088.wav\n",
      "1486 ../data/musan/music/rfm/music-rfm-0125.wav\n",
      "1487 ../data/musan/music/rfm/music-rfm-0024.wav\n",
      "1488 ../data/musan/music/rfm/music-rfm-0044.wav\n",
      "1489 ../data/musan/music/rfm/music-rfm-0016.wav\n",
      "1490 ../data/musan/music/rfm/music-rfm-0137.wav\n",
      "1491 ../data/musan/music/rfm/music-rfm-0058.wav\n",
      "1492 ../data/musan/music/rfm/music-rfm-0020.wav\n",
      "1493 ../data/musan/music/rfm/music-rfm-0085.wav\n",
      "1494 ../data/musan/music/rfm/music-rfm-0135.wav\n",
      "1495 ../data/musan/music/rfm/music-rfm-0127.wav\n",
      "1496 ../data/musan/music/rfm/music-rfm-0076.wav\n",
      "1497 ../data/musan/music/fma-western-art/music-fma-wa-0041.wav\n",
      "1498 ../data/musan/music/fma-western-art/music-fma-wa-0028.wav\n",
      "1499 ../data/musan/music/fma-western-art/music-fma-wa-0001.wav\n",
      "1500 ../data/musan/music/fma-western-art/music-fma-wa-0080.wav\n",
      "1501 ../data/musan/music/fma-western-art/music-fma-wa-0037.wav\n",
      "1502 ../data/musan/music/fma-western-art/music-fma-wa-0050.wav\n",
      "1503 ../data/musan/music/fma-western-art/music-fma-wa-0084.wav\n",
      "1504 ../data/musan/music/fma-western-art/music-fma-wa-0052.wav\n",
      "1505 ../data/musan/music/fma-western-art/music-fma-wa-0064.wav\n",
      "1506 ../data/musan/music/fma-western-art/music-fma-wa-0053.wav\n",
      "1507 ../data/musan/music/fma-western-art/music-fma-wa-0007.wav\n",
      "1508 ../data/musan/music/fma-western-art/music-fma-wa-0068.wav\n",
      "1509 ../data/musan/music/fma-western-art/music-fma-wa-0065.wav\n",
      "1510 ../data/musan/music/fma-western-art/music-fma-wa-0061.wav\n",
      "1511 ../data/musan/music/fma-western-art/music-fma-wa-0087.wav\n",
      "1512 ../data/musan/music/fma-western-art/music-fma-wa-0076.wav\n",
      "1513 ../data/musan/music/fma-western-art/music-fma-wa-0006.wav\n",
      "1514 ../data/musan/music/fma-western-art/music-fma-wa-0030.wav\n",
      "1515 ../data/musan/music/fma-western-art/music-fma-wa-0049.wav\n",
      "1516 ../data/musan/music/fma-western-art/music-fma-wa-0066.wav\n",
      "1517 ../data/musan/music/fma-western-art/music-fma-wa-0091.wav\n",
      "1518 ../data/musan/music/fma-western-art/music-fma-wa-0071.wav\n",
      "1519 ../data/musan/music/fma-western-art/music-fma-wa-0077.wav\n",
      "1520 ../data/musan/music/fma-western-art/music-fma-wa-0024.wav\n",
      "1521 ../data/musan/music/fma-western-art/music-fma-wa-0072.wav\n",
      "1522 ../data/musan/music/fma-western-art/music-fma-wa-0012.wav\n",
      "1523 ../data/musan/music/fma-western-art/music-fma-wa-0017.wav\n",
      "1524 ../data/musan/music/fma-western-art/music-fma-wa-0073.wav\n",
      "1525 ../data/musan/music/fma-western-art/music-fma-wa-0019.wav\n",
      "1526 ../data/musan/music/fma-western-art/music-fma-wa-0011.wav\n",
      "1527 ../data/musan/music/fma-western-art/music-fma-wa-0036.wav\n",
      "1528 ../data/musan/music/fma-western-art/music-fma-wa-0040.wav\n",
      "1529 ../data/musan/music/fma-western-art/music-fma-wa-0027.wav\n",
      "1530 ../data/musan/music/fma-western-art/music-fma-wa-0042.wav\n",
      "1531 ../data/musan/music/fma-western-art/music-fma-wa-0074.wav\n",
      "1532 ../data/musan/music/fma-western-art/music-fma-wa-0031.wav\n",
      "1533 ../data/musan/music/fma-western-art/music-fma-wa-0058.wav\n",
      "1534 ../data/musan/music/fma-western-art/music-fma-wa-0059.wav\n",
      "1535 ../data/musan/music/fma-western-art/music-fma-wa-0021.wav\n",
      "1536 ../data/musan/music/fma-western-art/music-fma-wa-0014.wav\n",
      "1537 ../data/musan/music/fma-western-art/music-fma-wa-0083.wav\n",
      "1538 ../data/musan/music/fma-western-art/music-fma-wa-0060.wav\n",
      "1539 ../data/musan/music/fma-western-art/music-fma-wa-0000.wav\n",
      "1540 ../data/musan/music/fma-western-art/music-fma-wa-0051.wav\n",
      "1541 ../data/musan/music/fma-western-art/music-fma-wa-0090.wav\n",
      "1542 ../data/musan/music/fma-western-art/music-fma-wa-0039.wav\n",
      "1543 ../data/musan/music/fma-western-art/music-fma-wa-0089.wav\n",
      "1544 ../data/musan/music/fma-western-art/music-fma-wa-0018.wav\n",
      "1545 ../data/musan/music/fma-western-art/music-fma-wa-0078.wav\n",
      "1546 ../data/musan/music/fma-western-art/music-fma-wa-0015.wav\n",
      "1547 ../data/musan/music/fma-western-art/music-fma-wa-0025.wav\n",
      "1548 ../data/musan/music/fma-western-art/music-fma-wa-0038.wav\n",
      "1549 ../data/musan/music/fma-western-art/music-fma-wa-0055.wav\n",
      "1550 ../data/musan/music/fma-western-art/music-fma-wa-0054.wav\n",
      "1551 ../data/musan/music/fma-western-art/music-fma-wa-0075.wav\n",
      "1552 ../data/musan/music/fma-western-art/music-fma-wa-0043.wav\n",
      "1553 ../data/musan/music/fma-western-art/music-fma-wa-0056.wav\n",
      "1554 ../data/musan/music/fma-western-art/music-fma-wa-0088.wav\n",
      "1555 ../data/musan/music/fma-western-art/music-fma-wa-0004.wav\n",
      "1556 ../data/musan/music/fma-western-art/music-fma-wa-0023.wav\n",
      "1557 ../data/musan/music/fma-western-art/music-fma-wa-0086.wav\n",
      "1558 ../data/musan/music/fma-western-art/music-fma-wa-0034.wav\n",
      "1559 ../data/musan/music/fma-western-art/music-fma-wa-0047.wav\n",
      "1560 ../data/musan/music/fma-western-art/music-fma-wa-0013.wav\n",
      "1561 ../data/musan/music/fma-western-art/music-fma-wa-0044.wav\n",
      "1562 ../data/musan/music/fma-western-art/music-fma-wa-0081.wav\n",
      "1563 ../data/musan/music/fma-western-art/music-fma-wa-0079.wav\n",
      "1564 ../data/musan/music/fma-western-art/music-fma-wa-0046.wav\n",
      "1565 ../data/musan/music/fma-western-art/music-fma-wa-0063.wav\n",
      "1566 ../data/musan/music/fma-western-art/music-fma-wa-0062.wav\n",
      "1567 ../data/musan/music/fma-western-art/music-fma-wa-0009.wav\n",
      "1568 ../data/musan/music/fma-western-art/music-fma-wa-0048.wav\n",
      "1569 ../data/musan/music/fma-western-art/music-fma-wa-0035.wav\n",
      "1570 ../data/musan/music/fma-western-art/music-fma-wa-0022.wav\n",
      "1571 ../data/musan/music/fma-western-art/music-fma-wa-0067.wav\n",
      "1572 ../data/musan/music/fma-western-art/music-fma-wa-0020.wav\n",
      "1573 ../data/musan/music/fma-western-art/music-fma-wa-0092.wav\n",
      "1574 ../data/musan/music/fma-western-art/music-fma-wa-0057.wav\n",
      "1575 ../data/musan/music/fma-western-art/music-fma-wa-0045.wav\n",
      "1576 ../data/musan/music/fma-western-art/music-fma-wa-0026.wav\n",
      "1577 ../data/musan/music/fma-western-art/music-fma-wa-0016.wav\n",
      "1578 ../data/musan/music/fma-western-art/music-fma-wa-0008.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1579 ../data/musan/music/fma-western-art/music-fma-wa-0029.wav\n",
      "1580 ../data/musan/music/fma-western-art/music-fma-wa-0032.wav\n",
      "1581 ../data/musan/music/fma-western-art/music-fma-wa-0005.wav\n",
      "1582 ../data/musan/music/fma-western-art/music-fma-wa-0002.wav\n",
      "1583 ../data/musan/music/fma-western-art/music-fma-wa-0033.wav\n",
      "1584 ../data/musan/music/fma-western-art/music-fma-wa-0082.wav\n",
      "1585 ../data/musan/music/fma-western-art/music-fma-wa-0003.wav\n",
      "1586 ../data/musan/music/fma-western-art/music-fma-wa-0069.wav\n",
      "1587 ../data/musan/music/fma-western-art/music-fma-wa-0070.wav\n",
      "1588 ../data/musan/music/fma-western-art/music-fma-wa-0085.wav\n",
      "1589 ../data/musan/music/fma-western-art/music-fma-wa-0010.wav\n",
      "1590 ../data/musan/speech/librivox/speech-librivox-0134.wav\n",
      "1591 ../data/musan/speech/librivox/speech-librivox-0005.wav\n",
      "1592 ../data/musan/speech/librivox/speech-librivox-0013.wav\n",
      "1593 ../data/musan/speech/librivox/speech-librivox-0171.wav\n",
      "1594 ../data/musan/speech/librivox/speech-librivox-0117.wav\n",
      "1595 ../data/musan/speech/librivox/speech-librivox-0142.wav\n",
      "1596 ../data/musan/speech/librivox/speech-librivox-0159.wav\n",
      "1597 ../data/musan/speech/librivox/speech-librivox-0083.wav\n",
      "1598 ../data/musan/speech/librivox/speech-librivox-0135.wav\n",
      "1599 ../data/musan/speech/librivox/speech-librivox-0070.wav\n",
      "1600 ../data/musan/speech/librivox/speech-librivox-0128.wav\n",
      "1601 ../data/musan/speech/librivox/speech-librivox-0049.wav\n",
      "1602 ../data/musan/speech/librivox/speech-librivox-0085.wav\n",
      "1603 ../data/musan/speech/librivox/speech-librivox-0114.wav\n",
      "1604 ../data/musan/speech/librivox/speech-librivox-0072.wav\n",
      "1605 ../data/musan/speech/librivox/speech-librivox-0098.wav\n",
      "1606 ../data/musan/speech/librivox/speech-librivox-0024.wav\n",
      "1607 ../data/musan/speech/librivox/speech-librivox-0064.wav\n",
      "1608 ../data/musan/speech/librivox/speech-librivox-0160.wav\n",
      "1609 ../data/musan/speech/librivox/speech-librivox-0108.wav\n",
      "1610 ../data/musan/speech/librivox/speech-librivox-0102.wav\n",
      "1611 ../data/musan/speech/librivox/speech-librivox-0119.wav\n",
      "1612 ../data/musan/speech/librivox/speech-librivox-0077.wav\n",
      "1613 ../data/musan/speech/librivox/speech-librivox-0004.wav\n",
      "1614 ../data/musan/speech/librivox/speech-librivox-0087.wav\n",
      "1615 ../data/musan/speech/librivox/speech-librivox-0112.wav\n",
      "1616 ../data/musan/speech/librivox/speech-librivox-0166.wav\n",
      "1617 ../data/musan/speech/librivox/speech-librivox-0162.wav\n",
      "1618 ../data/musan/speech/librivox/speech-librivox-0008.wav\n",
      "1619 ../data/musan/speech/librivox/speech-librivox-0155.wav\n",
      "1620 ../data/musan/speech/librivox/speech-librivox-0017.wav\n",
      "1621 ../data/musan/speech/librivox/speech-librivox-0029.wav\n",
      "1622 ../data/musan/speech/librivox/speech-librivox-0018.wav\n",
      "1623 ../data/musan/speech/librivox/speech-librivox-0014.wav\n",
      "1624 ../data/musan/speech/librivox/speech-librivox-0051.wav\n",
      "1625 ../data/musan/speech/librivox/speech-librivox-0059.wav\n",
      "1626 ../data/musan/speech/librivox/speech-librivox-0026.wav\n",
      "1627 ../data/musan/speech/librivox/speech-librivox-0138.wav\n",
      "1628 ../data/musan/speech/librivox/speech-librivox-0012.wav\n",
      "1629 ../data/musan/speech/librivox/speech-librivox-0161.wav\n",
      "1630 ../data/musan/speech/librivox/speech-librivox-0037.wav\n",
      "1631 ../data/musan/speech/librivox/speech-librivox-0079.wav\n",
      "1632 ../data/musan/speech/librivox/speech-librivox-0048.wav\n",
      "1633 ../data/musan/speech/librivox/speech-librivox-0094.wav\n",
      "1634 ../data/musan/speech/librivox/speech-librivox-0027.wav\n",
      "1635 ../data/musan/speech/librivox/speech-librivox-0052.wav\n",
      "1636 ../data/musan/speech/librivox/speech-librivox-0009.wav\n",
      "1637 ../data/musan/speech/librivox/speech-librivox-0042.wav\n",
      "1638 ../data/musan/speech/librivox/speech-librivox-0081.wav\n",
      "1639 ../data/musan/speech/librivox/speech-librivox-0078.wav\n",
      "1640 ../data/musan/speech/librivox/speech-librivox-0101.wav\n",
      "1641 ../data/musan/speech/librivox/speech-librivox-0163.wav\n",
      "1642 ../data/musan/speech/librivox/speech-librivox-0071.wav\n",
      "1643 ../data/musan/speech/librivox/speech-librivox-0140.wav\n",
      "1644 ../data/musan/speech/librivox/speech-librivox-0172.wav\n",
      "1645 ../data/musan/speech/librivox/speech-librivox-0066.wav\n",
      "1646 ../data/musan/speech/librivox/speech-librivox-0131.wav\n",
      "1647 ../data/musan/speech/librivox/speech-librivox-0146.wav\n",
      "1648 ../data/musan/speech/librivox/speech-librivox-0019.wav\n",
      "1649 ../data/musan/speech/librivox/speech-librivox-0086.wav\n",
      "1650 ../data/musan/speech/librivox/speech-librivox-0109.wav\n",
      "1651 ../data/musan/speech/librivox/speech-librivox-0157.wav\n",
      "1652 ../data/musan/speech/librivox/speech-librivox-0137.wav\n",
      "1653 ../data/musan/speech/librivox/speech-librivox-0073.wav\n",
      "1654 ../data/musan/speech/librivox/speech-librivox-0030.wav\n",
      "1655 ../data/musan/speech/librivox/speech-librivox-0105.wav\n",
      "1656 ../data/musan/speech/librivox/speech-librivox-0076.wav\n",
      "1657 ../data/musan/speech/librivox/speech-librivox-0133.wav\n",
      "1658 ../data/musan/speech/librivox/speech-librivox-0113.wav\n",
      "1659 ../data/musan/speech/librivox/speech-librivox-0038.wav\n",
      "1660 ../data/musan/speech/librivox/speech-librivox-0110.wav\n",
      "1661 ../data/musan/speech/librivox/speech-librivox-0150.wav\n",
      "1662 ../data/musan/speech/librivox/speech-librivox-0002.wav\n",
      "1663 ../data/musan/speech/librivox/speech-librivox-0154.wav\n",
      "1664 ../data/musan/speech/librivox/speech-librivox-0080.wav\n",
      "1665 ../data/musan/speech/librivox/speech-librivox-0122.wav\n",
      "1666 ../data/musan/speech/librivox/speech-librivox-0130.wav\n",
      "1667 ../data/musan/speech/librivox/speech-librivox-0123.wav\n",
      "1668 ../data/musan/speech/librivox/speech-librivox-0093.wav\n",
      "1669 ../data/musan/speech/librivox/speech-librivox-0100.wav\n",
      "1670 ../data/musan/speech/librivox/speech-librivox-0139.wav\n",
      "1671 ../data/musan/speech/librivox/speech-librivox-0121.wav\n",
      "1672 ../data/musan/speech/librivox/speech-librivox-0127.wav\n",
      "1673 ../data/musan/speech/librivox/speech-librivox-0015.wav\n",
      "1674 ../data/musan/speech/librivox/speech-librivox-0007.wav\n",
      "1675 ../data/musan/speech/librivox/speech-librivox-0156.wav\n",
      "1676 ../data/musan/speech/librivox/speech-librivox-0089.wav\n",
      "1677 ../data/musan/speech/librivox/speech-librivox-0125.wav\n",
      "1678 ../data/musan/speech/librivox/speech-librivox-0126.wav\n",
      "1679 ../data/musan/speech/librivox/speech-librivox-0169.wav\n",
      "1680 ../data/musan/speech/librivox/speech-librivox-0057.wav\n",
      "1681 ../data/musan/speech/librivox/speech-librivox-0053.wav\n",
      "1682 ../data/musan/speech/librivox/speech-librivox-0088.wav\n",
      "1683 ../data/musan/speech/librivox/speech-librivox-0141.wav\n",
      "1684 ../data/musan/speech/librivox/speech-librivox-0003.wav\n",
      "1685 ../data/musan/speech/librivox/speech-librivox-0116.wav\n",
      "1686 ../data/musan/speech/librivox/speech-librivox-0039.wav\n",
      "1687 ../data/musan/speech/librivox/speech-librivox-0103.wav\n",
      "1688 ../data/musan/speech/librivox/speech-librivox-0147.wav\n",
      "1689 ../data/musan/speech/librivox/speech-librivox-0106.wav\n",
      "1690 ../data/musan/speech/librivox/speech-librivox-0136.wav\n",
      "1691 ../data/musan/speech/librivox/speech-librivox-0044.wav\n",
      "1692 ../data/musan/speech/librivox/speech-librivox-0075.wav\n",
      "1693 ../data/musan/speech/librivox/speech-librivox-0120.wav\n",
      "1694 ../data/musan/speech/librivox/speech-librivox-0170.wav\n",
      "1695 ../data/musan/speech/librivox/speech-librivox-0144.wav\n",
      "1696 ../data/musan/speech/librivox/speech-librivox-0084.wav\n",
      "1697 ../data/musan/speech/librivox/speech-librivox-0063.wav\n",
      "1698 ../data/musan/speech/librivox/speech-librivox-0165.wav\n",
      "1699 ../data/musan/speech/librivox/speech-librivox-0069.wav\n",
      "1700 ../data/musan/speech/librivox/speech-librivox-0092.wav\n",
      "1701 ../data/musan/speech/librivox/speech-librivox-0061.wav\n",
      "1702 ../data/musan/speech/librivox/speech-librivox-0011.wav\n",
      "1703 ../data/musan/speech/librivox/speech-librivox-0050.wav\n",
      "1704 ../data/musan/speech/librivox/speech-librivox-0091.wav\n",
      "1705 ../data/musan/speech/librivox/speech-librivox-0047.wav\n",
      "1706 ../data/musan/speech/librivox/speech-librivox-0056.wav\n",
      "1707 ../data/musan/speech/librivox/speech-librivox-0118.wav\n",
      "1708 ../data/musan/speech/librivox/speech-librivox-0132.wav\n",
      "1709 ../data/musan/speech/librivox/speech-librivox-0145.wav\n",
      "1710 ../data/musan/speech/librivox/speech-librivox-0034.wav\n",
      "1711 ../data/musan/speech/librivox/speech-librivox-0041.wav\n",
      "1712 ../data/musan/speech/librivox/speech-librivox-0158.wav\n",
      "1713 ../data/musan/speech/librivox/speech-librivox-0115.wav\n",
      "1714 ../data/musan/speech/librivox/speech-librivox-0074.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1715 ../data/musan/speech/librivox/speech-librivox-0033.wav\n",
      "1716 ../data/musan/speech/librivox/speech-librivox-0062.wav\n",
      "1717 ../data/musan/speech/librivox/speech-librivox-0054.wav\n",
      "1718 ../data/musan/speech/librivox/speech-librivox-0010.wav\n",
      "1719 ../data/musan/speech/librivox/speech-librivox-0045.wav\n",
      "1720 ../data/musan/speech/librivox/speech-librivox-0124.wav\n",
      "1721 ../data/musan/speech/librivox/speech-librivox-0148.wav\n",
      "1722 ../data/musan/speech/librivox/speech-librivox-0000.wav\n",
      "1723 ../data/musan/speech/librivox/speech-librivox-0028.wav\n",
      "1724 ../data/musan/speech/librivox/speech-librivox-0001.wav\n",
      "1725 ../data/musan/speech/librivox/speech-librivox-0096.wav\n",
      "1726 ../data/musan/speech/librivox/speech-librivox-0023.wav\n",
      "1727 ../data/musan/speech/librivox/speech-librivox-0099.wav\n",
      "1728 ../data/musan/speech/librivox/speech-librivox-0043.wav\n",
      "1729 ../data/musan/speech/librivox/speech-librivox-0006.wav\n",
      "1730 ../data/musan/speech/librivox/speech-librivox-0167.wav\n",
      "1731 ../data/musan/speech/librivox/speech-librivox-0149.wav\n",
      "1732 ../data/musan/speech/librivox/speech-librivox-0032.wav\n",
      "1733 ../data/musan/speech/librivox/speech-librivox-0090.wav\n",
      "1734 ../data/musan/speech/librivox/speech-librivox-0016.wav\n",
      "1735 ../data/musan/speech/librivox/speech-librivox-0060.wav\n",
      "1736 ../data/musan/speech/librivox/speech-librivox-0151.wav\n",
      "1737 ../data/musan/speech/librivox/speech-librivox-0040.wav\n",
      "1738 ../data/musan/speech/librivox/speech-librivox-0129.wav\n",
      "1739 ../data/musan/speech/librivox/speech-librivox-0104.wav\n",
      "1740 ../data/musan/speech/librivox/speech-librivox-0020.wav\n",
      "1741 ../data/musan/speech/librivox/speech-librivox-0055.wav\n",
      "1742 ../data/musan/speech/librivox/speech-librivox-0058.wav\n",
      "1743 ../data/musan/speech/librivox/speech-librivox-0022.wav\n",
      "1744 ../data/musan/speech/librivox/speech-librivox-0025.wav\n",
      "1745 ../data/musan/speech/librivox/speech-librivox-0067.wav\n",
      "1746 ../data/musan/speech/librivox/speech-librivox-0082.wav\n",
      "1747 ../data/musan/speech/librivox/speech-librivox-0046.wav\n",
      "1748 ../data/musan/speech/librivox/speech-librivox-0036.wav\n",
      "1749 ../data/musan/speech/librivox/speech-librivox-0068.wav\n",
      "1750 ../data/musan/speech/librivox/speech-librivox-0164.wav\n",
      "1751 ../data/musan/speech/librivox/speech-librivox-0097.wav\n",
      "1752 ../data/musan/speech/librivox/speech-librivox-0095.wav\n",
      "1753 ../data/musan/speech/librivox/speech-librivox-0031.wav\n",
      "1754 ../data/musan/speech/librivox/speech-librivox-0065.wav\n",
      "1755 ../data/musan/speech/librivox/speech-librivox-0107.wav\n",
      "1756 ../data/musan/speech/librivox/speech-librivox-0021.wav\n",
      "1757 ../data/musan/speech/librivox/speech-librivox-0168.wav\n",
      "1758 ../data/musan/speech/librivox/speech-librivox-0152.wav\n",
      "1759 ../data/musan/speech/librivox/speech-librivox-0143.wav\n",
      "1760 ../data/musan/speech/librivox/speech-librivox-0111.wav\n",
      "1761 ../data/musan/speech/librivox/speech-librivox-0035.wav\n",
      "1762 ../data/musan/speech/librivox/speech-librivox-0153.wav\n",
      "1763 ../data/musan/speech/us-gov/speech-us-gov-0161.wav\n",
      "1764 ../data/musan/speech/us-gov/speech-us-gov-0167.wav\n",
      "1765 ../data/musan/speech/us-gov/speech-us-gov-0195.wav\n",
      "1766 ../data/musan/speech/us-gov/speech-us-gov-0192.wav\n",
      "1767 ../data/musan/speech/us-gov/speech-us-gov-0170.wav\n",
      "1768 ../data/musan/speech/us-gov/speech-us-gov-0160.wav\n",
      "1769 ../data/musan/speech/us-gov/speech-us-gov-0176.wav\n",
      "1770 ../data/musan/speech/us-gov/speech-us-gov-0097.wav\n",
      "1771 ../data/musan/speech/us-gov/speech-us-gov-0086.wav\n",
      "1772 ../data/musan/speech/us-gov/speech-us-gov-0032.wav\n",
      "1773 ../data/musan/speech/us-gov/speech-us-gov-0138.wav\n",
      "1774 ../data/musan/speech/us-gov/speech-us-gov-0215.wav\n",
      "1775 ../data/musan/speech/us-gov/speech-us-gov-0172.wav\n",
      "1776 ../data/musan/speech/us-gov/speech-us-gov-0054.wav\n",
      "1777 ../data/musan/speech/us-gov/speech-us-gov-0031.wav\n",
      "1778 ../data/musan/speech/us-gov/speech-us-gov-0101.wav\n",
      "1779 ../data/musan/speech/us-gov/speech-us-gov-0012.wav\n",
      "1780 ../data/musan/speech/us-gov/speech-us-gov-0165.wav\n",
      "1781 ../data/musan/speech/us-gov/speech-us-gov-0040.wav\n",
      "1782 ../data/musan/speech/us-gov/speech-us-gov-0045.wav\n",
      "1783 ../data/musan/speech/us-gov/speech-us-gov-0144.wav\n",
      "1784 ../data/musan/speech/us-gov/speech-us-gov-0199.wav\n",
      "1785 ../data/musan/speech/us-gov/speech-us-gov-0108.wav\n",
      "1786 ../data/musan/speech/us-gov/speech-us-gov-0153.wav\n",
      "1787 ../data/musan/speech/us-gov/speech-us-gov-0099.wav\n",
      "1788 ../data/musan/speech/us-gov/speech-us-gov-0042.wav\n",
      "1789 ../data/musan/speech/us-gov/speech-us-gov-0213.wav\n",
      "1790 ../data/musan/speech/us-gov/speech-us-gov-0078.wav\n",
      "1791 ../data/musan/speech/us-gov/speech-us-gov-0071.wav\n",
      "1792 ../data/musan/speech/us-gov/speech-us-gov-0037.wav\n",
      "1793 ../data/musan/speech/us-gov/speech-us-gov-0018.wav\n",
      "1794 ../data/musan/speech/us-gov/speech-us-gov-0052.wav\n",
      "1795 ../data/musan/speech/us-gov/speech-us-gov-0151.wav\n",
      "1796 ../data/musan/speech/us-gov/speech-us-gov-0150.wav\n",
      "1797 ../data/musan/speech/us-gov/speech-us-gov-0224.wav\n",
      "1798 ../data/musan/speech/us-gov/speech-us-gov-0141.wav\n",
      "1799 ../data/musan/speech/us-gov/speech-us-gov-0243.wav\n",
      "1800 ../data/musan/speech/us-gov/speech-us-gov-0118.wav\n",
      "1801 ../data/musan/speech/us-gov/speech-us-gov-0013.wav\n",
      "1802 ../data/musan/speech/us-gov/speech-us-gov-0206.wav\n",
      "1803 ../data/musan/speech/us-gov/speech-us-gov-0102.wav\n",
      "1804 ../data/musan/speech/us-gov/speech-us-gov-0060.wav\n",
      "1805 ../data/musan/speech/us-gov/speech-us-gov-0073.wav\n",
      "1806 ../data/musan/speech/us-gov/speech-us-gov-0194.wav\n",
      "1807 ../data/musan/speech/us-gov/speech-us-gov-0158.wav\n",
      "1808 ../data/musan/speech/us-gov/speech-us-gov-0117.wav\n",
      "1809 ../data/musan/speech/us-gov/speech-us-gov-0175.wav\n",
      "1810 ../data/musan/speech/us-gov/speech-us-gov-0182.wav\n",
      "1811 ../data/musan/speech/us-gov/speech-us-gov-0133.wav\n",
      "1812 ../data/musan/speech/us-gov/speech-us-gov-0025.wav\n",
      "1813 ../data/musan/speech/us-gov/speech-us-gov-0148.wav\n",
      "1814 ../data/musan/speech/us-gov/speech-us-gov-0021.wav\n",
      "1815 ../data/musan/speech/us-gov/speech-us-gov-0001.wav\n",
      "1816 ../data/musan/speech/us-gov/speech-us-gov-0004.wav\n",
      "1817 ../data/musan/speech/us-gov/speech-us-gov-0236.wav\n",
      "1818 ../data/musan/speech/us-gov/speech-us-gov-0128.wav\n",
      "1819 ../data/musan/speech/us-gov/speech-us-gov-0082.wav\n",
      "1820 ../data/musan/speech/us-gov/speech-us-gov-0179.wav\n",
      "1821 ../data/musan/speech/us-gov/speech-us-gov-0022.wav\n",
      "1822 ../data/musan/speech/us-gov/speech-us-gov-0240.wav\n",
      "1823 ../data/musan/speech/us-gov/speech-us-gov-0164.wav\n",
      "1824 ../data/musan/speech/us-gov/speech-us-gov-0005.wav\n",
      "1825 ../data/musan/speech/us-gov/speech-us-gov-0063.wav\n",
      "1826 ../data/musan/speech/us-gov/speech-us-gov-0163.wav\n",
      "1827 ../data/musan/speech/us-gov/speech-us-gov-0235.wav\n",
      "1828 ../data/musan/speech/us-gov/speech-us-gov-0188.wav\n",
      "1829 ../data/musan/speech/us-gov/speech-us-gov-0053.wav\n",
      "1830 ../data/musan/speech/us-gov/speech-us-gov-0034.wav\n",
      "1831 ../data/musan/speech/us-gov/speech-us-gov-0065.wav\n",
      "1832 ../data/musan/speech/us-gov/speech-us-gov-0017.wav\n",
      "1833 ../data/musan/speech/us-gov/speech-us-gov-0156.wav\n",
      "1834 ../data/musan/speech/us-gov/speech-us-gov-0014.wav\n",
      "1835 ../data/musan/speech/us-gov/speech-us-gov-0119.wav\n",
      "1836 ../data/musan/speech/us-gov/speech-us-gov-0223.wav\n",
      "1837 ../data/musan/speech/us-gov/speech-us-gov-0103.wav\n",
      "1838 ../data/musan/speech/us-gov/speech-us-gov-0076.wav\n",
      "1839 ../data/musan/speech/us-gov/speech-us-gov-0044.wav\n",
      "1840 ../data/musan/speech/us-gov/speech-us-gov-0109.wav\n",
      "1841 ../data/musan/speech/us-gov/speech-us-gov-0159.wav\n",
      "1842 ../data/musan/speech/us-gov/speech-us-gov-0135.wav\n",
      "1843 ../data/musan/speech/us-gov/speech-us-gov-0047.wav\n",
      "1844 ../data/musan/speech/us-gov/speech-us-gov-0216.wav\n",
      "1845 ../data/musan/speech/us-gov/speech-us-gov-0088.wav\n",
      "1846 ../data/musan/speech/us-gov/speech-us-gov-0218.wav\n",
      "1847 ../data/musan/speech/us-gov/speech-us-gov-0234.wav\n",
      "1848 ../data/musan/speech/us-gov/speech-us-gov-0171.wav\n",
      "1849 ../data/musan/speech/us-gov/speech-us-gov-0068.wav\n",
      "1850 ../data/musan/speech/us-gov/speech-us-gov-0083.wav\n",
      "1851 ../data/musan/speech/us-gov/speech-us-gov-0209.wav\n",
      "1852 ../data/musan/speech/us-gov/speech-us-gov-0127.wav\n",
      "1853 ../data/musan/speech/us-gov/speech-us-gov-0002.wav\n",
      "1854 ../data/musan/speech/us-gov/speech-us-gov-0225.wav\n",
      "1855 ../data/musan/speech/us-gov/speech-us-gov-0026.wav\n",
      "1856 ../data/musan/speech/us-gov/speech-us-gov-0190.wav\n",
      "1857 ../data/musan/speech/us-gov/speech-us-gov-0185.wav\n",
      "1858 ../data/musan/speech/us-gov/speech-us-gov-0212.wav\n",
      "1859 ../data/musan/speech/us-gov/speech-us-gov-0036.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1860 ../data/musan/speech/us-gov/speech-us-gov-0205.wav\n",
      "1861 ../data/musan/speech/us-gov/speech-us-gov-0187.wav\n",
      "1862 ../data/musan/speech/us-gov/speech-us-gov-0249.wav\n",
      "1863 ../data/musan/speech/us-gov/speech-us-gov-0193.wav\n",
      "1864 ../data/musan/speech/us-gov/speech-us-gov-0129.wav\n",
      "1865 ../data/musan/speech/us-gov/speech-us-gov-0055.wav\n",
      "1866 ../data/musan/speech/us-gov/speech-us-gov-0112.wav\n",
      "1867 ../data/musan/speech/us-gov/speech-us-gov-0247.wav\n",
      "1868 ../data/musan/speech/us-gov/speech-us-gov-0242.wav\n",
      "1869 ../data/musan/speech/us-gov/speech-us-gov-0094.wav\n",
      "1870 ../data/musan/speech/us-gov/speech-us-gov-0090.wav\n",
      "1871 ../data/musan/speech/us-gov/speech-us-gov-0211.wav\n",
      "1872 ../data/musan/speech/us-gov/speech-us-gov-0217.wav\n",
      "1873 ../data/musan/speech/us-gov/speech-us-gov-0121.wav\n",
      "1874 ../data/musan/speech/us-gov/speech-us-gov-0125.wav\n",
      "1875 ../data/musan/speech/us-gov/speech-us-gov-0140.wav\n",
      "1876 ../data/musan/speech/us-gov/speech-us-gov-0123.wav\n",
      "1877 ../data/musan/speech/us-gov/speech-us-gov-0100.wav\n",
      "1878 ../data/musan/speech/us-gov/speech-us-gov-0183.wav\n",
      "1879 ../data/musan/speech/us-gov/speech-us-gov-0237.wav\n",
      "1880 ../data/musan/speech/us-gov/speech-us-gov-0080.wav\n",
      "1881 ../data/musan/speech/us-gov/speech-us-gov-0028.wav\n",
      "1882 ../data/musan/speech/us-gov/speech-us-gov-0142.wav\n",
      "1883 ../data/musan/speech/us-gov/speech-us-gov-0248.wav\n",
      "1884 ../data/musan/speech/us-gov/speech-us-gov-0245.wav\n",
      "1885 ../data/musan/speech/us-gov/speech-us-gov-0157.wav\n",
      "1886 ../data/musan/speech/us-gov/speech-us-gov-0139.wav\n",
      "1887 ../data/musan/speech/us-gov/speech-us-gov-0204.wav\n",
      "1888 ../data/musan/speech/us-gov/speech-us-gov-0074.wav\n",
      "1889 ../data/musan/speech/us-gov/speech-us-gov-0184.wav\n",
      "1890 ../data/musan/speech/us-gov/speech-us-gov-0200.wav\n",
      "1891 ../data/musan/speech/us-gov/speech-us-gov-0104.wav\n",
      "1892 ../data/musan/speech/us-gov/speech-us-gov-0093.wav\n",
      "1893 ../data/musan/speech/us-gov/speech-us-gov-0061.wav\n",
      "1894 ../data/musan/speech/us-gov/speech-us-gov-0196.wav\n",
      "1895 ../data/musan/speech/us-gov/speech-us-gov-0228.wav\n",
      "1896 ../data/musan/speech/us-gov/speech-us-gov-0091.wav\n",
      "1897 ../data/musan/speech/us-gov/speech-us-gov-0131.wav\n",
      "1898 ../data/musan/speech/us-gov/speech-us-gov-0177.wav\n",
      "1899 ../data/musan/speech/us-gov/speech-us-gov-0252.wav\n",
      "1900 ../data/musan/speech/us-gov/speech-us-gov-0134.wav\n",
      "1901 ../data/musan/speech/us-gov/speech-us-gov-0011.wav\n",
      "1902 ../data/musan/speech/us-gov/speech-us-gov-0241.wav\n",
      "1903 ../data/musan/speech/us-gov/speech-us-gov-0226.wav\n",
      "1904 ../data/musan/speech/us-gov/speech-us-gov-0197.wav\n",
      "1905 ../data/musan/speech/us-gov/speech-us-gov-0155.wav\n",
      "1906 ../data/musan/speech/us-gov/speech-us-gov-0124.wav\n",
      "1907 ../data/musan/speech/us-gov/speech-us-gov-0107.wav\n",
      "1908 ../data/musan/speech/us-gov/speech-us-gov-0069.wav\n",
      "1909 ../data/musan/speech/us-gov/speech-us-gov-0096.wav\n",
      "1910 ../data/musan/speech/us-gov/speech-us-gov-0120.wav\n",
      "1911 ../data/musan/speech/us-gov/speech-us-gov-0039.wav\n",
      "1912 ../data/musan/speech/us-gov/speech-us-gov-0062.wav\n",
      "1913 ../data/musan/speech/us-gov/speech-us-gov-0085.wav\n",
      "1914 ../data/musan/speech/us-gov/speech-us-gov-0064.wav\n",
      "1915 ../data/musan/speech/us-gov/speech-us-gov-0089.wav\n",
      "1916 ../data/musan/speech/us-gov/speech-us-gov-0219.wav\n",
      "1917 ../data/musan/speech/us-gov/speech-us-gov-0244.wav\n",
      "1918 ../data/musan/speech/us-gov/speech-us-gov-0056.wav\n",
      "1919 ../data/musan/speech/us-gov/speech-us-gov-0050.wav\n",
      "1920 ../data/musan/speech/us-gov/speech-us-gov-0066.wav\n",
      "1921 ../data/musan/speech/us-gov/speech-us-gov-0111.wav\n",
      "1922 ../data/musan/speech/us-gov/speech-us-gov-0246.wav\n",
      "1923 ../data/musan/speech/us-gov/speech-us-gov-0006.wav\n",
      "1924 ../data/musan/speech/us-gov/speech-us-gov-0250.wav\n",
      "1925 ../data/musan/speech/us-gov/speech-us-gov-0191.wav\n",
      "1926 ../data/musan/speech/us-gov/speech-us-gov-0239.wav\n",
      "1927 ../data/musan/speech/us-gov/speech-us-gov-0110.wav\n",
      "1928 ../data/musan/speech/us-gov/speech-us-gov-0057.wav\n",
      "1929 ../data/musan/speech/us-gov/speech-us-gov-0115.wav\n",
      "1930 ../data/musan/speech/us-gov/speech-us-gov-0147.wav\n",
      "1931 ../data/musan/speech/us-gov/speech-us-gov-0059.wav\n",
      "1932 ../data/musan/speech/us-gov/speech-us-gov-0207.wav\n",
      "1933 ../data/musan/speech/us-gov/speech-us-gov-0130.wav\n",
      "1934 ../data/musan/speech/us-gov/speech-us-gov-0220.wav\n",
      "1935 ../data/musan/speech/us-gov/speech-us-gov-0152.wav\n",
      "1936 ../data/musan/speech/us-gov/speech-us-gov-0030.wav\n",
      "1937 ../data/musan/speech/us-gov/speech-us-gov-0051.wav\n",
      "1938 ../data/musan/speech/us-gov/speech-us-gov-0162.wav\n",
      "1939 ../data/musan/speech/us-gov/speech-us-gov-0016.wav\n",
      "1940 ../data/musan/speech/us-gov/speech-us-gov-0046.wav\n",
      "1941 ../data/musan/speech/us-gov/speech-us-gov-0024.wav\n",
      "1942 ../data/musan/speech/us-gov/speech-us-gov-0043.wav\n",
      "1943 ../data/musan/speech/us-gov/speech-us-gov-0092.wav\n",
      "1944 ../data/musan/speech/us-gov/speech-us-gov-0232.wav\n",
      "1945 ../data/musan/speech/us-gov/speech-us-gov-0003.wav\n",
      "1946 ../data/musan/speech/us-gov/speech-us-gov-0143.wav\n",
      "1947 ../data/musan/speech/us-gov/speech-us-gov-0029.wav\n",
      "1948 ../data/musan/speech/us-gov/speech-us-gov-0008.wav\n",
      "1949 ../data/musan/speech/us-gov/speech-us-gov-0106.wav\n",
      "1950 ../data/musan/speech/us-gov/speech-us-gov-0000.wav\n",
      "1951 ../data/musan/speech/us-gov/speech-us-gov-0113.wav\n",
      "1952 ../data/musan/speech/us-gov/speech-us-gov-0048.wav\n",
      "1953 ../data/musan/speech/us-gov/speech-us-gov-0181.wav\n",
      "1954 ../data/musan/speech/us-gov/speech-us-gov-0041.wav\n",
      "1955 ../data/musan/speech/us-gov/speech-us-gov-0238.wav\n",
      "1956 ../data/musan/speech/us-gov/speech-us-gov-0027.wav\n",
      "1957 ../data/musan/speech/us-gov/speech-us-gov-0221.wav\n",
      "1958 ../data/musan/speech/us-gov/speech-us-gov-0168.wav\n",
      "1959 ../data/musan/speech/us-gov/speech-us-gov-0230.wav\n",
      "1960 ../data/musan/speech/us-gov/speech-us-gov-0169.wav\n",
      "1961 ../data/musan/speech/us-gov/speech-us-gov-0145.wav\n",
      "1962 ../data/musan/speech/us-gov/speech-us-gov-0149.wav\n",
      "1963 ../data/musan/speech/us-gov/speech-us-gov-0009.wav\n",
      "1964 ../data/musan/speech/us-gov/speech-us-gov-0189.wav\n",
      "1965 ../data/musan/speech/us-gov/speech-us-gov-0079.wav\n",
      "1966 ../data/musan/speech/us-gov/speech-us-gov-0049.wav\n",
      "1967 ../data/musan/speech/us-gov/speech-us-gov-0019.wav\n",
      "1968 ../data/musan/speech/us-gov/speech-us-gov-0208.wav\n",
      "1969 ../data/musan/speech/us-gov/speech-us-gov-0105.wav\n",
      "1970 ../data/musan/speech/us-gov/speech-us-gov-0015.wav\n",
      "1971 ../data/musan/speech/us-gov/speech-us-gov-0136.wav\n",
      "1972 ../data/musan/speech/us-gov/speech-us-gov-0178.wav\n",
      "1973 ../data/musan/speech/us-gov/speech-us-gov-0132.wav\n",
      "1974 ../data/musan/speech/us-gov/speech-us-gov-0077.wav\n",
      "1975 ../data/musan/speech/us-gov/speech-us-gov-0020.wav\n",
      "1976 ../data/musan/speech/us-gov/speech-us-gov-0033.wav\n",
      "1977 ../data/musan/speech/us-gov/speech-us-gov-0210.wav\n",
      "1978 ../data/musan/speech/us-gov/speech-us-gov-0166.wav\n",
      "1979 ../data/musan/speech/us-gov/speech-us-gov-0098.wav\n",
      "1980 ../data/musan/speech/us-gov/speech-us-gov-0251.wav\n",
      "1981 ../data/musan/speech/us-gov/speech-us-gov-0146.wav\n",
      "1982 ../data/musan/speech/us-gov/speech-us-gov-0126.wav\n",
      "1983 ../data/musan/speech/us-gov/speech-us-gov-0010.wav\n",
      "1984 ../data/musan/speech/us-gov/speech-us-gov-0084.wav\n",
      "1985 ../data/musan/speech/us-gov/speech-us-gov-0174.wav\n",
      "1986 ../data/musan/speech/us-gov/speech-us-gov-0058.wav\n",
      "1987 ../data/musan/speech/us-gov/speech-us-gov-0203.wav\n",
      "1988 ../data/musan/speech/us-gov/speech-us-gov-0007.wav\n",
      "1989 ../data/musan/speech/us-gov/speech-us-gov-0116.wav\n",
      "1990 ../data/musan/speech/us-gov/speech-us-gov-0198.wav\n",
      "1991 ../data/musan/speech/us-gov/speech-us-gov-0231.wav\n",
      "1992 ../data/musan/speech/us-gov/speech-us-gov-0233.wav\n",
      "1993 ../data/musan/speech/us-gov/speech-us-gov-0180.wav\n",
      "1994 ../data/musan/speech/us-gov/speech-us-gov-0202.wav\n",
      "1995 ../data/musan/speech/us-gov/speech-us-gov-0087.wav\n",
      "1996 ../data/musan/speech/us-gov/speech-us-gov-0122.wav\n",
      "1997 ../data/musan/speech/us-gov/speech-us-gov-0070.wav\n",
      "1998 ../data/musan/speech/us-gov/speech-us-gov-0081.wav\n",
      "1999 ../data/musan/speech/us-gov/speech-us-gov-0227.wav\n",
      "2000 ../data/musan/speech/us-gov/speech-us-gov-0214.wav\n",
      "2001 ../data/musan/speech/us-gov/speech-us-gov-0154.wav\n",
      "2002 ../data/musan/speech/us-gov/speech-us-gov-0067.wav\n",
      "2003 ../data/musan/speech/us-gov/speech-us-gov-0035.wav\n",
      "2004 ../data/musan/speech/us-gov/speech-us-gov-0095.wav\n",
      "2005 ../data/musan/speech/us-gov/speech-us-gov-0075.wav\n",
      "2006 ../data/musan/speech/us-gov/speech-us-gov-0201.wav\n",
      "2007 ../data/musan/speech/us-gov/speech-us-gov-0186.wav\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008 ../data/musan/speech/us-gov/speech-us-gov-0173.wav\n",
      "2009 ../data/musan/speech/us-gov/speech-us-gov-0038.wav\n",
      "2010 ../data/musan/speech/us-gov/speech-us-gov-0137.wav\n",
      "2011 ../data/musan/speech/us-gov/speech-us-gov-0114.wav\n",
      "2012 ../data/musan/speech/us-gov/speech-us-gov-0229.wav\n",
      "2013 ../data/musan/speech/us-gov/speech-us-gov-0072.wav\n",
      "2014 ../data/musan/speech/us-gov/speech-us-gov-0023.wav\n",
      "2015 ../data/musan/speech/us-gov/speech-us-gov-0222.wav\n"
     ]
    }
   ],
   "source": [
    "# Split MUSAN (SLR17) dataset for faster random access\n",
    "split_musan(save_path='../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d6aee",
   "metadata": {},
   "source": [
    "**2. Обучение параметров блока построения дикторских моделей без учёта процедуры аугментации данных**\n",
    "\n",
    "Построение современных дикторских моделей, как правило, выполняется с использованием нейросетевых архитектур. В последние годы наилучшие результаты демонстрируют [трансформерные модели](https://arxiv.org/pdf/1706.03762), существенно превосходящие по качеству более ранние свёрточные нейросетевые архитектуры. В рамках настоящего пункта предлагается выполнить адаптацию нейросетевой архитектуры [Whisper tiny](https://arxiv.org/pdf/2212.04356) под решение задачи генерации дикторских моделей (дикторских эмбеддингов). *Дикторский эмбеддинг* – это высокоуровневый вектор-признаков, состоящий, например, из 128, 256 и т.п. значений, содержащий особенности голоса конкретного человека. При решении задачи распознавания диктора можно выделить эталонные и тестовые дикторские эмбеддинги. *Эталонные эмбеддинги* формируются на этапе регистрации дикторской модели определённого человека и находятся в некотором хранилище данных. *Тестовые эмбеддинги* формируются на этапе непосредственного использования системы голосовой биометрии на практике, когда некоторый пользователь пытается получить доступ к соответствующим ресурсам. Система голосовой биометрии сравнивает по определённой метрике эталонные и тестовые эмбеддинги, формируя оценку сравнения, которая, после её обработки блоком принятия решения, позволяет сделать вывод о том, эмбеддинги одинаковых или разных дикторов сравниваются между собой.\n",
    "\n",
    "Необходимо отметить, что построение дикторских моделей, как правило, требует наличия *акустических признаков*, вычисленных для звукозаписей тренировочной, валидационной и тестовой баз данных. В качестве примера подобных признаков в рамках настоящей лабораторной работы воспользуемся *логарифмами энергий на выходе мел-банка фильтров*. Важно отметить, что оригинальная модель Whisper tiny обучалась на 80-мерных признаках, причём входной сигнал перед обработкой путём дополнения с краю приводится к фиксированной длительности равной 30 секундам. В рамках настоящей лабораторной работы дополнение сигнала до 30 секунд не выполняется. Дополнительно, перед вычислением акустических признаков, осуществляется применение [нормализации экземпляра](https://docs.pytorch.org/docs/stable/generated/torch.nn.InstanceNorm1d.html) (instance normalization), которая выравнивает амплитуды всех сигналов в тренировочной выборке, стабилизируя процедуру сходимости нейросетевой модели, выполняющей построение дикторских моделей, на этапе обучения. Подобная нормализация отсутствует в процедуре обучения оригинальных моделей семейства Whisper.\n",
    "\n",
    "После того, как акустические признаки подготовлены, они могут быть переданы на блок построения дикторских моделей. Как правило, устройство современных дикторских моделей соответствует структуре [x-векторных архитектур](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf). Эти архитектуры состоят из четырёх ключевых элементов: \n",
    "\n",
    "1. **Фреймовый уровень.** Предназначен для формирования локальных представлений голоса конкретного человека. На этом уровне как раз и могут быть использованы нейросетевые архитектуры на базе трансормерных моделей. При организации фреймового уровня в рамках настоящей лабораторной работы предлагается использовать предобученный кодировщик трансформерной нейросетевой архитектуры Whisper tiny, а также два [нейросетевых блока с временной задержкой](https://www.cs.toronto.edu/~hinton/absps/waibelTDNN.pdf) (TDNN, time-delay neural betwork), которые выполняют согласование кодировщика Whisper tiny с оставшейся частью нейронной сети. Использование мощных, предварительно обученных на большом объёме размеченных или неразмеченных данных трансформерных моделей, таких как [wav2vec 2.0](https://arxiv.org/pdf/2006.11477), [HuBERT](https://arxiv.org/pdf/2106.07447), [Whisper](https://arxiv.org/pdf/2212.04356) и т.п., значительно упрощает решение множества разнообразных *целевых задач* (downstream tasks), связанных с обработкой аудиосигналов, ускоряя процедуру сходимости на этапе обучения и улучшая качество работы моделей в процессе тестирования относительно моделей, обученных из случайной начальной инициализации.\n",
    "\n",
    "2. **Уровень статистического пулинга** позволяет сформировать промежуточный вектор-признаков, фиксированной длины, которая является одинаковой для звукозаписи любой длительности. В ходе работы блока статистического пулинга происходит удаление временной размерности, присутствующей в картах-признаков. Это достигается путём выполнения процедуры усреднения карт-признаков вдоль оси времени. Выходом уровня статистического пулинга являются вектор среднего и вектор среднеквадратического отклонения, вычисленные на основе карт-признаков. Эти вектора конкатенируются и передаются для дальнейшей обработки на сегментом уровне.\n",
    "\n",
    "3. **Сегментный уровень.** Предназначен для трансформации промежуточного вектора, как правило, высокой размерности, в компактный вектор-признаков, представляющий собой дикторский эмбеддинг. Необходимо отметить, что на сегментном уровне расположены один или несколько полносвязных нейросетевых слоёв, а обработка данных выполняется по отношению ко всей звукозаписи, а не только к некоторому её локальному контексту, как на фреймовом уровне.\n",
    "\n",
    "4. **Уровень выходного слоя.** Представляет полносвязный слой с softmax-функциями активации. Количество активаций равно числу дикторов в тренирочной выборке. На вход выходноя слоя подаётся дикторский эмбеддинг, а на выходе – формируется набор апостериорных вероятностей, определяющих принадлежность эмбеддинга к одному из дикторских классов в тренировочной выборке. Необходимо отметить, что, как правило, в современных нейросетевых системах построения дикторских моделей выходной слой используется только на этапе обучения параметров и на этапе тестирования не применяется. На этапе тестирования используются только три первых уровня архитектуры.\n",
    "\n",
    "Обучение модели генерации дикторских эмбеддингов выполняется путём решения задачи *классификации* или, выражаясь терминами из области биометрии, *идентификации на закрытом множестве* (количество дикторских меток является строго фиксированным). В качестве используемой стоимостной функции выступает *категориальная кросс-энтропия*. Обучение выполняется с помощью мини-батчей, содержащих короткие фрагменты карт акустических признаков (длительностью несколько секунд) различных дикторов из тренировочной базы данных. Обучение на коротких фрагментах позволяет избежать сильного переобучения нейросетевой модели. При выполнении процедуры обучения требуется подобрать набор гиперпараметров, выбрать обучения и метод численной оптимизации. Обучение нейросетевых моделей при наличие претрейнов добавляет определённую специфику в процедуру обучения: использование более низкого значения параметра сходимости в случае, если вся нейросетевая модель *«разморожена»* (обучаются все веса модели на этапе тренировки), *«заморозка»* предобученной части модели (обучается часть весов модели на этапе тренировки) для ускорения процедуры сходимости и быстрой проверки гипотез и т.п.\n",
    "\n",
    "Для успешного выполнения настоящего пункта необходимо сделать следующее:\n",
    "\n",
    "1. Сгенерировать списки тренировочных и валидационных данных на основе идентификационного протокола базы VoxCeleb1, содержащегося в файле **../data/voxceleb1_test/iden_split.txt**. При генерации списков требуется исключить из них звукозаписи дикторов, которые входят в базу [VoxCeleb1 test set](https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip). Это позволит выполнить тестирования обученных блоков генерации дикторских моделей на протоколе [VoxCeleb1-O cleaned](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test2.txt), который составлен по отношению к данным из VoxCeleb1 test set и позволяет проверить качество работы биометрической системы в рамках открытой задачи верификации диктора.\n",
    "\n",
    "2. Инициализировать обучаемый экстрактор дикторских эмбеддингов и вгрузить в его фреймовый уровень веса предобученного кодировщика модели Whisper tiny. Необходимо отметить, что Whisper tiny не является единственной моделью семейства [Whisper](https://github.com/openai/whisper/tree/main). Так же в общедоступных источниках могут быть найдены веса предобученных моделей Whisper base/small/medium/large. Однако, вычислительная стоимость этих моделей в сравнении с Whisper tiny является выше при более высокой точности. Для достижения более высокого качества экстрактора дикторских эмбеддингов предлагается разморозить слои кодировщика Whisper tiny на этапе обучения. \n",
    "\n",
    "3. Инициализировать загрузчики тренировочной и валидационной выборок, используемые в рамках решения задачи идентификации на закрытом множестве, а также тестовой выборки, для оценки ошибки системы распознавания в рамках решения задачи верификации.\n",
    "\n",
    "4. Инициализировать оптимизатор и планировщик для выполнения процедуры обучения.\n",
    "\n",
    "5. Описать процедуру валидации блока построения дикторских моделей.\n",
    "\n",
    "6. Описать процедуру обучения и запустить её, контролируя значения стоимостной функции и доли правильных ответов на тренировочном и валидационном множествах, а также величину равновероятной ошибки при решении задачи верификации на тестовом множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3380f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hyperparameters\n",
    "\n",
    "nOut              = 512                                  # embedding size\n",
    "\n",
    "# Loss function for angular losses\n",
    "margin            = 0.35                                 # margin parameter\n",
    "scale             = 32.0                                 # scale parameter\n",
    "\n",
    "# Train dataloader (close-set identification)\n",
    "max_frames_train  = 400                                  # number of frame to train\n",
    "train_path        = '../data/voxceleb1_dev/wav'          # path to train wav files\n",
    "batch_size_train  = 128                                  # batch size to train\n",
    "pin_memory        = False                                # pin memory\n",
    "num_workers_train = 5                                    # number of workers to train\n",
    "shuffle           = True                                 # shuffling of training examples\n",
    "\n",
    "# Validation dataloader (close-set identification)\n",
    "max_frames_val    = 400                                  # number of frame to validate\n",
    "val_path          = '../data/voxceleb1_dev/wav'          # path to val wav files\n",
    "batch_size_val    = 128                                  # batch size to validate\n",
    "num_workers_val   = 5                                    # number of workers to validate\n",
    "\n",
    "# Test dataloader (open-set verification)\n",
    "max_frames_test   = 400                                  # number of frame to test\n",
    "test_path         = '../data/voxceleb1_test/wav/'        # path to val wav files\n",
    "batch_size_test   = 1                                    # batch size to test\n",
    "num_workers_test  = 5                                    # number of workers to test\n",
    "num_eval          = 5                                    # number of enroll/test chunks for scoring\n",
    "\n",
    "# Optimizer\n",
    "lr                = 0.0001                               # learning rate value # 0.0001\n",
    "weight_decay      = 0                                    # weight decay value\n",
    "\n",
    "# Scheduler\n",
    "val_interval      = 5                                    # frequency of validation step\n",
    "max_epoch         = 40                                   # number of epoches\n",
    "\n",
    "# Augmentation\n",
    "musan_path        = '../data/musan_split'                # path to splitted SLR17 dataset\n",
    "rir_path          = '../data/RIRS_NOISES/simulated_rirs' # path to SLR28 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "773bf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/validation data lists for close-set speaker identification\n",
    "train_list = []\n",
    "val_list   = []\n",
    "\n",
    "with open('../data/voxceleb1_test/iden_split.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "black_list = os.listdir('../data/voxceleb1_test/wav')   # exclude speaker IDs from VoxCeleb1 test set\n",
    "num_train_spk = []                                      # number of train speakers\n",
    "\n",
    "for line in lines:\n",
    "    line   = line.strip().split(' ')\n",
    "    spk_id = line[1].split('/')[0]\n",
    "    \n",
    "    if not (spk_id in black_list):\n",
    "        num_train_spk.append(spk_id)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Train list\n",
    "    if (line[0] == '1'):\n",
    "        train_list.append(' '.join([spk_id, line[1]]))\n",
    "    \n",
    "    # Validation list\n",
    "    elif (line[0] == '2'):\n",
    "        val_list.append(' '.join([spk_id, line[1]]))\n",
    "        \n",
    "num_train_spk = len(set(num_train_spk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f848ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataloader (with augmentation)\n",
    "train_dataset = train_dataset_loader(train_list=train_list, \n",
    "                                     max_frames=max_frames_train, \n",
    "                                     train_path=train_path, \n",
    "                                     augment=True, \n",
    "                                     musan_path=musan_path, \n",
    "                                     rir_path=rir_path)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, \n",
    "                           batch_size=batch_size_train, \n",
    "                           pin_memory=pin_memory, \n",
    "                           num_workers=num_workers_train, \n",
    "                           shuffle=shuffle)\n",
    "\n",
    "# Initialize validation dataloader\n",
    "val_dataset = test_dataset_loader(test_list=val_list, max_frames=max_frames_val, test_path=val_path)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size_val, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4752816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test protocol\n",
    "with open('../data/voxceleb1_test/veri_test2.txt', 'r') as f:\n",
    "    verif_protocol_lines = f.readlines()\n",
    "    \n",
    "# Get a list of unique file names\n",
    "files = list(itertools.chain(*[x.strip().split()[-2:] for x in verif_protocol_lines]))\n",
    "setfiles = list(set(files))\n",
    "setfiles.sort()\n",
    "\n",
    "# Initialize test dataloader\n",
    "test_dataset = test_dataset_loader_verification(setfiles, test_path=test_path, eval_frames=max_frames_test, num_eval=num_eval)\n",
    "test_loader  = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size_test, \n",
    "                          shuffle=False, \n",
    "                          num_workers=num_workers_test, \n",
    "                          drop_last=False, \n",
    "                          sampler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e497662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 72.1M/72.1M [00:02<00:00, 28.3MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised AAM softmax margin 0.350 scale 32.000.\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "preprocessor        = ExtractWhisperFbanks80(pad_data=False)\n",
    "feat_extractor      = WhisperEncoder.build_model(model_name=\"tiny\", load_weight=True, n_layer=-1) # Whisper tiny download to \n",
    "                                                                                                  # ~/.cache/whisper/tiny.pt\n",
    "frame_level_block   = TDNNSimple(n_layers=1, in_ch=384, out_ch=1024)\n",
    "pooling_block       = StatPoolLayer.by_string_mode(mode=\"MV\")\n",
    "segment_level_block = MaxoutSegmentLevel(input_dim=2048, output_dim=512, enable_batch_norm=True)\n",
    "\n",
    "whisper_sr_model    = SSLDownstream(preproc=preprocessor,\n",
    "                                    feat_extractor=feat_extractor,\n",
    "                                    frame_level_block=frame_level_block,\n",
    "                                    pooling_block=pooling_block,\n",
    "                                    segment_level_block=segment_level_block,\n",
    "                                    freeze_feats=False)\n",
    "trainfunc           = AAMSoftmaxLoss(nOut=nOut, nClasses=num_train_spk, margin=margin, scale=scale)\n",
    "main_model          = MainModel(whisper_sr_model, trainfunc).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5358d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised SGD optimizer.\n",
      "Initialised OneCycle LR scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = SGDOptimizer(main_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = OneCycleLRScheduler(optimizer, \n",
    "                                pct_start=0.05, \n",
    "                                cycle_momentum=False, \n",
    "                                max_lr=lr, \n",
    "                                div_factor=10, \n",
    "                                final_div_factor=10000, \n",
    "                                total_steps=max_epoch*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef333a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 1, LR 0.000010 Loss 19.094431, Accuracy 0.000%\n",
      "Epoch 0, Batch 2, LR 0.000010 Loss 19.085585, Accuracy 0.000%\n",
      "Epoch 0, Batch 3, LR 0.000010 Loss 19.000217, Accuracy 0.000%\n",
      "Epoch 0, Batch 4, LR 0.000010 Loss 18.988174, Accuracy 0.000%\n",
      "Epoch 0, Batch 5, LR 0.000010 Loss 18.989373, Accuracy 0.000%\n",
      "Epoch 0, Batch 6, LR 0.000010 Loss 19.016659, Accuracy 0.000%\n",
      "Epoch 0, Batch 7, LR 0.000010 Loss 18.974042, Accuracy 0.112%\n",
      "Epoch 0, Batch 8, LR 0.000010 Loss 18.988524, Accuracy 0.098%\n",
      "Epoch 0, Batch 9, LR 0.000010 Loss 18.991696, Accuracy 0.087%\n",
      "Epoch 0, Batch 10, LR 0.000010 Loss 18.981946, Accuracy 0.078%\n",
      "Epoch 0, Batch 11, LR 0.000010 Loss 18.988835, Accuracy 0.071%\n",
      "Epoch 0, Batch 12, LR 0.000010 Loss 18.987102, Accuracy 0.065%\n",
      "Epoch 0, Batch 13, LR 0.000010 Loss 18.988402, Accuracy 0.060%\n",
      "Epoch 0, Batch 14, LR 0.000010 Loss 18.991361, Accuracy 0.112%\n",
      "Epoch 0, Batch 15, LR 0.000010 Loss 18.993636, Accuracy 0.104%\n",
      "Epoch 0, Batch 16, LR 0.000010 Loss 19.006969, Accuracy 0.098%\n",
      "Epoch 0, Batch 17, LR 0.000010 Loss 19.011009, Accuracy 0.092%\n",
      "Epoch 0, Batch 18, LR 0.000010 Loss 19.010488, Accuracy 0.087%\n",
      "Epoch 0, Batch 19, LR 0.000010 Loss 18.991806, Accuracy 0.082%\n",
      "Epoch 0, Batch 20, LR 0.000010 Loss 18.992912, Accuracy 0.078%\n",
      "Epoch 0, Batch 21, LR 0.000010 Loss 18.986271, Accuracy 0.074%\n",
      "Epoch 0, Batch 22, LR 0.000010 Loss 18.992038, Accuracy 0.071%\n",
      "Epoch 0, Batch 23, LR 0.000010 Loss 18.996743, Accuracy 0.068%\n",
      "Epoch 0, Batch 24, LR 0.000010 Loss 18.989298, Accuracy 0.065%\n",
      "Epoch 0, Batch 25, LR 0.000010 Loss 18.998204, Accuracy 0.062%\n",
      "Epoch 0, Batch 26, LR 0.000010 Loss 18.991068, Accuracy 0.090%\n",
      "Epoch 0, Batch 27, LR 0.000010 Loss 18.987222, Accuracy 0.087%\n",
      "Epoch 0, Batch 28, LR 0.000010 Loss 18.985136, Accuracy 0.084%\n",
      "Epoch 0, Batch 29, LR 0.000010 Loss 18.987899, Accuracy 0.081%\n",
      "Epoch 0, Batch 30, LR 0.000010 Loss 18.991457, Accuracy 0.078%\n",
      "Epoch 0, Batch 31, LR 0.000010 Loss 18.997608, Accuracy 0.076%\n",
      "Epoch 0, Batch 32, LR 0.000010 Loss 18.995238, Accuracy 0.098%\n",
      "Epoch 0, Batch 33, LR 0.000010 Loss 18.993528, Accuracy 0.095%\n",
      "Epoch 0, Batch 34, LR 0.000010 Loss 18.994814, Accuracy 0.092%\n",
      "Epoch 0, Batch 35, LR 0.000010 Loss 18.994993, Accuracy 0.089%\n",
      "Epoch 0, Batch 36, LR 0.000010 Loss 18.994523, Accuracy 0.087%\n",
      "Epoch 0, Batch 37, LR 0.000010 Loss 18.994484, Accuracy 0.084%\n",
      "Epoch 0, Batch 38, LR 0.000010 Loss 18.994321, Accuracy 0.082%\n",
      "Epoch 0, Batch 39, LR 0.000010 Loss 18.995965, Accuracy 0.080%\n",
      "Epoch 0, Batch 40, LR 0.000010 Loss 18.997444, Accuracy 0.078%\n",
      "Epoch 0, Batch 41, LR 0.000010 Loss 18.993756, Accuracy 0.076%\n",
      "Epoch 0, Batch 42, LR 0.000010 Loss 18.997167, Accuracy 0.074%\n",
      "Epoch 0, Batch 43, LR 0.000010 Loss 18.997505, Accuracy 0.073%\n",
      "Epoch 0, Batch 44, LR 0.000010 Loss 18.997601, Accuracy 0.089%\n",
      "Epoch 0, Batch 45, LR 0.000010 Loss 18.998370, Accuracy 0.087%\n",
      "Epoch 0, Batch 46, LR 0.000010 Loss 19.000859, Accuracy 0.085%\n",
      "Epoch 0, Batch 47, LR 0.000010 Loss 18.998247, Accuracy 0.083%\n",
      "Epoch 0, Batch 48, LR 0.000010 Loss 18.997614, Accuracy 0.081%\n",
      "Epoch 0, Batch 49, LR 0.000010 Loss 18.998184, Accuracy 0.080%\n",
      "Epoch 0, Batch 50, LR 0.000010 Loss 18.998913, Accuracy 0.078%\n",
      "Epoch 0, Batch 51, LR 0.000010 Loss 18.998065, Accuracy 0.077%\n",
      "Epoch 0, Batch 52, LR 0.000010 Loss 19.000702, Accuracy 0.075%\n",
      "Epoch 0, Batch 53, LR 0.000010 Loss 19.000558, Accuracy 0.103%\n",
      "Epoch 0, Batch 54, LR 0.000010 Loss 18.998352, Accuracy 0.101%\n",
      "Epoch 0, Batch 55, LR 0.000010 Loss 18.999295, Accuracy 0.099%\n",
      "Epoch 0, Batch 56, LR 0.000010 Loss 18.999668, Accuracy 0.098%\n",
      "Epoch 0, Batch 57, LR 0.000010 Loss 19.000567, Accuracy 0.096%\n",
      "Epoch 0, Batch 58, LR 0.000010 Loss 19.000171, Accuracy 0.094%\n",
      "Epoch 0, Batch 59, LR 0.000010 Loss 19.000268, Accuracy 0.093%\n",
      "Epoch 0, Batch 60, LR 0.000010 Loss 18.998898, Accuracy 0.091%\n",
      "Epoch 0, Batch 61, LR 0.000010 Loss 19.000690, Accuracy 0.090%\n",
      "Epoch 0, Batch 62, LR 0.000010 Loss 19.002305, Accuracy 0.088%\n",
      "Epoch 0, Batch 63, LR 0.000010 Loss 19.004398, Accuracy 0.087%\n",
      "Epoch 0, Batch 64, LR 0.000010 Loss 19.009027, Accuracy 0.098%\n",
      "Epoch 0, Batch 65, LR 0.000010 Loss 19.008510, Accuracy 0.096%\n",
      "Epoch 0, Batch 66, LR 0.000010 Loss 19.008385, Accuracy 0.095%\n",
      "Epoch 0, Batch 67, LR 0.000010 Loss 19.010827, Accuracy 0.105%\n",
      "Epoch 0, Batch 68, LR 0.000010 Loss 19.011301, Accuracy 0.103%\n",
      "Epoch 0, Batch 69, LR 0.000010 Loss 19.008770, Accuracy 0.102%\n",
      "Epoch 0, Batch 70, LR 0.000010 Loss 19.008472, Accuracy 0.100%\n",
      "Epoch 0, Batch 71, LR 0.000010 Loss 19.006686, Accuracy 0.099%\n",
      "Epoch 0, Batch 72, LR 0.000010 Loss 19.009116, Accuracy 0.098%\n",
      "Epoch 0, Batch 73, LR 0.000010 Loss 19.008477, Accuracy 0.096%\n",
      "Epoch 0, Batch 74, LR 0.000010 Loss 19.006003, Accuracy 0.106%\n",
      "Epoch 0, Batch 75, LR 0.000010 Loss 19.004651, Accuracy 0.104%\n",
      "Epoch 0, Batch 76, LR 0.000010 Loss 19.005139, Accuracy 0.103%\n",
      "Epoch 0, Batch 77, LR 0.000010 Loss 19.007166, Accuracy 0.101%\n",
      "Epoch 0, Batch 78, LR 0.000010 Loss 19.005558, Accuracy 0.110%\n",
      "Epoch 0, Batch 79, LR 0.000010 Loss 19.004693, Accuracy 0.109%\n",
      "Epoch 0, Batch 80, LR 0.000010 Loss 19.006086, Accuracy 0.107%\n",
      "Epoch 0, Batch 81, LR 0.000010 Loss 19.008709, Accuracy 0.106%\n",
      "Epoch 0, Batch 82, LR 0.000010 Loss 19.007239, Accuracy 0.114%\n",
      "Epoch 0, Batch 83, LR 0.000010 Loss 19.007411, Accuracy 0.113%\n",
      "Epoch 0, Batch 84, LR 0.000010 Loss 19.006827, Accuracy 0.112%\n",
      "Epoch 0, Batch 85, LR 0.000010 Loss 19.007548, Accuracy 0.110%\n",
      "Epoch 0, Batch 86, LR 0.000010 Loss 19.006416, Accuracy 0.109%\n",
      "Epoch 0, Batch 87, LR 0.000010 Loss 19.007840, Accuracy 0.108%\n",
      "Epoch 0, Batch 88, LR 0.000010 Loss 19.008126, Accuracy 0.107%\n",
      "Epoch 0, Batch 89, LR 0.000010 Loss 19.008904, Accuracy 0.105%\n",
      "Epoch 0, Batch 90, LR 0.000010 Loss 19.008318, Accuracy 0.104%\n",
      "Epoch 0, Batch 91, LR 0.000010 Loss 19.007217, Accuracy 0.103%\n",
      "Epoch 0, Batch 92, LR 0.000010 Loss 19.006556, Accuracy 0.102%\n",
      "Epoch 0, Batch 93, LR 0.000010 Loss 19.006705, Accuracy 0.101%\n",
      "Epoch 0, Batch 94, LR 0.000010 Loss 19.005523, Accuracy 0.100%\n",
      "Epoch 0, Batch 95, LR 0.000010 Loss 19.007476, Accuracy 0.107%\n",
      "Epoch 0, Batch 96, LR 0.000010 Loss 19.007210, Accuracy 0.106%\n",
      "Epoch 0, Batch 97, LR 0.000010 Loss 19.006584, Accuracy 0.105%\n",
      "Epoch 0, Batch 98, LR 0.000010 Loss 19.006133, Accuracy 0.104%\n",
      "Epoch 0, Batch 99, LR 0.000010 Loss 19.006051, Accuracy 0.103%\n",
      "Epoch 0, Batch 100, LR 0.000010 Loss 19.007779, Accuracy 0.102%\n",
      "Epoch 0, Batch 101, LR 0.000011 Loss 19.007703, Accuracy 0.101%\n",
      "Epoch 0, Batch 102, LR 0.000011 Loss 19.007379, Accuracy 0.100%\n",
      "Epoch 0, Batch 103, LR 0.000011 Loss 19.005600, Accuracy 0.099%\n",
      "Epoch 0, Batch 104, LR 0.000011 Loss 19.003165, Accuracy 0.098%\n",
      "Epoch 0, Batch 105, LR 0.000011 Loss 19.001786, Accuracy 0.097%\n",
      "Epoch 0, Batch 106, LR 0.000011 Loss 18.999937, Accuracy 0.096%\n",
      "Epoch 0, Batch 107, LR 0.000011 Loss 18.998906, Accuracy 0.095%\n",
      "Epoch 0, Batch 108, LR 0.000011 Loss 18.998583, Accuracy 0.094%\n",
      "Epoch 0, Batch 109, LR 0.000011 Loss 18.997287, Accuracy 0.108%\n",
      "Epoch 0, Batch 110, LR 0.000011 Loss 18.996817, Accuracy 0.107%\n",
      "Epoch 0, Batch 111, LR 0.000011 Loss 18.995846, Accuracy 0.106%\n",
      "Epoch 0, Batch 112, LR 0.000011 Loss 18.996334, Accuracy 0.105%\n",
      "Epoch 0, Batch 113, LR 0.000011 Loss 18.998038, Accuracy 0.104%\n",
      "Epoch 0, Batch 114, LR 0.000011 Loss 18.996199, Accuracy 0.103%\n",
      "Epoch 0, Batch 115, LR 0.000011 Loss 18.994865, Accuracy 0.102%\n",
      "Epoch 0, Batch 116, LR 0.000011 Loss 18.994448, Accuracy 0.101%\n",
      "Epoch 0, Batch 117, LR 0.000011 Loss 18.993691, Accuracy 0.100%\n",
      "Epoch 0, Batch 118, LR 0.000011 Loss 18.994224, Accuracy 0.099%\n",
      "Epoch 0, Batch 119, LR 0.000011 Loss 18.992614, Accuracy 0.098%\n",
      "Epoch 0, Batch 120, LR 0.000011 Loss 18.992180, Accuracy 0.098%\n",
      "Epoch 0, Batch 121, LR 0.000011 Loss 18.991965, Accuracy 0.097%\n",
      "Epoch 0, Batch 122, LR 0.000011 Loss 18.992030, Accuracy 0.102%\n",
      "Epoch 0, Batch 123, LR 0.000011 Loss 18.992029, Accuracy 0.102%\n",
      "Epoch 0, Batch 124, LR 0.000011 Loss 18.992640, Accuracy 0.101%\n",
      "Epoch 0, Batch 125, LR 0.000011 Loss 18.991390, Accuracy 0.100%\n",
      "Epoch 0, Batch 126, LR 0.000011 Loss 18.991037, Accuracy 0.099%\n",
      "Epoch 0, Batch 127, LR 0.000011 Loss 18.989979, Accuracy 0.098%\n",
      "Epoch 0, Batch 128, LR 0.000011 Loss 18.989584, Accuracy 0.104%\n",
      "Epoch 0, Batch 129, LR 0.000011 Loss 18.989085, Accuracy 0.103%\n",
      "Epoch 0, Batch 130, LR 0.000011 Loss 18.991190, Accuracy 0.102%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 131, LR 0.000011 Loss 18.991395, Accuracy 0.101%\n",
      "Epoch 0, Batch 132, LR 0.000011 Loss 18.992251, Accuracy 0.101%\n",
      "Epoch 0, Batch 133, LR 0.000011 Loss 18.991399, Accuracy 0.100%\n",
      "Epoch 0, Batch 134, LR 0.000011 Loss 18.990885, Accuracy 0.099%\n",
      "Epoch 0, Batch 135, LR 0.000011 Loss 18.989703, Accuracy 0.104%\n",
      "Epoch 0, Batch 136, LR 0.000011 Loss 18.988801, Accuracy 0.103%\n",
      "Epoch 0, Batch 137, LR 0.000011 Loss 18.989222, Accuracy 0.103%\n",
      "Epoch 0, Batch 138, LR 0.000011 Loss 18.989387, Accuracy 0.102%\n",
      "Epoch 0, Batch 139, LR 0.000011 Loss 18.989236, Accuracy 0.101%\n",
      "Epoch 0, Batch 140, LR 0.000011 Loss 18.990814, Accuracy 0.100%\n",
      "Epoch 0, Batch 141, LR 0.000011 Loss 18.991747, Accuracy 0.100%\n",
      "Epoch 0, Batch 142, LR 0.000011 Loss 18.991525, Accuracy 0.099%\n",
      "Epoch 0, Batch 143, LR 0.000011 Loss 18.990531, Accuracy 0.098%\n",
      "Epoch 0, Batch 144, LR 0.000011 Loss 18.990469, Accuracy 0.098%\n",
      "Epoch 0, Batch 145, LR 0.000011 Loss 18.989457, Accuracy 0.097%\n",
      "Epoch 0, Batch 146, LR 0.000011 Loss 18.990844, Accuracy 0.096%\n",
      "Epoch 0, Batch 147, LR 0.000011 Loss 18.988045, Accuracy 0.101%\n",
      "Epoch 0, Batch 148, LR 0.000011 Loss 18.986904, Accuracy 0.100%\n",
      "Epoch 0, Batch 149, LR 0.000011 Loss 18.985374, Accuracy 0.100%\n",
      "Epoch 0, Batch 150, LR 0.000011 Loss 18.984825, Accuracy 0.099%\n",
      "Epoch 0, Batch 151, LR 0.000011 Loss 18.985046, Accuracy 0.098%\n",
      "Epoch 0, Batch 152, LR 0.000011 Loss 18.984532, Accuracy 0.098%\n",
      "Epoch 0, Batch 153, LR 0.000011 Loss 18.985835, Accuracy 0.097%\n",
      "Epoch 0, Batch 154, LR 0.000011 Loss 18.985155, Accuracy 0.096%\n",
      "Epoch 0, Batch 155, LR 0.000011 Loss 18.984411, Accuracy 0.096%\n",
      "Epoch 0, Batch 156, LR 0.000011 Loss 18.982469, Accuracy 0.095%\n",
      "Epoch 0, Batch 157, LR 0.000011 Loss 18.983113, Accuracy 0.095%\n",
      "Epoch 0, Batch 158, LR 0.000011 Loss 18.982603, Accuracy 0.099%\n",
      "Epoch 0, Batch 159, LR 0.000011 Loss 18.983987, Accuracy 0.098%\n",
      "Epoch 0, Batch 160, LR 0.000011 Loss 18.984566, Accuracy 0.098%\n",
      "Epoch 0, Batch 161, LR 0.000011 Loss 18.982524, Accuracy 0.097%\n",
      "Epoch 0, Batch 162, LR 0.000011 Loss 18.981566, Accuracy 0.096%\n",
      "Epoch 0, Batch 163, LR 0.000011 Loss 18.981232, Accuracy 0.101%\n",
      "Epoch 0, Batch 164, LR 0.000011 Loss 18.981116, Accuracy 0.100%\n",
      "Epoch 0, Batch 165, LR 0.000011 Loss 18.980561, Accuracy 0.099%\n",
      "Epoch 0, Batch 166, LR 0.000011 Loss 18.980167, Accuracy 0.099%\n",
      "Epoch 0, Batch 167, LR 0.000011 Loss 18.978426, Accuracy 0.098%\n",
      "Epoch 0, Batch 168, LR 0.000011 Loss 18.977255, Accuracy 0.098%\n",
      "Epoch 0, Batch 169, LR 0.000011 Loss 18.977352, Accuracy 0.097%\n",
      "Epoch 0, Batch 170, LR 0.000011 Loss 18.977133, Accuracy 0.097%\n",
      "Epoch 0, Batch 171, LR 0.000011 Loss 18.976337, Accuracy 0.096%\n",
      "Epoch 0, Batch 172, LR 0.000011 Loss 18.975720, Accuracy 0.095%\n",
      "Epoch 0, Batch 173, LR 0.000011 Loss 18.974479, Accuracy 0.095%\n",
      "Epoch 0, Batch 174, LR 0.000012 Loss 18.974232, Accuracy 0.094%\n",
      "Epoch 0, Batch 175, LR 0.000012 Loss 18.973765, Accuracy 0.094%\n",
      "Epoch 0, Batch 176, LR 0.000012 Loss 18.973820, Accuracy 0.093%\n",
      "Epoch 0, Batch 177, LR 0.000012 Loss 18.972422, Accuracy 0.093%\n",
      "Epoch 0, Batch 178, LR 0.000012 Loss 18.972543, Accuracy 0.092%\n",
      "Epoch 0, Batch 179, LR 0.000012 Loss 18.971572, Accuracy 0.092%\n",
      "Epoch 0, Batch 180, LR 0.000012 Loss 18.970590, Accuracy 0.091%\n",
      "Epoch 0, Batch 181, LR 0.000012 Loss 18.970832, Accuracy 0.091%\n",
      "Epoch 0, Batch 182, LR 0.000012 Loss 18.970237, Accuracy 0.090%\n",
      "Epoch 0, Batch 183, LR 0.000012 Loss 18.969943, Accuracy 0.090%\n",
      "Epoch 0, Batch 184, LR 0.000012 Loss 18.970692, Accuracy 0.089%\n",
      "Epoch 0, Batch 185, LR 0.000012 Loss 18.970799, Accuracy 0.089%\n",
      "Epoch 0, Batch 186, LR 0.000012 Loss 18.970481, Accuracy 0.088%\n",
      "Epoch 0, Batch 187, LR 0.000012 Loss 18.969180, Accuracy 0.088%\n",
      "Epoch 0, Batch 188, LR 0.000012 Loss 18.969428, Accuracy 0.087%\n",
      "Epoch 0, Batch 189, LR 0.000012 Loss 18.969905, Accuracy 0.087%\n",
      "Epoch 0, Batch 190, LR 0.000012 Loss 18.969523, Accuracy 0.086%\n",
      "Epoch 0, Batch 191, LR 0.000012 Loss 18.968573, Accuracy 0.094%\n",
      "Epoch 0, Batch 192, LR 0.000012 Loss 18.967514, Accuracy 0.094%\n",
      "Epoch 0, Batch 193, LR 0.000012 Loss 18.966664, Accuracy 0.093%\n",
      "Epoch 0, Batch 194, LR 0.000012 Loss 18.966504, Accuracy 0.093%\n",
      "Epoch 0, Batch 195, LR 0.000012 Loss 18.965493, Accuracy 0.092%\n",
      "Epoch 0, Batch 196, LR 0.000012 Loss 18.965374, Accuracy 0.092%\n",
      "Epoch 0, Batch 197, LR 0.000012 Loss 18.964772, Accuracy 0.095%\n",
      "Epoch 0, Batch 198, LR 0.000012 Loss 18.964361, Accuracy 0.095%\n",
      "Epoch 0, Batch 199, LR 0.000012 Loss 18.964089, Accuracy 0.102%\n",
      "Epoch 0, Batch 200, LR 0.000012 Loss 18.964070, Accuracy 0.102%\n",
      "Epoch 0, Batch 201, LR 0.000012 Loss 18.963966, Accuracy 0.101%\n",
      "Epoch 0, Batch 202, LR 0.000012 Loss 18.963642, Accuracy 0.101%\n",
      "Epoch 0, Batch 203, LR 0.000012 Loss 18.962799, Accuracy 0.100%\n",
      "Epoch 0, Batch 204, LR 0.000012 Loss 18.962411, Accuracy 0.100%\n",
      "Epoch 0, Batch 205, LR 0.000012 Loss 18.961793, Accuracy 0.099%\n",
      "Epoch 0, Batch 206, LR 0.000012 Loss 18.961278, Accuracy 0.099%\n",
      "Epoch 0, Batch 207, LR 0.000012 Loss 18.960904, Accuracy 0.098%\n",
      "Epoch 0, Batch 208, LR 0.000012 Loss 18.960497, Accuracy 0.101%\n",
      "Epoch 0, Batch 209, LR 0.000012 Loss 18.959837, Accuracy 0.101%\n",
      "Epoch 0, Batch 210, LR 0.000012 Loss 18.959459, Accuracy 0.100%\n",
      "Epoch 0, Batch 211, LR 0.000012 Loss 18.958054, Accuracy 0.100%\n",
      "Epoch 0, Batch 212, LR 0.000012 Loss 18.957670, Accuracy 0.099%\n",
      "Epoch 0, Batch 213, LR 0.000012 Loss 18.957362, Accuracy 0.099%\n",
      "Epoch 0, Batch 214, LR 0.000012 Loss 18.958292, Accuracy 0.099%\n",
      "Epoch 0, Batch 215, LR 0.000012 Loss 18.958006, Accuracy 0.098%\n",
      "Epoch 0, Batch 216, LR 0.000012 Loss 18.957416, Accuracy 0.098%\n",
      "Epoch 0, Batch 217, LR 0.000012 Loss 18.956846, Accuracy 0.097%\n",
      "Epoch 0, Batch 218, LR 0.000012 Loss 18.956360, Accuracy 0.100%\n",
      "Epoch 0, Batch 219, LR 0.000012 Loss 18.956400, Accuracy 0.100%\n",
      "Epoch 0, Batch 220, LR 0.000012 Loss 18.955694, Accuracy 0.103%\n",
      "Epoch 0, Batch 221, LR 0.000012 Loss 18.955270, Accuracy 0.103%\n",
      "Epoch 0, Batch 222, LR 0.000012 Loss 18.954769, Accuracy 0.102%\n",
      "Epoch 0, Batch 223, LR 0.000012 Loss 18.954056, Accuracy 0.102%\n",
      "Epoch 0, Batch 224, LR 0.000012 Loss 18.953525, Accuracy 0.101%\n",
      "Epoch 0, Batch 225, LR 0.000013 Loss 18.952824, Accuracy 0.108%\n",
      "Epoch 0, Batch 226, LR 0.000013 Loss 18.952438, Accuracy 0.107%\n",
      "Epoch 0, Batch 227, LR 0.000013 Loss 18.951861, Accuracy 0.110%\n",
      "Epoch 0, Batch 228, LR 0.000013 Loss 18.951323, Accuracy 0.110%\n",
      "Epoch 0, Batch 229, LR 0.000013 Loss 18.950373, Accuracy 0.109%\n",
      "Epoch 0, Batch 230, LR 0.000013 Loss 18.949778, Accuracy 0.109%\n",
      "Epoch 0, Batch 231, LR 0.000013 Loss 18.949233, Accuracy 0.108%\n",
      "Epoch 0, Batch 232, LR 0.000013 Loss 18.948971, Accuracy 0.108%\n",
      "Epoch 0, Batch 233, LR 0.000013 Loss 18.948413, Accuracy 0.111%\n",
      "Epoch 0, Batch 234, LR 0.000013 Loss 18.947898, Accuracy 0.110%\n",
      "Epoch 0, Batch 235, LR 0.000013 Loss 18.947001, Accuracy 0.113%\n",
      "Epoch 0, Batch 236, LR 0.000013 Loss 18.945951, Accuracy 0.113%\n",
      "Epoch 0, Batch 237, LR 0.000013 Loss 18.945424, Accuracy 0.115%\n",
      "Epoch 0, Batch 238, LR 0.000013 Loss 18.945065, Accuracy 0.115%\n",
      "Epoch 0, Batch 239, LR 0.000013 Loss 18.945680, Accuracy 0.114%\n",
      "Epoch 0, Batch 240, LR 0.000013 Loss 18.945659, Accuracy 0.114%\n",
      "Epoch 0, Batch 241, LR 0.000013 Loss 18.944495, Accuracy 0.113%\n",
      "Epoch 0, Batch 242, LR 0.000013 Loss 18.942870, Accuracy 0.119%\n",
      "Epoch 0, Batch 243, LR 0.000013 Loss 18.942402, Accuracy 0.119%\n",
      "Epoch 0, Batch 244, LR 0.000013 Loss 18.941052, Accuracy 0.118%\n",
      "Epoch 0, Batch 245, LR 0.000013 Loss 18.939874, Accuracy 0.121%\n",
      "Epoch 0, Batch 246, LR 0.000013 Loss 18.939069, Accuracy 0.124%\n",
      "Epoch 0, Batch 247, LR 0.000013 Loss 18.938702, Accuracy 0.123%\n",
      "Epoch 0, Batch 248, LR 0.000013 Loss 18.937623, Accuracy 0.123%\n",
      "Epoch 0, Batch 249, LR 0.000013 Loss 18.936647, Accuracy 0.126%\n",
      "Epoch 0, Batch 250, LR 0.000013 Loss 18.936323, Accuracy 0.125%\n",
      "Epoch 0, Batch 251, LR 0.000013 Loss 18.937080, Accuracy 0.125%\n",
      "Epoch 0, Batch 252, LR 0.000013 Loss 18.936068, Accuracy 0.124%\n",
      "Epoch 0, Batch 253, LR 0.000013 Loss 18.935193, Accuracy 0.124%\n",
      "Epoch 0, Batch 254, LR 0.000013 Loss 18.935014, Accuracy 0.123%\n",
      "Epoch 0, Batch 255, LR 0.000013 Loss 18.933992, Accuracy 0.123%\n",
      "Epoch 0, Batch 256, LR 0.000013 Loss 18.934182, Accuracy 0.122%\n",
      "Epoch 0, Batch 257, LR 0.000013 Loss 18.933637, Accuracy 0.122%\n",
      "Epoch 0, Batch 258, LR 0.000013 Loss 18.933060, Accuracy 0.121%\n",
      "Epoch 0, Batch 259, LR 0.000013 Loss 18.932767, Accuracy 0.121%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 260, LR 0.000013 Loss 18.932750, Accuracy 0.120%\n",
      "Epoch 0, Batch 261, LR 0.000013 Loss 18.933029, Accuracy 0.120%\n",
      "Epoch 0, Batch 262, LR 0.000013 Loss 18.932906, Accuracy 0.119%\n",
      "Epoch 0, Batch 263, LR 0.000013 Loss 18.931665, Accuracy 0.119%\n",
      "Epoch 0, Batch 264, LR 0.000013 Loss 18.931430, Accuracy 0.118%\n",
      "Epoch 0, Batch 265, LR 0.000013 Loss 18.931025, Accuracy 0.118%\n",
      "Epoch 0, Batch 266, LR 0.000014 Loss 18.930666, Accuracy 0.117%\n",
      "Epoch 0, Batch 267, LR 0.000014 Loss 18.929953, Accuracy 0.120%\n",
      "Epoch 0, Batch 268, LR 0.000014 Loss 18.928973, Accuracy 0.120%\n",
      "Epoch 0, Batch 269, LR 0.000014 Loss 18.928628, Accuracy 0.119%\n",
      "Epoch 0, Batch 270, LR 0.000014 Loss 18.928197, Accuracy 0.119%\n",
      "Epoch 0, Batch 271, LR 0.000014 Loss 18.927633, Accuracy 0.118%\n",
      "Epoch 0, Batch 272, LR 0.000014 Loss 18.927108, Accuracy 0.118%\n",
      "Epoch 0, Batch 273, LR 0.000014 Loss 18.926572, Accuracy 0.117%\n",
      "Epoch 0, Batch 274, LR 0.000014 Loss 18.925583, Accuracy 0.117%\n",
      "Epoch 0, Batch 275, LR 0.000014 Loss 18.924481, Accuracy 0.119%\n",
      "Epoch 0, Batch 276, LR 0.000014 Loss 18.924161, Accuracy 0.119%\n",
      "Epoch 0, Batch 277, LR 0.000014 Loss 18.923473, Accuracy 0.118%\n",
      "Epoch 0, Batch 278, LR 0.000014 Loss 18.922616, Accuracy 0.118%\n",
      "Epoch 0, Batch 279, LR 0.000014 Loss 18.921874, Accuracy 0.120%\n",
      "Epoch 0, Batch 280, LR 0.000014 Loss 18.920989, Accuracy 0.120%\n",
      "Epoch 0, Batch 281, LR 0.000014 Loss 18.920248, Accuracy 0.120%\n",
      "Epoch 0, Batch 282, LR 0.000014 Loss 18.919152, Accuracy 0.119%\n",
      "Epoch 0, Batch 283, LR 0.000014 Loss 18.918368, Accuracy 0.121%\n",
      "Epoch 0, Batch 284, LR 0.000014 Loss 18.917420, Accuracy 0.121%\n",
      "Epoch 0, Batch 285, LR 0.000014 Loss 18.916600, Accuracy 0.123%\n",
      "Epoch 0, Batch 286, LR 0.000014 Loss 18.916571, Accuracy 0.123%\n",
      "Epoch 0, Batch 287, LR 0.000014 Loss 18.916469, Accuracy 0.122%\n",
      "Epoch 0, Batch 288, LR 0.000014 Loss 18.916097, Accuracy 0.122%\n",
      "Epoch 0, Batch 289, LR 0.000014 Loss 18.915371, Accuracy 0.122%\n",
      "Epoch 0, Batch 290, LR 0.000014 Loss 18.914660, Accuracy 0.127%\n",
      "Epoch 0, Batch 291, LR 0.000014 Loss 18.913376, Accuracy 0.126%\n",
      "Epoch 0, Batch 292, LR 0.000014 Loss 18.912414, Accuracy 0.126%\n",
      "Epoch 0, Batch 293, LR 0.000014 Loss 18.911766, Accuracy 0.125%\n",
      "Epoch 0, Batch 294, LR 0.000014 Loss 18.911684, Accuracy 0.125%\n",
      "Epoch 0, Batch 295, LR 0.000014 Loss 18.911382, Accuracy 0.124%\n",
      "Epoch 0, Batch 296, LR 0.000014 Loss 18.910354, Accuracy 0.124%\n",
      "Epoch 0, Batch 297, LR 0.000014 Loss 18.910128, Accuracy 0.124%\n",
      "Epoch 0, Batch 298, LR 0.000014 Loss 18.909918, Accuracy 0.123%\n",
      "Epoch 0, Batch 299, LR 0.000014 Loss 18.909717, Accuracy 0.123%\n",
      "Epoch 0, Batch 300, LR 0.000014 Loss 18.909774, Accuracy 0.122%\n",
      "Epoch 0, Batch 301, LR 0.000014 Loss 18.909321, Accuracy 0.125%\n",
      "Epoch 0, Batch 302, LR 0.000015 Loss 18.909043, Accuracy 0.124%\n",
      "Epoch 0, Batch 303, LR 0.000015 Loss 18.908780, Accuracy 0.124%\n",
      "Epoch 0, Batch 304, LR 0.000015 Loss 18.908151, Accuracy 0.123%\n",
      "Epoch 0, Batch 305, LR 0.000015 Loss 18.907751, Accuracy 0.123%\n",
      "Epoch 0, Batch 306, LR 0.000015 Loss 18.906391, Accuracy 0.125%\n",
      "Epoch 0, Batch 307, LR 0.000015 Loss 18.905733, Accuracy 0.125%\n",
      "Epoch 0, Batch 308, LR 0.000015 Loss 18.904283, Accuracy 0.124%\n",
      "Epoch 0, Batch 309, LR 0.000015 Loss 18.903456, Accuracy 0.124%\n",
      "Epoch 0, Batch 310, LR 0.000015 Loss 18.903023, Accuracy 0.126%\n",
      "Epoch 0, Batch 311, LR 0.000015 Loss 18.902562, Accuracy 0.126%\n",
      "Epoch 0, Batch 312, LR 0.000015 Loss 18.902138, Accuracy 0.125%\n",
      "Epoch 0, Batch 313, LR 0.000015 Loss 18.901544, Accuracy 0.125%\n",
      "Epoch 0, Batch 314, LR 0.000015 Loss 18.901334, Accuracy 0.124%\n",
      "Epoch 0, Batch 315, LR 0.000015 Loss 18.900328, Accuracy 0.124%\n",
      "Epoch 0, Batch 316, LR 0.000015 Loss 18.899824, Accuracy 0.124%\n",
      "Epoch 0, Batch 317, LR 0.000015 Loss 18.899292, Accuracy 0.126%\n",
      "Epoch 0, Batch 318, LR 0.000015 Loss 18.899125, Accuracy 0.128%\n",
      "Epoch 0, Batch 319, LR 0.000015 Loss 18.898279, Accuracy 0.127%\n",
      "Epoch 0, Batch 320, LR 0.000015 Loss 18.897200, Accuracy 0.127%\n",
      "Epoch 0, Batch 321, LR 0.000015 Loss 18.896275, Accuracy 0.127%\n",
      "Epoch 0, Batch 322, LR 0.000015 Loss 18.895078, Accuracy 0.126%\n",
      "Epoch 0, Batch 323, LR 0.000015 Loss 18.894521, Accuracy 0.126%\n",
      "Epoch 0, Batch 324, LR 0.000015 Loss 18.893799, Accuracy 0.125%\n",
      "Epoch 0, Batch 325, LR 0.000015 Loss 18.893920, Accuracy 0.125%\n",
      "Epoch 0, Batch 326, LR 0.000015 Loss 18.892907, Accuracy 0.129%\n",
      "Epoch 0, Batch 327, LR 0.000015 Loss 18.891936, Accuracy 0.129%\n",
      "Epoch 0, Batch 328, LR 0.000015 Loss 18.891334, Accuracy 0.133%\n",
      "Epoch 0, Batch 329, LR 0.000015 Loss 18.889829, Accuracy 0.133%\n",
      "Epoch 0, Batch 330, LR 0.000015 Loss 18.889336, Accuracy 0.135%\n",
      "Epoch 0, Batch 331, LR 0.000015 Loss 18.888468, Accuracy 0.137%\n",
      "Epoch 0, Batch 332, LR 0.000015 Loss 18.887143, Accuracy 0.136%\n",
      "Epoch 0, Batch 333, LR 0.000015 Loss 18.885914, Accuracy 0.138%\n",
      "Epoch 0, Batch 334, LR 0.000016 Loss 18.885489, Accuracy 0.140%\n",
      "Epoch 0, Batch 335, LR 0.000016 Loss 18.884675, Accuracy 0.140%\n",
      "Epoch 0, Batch 336, LR 0.000016 Loss 18.883361, Accuracy 0.140%\n",
      "Epoch 0, Batch 337, LR 0.000016 Loss 18.882539, Accuracy 0.144%\n",
      "Epoch 0, Batch 338, LR 0.000016 Loss 18.881887, Accuracy 0.148%\n",
      "Epoch 0, Batch 339, LR 0.000016 Loss 18.881116, Accuracy 0.147%\n",
      "Epoch 0, Batch 340, LR 0.000016 Loss 18.880565, Accuracy 0.147%\n",
      "Epoch 0, Batch 341, LR 0.000016 Loss 18.880236, Accuracy 0.149%\n",
      "Epoch 0, Batch 342, LR 0.000016 Loss 18.880077, Accuracy 0.148%\n",
      "Epoch 0, Batch 343, LR 0.000016 Loss 18.879291, Accuracy 0.148%\n",
      "Epoch 0, Batch 344, LR 0.000016 Loss 18.878960, Accuracy 0.148%\n",
      "Epoch 0, Batch 345, LR 0.000016 Loss 18.878660, Accuracy 0.147%\n",
      "Epoch 0, Batch 346, LR 0.000016 Loss 18.878047, Accuracy 0.147%\n",
      "Epoch 0, Batch 347, LR 0.000016 Loss 18.877498, Accuracy 0.146%\n",
      "Epoch 0, Batch 348, LR 0.000016 Loss 18.876835, Accuracy 0.146%\n",
      "Epoch 0, Batch 349, LR 0.000016 Loss 18.876543, Accuracy 0.148%\n",
      "Epoch 0, Batch 350, LR 0.000016 Loss 18.875612, Accuracy 0.147%\n",
      "Epoch 0, Batch 351, LR 0.000016 Loss 18.875872, Accuracy 0.147%\n",
      "Epoch 0, Batch 352, LR 0.000016 Loss 18.875337, Accuracy 0.146%\n",
      "Epoch 0, Batch 353, LR 0.000016 Loss 18.875090, Accuracy 0.146%\n",
      "Epoch 0, Batch 354, LR 0.000016 Loss 18.874276, Accuracy 0.146%\n",
      "Epoch 0, Batch 355, LR 0.000016 Loss 18.873456, Accuracy 0.147%\n",
      "Epoch 0, Batch 356, LR 0.000016 Loss 18.872684, Accuracy 0.147%\n",
      "Epoch 0, Batch 357, LR 0.000016 Loss 18.872489, Accuracy 0.147%\n",
      "Epoch 0, Batch 358, LR 0.000016 Loss 18.871616, Accuracy 0.148%\n",
      "Epoch 0, Batch 359, LR 0.000016 Loss 18.871112, Accuracy 0.148%\n",
      "Epoch 0, Batch 360, LR 0.000016 Loss 18.870322, Accuracy 0.148%\n",
      "Epoch 0, Batch 361, LR 0.000016 Loss 18.869162, Accuracy 0.147%\n",
      "Epoch 0, Batch 362, LR 0.000016 Loss 18.868428, Accuracy 0.149%\n",
      "Epoch 0, Batch 363, LR 0.000016 Loss 18.867819, Accuracy 0.149%\n",
      "Epoch 0, Batch 364, LR 0.000017 Loss 18.866699, Accuracy 0.148%\n",
      "Epoch 0, Batch 365, LR 0.000017 Loss 18.865758, Accuracy 0.150%\n",
      "Epoch 0, Batch 366, LR 0.000017 Loss 18.864990, Accuracy 0.152%\n",
      "Epoch 0, Batch 367, LR 0.000017 Loss 18.863864, Accuracy 0.153%\n",
      "Epoch 0, Batch 368, LR 0.000017 Loss 18.862755, Accuracy 0.153%\n",
      "Epoch 0, Batch 369, LR 0.000017 Loss 18.861980, Accuracy 0.155%\n",
      "Epoch 0, Batch 370, LR 0.000017 Loss 18.860603, Accuracy 0.154%\n",
      "Epoch 0, Batch 371, LR 0.000017 Loss 18.859925, Accuracy 0.154%\n",
      "Epoch 0, Batch 372, LR 0.000017 Loss 18.858505, Accuracy 0.158%\n",
      "Epoch 0, Batch 373, LR 0.000017 Loss 18.857788, Accuracy 0.157%\n",
      "Epoch 0, Batch 374, LR 0.000017 Loss 18.857356, Accuracy 0.157%\n",
      "Epoch 0, Batch 375, LR 0.000017 Loss 18.856783, Accuracy 0.158%\n",
      "Epoch 0, Batch 376, LR 0.000017 Loss 18.855755, Accuracy 0.162%\n",
      "Epoch 0, Batch 377, LR 0.000017 Loss 18.854949, Accuracy 0.164%\n",
      "Epoch 0, Batch 378, LR 0.000017 Loss 18.854147, Accuracy 0.163%\n",
      "Epoch 0, Batch 379, LR 0.000017 Loss 18.853566, Accuracy 0.165%\n",
      "Epoch 0, Batch 380, LR 0.000017 Loss 18.853154, Accuracy 0.164%\n",
      "Epoch 0, Batch 381, LR 0.000017 Loss 18.852834, Accuracy 0.164%\n",
      "Epoch 0, Batch 382, LR 0.000017 Loss 18.852045, Accuracy 0.166%\n",
      "Epoch 0, Batch 383, LR 0.000017 Loss 18.850858, Accuracy 0.171%\n",
      "Epoch 0, Batch 384, LR 0.000017 Loss 18.850358, Accuracy 0.171%\n",
      "Epoch 0, Batch 385, LR 0.000017 Loss 18.849585, Accuracy 0.170%\n",
      "Epoch 0, Batch 386, LR 0.000017 Loss 18.848875, Accuracy 0.170%\n",
      "Epoch 0, Batch 387, LR 0.000017 Loss 18.848175, Accuracy 0.172%\n",
      "Epoch 0, Batch 388, LR 0.000017 Loss 18.847666, Accuracy 0.177%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 389, LR 0.000017 Loss 18.846912, Accuracy 0.177%\n",
      "Epoch 0, Batch 390, LR 0.000017 Loss 18.846428, Accuracy 0.176%\n",
      "Epoch 0, Batch 391, LR 0.000017 Loss 18.845804, Accuracy 0.176%\n",
      "Epoch 0, Batch 392, LR 0.000018 Loss 18.845204, Accuracy 0.175%\n",
      "Epoch 0, Batch 393, LR 0.000018 Loss 18.844888, Accuracy 0.175%\n",
      "Epoch 0, Batch 394, LR 0.000018 Loss 18.844568, Accuracy 0.174%\n",
      "Epoch 0, Batch 395, LR 0.000018 Loss 18.843977, Accuracy 0.174%\n",
      "Epoch 0, Batch 396, LR 0.000018 Loss 18.843172, Accuracy 0.180%\n",
      "Epoch 0, Batch 397, LR 0.000018 Loss 18.842533, Accuracy 0.179%\n",
      "Epoch 0, Batch 398, LR 0.000018 Loss 18.841378, Accuracy 0.181%\n",
      "Epoch 0, Batch 399, LR 0.000018 Loss 18.840266, Accuracy 0.182%\n",
      "Epoch 0, Batch 400, LR 0.000018 Loss 18.839765, Accuracy 0.184%\n",
      "Epoch 0, Batch 401, LR 0.000018 Loss 18.838706, Accuracy 0.185%\n",
      "Epoch 0, Batch 402, LR 0.000018 Loss 18.838511, Accuracy 0.185%\n",
      "Epoch 0, Batch 403, LR 0.000018 Loss 18.838162, Accuracy 0.188%\n",
      "Epoch 0, Batch 404, LR 0.000018 Loss 18.837324, Accuracy 0.188%\n",
      "Epoch 0, Batch 405, LR 0.000018 Loss 18.836280, Accuracy 0.187%\n",
      "Epoch 0, Batch 406, LR 0.000018 Loss 18.835021, Accuracy 0.187%\n",
      "Epoch 0, Batch 407, LR 0.000018 Loss 18.834168, Accuracy 0.190%\n",
      "Epoch 0, Batch 408, LR 0.000018 Loss 18.833591, Accuracy 0.193%\n",
      "Epoch 0, Batch 409, LR 0.000018 Loss 18.832876, Accuracy 0.193%\n",
      "Epoch 0, Batch 410, LR 0.000018 Loss 18.831718, Accuracy 0.192%\n",
      "Epoch 0, Batch 411, LR 0.000018 Loss 18.831071, Accuracy 0.194%\n",
      "Epoch 0, Batch 412, LR 0.000018 Loss 18.830199, Accuracy 0.193%\n",
      "Epoch 0, Batch 413, LR 0.000018 Loss 18.829726, Accuracy 0.195%\n",
      "Epoch 0, Batch 414, LR 0.000018 Loss 18.828238, Accuracy 0.200%\n",
      "Epoch 0, Batch 415, LR 0.000018 Loss 18.827305, Accuracy 0.201%\n",
      "Epoch 0, Batch 416, LR 0.000018 Loss 18.826044, Accuracy 0.203%\n",
      "Epoch 0, Batch 417, LR 0.000018 Loss 18.825403, Accuracy 0.202%\n",
      "Epoch 0, Batch 418, LR 0.000019 Loss 18.824594, Accuracy 0.206%\n",
      "Epoch 0, Batch 419, LR 0.000019 Loss 18.823889, Accuracy 0.207%\n",
      "Epoch 0, Batch 420, LR 0.000019 Loss 18.823216, Accuracy 0.208%\n",
      "Epoch 0, Batch 421, LR 0.000019 Loss 18.822056, Accuracy 0.210%\n",
      "Epoch 0, Batch 422, LR 0.000019 Loss 18.821137, Accuracy 0.209%\n",
      "Epoch 0, Batch 423, LR 0.000019 Loss 18.820444, Accuracy 0.209%\n",
      "Epoch 0, Batch 424, LR 0.000019 Loss 18.819340, Accuracy 0.210%\n",
      "Epoch 0, Batch 425, LR 0.000019 Loss 18.818600, Accuracy 0.211%\n",
      "Epoch 0, Batch 426, LR 0.000019 Loss 18.817885, Accuracy 0.211%\n",
      "Epoch 0, Batch 427, LR 0.000019 Loss 18.816433, Accuracy 0.212%\n",
      "Epoch 0, Batch 428, LR 0.000019 Loss 18.815608, Accuracy 0.215%\n",
      "Epoch 0, Batch 429, LR 0.000019 Loss 18.814791, Accuracy 0.215%\n",
      "Epoch 0, Batch 430, LR 0.000019 Loss 18.813996, Accuracy 0.216%\n",
      "Epoch 0, Batch 431, LR 0.000019 Loss 18.813894, Accuracy 0.218%\n",
      "Epoch 0, Batch 432, LR 0.000019 Loss 18.813285, Accuracy 0.217%\n",
      "Epoch 0, Batch 433, LR 0.000019 Loss 18.812434, Accuracy 0.217%\n",
      "Epoch 0, Batch 434, LR 0.000019 Loss 18.811574, Accuracy 0.218%\n",
      "Epoch 0, Batch 435, LR 0.000019 Loss 18.810370, Accuracy 0.219%\n",
      "Epoch 0, Batch 436, LR 0.000019 Loss 18.809605, Accuracy 0.220%\n",
      "Epoch 0, Batch 437, LR 0.000019 Loss 18.809081, Accuracy 0.222%\n",
      "Epoch 0, Batch 438, LR 0.000019 Loss 18.808326, Accuracy 0.223%\n",
      "Epoch 0, Batch 439, LR 0.000019 Loss 18.807387, Accuracy 0.222%\n",
      "Epoch 0, Batch 440, LR 0.000019 Loss 18.806335, Accuracy 0.224%\n",
      "Epoch 0, Batch 441, LR 0.000019 Loss 18.805246, Accuracy 0.223%\n",
      "Epoch 0, Batch 442, LR 0.000020 Loss 18.803906, Accuracy 0.226%\n",
      "Epoch 0, Batch 443, LR 0.000020 Loss 18.803160, Accuracy 0.226%\n",
      "Epoch 0, Batch 444, LR 0.000020 Loss 18.803164, Accuracy 0.225%\n",
      "Epoch 0, Batch 445, LR 0.000020 Loss 18.802135, Accuracy 0.226%\n",
      "Epoch 0, Batch 446, LR 0.000020 Loss 18.801042, Accuracy 0.228%\n",
      "Epoch 0, Batch 447, LR 0.000020 Loss 18.800278, Accuracy 0.227%\n",
      "Epoch 0, Batch 448, LR 0.000020 Loss 18.799648, Accuracy 0.227%\n",
      "Epoch 0, Batch 449, LR 0.000020 Loss 18.799199, Accuracy 0.226%\n",
      "Epoch 0, Batch 450, LR 0.000020 Loss 18.798167, Accuracy 0.229%\n",
      "Epoch 0, Batch 451, LR 0.000020 Loss 18.797022, Accuracy 0.230%\n",
      "Epoch 0, Batch 452, LR 0.000020 Loss 18.796210, Accuracy 0.230%\n",
      "Epoch 0, Batch 453, LR 0.000020 Loss 18.795577, Accuracy 0.231%\n",
      "Epoch 0, Batch 454, LR 0.000020 Loss 18.794718, Accuracy 0.234%\n",
      "Epoch 0, Batch 455, LR 0.000020 Loss 18.793796, Accuracy 0.234%\n",
      "Epoch 0, Batch 456, LR 0.000020 Loss 18.792978, Accuracy 0.235%\n",
      "Epoch 0, Batch 457, LR 0.000020 Loss 18.792002, Accuracy 0.239%\n",
      "Epoch 0, Batch 458, LR 0.000020 Loss 18.791059, Accuracy 0.242%\n",
      "Epoch 0, Batch 459, LR 0.000020 Loss 18.790451, Accuracy 0.245%\n",
      "Epoch 0, Batch 460, LR 0.000020 Loss 18.789692, Accuracy 0.245%\n",
      "Epoch 0, Batch 461, LR 0.000020 Loss 18.789073, Accuracy 0.244%\n",
      "Epoch 0, Batch 462, LR 0.000020 Loss 18.788411, Accuracy 0.245%\n",
      "Epoch 0, Batch 463, LR 0.000020 Loss 18.787857, Accuracy 0.248%\n",
      "Epoch 0, Batch 464, LR 0.000020 Loss 18.787050, Accuracy 0.249%\n",
      "Epoch 0, Batch 465, LR 0.000020 Loss 18.786058, Accuracy 0.252%\n",
      "Epoch 0, Batch 466, LR 0.000021 Loss 18.785595, Accuracy 0.251%\n",
      "Epoch 0, Batch 467, LR 0.000021 Loss 18.785114, Accuracy 0.251%\n",
      "Epoch 0, Batch 468, LR 0.000021 Loss 18.784185, Accuracy 0.255%\n",
      "Epoch 0, Batch 469, LR 0.000021 Loss 18.783646, Accuracy 0.257%\n",
      "Epoch 0, Batch 470, LR 0.000021 Loss 18.782489, Accuracy 0.261%\n",
      "Epoch 0, Batch 471, LR 0.000021 Loss 18.781607, Accuracy 0.265%\n",
      "Epoch 0, Batch 472, LR 0.000021 Loss 18.780914, Accuracy 0.266%\n",
      "Epoch 0, Batch 473, LR 0.000021 Loss 18.780472, Accuracy 0.266%\n",
      "Epoch 0, Batch 474, LR 0.000021 Loss 18.779896, Accuracy 0.270%\n",
      "Epoch 0, Batch 475, LR 0.000021 Loss 18.779072, Accuracy 0.270%\n",
      "Epoch 0, Batch 476, LR 0.000021 Loss 18.777943, Accuracy 0.271%\n",
      "Epoch 0, Batch 477, LR 0.000021 Loss 18.776952, Accuracy 0.272%\n",
      "Epoch 0, Batch 478, LR 0.000021 Loss 18.775939, Accuracy 0.273%\n",
      "Epoch 0, Batch 479, LR 0.000021 Loss 18.774994, Accuracy 0.276%\n",
      "Epoch 0, Batch 480, LR 0.000021 Loss 18.773922, Accuracy 0.280%\n",
      "Epoch 0, Batch 481, LR 0.000021 Loss 18.773344, Accuracy 0.279%\n",
      "Epoch 0, Batch 482, LR 0.000021 Loss 18.772382, Accuracy 0.279%\n",
      "Epoch 0, Batch 483, LR 0.000021 Loss 18.771568, Accuracy 0.278%\n",
      "Epoch 0, Batch 484, LR 0.000021 Loss 18.770193, Accuracy 0.284%\n",
      "Epoch 0, Batch 485, LR 0.000021 Loss 18.769481, Accuracy 0.284%\n",
      "Epoch 0, Batch 486, LR 0.000021 Loss 18.768651, Accuracy 0.288%\n",
      "Epoch 0, Batch 487, LR 0.000021 Loss 18.767689, Accuracy 0.289%\n",
      "Epoch 0, Batch 488, LR 0.000021 Loss 18.766888, Accuracy 0.290%\n",
      "Epoch 0, Batch 489, LR 0.000022 Loss 18.766044, Accuracy 0.291%\n",
      "Epoch 0, Batch 490, LR 0.000022 Loss 18.765388, Accuracy 0.290%\n",
      "Epoch 0, Batch 491, LR 0.000022 Loss 18.764743, Accuracy 0.293%\n",
      "Epoch 0, Batch 492, LR 0.000022 Loss 18.763878, Accuracy 0.294%\n",
      "Epoch 0, Batch 493, LR 0.000022 Loss 18.763156, Accuracy 0.295%\n",
      "Epoch 0, Batch 494, LR 0.000022 Loss 18.762266, Accuracy 0.302%\n",
      "Epoch 0, Batch 495, LR 0.000022 Loss 18.761465, Accuracy 0.308%\n",
      "Epoch 0, Batch 496, LR 0.000022 Loss 18.760166, Accuracy 0.313%\n",
      "Epoch 0, Batch 497, LR 0.000022 Loss 18.759613, Accuracy 0.313%\n",
      "Epoch 0, Batch 498, LR 0.000022 Loss 18.758580, Accuracy 0.317%\n",
      "Epoch 0, Batch 499, LR 0.000022 Loss 18.757469, Accuracy 0.318%\n",
      "Epoch 0, Batch 500, LR 0.000022 Loss 18.756845, Accuracy 0.319%\n",
      "Epoch 0, Batch 501, LR 0.000022 Loss 18.756087, Accuracy 0.318%\n",
      "Epoch 0, Batch 502, LR 0.000022 Loss 18.754952, Accuracy 0.322%\n",
      "Epoch 0, Batch 503, LR 0.000022 Loss 18.753874, Accuracy 0.325%\n",
      "Epoch 0, Batch 504, LR 0.000022 Loss 18.752954, Accuracy 0.324%\n",
      "Epoch 0, Batch 505, LR 0.000022 Loss 18.752380, Accuracy 0.325%\n",
      "Epoch 0, Batch 506, LR 0.000022 Loss 18.751343, Accuracy 0.327%\n",
      "Epoch 0, Batch 507, LR 0.000022 Loss 18.750270, Accuracy 0.328%\n",
      "Epoch 0, Batch 508, LR 0.000022 Loss 18.749378, Accuracy 0.328%\n",
      "Epoch 0, Batch 509, LR 0.000022 Loss 18.748518, Accuracy 0.328%\n",
      "Epoch 0, Batch 510, LR 0.000023 Loss 18.747308, Accuracy 0.329%\n",
      "Epoch 0, Batch 511, LR 0.000023 Loss 18.746515, Accuracy 0.332%\n",
      "Epoch 0, Batch 512, LR 0.000023 Loss 18.745346, Accuracy 0.334%\n",
      "Epoch 0, Batch 513, LR 0.000023 Loss 18.744394, Accuracy 0.338%\n",
      "Epoch 0, Batch 514, LR 0.000023 Loss 18.743660, Accuracy 0.339%\n",
      "Epoch 0, Batch 515, LR 0.000023 Loss 18.742719, Accuracy 0.341%\n",
      "Epoch 0, Batch 516, LR 0.000023 Loss 18.742559, Accuracy 0.342%\n",
      "Epoch 0, Batch 517, LR 0.000023 Loss 18.741683, Accuracy 0.343%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 518, LR 0.000023 Loss 18.740424, Accuracy 0.350%\n",
      "Epoch 0, Batch 519, LR 0.000023 Loss 18.739587, Accuracy 0.351%\n",
      "Epoch 0, Batch 520, LR 0.000023 Loss 18.738864, Accuracy 0.352%\n",
      "Epoch 0, Batch 521, LR 0.000023 Loss 18.737862, Accuracy 0.352%\n",
      "Epoch 0, Batch 522, LR 0.000023 Loss 18.737096, Accuracy 0.353%\n",
      "Epoch 0, Batch 523, LR 0.000023 Loss 18.736145, Accuracy 0.356%\n",
      "Epoch 0, Batch 524, LR 0.000023 Loss 18.735064, Accuracy 0.355%\n",
      "Epoch 0, Batch 525, LR 0.000023 Loss 18.734261, Accuracy 0.356%\n",
      "Epoch 0, Batch 526, LR 0.000023 Loss 18.733333, Accuracy 0.356%\n",
      "Epoch 0, Batch 527, LR 0.000023 Loss 18.732339, Accuracy 0.362%\n",
      "Epoch 0, Batch 528, LR 0.000023 Loss 18.731287, Accuracy 0.368%\n",
      "Epoch 0, Batch 529, LR 0.000023 Loss 18.730543, Accuracy 0.369%\n",
      "Epoch 0, Batch 530, LR 0.000023 Loss 18.729657, Accuracy 0.369%\n",
      "Epoch 0, Batch 531, LR 0.000024 Loss 18.728807, Accuracy 0.372%\n",
      "Epoch 0, Batch 532, LR 0.000024 Loss 18.727717, Accuracy 0.377%\n",
      "Epoch 0, Batch 533, LR 0.000024 Loss 18.726959, Accuracy 0.378%\n",
      "Epoch 0, Batch 534, LR 0.000024 Loss 18.725980, Accuracy 0.380%\n",
      "Epoch 0, Batch 535, LR 0.000024 Loss 18.724965, Accuracy 0.381%\n",
      "Epoch 0, Batch 536, LR 0.000024 Loss 18.723957, Accuracy 0.383%\n",
      "Epoch 0, Batch 537, LR 0.000024 Loss 18.722759, Accuracy 0.388%\n",
      "Epoch 0, Batch 538, LR 0.000024 Loss 18.722123, Accuracy 0.392%\n",
      "Epoch 0, Batch 539, LR 0.000024 Loss 18.721331, Accuracy 0.394%\n",
      "Epoch 0, Batch 540, LR 0.000024 Loss 18.720523, Accuracy 0.395%\n",
      "Epoch 0, Batch 541, LR 0.000024 Loss 18.719576, Accuracy 0.394%\n",
      "Epoch 0, Batch 542, LR 0.000024 Loss 18.718456, Accuracy 0.395%\n",
      "Epoch 0, Batch 543, LR 0.000024 Loss 18.717269, Accuracy 0.397%\n",
      "Epoch 0, Batch 544, LR 0.000024 Loss 18.716176, Accuracy 0.402%\n",
      "Epoch 0, Batch 545, LR 0.000024 Loss 18.715412, Accuracy 0.406%\n",
      "Epoch 0, Batch 546, LR 0.000024 Loss 18.714360, Accuracy 0.408%\n",
      "Epoch 0, Batch 547, LR 0.000024 Loss 18.713539, Accuracy 0.408%\n",
      "Epoch 0, Batch 548, LR 0.000024 Loss 18.712853, Accuracy 0.408%\n",
      "Epoch 0, Batch 549, LR 0.000024 Loss 18.711976, Accuracy 0.407%\n",
      "Epoch 0, Batch 550, LR 0.000024 Loss 18.711157, Accuracy 0.408%\n",
      "Epoch 0, Batch 551, LR 0.000024 Loss 18.710056, Accuracy 0.411%\n",
      "Epoch 0, Batch 552, LR 0.000025 Loss 18.708937, Accuracy 0.413%\n",
      "Epoch 0, Batch 553, LR 0.000025 Loss 18.708029, Accuracy 0.415%\n",
      "Epoch 0, Batch 554, LR 0.000025 Loss 18.707296, Accuracy 0.417%\n",
      "Epoch 0, Batch 555, LR 0.000025 Loss 18.706029, Accuracy 0.418%\n",
      "Epoch 0, Batch 556, LR 0.000025 Loss 18.705065, Accuracy 0.419%\n",
      "Epoch 0, Batch 557, LR 0.000025 Loss 18.704020, Accuracy 0.418%\n",
      "Epoch 0, Batch 558, LR 0.000025 Loss 18.703364, Accuracy 0.419%\n",
      "Epoch 0, Batch 559, LR 0.000025 Loss 18.702373, Accuracy 0.421%\n",
      "Epoch 0, Batch 560, LR 0.000025 Loss 18.701249, Accuracy 0.421%\n",
      "Epoch 0, Batch 561, LR 0.000025 Loss 18.700014, Accuracy 0.428%\n",
      "Epoch 0, Batch 562, LR 0.000025 Loss 18.698902, Accuracy 0.430%\n",
      "Epoch 0, Batch 563, LR 0.000025 Loss 18.697616, Accuracy 0.434%\n",
      "Epoch 0, Batch 564, LR 0.000025 Loss 18.696517, Accuracy 0.438%\n",
      "Epoch 0, Batch 565, LR 0.000025 Loss 18.695740, Accuracy 0.438%\n",
      "Epoch 0, Batch 566, LR 0.000025 Loss 18.695149, Accuracy 0.438%\n",
      "Epoch 0, Batch 567, LR 0.000025 Loss 18.694014, Accuracy 0.441%\n",
      "Epoch 0, Batch 568, LR 0.000025 Loss 18.693291, Accuracy 0.444%\n",
      "Epoch 0, Batch 569, LR 0.000025 Loss 18.692051, Accuracy 0.449%\n",
      "Epoch 0, Batch 570, LR 0.000025 Loss 18.690976, Accuracy 0.452%\n",
      "Epoch 0, Batch 571, LR 0.000025 Loss 18.690067, Accuracy 0.453%\n",
      "Epoch 0, Batch 572, LR 0.000026 Loss 18.689061, Accuracy 0.452%\n",
      "Epoch 0, Batch 573, LR 0.000026 Loss 18.688241, Accuracy 0.454%\n",
      "Epoch 0, Batch 574, LR 0.000026 Loss 18.687385, Accuracy 0.455%\n",
      "Epoch 0, Batch 575, LR 0.000026 Loss 18.686507, Accuracy 0.455%\n",
      "Epoch 0, Batch 576, LR 0.000026 Loss 18.685743, Accuracy 0.456%\n",
      "Epoch 0, Batch 577, LR 0.000026 Loss 18.684765, Accuracy 0.459%\n",
      "Epoch 0, Batch 578, LR 0.000026 Loss 18.683764, Accuracy 0.462%\n",
      "Epoch 0, Batch 579, LR 0.000026 Loss 18.682635, Accuracy 0.468%\n",
      "Epoch 0, Batch 580, LR 0.000026 Loss 18.681415, Accuracy 0.471%\n",
      "Epoch 0, Batch 581, LR 0.000026 Loss 18.680673, Accuracy 0.472%\n",
      "Epoch 0, Batch 582, LR 0.000026 Loss 18.679638, Accuracy 0.471%\n",
      "Epoch 0, Batch 583, LR 0.000026 Loss 18.678529, Accuracy 0.470%\n",
      "Epoch 0, Batch 584, LR 0.000026 Loss 18.677468, Accuracy 0.472%\n",
      "Epoch 0, Batch 585, LR 0.000026 Loss 18.676724, Accuracy 0.474%\n",
      "Epoch 0, Batch 586, LR 0.000026 Loss 18.675808, Accuracy 0.475%\n",
      "Epoch 0, Batch 587, LR 0.000026 Loss 18.674564, Accuracy 0.478%\n",
      "Epoch 0, Batch 588, LR 0.000026 Loss 18.673653, Accuracy 0.478%\n",
      "Epoch 0, Batch 589, LR 0.000026 Loss 18.672932, Accuracy 0.478%\n",
      "Epoch 0, Batch 590, LR 0.000026 Loss 18.672148, Accuracy 0.477%\n",
      "Epoch 0, Batch 591, LR 0.000027 Loss 18.671192, Accuracy 0.479%\n",
      "Epoch 0, Batch 592, LR 0.000027 Loss 18.670323, Accuracy 0.480%\n",
      "Epoch 0, Batch 593, LR 0.000027 Loss 18.669510, Accuracy 0.485%\n",
      "Epoch 0, Batch 594, LR 0.000027 Loss 18.668882, Accuracy 0.488%\n",
      "Epoch 0, Batch 595, LR 0.000027 Loss 18.667876, Accuracy 0.492%\n",
      "Epoch 0, Batch 596, LR 0.000027 Loss 18.666462, Accuracy 0.494%\n",
      "Epoch 0, Batch 597, LR 0.000027 Loss 18.665583, Accuracy 0.495%\n",
      "Epoch 0, Batch 598, LR 0.000027 Loss 18.664900, Accuracy 0.495%\n",
      "Epoch 0, Batch 599, LR 0.000027 Loss 18.664257, Accuracy 0.497%\n",
      "Epoch 0, Batch 600, LR 0.000027 Loss 18.663323, Accuracy 0.499%\n",
      "Epoch 0, Batch 601, LR 0.000027 Loss 18.662965, Accuracy 0.498%\n",
      "Epoch 0, Batch 602, LR 0.000027 Loss 18.661920, Accuracy 0.500%\n",
      "Epoch 0, Batch 603, LR 0.000027 Loss 18.661063, Accuracy 0.500%\n",
      "Epoch 0, Batch 604, LR 0.000027 Loss 18.660383, Accuracy 0.503%\n",
      "Epoch 0, Batch 605, LR 0.000027 Loss 18.659499, Accuracy 0.504%\n",
      "Epoch 0, Batch 606, LR 0.000027 Loss 18.658497, Accuracy 0.503%\n",
      "Epoch 0, Batch 607, LR 0.000027 Loss 18.657604, Accuracy 0.503%\n",
      "Epoch 0, Batch 608, LR 0.000027 Loss 18.656473, Accuracy 0.504%\n",
      "Epoch 0, Batch 609, LR 0.000027 Loss 18.655335, Accuracy 0.508%\n",
      "Epoch 0, Batch 610, LR 0.000028 Loss 18.654243, Accuracy 0.510%\n",
      "Epoch 0, Batch 611, LR 0.000028 Loss 18.653621, Accuracy 0.509%\n",
      "Epoch 0, Batch 612, LR 0.000028 Loss 18.652857, Accuracy 0.509%\n",
      "Epoch 0, Batch 613, LR 0.000028 Loss 18.651761, Accuracy 0.511%\n",
      "Epoch 0, Batch 614, LR 0.000028 Loss 18.650708, Accuracy 0.512%\n",
      "Epoch 0, Batch 615, LR 0.000028 Loss 18.649548, Accuracy 0.513%\n",
      "Epoch 0, Batch 616, LR 0.000028 Loss 18.648487, Accuracy 0.516%\n",
      "Epoch 0, Batch 617, LR 0.000028 Loss 18.647916, Accuracy 0.515%\n",
      "Epoch 0, Batch 618, LR 0.000028 Loss 18.646790, Accuracy 0.518%\n",
      "Epoch 0, Batch 619, LR 0.000028 Loss 18.645555, Accuracy 0.523%\n",
      "Epoch 0, Batch 620, LR 0.000028 Loss 18.644452, Accuracy 0.522%\n",
      "Epoch 0, Batch 621, LR 0.000028 Loss 18.643866, Accuracy 0.523%\n",
      "Epoch 0, Batch 622, LR 0.000028 Loss 18.642680, Accuracy 0.525%\n",
      "Epoch 0, Batch 623, LR 0.000028 Loss 18.641773, Accuracy 0.525%\n",
      "Epoch 0, Batch 624, LR 0.000028 Loss 18.641087, Accuracy 0.527%\n",
      "Epoch 0, Batch 625, LR 0.000028 Loss 18.640011, Accuracy 0.527%\n",
      "Epoch 0, Batch 626, LR 0.000028 Loss 18.638930, Accuracy 0.530%\n",
      "Epoch 0, Batch 627, LR 0.000028 Loss 18.637663, Accuracy 0.532%\n",
      "Epoch 0, Batch 628, LR 0.000029 Loss 18.636892, Accuracy 0.531%\n",
      "Epoch 0, Batch 629, LR 0.000029 Loss 18.636064, Accuracy 0.533%\n",
      "Epoch 0, Batch 630, LR 0.000029 Loss 18.635052, Accuracy 0.533%\n",
      "Epoch 0, Batch 631, LR 0.000029 Loss 18.633840, Accuracy 0.535%\n",
      "Epoch 0, Batch 632, LR 0.000029 Loss 18.633296, Accuracy 0.535%\n",
      "Epoch 0, Batch 633, LR 0.000029 Loss 18.632613, Accuracy 0.536%\n",
      "Epoch 0, Batch 634, LR 0.000029 Loss 18.631809, Accuracy 0.540%\n",
      "Epoch 0, Batch 635, LR 0.000029 Loss 18.631034, Accuracy 0.540%\n",
      "Epoch 0, Batch 636, LR 0.000029 Loss 18.630288, Accuracy 0.544%\n",
      "Epoch 0, Batch 637, LR 0.000029 Loss 18.629221, Accuracy 0.546%\n",
      "Epoch 0, Batch 638, LR 0.000029 Loss 18.628279, Accuracy 0.547%\n",
      "Epoch 0, Batch 639, LR 0.000029 Loss 18.627295, Accuracy 0.549%\n",
      "Epoch 0, Batch 640, LR 0.000029 Loss 18.626486, Accuracy 0.549%\n",
      "Epoch 0, Batch 641, LR 0.000029 Loss 18.625509, Accuracy 0.550%\n",
      "Epoch 0, Batch 642, LR 0.000029 Loss 18.624784, Accuracy 0.549%\n",
      "Epoch 0, Batch 643, LR 0.000029 Loss 18.623415, Accuracy 0.553%\n",
      "Epoch 0, Batch 644, LR 0.000029 Loss 18.622776, Accuracy 0.554%\n",
      "Epoch 0, Batch 645, LR 0.000029 Loss 18.621829, Accuracy 0.558%\n",
      "Epoch 0, Batch 646, LR 0.000029 Loss 18.620585, Accuracy 0.567%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 647, LR 0.000030 Loss 18.619822, Accuracy 0.566%\n",
      "Epoch 0, Batch 648, LR 0.000030 Loss 18.618985, Accuracy 0.568%\n",
      "Epoch 0, Batch 649, LR 0.000030 Loss 18.617825, Accuracy 0.572%\n",
      "Epoch 0, Batch 650, LR 0.000030 Loss 18.616769, Accuracy 0.573%\n",
      "Epoch 0, Batch 651, LR 0.000030 Loss 18.615954, Accuracy 0.576%\n",
      "Epoch 0, Batch 652, LR 0.000030 Loss 18.614713, Accuracy 0.581%\n",
      "Epoch 0, Batch 653, LR 0.000030 Loss 18.613501, Accuracy 0.583%\n",
      "Epoch 0, Batch 654, LR 0.000030 Loss 18.612537, Accuracy 0.587%\n",
      "Epoch 0, Batch 655, LR 0.000030 Loss 18.611526, Accuracy 0.587%\n",
      "Epoch 0, Batch 656, LR 0.000030 Loss 18.610353, Accuracy 0.592%\n",
      "Epoch 0, Batch 657, LR 0.000030 Loss 18.609286, Accuracy 0.592%\n",
      "Epoch 0, Batch 658, LR 0.000030 Loss 18.608151, Accuracy 0.594%\n",
      "Epoch 0, Batch 659, LR 0.000030 Loss 18.607223, Accuracy 0.600%\n",
      "Epoch 0, Batch 660, LR 0.000030 Loss 18.606431, Accuracy 0.600%\n",
      "Epoch 0, Batch 661, LR 0.000030 Loss 18.605440, Accuracy 0.604%\n",
      "Epoch 0, Batch 662, LR 0.000030 Loss 18.604383, Accuracy 0.607%\n",
      "Epoch 0, Batch 663, LR 0.000030 Loss 18.603575, Accuracy 0.608%\n",
      "Epoch 0, Batch 664, LR 0.000031 Loss 18.602764, Accuracy 0.608%\n",
      "Epoch 0, Batch 665, LR 0.000031 Loss 18.601824, Accuracy 0.610%\n",
      "Epoch 0, Batch 666, LR 0.000031 Loss 18.600742, Accuracy 0.611%\n",
      "Epoch 0, Batch 667, LR 0.000031 Loss 18.599893, Accuracy 0.613%\n",
      "Epoch 0, Batch 668, LR 0.000031 Loss 18.599042, Accuracy 0.618%\n",
      "Epoch 0, Batch 669, LR 0.000031 Loss 18.598214, Accuracy 0.617%\n",
      "Epoch 0, Batch 670, LR 0.000031 Loss 18.596818, Accuracy 0.622%\n",
      "Epoch 0, Batch 671, LR 0.000031 Loss 18.596033, Accuracy 0.622%\n",
      "Epoch 0, Batch 672, LR 0.000031 Loss 18.595113, Accuracy 0.622%\n",
      "Epoch 0, Batch 673, LR 0.000031 Loss 18.594156, Accuracy 0.622%\n",
      "Epoch 0, Batch 674, LR 0.000031 Loss 18.593195, Accuracy 0.622%\n",
      "Epoch 0, Batch 675, LR 0.000031 Loss 18.592045, Accuracy 0.626%\n",
      "Epoch 0, Batch 676, LR 0.000031 Loss 18.591121, Accuracy 0.628%\n",
      "Epoch 0, Batch 677, LR 0.000031 Loss 18.589673, Accuracy 0.635%\n",
      "Epoch 0, Batch 678, LR 0.000031 Loss 18.588540, Accuracy 0.638%\n",
      "Epoch 0, Batch 679, LR 0.000031 Loss 18.587673, Accuracy 0.641%\n",
      "Epoch 0, Batch 680, LR 0.000031 Loss 18.586409, Accuracy 0.645%\n",
      "Epoch 0, Batch 681, LR 0.000031 Loss 18.585535, Accuracy 0.647%\n",
      "Epoch 0, Batch 682, LR 0.000032 Loss 18.584750, Accuracy 0.647%\n",
      "Epoch 0, Batch 683, LR 0.000032 Loss 18.583558, Accuracy 0.647%\n",
      "Epoch 0, Batch 684, LR 0.000032 Loss 18.582992, Accuracy 0.648%\n",
      "Epoch 0, Batch 685, LR 0.000032 Loss 18.582195, Accuracy 0.649%\n",
      "Epoch 0, Batch 686, LR 0.000032 Loss 18.581231, Accuracy 0.653%\n",
      "Epoch 0, Batch 687, LR 0.000032 Loss 18.580090, Accuracy 0.654%\n",
      "Epoch 0, Batch 688, LR 0.000032 Loss 18.579028, Accuracy 0.657%\n",
      "Epoch 0, Batch 689, LR 0.000032 Loss 18.578015, Accuracy 0.662%\n",
      "Epoch 0, Batch 690, LR 0.000032 Loss 18.577372, Accuracy 0.662%\n",
      "Epoch 0, Batch 691, LR 0.000032 Loss 18.576820, Accuracy 0.663%\n",
      "Epoch 0, Batch 692, LR 0.000032 Loss 18.575903, Accuracy 0.664%\n",
      "Epoch 0, Batch 693, LR 0.000032 Loss 18.574972, Accuracy 0.666%\n",
      "Epoch 0, Batch 694, LR 0.000032 Loss 18.573716, Accuracy 0.671%\n",
      "Epoch 0, Batch 695, LR 0.000032 Loss 18.572663, Accuracy 0.673%\n",
      "Epoch 0, Batch 696, LR 0.000032 Loss 18.572141, Accuracy 0.675%\n",
      "Epoch 0, Batch 697, LR 0.000032 Loss 18.571219, Accuracy 0.679%\n",
      "Epoch 0, Batch 698, LR 0.000032 Loss 18.570344, Accuracy 0.682%\n",
      "Epoch 0, Batch 699, LR 0.000033 Loss 18.569620, Accuracy 0.686%\n",
      "Epoch 0, Batch 700, LR 0.000033 Loss 18.568854, Accuracy 0.688%\n",
      "Epoch 0, Batch 701, LR 0.000033 Loss 18.567982, Accuracy 0.688%\n",
      "Epoch 0, Batch 702, LR 0.000033 Loss 18.567212, Accuracy 0.691%\n",
      "Epoch 0, Batch 703, LR 0.000033 Loss 18.566351, Accuracy 0.695%\n",
      "Epoch 0, Batch 704, LR 0.000033 Loss 18.565671, Accuracy 0.696%\n",
      "Epoch 0, Batch 705, LR 0.000033 Loss 18.564563, Accuracy 0.698%\n",
      "Epoch 0, Batch 706, LR 0.000033 Loss 18.563792, Accuracy 0.699%\n",
      "Epoch 0, Batch 707, LR 0.000033 Loss 18.563018, Accuracy 0.701%\n",
      "Epoch 0, Batch 708, LR 0.000033 Loss 18.562013, Accuracy 0.706%\n",
      "Epoch 0, Batch 709, LR 0.000033 Loss 18.561272, Accuracy 0.707%\n",
      "Epoch 0, Batch 710, LR 0.000033 Loss 18.560465, Accuracy 0.711%\n",
      "Epoch 0, Batch 711, LR 0.000033 Loss 18.559774, Accuracy 0.712%\n",
      "Epoch 0, Batch 712, LR 0.000033 Loss 18.558669, Accuracy 0.713%\n",
      "Epoch 0, Batch 713, LR 0.000033 Loss 18.557908, Accuracy 0.714%\n",
      "Epoch 0, Batch 714, LR 0.000033 Loss 18.556892, Accuracy 0.713%\n",
      "Epoch 0, Batch 715, LR 0.000033 Loss 18.556123, Accuracy 0.716%\n",
      "Epoch 0, Batch 716, LR 0.000034 Loss 18.555254, Accuracy 0.721%\n",
      "Epoch 0, Batch 717, LR 0.000034 Loss 18.554396, Accuracy 0.721%\n",
      "Epoch 0, Batch 718, LR 0.000034 Loss 18.553677, Accuracy 0.725%\n",
      "Epoch 0, Batch 719, LR 0.000034 Loss 18.553009, Accuracy 0.729%\n",
      "Epoch 0, Batch 720, LR 0.000034 Loss 18.552200, Accuracy 0.730%\n",
      "Epoch 0, Batch 721, LR 0.000034 Loss 18.551223, Accuracy 0.732%\n",
      "Epoch 0, Batch 722, LR 0.000034 Loss 18.550265, Accuracy 0.734%\n",
      "Epoch 0, Batch 723, LR 0.000034 Loss 18.549209, Accuracy 0.736%\n",
      "Epoch 0, Batch 724, LR 0.000034 Loss 18.548458, Accuracy 0.737%\n",
      "Epoch 0, Batch 725, LR 0.000034 Loss 18.547587, Accuracy 0.738%\n",
      "Epoch 0, Batch 726, LR 0.000034 Loss 18.546603, Accuracy 0.740%\n",
      "Epoch 0, Batch 727, LR 0.000034 Loss 18.545719, Accuracy 0.746%\n",
      "Epoch 0, Batch 728, LR 0.000034 Loss 18.544779, Accuracy 0.747%\n",
      "Epoch 0, Batch 729, LR 0.000034 Loss 18.543882, Accuracy 0.748%\n",
      "Epoch 0, Batch 730, LR 0.000034 Loss 18.542631, Accuracy 0.752%\n",
      "Epoch 0, Batch 731, LR 0.000034 Loss 18.541624, Accuracy 0.752%\n",
      "Epoch 0, Batch 732, LR 0.000034 Loss 18.540715, Accuracy 0.759%\n",
      "Epoch 0, Batch 733, LR 0.000035 Loss 18.539325, Accuracy 0.764%\n",
      "Epoch 0, Batch 734, LR 0.000035 Loss 18.538784, Accuracy 0.765%\n",
      "Epoch 0, Batch 735, LR 0.000035 Loss 18.537983, Accuracy 0.768%\n",
      "Epoch 0, Batch 736, LR 0.000035 Loss 18.537015, Accuracy 0.770%\n",
      "Epoch 0, Batch 737, LR 0.000035 Loss 18.536268, Accuracy 0.773%\n",
      "Epoch 0, Batch 738, LR 0.000035 Loss 18.535392, Accuracy 0.775%\n",
      "Epoch 0, Batch 739, LR 0.000035 Loss 18.534587, Accuracy 0.775%\n",
      "Epoch 0, Batch 740, LR 0.000035 Loss 18.533647, Accuracy 0.776%\n",
      "Epoch 0, Batch 741, LR 0.000035 Loss 18.532778, Accuracy 0.778%\n",
      "Epoch 0, Batch 742, LR 0.000035 Loss 18.531857, Accuracy 0.780%\n",
      "Epoch 0, Batch 743, LR 0.000035 Loss 18.530932, Accuracy 0.782%\n",
      "Epoch 0, Batch 744, LR 0.000035 Loss 18.530232, Accuracy 0.784%\n",
      "Epoch 0, Batch 745, LR 0.000035 Loss 18.529401, Accuracy 0.786%\n",
      "Epoch 0, Batch 746, LR 0.000035 Loss 18.528246, Accuracy 0.790%\n",
      "Epoch 0, Batch 747, LR 0.000035 Loss 18.527181, Accuracy 0.791%\n",
      "Epoch 0, Batch 748, LR 0.000035 Loss 18.526347, Accuracy 0.791%\n",
      "Epoch 0, Batch 749, LR 0.000036 Loss 18.525320, Accuracy 0.796%\n",
      "Epoch 0, Batch 750, LR 0.000036 Loss 18.524533, Accuracy 0.797%\n",
      "Epoch 0, Batch 751, LR 0.000036 Loss 18.523574, Accuracy 0.797%\n",
      "Epoch 0, Batch 752, LR 0.000036 Loss 18.522596, Accuracy 0.801%\n",
      "Epoch 0, Batch 753, LR 0.000036 Loss 18.521703, Accuracy 0.806%\n",
      "Epoch 0, Batch 754, LR 0.000036 Loss 18.520840, Accuracy 0.806%\n",
      "Epoch 0, Batch 755, LR 0.000036 Loss 18.519836, Accuracy 0.808%\n",
      "Epoch 0, Batch 756, LR 0.000036 Loss 18.518863, Accuracy 0.811%\n",
      "Epoch 0, Batch 757, LR 0.000036 Loss 18.518043, Accuracy 0.811%\n",
      "Epoch 0, Batch 758, LR 0.000036 Loss 18.517208, Accuracy 0.810%\n",
      "Epoch 0, Batch 759, LR 0.000036 Loss 18.516106, Accuracy 0.815%\n",
      "Epoch 0, Batch 760, LR 0.000036 Loss 18.515464, Accuracy 0.816%\n",
      "Epoch 0, Batch 761, LR 0.000036 Loss 18.514354, Accuracy 0.819%\n",
      "Epoch 0, Batch 762, LR 0.000036 Loss 18.513265, Accuracy 0.824%\n",
      "Epoch 0, Batch 763, LR 0.000036 Loss 18.512599, Accuracy 0.824%\n",
      "Epoch 0, Batch 764, LR 0.000036 Loss 18.511705, Accuracy 0.825%\n",
      "Epoch 0, Batch 765, LR 0.000036 Loss 18.510764, Accuracy 0.827%\n",
      "Epoch 0, Batch 766, LR 0.000037 Loss 18.509730, Accuracy 0.830%\n",
      "Epoch 0, Batch 767, LR 0.000037 Loss 18.509057, Accuracy 0.832%\n",
      "Epoch 0, Batch 768, LR 0.000037 Loss 18.508259, Accuracy 0.832%\n",
      "Epoch 0, Batch 769, LR 0.000037 Loss 18.507441, Accuracy 0.833%\n",
      "Epoch 0, Batch 770, LR 0.000037 Loss 18.506398, Accuracy 0.836%\n",
      "Epoch 0, Batch 771, LR 0.000037 Loss 18.505701, Accuracy 0.839%\n",
      "Epoch 0, Batch 772, LR 0.000037 Loss 18.504987, Accuracy 0.840%\n",
      "Epoch 0, Batch 773, LR 0.000037 Loss 18.504069, Accuracy 0.841%\n",
      "Epoch 0, Batch 774, LR 0.000037 Loss 18.503107, Accuracy 0.844%\n",
      "Epoch 0, Batch 775, LR 0.000037 Loss 18.502112, Accuracy 0.847%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 776, LR 0.000037 Loss 18.501370, Accuracy 0.848%\n",
      "Epoch 0, Batch 777, LR 0.000037 Loss 18.500645, Accuracy 0.848%\n",
      "Epoch 0, Batch 778, LR 0.000037 Loss 18.500001, Accuracy 0.847%\n",
      "Epoch 0, Batch 779, LR 0.000037 Loss 18.498953, Accuracy 0.850%\n",
      "Epoch 0, Batch 780, LR 0.000037 Loss 18.498075, Accuracy 0.851%\n",
      "Epoch 0, Batch 781, LR 0.000037 Loss 18.497386, Accuracy 0.855%\n",
      "Epoch 0, Batch 782, LR 0.000038 Loss 18.496648, Accuracy 0.857%\n",
      "Epoch 0, Batch 783, LR 0.000038 Loss 18.495848, Accuracy 0.859%\n",
      "Epoch 0, Batch 784, LR 0.000038 Loss 18.495049, Accuracy 0.862%\n",
      "Epoch 0, Batch 785, LR 0.000038 Loss 18.494279, Accuracy 0.862%\n",
      "Epoch 0, Batch 786, LR 0.000038 Loss 18.493280, Accuracy 0.866%\n",
      "Epoch 0, Batch 787, LR 0.000038 Loss 18.492302, Accuracy 0.871%\n",
      "Epoch 0, Batch 788, LR 0.000038 Loss 18.491460, Accuracy 0.872%\n",
      "Epoch 0, Batch 789, LR 0.000038 Loss 18.490282, Accuracy 0.876%\n",
      "Epoch 0, Batch 790, LR 0.000038 Loss 18.489427, Accuracy 0.881%\n",
      "Epoch 0, Batch 791, LR 0.000038 Loss 18.488716, Accuracy 0.883%\n",
      "Epoch 0, Batch 792, LR 0.000038 Loss 18.487714, Accuracy 0.886%\n",
      "Epoch 0, Batch 793, LR 0.000038 Loss 18.486531, Accuracy 0.890%\n",
      "Epoch 0, Batch 794, LR 0.000038 Loss 18.485828, Accuracy 0.890%\n",
      "Epoch 0, Batch 795, LR 0.000038 Loss 18.484922, Accuracy 0.890%\n",
      "Epoch 0, Batch 796, LR 0.000038 Loss 18.483999, Accuracy 0.891%\n",
      "Epoch 0, Batch 797, LR 0.000038 Loss 18.483136, Accuracy 0.897%\n",
      "Epoch 0, Batch 798, LR 0.000039 Loss 18.482146, Accuracy 0.898%\n",
      "Epoch 0, Batch 799, LR 0.000039 Loss 18.481299, Accuracy 0.901%\n",
      "Epoch 0, Batch 800, LR 0.000039 Loss 18.480388, Accuracy 0.904%\n",
      "Epoch 0, Batch 801, LR 0.000039 Loss 18.479447, Accuracy 0.907%\n",
      "Epoch 0, Batch 802, LR 0.000039 Loss 18.478449, Accuracy 0.911%\n",
      "Epoch 0, Batch 803, LR 0.000039 Loss 18.477526, Accuracy 0.915%\n",
      "Epoch 0, Batch 804, LR 0.000039 Loss 18.476758, Accuracy 0.913%\n",
      "Epoch 0, Batch 805, LR 0.000039 Loss 18.476017, Accuracy 0.914%\n",
      "Epoch 0, Batch 806, LR 0.000039 Loss 18.475103, Accuracy 0.916%\n",
      "Epoch 0, Batch 807, LR 0.000039 Loss 18.474281, Accuracy 0.919%\n",
      "Epoch 0, Batch 808, LR 0.000039 Loss 18.473413, Accuracy 0.920%\n",
      "Epoch 0, Batch 809, LR 0.000039 Loss 18.472487, Accuracy 0.921%\n",
      "Epoch 0, Batch 810, LR 0.000039 Loss 18.471453, Accuracy 0.928%\n",
      "Epoch 0, Batch 811, LR 0.000039 Loss 18.470362, Accuracy 0.934%\n",
      "Epoch 0, Batch 812, LR 0.000039 Loss 18.469302, Accuracy 0.935%\n",
      "Epoch 0, Batch 813, LR 0.000039 Loss 18.468378, Accuracy 0.941%\n",
      "Epoch 0, Batch 814, LR 0.000040 Loss 18.467502, Accuracy 0.942%\n",
      "Epoch 0, Batch 815, LR 0.000040 Loss 18.466801, Accuracy 0.944%\n",
      "Epoch 0, Batch 816, LR 0.000040 Loss 18.465853, Accuracy 0.950%\n",
      "Epoch 0, Batch 817, LR 0.000040 Loss 18.464687, Accuracy 0.953%\n",
      "Epoch 0, Batch 818, LR 0.000040 Loss 18.463608, Accuracy 0.959%\n",
      "Epoch 0, Batch 819, LR 0.000040 Loss 18.462788, Accuracy 0.963%\n",
      "Epoch 0, Batch 820, LR 0.000040 Loss 18.461645, Accuracy 0.965%\n",
      "Epoch 0, Batch 821, LR 0.000040 Loss 18.460772, Accuracy 0.967%\n",
      "Epoch 0, Batch 822, LR 0.000040 Loss 18.459931, Accuracy 0.969%\n",
      "Epoch 0, Batch 823, LR 0.000040 Loss 18.459272, Accuracy 0.969%\n",
      "Epoch 0, Batch 824, LR 0.000040 Loss 18.458439, Accuracy 0.971%\n",
      "Epoch 0, Batch 825, LR 0.000040 Loss 18.457422, Accuracy 0.975%\n",
      "Epoch 0, Batch 826, LR 0.000040 Loss 18.456503, Accuracy 0.977%\n",
      "Epoch 0, Batch 827, LR 0.000040 Loss 18.455654, Accuracy 0.978%\n",
      "Epoch 0, Batch 828, LR 0.000040 Loss 18.454678, Accuracy 0.978%\n",
      "Epoch 0, Batch 829, LR 0.000041 Loss 18.453944, Accuracy 0.981%\n",
      "Epoch 0, Batch 830, LR 0.000041 Loss 18.453215, Accuracy 0.983%\n",
      "Epoch 0, Batch 831, LR 0.000041 Loss 18.452338, Accuracy 0.984%\n",
      "Epoch 0, Batch 832, LR 0.000041 Loss 18.451492, Accuracy 0.985%\n",
      "Epoch 0, Batch 833, LR 0.000041 Loss 18.450466, Accuracy 0.988%\n",
      "Epoch 0, Batch 834, LR 0.000041 Loss 18.449418, Accuracy 0.990%\n",
      "Epoch 0, Batch 835, LR 0.000041 Loss 18.448422, Accuracy 0.992%\n",
      "Epoch 0, Batch 836, LR 0.000041 Loss 18.447915, Accuracy 0.993%\n",
      "Epoch 0, Batch 837, LR 0.000041 Loss 18.447188, Accuracy 0.995%\n",
      "Epoch 0, Batch 838, LR 0.000041 Loss 18.446275, Accuracy 0.997%\n",
      "Epoch 0, Batch 839, LR 0.000041 Loss 18.445549, Accuracy 0.997%\n",
      "Epoch 0, Batch 840, LR 0.000041 Loss 18.444663, Accuracy 0.999%\n",
      "Epoch 0, Batch 841, LR 0.000041 Loss 18.443932, Accuracy 0.999%\n",
      "Epoch 0, Batch 842, LR 0.000041 Loss 18.443036, Accuracy 1.004%\n",
      "Epoch 0, Batch 843, LR 0.000041 Loss 18.442446, Accuracy 1.005%\n",
      "Epoch 0, Batch 844, LR 0.000041 Loss 18.441569, Accuracy 1.006%\n",
      "Epoch 0, Batch 845, LR 0.000042 Loss 18.440558, Accuracy 1.011%\n",
      "Epoch 0, Batch 846, LR 0.000042 Loss 18.439590, Accuracy 1.013%\n",
      "Epoch 0, Batch 847, LR 0.000042 Loss 18.438675, Accuracy 1.018%\n",
      "Epoch 0, Batch 848, LR 0.000042 Loss 18.437839, Accuracy 1.023%\n",
      "Epoch 0, Batch 849, LR 0.000042 Loss 18.436735, Accuracy 1.024%\n",
      "Epoch 0, Batch 850, LR 0.000042 Loss 18.435502, Accuracy 1.029%\n",
      "Epoch 0, Batch 851, LR 0.000042 Loss 18.434280, Accuracy 1.036%\n",
      "Epoch 0, Batch 852, LR 0.000042 Loss 18.433494, Accuracy 1.040%\n",
      "Epoch 0, Batch 853, LR 0.000042 Loss 18.432881, Accuracy 1.040%\n",
      "Epoch 0, Batch 854, LR 0.000042 Loss 18.431925, Accuracy 1.042%\n",
      "Epoch 0, Batch 855, LR 0.000042 Loss 18.431195, Accuracy 1.043%\n",
      "Epoch 0, Batch 856, LR 0.000042 Loss 18.430434, Accuracy 1.045%\n",
      "Epoch 0, Batch 857, LR 0.000042 Loss 18.429426, Accuracy 1.047%\n",
      "Epoch 0, Batch 858, LR 0.000042 Loss 18.428664, Accuracy 1.051%\n",
      "Epoch 0, Batch 859, LR 0.000042 Loss 18.427528, Accuracy 1.056%\n",
      "Epoch 0, Batch 860, LR 0.000043 Loss 18.426317, Accuracy 1.058%\n",
      "Epoch 0, Batch 861, LR 0.000043 Loss 18.425290, Accuracy 1.063%\n",
      "Epoch 0, Batch 862, LR 0.000043 Loss 18.424556, Accuracy 1.063%\n",
      "Epoch 0, Batch 863, LR 0.000043 Loss 18.423816, Accuracy 1.066%\n",
      "Epoch 0, Batch 864, LR 0.000043 Loss 18.423072, Accuracy 1.070%\n",
      "Epoch 0, Batch 865, LR 0.000043 Loss 18.422158, Accuracy 1.070%\n",
      "Epoch 0, Batch 866, LR 0.000043 Loss 18.420998, Accuracy 1.074%\n",
      "Epoch 0, Batch 867, LR 0.000043 Loss 18.420403, Accuracy 1.075%\n",
      "Epoch 0, Batch 868, LR 0.000043 Loss 18.419584, Accuracy 1.076%\n",
      "Epoch 0, Batch 869, LR 0.000043 Loss 18.418844, Accuracy 1.078%\n",
      "Epoch 0, Batch 870, LR 0.000043 Loss 18.417768, Accuracy 1.083%\n",
      "Epoch 0, Batch 871, LR 0.000043 Loss 18.416791, Accuracy 1.084%\n",
      "Epoch 0, Batch 872, LR 0.000043 Loss 18.415727, Accuracy 1.089%\n",
      "Epoch 0, Batch 873, LR 0.000043 Loss 18.414932, Accuracy 1.090%\n",
      "Epoch 0, Batch 874, LR 0.000043 Loss 18.413833, Accuracy 1.095%\n",
      "Epoch 0, Batch 875, LR 0.000043 Loss 18.413017, Accuracy 1.095%\n",
      "Epoch 0, Batch 876, LR 0.000044 Loss 18.412123, Accuracy 1.098%\n",
      "Epoch 0, Batch 877, LR 0.000044 Loss 18.411171, Accuracy 1.100%\n",
      "Epoch 0, Batch 878, LR 0.000044 Loss 18.410235, Accuracy 1.105%\n",
      "Epoch 0, Batch 879, LR 0.000044 Loss 18.409311, Accuracy 1.105%\n",
      "Epoch 0, Batch 880, LR 0.000044 Loss 18.408195, Accuracy 1.107%\n",
      "Epoch 0, Batch 881, LR 0.000044 Loss 18.407345, Accuracy 1.111%\n",
      "Epoch 0, Batch 882, LR 0.000044 Loss 18.406536, Accuracy 1.110%\n",
      "Epoch 0, Batch 883, LR 0.000044 Loss 18.405628, Accuracy 1.113%\n",
      "Epoch 0, Batch 884, LR 0.000044 Loss 18.404763, Accuracy 1.114%\n",
      "Epoch 0, Batch 885, LR 0.000044 Loss 18.403646, Accuracy 1.117%\n",
      "Epoch 0, Batch 886, LR 0.000044 Loss 18.402603, Accuracy 1.118%\n",
      "Epoch 0, Batch 887, LR 0.000044 Loss 18.401976, Accuracy 1.119%\n",
      "Epoch 0, Batch 888, LR 0.000044 Loss 18.401371, Accuracy 1.122%\n",
      "Epoch 0, Batch 889, LR 0.000044 Loss 18.400513, Accuracy 1.126%\n",
      "Epoch 0, Batch 890, LR 0.000044 Loss 18.399498, Accuracy 1.130%\n",
      "Epoch 0, Batch 891, LR 0.000045 Loss 18.398904, Accuracy 1.133%\n",
      "Epoch 0, Batch 892, LR 0.000045 Loss 18.398143, Accuracy 1.133%\n",
      "Epoch 0, Batch 893, LR 0.000045 Loss 18.397122, Accuracy 1.137%\n",
      "Epoch 0, Batch 894, LR 0.000045 Loss 18.396068, Accuracy 1.140%\n",
      "Epoch 0, Batch 895, LR 0.000045 Loss 18.395011, Accuracy 1.146%\n",
      "Epoch 0, Batch 896, LR 0.000045 Loss 18.394210, Accuracy 1.148%\n",
      "Epoch 0, Batch 897, LR 0.000045 Loss 18.393437, Accuracy 1.151%\n",
      "Epoch 0, Batch 898, LR 0.000045 Loss 18.392377, Accuracy 1.156%\n",
      "Epoch 0, Batch 899, LR 0.000045 Loss 18.391672, Accuracy 1.158%\n",
      "Epoch 0, Batch 900, LR 0.000045 Loss 18.390736, Accuracy 1.162%\n",
      "Epoch 0, Batch 901, LR 0.000045 Loss 18.389838, Accuracy 1.163%\n",
      "Epoch 0, Batch 902, LR 0.000045 Loss 18.389090, Accuracy 1.164%\n",
      "Epoch 0, Batch 903, LR 0.000045 Loss 18.388320, Accuracy 1.168%\n",
      "Epoch 0, Batch 904, LR 0.000045 Loss 18.387432, Accuracy 1.169%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 905, LR 0.000045 Loss 18.386512, Accuracy 1.174%\n",
      "Epoch 0, Batch 906, LR 0.000046 Loss 18.385547, Accuracy 1.178%\n",
      "Epoch 0, Batch 907, LR 0.000046 Loss 18.384835, Accuracy 1.179%\n",
      "Epoch 0, Batch 908, LR 0.000046 Loss 18.384005, Accuracy 1.181%\n",
      "Epoch 0, Batch 909, LR 0.000046 Loss 18.383183, Accuracy 1.184%\n",
      "Epoch 0, Batch 910, LR 0.000046 Loss 18.382320, Accuracy 1.184%\n",
      "Epoch 0, Batch 911, LR 0.000046 Loss 18.381468, Accuracy 1.186%\n",
      "Epoch 0, Batch 912, LR 0.000046 Loss 18.380574, Accuracy 1.188%\n",
      "Epoch 0, Batch 913, LR 0.000046 Loss 18.379667, Accuracy 1.193%\n",
      "Epoch 0, Batch 914, LR 0.000046 Loss 18.378899, Accuracy 1.193%\n",
      "Epoch 0, Batch 915, LR 0.000046 Loss 18.378036, Accuracy 1.194%\n",
      "Epoch 0, Batch 916, LR 0.000046 Loss 18.377134, Accuracy 1.199%\n",
      "Epoch 0, Batch 917, LR 0.000046 Loss 18.376246, Accuracy 1.204%\n",
      "Epoch 0, Batch 918, LR 0.000046 Loss 18.375466, Accuracy 1.206%\n",
      "Epoch 0, Batch 919, LR 0.000046 Loss 18.374393, Accuracy 1.211%\n",
      "Epoch 0, Batch 920, LR 0.000046 Loss 18.373503, Accuracy 1.210%\n",
      "Epoch 0, Batch 921, LR 0.000047 Loss 18.372775, Accuracy 1.210%\n",
      "Epoch 0, Batch 922, LR 0.000047 Loss 18.372074, Accuracy 1.213%\n",
      "Epoch 0, Batch 923, LR 0.000047 Loss 18.371186, Accuracy 1.215%\n",
      "Epoch 0, Batch 924, LR 0.000047 Loss 18.370180, Accuracy 1.217%\n",
      "Epoch 0, Batch 925, LR 0.000047 Loss 18.369432, Accuracy 1.219%\n",
      "Epoch 0, Batch 926, LR 0.000047 Loss 18.368474, Accuracy 1.221%\n",
      "Epoch 0, Batch 927, LR 0.000047 Loss 18.367555, Accuracy 1.225%\n",
      "Epoch 0, Batch 928, LR 0.000047 Loss 18.366732, Accuracy 1.230%\n",
      "Epoch 0, Batch 929, LR 0.000047 Loss 18.365969, Accuracy 1.235%\n",
      "Epoch 0, Batch 930, LR 0.000047 Loss 18.365099, Accuracy 1.237%\n",
      "Epoch 0, Batch 931, LR 0.000047 Loss 18.364104, Accuracy 1.237%\n",
      "Epoch 0, Batch 932, LR 0.000047 Loss 18.363040, Accuracy 1.241%\n",
      "Epoch 0, Batch 933, LR 0.000047 Loss 18.361873, Accuracy 1.245%\n",
      "Epoch 0, Batch 934, LR 0.000047 Loss 18.361231, Accuracy 1.245%\n",
      "Epoch 0, Batch 935, LR 0.000047 Loss 18.360074, Accuracy 1.250%\n",
      "Epoch 0, Batch 936, LR 0.000048 Loss 18.359365, Accuracy 1.251%\n",
      "Epoch 0, Batch 937, LR 0.000048 Loss 18.358281, Accuracy 1.252%\n",
      "Epoch 0, Batch 938, LR 0.000048 Loss 18.357482, Accuracy 1.257%\n",
      "Epoch 0, Batch 939, LR 0.000048 Loss 18.356398, Accuracy 1.260%\n",
      "Epoch 0, Batch 940, LR 0.000048 Loss 18.355435, Accuracy 1.265%\n",
      "Epoch 0, Batch 941, LR 0.000048 Loss 18.354653, Accuracy 1.266%\n",
      "Epoch 0, Batch 942, LR 0.000048 Loss 18.353679, Accuracy 1.270%\n",
      "Epoch 0, Batch 943, LR 0.000048 Loss 18.352846, Accuracy 1.273%\n",
      "Epoch 0, Batch 944, LR 0.000048 Loss 18.351885, Accuracy 1.280%\n",
      "Epoch 0, Batch 945, LR 0.000048 Loss 18.351002, Accuracy 1.282%\n",
      "Epoch 0, Batch 946, LR 0.000048 Loss 18.350257, Accuracy 1.284%\n",
      "Epoch 0, Batch 947, LR 0.000048 Loss 18.349553, Accuracy 1.286%\n",
      "Epoch 0, Batch 948, LR 0.000048 Loss 18.348537, Accuracy 1.291%\n",
      "Epoch 0, Batch 949, LR 0.000048 Loss 18.347825, Accuracy 1.293%\n",
      "Epoch 0, Batch 950, LR 0.000048 Loss 18.346952, Accuracy 1.297%\n",
      "Epoch 0, Batch 951, LR 0.000049 Loss 18.345968, Accuracy 1.303%\n",
      "Epoch 0, Batch 952, LR 0.000049 Loss 18.345264, Accuracy 1.304%\n",
      "Epoch 0, Batch 953, LR 0.000049 Loss 18.344428, Accuracy 1.309%\n",
      "Epoch 0, Batch 954, LR 0.000049 Loss 18.343575, Accuracy 1.313%\n",
      "Epoch 0, Batch 955, LR 0.000049 Loss 18.342706, Accuracy 1.318%\n",
      "Epoch 0, Batch 956, LR 0.000049 Loss 18.341709, Accuracy 1.328%\n",
      "Epoch 0, Batch 957, LR 0.000049 Loss 18.340999, Accuracy 1.331%\n",
      "Epoch 0, Batch 958, LR 0.000049 Loss 18.340152, Accuracy 1.333%\n",
      "Epoch 0, Batch 959, LR 0.000049 Loss 18.339176, Accuracy 1.337%\n",
      "Epoch 0, Batch 960, LR 0.000049 Loss 18.338451, Accuracy 1.340%\n",
      "Epoch 0, Batch 961, LR 0.000049 Loss 18.337460, Accuracy 1.344%\n",
      "Epoch 0, Batch 962, LR 0.000049 Loss 18.336761, Accuracy 1.345%\n",
      "Epoch 0, Batch 963, LR 0.000049 Loss 18.336186, Accuracy 1.346%\n",
      "Epoch 0, Batch 964, LR 0.000049 Loss 18.335271, Accuracy 1.349%\n",
      "Epoch 0, Batch 965, LR 0.000049 Loss 18.334331, Accuracy 1.353%\n",
      "Epoch 0, Batch 966, LR 0.000050 Loss 18.333492, Accuracy 1.357%\n",
      "Epoch 0, Batch 967, LR 0.000050 Loss 18.332758, Accuracy 1.359%\n",
      "Epoch 0, Batch 968, LR 0.000050 Loss 18.332099, Accuracy 1.359%\n",
      "Epoch 0, Batch 969, LR 0.000050 Loss 18.331312, Accuracy 1.363%\n",
      "Epoch 0, Batch 970, LR 0.000050 Loss 18.330290, Accuracy 1.368%\n",
      "Epoch 0, Batch 971, LR 0.000050 Loss 18.329309, Accuracy 1.369%\n",
      "Epoch 0, Batch 972, LR 0.000050 Loss 18.328414, Accuracy 1.374%\n",
      "Epoch 0, Batch 973, LR 0.000050 Loss 18.327505, Accuracy 1.375%\n",
      "Epoch 0, Batch 974, LR 0.000050 Loss 18.326961, Accuracy 1.377%\n",
      "Epoch 0, Batch 975, LR 0.000050 Loss 18.326230, Accuracy 1.379%\n",
      "Epoch 0, Batch 976, LR 0.000050 Loss 18.325144, Accuracy 1.382%\n",
      "Epoch 0, Batch 977, LR 0.000050 Loss 18.324368, Accuracy 1.384%\n",
      "Epoch 0, Batch 978, LR 0.000050 Loss 18.323177, Accuracy 1.393%\n",
      "Epoch 0, Batch 979, LR 0.000050 Loss 18.322396, Accuracy 1.396%\n",
      "Epoch 0, Batch 980, LR 0.000050 Loss 18.321739, Accuracy 1.397%\n",
      "Epoch 0, Batch 981, LR 0.000051 Loss 18.320838, Accuracy 1.402%\n",
      "Epoch 0, Batch 982, LR 0.000051 Loss 18.319773, Accuracy 1.405%\n",
      "Epoch 0, Batch 983, LR 0.000051 Loss 18.318621, Accuracy 1.411%\n",
      "Epoch 0, Batch 984, LR 0.000051 Loss 18.317770, Accuracy 1.412%\n",
      "Epoch 0, Batch 985, LR 0.000051 Loss 18.316933, Accuracy 1.413%\n",
      "Epoch 0, Batch 986, LR 0.000051 Loss 18.316179, Accuracy 1.415%\n",
      "Epoch 0, Batch 987, LR 0.000051 Loss 18.315447, Accuracy 1.418%\n",
      "Epoch 0, Batch 988, LR 0.000051 Loss 18.314625, Accuracy 1.418%\n",
      "Epoch 0, Batch 989, LR 0.000051 Loss 18.313821, Accuracy 1.421%\n",
      "Epoch 0, Batch 990, LR 0.000051 Loss 18.312990, Accuracy 1.426%\n",
      "Epoch 0, Batch 991, LR 0.000051 Loss 18.311937, Accuracy 1.427%\n",
      "Epoch 0, Batch 992, LR 0.000051 Loss 18.311374, Accuracy 1.427%\n",
      "Epoch 0, Batch 993, LR 0.000051 Loss 18.310406, Accuracy 1.433%\n",
      "Epoch 0, Batch 994, LR 0.000051 Loss 18.309946, Accuracy 1.435%\n",
      "Epoch 0, Batch 995, LR 0.000051 Loss 18.309317, Accuracy 1.437%\n",
      "Epoch 0, Batch 996, LR 0.000052 Loss 18.308413, Accuracy 1.440%\n",
      "Epoch 0, Batch 997, LR 0.000052 Loss 18.307628, Accuracy 1.446%\n",
      "Epoch 0, Batch 998, LR 0.000052 Loss 18.306874, Accuracy 1.448%\n",
      "Epoch 0, Batch 999, LR 0.000052 Loss 18.306098, Accuracy 1.449%\n",
      "Epoch 0, Batch 1000, LR 0.000052 Loss 18.305202, Accuracy 1.451%\n",
      "Epoch 0, Batch 1001, LR 0.000052 Loss 18.304559, Accuracy 1.451%\n",
      "Epoch 0, Batch 1002, LR 0.000052 Loss 18.303786, Accuracy 1.452%\n",
      "Epoch 0, Batch 1003, LR 0.000052 Loss 18.302993, Accuracy 1.455%\n",
      "Epoch 0, Batch 1004, LR 0.000052 Loss 18.302140, Accuracy 1.458%\n",
      "Epoch 0, Batch 1005, LR 0.000052 Loss 18.301075, Accuracy 1.462%\n",
      "Epoch 0, Batch 1006, LR 0.000052 Loss 18.300253, Accuracy 1.465%\n",
      "Epoch 0, Batch 1007, LR 0.000052 Loss 18.299251, Accuracy 1.469%\n",
      "Epoch 0, Batch 1008, LR 0.000052 Loss 18.298401, Accuracy 1.469%\n",
      "Epoch 0, Batch 1009, LR 0.000052 Loss 18.297567, Accuracy 1.472%\n",
      "Epoch 0, Batch 1010, LR 0.000052 Loss 18.296697, Accuracy 1.474%\n",
      "Epoch 0, Batch 1011, LR 0.000053 Loss 18.295959, Accuracy 1.477%\n",
      "Epoch 0, Batch 1012, LR 0.000053 Loss 18.295188, Accuracy 1.480%\n",
      "Epoch 0, Batch 1013, LR 0.000053 Loss 18.294402, Accuracy 1.482%\n",
      "Epoch 0, Batch 1014, LR 0.000053 Loss 18.293393, Accuracy 1.484%\n",
      "Epoch 0, Batch 1015, LR 0.000053 Loss 18.292736, Accuracy 1.484%\n",
      "Epoch 0, Batch 1016, LR 0.000053 Loss 18.291991, Accuracy 1.486%\n",
      "Epoch 0, Batch 1017, LR 0.000053 Loss 18.291187, Accuracy 1.489%\n",
      "Epoch 0, Batch 1018, LR 0.000053 Loss 18.290410, Accuracy 1.492%\n",
      "Epoch 0, Batch 1019, LR 0.000053 Loss 18.289560, Accuracy 1.494%\n",
      "Epoch 0, Batch 1020, LR 0.000053 Loss 18.288617, Accuracy 1.495%\n",
      "Epoch 0, Batch 1021, LR 0.000053 Loss 18.287660, Accuracy 1.502%\n",
      "Epoch 0, Batch 1022, LR 0.000053 Loss 18.286917, Accuracy 1.504%\n",
      "Epoch 0, Batch 1023, LR 0.000053 Loss 18.286020, Accuracy 1.508%\n",
      "Epoch 0, Batch 1024, LR 0.000053 Loss 18.285209, Accuracy 1.513%\n",
      "Epoch 0, Batch 1025, LR 0.000053 Loss 18.284464, Accuracy 1.516%\n",
      "Epoch 0, Batch 1026, LR 0.000054 Loss 18.283816, Accuracy 1.518%\n",
      "Epoch 0, Batch 1027, LR 0.000054 Loss 18.282844, Accuracy 1.521%\n",
      "Epoch 0, Batch 1028, LR 0.000054 Loss 18.281852, Accuracy 1.528%\n",
      "Epoch 0, Batch 1029, LR 0.000054 Loss 18.281038, Accuracy 1.532%\n",
      "Epoch 0, Batch 1030, LR 0.000054 Loss 18.280303, Accuracy 1.533%\n",
      "Epoch 0, Batch 1031, LR 0.000054 Loss 18.279222, Accuracy 1.541%\n",
      "Epoch 0, Batch 1032, LR 0.000054 Loss 18.278627, Accuracy 1.541%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 1033, LR 0.000054 Loss 18.278003, Accuracy 1.541%\n",
      "Epoch 0, Batch 1034, LR 0.000054 Loss 18.277187, Accuracy 1.544%\n",
      "Epoch 0, Batch 1035, LR 0.000054 Loss 18.276333, Accuracy 1.548%\n",
      "Epoch 0, Batch 1036, LR 0.000054 Loss 18.275238, Accuracy 1.552%\n",
      "Epoch 0, Batch 1037, LR 0.000054 Loss 18.274158, Accuracy 1.557%\n",
      "Epoch 0, Batch 1038, LR 0.000054 Loss 18.273299, Accuracy 1.559%\n",
      "Epoch 0, Batch 1039, LR 0.000054 Loss 18.272515, Accuracy 1.562%\n",
      "Epoch 0, Batch 1040, LR 0.000054 Loss 18.271600, Accuracy 1.567%\n",
      "Epoch 0, Batch 1041, LR 0.000055 Loss 18.270812, Accuracy 1.572%\n",
      "Epoch 0, Batch 1042, LR 0.000055 Loss 18.270214, Accuracy 1.572%\n",
      "Epoch 0, Batch 1043, LR 0.000055 Loss 18.269473, Accuracy 1.574%\n",
      "Epoch 0, Batch 1044, LR 0.000055 Loss 18.268705, Accuracy 1.577%\n",
      "Epoch 0, Batch 1045, LR 0.000055 Loss 18.267691, Accuracy 1.580%\n",
      "Epoch 0, Batch 1046, LR 0.000055 Loss 18.266918, Accuracy 1.583%\n",
      "Epoch 0, Batch 1047, LR 0.000055 Loss 18.265968, Accuracy 1.588%\n",
      "Epoch 0, Loss (train set) 18.265968, Accuracy (train set) 1.588%\n",
      "Epoch 1, Batch 1, LR 0.000055 Loss 17.297108, Accuracy 8.594%\n",
      "Epoch 1, Batch 2, LR 0.000055 Loss 17.343121, Accuracy 7.031%\n",
      "Epoch 1, Batch 3, LR 0.000055 Loss 17.342971, Accuracy 6.510%\n",
      "Epoch 1, Batch 4, LR 0.000055 Loss 17.369441, Accuracy 6.445%\n",
      "Epoch 1, Batch 5, LR 0.000055 Loss 17.372423, Accuracy 6.094%\n",
      "Epoch 1, Batch 6, LR 0.000055 Loss 17.396202, Accuracy 5.599%\n",
      "Epoch 1, Batch 7, LR 0.000055 Loss 17.389748, Accuracy 5.469%\n",
      "Epoch 1, Batch 8, LR 0.000056 Loss 17.391996, Accuracy 5.664%\n",
      "Epoch 1, Batch 9, LR 0.000056 Loss 17.384270, Accuracy 5.556%\n",
      "Epoch 1, Batch 10, LR 0.000056 Loss 17.392976, Accuracy 5.391%\n",
      "Epoch 1, Batch 11, LR 0.000056 Loss 17.386947, Accuracy 5.256%\n",
      "Epoch 1, Batch 12, LR 0.000056 Loss 17.405028, Accuracy 5.143%\n",
      "Epoch 1, Batch 13, LR 0.000056 Loss 17.422523, Accuracy 4.928%\n",
      "Epoch 1, Batch 14, LR 0.000056 Loss 17.435871, Accuracy 4.632%\n",
      "Epoch 1, Batch 15, LR 0.000056 Loss 17.425598, Accuracy 4.688%\n",
      "Epoch 1, Batch 16, LR 0.000056 Loss 17.431412, Accuracy 4.590%\n",
      "Epoch 1, Batch 17, LR 0.000056 Loss 17.420141, Accuracy 4.825%\n",
      "Epoch 1, Batch 18, LR 0.000056 Loss 17.421698, Accuracy 4.731%\n",
      "Epoch 1, Batch 19, LR 0.000056 Loss 17.419348, Accuracy 4.564%\n",
      "Epoch 1, Batch 20, LR 0.000056 Loss 17.422551, Accuracy 4.492%\n",
      "Epoch 1, Batch 21, LR 0.000056 Loss 17.421737, Accuracy 4.650%\n",
      "Epoch 1, Batch 22, LR 0.000056 Loss 17.420987, Accuracy 4.616%\n",
      "Epoch 1, Batch 23, LR 0.000057 Loss 17.421665, Accuracy 4.586%\n",
      "Epoch 1, Batch 24, LR 0.000057 Loss 17.411919, Accuracy 4.590%\n",
      "Epoch 1, Batch 25, LR 0.000057 Loss 17.414956, Accuracy 4.656%\n",
      "Epoch 1, Batch 26, LR 0.000057 Loss 17.409878, Accuracy 4.748%\n",
      "Epoch 1, Batch 27, LR 0.000057 Loss 17.408632, Accuracy 4.803%\n",
      "Epoch 1, Batch 28, LR 0.000057 Loss 17.404459, Accuracy 4.771%\n",
      "Epoch 1, Batch 29, LR 0.000057 Loss 17.399270, Accuracy 4.876%\n",
      "Epoch 1, Batch 30, LR 0.000057 Loss 17.399930, Accuracy 4.844%\n",
      "Epoch 1, Batch 31, LR 0.000057 Loss 17.393086, Accuracy 4.839%\n",
      "Epoch 1, Batch 32, LR 0.000057 Loss 17.393598, Accuracy 4.810%\n",
      "Epoch 1, Batch 33, LR 0.000057 Loss 17.384389, Accuracy 4.901%\n",
      "Epoch 1, Batch 34, LR 0.000057 Loss 17.383436, Accuracy 4.871%\n",
      "Epoch 1, Batch 35, LR 0.000057 Loss 17.373584, Accuracy 5.022%\n",
      "Epoch 1, Batch 36, LR 0.000057 Loss 17.373797, Accuracy 5.013%\n",
      "Epoch 1, Batch 37, LR 0.000057 Loss 17.369501, Accuracy 5.025%\n",
      "Epoch 1, Batch 38, LR 0.000058 Loss 17.369138, Accuracy 4.975%\n",
      "Epoch 1, Batch 39, LR 0.000058 Loss 17.369328, Accuracy 4.968%\n",
      "Epoch 1, Batch 40, LR 0.000058 Loss 17.372100, Accuracy 4.980%\n",
      "Epoch 1, Batch 41, LR 0.000058 Loss 17.372244, Accuracy 4.973%\n",
      "Epoch 1, Batch 42, LR 0.000058 Loss 17.369859, Accuracy 4.985%\n",
      "Epoch 1, Batch 43, LR 0.000058 Loss 17.370389, Accuracy 5.033%\n",
      "Epoch 1, Batch 44, LR 0.000058 Loss 17.370728, Accuracy 5.043%\n",
      "Epoch 1, Batch 45, LR 0.000058 Loss 17.369437, Accuracy 5.069%\n",
      "Epoch 1, Batch 46, LR 0.000058 Loss 17.364830, Accuracy 5.027%\n",
      "Epoch 1, Batch 47, LR 0.000058 Loss 17.362380, Accuracy 4.987%\n",
      "Epoch 1, Batch 48, LR 0.000058 Loss 17.363406, Accuracy 4.997%\n",
      "Epoch 1, Batch 49, LR 0.000058 Loss 17.361769, Accuracy 5.006%\n",
      "Epoch 1, Batch 50, LR 0.000058 Loss 17.359294, Accuracy 5.078%\n",
      "Epoch 1, Batch 51, LR 0.000058 Loss 17.356254, Accuracy 5.086%\n",
      "Epoch 1, Batch 52, LR 0.000058 Loss 17.356716, Accuracy 5.048%\n",
      "Epoch 1, Batch 53, LR 0.000059 Loss 17.352741, Accuracy 5.056%\n",
      "Epoch 1, Batch 54, LR 0.000059 Loss 17.351899, Accuracy 5.006%\n",
      "Epoch 1, Batch 55, LR 0.000059 Loss 17.350933, Accuracy 4.986%\n",
      "Epoch 1, Batch 56, LR 0.000059 Loss 17.347388, Accuracy 5.078%\n",
      "Epoch 1, Batch 57, LR 0.000059 Loss 17.345786, Accuracy 5.099%\n",
      "Epoch 1, Batch 58, LR 0.000059 Loss 17.345747, Accuracy 5.132%\n",
      "Epoch 1, Batch 59, LR 0.000059 Loss 17.346136, Accuracy 5.138%\n",
      "Epoch 1, Batch 60, LR 0.000059 Loss 17.345617, Accuracy 5.130%\n",
      "Epoch 1, Batch 61, LR 0.000059 Loss 17.342208, Accuracy 5.174%\n",
      "Epoch 1, Batch 62, LR 0.000059 Loss 17.341241, Accuracy 5.141%\n",
      "Epoch 1, Batch 63, LR 0.000059 Loss 17.337653, Accuracy 5.159%\n",
      "Epoch 1, Batch 64, LR 0.000059 Loss 17.335591, Accuracy 5.249%\n",
      "Epoch 1, Batch 65, LR 0.000059 Loss 17.331436, Accuracy 5.300%\n",
      "Epoch 1, Batch 66, LR 0.000059 Loss 17.327272, Accuracy 5.327%\n",
      "Epoch 1, Batch 67, LR 0.000059 Loss 17.328211, Accuracy 5.306%\n",
      "Epoch 1, Batch 68, LR 0.000060 Loss 17.327835, Accuracy 5.308%\n",
      "Epoch 1, Batch 69, LR 0.000060 Loss 17.328363, Accuracy 5.310%\n",
      "Epoch 1, Batch 70, LR 0.000060 Loss 17.326139, Accuracy 5.346%\n",
      "Epoch 1, Batch 71, LR 0.000060 Loss 17.325077, Accuracy 5.392%\n",
      "Epoch 1, Batch 72, LR 0.000060 Loss 17.319301, Accuracy 5.414%\n",
      "Epoch 1, Batch 73, LR 0.000060 Loss 17.319416, Accuracy 5.415%\n",
      "Epoch 1, Batch 74, LR 0.000060 Loss 17.318600, Accuracy 5.458%\n",
      "Epoch 1, Batch 75, LR 0.000060 Loss 17.317915, Accuracy 5.458%\n",
      "Epoch 1, Batch 76, LR 0.000060 Loss 17.317239, Accuracy 5.438%\n",
      "Epoch 1, Batch 77, LR 0.000060 Loss 17.319529, Accuracy 5.408%\n",
      "Epoch 1, Batch 78, LR 0.000060 Loss 17.321078, Accuracy 5.399%\n",
      "Epoch 1, Batch 79, LR 0.000060 Loss 17.318581, Accuracy 5.459%\n",
      "Epoch 1, Batch 80, LR 0.000060 Loss 17.317272, Accuracy 5.508%\n",
      "Epoch 1, Batch 81, LR 0.000060 Loss 17.316759, Accuracy 5.498%\n",
      "Epoch 1, Batch 82, LR 0.000060 Loss 17.316199, Accuracy 5.535%\n",
      "Epoch 1, Batch 83, LR 0.000061 Loss 17.312108, Accuracy 5.591%\n",
      "Epoch 1, Batch 84, LR 0.000061 Loss 17.310270, Accuracy 5.608%\n",
      "Epoch 1, Batch 85, LR 0.000061 Loss 17.307693, Accuracy 5.625%\n",
      "Epoch 1, Batch 86, LR 0.000061 Loss 17.307885, Accuracy 5.623%\n",
      "Epoch 1, Batch 87, LR 0.000061 Loss 17.304423, Accuracy 5.657%\n",
      "Epoch 1, Batch 88, LR 0.000061 Loss 17.302904, Accuracy 5.664%\n",
      "Epoch 1, Batch 89, LR 0.000061 Loss 17.301363, Accuracy 5.671%\n",
      "Epoch 1, Batch 90, LR 0.000061 Loss 17.301398, Accuracy 5.642%\n",
      "Epoch 1, Batch 91, LR 0.000061 Loss 17.301600, Accuracy 5.632%\n",
      "Epoch 1, Batch 92, LR 0.000061 Loss 17.301435, Accuracy 5.605%\n",
      "Epoch 1, Batch 93, LR 0.000061 Loss 17.298580, Accuracy 5.620%\n",
      "Epoch 1, Batch 94, LR 0.000061 Loss 17.296227, Accuracy 5.635%\n",
      "Epoch 1, Batch 95, LR 0.000061 Loss 17.298123, Accuracy 5.617%\n",
      "Epoch 1, Batch 96, LR 0.000061 Loss 17.298249, Accuracy 5.615%\n",
      "Epoch 1, Batch 97, LR 0.000061 Loss 17.295582, Accuracy 5.638%\n",
      "Epoch 1, Batch 98, LR 0.000062 Loss 17.293405, Accuracy 5.668%\n",
      "Epoch 1, Batch 99, LR 0.000062 Loss 17.292832, Accuracy 5.666%\n",
      "Epoch 1, Batch 100, LR 0.000062 Loss 17.293113, Accuracy 5.648%\n",
      "Epoch 1, Batch 101, LR 0.000062 Loss 17.288932, Accuracy 5.670%\n",
      "Epoch 1, Batch 102, LR 0.000062 Loss 17.287603, Accuracy 5.691%\n",
      "Epoch 1, Batch 103, LR 0.000062 Loss 17.289559, Accuracy 5.674%\n",
      "Epoch 1, Batch 104, LR 0.000062 Loss 17.289366, Accuracy 5.694%\n",
      "Epoch 1, Batch 105, LR 0.000062 Loss 17.289462, Accuracy 5.670%\n",
      "Epoch 1, Batch 106, LR 0.000062 Loss 17.290381, Accuracy 5.638%\n",
      "Epoch 1, Batch 107, LR 0.000062 Loss 17.289803, Accuracy 5.615%\n",
      "Epoch 1, Batch 108, LR 0.000062 Loss 17.290011, Accuracy 5.635%\n",
      "Epoch 1, Batch 109, LR 0.000062 Loss 17.286878, Accuracy 5.662%\n",
      "Epoch 1, Batch 110, LR 0.000062 Loss 17.287256, Accuracy 5.682%\n",
      "Epoch 1, Batch 111, LR 0.000062 Loss 17.287112, Accuracy 5.701%\n",
      "Epoch 1, Batch 112, LR 0.000062 Loss 17.287455, Accuracy 5.692%\n",
      "Epoch 1, Batch 113, LR 0.000063 Loss 17.286258, Accuracy 5.690%\n",
      "Epoch 1, Batch 114, LR 0.000063 Loss 17.286671, Accuracy 5.688%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 115, LR 0.000063 Loss 17.287279, Accuracy 5.686%\n",
      "Epoch 1, Batch 116, LR 0.000063 Loss 17.288807, Accuracy 5.664%\n",
      "Epoch 1, Batch 117, LR 0.000063 Loss 17.287952, Accuracy 5.662%\n",
      "Epoch 1, Batch 118, LR 0.000063 Loss 17.289284, Accuracy 5.634%\n",
      "Epoch 1, Batch 119, LR 0.000063 Loss 17.287735, Accuracy 5.666%\n",
      "Epoch 1, Batch 120, LR 0.000063 Loss 17.285433, Accuracy 5.684%\n",
      "Epoch 1, Batch 121, LR 0.000063 Loss 17.285565, Accuracy 5.688%\n",
      "Epoch 1, Batch 122, LR 0.000063 Loss 17.283787, Accuracy 5.693%\n",
      "Epoch 1, Batch 123, LR 0.000063 Loss 17.283616, Accuracy 5.678%\n",
      "Epoch 1, Batch 124, LR 0.000063 Loss 17.280753, Accuracy 5.708%\n",
      "Epoch 1, Batch 125, LR 0.000063 Loss 17.281127, Accuracy 5.694%\n",
      "Epoch 1, Batch 126, LR 0.000063 Loss 17.281669, Accuracy 5.680%\n",
      "Epoch 1, Batch 127, LR 0.000063 Loss 17.280388, Accuracy 5.684%\n",
      "Epoch 1, Batch 128, LR 0.000064 Loss 17.279975, Accuracy 5.682%\n",
      "Epoch 1, Batch 129, LR 0.000064 Loss 17.277962, Accuracy 5.711%\n",
      "Epoch 1, Batch 130, LR 0.000064 Loss 17.276257, Accuracy 5.715%\n",
      "Epoch 1, Batch 131, LR 0.000064 Loss 17.276365, Accuracy 5.707%\n",
      "Epoch 1, Batch 132, LR 0.000064 Loss 17.274547, Accuracy 5.711%\n",
      "Epoch 1, Batch 133, LR 0.000064 Loss 17.273624, Accuracy 5.710%\n",
      "Epoch 1, Batch 134, LR 0.000064 Loss 17.271865, Accuracy 5.719%\n",
      "Epoch 1, Batch 135, LR 0.000064 Loss 17.269230, Accuracy 5.752%\n",
      "Epoch 1, Batch 136, LR 0.000064 Loss 17.269883, Accuracy 5.750%\n",
      "Epoch 1, Batch 137, LR 0.000064 Loss 17.268583, Accuracy 5.760%\n",
      "Epoch 1, Batch 138, LR 0.000064 Loss 17.267792, Accuracy 5.769%\n",
      "Epoch 1, Batch 139, LR 0.000064 Loss 17.267620, Accuracy 5.767%\n",
      "Epoch 1, Batch 140, LR 0.000064 Loss 17.265995, Accuracy 5.776%\n",
      "Epoch 1, Batch 141, LR 0.000064 Loss 17.266305, Accuracy 5.773%\n",
      "Epoch 1, Batch 142, LR 0.000064 Loss 17.266247, Accuracy 5.777%\n",
      "Epoch 1, Batch 143, LR 0.000065 Loss 17.265609, Accuracy 5.791%\n",
      "Epoch 1, Batch 144, LR 0.000065 Loss 17.266651, Accuracy 5.773%\n",
      "Epoch 1, Batch 145, LR 0.000065 Loss 17.266388, Accuracy 5.760%\n",
      "Epoch 1, Batch 146, LR 0.000065 Loss 17.263975, Accuracy 5.801%\n",
      "Epoch 1, Batch 147, LR 0.000065 Loss 17.263196, Accuracy 5.820%\n",
      "Epoch 1, Batch 148, LR 0.000065 Loss 17.261982, Accuracy 5.844%\n",
      "Epoch 1, Batch 149, LR 0.000065 Loss 17.260251, Accuracy 5.852%\n",
      "Epoch 1, Batch 150, LR 0.000065 Loss 17.258297, Accuracy 5.865%\n",
      "Epoch 1, Batch 151, LR 0.000065 Loss 17.257834, Accuracy 5.862%\n",
      "Epoch 1, Batch 152, LR 0.000065 Loss 17.256885, Accuracy 5.875%\n",
      "Epoch 1, Batch 153, LR 0.000065 Loss 17.254878, Accuracy 5.887%\n",
      "Epoch 1, Batch 154, LR 0.000065 Loss 17.254336, Accuracy 5.890%\n",
      "Epoch 1, Batch 155, LR 0.000065 Loss 17.254184, Accuracy 5.897%\n",
      "Epoch 1, Batch 156, LR 0.000065 Loss 17.254780, Accuracy 5.889%\n",
      "Epoch 1, Batch 157, LR 0.000065 Loss 17.255100, Accuracy 5.892%\n",
      "Epoch 1, Batch 158, LR 0.000066 Loss 17.255360, Accuracy 5.874%\n",
      "Epoch 1, Batch 159, LR 0.000066 Loss 17.254785, Accuracy 5.877%\n",
      "Epoch 1, Batch 160, LR 0.000066 Loss 17.253975, Accuracy 5.884%\n",
      "Epoch 1, Batch 161, LR 0.000066 Loss 17.254653, Accuracy 5.867%\n",
      "Epoch 1, Batch 162, LR 0.000066 Loss 17.252639, Accuracy 5.888%\n",
      "Epoch 1, Batch 163, LR 0.000066 Loss 17.252271, Accuracy 5.895%\n",
      "Epoch 1, Batch 164, LR 0.000066 Loss 17.251294, Accuracy 5.912%\n",
      "Epoch 1, Batch 165, LR 0.000066 Loss 17.248957, Accuracy 5.933%\n",
      "Epoch 1, Batch 166, LR 0.000066 Loss 17.248474, Accuracy 5.935%\n",
      "Epoch 1, Batch 167, LR 0.000066 Loss 17.249018, Accuracy 5.913%\n",
      "Epoch 1, Batch 168, LR 0.000066 Loss 17.248972, Accuracy 5.911%\n",
      "Epoch 1, Batch 169, LR 0.000066 Loss 17.248641, Accuracy 5.936%\n",
      "Epoch 1, Batch 170, LR 0.000066 Loss 17.248030, Accuracy 5.947%\n",
      "Epoch 1, Batch 171, LR 0.000066 Loss 17.246738, Accuracy 5.944%\n",
      "Epoch 1, Batch 172, LR 0.000066 Loss 17.245154, Accuracy 5.959%\n",
      "Epoch 1, Batch 173, LR 0.000067 Loss 17.245917, Accuracy 5.961%\n",
      "Epoch 1, Batch 174, LR 0.000067 Loss 17.245684, Accuracy 5.967%\n",
      "Epoch 1, Batch 175, LR 0.000067 Loss 17.245875, Accuracy 5.960%\n",
      "Epoch 1, Batch 176, LR 0.000067 Loss 17.244871, Accuracy 5.966%\n",
      "Epoch 1, Batch 177, LR 0.000067 Loss 17.244454, Accuracy 5.968%\n",
      "Epoch 1, Batch 178, LR 0.000067 Loss 17.244105, Accuracy 5.978%\n",
      "Epoch 1, Batch 179, LR 0.000067 Loss 17.243337, Accuracy 5.971%\n",
      "Epoch 1, Batch 180, LR 0.000067 Loss 17.243021, Accuracy 5.968%\n",
      "Epoch 1, Batch 181, LR 0.000067 Loss 17.243823, Accuracy 5.956%\n",
      "Epoch 1, Batch 182, LR 0.000067 Loss 17.243277, Accuracy 5.967%\n",
      "Epoch 1, Batch 183, LR 0.000067 Loss 17.242578, Accuracy 5.964%\n",
      "Epoch 1, Batch 184, LR 0.000067 Loss 17.241267, Accuracy 5.966%\n",
      "Epoch 1, Batch 185, LR 0.000067 Loss 17.241063, Accuracy 5.980%\n",
      "Epoch 1, Batch 186, LR 0.000067 Loss 17.240806, Accuracy 5.969%\n",
      "Epoch 1, Batch 187, LR 0.000067 Loss 17.240684, Accuracy 5.978%\n",
      "Epoch 1, Batch 188, LR 0.000067 Loss 17.239827, Accuracy 5.976%\n",
      "Epoch 1, Batch 189, LR 0.000068 Loss 17.239461, Accuracy 5.985%\n",
      "Epoch 1, Batch 190, LR 0.000068 Loss 17.238850, Accuracy 5.983%\n",
      "Epoch 1, Batch 191, LR 0.000068 Loss 17.238047, Accuracy 6.005%\n",
      "Epoch 1, Batch 192, LR 0.000068 Loss 17.237517, Accuracy 6.002%\n",
      "Epoch 1, Batch 193, LR 0.000068 Loss 17.238003, Accuracy 5.987%\n",
      "Epoch 1, Batch 194, LR 0.000068 Loss 17.235880, Accuracy 6.012%\n",
      "Epoch 1, Batch 195, LR 0.000068 Loss 17.234739, Accuracy 6.050%\n",
      "Epoch 1, Batch 196, LR 0.000068 Loss 17.234123, Accuracy 6.047%\n",
      "Epoch 1, Batch 197, LR 0.000068 Loss 17.231992, Accuracy 6.056%\n",
      "Epoch 1, Batch 198, LR 0.000068 Loss 17.231482, Accuracy 6.061%\n",
      "Epoch 1, Batch 199, LR 0.000068 Loss 17.230063, Accuracy 6.073%\n",
      "Epoch 1, Batch 200, LR 0.000068 Loss 17.229581, Accuracy 6.066%\n",
      "Epoch 1, Batch 201, LR 0.000068 Loss 17.228007, Accuracy 6.091%\n",
      "Epoch 1, Batch 202, LR 0.000068 Loss 17.227232, Accuracy 6.091%\n",
      "Epoch 1, Batch 203, LR 0.000068 Loss 17.226996, Accuracy 6.092%\n",
      "Epoch 1, Batch 204, LR 0.000069 Loss 17.227611, Accuracy 6.093%\n",
      "Epoch 1, Batch 205, LR 0.000069 Loss 17.226368, Accuracy 6.094%\n",
      "Epoch 1, Batch 206, LR 0.000069 Loss 17.225027, Accuracy 6.113%\n",
      "Epoch 1, Batch 207, LR 0.000069 Loss 17.223669, Accuracy 6.118%\n",
      "Epoch 1, Batch 208, LR 0.000069 Loss 17.222864, Accuracy 6.134%\n",
      "Epoch 1, Batch 209, LR 0.000069 Loss 17.222473, Accuracy 6.127%\n",
      "Epoch 1, Batch 210, LR 0.000069 Loss 17.222838, Accuracy 6.105%\n",
      "Epoch 1, Batch 211, LR 0.000069 Loss 17.222692, Accuracy 6.094%\n",
      "Epoch 1, Batch 212, LR 0.000069 Loss 17.222276, Accuracy 6.099%\n",
      "Epoch 1, Batch 213, LR 0.000069 Loss 17.221973, Accuracy 6.107%\n",
      "Epoch 1, Batch 214, LR 0.000069 Loss 17.221803, Accuracy 6.115%\n",
      "Epoch 1, Batch 215, LR 0.000069 Loss 17.220826, Accuracy 6.112%\n",
      "Epoch 1, Batch 216, LR 0.000069 Loss 17.221382, Accuracy 6.102%\n",
      "Epoch 1, Batch 217, LR 0.000069 Loss 17.220159, Accuracy 6.128%\n",
      "Epoch 1, Batch 218, LR 0.000069 Loss 17.220103, Accuracy 6.117%\n",
      "Epoch 1, Batch 219, LR 0.000069 Loss 17.219609, Accuracy 6.111%\n",
      "Epoch 1, Batch 220, LR 0.000070 Loss 17.218909, Accuracy 6.104%\n",
      "Epoch 1, Batch 221, LR 0.000070 Loss 17.218458, Accuracy 6.109%\n",
      "Epoch 1, Batch 222, LR 0.000070 Loss 17.217277, Accuracy 6.116%\n",
      "Epoch 1, Batch 223, LR 0.000070 Loss 17.216834, Accuracy 6.113%\n",
      "Epoch 1, Batch 224, LR 0.000070 Loss 17.216357, Accuracy 6.114%\n",
      "Epoch 1, Batch 225, LR 0.000070 Loss 17.214432, Accuracy 6.135%\n",
      "Epoch 1, Batch 226, LR 0.000070 Loss 17.214141, Accuracy 6.139%\n",
      "Epoch 1, Batch 227, LR 0.000070 Loss 17.212149, Accuracy 6.178%\n",
      "Epoch 1, Batch 228, LR 0.000070 Loss 17.211223, Accuracy 6.178%\n",
      "Epoch 1, Batch 229, LR 0.000070 Loss 17.210033, Accuracy 6.189%\n",
      "Epoch 1, Batch 230, LR 0.000070 Loss 17.209678, Accuracy 6.179%\n",
      "Epoch 1, Batch 231, LR 0.000070 Loss 17.208793, Accuracy 6.172%\n",
      "Epoch 1, Batch 232, LR 0.000070 Loss 17.207774, Accuracy 6.169%\n",
      "Epoch 1, Batch 233, LR 0.000070 Loss 17.207097, Accuracy 6.173%\n",
      "Epoch 1, Batch 234, LR 0.000070 Loss 17.205566, Accuracy 6.180%\n",
      "Epoch 1, Batch 235, LR 0.000071 Loss 17.204347, Accuracy 6.197%\n",
      "Epoch 1, Batch 236, LR 0.000071 Loss 17.204370, Accuracy 6.190%\n",
      "Epoch 1, Batch 237, LR 0.000071 Loss 17.202879, Accuracy 6.201%\n",
      "Epoch 1, Batch 238, LR 0.000071 Loss 17.202093, Accuracy 6.194%\n",
      "Epoch 1, Batch 239, LR 0.000071 Loss 17.201658, Accuracy 6.194%\n",
      "Epoch 1, Batch 240, LR 0.000071 Loss 17.200904, Accuracy 6.201%\n",
      "Epoch 1, Batch 241, LR 0.000071 Loss 17.199837, Accuracy 6.208%\n",
      "Epoch 1, Batch 242, LR 0.000071 Loss 17.198398, Accuracy 6.227%\n",
      "Epoch 1, Batch 243, LR 0.000071 Loss 17.197539, Accuracy 6.224%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 244, LR 0.000071 Loss 17.196416, Accuracy 6.231%\n",
      "Epoch 1, Batch 245, LR 0.000071 Loss 17.196243, Accuracy 6.240%\n",
      "Epoch 1, Batch 246, LR 0.000071 Loss 17.195374, Accuracy 6.240%\n",
      "Epoch 1, Batch 247, LR 0.000071 Loss 17.193573, Accuracy 6.266%\n",
      "Epoch 1, Batch 248, LR 0.000071 Loss 17.193616, Accuracy 6.263%\n",
      "Epoch 1, Batch 249, LR 0.000071 Loss 17.193417, Accuracy 6.250%\n",
      "Epoch 1, Batch 250, LR 0.000071 Loss 17.193188, Accuracy 6.241%\n",
      "Epoch 1, Batch 251, LR 0.000072 Loss 17.191951, Accuracy 6.262%\n",
      "Epoch 1, Batch 252, LR 0.000072 Loss 17.191312, Accuracy 6.259%\n",
      "Epoch 1, Batch 253, LR 0.000072 Loss 17.190793, Accuracy 6.256%\n",
      "Epoch 1, Batch 254, LR 0.000072 Loss 17.189656, Accuracy 6.256%\n",
      "Epoch 1, Batch 255, LR 0.000072 Loss 17.188432, Accuracy 6.265%\n",
      "Epoch 1, Batch 256, LR 0.000072 Loss 17.187704, Accuracy 6.274%\n",
      "Epoch 1, Batch 257, LR 0.000072 Loss 17.186209, Accuracy 6.277%\n",
      "Epoch 1, Batch 258, LR 0.000072 Loss 17.185761, Accuracy 6.286%\n",
      "Epoch 1, Batch 259, LR 0.000072 Loss 17.184636, Accuracy 6.289%\n",
      "Epoch 1, Batch 260, LR 0.000072 Loss 17.183404, Accuracy 6.301%\n",
      "Epoch 1, Batch 261, LR 0.000072 Loss 17.183075, Accuracy 6.298%\n",
      "Epoch 1, Batch 262, LR 0.000072 Loss 17.182806, Accuracy 6.292%\n",
      "Epoch 1, Batch 263, LR 0.000072 Loss 17.181184, Accuracy 6.303%\n",
      "Epoch 1, Batch 264, LR 0.000072 Loss 17.180821, Accuracy 6.297%\n",
      "Epoch 1, Batch 265, LR 0.000072 Loss 17.179943, Accuracy 6.306%\n",
      "Epoch 1, Batch 266, LR 0.000072 Loss 17.178705, Accuracy 6.303%\n",
      "Epoch 1, Batch 267, LR 0.000073 Loss 17.177819, Accuracy 6.311%\n",
      "Epoch 1, Batch 268, LR 0.000073 Loss 17.176164, Accuracy 6.335%\n",
      "Epoch 1, Batch 269, LR 0.000073 Loss 17.174107, Accuracy 6.360%\n",
      "Epoch 1, Batch 270, LR 0.000073 Loss 17.173744, Accuracy 6.363%\n",
      "Epoch 1, Batch 271, LR 0.000073 Loss 17.173110, Accuracy 6.354%\n",
      "Epoch 1, Batch 272, LR 0.000073 Loss 17.172173, Accuracy 6.359%\n",
      "Epoch 1, Batch 273, LR 0.000073 Loss 17.170091, Accuracy 6.373%\n",
      "Epoch 1, Batch 274, LR 0.000073 Loss 17.169905, Accuracy 6.367%\n",
      "Epoch 1, Batch 275, LR 0.000073 Loss 17.168717, Accuracy 6.384%\n",
      "Epoch 1, Batch 276, LR 0.000073 Loss 17.168054, Accuracy 6.380%\n",
      "Epoch 1, Batch 277, LR 0.000073 Loss 17.167196, Accuracy 6.385%\n",
      "Epoch 1, Batch 278, LR 0.000073 Loss 17.166647, Accuracy 6.391%\n",
      "Epoch 1, Batch 279, LR 0.000073 Loss 17.166034, Accuracy 6.398%\n",
      "Epoch 1, Batch 280, LR 0.000073 Loss 17.165243, Accuracy 6.401%\n",
      "Epoch 1, Batch 281, LR 0.000073 Loss 17.164874, Accuracy 6.403%\n",
      "Epoch 1, Batch 282, LR 0.000073 Loss 17.163941, Accuracy 6.413%\n",
      "Epoch 1, Batch 283, LR 0.000074 Loss 17.163244, Accuracy 6.410%\n",
      "Epoch 1, Batch 284, LR 0.000074 Loss 17.161850, Accuracy 6.418%\n",
      "Epoch 1, Batch 285, LR 0.000074 Loss 17.161112, Accuracy 6.423%\n",
      "Epoch 1, Batch 286, LR 0.000074 Loss 17.159984, Accuracy 6.433%\n",
      "Epoch 1, Batch 287, LR 0.000074 Loss 17.157903, Accuracy 6.462%\n",
      "Epoch 1, Batch 288, LR 0.000074 Loss 17.155986, Accuracy 6.472%\n",
      "Epoch 1, Batch 289, LR 0.000074 Loss 17.155295, Accuracy 6.474%\n",
      "Epoch 1, Batch 290, LR 0.000074 Loss 17.154050, Accuracy 6.492%\n",
      "Epoch 1, Batch 291, LR 0.000074 Loss 17.154127, Accuracy 6.494%\n",
      "Epoch 1, Batch 292, LR 0.000074 Loss 17.153138, Accuracy 6.510%\n",
      "Epoch 1, Batch 293, LR 0.000074 Loss 17.152969, Accuracy 6.514%\n",
      "Epoch 1, Batch 294, LR 0.000074 Loss 17.151956, Accuracy 6.518%\n",
      "Epoch 1, Batch 295, LR 0.000074 Loss 17.151185, Accuracy 6.523%\n",
      "Epoch 1, Batch 296, LR 0.000074 Loss 17.150191, Accuracy 6.535%\n",
      "Epoch 1, Batch 297, LR 0.000074 Loss 17.149648, Accuracy 6.531%\n",
      "Epoch 1, Batch 298, LR 0.000074 Loss 17.148143, Accuracy 6.544%\n",
      "Epoch 1, Batch 299, LR 0.000074 Loss 17.147773, Accuracy 6.543%\n",
      "Epoch 1, Batch 300, LR 0.000075 Loss 17.146850, Accuracy 6.544%\n",
      "Epoch 1, Batch 301, LR 0.000075 Loss 17.145524, Accuracy 6.559%\n",
      "Epoch 1, Batch 302, LR 0.000075 Loss 17.144517, Accuracy 6.555%\n",
      "Epoch 1, Batch 303, LR 0.000075 Loss 17.143994, Accuracy 6.552%\n",
      "Epoch 1, Batch 304, LR 0.000075 Loss 17.143099, Accuracy 6.564%\n",
      "Epoch 1, Batch 305, LR 0.000075 Loss 17.142625, Accuracy 6.557%\n",
      "Epoch 1, Batch 306, LR 0.000075 Loss 17.141750, Accuracy 6.567%\n",
      "Epoch 1, Batch 307, LR 0.000075 Loss 17.141150, Accuracy 6.573%\n",
      "Epoch 1, Batch 308, LR 0.000075 Loss 17.142026, Accuracy 6.567%\n",
      "Epoch 1, Batch 309, LR 0.000075 Loss 17.141801, Accuracy 6.566%\n",
      "Epoch 1, Batch 310, LR 0.000075 Loss 17.140272, Accuracy 6.585%\n",
      "Epoch 1, Batch 311, LR 0.000075 Loss 17.138900, Accuracy 6.599%\n",
      "Epoch 1, Batch 312, LR 0.000075 Loss 17.138487, Accuracy 6.593%\n",
      "Epoch 1, Batch 313, LR 0.000075 Loss 17.137304, Accuracy 6.609%\n",
      "Epoch 1, Batch 314, LR 0.000075 Loss 17.135713, Accuracy 6.626%\n",
      "Epoch 1, Batch 315, LR 0.000075 Loss 17.134538, Accuracy 6.647%\n",
      "Epoch 1, Batch 316, LR 0.000076 Loss 17.134566, Accuracy 6.648%\n",
      "Epoch 1, Batch 317, LR 0.000076 Loss 17.132770, Accuracy 6.664%\n",
      "Epoch 1, Batch 318, LR 0.000076 Loss 17.131336, Accuracy 6.680%\n",
      "Epoch 1, Batch 319, LR 0.000076 Loss 17.130717, Accuracy 6.676%\n",
      "Epoch 1, Batch 320, LR 0.000076 Loss 17.129553, Accuracy 6.677%\n",
      "Epoch 1, Batch 321, LR 0.000076 Loss 17.127975, Accuracy 6.691%\n",
      "Epoch 1, Batch 322, LR 0.000076 Loss 17.127661, Accuracy 6.687%\n",
      "Epoch 1, Batch 323, LR 0.000076 Loss 17.126825, Accuracy 6.685%\n",
      "Epoch 1, Batch 324, LR 0.000076 Loss 17.126395, Accuracy 6.684%\n",
      "Epoch 1, Batch 325, LR 0.000076 Loss 17.125114, Accuracy 6.695%\n",
      "Epoch 1, Batch 326, LR 0.000076 Loss 17.123999, Accuracy 6.715%\n",
      "Epoch 1, Batch 327, LR 0.000076 Loss 17.122384, Accuracy 6.730%\n",
      "Epoch 1, Batch 328, LR 0.000076 Loss 17.121321, Accuracy 6.736%\n",
      "Epoch 1, Batch 329, LR 0.000076 Loss 17.120452, Accuracy 6.746%\n",
      "Epoch 1, Batch 330, LR 0.000076 Loss 17.119665, Accuracy 6.752%\n",
      "Epoch 1, Batch 331, LR 0.000076 Loss 17.119719, Accuracy 6.743%\n",
      "Epoch 1, Batch 332, LR 0.000076 Loss 17.119558, Accuracy 6.737%\n",
      "Epoch 1, Batch 333, LR 0.000077 Loss 17.118974, Accuracy 6.736%\n",
      "Epoch 1, Batch 334, LR 0.000077 Loss 17.118300, Accuracy 6.732%\n",
      "Epoch 1, Batch 335, LR 0.000077 Loss 17.117290, Accuracy 6.737%\n",
      "Epoch 1, Batch 336, LR 0.000077 Loss 17.116559, Accuracy 6.743%\n",
      "Epoch 1, Batch 337, LR 0.000077 Loss 17.115513, Accuracy 6.753%\n",
      "Epoch 1, Batch 338, LR 0.000077 Loss 17.114982, Accuracy 6.763%\n",
      "Epoch 1, Batch 339, LR 0.000077 Loss 17.113842, Accuracy 6.764%\n",
      "Epoch 1, Batch 340, LR 0.000077 Loss 17.113054, Accuracy 6.767%\n",
      "Epoch 1, Batch 341, LR 0.000077 Loss 17.112073, Accuracy 6.768%\n",
      "Epoch 1, Batch 342, LR 0.000077 Loss 17.110757, Accuracy 6.773%\n",
      "Epoch 1, Batch 343, LR 0.000077 Loss 17.110121, Accuracy 6.774%\n",
      "Epoch 1, Batch 344, LR 0.000077 Loss 17.108901, Accuracy 6.772%\n",
      "Epoch 1, Batch 345, LR 0.000077 Loss 17.107961, Accuracy 6.782%\n",
      "Epoch 1, Batch 346, LR 0.000077 Loss 17.106632, Accuracy 6.785%\n",
      "Epoch 1, Batch 347, LR 0.000077 Loss 17.105458, Accuracy 6.799%\n",
      "Epoch 1, Batch 348, LR 0.000077 Loss 17.104583, Accuracy 6.802%\n",
      "Epoch 1, Batch 349, LR 0.000077 Loss 17.103829, Accuracy 6.796%\n",
      "Epoch 1, Batch 350, LR 0.000078 Loss 17.103083, Accuracy 6.801%\n",
      "Epoch 1, Batch 351, LR 0.000078 Loss 17.102153, Accuracy 6.811%\n",
      "Epoch 1, Batch 352, LR 0.000078 Loss 17.101174, Accuracy 6.812%\n",
      "Epoch 1, Batch 353, LR 0.000078 Loss 17.100411, Accuracy 6.819%\n",
      "Epoch 1, Batch 354, LR 0.000078 Loss 17.099660, Accuracy 6.819%\n",
      "Epoch 1, Batch 355, LR 0.000078 Loss 17.098313, Accuracy 6.833%\n",
      "Epoch 1, Batch 356, LR 0.000078 Loss 17.097355, Accuracy 6.843%\n",
      "Epoch 1, Batch 357, LR 0.000078 Loss 17.096756, Accuracy 6.850%\n",
      "Epoch 1, Batch 358, LR 0.000078 Loss 17.095992, Accuracy 6.859%\n",
      "Epoch 1, Batch 359, LR 0.000078 Loss 17.095257, Accuracy 6.848%\n",
      "Epoch 1, Batch 360, LR 0.000078 Loss 17.094417, Accuracy 6.853%\n",
      "Epoch 1, Batch 361, LR 0.000078 Loss 17.094016, Accuracy 6.845%\n",
      "Epoch 1, Batch 362, LR 0.000078 Loss 17.093497, Accuracy 6.848%\n",
      "Epoch 1, Batch 363, LR 0.000078 Loss 17.092601, Accuracy 6.850%\n",
      "Epoch 1, Batch 364, LR 0.000078 Loss 17.091214, Accuracy 6.855%\n",
      "Epoch 1, Batch 365, LR 0.000078 Loss 17.090190, Accuracy 6.869%\n",
      "Epoch 1, Batch 366, LR 0.000078 Loss 17.090364, Accuracy 6.867%\n",
      "Epoch 1, Batch 367, LR 0.000079 Loss 17.089539, Accuracy 6.867%\n",
      "Epoch 1, Batch 368, LR 0.000079 Loss 17.089151, Accuracy 6.864%\n",
      "Epoch 1, Batch 369, LR 0.000079 Loss 17.087781, Accuracy 6.866%\n",
      "Epoch 1, Batch 370, LR 0.000079 Loss 17.086544, Accuracy 6.886%\n",
      "Epoch 1, Batch 371, LR 0.000079 Loss 17.085378, Accuracy 6.901%\n",
      "Epoch 1, Batch 372, LR 0.000079 Loss 17.084173, Accuracy 6.909%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 373, LR 0.000079 Loss 17.083552, Accuracy 6.918%\n",
      "Epoch 1, Batch 374, LR 0.000079 Loss 17.082325, Accuracy 6.931%\n",
      "Epoch 1, Batch 375, LR 0.000079 Loss 17.081572, Accuracy 6.938%\n",
      "Epoch 1, Batch 376, LR 0.000079 Loss 17.081046, Accuracy 6.948%\n",
      "Epoch 1, Batch 377, LR 0.000079 Loss 17.079930, Accuracy 6.963%\n",
      "Epoch 1, Batch 378, LR 0.000079 Loss 17.079725, Accuracy 6.963%\n",
      "Epoch 1, Batch 379, LR 0.000079 Loss 17.078360, Accuracy 6.978%\n",
      "Epoch 1, Batch 380, LR 0.000079 Loss 17.077244, Accuracy 6.986%\n",
      "Epoch 1, Batch 381, LR 0.000079 Loss 17.076358, Accuracy 7.000%\n",
      "Epoch 1, Batch 382, LR 0.000079 Loss 17.075568, Accuracy 7.013%\n",
      "Epoch 1, Batch 383, LR 0.000079 Loss 17.074498, Accuracy 7.029%\n",
      "Epoch 1, Batch 384, LR 0.000079 Loss 17.074167, Accuracy 7.025%\n",
      "Epoch 1, Batch 385, LR 0.000080 Loss 17.073900, Accuracy 7.025%\n",
      "Epoch 1, Batch 386, LR 0.000080 Loss 17.073272, Accuracy 7.035%\n",
      "Epoch 1, Batch 387, LR 0.000080 Loss 17.072761, Accuracy 7.039%\n",
      "Epoch 1, Batch 388, LR 0.000080 Loss 17.071556, Accuracy 7.049%\n",
      "Epoch 1, Batch 389, LR 0.000080 Loss 17.071340, Accuracy 7.043%\n",
      "Epoch 1, Batch 390, LR 0.000080 Loss 17.069922, Accuracy 7.053%\n",
      "Epoch 1, Batch 391, LR 0.000080 Loss 17.069159, Accuracy 7.055%\n",
      "Epoch 1, Batch 392, LR 0.000080 Loss 17.068050, Accuracy 7.063%\n",
      "Epoch 1, Batch 393, LR 0.000080 Loss 17.067517, Accuracy 7.057%\n",
      "Epoch 1, Batch 394, LR 0.000080 Loss 17.066875, Accuracy 7.055%\n",
      "Epoch 1, Batch 395, LR 0.000080 Loss 17.066898, Accuracy 7.053%\n",
      "Epoch 1, Batch 396, LR 0.000080 Loss 17.065978, Accuracy 7.059%\n",
      "Epoch 1, Batch 397, LR 0.000080 Loss 17.065195, Accuracy 7.067%\n",
      "Epoch 1, Batch 398, LR 0.000080 Loss 17.065167, Accuracy 7.061%\n",
      "Epoch 1, Batch 399, LR 0.000080 Loss 17.064642, Accuracy 7.063%\n",
      "Epoch 1, Batch 400, LR 0.000080 Loss 17.063538, Accuracy 7.059%\n",
      "Epoch 1, Batch 401, LR 0.000080 Loss 17.062660, Accuracy 7.066%\n",
      "Epoch 1, Batch 402, LR 0.000081 Loss 17.062642, Accuracy 7.064%\n",
      "Epoch 1, Batch 403, LR 0.000081 Loss 17.061888, Accuracy 7.074%\n",
      "Epoch 1, Batch 404, LR 0.000081 Loss 17.061009, Accuracy 7.085%\n",
      "Epoch 1, Batch 405, LR 0.000081 Loss 17.060491, Accuracy 7.089%\n",
      "Epoch 1, Batch 406, LR 0.000081 Loss 17.060102, Accuracy 7.091%\n",
      "Epoch 1, Batch 407, LR 0.000081 Loss 17.059522, Accuracy 7.087%\n",
      "Epoch 1, Batch 408, LR 0.000081 Loss 17.058845, Accuracy 7.096%\n",
      "Epoch 1, Batch 409, LR 0.000081 Loss 17.057865, Accuracy 7.090%\n",
      "Epoch 1, Batch 410, LR 0.000081 Loss 17.056536, Accuracy 7.100%\n",
      "Epoch 1, Batch 411, LR 0.000081 Loss 17.055628, Accuracy 7.096%\n",
      "Epoch 1, Batch 412, LR 0.000081 Loss 17.054537, Accuracy 7.092%\n",
      "Epoch 1, Batch 413, LR 0.000081 Loss 17.053688, Accuracy 7.096%\n",
      "Epoch 1, Batch 414, LR 0.000081 Loss 17.052592, Accuracy 7.101%\n",
      "Epoch 1, Batch 415, LR 0.000081 Loss 17.051798, Accuracy 7.105%\n",
      "Epoch 1, Batch 416, LR 0.000081 Loss 17.050444, Accuracy 7.112%\n",
      "Epoch 1, Batch 417, LR 0.000081 Loss 17.050082, Accuracy 7.119%\n",
      "Epoch 1, Batch 418, LR 0.000081 Loss 17.048972, Accuracy 7.127%\n",
      "Epoch 1, Batch 419, LR 0.000081 Loss 17.047594, Accuracy 7.139%\n",
      "Epoch 1, Batch 420, LR 0.000081 Loss 17.046787, Accuracy 7.143%\n",
      "Epoch 1, Batch 421, LR 0.000082 Loss 17.046810, Accuracy 7.135%\n",
      "Epoch 1, Batch 422, LR 0.000082 Loss 17.045331, Accuracy 7.144%\n",
      "Epoch 1, Batch 423, LR 0.000082 Loss 17.043838, Accuracy 7.157%\n",
      "Epoch 1, Batch 424, LR 0.000082 Loss 17.042472, Accuracy 7.177%\n",
      "Epoch 1, Batch 425, LR 0.000082 Loss 17.041754, Accuracy 7.180%\n",
      "Epoch 1, Batch 426, LR 0.000082 Loss 17.040803, Accuracy 7.185%\n",
      "Epoch 1, Batch 427, LR 0.000082 Loss 17.040528, Accuracy 7.198%\n",
      "Epoch 1, Batch 428, LR 0.000082 Loss 17.039589, Accuracy 7.208%\n",
      "Epoch 1, Batch 429, LR 0.000082 Loss 17.039144, Accuracy 7.208%\n",
      "Epoch 1, Batch 430, LR 0.000082 Loss 17.038679, Accuracy 7.215%\n",
      "Epoch 1, Batch 431, LR 0.000082 Loss 17.037761, Accuracy 7.223%\n",
      "Epoch 1, Batch 432, LR 0.000082 Loss 17.037184, Accuracy 7.221%\n",
      "Epoch 1, Batch 433, LR 0.000082 Loss 17.035922, Accuracy 7.233%\n",
      "Epoch 1, Batch 434, LR 0.000082 Loss 17.034079, Accuracy 7.249%\n",
      "Epoch 1, Batch 435, LR 0.000082 Loss 17.033271, Accuracy 7.250%\n",
      "Epoch 1, Batch 436, LR 0.000082 Loss 17.032087, Accuracy 7.261%\n",
      "Epoch 1, Batch 437, LR 0.000082 Loss 17.031816, Accuracy 7.253%\n",
      "Epoch 1, Batch 438, LR 0.000082 Loss 17.031047, Accuracy 7.260%\n",
      "Epoch 1, Batch 439, LR 0.000083 Loss 17.029704, Accuracy 7.261%\n",
      "Epoch 1, Batch 440, LR 0.000083 Loss 17.028172, Accuracy 7.276%\n",
      "Epoch 1, Batch 441, LR 0.000083 Loss 17.027703, Accuracy 7.277%\n",
      "Epoch 1, Batch 442, LR 0.000083 Loss 17.027558, Accuracy 7.273%\n",
      "Epoch 1, Batch 443, LR 0.000083 Loss 17.026551, Accuracy 7.276%\n",
      "Epoch 1, Batch 444, LR 0.000083 Loss 17.025581, Accuracy 7.278%\n",
      "Epoch 1, Batch 445, LR 0.000083 Loss 17.025160, Accuracy 7.284%\n",
      "Epoch 1, Batch 446, LR 0.000083 Loss 17.024411, Accuracy 7.294%\n",
      "Epoch 1, Batch 447, LR 0.000083 Loss 17.024032, Accuracy 7.290%\n",
      "Epoch 1, Batch 448, LR 0.000083 Loss 17.023108, Accuracy 7.296%\n",
      "Epoch 1, Batch 449, LR 0.000083 Loss 17.021845, Accuracy 7.317%\n",
      "Epoch 1, Batch 450, LR 0.000083 Loss 17.021296, Accuracy 7.323%\n",
      "Epoch 1, Batch 451, LR 0.000083 Loss 17.020286, Accuracy 7.327%\n",
      "Epoch 1, Batch 452, LR 0.000083 Loss 17.019190, Accuracy 7.332%\n",
      "Epoch 1, Batch 453, LR 0.000083 Loss 17.018618, Accuracy 7.338%\n",
      "Epoch 1, Batch 454, LR 0.000083 Loss 17.018053, Accuracy 7.346%\n",
      "Epoch 1, Batch 455, LR 0.000083 Loss 17.017594, Accuracy 7.345%\n",
      "Epoch 1, Batch 456, LR 0.000083 Loss 17.016252, Accuracy 7.362%\n",
      "Epoch 1, Batch 457, LR 0.000083 Loss 17.015932, Accuracy 7.366%\n",
      "Epoch 1, Batch 458, LR 0.000084 Loss 17.014888, Accuracy 7.372%\n",
      "Epoch 1, Batch 459, LR 0.000084 Loss 17.013699, Accuracy 7.378%\n",
      "Epoch 1, Batch 460, LR 0.000084 Loss 17.012630, Accuracy 7.390%\n",
      "Epoch 1, Batch 461, LR 0.000084 Loss 17.012162, Accuracy 7.394%\n",
      "Epoch 1, Batch 462, LR 0.000084 Loss 17.010980, Accuracy 7.405%\n",
      "Epoch 1, Batch 463, LR 0.000084 Loss 17.010423, Accuracy 7.404%\n",
      "Epoch 1, Batch 464, LR 0.000084 Loss 17.010100, Accuracy 7.402%\n",
      "Epoch 1, Batch 465, LR 0.000084 Loss 17.010134, Accuracy 7.394%\n",
      "Epoch 1, Batch 466, LR 0.000084 Loss 17.009374, Accuracy 7.403%\n",
      "Epoch 1, Batch 467, LR 0.000084 Loss 17.008506, Accuracy 7.414%\n",
      "Epoch 1, Batch 468, LR 0.000084 Loss 17.007672, Accuracy 7.417%\n",
      "Epoch 1, Batch 469, LR 0.000084 Loss 17.006989, Accuracy 7.424%\n",
      "Epoch 1, Batch 470, LR 0.000084 Loss 17.006554, Accuracy 7.420%\n",
      "Epoch 1, Batch 471, LR 0.000084 Loss 17.006167, Accuracy 7.424%\n",
      "Epoch 1, Batch 472, LR 0.000084 Loss 17.005002, Accuracy 7.430%\n",
      "Epoch 1, Batch 473, LR 0.000084 Loss 17.004476, Accuracy 7.436%\n",
      "Epoch 1, Batch 474, LR 0.000084 Loss 17.004503, Accuracy 7.438%\n",
      "Epoch 1, Batch 475, LR 0.000084 Loss 17.003956, Accuracy 7.444%\n",
      "Epoch 1, Batch 476, LR 0.000084 Loss 17.002803, Accuracy 7.455%\n",
      "Epoch 1, Batch 477, LR 0.000085 Loss 17.002520, Accuracy 7.449%\n",
      "Epoch 1, Batch 478, LR 0.000085 Loss 17.001883, Accuracy 7.446%\n",
      "Epoch 1, Batch 479, LR 0.000085 Loss 17.000912, Accuracy 7.454%\n",
      "Epoch 1, Batch 480, LR 0.000085 Loss 16.999902, Accuracy 7.463%\n",
      "Epoch 1, Batch 481, LR 0.000085 Loss 16.999253, Accuracy 7.467%\n",
      "Epoch 1, Batch 482, LR 0.000085 Loss 16.998300, Accuracy 7.472%\n",
      "Epoch 1, Batch 483, LR 0.000085 Loss 16.998067, Accuracy 7.473%\n",
      "Epoch 1, Batch 484, LR 0.000085 Loss 16.997597, Accuracy 7.482%\n",
      "Epoch 1, Batch 485, LR 0.000085 Loss 16.997345, Accuracy 7.486%\n",
      "Epoch 1, Batch 486, LR 0.000085 Loss 16.996657, Accuracy 7.494%\n",
      "Epoch 1, Batch 487, LR 0.000085 Loss 16.995918, Accuracy 7.504%\n",
      "Epoch 1, Batch 488, LR 0.000085 Loss 16.995146, Accuracy 7.508%\n",
      "Epoch 1, Batch 489, LR 0.000085 Loss 16.994422, Accuracy 7.517%\n",
      "Epoch 1, Batch 490, LR 0.000085 Loss 16.993877, Accuracy 7.518%\n",
      "Epoch 1, Batch 491, LR 0.000085 Loss 16.992967, Accuracy 7.525%\n",
      "Epoch 1, Batch 492, LR 0.000085 Loss 16.992081, Accuracy 7.531%\n",
      "Epoch 1, Batch 493, LR 0.000085 Loss 16.991516, Accuracy 7.535%\n",
      "Epoch 1, Batch 494, LR 0.000085 Loss 16.990784, Accuracy 7.537%\n",
      "Epoch 1, Batch 495, LR 0.000085 Loss 16.989529, Accuracy 7.547%\n",
      "Epoch 1, Batch 496, LR 0.000085 Loss 16.989063, Accuracy 7.546%\n",
      "Epoch 1, Batch 497, LR 0.000086 Loss 16.987536, Accuracy 7.556%\n",
      "Epoch 1, Batch 498, LR 0.000086 Loss 16.987499, Accuracy 7.552%\n",
      "Epoch 1, Batch 499, LR 0.000086 Loss 16.987081, Accuracy 7.548%\n",
      "Epoch 1, Batch 500, LR 0.000086 Loss 16.986820, Accuracy 7.548%\n",
      "Epoch 1, Batch 501, LR 0.000086 Loss 16.985227, Accuracy 7.557%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 502, LR 0.000086 Loss 16.983919, Accuracy 7.574%\n",
      "Epoch 1, Batch 503, LR 0.000086 Loss 16.983381, Accuracy 7.578%\n",
      "Epoch 1, Batch 504, LR 0.000086 Loss 16.982862, Accuracy 7.583%\n",
      "Epoch 1, Batch 505, LR 0.000086 Loss 16.982676, Accuracy 7.584%\n",
      "Epoch 1, Batch 506, LR 0.000086 Loss 16.982397, Accuracy 7.578%\n",
      "Epoch 1, Batch 507, LR 0.000086 Loss 16.981574, Accuracy 7.586%\n",
      "Epoch 1, Batch 508, LR 0.000086 Loss 16.980702, Accuracy 7.591%\n",
      "Epoch 1, Batch 509, LR 0.000086 Loss 16.980320, Accuracy 7.593%\n",
      "Epoch 1, Batch 510, LR 0.000086 Loss 16.979057, Accuracy 7.610%\n",
      "Epoch 1, Batch 511, LR 0.000086 Loss 16.977830, Accuracy 7.609%\n",
      "Epoch 1, Batch 512, LR 0.000086 Loss 16.976996, Accuracy 7.620%\n",
      "Epoch 1, Batch 513, LR 0.000086 Loss 16.976269, Accuracy 7.624%\n",
      "Epoch 1, Batch 514, LR 0.000086 Loss 16.975208, Accuracy 7.633%\n",
      "Epoch 1, Batch 515, LR 0.000086 Loss 16.974357, Accuracy 7.638%\n",
      "Epoch 1, Batch 516, LR 0.000086 Loss 16.973514, Accuracy 7.646%\n",
      "Epoch 1, Batch 517, LR 0.000086 Loss 16.973236, Accuracy 7.649%\n",
      "Epoch 1, Batch 518, LR 0.000087 Loss 16.972396, Accuracy 7.654%\n",
      "Epoch 1, Batch 519, LR 0.000087 Loss 16.971754, Accuracy 7.657%\n",
      "Epoch 1, Batch 520, LR 0.000087 Loss 16.971104, Accuracy 7.664%\n",
      "Epoch 1, Batch 521, LR 0.000087 Loss 16.970273, Accuracy 7.673%\n",
      "Epoch 1, Batch 522, LR 0.000087 Loss 16.969428, Accuracy 7.681%\n",
      "Epoch 1, Batch 523, LR 0.000087 Loss 16.968623, Accuracy 7.686%\n",
      "Epoch 1, Batch 524, LR 0.000087 Loss 16.967771, Accuracy 7.689%\n",
      "Epoch 1, Batch 525, LR 0.000087 Loss 16.967201, Accuracy 7.689%\n",
      "Epoch 1, Batch 526, LR 0.000087 Loss 16.965301, Accuracy 7.709%\n",
      "Epoch 1, Batch 527, LR 0.000087 Loss 16.964498, Accuracy 7.719%\n",
      "Epoch 1, Batch 528, LR 0.000087 Loss 16.963747, Accuracy 7.725%\n",
      "Epoch 1, Batch 529, LR 0.000087 Loss 16.962476, Accuracy 7.739%\n",
      "Epoch 1, Batch 530, LR 0.000087 Loss 16.961319, Accuracy 7.752%\n",
      "Epoch 1, Batch 531, LR 0.000087 Loss 16.959952, Accuracy 7.762%\n",
      "Epoch 1, Batch 532, LR 0.000087 Loss 16.958987, Accuracy 7.771%\n",
      "Epoch 1, Batch 533, LR 0.000087 Loss 16.958107, Accuracy 7.776%\n",
      "Epoch 1, Batch 534, LR 0.000087 Loss 16.956827, Accuracy 7.786%\n",
      "Epoch 1, Batch 535, LR 0.000087 Loss 16.956788, Accuracy 7.783%\n",
      "Epoch 1, Batch 536, LR 0.000087 Loss 16.955970, Accuracy 7.788%\n",
      "Epoch 1, Batch 537, LR 0.000087 Loss 16.955179, Accuracy 7.791%\n",
      "Epoch 1, Batch 538, LR 0.000087 Loss 16.954158, Accuracy 7.799%\n",
      "Epoch 1, Batch 539, LR 0.000088 Loss 16.953638, Accuracy 7.797%\n",
      "Epoch 1, Batch 540, LR 0.000088 Loss 16.953106, Accuracy 7.801%\n",
      "Epoch 1, Batch 541, LR 0.000088 Loss 16.952336, Accuracy 7.804%\n",
      "Epoch 1, Batch 542, LR 0.000088 Loss 16.952071, Accuracy 7.802%\n",
      "Epoch 1, Batch 543, LR 0.000088 Loss 16.952010, Accuracy 7.797%\n",
      "Epoch 1, Batch 544, LR 0.000088 Loss 16.950600, Accuracy 7.808%\n",
      "Epoch 1, Batch 545, LR 0.000088 Loss 16.949660, Accuracy 7.820%\n",
      "Epoch 1, Batch 546, LR 0.000088 Loss 16.948641, Accuracy 7.828%\n",
      "Epoch 1, Batch 547, LR 0.000088 Loss 16.948049, Accuracy 7.832%\n",
      "Epoch 1, Batch 548, LR 0.000088 Loss 16.947710, Accuracy 7.832%\n",
      "Epoch 1, Batch 549, LR 0.000088 Loss 16.946827, Accuracy 7.841%\n",
      "Epoch 1, Batch 550, LR 0.000088 Loss 16.945931, Accuracy 7.845%\n",
      "Epoch 1, Batch 551, LR 0.000088 Loss 16.945459, Accuracy 7.851%\n",
      "Epoch 1, Batch 552, LR 0.000088 Loss 16.944606, Accuracy 7.851%\n",
      "Epoch 1, Batch 553, LR 0.000088 Loss 16.943478, Accuracy 7.861%\n",
      "Epoch 1, Batch 554, LR 0.000088 Loss 16.943342, Accuracy 7.858%\n",
      "Epoch 1, Batch 555, LR 0.000088 Loss 16.942850, Accuracy 7.862%\n",
      "Epoch 1, Batch 556, LR 0.000088 Loss 16.941721, Accuracy 7.873%\n",
      "Epoch 1, Batch 557, LR 0.000088 Loss 16.940797, Accuracy 7.877%\n",
      "Epoch 1, Batch 558, LR 0.000088 Loss 16.940571, Accuracy 7.873%\n",
      "Epoch 1, Batch 559, LR 0.000088 Loss 16.939840, Accuracy 7.878%\n",
      "Epoch 1, Batch 560, LR 0.000089 Loss 16.938700, Accuracy 7.892%\n",
      "Epoch 1, Batch 561, LR 0.000089 Loss 16.938124, Accuracy 7.902%\n",
      "Epoch 1, Batch 562, LR 0.000089 Loss 16.937296, Accuracy 7.910%\n",
      "Epoch 1, Batch 563, LR 0.000089 Loss 16.936286, Accuracy 7.925%\n",
      "Epoch 1, Batch 564, LR 0.000089 Loss 16.935773, Accuracy 7.923%\n",
      "Epoch 1, Batch 565, LR 0.000089 Loss 16.935141, Accuracy 7.925%\n",
      "Epoch 1, Batch 566, LR 0.000089 Loss 16.934424, Accuracy 7.930%\n",
      "Epoch 1, Batch 567, LR 0.000089 Loss 16.933223, Accuracy 7.943%\n",
      "Epoch 1, Batch 568, LR 0.000089 Loss 16.932491, Accuracy 7.945%\n",
      "Epoch 1, Batch 569, LR 0.000089 Loss 16.931917, Accuracy 7.947%\n",
      "Epoch 1, Batch 570, LR 0.000089 Loss 16.930479, Accuracy 7.959%\n",
      "Epoch 1, Batch 571, LR 0.000089 Loss 16.929584, Accuracy 7.963%\n",
      "Epoch 1, Batch 572, LR 0.000089 Loss 16.928783, Accuracy 7.967%\n",
      "Epoch 1, Batch 573, LR 0.000089 Loss 16.927789, Accuracy 7.969%\n",
      "Epoch 1, Batch 574, LR 0.000089 Loss 16.926823, Accuracy 7.979%\n",
      "Epoch 1, Batch 575, LR 0.000089 Loss 16.926317, Accuracy 7.981%\n",
      "Epoch 1, Batch 576, LR 0.000089 Loss 16.924946, Accuracy 7.996%\n",
      "Epoch 1, Batch 577, LR 0.000089 Loss 16.923978, Accuracy 8.007%\n",
      "Epoch 1, Batch 578, LR 0.000089 Loss 16.923247, Accuracy 8.011%\n",
      "Epoch 1, Batch 579, LR 0.000089 Loss 16.922433, Accuracy 8.015%\n",
      "Epoch 1, Batch 580, LR 0.000089 Loss 16.921376, Accuracy 8.021%\n",
      "Epoch 1, Batch 581, LR 0.000089 Loss 16.920799, Accuracy 8.025%\n",
      "Epoch 1, Batch 582, LR 0.000089 Loss 16.919868, Accuracy 8.035%\n",
      "Epoch 1, Batch 583, LR 0.000090 Loss 16.919155, Accuracy 8.047%\n",
      "Epoch 1, Batch 584, LR 0.000090 Loss 16.917985, Accuracy 8.056%\n",
      "Epoch 1, Batch 585, LR 0.000090 Loss 16.916858, Accuracy 8.077%\n",
      "Epoch 1, Batch 586, LR 0.000090 Loss 16.915837, Accuracy 8.086%\n",
      "Epoch 1, Batch 587, LR 0.000090 Loss 16.915232, Accuracy 8.092%\n",
      "Epoch 1, Batch 588, LR 0.000090 Loss 16.914032, Accuracy 8.105%\n",
      "Epoch 1, Batch 589, LR 0.000090 Loss 16.913318, Accuracy 8.111%\n",
      "Epoch 1, Batch 590, LR 0.000090 Loss 16.912261, Accuracy 8.126%\n",
      "Epoch 1, Batch 591, LR 0.000090 Loss 16.911050, Accuracy 8.140%\n",
      "Epoch 1, Batch 592, LR 0.000090 Loss 16.910218, Accuracy 8.152%\n",
      "Epoch 1, Batch 593, LR 0.000090 Loss 16.908717, Accuracy 8.170%\n",
      "Epoch 1, Batch 594, LR 0.000090 Loss 16.907845, Accuracy 8.174%\n",
      "Epoch 1, Batch 595, LR 0.000090 Loss 16.906389, Accuracy 8.188%\n",
      "Epoch 1, Batch 596, LR 0.000090 Loss 16.905475, Accuracy 8.189%\n",
      "Epoch 1, Batch 597, LR 0.000090 Loss 16.904788, Accuracy 8.196%\n",
      "Epoch 1, Batch 598, LR 0.000090 Loss 16.904207, Accuracy 8.195%\n",
      "Epoch 1, Batch 599, LR 0.000090 Loss 16.903815, Accuracy 8.196%\n",
      "Epoch 1, Batch 600, LR 0.000090 Loss 16.902980, Accuracy 8.215%\n",
      "Epoch 1, Batch 601, LR 0.000090 Loss 16.901915, Accuracy 8.227%\n",
      "Epoch 1, Batch 602, LR 0.000090 Loss 16.900912, Accuracy 8.241%\n",
      "Epoch 1, Batch 603, LR 0.000090 Loss 16.899932, Accuracy 8.254%\n",
      "Epoch 1, Batch 604, LR 0.000090 Loss 16.898974, Accuracy 8.263%\n",
      "Epoch 1, Batch 605, LR 0.000090 Loss 16.898265, Accuracy 8.264%\n",
      "Epoch 1, Batch 606, LR 0.000090 Loss 16.897501, Accuracy 8.273%\n",
      "Epoch 1, Batch 607, LR 0.000091 Loss 16.896358, Accuracy 8.290%\n",
      "Epoch 1, Batch 608, LR 0.000091 Loss 16.895512, Accuracy 8.298%\n",
      "Epoch 1, Batch 609, LR 0.000091 Loss 16.894389, Accuracy 8.305%\n",
      "Epoch 1, Batch 610, LR 0.000091 Loss 16.893526, Accuracy 8.316%\n",
      "Epoch 1, Batch 611, LR 0.000091 Loss 16.892368, Accuracy 8.327%\n",
      "Epoch 1, Batch 612, LR 0.000091 Loss 16.890913, Accuracy 8.340%\n",
      "Epoch 1, Batch 613, LR 0.000091 Loss 16.889957, Accuracy 8.349%\n",
      "Epoch 1, Batch 614, LR 0.000091 Loss 16.888934, Accuracy 8.358%\n",
      "Epoch 1, Batch 615, LR 0.000091 Loss 16.888405, Accuracy 8.361%\n",
      "Epoch 1, Batch 616, LR 0.000091 Loss 16.887215, Accuracy 8.371%\n",
      "Epoch 1, Batch 617, LR 0.000091 Loss 16.885775, Accuracy 8.381%\n",
      "Epoch 1, Batch 618, LR 0.000091 Loss 16.884980, Accuracy 8.384%\n",
      "Epoch 1, Batch 619, LR 0.000091 Loss 16.884526, Accuracy 8.383%\n",
      "Epoch 1, Batch 620, LR 0.000091 Loss 16.883668, Accuracy 8.391%\n",
      "Epoch 1, Batch 621, LR 0.000091 Loss 16.882214, Accuracy 8.403%\n",
      "Epoch 1, Batch 622, LR 0.000091 Loss 16.881633, Accuracy 8.418%\n",
      "Epoch 1, Batch 623, LR 0.000091 Loss 16.880824, Accuracy 8.424%\n",
      "Epoch 1, Batch 624, LR 0.000091 Loss 16.879835, Accuracy 8.439%\n",
      "Epoch 1, Batch 625, LR 0.000091 Loss 16.878695, Accuracy 8.454%\n",
      "Epoch 1, Batch 626, LR 0.000091 Loss 16.878067, Accuracy 8.460%\n",
      "Epoch 1, Batch 627, LR 0.000091 Loss 16.876884, Accuracy 8.474%\n",
      "Epoch 1, Batch 628, LR 0.000091 Loss 16.875926, Accuracy 8.478%\n",
      "Epoch 1, Batch 629, LR 0.000091 Loss 16.874998, Accuracy 8.484%\n",
      "Epoch 1, Batch 630, LR 0.000091 Loss 16.874647, Accuracy 8.480%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 631, LR 0.000092 Loss 16.873674, Accuracy 8.486%\n",
      "Epoch 1, Batch 632, LR 0.000092 Loss 16.873634, Accuracy 8.484%\n",
      "Epoch 1, Batch 633, LR 0.000092 Loss 16.872683, Accuracy 8.490%\n",
      "Epoch 1, Batch 634, LR 0.000092 Loss 16.871850, Accuracy 8.488%\n",
      "Epoch 1, Batch 635, LR 0.000092 Loss 16.870902, Accuracy 8.495%\n",
      "Epoch 1, Batch 636, LR 0.000092 Loss 16.870193, Accuracy 8.502%\n",
      "Epoch 1, Batch 637, LR 0.000092 Loss 16.869264, Accuracy 8.502%\n",
      "Epoch 1, Batch 638, LR 0.000092 Loss 16.868362, Accuracy 8.512%\n",
      "Epoch 1, Batch 639, LR 0.000092 Loss 16.867268, Accuracy 8.522%\n",
      "Epoch 1, Batch 640, LR 0.000092 Loss 16.866487, Accuracy 8.527%\n",
      "Epoch 1, Batch 641, LR 0.000092 Loss 16.865610, Accuracy 8.530%\n",
      "Epoch 1, Batch 642, LR 0.000092 Loss 16.864479, Accuracy 8.543%\n",
      "Epoch 1, Batch 643, LR 0.000092 Loss 16.863636, Accuracy 8.556%\n",
      "Epoch 1, Batch 644, LR 0.000092 Loss 16.862743, Accuracy 8.565%\n",
      "Epoch 1, Batch 645, LR 0.000092 Loss 16.862138, Accuracy 8.566%\n",
      "Epoch 1, Batch 646, LR 0.000092 Loss 16.860916, Accuracy 8.577%\n",
      "Epoch 1, Batch 647, LR 0.000092 Loss 16.860483, Accuracy 8.582%\n",
      "Epoch 1, Batch 648, LR 0.000092 Loss 16.859629, Accuracy 8.582%\n",
      "Epoch 1, Batch 649, LR 0.000092 Loss 16.858776, Accuracy 8.591%\n",
      "Epoch 1, Batch 650, LR 0.000092 Loss 16.858218, Accuracy 8.593%\n",
      "Epoch 1, Batch 651, LR 0.000092 Loss 16.857757, Accuracy 8.594%\n",
      "Epoch 1, Batch 652, LR 0.000092 Loss 16.856404, Accuracy 8.603%\n",
      "Epoch 1, Batch 653, LR 0.000092 Loss 16.855660, Accuracy 8.615%\n",
      "Epoch 1, Batch 654, LR 0.000092 Loss 16.855345, Accuracy 8.620%\n",
      "Epoch 1, Batch 655, LR 0.000092 Loss 16.854413, Accuracy 8.630%\n",
      "Epoch 1, Batch 656, LR 0.000092 Loss 16.853904, Accuracy 8.633%\n",
      "Epoch 1, Batch 657, LR 0.000093 Loss 16.853347, Accuracy 8.635%\n",
      "Epoch 1, Batch 658, LR 0.000093 Loss 16.852526, Accuracy 8.642%\n",
      "Epoch 1, Batch 659, LR 0.000093 Loss 16.851309, Accuracy 8.659%\n",
      "Epoch 1, Batch 660, LR 0.000093 Loss 16.850480, Accuracy 8.671%\n",
      "Epoch 1, Batch 661, LR 0.000093 Loss 16.849737, Accuracy 8.676%\n",
      "Epoch 1, Batch 662, LR 0.000093 Loss 16.849379, Accuracy 8.680%\n",
      "Epoch 1, Batch 663, LR 0.000093 Loss 16.848785, Accuracy 8.680%\n",
      "Epoch 1, Batch 664, LR 0.000093 Loss 16.847738, Accuracy 8.689%\n",
      "Epoch 1, Batch 665, LR 0.000093 Loss 16.846922, Accuracy 8.698%\n",
      "Epoch 1, Batch 666, LR 0.000093 Loss 16.845681, Accuracy 8.712%\n",
      "Epoch 1, Batch 667, LR 0.000093 Loss 16.843893, Accuracy 8.731%\n",
      "Epoch 1, Batch 668, LR 0.000093 Loss 16.843165, Accuracy 8.729%\n",
      "Epoch 1, Batch 669, LR 0.000093 Loss 16.842778, Accuracy 8.732%\n",
      "Epoch 1, Batch 670, LR 0.000093 Loss 16.841666, Accuracy 8.736%\n",
      "Epoch 1, Batch 671, LR 0.000093 Loss 16.841114, Accuracy 8.743%\n",
      "Epoch 1, Batch 672, LR 0.000093 Loss 16.840177, Accuracy 8.758%\n",
      "Epoch 1, Batch 673, LR 0.000093 Loss 16.839738, Accuracy 8.764%\n",
      "Epoch 1, Batch 674, LR 0.000093 Loss 16.839313, Accuracy 8.764%\n",
      "Epoch 1, Batch 675, LR 0.000093 Loss 16.838730, Accuracy 8.769%\n",
      "Epoch 1, Batch 676, LR 0.000093 Loss 16.837291, Accuracy 8.784%\n",
      "Epoch 1, Batch 677, LR 0.000093 Loss 16.836753, Accuracy 8.784%\n",
      "Epoch 1, Batch 678, LR 0.000093 Loss 16.836167, Accuracy 8.786%\n",
      "Epoch 1, Batch 679, LR 0.000093 Loss 16.835642, Accuracy 8.794%\n",
      "Epoch 1, Batch 680, LR 0.000093 Loss 16.835263, Accuracy 8.799%\n",
      "Epoch 1, Batch 681, LR 0.000093 Loss 16.833872, Accuracy 8.808%\n",
      "Epoch 1, Batch 682, LR 0.000093 Loss 16.833242, Accuracy 8.817%\n",
      "Epoch 1, Batch 683, LR 0.000093 Loss 16.832536, Accuracy 8.820%\n",
      "Epoch 1, Batch 684, LR 0.000093 Loss 16.831291, Accuracy 8.829%\n",
      "Epoch 1, Batch 685, LR 0.000094 Loss 16.830787, Accuracy 8.834%\n",
      "Epoch 1, Batch 686, LR 0.000094 Loss 16.829784, Accuracy 8.839%\n",
      "Epoch 1, Batch 687, LR 0.000094 Loss 16.829083, Accuracy 8.844%\n",
      "Epoch 1, Batch 688, LR 0.000094 Loss 16.828352, Accuracy 8.850%\n",
      "Epoch 1, Batch 689, LR 0.000094 Loss 16.827314, Accuracy 8.858%\n",
      "Epoch 1, Batch 690, LR 0.000094 Loss 16.826723, Accuracy 8.861%\n",
      "Epoch 1, Batch 691, LR 0.000094 Loss 16.825947, Accuracy 8.873%\n",
      "Epoch 1, Batch 692, LR 0.000094 Loss 16.825037, Accuracy 8.882%\n",
      "Epoch 1, Batch 693, LR 0.000094 Loss 16.824283, Accuracy 8.886%\n",
      "Epoch 1, Batch 694, LR 0.000094 Loss 16.823931, Accuracy 8.890%\n",
      "Epoch 1, Batch 695, LR 0.000094 Loss 16.822909, Accuracy 8.896%\n",
      "Epoch 1, Batch 696, LR 0.000094 Loss 16.822630, Accuracy 8.900%\n",
      "Epoch 1, Batch 697, LR 0.000094 Loss 16.821584, Accuracy 8.905%\n",
      "Epoch 1, Batch 698, LR 0.000094 Loss 16.820938, Accuracy 8.906%\n",
      "Epoch 1, Batch 699, LR 0.000094 Loss 16.819982, Accuracy 8.912%\n",
      "Epoch 1, Batch 700, LR 0.000094 Loss 16.818910, Accuracy 8.925%\n",
      "Epoch 1, Batch 701, LR 0.000094 Loss 16.818322, Accuracy 8.929%\n",
      "Epoch 1, Batch 702, LR 0.000094 Loss 16.816951, Accuracy 8.938%\n",
      "Epoch 1, Batch 703, LR 0.000094 Loss 16.816550, Accuracy 8.937%\n",
      "Epoch 1, Batch 704, LR 0.000094 Loss 16.815700, Accuracy 8.940%\n",
      "Epoch 1, Batch 705, LR 0.000094 Loss 16.815504, Accuracy 8.938%\n",
      "Epoch 1, Batch 706, LR 0.000094 Loss 16.814143, Accuracy 8.950%\n",
      "Epoch 1, Batch 707, LR 0.000094 Loss 16.812466, Accuracy 8.966%\n",
      "Epoch 1, Batch 708, LR 0.000094 Loss 16.811590, Accuracy 8.971%\n",
      "Epoch 1, Batch 709, LR 0.000094 Loss 16.810743, Accuracy 8.974%\n",
      "Epoch 1, Batch 710, LR 0.000094 Loss 16.809659, Accuracy 8.985%\n",
      "Epoch 1, Batch 711, LR 0.000094 Loss 16.808760, Accuracy 8.994%\n",
      "Epoch 1, Batch 712, LR 0.000094 Loss 16.808536, Accuracy 8.994%\n",
      "Epoch 1, Batch 713, LR 0.000094 Loss 16.807891, Accuracy 9.001%\n",
      "Epoch 1, Batch 714, LR 0.000094 Loss 16.807210, Accuracy 9.008%\n",
      "Epoch 1, Batch 715, LR 0.000095 Loss 16.806895, Accuracy 9.009%\n",
      "Epoch 1, Batch 716, LR 0.000095 Loss 16.806700, Accuracy 9.014%\n",
      "Epoch 1, Batch 717, LR 0.000095 Loss 16.806005, Accuracy 9.021%\n",
      "Epoch 1, Batch 718, LR 0.000095 Loss 16.805512, Accuracy 9.024%\n",
      "Epoch 1, Batch 719, LR 0.000095 Loss 16.804742, Accuracy 9.037%\n",
      "Epoch 1, Batch 720, LR 0.000095 Loss 16.804564, Accuracy 9.043%\n",
      "Epoch 1, Batch 721, LR 0.000095 Loss 16.803756, Accuracy 9.048%\n",
      "Epoch 1, Batch 722, LR 0.000095 Loss 16.803110, Accuracy 9.047%\n",
      "Epoch 1, Batch 723, LR 0.000095 Loss 16.801885, Accuracy 9.054%\n",
      "Epoch 1, Batch 724, LR 0.000095 Loss 16.801395, Accuracy 9.063%\n",
      "Epoch 1, Batch 725, LR 0.000095 Loss 16.801014, Accuracy 9.067%\n",
      "Epoch 1, Batch 726, LR 0.000095 Loss 16.799811, Accuracy 9.076%\n",
      "Epoch 1, Batch 727, LR 0.000095 Loss 16.799339, Accuracy 9.075%\n",
      "Epoch 1, Batch 728, LR 0.000095 Loss 16.798678, Accuracy 9.078%\n",
      "Epoch 1, Batch 729, LR 0.000095 Loss 16.797980, Accuracy 9.081%\n",
      "Epoch 1, Batch 730, LR 0.000095 Loss 16.796774, Accuracy 9.091%\n",
      "Epoch 1, Batch 731, LR 0.000095 Loss 16.795875, Accuracy 9.101%\n",
      "Epoch 1, Batch 732, LR 0.000095 Loss 16.795570, Accuracy 9.102%\n",
      "Epoch 1, Batch 733, LR 0.000095 Loss 16.795252, Accuracy 9.101%\n",
      "Epoch 1, Batch 734, LR 0.000095 Loss 16.794442, Accuracy 9.109%\n",
      "Epoch 1, Batch 735, LR 0.000095 Loss 16.793493, Accuracy 9.114%\n",
      "Epoch 1, Batch 736, LR 0.000095 Loss 16.793031, Accuracy 9.119%\n",
      "Epoch 1, Batch 737, LR 0.000095 Loss 16.791839, Accuracy 9.130%\n",
      "Epoch 1, Batch 738, LR 0.000095 Loss 16.791479, Accuracy 9.134%\n",
      "Epoch 1, Batch 739, LR 0.000095 Loss 16.790871, Accuracy 9.139%\n",
      "Epoch 1, Batch 740, LR 0.000095 Loss 16.790024, Accuracy 9.145%\n",
      "Epoch 1, Batch 741, LR 0.000095 Loss 16.789414, Accuracy 9.146%\n",
      "Epoch 1, Batch 742, LR 0.000095 Loss 16.788728, Accuracy 9.151%\n",
      "Epoch 1, Batch 743, LR 0.000095 Loss 16.788138, Accuracy 9.153%\n",
      "Epoch 1, Batch 744, LR 0.000095 Loss 16.787302, Accuracy 9.162%\n",
      "Epoch 1, Batch 745, LR 0.000095 Loss 16.786791, Accuracy 9.163%\n",
      "Epoch 1, Batch 746, LR 0.000095 Loss 16.785897, Accuracy 9.173%\n",
      "Epoch 1, Batch 747, LR 0.000096 Loss 16.785193, Accuracy 9.179%\n",
      "Epoch 1, Batch 748, LR 0.000096 Loss 16.784405, Accuracy 9.188%\n",
      "Epoch 1, Batch 749, LR 0.000096 Loss 16.783473, Accuracy 9.198%\n",
      "Epoch 1, Batch 750, LR 0.000096 Loss 16.782818, Accuracy 9.206%\n",
      "Epoch 1, Batch 751, LR 0.000096 Loss 16.781830, Accuracy 9.213%\n",
      "Epoch 1, Batch 752, LR 0.000096 Loss 16.780588, Accuracy 9.229%\n",
      "Epoch 1, Batch 753, LR 0.000096 Loss 16.779923, Accuracy 9.235%\n",
      "Epoch 1, Batch 754, LR 0.000096 Loss 16.779190, Accuracy 9.238%\n",
      "Epoch 1, Batch 755, LR 0.000096 Loss 16.778758, Accuracy 9.239%\n",
      "Epoch 1, Batch 756, LR 0.000096 Loss 16.777977, Accuracy 9.248%\n",
      "Epoch 1, Batch 757, LR 0.000096 Loss 16.777345, Accuracy 9.255%\n",
      "Epoch 1, Batch 758, LR 0.000096 Loss 16.776202, Accuracy 9.262%\n",
      "Epoch 1, Batch 759, LR 0.000096 Loss 16.775281, Accuracy 9.266%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 760, LR 0.000096 Loss 16.774913, Accuracy 9.267%\n",
      "Epoch 1, Batch 761, LR 0.000096 Loss 16.774210, Accuracy 9.271%\n",
      "Epoch 1, Batch 762, LR 0.000096 Loss 16.773671, Accuracy 9.273%\n",
      "Epoch 1, Batch 763, LR 0.000096 Loss 16.772683, Accuracy 9.280%\n",
      "Epoch 1, Batch 764, LR 0.000096 Loss 16.771640, Accuracy 9.290%\n",
      "Epoch 1, Batch 765, LR 0.000096 Loss 16.770962, Accuracy 9.291%\n",
      "Epoch 1, Batch 766, LR 0.000096 Loss 16.770188, Accuracy 9.297%\n",
      "Epoch 1, Batch 767, LR 0.000096 Loss 16.769009, Accuracy 9.309%\n",
      "Epoch 1, Batch 768, LR 0.000096 Loss 16.768128, Accuracy 9.320%\n",
      "Epoch 1, Batch 769, LR 0.000096 Loss 16.767366, Accuracy 9.324%\n",
      "Epoch 1, Batch 770, LR 0.000096 Loss 16.766521, Accuracy 9.332%\n",
      "Epoch 1, Batch 771, LR 0.000096 Loss 16.765345, Accuracy 9.344%\n",
      "Epoch 1, Batch 772, LR 0.000096 Loss 16.764507, Accuracy 9.357%\n",
      "Epoch 1, Batch 773, LR 0.000096 Loss 16.763934, Accuracy 9.355%\n",
      "Epoch 1, Batch 774, LR 0.000096 Loss 16.763002, Accuracy 9.363%\n",
      "Epoch 1, Batch 775, LR 0.000096 Loss 16.761980, Accuracy 9.376%\n",
      "Epoch 1, Batch 776, LR 0.000096 Loss 16.761485, Accuracy 9.376%\n",
      "Epoch 1, Batch 777, LR 0.000096 Loss 16.760427, Accuracy 9.380%\n",
      "Epoch 1, Batch 778, LR 0.000096 Loss 16.759466, Accuracy 9.388%\n",
      "Epoch 1, Batch 779, LR 0.000096 Loss 16.758857, Accuracy 9.391%\n",
      "Epoch 1, Batch 780, LR 0.000096 Loss 16.757828, Accuracy 9.402%\n",
      "Epoch 1, Batch 781, LR 0.000096 Loss 16.756969, Accuracy 9.412%\n",
      "Epoch 1, Batch 782, LR 0.000096 Loss 16.756455, Accuracy 9.410%\n",
      "Epoch 1, Batch 783, LR 0.000097 Loss 16.755729, Accuracy 9.419%\n",
      "Epoch 1, Batch 784, LR 0.000097 Loss 16.754515, Accuracy 9.438%\n",
      "Epoch 1, Batch 785, LR 0.000097 Loss 16.753664, Accuracy 9.441%\n",
      "Epoch 1, Batch 786, LR 0.000097 Loss 16.753114, Accuracy 9.451%\n",
      "Epoch 1, Batch 787, LR 0.000097 Loss 16.752192, Accuracy 9.455%\n",
      "Epoch 1, Batch 788, LR 0.000097 Loss 16.751264, Accuracy 9.466%\n",
      "Epoch 1, Batch 789, LR 0.000097 Loss 16.750786, Accuracy 9.472%\n",
      "Epoch 1, Batch 790, LR 0.000097 Loss 16.749543, Accuracy 9.481%\n",
      "Epoch 1, Batch 791, LR 0.000097 Loss 16.748880, Accuracy 9.487%\n",
      "Epoch 1, Batch 792, LR 0.000097 Loss 16.748469, Accuracy 9.490%\n",
      "Epoch 1, Batch 793, LR 0.000097 Loss 16.747642, Accuracy 9.501%\n",
      "Epoch 1, Batch 794, LR 0.000097 Loss 16.747078, Accuracy 9.500%\n",
      "Epoch 1, Batch 795, LR 0.000097 Loss 16.746322, Accuracy 9.509%\n",
      "Epoch 1, Batch 796, LR 0.000097 Loss 16.745708, Accuracy 9.518%\n",
      "Epoch 1, Batch 797, LR 0.000097 Loss 16.744690, Accuracy 9.531%\n",
      "Epoch 1, Batch 798, LR 0.000097 Loss 16.744172, Accuracy 9.535%\n",
      "Epoch 1, Batch 799, LR 0.000097 Loss 16.743317, Accuracy 9.543%\n",
      "Epoch 1, Batch 800, LR 0.000097 Loss 16.742374, Accuracy 9.555%\n",
      "Epoch 1, Batch 801, LR 0.000097 Loss 16.741721, Accuracy 9.558%\n",
      "Epoch 1, Batch 802, LR 0.000097 Loss 16.740482, Accuracy 9.570%\n",
      "Epoch 1, Batch 803, LR 0.000097 Loss 16.739934, Accuracy 9.574%\n",
      "Epoch 1, Batch 804, LR 0.000097 Loss 16.739498, Accuracy 9.576%\n",
      "Epoch 1, Batch 805, LR 0.000097 Loss 16.738704, Accuracy 9.579%\n",
      "Epoch 1, Batch 806, LR 0.000097 Loss 16.738052, Accuracy 9.586%\n",
      "Epoch 1, Batch 807, LR 0.000097 Loss 16.737088, Accuracy 9.597%\n",
      "Epoch 1, Batch 808, LR 0.000097 Loss 16.736326, Accuracy 9.598%\n",
      "Epoch 1, Batch 809, LR 0.000097 Loss 16.735634, Accuracy 9.606%\n",
      "Epoch 1, Batch 810, LR 0.000097 Loss 16.734749, Accuracy 9.613%\n",
      "Epoch 1, Batch 811, LR 0.000097 Loss 16.733743, Accuracy 9.625%\n",
      "Epoch 1, Batch 812, LR 0.000097 Loss 16.732989, Accuracy 9.628%\n",
      "Epoch 1, Batch 813, LR 0.000097 Loss 16.732354, Accuracy 9.635%\n",
      "Epoch 1, Batch 814, LR 0.000097 Loss 16.732123, Accuracy 9.640%\n",
      "Epoch 1, Batch 815, LR 0.000097 Loss 16.731153, Accuracy 9.649%\n",
      "Epoch 1, Batch 816, LR 0.000097 Loss 16.730128, Accuracy 9.653%\n",
      "Epoch 1, Batch 817, LR 0.000097 Loss 16.729067, Accuracy 9.662%\n",
      "Epoch 1, Batch 818, LR 0.000097 Loss 16.728355, Accuracy 9.666%\n",
      "Epoch 1, Batch 819, LR 0.000097 Loss 16.727401, Accuracy 9.673%\n",
      "Epoch 1, Batch 820, LR 0.000097 Loss 16.726484, Accuracy 9.677%\n",
      "Epoch 1, Batch 821, LR 0.000097 Loss 16.725946, Accuracy 9.682%\n",
      "Epoch 1, Batch 822, LR 0.000097 Loss 16.725057, Accuracy 9.689%\n",
      "Epoch 1, Batch 823, LR 0.000097 Loss 16.724381, Accuracy 9.693%\n",
      "Epoch 1, Batch 824, LR 0.000098 Loss 16.723874, Accuracy 9.695%\n",
      "Epoch 1, Batch 825, LR 0.000098 Loss 16.722975, Accuracy 9.706%\n",
      "Epoch 1, Batch 826, LR 0.000098 Loss 16.721898, Accuracy 9.720%\n",
      "Epoch 1, Batch 827, LR 0.000098 Loss 16.721259, Accuracy 9.723%\n",
      "Epoch 1, Batch 828, LR 0.000098 Loss 16.720316, Accuracy 9.730%\n",
      "Epoch 1, Batch 829, LR 0.000098 Loss 16.719471, Accuracy 9.739%\n",
      "Epoch 1, Batch 830, LR 0.000098 Loss 16.719052, Accuracy 9.741%\n",
      "Epoch 1, Batch 831, LR 0.000098 Loss 16.717358, Accuracy 9.753%\n",
      "Epoch 1, Batch 832, LR 0.000098 Loss 16.716283, Accuracy 9.762%\n",
      "Epoch 1, Batch 833, LR 0.000098 Loss 16.715674, Accuracy 9.765%\n",
      "Epoch 1, Batch 834, LR 0.000098 Loss 16.714641, Accuracy 9.772%\n",
      "Epoch 1, Batch 835, LR 0.000098 Loss 16.714044, Accuracy 9.775%\n",
      "Epoch 1, Batch 836, LR 0.000098 Loss 16.713005, Accuracy 9.786%\n",
      "Epoch 1, Batch 837, LR 0.000098 Loss 16.712533, Accuracy 9.793%\n",
      "Epoch 1, Batch 838, LR 0.000098 Loss 16.712078, Accuracy 9.797%\n",
      "Epoch 1, Batch 839, LR 0.000098 Loss 16.710989, Accuracy 9.810%\n",
      "Epoch 1, Batch 840, LR 0.000098 Loss 16.710352, Accuracy 9.813%\n",
      "Epoch 1, Batch 841, LR 0.000098 Loss 16.710018, Accuracy 9.818%\n",
      "Epoch 1, Batch 842, LR 0.000098 Loss 16.709071, Accuracy 9.822%\n",
      "Epoch 1, Batch 843, LR 0.000098 Loss 16.707916, Accuracy 9.833%\n",
      "Epoch 1, Batch 844, LR 0.000098 Loss 16.706896, Accuracy 9.839%\n",
      "Epoch 1, Batch 845, LR 0.000098 Loss 16.705999, Accuracy 9.849%\n",
      "Epoch 1, Batch 846, LR 0.000098 Loss 16.704927, Accuracy 9.859%\n",
      "Epoch 1, Batch 847, LR 0.000098 Loss 16.704209, Accuracy 9.860%\n",
      "Epoch 1, Batch 848, LR 0.000098 Loss 16.703348, Accuracy 9.866%\n",
      "Epoch 1, Batch 849, LR 0.000098 Loss 16.703002, Accuracy 9.871%\n",
      "Epoch 1, Batch 850, LR 0.000098 Loss 16.701670, Accuracy 9.886%\n",
      "Epoch 1, Batch 851, LR 0.000098 Loss 16.700910, Accuracy 9.890%\n",
      "Epoch 1, Batch 852, LR 0.000098 Loss 16.700420, Accuracy 9.898%\n",
      "Epoch 1, Batch 853, LR 0.000098 Loss 16.699300, Accuracy 9.906%\n",
      "Epoch 1, Batch 854, LR 0.000098 Loss 16.698308, Accuracy 9.913%\n",
      "Epoch 1, Batch 855, LR 0.000098 Loss 16.697558, Accuracy 9.915%\n",
      "Epoch 1, Batch 856, LR 0.000098 Loss 16.696192, Accuracy 9.928%\n",
      "Epoch 1, Batch 857, LR 0.000098 Loss 16.695079, Accuracy 9.937%\n",
      "Epoch 1, Batch 858, LR 0.000098 Loss 16.694564, Accuracy 9.934%\n",
      "Epoch 1, Batch 859, LR 0.000098 Loss 16.693928, Accuracy 9.937%\n",
      "Epoch 1, Batch 860, LR 0.000098 Loss 16.693198, Accuracy 9.945%\n",
      "Epoch 1, Batch 861, LR 0.000098 Loss 16.691944, Accuracy 9.960%\n",
      "Epoch 1, Batch 862, LR 0.000098 Loss 16.691468, Accuracy 9.964%\n",
      "Epoch 1, Batch 863, LR 0.000098 Loss 16.690650, Accuracy 9.970%\n",
      "Epoch 1, Batch 864, LR 0.000098 Loss 16.689899, Accuracy 9.970%\n",
      "Epoch 1, Batch 865, LR 0.000098 Loss 16.689179, Accuracy 9.978%\n",
      "Epoch 1, Batch 866, LR 0.000098 Loss 16.688568, Accuracy 9.980%\n",
      "Epoch 1, Batch 867, LR 0.000098 Loss 16.688024, Accuracy 9.985%\n",
      "Epoch 1, Batch 868, LR 0.000098 Loss 16.686878, Accuracy 9.992%\n",
      "Epoch 1, Batch 869, LR 0.000098 Loss 16.686546, Accuracy 9.994%\n",
      "Epoch 1, Batch 870, LR 0.000098 Loss 16.685627, Accuracy 10.001%\n",
      "Epoch 1, Batch 871, LR 0.000098 Loss 16.685360, Accuracy 10.004%\n",
      "Epoch 1, Batch 872, LR 0.000098 Loss 16.684499, Accuracy 10.006%\n",
      "Epoch 1, Batch 873, LR 0.000098 Loss 16.683917, Accuracy 10.008%\n",
      "Epoch 1, Batch 874, LR 0.000098 Loss 16.683092, Accuracy 10.016%\n",
      "Epoch 1, Batch 875, LR 0.000099 Loss 16.682195, Accuracy 10.023%\n",
      "Epoch 1, Batch 876, LR 0.000099 Loss 16.681632, Accuracy 10.030%\n",
      "Epoch 1, Batch 877, LR 0.000099 Loss 16.680493, Accuracy 10.038%\n",
      "Epoch 1, Batch 878, LR 0.000099 Loss 16.679797, Accuracy 10.042%\n",
      "Epoch 1, Batch 879, LR 0.000099 Loss 16.679165, Accuracy 10.045%\n",
      "Epoch 1, Batch 880, LR 0.000099 Loss 16.678164, Accuracy 10.053%\n",
      "Epoch 1, Batch 881, LR 0.000099 Loss 16.677308, Accuracy 10.062%\n",
      "Epoch 1, Batch 882, LR 0.000099 Loss 16.676376, Accuracy 10.070%\n",
      "Epoch 1, Batch 883, LR 0.000099 Loss 16.675408, Accuracy 10.080%\n",
      "Epoch 1, Batch 884, LR 0.000099 Loss 16.674576, Accuracy 10.084%\n",
      "Epoch 1, Batch 885, LR 0.000099 Loss 16.673975, Accuracy 10.093%\n",
      "Epoch 1, Batch 886, LR 0.000099 Loss 16.672672, Accuracy 10.109%\n",
      "Epoch 1, Batch 887, LR 0.000099 Loss 16.672060, Accuracy 10.113%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 888, LR 0.000099 Loss 16.670947, Accuracy 10.121%\n",
      "Epoch 1, Batch 889, LR 0.000099 Loss 16.670025, Accuracy 10.126%\n",
      "Epoch 1, Batch 890, LR 0.000099 Loss 16.669566, Accuracy 10.127%\n",
      "Epoch 1, Batch 891, LR 0.000099 Loss 16.668949, Accuracy 10.133%\n",
      "Epoch 1, Batch 892, LR 0.000099 Loss 16.668113, Accuracy 10.134%\n",
      "Epoch 1, Batch 893, LR 0.000099 Loss 16.667181, Accuracy 10.141%\n",
      "Epoch 1, Batch 894, LR 0.000099 Loss 16.666338, Accuracy 10.148%\n",
      "Epoch 1, Batch 895, LR 0.000099 Loss 16.665668, Accuracy 10.153%\n",
      "Epoch 1, Batch 896, LR 0.000099 Loss 16.664887, Accuracy 10.161%\n",
      "Epoch 1, Batch 897, LR 0.000099 Loss 16.663892, Accuracy 10.170%\n",
      "Epoch 1, Batch 898, LR 0.000099 Loss 16.663329, Accuracy 10.174%\n",
      "Epoch 1, Batch 899, LR 0.000099 Loss 16.662453, Accuracy 10.181%\n",
      "Epoch 1, Batch 900, LR 0.000099 Loss 16.661936, Accuracy 10.184%\n",
      "Epoch 1, Batch 901, LR 0.000099 Loss 16.661086, Accuracy 10.189%\n",
      "Epoch 1, Batch 902, LR 0.000099 Loss 16.660500, Accuracy 10.193%\n",
      "Epoch 1, Batch 903, LR 0.000099 Loss 16.659962, Accuracy 10.195%\n",
      "Epoch 1, Batch 904, LR 0.000099 Loss 16.659581, Accuracy 10.198%\n",
      "Epoch 1, Batch 905, LR 0.000099 Loss 16.659080, Accuracy 10.201%\n",
      "Epoch 1, Batch 906, LR 0.000099 Loss 16.658295, Accuracy 10.207%\n",
      "Epoch 1, Batch 907, LR 0.000099 Loss 16.657318, Accuracy 10.216%\n",
      "Epoch 1, Batch 908, LR 0.000099 Loss 16.656804, Accuracy 10.223%\n",
      "Epoch 1, Batch 909, LR 0.000099 Loss 16.656015, Accuracy 10.228%\n",
      "Epoch 1, Batch 910, LR 0.000099 Loss 16.655445, Accuracy 10.236%\n",
      "Epoch 1, Batch 911, LR 0.000099 Loss 16.654436, Accuracy 10.245%\n",
      "Epoch 1, Batch 912, LR 0.000099 Loss 16.653618, Accuracy 10.254%\n",
      "Epoch 1, Batch 913, LR 0.000099 Loss 16.653040, Accuracy 10.259%\n",
      "Epoch 1, Batch 914, LR 0.000099 Loss 16.652565, Accuracy 10.261%\n",
      "Epoch 1, Batch 915, LR 0.000099 Loss 16.651475, Accuracy 10.275%\n",
      "Epoch 1, Batch 916, LR 0.000099 Loss 16.650696, Accuracy 10.279%\n",
      "Epoch 1, Batch 917, LR 0.000099 Loss 16.649971, Accuracy 10.283%\n",
      "Epoch 1, Batch 918, LR 0.000099 Loss 16.649164, Accuracy 10.290%\n",
      "Epoch 1, Batch 919, LR 0.000099 Loss 16.647895, Accuracy 10.307%\n",
      "Epoch 1, Batch 920, LR 0.000099 Loss 16.647288, Accuracy 10.309%\n",
      "Epoch 1, Batch 921, LR 0.000099 Loss 16.646205, Accuracy 10.317%\n",
      "Epoch 1, Batch 922, LR 0.000099 Loss 16.645543, Accuracy 10.322%\n",
      "Epoch 1, Batch 923, LR 0.000099 Loss 16.644803, Accuracy 10.335%\n",
      "Epoch 1, Batch 924, LR 0.000099 Loss 16.643895, Accuracy 10.346%\n",
      "Epoch 1, Batch 925, LR 0.000099 Loss 16.643055, Accuracy 10.351%\n",
      "Epoch 1, Batch 926, LR 0.000099 Loss 16.642447, Accuracy 10.356%\n",
      "Epoch 1, Batch 927, LR 0.000099 Loss 16.641633, Accuracy 10.363%\n",
      "Epoch 1, Batch 928, LR 0.000099 Loss 16.640700, Accuracy 10.372%\n",
      "Epoch 1, Batch 929, LR 0.000099 Loss 16.640042, Accuracy 10.376%\n",
      "Epoch 1, Batch 930, LR 0.000099 Loss 16.639566, Accuracy 10.377%\n",
      "Epoch 1, Batch 931, LR 0.000099 Loss 16.638792, Accuracy 10.387%\n",
      "Epoch 1, Batch 932, LR 0.000099 Loss 16.638257, Accuracy 10.390%\n",
      "Epoch 1, Batch 933, LR 0.000099 Loss 16.637403, Accuracy 10.399%\n",
      "Epoch 1, Batch 934, LR 0.000099 Loss 16.636602, Accuracy 10.401%\n",
      "Epoch 1, Batch 935, LR 0.000099 Loss 16.635932, Accuracy 10.405%\n",
      "Epoch 1, Batch 936, LR 0.000099 Loss 16.634977, Accuracy 10.416%\n",
      "Epoch 1, Batch 937, LR 0.000099 Loss 16.634573, Accuracy 10.420%\n",
      "Epoch 1, Batch 938, LR 0.000099 Loss 16.634002, Accuracy 10.424%\n",
      "Epoch 1, Batch 939, LR 0.000099 Loss 16.633377, Accuracy 10.427%\n",
      "Epoch 1, Batch 940, LR 0.000099 Loss 16.632721, Accuracy 10.434%\n",
      "Epoch 1, Batch 941, LR 0.000099 Loss 16.632007, Accuracy 10.440%\n",
      "Epoch 1, Batch 942, LR 0.000099 Loss 16.631396, Accuracy 10.447%\n",
      "Epoch 1, Batch 943, LR 0.000099 Loss 16.630725, Accuracy 10.451%\n",
      "Epoch 1, Batch 944, LR 0.000099 Loss 16.629759, Accuracy 10.459%\n",
      "Epoch 1, Batch 945, LR 0.000099 Loss 16.628944, Accuracy 10.465%\n",
      "Epoch 1, Batch 946, LR 0.000099 Loss 16.628292, Accuracy 10.468%\n",
      "Epoch 1, Batch 947, LR 0.000099 Loss 16.627706, Accuracy 10.472%\n",
      "Epoch 1, Batch 948, LR 0.000100 Loss 16.627097, Accuracy 10.475%\n",
      "Epoch 1, Batch 949, LR 0.000100 Loss 16.626349, Accuracy 10.486%\n",
      "Epoch 1, Batch 950, LR 0.000100 Loss 16.625552, Accuracy 10.495%\n",
      "Epoch 1, Batch 951, LR 0.000100 Loss 16.625030, Accuracy 10.495%\n",
      "Epoch 1, Batch 952, LR 0.000100 Loss 16.624186, Accuracy 10.504%\n",
      "Epoch 1, Batch 953, LR 0.000100 Loss 16.623639, Accuracy 10.505%\n",
      "Epoch 1, Batch 954, LR 0.000100 Loss 16.623057, Accuracy 10.508%\n",
      "Epoch 1, Batch 955, LR 0.000100 Loss 16.622565, Accuracy 10.510%\n",
      "Epoch 1, Batch 956, LR 0.000100 Loss 16.621738, Accuracy 10.513%\n",
      "Epoch 1, Batch 957, LR 0.000100 Loss 16.620963, Accuracy 10.515%\n",
      "Epoch 1, Batch 958, LR 0.000100 Loss 16.620369, Accuracy 10.518%\n",
      "Epoch 1, Batch 959, LR 0.000100 Loss 16.619639, Accuracy 10.524%\n",
      "Epoch 1, Batch 960, LR 0.000100 Loss 16.618160, Accuracy 10.537%\n",
      "Epoch 1, Batch 961, LR 0.000100 Loss 16.617199, Accuracy 10.550%\n",
      "Epoch 1, Batch 962, LR 0.000100 Loss 16.616465, Accuracy 10.551%\n",
      "Epoch 1, Batch 963, LR 0.000100 Loss 16.615298, Accuracy 10.558%\n",
      "Epoch 1, Batch 964, LR 0.000100 Loss 16.614395, Accuracy 10.568%\n",
      "Epoch 1, Batch 965, LR 0.000100 Loss 16.613649, Accuracy 10.570%\n",
      "Epoch 1, Batch 966, LR 0.000100 Loss 16.612216, Accuracy 10.586%\n",
      "Epoch 1, Batch 967, LR 0.000100 Loss 16.611421, Accuracy 10.592%\n",
      "Epoch 1, Batch 968, LR 0.000100 Loss 16.610789, Accuracy 10.599%\n",
      "Epoch 1, Batch 969, LR 0.000100 Loss 16.609866, Accuracy 10.609%\n",
      "Epoch 1, Batch 970, LR 0.000100 Loss 16.608881, Accuracy 10.619%\n",
      "Epoch 1, Batch 971, LR 0.000100 Loss 16.607820, Accuracy 10.627%\n",
      "Epoch 1, Batch 972, LR 0.000100 Loss 16.606496, Accuracy 10.642%\n",
      "Epoch 1, Batch 973, LR 0.000100 Loss 16.605987, Accuracy 10.647%\n",
      "Epoch 1, Batch 974, LR 0.000100 Loss 16.604793, Accuracy 10.658%\n",
      "Epoch 1, Batch 975, LR 0.000100 Loss 16.603899, Accuracy 10.660%\n",
      "Epoch 1, Batch 976, LR 0.000100 Loss 16.603082, Accuracy 10.669%\n",
      "Epoch 1, Batch 977, LR 0.000100 Loss 16.602533, Accuracy 10.674%\n",
      "Epoch 1, Batch 978, LR 0.000100 Loss 16.601504, Accuracy 10.679%\n",
      "Epoch 1, Batch 979, LR 0.000100 Loss 16.600936, Accuracy 10.681%\n",
      "Epoch 1, Batch 980, LR 0.000100 Loss 16.600270, Accuracy 10.690%\n",
      "Epoch 1, Batch 981, LR 0.000100 Loss 16.599402, Accuracy 10.701%\n",
      "Epoch 1, Batch 982, LR 0.000100 Loss 16.598633, Accuracy 10.705%\n",
      "Epoch 1, Batch 983, LR 0.000100 Loss 16.598169, Accuracy 10.709%\n",
      "Epoch 1, Batch 984, LR 0.000100 Loss 16.597387, Accuracy 10.715%\n",
      "Epoch 1, Batch 985, LR 0.000100 Loss 16.596682, Accuracy 10.719%\n",
      "Epoch 1, Batch 986, LR 0.000100 Loss 16.595770, Accuracy 10.725%\n",
      "Epoch 1, Batch 987, LR 0.000100 Loss 16.594704, Accuracy 10.736%\n",
      "Epoch 1, Batch 988, LR 0.000100 Loss 16.593826, Accuracy 10.745%\n",
      "Epoch 1, Batch 989, LR 0.000100 Loss 16.592685, Accuracy 10.757%\n",
      "Epoch 1, Batch 990, LR 0.000100 Loss 16.592257, Accuracy 10.758%\n",
      "Epoch 1, Batch 991, LR 0.000100 Loss 16.591667, Accuracy 10.762%\n",
      "Epoch 1, Batch 992, LR 0.000100 Loss 16.590886, Accuracy 10.765%\n",
      "Epoch 1, Batch 993, LR 0.000100 Loss 16.589943, Accuracy 10.773%\n",
      "Epoch 1, Batch 994, LR 0.000100 Loss 16.589131, Accuracy 10.786%\n",
      "Epoch 1, Batch 995, LR 0.000100 Loss 16.588713, Accuracy 10.787%\n",
      "Epoch 1, Batch 996, LR 0.000100 Loss 16.587710, Accuracy 10.792%\n",
      "Epoch 1, Batch 997, LR 0.000100 Loss 16.586709, Accuracy 10.796%\n",
      "Epoch 1, Batch 998, LR 0.000100 Loss 16.586076, Accuracy 10.800%\n",
      "Epoch 1, Batch 999, LR 0.000100 Loss 16.585087, Accuracy 10.805%\n",
      "Epoch 1, Batch 1000, LR 0.000100 Loss 16.584660, Accuracy 10.810%\n",
      "Epoch 1, Batch 1001, LR 0.000100 Loss 16.583827, Accuracy 10.813%\n",
      "Epoch 1, Batch 1002, LR 0.000100 Loss 16.582480, Accuracy 10.829%\n",
      "Epoch 1, Batch 1003, LR 0.000100 Loss 16.581701, Accuracy 10.835%\n",
      "Epoch 1, Batch 1004, LR 0.000100 Loss 16.580811, Accuracy 10.839%\n",
      "Epoch 1, Batch 1005, LR 0.000100 Loss 16.579903, Accuracy 10.850%\n",
      "Epoch 1, Batch 1006, LR 0.000100 Loss 16.579046, Accuracy 10.858%\n",
      "Epoch 1, Batch 1007, LR 0.000100 Loss 16.578205, Accuracy 10.865%\n",
      "Epoch 1, Batch 1008, LR 0.000100 Loss 16.577296, Accuracy 10.872%\n",
      "Epoch 1, Batch 1009, LR 0.000100 Loss 16.576503, Accuracy 10.879%\n",
      "Epoch 1, Batch 1010, LR 0.000100 Loss 16.575517, Accuracy 10.885%\n",
      "Epoch 1, Batch 1011, LR 0.000100 Loss 16.575021, Accuracy 10.887%\n",
      "Epoch 1, Batch 1012, LR 0.000100 Loss 16.573521, Accuracy 10.901%\n",
      "Epoch 1, Batch 1013, LR 0.000100 Loss 16.573081, Accuracy 10.905%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 1014, LR 0.000100 Loss 16.572319, Accuracy 10.909%\n",
      "Epoch 1, Batch 1015, LR 0.000100 Loss 16.571452, Accuracy 10.919%\n",
      "Epoch 1, Batch 1016, LR 0.000100 Loss 16.570394, Accuracy 10.930%\n",
      "Epoch 1, Batch 1017, LR 0.000100 Loss 16.569870, Accuracy 10.934%\n",
      "Epoch 1, Batch 1018, LR 0.000100 Loss 16.569241, Accuracy 10.939%\n",
      "Epoch 1, Batch 1019, LR 0.000100 Loss 16.568660, Accuracy 10.942%\n",
      "Epoch 1, Batch 1020, LR 0.000100 Loss 16.567883, Accuracy 10.952%\n",
      "Epoch 1, Batch 1021, LR 0.000100 Loss 16.567169, Accuracy 10.957%\n",
      "Epoch 1, Batch 1022, LR 0.000100 Loss 16.566555, Accuracy 10.965%\n",
      "Epoch 1, Batch 1023, LR 0.000100 Loss 16.565859, Accuracy 10.970%\n",
      "Epoch 1, Batch 1024, LR 0.000100 Loss 16.565119, Accuracy 10.976%\n",
      "Epoch 1, Batch 1025, LR 0.000100 Loss 16.564100, Accuracy 10.986%\n",
      "Epoch 1, Batch 1026, LR 0.000100 Loss 16.563402, Accuracy 10.989%\n",
      "Epoch 1, Batch 1027, LR 0.000100 Loss 16.562215, Accuracy 11.004%\n",
      "Epoch 1, Batch 1028, LR 0.000100 Loss 16.561842, Accuracy 11.004%\n",
      "Epoch 1, Batch 1029, LR 0.000100 Loss 16.560862, Accuracy 11.010%\n",
      "Epoch 1, Batch 1030, LR 0.000100 Loss 16.560173, Accuracy 11.016%\n",
      "Epoch 1, Batch 1031, LR 0.000100 Loss 16.559058, Accuracy 11.025%\n",
      "Epoch 1, Batch 1032, LR 0.000100 Loss 16.558239, Accuracy 11.031%\n",
      "Epoch 1, Batch 1033, LR 0.000100 Loss 16.557275, Accuracy 11.038%\n",
      "Epoch 1, Batch 1034, LR 0.000100 Loss 16.556298, Accuracy 11.046%\n",
      "Epoch 1, Batch 1035, LR 0.000100 Loss 16.555729, Accuracy 11.048%\n",
      "Epoch 1, Batch 1036, LR 0.000100 Loss 16.555010, Accuracy 11.051%\n",
      "Epoch 1, Batch 1037, LR 0.000100 Loss 16.554424, Accuracy 11.057%\n",
      "Epoch 1, Batch 1038, LR 0.000100 Loss 16.553512, Accuracy 11.067%\n",
      "Epoch 1, Batch 1039, LR 0.000100 Loss 16.552355, Accuracy 11.074%\n",
      "Epoch 1, Batch 1040, LR 0.000100 Loss 16.551575, Accuracy 11.078%\n",
      "Epoch 1, Batch 1041, LR 0.000100 Loss 16.550693, Accuracy 11.085%\n",
      "Epoch 1, Batch 1042, LR 0.000100 Loss 16.550147, Accuracy 11.090%\n",
      "Epoch 1, Batch 1043, LR 0.000100 Loss 16.549020, Accuracy 11.098%\n",
      "Epoch 1, Batch 1044, LR 0.000100 Loss 16.547893, Accuracy 11.108%\n",
      "Epoch 1, Batch 1045, LR 0.000100 Loss 16.547461, Accuracy 11.108%\n",
      "Epoch 1, Batch 1046, LR 0.000100 Loss 16.546840, Accuracy 11.114%\n",
      "Epoch 1, Batch 1047, LR 0.000100 Loss 16.546033, Accuracy 11.122%\n",
      "Epoch 1, Loss (train set) 16.546033, Accuracy (train set) 11.122%\n",
      "Epoch 2, Batch 1, LR 0.000100 Loss 15.954938, Accuracy 16.406%\n",
      "Epoch 2, Batch 2, LR 0.000100 Loss 16.038856, Accuracy 14.844%\n",
      "Epoch 2, Batch 3, LR 0.000100 Loss 16.020984, Accuracy 14.583%\n",
      "Epoch 2, Batch 4, LR 0.000100 Loss 15.921592, Accuracy 15.430%\n",
      "Epoch 2, Batch 5, LR 0.000100 Loss 15.865880, Accuracy 16.250%\n",
      "Epoch 2, Batch 6, LR 0.000100 Loss 15.822043, Accuracy 17.318%\n",
      "Epoch 2, Batch 7, LR 0.000100 Loss 15.837177, Accuracy 16.853%\n",
      "Epoch 2, Batch 8, LR 0.000100 Loss 15.830463, Accuracy 16.992%\n",
      "Epoch 2, Batch 9, LR 0.000100 Loss 15.836593, Accuracy 17.361%\n",
      "Epoch 2, Batch 10, LR 0.000100 Loss 15.821768, Accuracy 17.500%\n",
      "Epoch 2, Batch 11, LR 0.000100 Loss 15.811525, Accuracy 17.188%\n",
      "Epoch 2, Batch 12, LR 0.000100 Loss 15.780107, Accuracy 17.643%\n",
      "Epoch 2, Batch 13, LR 0.000100 Loss 15.775273, Accuracy 17.428%\n",
      "Epoch 2, Batch 14, LR 0.000100 Loss 15.756702, Accuracy 17.746%\n",
      "Epoch 2, Batch 15, LR 0.000100 Loss 15.741583, Accuracy 17.760%\n",
      "Epoch 2, Batch 16, LR 0.000100 Loss 15.728919, Accuracy 17.871%\n",
      "Epoch 2, Batch 17, LR 0.000100 Loss 15.727497, Accuracy 17.831%\n",
      "Epoch 2, Batch 18, LR 0.000100 Loss 15.741453, Accuracy 17.795%\n",
      "Epoch 2, Batch 19, LR 0.000100 Loss 15.741264, Accuracy 17.887%\n",
      "Epoch 2, Batch 20, LR 0.000100 Loss 15.755066, Accuracy 17.812%\n",
      "Epoch 2, Batch 21, LR 0.000100 Loss 15.741531, Accuracy 17.894%\n",
      "Epoch 2, Batch 22, LR 0.000100 Loss 15.740198, Accuracy 17.827%\n",
      "Epoch 2, Batch 23, LR 0.000100 Loss 15.747338, Accuracy 17.663%\n",
      "Epoch 2, Batch 24, LR 0.000100 Loss 15.738206, Accuracy 17.871%\n",
      "Epoch 2, Batch 25, LR 0.000100 Loss 15.745794, Accuracy 17.875%\n",
      "Epoch 2, Batch 26, LR 0.000100 Loss 15.734424, Accuracy 17.969%\n",
      "Epoch 2, Batch 27, LR 0.000100 Loss 15.724972, Accuracy 17.998%\n",
      "Epoch 2, Batch 28, LR 0.000100 Loss 15.725512, Accuracy 17.969%\n",
      "Epoch 2, Batch 29, LR 0.000100 Loss 15.732076, Accuracy 17.807%\n",
      "Epoch 2, Batch 30, LR 0.000100 Loss 15.713124, Accuracy 18.125%\n",
      "Epoch 2, Batch 31, LR 0.000100 Loss 15.719450, Accuracy 18.120%\n",
      "Epoch 2, Batch 32, LR 0.000100 Loss 15.726202, Accuracy 18.115%\n",
      "Epoch 2, Batch 33, LR 0.000100 Loss 15.716678, Accuracy 18.087%\n",
      "Epoch 2, Batch 34, LR 0.000100 Loss 15.711600, Accuracy 18.107%\n",
      "Epoch 2, Batch 35, LR 0.000100 Loss 15.722283, Accuracy 18.013%\n",
      "Epoch 2, Batch 36, LR 0.000100 Loss 15.721195, Accuracy 17.990%\n",
      "Epoch 2, Batch 37, LR 0.000100 Loss 15.712481, Accuracy 18.201%\n",
      "Epoch 2, Batch 38, LR 0.000100 Loss 15.706861, Accuracy 18.339%\n",
      "Epoch 2, Batch 39, LR 0.000100 Loss 15.711311, Accuracy 18.249%\n",
      "Epoch 2, Batch 40, LR 0.000100 Loss 15.711733, Accuracy 18.203%\n",
      "Epoch 2, Batch 41, LR 0.000100 Loss 15.721787, Accuracy 18.159%\n",
      "Epoch 2, Batch 42, LR 0.000100 Loss 15.709677, Accuracy 18.285%\n",
      "Epoch 2, Batch 43, LR 0.000100 Loss 15.707766, Accuracy 18.278%\n",
      "Epoch 2, Batch 44, LR 0.000100 Loss 15.700796, Accuracy 18.288%\n",
      "Epoch 2, Batch 45, LR 0.000100 Loss 15.700266, Accuracy 18.333%\n",
      "Epoch 2, Batch 46, LR 0.000100 Loss 15.697965, Accuracy 18.342%\n",
      "Epoch 2, Batch 47, LR 0.000100 Loss 15.693219, Accuracy 18.384%\n",
      "Epoch 2, Batch 48, LR 0.000100 Loss 15.688442, Accuracy 18.408%\n",
      "Epoch 2, Batch 49, LR 0.000100 Loss 15.681713, Accuracy 18.543%\n",
      "Epoch 2, Batch 50, LR 0.000100 Loss 15.688602, Accuracy 18.500%\n",
      "Epoch 2, Batch 51, LR 0.000100 Loss 15.682996, Accuracy 18.627%\n",
      "Epoch 2, Batch 52, LR 0.000100 Loss 15.681089, Accuracy 18.690%\n",
      "Epoch 2, Batch 53, LR 0.000100 Loss 15.676670, Accuracy 18.676%\n",
      "Epoch 2, Batch 54, LR 0.000100 Loss 15.674557, Accuracy 18.707%\n",
      "Epoch 2, Batch 55, LR 0.000100 Loss 15.670719, Accuracy 18.764%\n",
      "Epoch 2, Batch 56, LR 0.000100 Loss 15.671210, Accuracy 18.694%\n",
      "Epoch 2, Batch 57, LR 0.000100 Loss 15.670890, Accuracy 18.736%\n",
      "Epoch 2, Batch 58, LR 0.000100 Loss 15.672424, Accuracy 18.683%\n",
      "Epoch 2, Batch 59, LR 0.000100 Loss 15.666598, Accuracy 18.697%\n",
      "Epoch 2, Batch 60, LR 0.000100 Loss 15.666836, Accuracy 18.698%\n",
      "Epoch 2, Batch 61, LR 0.000100 Loss 15.671117, Accuracy 18.699%\n",
      "Epoch 2, Batch 62, LR 0.000100 Loss 15.660319, Accuracy 18.826%\n",
      "Epoch 2, Batch 63, LR 0.000100 Loss 15.655403, Accuracy 18.862%\n",
      "Epoch 2, Batch 64, LR 0.000100 Loss 15.655845, Accuracy 18.774%\n",
      "Epoch 2, Batch 65, LR 0.000100 Loss 15.653355, Accuracy 18.786%\n",
      "Epoch 2, Batch 66, LR 0.000100 Loss 15.651392, Accuracy 18.797%\n",
      "Epoch 2, Batch 67, LR 0.000100 Loss 15.649440, Accuracy 18.738%\n",
      "Epoch 2, Batch 68, LR 0.000100 Loss 15.649805, Accuracy 18.819%\n",
      "Epoch 2, Batch 69, LR 0.000100 Loss 15.650707, Accuracy 18.852%\n",
      "Epoch 2, Batch 70, LR 0.000100 Loss 15.647877, Accuracy 18.806%\n",
      "Epoch 2, Batch 71, LR 0.000100 Loss 15.644775, Accuracy 18.827%\n",
      "Epoch 2, Batch 72, LR 0.000100 Loss 15.642439, Accuracy 18.837%\n",
      "Epoch 2, Batch 73, LR 0.000100 Loss 15.637871, Accuracy 18.878%\n",
      "Epoch 2, Batch 74, LR 0.000100 Loss 15.637467, Accuracy 18.877%\n",
      "Epoch 2, Batch 75, LR 0.000100 Loss 15.634254, Accuracy 18.917%\n",
      "Epoch 2, Batch 76, LR 0.000100 Loss 15.632048, Accuracy 18.956%\n",
      "Epoch 2, Batch 77, LR 0.000100 Loss 15.629995, Accuracy 18.933%\n",
      "Epoch 2, Batch 78, LR 0.000100 Loss 15.629416, Accuracy 18.970%\n",
      "Epoch 2, Batch 79, LR 0.000100 Loss 15.629031, Accuracy 18.977%\n",
      "Epoch 2, Batch 80, LR 0.000100 Loss 15.624190, Accuracy 19.062%\n",
      "Epoch 2, Batch 81, LR 0.000100 Loss 15.623634, Accuracy 19.059%\n",
      "Epoch 2, Batch 82, LR 0.000100 Loss 15.619538, Accuracy 19.122%\n",
      "Epoch 2, Batch 83, LR 0.000100 Loss 15.620068, Accuracy 19.155%\n",
      "Epoch 2, Batch 84, LR 0.000100 Loss 15.615912, Accuracy 19.224%\n",
      "Epoch 2, Batch 85, LR 0.000100 Loss 15.618045, Accuracy 19.210%\n",
      "Epoch 2, Batch 86, LR 0.000100 Loss 15.621483, Accuracy 19.168%\n",
      "Epoch 2, Batch 87, LR 0.000100 Loss 15.622646, Accuracy 19.127%\n",
      "Epoch 2, Batch 88, LR 0.000100 Loss 15.617852, Accuracy 19.158%\n",
      "Epoch 2, Batch 89, LR 0.000100 Loss 15.614899, Accuracy 19.180%\n",
      "Epoch 2, Batch 90, LR 0.000100 Loss 15.615713, Accuracy 19.149%\n",
      "Epoch 2, Batch 91, LR 0.000100 Loss 15.612249, Accuracy 19.188%\n",
      "Epoch 2, Batch 92, LR 0.000100 Loss 15.614095, Accuracy 19.166%\n",
      "Epoch 2, Batch 93, LR 0.000100 Loss 15.613879, Accuracy 19.170%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 94, LR 0.000100 Loss 15.614010, Accuracy 19.157%\n",
      "Epoch 2, Batch 95, LR 0.000100 Loss 15.610869, Accuracy 19.194%\n",
      "Epoch 2, Batch 96, LR 0.000100 Loss 15.610506, Accuracy 19.181%\n",
      "Epoch 2, Batch 97, LR 0.000100 Loss 15.608207, Accuracy 19.153%\n",
      "Epoch 2, Batch 98, LR 0.000100 Loss 15.606225, Accuracy 19.212%\n",
      "Epoch 2, Batch 99, LR 0.000100 Loss 15.606785, Accuracy 19.231%\n",
      "Epoch 2, Batch 100, LR 0.000100 Loss 15.607825, Accuracy 19.227%\n",
      "Epoch 2, Batch 101, LR 0.000100 Loss 15.601456, Accuracy 19.330%\n",
      "Epoch 2, Batch 102, LR 0.000100 Loss 15.602775, Accuracy 19.271%\n",
      "Epoch 2, Batch 103, LR 0.000100 Loss 15.602097, Accuracy 19.319%\n",
      "Epoch 2, Batch 104, LR 0.000100 Loss 15.605769, Accuracy 19.321%\n",
      "Epoch 2, Batch 105, LR 0.000100 Loss 15.607326, Accuracy 19.271%\n",
      "Epoch 2, Batch 106, LR 0.000100 Loss 15.606147, Accuracy 19.251%\n",
      "Epoch 2, Batch 107, LR 0.000100 Loss 15.605718, Accuracy 19.246%\n",
      "Epoch 2, Batch 108, LR 0.000100 Loss 15.610351, Accuracy 19.213%\n",
      "Epoch 2, Batch 109, LR 0.000100 Loss 15.606960, Accuracy 19.273%\n",
      "Epoch 2, Batch 110, LR 0.000100 Loss 15.607756, Accuracy 19.261%\n",
      "Epoch 2, Batch 111, LR 0.000100 Loss 15.610245, Accuracy 19.257%\n",
      "Epoch 2, Batch 112, LR 0.000100 Loss 15.607211, Accuracy 19.259%\n",
      "Epoch 2, Batch 113, LR 0.000100 Loss 15.603372, Accuracy 19.289%\n",
      "Epoch 2, Batch 114, LR 0.000100 Loss 15.608292, Accuracy 19.250%\n",
      "Epoch 2, Batch 115, LR 0.000100 Loss 15.609719, Accuracy 19.246%\n",
      "Epoch 2, Batch 116, LR 0.000100 Loss 15.607622, Accuracy 19.248%\n",
      "Epoch 2, Batch 117, LR 0.000100 Loss 15.608644, Accuracy 19.237%\n",
      "Epoch 2, Batch 118, LR 0.000100 Loss 15.606519, Accuracy 19.247%\n",
      "Epoch 2, Batch 119, LR 0.000100 Loss 15.608341, Accuracy 19.216%\n",
      "Epoch 2, Batch 120, LR 0.000100 Loss 15.608698, Accuracy 19.193%\n",
      "Epoch 2, Batch 121, LR 0.000100 Loss 15.610979, Accuracy 19.189%\n",
      "Epoch 2, Batch 122, LR 0.000100 Loss 15.611581, Accuracy 19.134%\n",
      "Epoch 2, Batch 123, LR 0.000100 Loss 15.609440, Accuracy 19.188%\n",
      "Epoch 2, Batch 124, LR 0.000100 Loss 15.606067, Accuracy 19.229%\n",
      "Epoch 2, Batch 125, LR 0.000100 Loss 15.606405, Accuracy 19.219%\n",
      "Epoch 2, Batch 126, LR 0.000100 Loss 15.607928, Accuracy 19.184%\n",
      "Epoch 2, Batch 127, LR 0.000100 Loss 15.607098, Accuracy 19.187%\n",
      "Epoch 2, Batch 128, LR 0.000100 Loss 15.605311, Accuracy 19.196%\n",
      "Epoch 2, Batch 129, LR 0.000100 Loss 15.604487, Accuracy 19.186%\n",
      "Epoch 2, Batch 130, LR 0.000100 Loss 15.605473, Accuracy 19.183%\n",
      "Epoch 2, Batch 131, LR 0.000100 Loss 15.605304, Accuracy 19.185%\n",
      "Epoch 2, Batch 132, LR 0.000100 Loss 15.606729, Accuracy 19.152%\n",
      "Epoch 2, Batch 133, LR 0.000100 Loss 15.605131, Accuracy 19.173%\n",
      "Epoch 2, Batch 134, LR 0.000100 Loss 15.602830, Accuracy 19.193%\n",
      "Epoch 2, Batch 135, LR 0.000100 Loss 15.601209, Accuracy 19.196%\n",
      "Epoch 2, Batch 136, LR 0.000100 Loss 15.598792, Accuracy 19.221%\n",
      "Epoch 2, Batch 137, LR 0.000100 Loss 15.598088, Accuracy 19.223%\n",
      "Epoch 2, Batch 138, LR 0.000100 Loss 15.598825, Accuracy 19.180%\n",
      "Epoch 2, Batch 139, LR 0.000100 Loss 15.597667, Accuracy 19.205%\n",
      "Epoch 2, Batch 140, LR 0.000100 Loss 15.598655, Accuracy 19.213%\n",
      "Epoch 2, Batch 141, LR 0.000100 Loss 15.597236, Accuracy 19.249%\n",
      "Epoch 2, Batch 142, LR 0.000100 Loss 15.597151, Accuracy 19.234%\n",
      "Epoch 2, Batch 143, LR 0.000100 Loss 15.593948, Accuracy 19.285%\n",
      "Epoch 2, Batch 144, LR 0.000100 Loss 15.593401, Accuracy 19.309%\n",
      "Epoch 2, Batch 145, LR 0.000100 Loss 15.589509, Accuracy 19.353%\n",
      "Epoch 2, Batch 146, LR 0.000100 Loss 15.586938, Accuracy 19.376%\n",
      "Epoch 2, Batch 147, LR 0.000100 Loss 15.586486, Accuracy 19.372%\n",
      "Epoch 2, Batch 148, LR 0.000100 Loss 15.587906, Accuracy 19.341%\n",
      "Epoch 2, Batch 149, LR 0.000100 Loss 15.587324, Accuracy 19.348%\n",
      "Epoch 2, Batch 150, LR 0.000100 Loss 15.586295, Accuracy 19.385%\n",
      "Epoch 2, Batch 151, LR 0.000100 Loss 15.585992, Accuracy 19.386%\n",
      "Epoch 2, Batch 152, LR 0.000100 Loss 15.584702, Accuracy 19.403%\n",
      "Epoch 2, Batch 153, LR 0.000100 Loss 15.583346, Accuracy 19.424%\n",
      "Epoch 2, Batch 154, LR 0.000100 Loss 15.583051, Accuracy 19.465%\n",
      "Epoch 2, Batch 155, LR 0.000100 Loss 15.580136, Accuracy 19.476%\n",
      "Epoch 2, Batch 156, LR 0.000100 Loss 15.576410, Accuracy 19.516%\n",
      "Epoch 2, Batch 157, LR 0.000100 Loss 15.575055, Accuracy 19.531%\n",
      "Epoch 2, Batch 158, LR 0.000100 Loss 15.573625, Accuracy 19.546%\n",
      "Epoch 2, Batch 159, LR 0.000100 Loss 15.573566, Accuracy 19.556%\n",
      "Epoch 2, Batch 160, LR 0.000100 Loss 15.572704, Accuracy 19.541%\n",
      "Epoch 2, Batch 161, LR 0.000100 Loss 15.572710, Accuracy 19.536%\n",
      "Epoch 2, Batch 162, LR 0.000100 Loss 15.572382, Accuracy 19.555%\n",
      "Epoch 2, Batch 163, LR 0.000100 Loss 15.571596, Accuracy 19.570%\n",
      "Epoch 2, Batch 164, LR 0.000100 Loss 15.568943, Accuracy 19.607%\n",
      "Epoch 2, Batch 165, LR 0.000100 Loss 15.567062, Accuracy 19.650%\n",
      "Epoch 2, Batch 166, LR 0.000100 Loss 15.568190, Accuracy 19.649%\n",
      "Epoch 2, Batch 167, LR 0.000100 Loss 15.568956, Accuracy 19.634%\n",
      "Epoch 2, Batch 168, LR 0.000100 Loss 15.568260, Accuracy 19.638%\n",
      "Epoch 2, Batch 169, LR 0.000100 Loss 15.570974, Accuracy 19.596%\n",
      "Epoch 2, Batch 170, LR 0.000100 Loss 15.571407, Accuracy 19.609%\n",
      "Epoch 2, Batch 171, LR 0.000100 Loss 15.571765, Accuracy 19.618%\n",
      "Epoch 2, Batch 172, LR 0.000100 Loss 15.569662, Accuracy 19.627%\n",
      "Epoch 2, Batch 173, LR 0.000100 Loss 15.567987, Accuracy 19.649%\n",
      "Epoch 2, Batch 174, LR 0.000100 Loss 15.567862, Accuracy 19.639%\n",
      "Epoch 2, Batch 175, LR 0.000100 Loss 15.567661, Accuracy 19.634%\n",
      "Epoch 2, Batch 176, LR 0.000100 Loss 15.570091, Accuracy 19.611%\n",
      "Epoch 2, Batch 177, LR 0.000100 Loss 15.570906, Accuracy 19.620%\n",
      "Epoch 2, Batch 178, LR 0.000100 Loss 15.570228, Accuracy 19.619%\n",
      "Epoch 2, Batch 179, LR 0.000100 Loss 15.566624, Accuracy 19.658%\n",
      "Epoch 2, Batch 180, LR 0.000100 Loss 15.564455, Accuracy 19.696%\n",
      "Epoch 2, Batch 181, LR 0.000100 Loss 15.563467, Accuracy 19.665%\n",
      "Epoch 2, Batch 182, LR 0.000100 Loss 15.561069, Accuracy 19.712%\n",
      "Epoch 2, Batch 183, LR 0.000100 Loss 15.560897, Accuracy 19.698%\n",
      "Epoch 2, Batch 184, LR 0.000100 Loss 15.561137, Accuracy 19.710%\n",
      "Epoch 2, Batch 185, LR 0.000100 Loss 15.561473, Accuracy 19.726%\n",
      "Epoch 2, Batch 186, LR 0.000100 Loss 15.560662, Accuracy 19.771%\n",
      "Epoch 2, Batch 187, LR 0.000100 Loss 15.558767, Accuracy 19.799%\n",
      "Epoch 2, Batch 188, LR 0.000100 Loss 15.556798, Accuracy 19.839%\n",
      "Epoch 2, Batch 189, LR 0.000100 Loss 15.554354, Accuracy 19.866%\n",
      "Epoch 2, Batch 190, LR 0.000100 Loss 15.551797, Accuracy 19.893%\n",
      "Epoch 2, Batch 191, LR 0.000100 Loss 15.550197, Accuracy 19.903%\n",
      "Epoch 2, Batch 192, LR 0.000100 Loss 15.550584, Accuracy 19.897%\n",
      "Epoch 2, Batch 193, LR 0.000100 Loss 15.550185, Accuracy 19.879%\n",
      "Epoch 2, Batch 194, LR 0.000100 Loss 15.552052, Accuracy 19.841%\n",
      "Epoch 2, Batch 195, LR 0.000100 Loss 15.551223, Accuracy 19.864%\n",
      "Epoch 2, Batch 196, LR 0.000100 Loss 15.552469, Accuracy 19.838%\n",
      "Epoch 2, Batch 197, LR 0.000100 Loss 15.551857, Accuracy 19.845%\n",
      "Epoch 2, Batch 198, LR 0.000100 Loss 15.550457, Accuracy 19.871%\n",
      "Epoch 2, Batch 199, LR 0.000100 Loss 15.550261, Accuracy 19.877%\n",
      "Epoch 2, Batch 200, LR 0.000100 Loss 15.549331, Accuracy 19.875%\n",
      "Epoch 2, Batch 201, LR 0.000100 Loss 15.548256, Accuracy 19.881%\n",
      "Epoch 2, Batch 202, LR 0.000100 Loss 15.549215, Accuracy 19.845%\n",
      "Epoch 2, Batch 203, LR 0.000100 Loss 15.549829, Accuracy 19.824%\n",
      "Epoch 2, Batch 204, LR 0.000100 Loss 15.547671, Accuracy 19.826%\n",
      "Epoch 2, Batch 205, LR 0.000100 Loss 15.547839, Accuracy 19.836%\n",
      "Epoch 2, Batch 206, LR 0.000100 Loss 15.548730, Accuracy 19.816%\n",
      "Epoch 2, Batch 207, LR 0.000100 Loss 15.548692, Accuracy 19.822%\n",
      "Epoch 2, Batch 208, LR 0.000100 Loss 15.548648, Accuracy 19.824%\n",
      "Epoch 2, Batch 209, LR 0.000100 Loss 15.546004, Accuracy 19.842%\n",
      "Epoch 2, Batch 210, LR 0.000100 Loss 15.546345, Accuracy 19.818%\n",
      "Epoch 2, Batch 211, LR 0.000100 Loss 15.545656, Accuracy 19.839%\n",
      "Epoch 2, Batch 212, LR 0.000100 Loss 15.545832, Accuracy 19.819%\n",
      "Epoch 2, Batch 213, LR 0.000100 Loss 15.544425, Accuracy 19.825%\n",
      "Epoch 2, Batch 214, LR 0.000100 Loss 15.540759, Accuracy 19.885%\n",
      "Epoch 2, Batch 215, LR 0.000100 Loss 15.539431, Accuracy 19.898%\n",
      "Epoch 2, Batch 216, LR 0.000100 Loss 15.539308, Accuracy 19.893%\n",
      "Epoch 2, Batch 217, LR 0.000100 Loss 15.538736, Accuracy 19.888%\n",
      "Epoch 2, Batch 218, LR 0.000100 Loss 15.536401, Accuracy 19.908%\n",
      "Epoch 2, Batch 219, LR 0.000100 Loss 15.537001, Accuracy 19.877%\n",
      "Epoch 2, Batch 220, LR 0.000100 Loss 15.535442, Accuracy 19.897%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 221, LR 0.000100 Loss 15.534877, Accuracy 19.899%\n",
      "Epoch 2, Batch 222, LR 0.000100 Loss 15.534968, Accuracy 19.894%\n",
      "Epoch 2, Batch 223, LR 0.000100 Loss 15.534694, Accuracy 19.889%\n",
      "Epoch 2, Batch 224, LR 0.000100 Loss 15.534358, Accuracy 19.897%\n",
      "Epoch 2, Batch 225, LR 0.000100 Loss 15.533035, Accuracy 19.903%\n",
      "Epoch 2, Batch 226, LR 0.000100 Loss 15.534058, Accuracy 19.891%\n",
      "Epoch 2, Batch 227, LR 0.000100 Loss 15.533091, Accuracy 19.903%\n",
      "Epoch 2, Batch 228, LR 0.000100 Loss 15.534329, Accuracy 19.905%\n",
      "Epoch 2, Batch 229, LR 0.000100 Loss 15.532687, Accuracy 19.907%\n",
      "Epoch 2, Batch 230, LR 0.000100 Loss 15.532956, Accuracy 19.922%\n",
      "Epoch 2, Batch 231, LR 0.000100 Loss 15.532806, Accuracy 19.920%\n",
      "Epoch 2, Batch 232, LR 0.000100 Loss 15.532639, Accuracy 19.915%\n",
      "Epoch 2, Batch 233, LR 0.000100 Loss 15.532513, Accuracy 19.927%\n",
      "Epoch 2, Batch 234, LR 0.000100 Loss 15.529841, Accuracy 19.942%\n",
      "Epoch 2, Batch 235, LR 0.000100 Loss 15.526594, Accuracy 19.973%\n",
      "Epoch 2, Batch 236, LR 0.000100 Loss 15.527060, Accuracy 19.965%\n",
      "Epoch 2, Batch 237, LR 0.000100 Loss 15.526132, Accuracy 19.980%\n",
      "Epoch 2, Batch 238, LR 0.000100 Loss 15.524803, Accuracy 20.004%\n",
      "Epoch 2, Batch 239, LR 0.000100 Loss 15.524942, Accuracy 20.008%\n",
      "Epoch 2, Batch 240, LR 0.000100 Loss 15.521200, Accuracy 20.049%\n",
      "Epoch 2, Batch 241, LR 0.000100 Loss 15.520091, Accuracy 20.056%\n",
      "Epoch 2, Batch 242, LR 0.000100 Loss 15.520735, Accuracy 20.028%\n",
      "Epoch 2, Batch 243, LR 0.000100 Loss 15.520541, Accuracy 20.030%\n",
      "Epoch 2, Batch 244, LR 0.000100 Loss 15.520524, Accuracy 20.034%\n",
      "Epoch 2, Batch 245, LR 0.000100 Loss 15.519322, Accuracy 20.048%\n",
      "Epoch 2, Batch 246, LR 0.000100 Loss 15.519427, Accuracy 20.062%\n",
      "Epoch 2, Batch 247, LR 0.000100 Loss 15.518206, Accuracy 20.085%\n",
      "Epoch 2, Batch 248, LR 0.000100 Loss 15.517522, Accuracy 20.079%\n",
      "Epoch 2, Batch 249, LR 0.000100 Loss 15.512921, Accuracy 20.115%\n",
      "Epoch 2, Batch 250, LR 0.000100 Loss 15.512297, Accuracy 20.122%\n",
      "Epoch 2, Batch 251, LR 0.000100 Loss 15.512321, Accuracy 20.126%\n",
      "Epoch 2, Batch 252, LR 0.000100 Loss 15.509439, Accuracy 20.176%\n",
      "Epoch 2, Batch 253, LR 0.000100 Loss 15.507676, Accuracy 20.201%\n",
      "Epoch 2, Batch 254, LR 0.000100 Loss 15.507989, Accuracy 20.199%\n",
      "Epoch 2, Batch 255, LR 0.000100 Loss 15.507097, Accuracy 20.202%\n",
      "Epoch 2, Batch 256, LR 0.000100 Loss 15.505868, Accuracy 20.212%\n",
      "Epoch 2, Batch 257, LR 0.000100 Loss 15.505914, Accuracy 20.212%\n",
      "Epoch 2, Batch 258, LR 0.000100 Loss 15.505700, Accuracy 20.216%\n",
      "Epoch 2, Batch 259, LR 0.000100 Loss 15.505746, Accuracy 20.213%\n",
      "Epoch 2, Batch 260, LR 0.000100 Loss 15.506421, Accuracy 20.216%\n",
      "Epoch 2, Batch 261, LR 0.000100 Loss 15.506375, Accuracy 20.211%\n",
      "Epoch 2, Batch 262, LR 0.000100 Loss 15.505924, Accuracy 20.226%\n",
      "Epoch 2, Batch 263, LR 0.000100 Loss 15.506586, Accuracy 20.229%\n",
      "Epoch 2, Batch 264, LR 0.000100 Loss 15.506024, Accuracy 20.244%\n",
      "Epoch 2, Batch 265, LR 0.000100 Loss 15.506899, Accuracy 20.221%\n",
      "Epoch 2, Batch 266, LR 0.000100 Loss 15.505628, Accuracy 20.230%\n",
      "Epoch 2, Batch 267, LR 0.000100 Loss 15.506043, Accuracy 20.222%\n",
      "Epoch 2, Batch 268, LR 0.000100 Loss 15.505019, Accuracy 20.231%\n",
      "Epoch 2, Batch 269, LR 0.000100 Loss 15.503467, Accuracy 20.237%\n",
      "Epoch 2, Batch 270, LR 0.000100 Loss 15.501714, Accuracy 20.258%\n",
      "Epoch 2, Batch 271, LR 0.000100 Loss 15.501020, Accuracy 20.255%\n",
      "Epoch 2, Batch 272, LR 0.000100 Loss 15.501135, Accuracy 20.255%\n",
      "Epoch 2, Batch 273, LR 0.000100 Loss 15.500787, Accuracy 20.270%\n",
      "Epoch 2, Batch 274, LR 0.000100 Loss 15.499665, Accuracy 20.267%\n",
      "Epoch 2, Batch 275, LR 0.000100 Loss 15.498707, Accuracy 20.270%\n",
      "Epoch 2, Batch 276, LR 0.000100 Loss 15.497853, Accuracy 20.273%\n",
      "Epoch 2, Batch 277, LR 0.000100 Loss 15.496613, Accuracy 20.273%\n",
      "Epoch 2, Batch 278, LR 0.000100 Loss 15.495772, Accuracy 20.276%\n",
      "Epoch 2, Batch 279, LR 0.000100 Loss 15.494535, Accuracy 20.276%\n",
      "Epoch 2, Batch 280, LR 0.000100 Loss 15.493705, Accuracy 20.290%\n",
      "Epoch 2, Batch 281, LR 0.000100 Loss 15.492186, Accuracy 20.299%\n",
      "Epoch 2, Batch 282, LR 0.000100 Loss 15.491257, Accuracy 20.315%\n",
      "Epoch 2, Batch 283, LR 0.000100 Loss 15.490002, Accuracy 20.321%\n",
      "Epoch 2, Batch 284, LR 0.000100 Loss 15.489499, Accuracy 20.337%\n",
      "Epoch 2, Batch 285, LR 0.000100 Loss 15.487879, Accuracy 20.348%\n",
      "Epoch 2, Batch 286, LR 0.000100 Loss 15.488052, Accuracy 20.353%\n",
      "Epoch 2, Batch 287, LR 0.000100 Loss 15.487391, Accuracy 20.359%\n",
      "Epoch 2, Batch 288, LR 0.000100 Loss 15.485577, Accuracy 20.378%\n",
      "Epoch 2, Batch 289, LR 0.000100 Loss 15.484204, Accuracy 20.404%\n",
      "Epoch 2, Batch 290, LR 0.000100 Loss 15.482340, Accuracy 20.426%\n",
      "Epoch 2, Batch 291, LR 0.000100 Loss 15.482830, Accuracy 20.420%\n",
      "Epoch 2, Batch 292, LR 0.000100 Loss 15.482693, Accuracy 20.420%\n",
      "Epoch 2, Batch 293, LR 0.000100 Loss 15.482107, Accuracy 20.414%\n",
      "Epoch 2, Batch 294, LR 0.000100 Loss 15.480641, Accuracy 20.429%\n",
      "Epoch 2, Batch 295, LR 0.000100 Loss 15.482182, Accuracy 20.418%\n",
      "Epoch 2, Batch 296, LR 0.000100 Loss 15.482212, Accuracy 20.418%\n",
      "Epoch 2, Batch 297, LR 0.000100 Loss 15.483525, Accuracy 20.394%\n",
      "Epoch 2, Batch 298, LR 0.000100 Loss 15.482831, Accuracy 20.386%\n",
      "Epoch 2, Batch 299, LR 0.000100 Loss 15.481847, Accuracy 20.393%\n",
      "Epoch 2, Batch 300, LR 0.000100 Loss 15.479428, Accuracy 20.417%\n",
      "Epoch 2, Batch 301, LR 0.000100 Loss 15.478996, Accuracy 20.411%\n",
      "Epoch 2, Batch 302, LR 0.000100 Loss 15.478839, Accuracy 20.413%\n",
      "Epoch 2, Batch 303, LR 0.000100 Loss 15.476424, Accuracy 20.447%\n",
      "Epoch 2, Batch 304, LR 0.000100 Loss 15.475875, Accuracy 20.454%\n",
      "Epoch 2, Batch 305, LR 0.000100 Loss 15.475082, Accuracy 20.466%\n",
      "Epoch 2, Batch 306, LR 0.000100 Loss 15.473006, Accuracy 20.494%\n",
      "Epoch 2, Batch 307, LR 0.000100 Loss 15.473491, Accuracy 20.496%\n",
      "Epoch 2, Batch 308, LR 0.000100 Loss 15.473742, Accuracy 20.493%\n",
      "Epoch 2, Batch 309, LR 0.000100 Loss 15.473096, Accuracy 20.510%\n",
      "Epoch 2, Batch 310, LR 0.000100 Loss 15.471438, Accuracy 20.519%\n",
      "Epoch 2, Batch 311, LR 0.000100 Loss 15.471362, Accuracy 20.503%\n",
      "Epoch 2, Batch 312, LR 0.000100 Loss 15.471614, Accuracy 20.480%\n",
      "Epoch 2, Batch 313, LR 0.000100 Loss 15.471002, Accuracy 20.487%\n",
      "Epoch 2, Batch 314, LR 0.000100 Loss 15.468724, Accuracy 20.514%\n",
      "Epoch 2, Batch 315, LR 0.000100 Loss 15.467746, Accuracy 20.516%\n",
      "Epoch 2, Batch 316, LR 0.000100 Loss 15.465830, Accuracy 20.545%\n",
      "Epoch 2, Batch 317, LR 0.000100 Loss 15.463896, Accuracy 20.564%\n",
      "Epoch 2, Batch 318, LR 0.000100 Loss 15.461852, Accuracy 20.580%\n",
      "Epoch 2, Batch 319, LR 0.000100 Loss 15.461724, Accuracy 20.584%\n",
      "Epoch 2, Batch 320, LR 0.000100 Loss 15.460000, Accuracy 20.591%\n",
      "Epoch 2, Batch 321, LR 0.000100 Loss 15.458760, Accuracy 20.597%\n",
      "Epoch 2, Batch 322, LR 0.000100 Loss 15.458299, Accuracy 20.616%\n",
      "Epoch 2, Batch 323, LR 0.000100 Loss 15.457479, Accuracy 20.625%\n",
      "Epoch 2, Batch 324, LR 0.000100 Loss 15.457253, Accuracy 20.628%\n",
      "Epoch 2, Batch 325, LR 0.000100 Loss 15.457835, Accuracy 20.615%\n",
      "Epoch 2, Batch 326, LR 0.000100 Loss 15.456014, Accuracy 20.631%\n",
      "Epoch 2, Batch 327, LR 0.000100 Loss 15.455790, Accuracy 20.637%\n",
      "Epoch 2, Batch 328, LR 0.000100 Loss 15.455663, Accuracy 20.644%\n",
      "Epoch 2, Batch 329, LR 0.000100 Loss 15.455092, Accuracy 20.638%\n",
      "Epoch 2, Batch 330, LR 0.000100 Loss 15.455757, Accuracy 20.623%\n",
      "Epoch 2, Batch 331, LR 0.000100 Loss 15.455253, Accuracy 20.626%\n",
      "Epoch 2, Batch 332, LR 0.000100 Loss 15.454497, Accuracy 20.628%\n",
      "Epoch 2, Batch 333, LR 0.000100 Loss 15.455010, Accuracy 20.622%\n",
      "Epoch 2, Batch 334, LR 0.000100 Loss 15.453655, Accuracy 20.624%\n",
      "Epoch 2, Batch 335, LR 0.000100 Loss 15.453260, Accuracy 20.630%\n",
      "Epoch 2, Batch 336, LR 0.000100 Loss 15.452895, Accuracy 20.643%\n",
      "Epoch 2, Batch 337, LR 0.000100 Loss 15.453006, Accuracy 20.630%\n",
      "Epoch 2, Batch 338, LR 0.000100 Loss 15.452207, Accuracy 20.631%\n",
      "Epoch 2, Batch 339, LR 0.000100 Loss 15.452615, Accuracy 20.626%\n",
      "Epoch 2, Batch 340, LR 0.000100 Loss 15.451408, Accuracy 20.630%\n",
      "Epoch 2, Batch 341, LR 0.000100 Loss 15.450920, Accuracy 20.638%\n",
      "Epoch 2, Batch 342, LR 0.000100 Loss 15.448948, Accuracy 20.653%\n",
      "Epoch 2, Batch 343, LR 0.000100 Loss 15.447784, Accuracy 20.677%\n",
      "Epoch 2, Batch 344, LR 0.000100 Loss 15.447184, Accuracy 20.680%\n",
      "Epoch 2, Batch 345, LR 0.000100 Loss 15.446231, Accuracy 20.686%\n",
      "Epoch 2, Batch 346, LR 0.000100 Loss 15.445378, Accuracy 20.692%\n",
      "Epoch 2, Batch 347, LR 0.000100 Loss 15.443714, Accuracy 20.704%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 348, LR 0.000100 Loss 15.443996, Accuracy 20.703%\n",
      "Epoch 2, Batch 349, LR 0.000100 Loss 15.443033, Accuracy 20.706%\n",
      "Epoch 2, Batch 350, LR 0.000100 Loss 15.442123, Accuracy 20.708%\n",
      "Epoch 2, Batch 351, LR 0.000100 Loss 15.441230, Accuracy 20.709%\n",
      "Epoch 2, Batch 352, LR 0.000100 Loss 15.440560, Accuracy 20.719%\n",
      "Epoch 2, Batch 353, LR 0.000100 Loss 15.441249, Accuracy 20.715%\n",
      "Epoch 2, Batch 354, LR 0.000100 Loss 15.439747, Accuracy 20.736%\n",
      "Epoch 2, Batch 355, LR 0.000100 Loss 15.439754, Accuracy 20.731%\n",
      "Epoch 2, Batch 356, LR 0.000100 Loss 15.439260, Accuracy 20.734%\n",
      "Epoch 2, Batch 357, LR 0.000100 Loss 15.438193, Accuracy 20.737%\n",
      "Epoch 2, Batch 358, LR 0.000100 Loss 15.437374, Accuracy 20.745%\n",
      "Epoch 2, Batch 359, LR 0.000100 Loss 15.436489, Accuracy 20.761%\n",
      "Epoch 2, Batch 360, LR 0.000100 Loss 15.436027, Accuracy 20.762%\n",
      "Epoch 2, Batch 361, LR 0.000100 Loss 15.434556, Accuracy 20.786%\n",
      "Epoch 2, Batch 362, LR 0.000100 Loss 15.432693, Accuracy 20.820%\n",
      "Epoch 2, Batch 363, LR 0.000100 Loss 15.432314, Accuracy 20.818%\n",
      "Epoch 2, Batch 364, LR 0.000100 Loss 15.431517, Accuracy 20.823%\n",
      "Epoch 2, Batch 365, LR 0.000100 Loss 15.431376, Accuracy 20.815%\n",
      "Epoch 2, Batch 366, LR 0.000100 Loss 15.430248, Accuracy 20.835%\n",
      "Epoch 2, Batch 367, LR 0.000100 Loss 15.429100, Accuracy 20.851%\n",
      "Epoch 2, Batch 368, LR 0.000100 Loss 15.429003, Accuracy 20.858%\n",
      "Epoch 2, Batch 369, LR 0.000100 Loss 15.428725, Accuracy 20.867%\n",
      "Epoch 2, Batch 370, LR 0.000100 Loss 15.428256, Accuracy 20.866%\n",
      "Epoch 2, Batch 371, LR 0.000100 Loss 15.427455, Accuracy 20.868%\n",
      "Epoch 2, Batch 372, LR 0.000100 Loss 15.427313, Accuracy 20.861%\n",
      "Epoch 2, Batch 373, LR 0.000100 Loss 15.425774, Accuracy 20.874%\n",
      "Epoch 2, Batch 374, LR 0.000100 Loss 15.426038, Accuracy 20.881%\n",
      "Epoch 2, Batch 375, LR 0.000100 Loss 15.425408, Accuracy 20.892%\n",
      "Epoch 2, Batch 376, LR 0.000100 Loss 15.424493, Accuracy 20.903%\n",
      "Epoch 2, Batch 377, LR 0.000100 Loss 15.424612, Accuracy 20.903%\n",
      "Epoch 2, Batch 378, LR 0.000100 Loss 15.423730, Accuracy 20.906%\n",
      "Epoch 2, Batch 379, LR 0.000100 Loss 15.422183, Accuracy 20.925%\n",
      "Epoch 2, Batch 380, LR 0.000100 Loss 15.419760, Accuracy 20.962%\n",
      "Epoch 2, Batch 381, LR 0.000100 Loss 15.419877, Accuracy 20.963%\n",
      "Epoch 2, Batch 382, LR 0.000100 Loss 15.419231, Accuracy 20.969%\n",
      "Epoch 2, Batch 383, LR 0.000100 Loss 15.418775, Accuracy 20.980%\n",
      "Epoch 2, Batch 384, LR 0.000100 Loss 15.416809, Accuracy 20.988%\n",
      "Epoch 2, Batch 385, LR 0.000100 Loss 15.415879, Accuracy 21.002%\n",
      "Epoch 2, Batch 386, LR 0.000100 Loss 15.415920, Accuracy 21.005%\n",
      "Epoch 2, Batch 387, LR 0.000100 Loss 15.415201, Accuracy 21.013%\n",
      "Epoch 2, Batch 388, LR 0.000100 Loss 15.415018, Accuracy 21.005%\n",
      "Epoch 2, Batch 389, LR 0.000100 Loss 15.414778, Accuracy 21.011%\n",
      "Epoch 2, Batch 390, LR 0.000100 Loss 15.412462, Accuracy 21.018%\n",
      "Epoch 2, Batch 391, LR 0.000100 Loss 15.412195, Accuracy 21.012%\n",
      "Epoch 2, Batch 392, LR 0.000100 Loss 15.411929, Accuracy 21.028%\n",
      "Epoch 2, Batch 393, LR 0.000100 Loss 15.411727, Accuracy 21.032%\n",
      "Epoch 2, Batch 394, LR 0.000100 Loss 15.411199, Accuracy 21.042%\n",
      "Epoch 2, Batch 395, LR 0.000100 Loss 15.411667, Accuracy 21.034%\n",
      "Epoch 2, Batch 396, LR 0.000100 Loss 15.411299, Accuracy 21.040%\n",
      "Epoch 2, Batch 397, LR 0.000100 Loss 15.411279, Accuracy 21.043%\n",
      "Epoch 2, Batch 398, LR 0.000100 Loss 15.411091, Accuracy 21.043%\n",
      "Epoch 2, Batch 399, LR 0.000100 Loss 15.411222, Accuracy 21.049%\n",
      "Epoch 2, Batch 400, LR 0.000100 Loss 15.410685, Accuracy 21.051%\n",
      "Epoch 2, Batch 401, LR 0.000100 Loss 15.409805, Accuracy 21.043%\n",
      "Epoch 2, Batch 402, LR 0.000100 Loss 15.409884, Accuracy 21.037%\n",
      "Epoch 2, Batch 403, LR 0.000100 Loss 15.406879, Accuracy 21.051%\n",
      "Epoch 2, Batch 404, LR 0.000100 Loss 15.407081, Accuracy 21.055%\n",
      "Epoch 2, Batch 405, LR 0.000100 Loss 15.406342, Accuracy 21.053%\n",
      "Epoch 2, Batch 406, LR 0.000100 Loss 15.405814, Accuracy 21.071%\n",
      "Epoch 2, Batch 407, LR 0.000100 Loss 15.406398, Accuracy 21.059%\n",
      "Epoch 2, Batch 408, LR 0.000100 Loss 15.404958, Accuracy 21.075%\n",
      "Epoch 2, Batch 409, LR 0.000100 Loss 15.405077, Accuracy 21.065%\n",
      "Epoch 2, Batch 410, LR 0.000100 Loss 15.404363, Accuracy 21.073%\n",
      "Epoch 2, Batch 411, LR 0.000100 Loss 15.403552, Accuracy 21.079%\n",
      "Epoch 2, Batch 412, LR 0.000100 Loss 15.403856, Accuracy 21.080%\n",
      "Epoch 2, Batch 413, LR 0.000100 Loss 15.403545, Accuracy 21.086%\n",
      "Epoch 2, Batch 414, LR 0.000100 Loss 15.403916, Accuracy 21.081%\n",
      "Epoch 2, Batch 415, LR 0.000100 Loss 15.403889, Accuracy 21.082%\n",
      "Epoch 2, Batch 416, LR 0.000100 Loss 15.403448, Accuracy 21.082%\n",
      "Epoch 2, Batch 417, LR 0.000100 Loss 15.402153, Accuracy 21.101%\n",
      "Epoch 2, Batch 418, LR 0.000100 Loss 15.401761, Accuracy 21.099%\n",
      "Epoch 2, Batch 419, LR 0.000100 Loss 15.400451, Accuracy 21.120%\n",
      "Epoch 2, Batch 420, LR 0.000100 Loss 15.399977, Accuracy 21.131%\n",
      "Epoch 2, Batch 421, LR 0.000100 Loss 15.400176, Accuracy 21.123%\n",
      "Epoch 2, Batch 422, LR 0.000100 Loss 15.398927, Accuracy 21.134%\n",
      "Epoch 2, Batch 423, LR 0.000100 Loss 15.396921, Accuracy 21.147%\n",
      "Epoch 2, Batch 424, LR 0.000100 Loss 15.396019, Accuracy 21.164%\n",
      "Epoch 2, Batch 425, LR 0.000100 Loss 15.395227, Accuracy 21.171%\n",
      "Epoch 2, Batch 426, LR 0.000100 Loss 15.395068, Accuracy 21.184%\n",
      "Epoch 2, Batch 427, LR 0.000100 Loss 15.393978, Accuracy 21.187%\n",
      "Epoch 2, Batch 428, LR 0.000100 Loss 15.393170, Accuracy 21.201%\n",
      "Epoch 2, Batch 429, LR 0.000100 Loss 15.393738, Accuracy 21.192%\n",
      "Epoch 2, Batch 430, LR 0.000100 Loss 15.392652, Accuracy 21.199%\n",
      "Epoch 2, Batch 431, LR 0.000100 Loss 15.391790, Accuracy 21.201%\n",
      "Epoch 2, Batch 432, LR 0.000100 Loss 15.390487, Accuracy 21.215%\n",
      "Epoch 2, Batch 433, LR 0.000100 Loss 15.389204, Accuracy 21.236%\n",
      "Epoch 2, Batch 434, LR 0.000100 Loss 15.388834, Accuracy 21.245%\n",
      "Epoch 2, Batch 435, LR 0.000100 Loss 15.387719, Accuracy 21.250%\n",
      "Epoch 2, Batch 436, LR 0.000100 Loss 15.387448, Accuracy 21.253%\n",
      "Epoch 2, Batch 437, LR 0.000100 Loss 15.385895, Accuracy 21.273%\n",
      "Epoch 2, Batch 438, LR 0.000100 Loss 15.385138, Accuracy 21.285%\n",
      "Epoch 2, Batch 439, LR 0.000100 Loss 15.384837, Accuracy 21.286%\n",
      "Epoch 2, Batch 440, LR 0.000100 Loss 15.384325, Accuracy 21.287%\n",
      "Epoch 2, Batch 441, LR 0.000100 Loss 15.382994, Accuracy 21.303%\n",
      "Epoch 2, Batch 442, LR 0.000100 Loss 15.382431, Accuracy 21.299%\n",
      "Epoch 2, Batch 443, LR 0.000100 Loss 15.381966, Accuracy 21.304%\n",
      "Epoch 2, Batch 444, LR 0.000100 Loss 15.381301, Accuracy 21.314%\n",
      "Epoch 2, Batch 445, LR 0.000100 Loss 15.380251, Accuracy 21.320%\n",
      "Epoch 2, Batch 446, LR 0.000100 Loss 15.380600, Accuracy 21.314%\n",
      "Epoch 2, Batch 447, LR 0.000100 Loss 15.380974, Accuracy 21.314%\n",
      "Epoch 2, Batch 448, LR 0.000100 Loss 15.381455, Accuracy 21.310%\n",
      "Epoch 2, Batch 449, LR 0.000100 Loss 15.381149, Accuracy 21.316%\n",
      "Epoch 2, Batch 450, LR 0.000100 Loss 15.381095, Accuracy 21.321%\n",
      "Epoch 2, Batch 451, LR 0.000100 Loss 15.380461, Accuracy 21.322%\n",
      "Epoch 2, Batch 452, LR 0.000100 Loss 15.379360, Accuracy 21.325%\n",
      "Epoch 2, Batch 453, LR 0.000100 Loss 15.379967, Accuracy 21.309%\n",
      "Epoch 2, Batch 454, LR 0.000100 Loss 15.379636, Accuracy 21.316%\n",
      "Epoch 2, Batch 455, LR 0.000100 Loss 15.378659, Accuracy 21.322%\n",
      "Epoch 2, Batch 456, LR 0.000100 Loss 15.378407, Accuracy 21.322%\n",
      "Epoch 2, Batch 457, LR 0.000100 Loss 15.375804, Accuracy 21.340%\n",
      "Epoch 2, Batch 458, LR 0.000100 Loss 15.375513, Accuracy 21.345%\n",
      "Epoch 2, Batch 459, LR 0.000100 Loss 15.374651, Accuracy 21.361%\n",
      "Epoch 2, Batch 460, LR 0.000100 Loss 15.372938, Accuracy 21.372%\n",
      "Epoch 2, Batch 461, LR 0.000100 Loss 15.373305, Accuracy 21.360%\n",
      "Epoch 2, Batch 462, LR 0.000100 Loss 15.372778, Accuracy 21.363%\n",
      "Epoch 2, Batch 463, LR 0.000100 Loss 15.372336, Accuracy 21.364%\n",
      "Epoch 2, Batch 464, LR 0.000100 Loss 15.370787, Accuracy 21.378%\n",
      "Epoch 2, Batch 465, LR 0.000100 Loss 15.371539, Accuracy 21.373%\n",
      "Epoch 2, Batch 466, LR 0.000100 Loss 15.371892, Accuracy 21.374%\n",
      "Epoch 2, Batch 467, LR 0.000100 Loss 15.370250, Accuracy 21.390%\n",
      "Epoch 2, Batch 468, LR 0.000100 Loss 15.369996, Accuracy 21.394%\n",
      "Epoch 2, Batch 469, LR 0.000100 Loss 15.369167, Accuracy 21.405%\n",
      "Epoch 2, Batch 470, LR 0.000100 Loss 15.368831, Accuracy 21.413%\n",
      "Epoch 2, Batch 471, LR 0.000100 Loss 15.368826, Accuracy 21.419%\n",
      "Epoch 2, Batch 472, LR 0.000100 Loss 15.368273, Accuracy 21.420%\n",
      "Epoch 2, Batch 473, LR 0.000100 Loss 15.368429, Accuracy 21.417%\n",
      "Epoch 2, Batch 474, LR 0.000100 Loss 15.367671, Accuracy 21.430%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 475, LR 0.000100 Loss 15.368058, Accuracy 21.428%\n",
      "Epoch 2, Batch 476, LR 0.000100 Loss 15.367267, Accuracy 21.435%\n",
      "Epoch 2, Batch 477, LR 0.000100 Loss 15.366717, Accuracy 21.436%\n",
      "Epoch 2, Batch 478, LR 0.000100 Loss 15.366662, Accuracy 21.437%\n",
      "Epoch 2, Batch 479, LR 0.000100 Loss 15.366039, Accuracy 21.454%\n",
      "Epoch 2, Batch 480, LR 0.000100 Loss 15.366366, Accuracy 21.453%\n",
      "Epoch 2, Batch 481, LR 0.000100 Loss 15.365486, Accuracy 21.469%\n",
      "Epoch 2, Batch 482, LR 0.000100 Loss 15.365513, Accuracy 21.476%\n",
      "Epoch 2, Batch 483, LR 0.000100 Loss 15.365082, Accuracy 21.474%\n",
      "Epoch 2, Batch 484, LR 0.000100 Loss 15.363041, Accuracy 21.492%\n",
      "Epoch 2, Batch 485, LR 0.000100 Loss 15.362389, Accuracy 21.503%\n",
      "Epoch 2, Batch 486, LR 0.000100 Loss 15.361040, Accuracy 21.510%\n",
      "Epoch 2, Batch 487, LR 0.000100 Loss 15.359936, Accuracy 21.525%\n",
      "Epoch 2, Batch 488, LR 0.000100 Loss 15.359883, Accuracy 21.534%\n",
      "Epoch 2, Batch 489, LR 0.000100 Loss 15.359502, Accuracy 21.535%\n",
      "Epoch 2, Batch 490, LR 0.000100 Loss 15.359329, Accuracy 21.537%\n",
      "Epoch 2, Batch 491, LR 0.000100 Loss 15.358401, Accuracy 21.546%\n",
      "Epoch 2, Batch 492, LR 0.000100 Loss 15.356436, Accuracy 21.562%\n",
      "Epoch 2, Batch 493, LR 0.000100 Loss 15.355305, Accuracy 21.583%\n",
      "Epoch 2, Batch 494, LR 0.000100 Loss 15.355708, Accuracy 21.579%\n",
      "Epoch 2, Batch 495, LR 0.000100 Loss 15.354465, Accuracy 21.588%\n",
      "Epoch 2, Batch 496, LR 0.000100 Loss 15.354553, Accuracy 21.588%\n",
      "Epoch 2, Batch 497, LR 0.000100 Loss 15.354384, Accuracy 21.592%\n",
      "Epoch 2, Batch 498, LR 0.000100 Loss 15.354261, Accuracy 21.597%\n",
      "Epoch 2, Batch 499, LR 0.000100 Loss 15.353316, Accuracy 21.607%\n",
      "Epoch 2, Batch 500, LR 0.000100 Loss 15.352768, Accuracy 21.605%\n",
      "Epoch 2, Batch 501, LR 0.000100 Loss 15.351723, Accuracy 21.618%\n",
      "Epoch 2, Batch 502, LR 0.000100 Loss 15.350474, Accuracy 21.624%\n",
      "Epoch 2, Batch 503, LR 0.000100 Loss 15.349979, Accuracy 21.628%\n",
      "Epoch 2, Batch 504, LR 0.000100 Loss 15.349120, Accuracy 21.638%\n",
      "Epoch 2, Batch 505, LR 0.000100 Loss 15.348491, Accuracy 21.648%\n",
      "Epoch 2, Batch 506, LR 0.000100 Loss 15.348374, Accuracy 21.650%\n",
      "Epoch 2, Batch 507, LR 0.000100 Loss 15.346833, Accuracy 21.672%\n",
      "Epoch 2, Batch 508, LR 0.000100 Loss 15.345749, Accuracy 21.681%\n",
      "Epoch 2, Batch 509, LR 0.000100 Loss 15.345179, Accuracy 21.689%\n",
      "Epoch 2, Batch 510, LR 0.000100 Loss 15.344336, Accuracy 21.700%\n",
      "Epoch 2, Batch 511, LR 0.000100 Loss 15.344028, Accuracy 21.705%\n",
      "Epoch 2, Batch 512, LR 0.000100 Loss 15.342294, Accuracy 21.718%\n",
      "Epoch 2, Batch 513, LR 0.000100 Loss 15.340955, Accuracy 21.727%\n",
      "Epoch 2, Batch 514, LR 0.000100 Loss 15.341267, Accuracy 21.720%\n",
      "Epoch 2, Batch 515, LR 0.000100 Loss 15.341565, Accuracy 21.714%\n",
      "Epoch 2, Batch 516, LR 0.000100 Loss 15.341532, Accuracy 21.716%\n",
      "Epoch 2, Batch 517, LR 0.000100 Loss 15.341226, Accuracy 21.718%\n",
      "Epoch 2, Batch 518, LR 0.000100 Loss 15.340616, Accuracy 21.730%\n",
      "Epoch 2, Batch 519, LR 0.000100 Loss 15.339717, Accuracy 21.749%\n",
      "Epoch 2, Batch 520, LR 0.000100 Loss 15.339722, Accuracy 21.740%\n",
      "Epoch 2, Batch 521, LR 0.000100 Loss 15.339838, Accuracy 21.734%\n",
      "Epoch 2, Batch 522, LR 0.000100 Loss 15.340080, Accuracy 21.731%\n",
      "Epoch 2, Batch 523, LR 0.000100 Loss 15.339559, Accuracy 21.736%\n",
      "Epoch 2, Batch 524, LR 0.000100 Loss 15.339505, Accuracy 21.736%\n",
      "Epoch 2, Batch 525, LR 0.000100 Loss 15.338172, Accuracy 21.740%\n",
      "Epoch 2, Batch 526, LR 0.000100 Loss 15.337320, Accuracy 21.752%\n",
      "Epoch 2, Batch 527, LR 0.000100 Loss 15.337521, Accuracy 21.749%\n",
      "Epoch 2, Batch 528, LR 0.000100 Loss 15.337554, Accuracy 21.751%\n",
      "Epoch 2, Batch 529, LR 0.000100 Loss 15.336412, Accuracy 21.757%\n",
      "Epoch 2, Batch 530, LR 0.000100 Loss 15.335268, Accuracy 21.778%\n",
      "Epoch 2, Batch 531, LR 0.000100 Loss 15.334237, Accuracy 21.794%\n",
      "Epoch 2, Batch 532, LR 0.000100 Loss 15.334117, Accuracy 21.797%\n",
      "Epoch 2, Batch 533, LR 0.000100 Loss 15.333715, Accuracy 21.809%\n",
      "Epoch 2, Batch 534, LR 0.000100 Loss 15.332892, Accuracy 21.811%\n",
      "Epoch 2, Batch 535, LR 0.000100 Loss 15.332524, Accuracy 21.814%\n",
      "Epoch 2, Batch 536, LR 0.000100 Loss 15.332271, Accuracy 21.818%\n",
      "Epoch 2, Batch 537, LR 0.000100 Loss 15.331836, Accuracy 21.830%\n",
      "Epoch 2, Batch 538, LR 0.000100 Loss 15.331042, Accuracy 21.836%\n",
      "Epoch 2, Batch 539, LR 0.000100 Loss 15.330320, Accuracy 21.845%\n",
      "Epoch 2, Batch 540, LR 0.000100 Loss 15.328538, Accuracy 21.862%\n",
      "Epoch 2, Batch 541, LR 0.000100 Loss 15.327629, Accuracy 21.874%\n",
      "Epoch 2, Batch 542, LR 0.000100 Loss 15.328207, Accuracy 21.861%\n",
      "Epoch 2, Batch 543, LR 0.000100 Loss 15.328570, Accuracy 21.858%\n",
      "Epoch 2, Batch 544, LR 0.000100 Loss 15.327319, Accuracy 21.859%\n",
      "Epoch 2, Batch 545, LR 0.000100 Loss 15.326961, Accuracy 21.859%\n",
      "Epoch 2, Batch 546, LR 0.000100 Loss 15.326433, Accuracy 21.859%\n",
      "Epoch 2, Batch 547, LR 0.000100 Loss 15.325563, Accuracy 21.865%\n",
      "Epoch 2, Batch 548, LR 0.000100 Loss 15.324807, Accuracy 21.868%\n",
      "Epoch 2, Batch 549, LR 0.000100 Loss 15.324933, Accuracy 21.869%\n",
      "Epoch 2, Batch 550, LR 0.000100 Loss 15.324590, Accuracy 21.869%\n",
      "Epoch 2, Batch 551, LR 0.000100 Loss 15.322729, Accuracy 21.886%\n",
      "Epoch 2, Batch 552, LR 0.000100 Loss 15.322784, Accuracy 21.882%\n",
      "Epoch 2, Batch 553, LR 0.000100 Loss 15.322762, Accuracy 21.878%\n",
      "Epoch 2, Batch 554, LR 0.000100 Loss 15.322313, Accuracy 21.885%\n",
      "Epoch 2, Batch 555, LR 0.000100 Loss 15.321680, Accuracy 21.888%\n",
      "Epoch 2, Batch 556, LR 0.000100 Loss 15.320888, Accuracy 21.890%\n",
      "Epoch 2, Batch 557, LR 0.000100 Loss 15.319909, Accuracy 21.902%\n",
      "Epoch 2, Batch 558, LR 0.000100 Loss 15.318089, Accuracy 21.931%\n",
      "Epoch 2, Batch 559, LR 0.000100 Loss 15.316698, Accuracy 21.949%\n",
      "Epoch 2, Batch 560, LR 0.000100 Loss 15.316678, Accuracy 21.953%\n",
      "Epoch 2, Batch 561, LR 0.000100 Loss 15.316221, Accuracy 21.952%\n",
      "Epoch 2, Batch 562, LR 0.000100 Loss 15.316018, Accuracy 21.954%\n",
      "Epoch 2, Batch 563, LR 0.000100 Loss 15.315221, Accuracy 21.965%\n",
      "Epoch 2, Batch 564, LR 0.000100 Loss 15.315364, Accuracy 21.961%\n",
      "Epoch 2, Batch 565, LR 0.000100 Loss 15.315041, Accuracy 21.969%\n",
      "Epoch 2, Batch 566, LR 0.000100 Loss 15.314275, Accuracy 21.973%\n",
      "Epoch 2, Batch 567, LR 0.000100 Loss 15.314443, Accuracy 21.966%\n",
      "Epoch 2, Batch 568, LR 0.000100 Loss 15.313827, Accuracy 21.964%\n",
      "Epoch 2, Batch 569, LR 0.000100 Loss 15.313812, Accuracy 21.964%\n",
      "Epoch 2, Batch 570, LR 0.000100 Loss 15.312933, Accuracy 21.965%\n",
      "Epoch 2, Batch 571, LR 0.000100 Loss 15.312097, Accuracy 21.975%\n",
      "Epoch 2, Batch 572, LR 0.000100 Loss 15.312038, Accuracy 21.976%\n",
      "Epoch 2, Batch 573, LR 0.000100 Loss 15.311738, Accuracy 21.979%\n",
      "Epoch 2, Batch 574, LR 0.000100 Loss 15.311074, Accuracy 21.995%\n",
      "Epoch 2, Batch 575, LR 0.000100 Loss 15.310451, Accuracy 22.004%\n",
      "Epoch 2, Batch 576, LR 0.000100 Loss 15.309902, Accuracy 22.007%\n",
      "Epoch 2, Batch 577, LR 0.000100 Loss 15.309530, Accuracy 22.016%\n",
      "Epoch 2, Batch 578, LR 0.000100 Loss 15.308911, Accuracy 22.020%\n",
      "Epoch 2, Batch 579, LR 0.000100 Loss 15.308698, Accuracy 22.014%\n",
      "Epoch 2, Batch 580, LR 0.000100 Loss 15.308150, Accuracy 22.018%\n",
      "Epoch 2, Batch 581, LR 0.000100 Loss 15.307364, Accuracy 22.026%\n",
      "Epoch 2, Batch 582, LR 0.000100 Loss 15.307687, Accuracy 22.024%\n",
      "Epoch 2, Batch 583, LR 0.000100 Loss 15.307073, Accuracy 22.029%\n",
      "Epoch 2, Batch 584, LR 0.000100 Loss 15.305849, Accuracy 22.049%\n",
      "Epoch 2, Batch 585, LR 0.000100 Loss 15.305228, Accuracy 22.049%\n",
      "Epoch 2, Batch 586, LR 0.000100 Loss 15.304703, Accuracy 22.048%\n",
      "Epoch 2, Batch 587, LR 0.000100 Loss 15.305288, Accuracy 22.036%\n",
      "Epoch 2, Batch 588, LR 0.000100 Loss 15.304590, Accuracy 22.036%\n",
      "Epoch 2, Batch 589, LR 0.000100 Loss 15.303983, Accuracy 22.047%\n",
      "Epoch 2, Batch 590, LR 0.000100 Loss 15.303591, Accuracy 22.047%\n",
      "Epoch 2, Batch 591, LR 0.000100 Loss 15.302865, Accuracy 22.052%\n",
      "Epoch 2, Batch 592, LR 0.000100 Loss 15.301870, Accuracy 22.065%\n",
      "Epoch 2, Batch 593, LR 0.000100 Loss 15.300051, Accuracy 22.081%\n",
      "Epoch 2, Batch 594, LR 0.000100 Loss 15.299656, Accuracy 22.087%\n",
      "Epoch 2, Batch 595, LR 0.000100 Loss 15.299417, Accuracy 22.085%\n",
      "Epoch 2, Batch 596, LR 0.000100 Loss 15.298809, Accuracy 22.099%\n",
      "Epoch 2, Batch 597, LR 0.000100 Loss 15.297627, Accuracy 22.103%\n",
      "Epoch 2, Batch 598, LR 0.000100 Loss 15.297946, Accuracy 22.096%\n",
      "Epoch 2, Batch 599, LR 0.000100 Loss 15.296956, Accuracy 22.095%\n",
      "Epoch 2, Batch 600, LR 0.000100 Loss 15.296215, Accuracy 22.099%\n",
      "Epoch 2, Batch 601, LR 0.000100 Loss 15.295764, Accuracy 22.102%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 602, LR 0.000100 Loss 15.294957, Accuracy 22.116%\n",
      "Epoch 2, Batch 603, LR 0.000100 Loss 15.294523, Accuracy 22.119%\n",
      "Epoch 2, Batch 604, LR 0.000100 Loss 15.293517, Accuracy 22.130%\n",
      "Epoch 2, Batch 605, LR 0.000100 Loss 15.293052, Accuracy 22.127%\n",
      "Epoch 2, Batch 606, LR 0.000100 Loss 15.292082, Accuracy 22.141%\n",
      "Epoch 2, Batch 607, LR 0.000100 Loss 15.291107, Accuracy 22.161%\n",
      "Epoch 2, Batch 608, LR 0.000100 Loss 15.290940, Accuracy 22.163%\n",
      "Epoch 2, Batch 609, LR 0.000100 Loss 15.289776, Accuracy 22.174%\n",
      "Epoch 2, Batch 610, LR 0.000100 Loss 15.289467, Accuracy 22.180%\n",
      "Epoch 2, Batch 611, LR 0.000100 Loss 15.289170, Accuracy 22.177%\n",
      "Epoch 2, Batch 612, LR 0.000100 Loss 15.288916, Accuracy 22.180%\n",
      "Epoch 2, Batch 613, LR 0.000100 Loss 15.288486, Accuracy 22.187%\n",
      "Epoch 2, Batch 614, LR 0.000100 Loss 15.287670, Accuracy 22.201%\n",
      "Epoch 2, Batch 615, LR 0.000100 Loss 15.286312, Accuracy 22.212%\n",
      "Epoch 2, Batch 616, LR 0.000100 Loss 15.285509, Accuracy 22.214%\n",
      "Epoch 2, Batch 617, LR 0.000100 Loss 15.285332, Accuracy 22.223%\n",
      "Epoch 2, Batch 618, LR 0.000100 Loss 15.283501, Accuracy 22.233%\n",
      "Epoch 2, Batch 619, LR 0.000100 Loss 15.282824, Accuracy 22.236%\n",
      "Epoch 2, Batch 620, LR 0.000100 Loss 15.282401, Accuracy 22.232%\n",
      "Epoch 2, Batch 621, LR 0.000100 Loss 15.281227, Accuracy 22.244%\n",
      "Epoch 2, Batch 622, LR 0.000100 Loss 15.280984, Accuracy 22.247%\n",
      "Epoch 2, Batch 623, LR 0.000100 Loss 15.280951, Accuracy 22.246%\n",
      "Epoch 2, Batch 624, LR 0.000100 Loss 15.280706, Accuracy 22.249%\n",
      "Epoch 2, Batch 625, LR 0.000100 Loss 15.280287, Accuracy 22.256%\n",
      "Epoch 2, Batch 626, LR 0.000100 Loss 15.280514, Accuracy 22.248%\n",
      "Epoch 2, Batch 627, LR 0.000100 Loss 15.280057, Accuracy 22.250%\n",
      "Epoch 2, Batch 628, LR 0.000100 Loss 15.279077, Accuracy 22.256%\n",
      "Epoch 2, Batch 629, LR 0.000100 Loss 15.278244, Accuracy 22.266%\n",
      "Epoch 2, Batch 630, LR 0.000100 Loss 15.276332, Accuracy 22.288%\n",
      "Epoch 2, Batch 631, LR 0.000100 Loss 15.275738, Accuracy 22.290%\n",
      "Epoch 2, Batch 632, LR 0.000100 Loss 15.274548, Accuracy 22.305%\n",
      "Epoch 2, Batch 633, LR 0.000100 Loss 15.273123, Accuracy 22.322%\n",
      "Epoch 2, Batch 634, LR 0.000100 Loss 15.272805, Accuracy 22.322%\n",
      "Epoch 2, Batch 635, LR 0.000100 Loss 15.272306, Accuracy 22.330%\n",
      "Epoch 2, Batch 636, LR 0.000100 Loss 15.271454, Accuracy 22.334%\n",
      "Epoch 2, Batch 637, LR 0.000100 Loss 15.270965, Accuracy 22.334%\n",
      "Epoch 2, Batch 638, LR 0.000100 Loss 15.270417, Accuracy 22.337%\n",
      "Epoch 2, Batch 639, LR 0.000100 Loss 15.270858, Accuracy 22.327%\n",
      "Epoch 2, Batch 640, LR 0.000100 Loss 15.269840, Accuracy 22.344%\n",
      "Epoch 2, Batch 641, LR 0.000100 Loss 15.268854, Accuracy 22.354%\n",
      "Epoch 2, Batch 642, LR 0.000100 Loss 15.268000, Accuracy 22.358%\n",
      "Epoch 2, Batch 643, LR 0.000100 Loss 15.268120, Accuracy 22.356%\n",
      "Epoch 2, Batch 644, LR 0.000100 Loss 15.267366, Accuracy 22.360%\n",
      "Epoch 2, Batch 645, LR 0.000100 Loss 15.266534, Accuracy 22.376%\n",
      "Epoch 2, Batch 646, LR 0.000100 Loss 15.265869, Accuracy 22.382%\n",
      "Epoch 2, Batch 647, LR 0.000100 Loss 15.265730, Accuracy 22.386%\n",
      "Epoch 2, Batch 648, LR 0.000100 Loss 15.265881, Accuracy 22.385%\n",
      "Epoch 2, Batch 649, LR 0.000100 Loss 15.264781, Accuracy 22.399%\n",
      "Epoch 2, Batch 650, LR 0.000100 Loss 15.264577, Accuracy 22.394%\n",
      "Epoch 2, Batch 651, LR 0.000100 Loss 15.263890, Accuracy 22.404%\n",
      "Epoch 2, Batch 652, LR 0.000100 Loss 15.263602, Accuracy 22.413%\n",
      "Epoch 2, Batch 653, LR 0.000100 Loss 15.262719, Accuracy 22.417%\n",
      "Epoch 2, Batch 654, LR 0.000100 Loss 15.262311, Accuracy 22.417%\n",
      "Epoch 2, Batch 655, LR 0.000100 Loss 15.261560, Accuracy 22.430%\n",
      "Epoch 2, Batch 656, LR 0.000100 Loss 15.261314, Accuracy 22.429%\n",
      "Epoch 2, Batch 657, LR 0.000100 Loss 15.260726, Accuracy 22.430%\n",
      "Epoch 2, Batch 658, LR 0.000100 Loss 15.259378, Accuracy 22.451%\n",
      "Epoch 2, Batch 659, LR 0.000100 Loss 15.258234, Accuracy 22.456%\n",
      "Epoch 2, Batch 660, LR 0.000100 Loss 15.257329, Accuracy 22.461%\n",
      "Epoch 2, Batch 661, LR 0.000100 Loss 15.256945, Accuracy 22.458%\n",
      "Epoch 2, Batch 662, LR 0.000100 Loss 15.256630, Accuracy 22.466%\n",
      "Epoch 2, Batch 663, LR 0.000100 Loss 15.256167, Accuracy 22.467%\n",
      "Epoch 2, Batch 664, LR 0.000100 Loss 15.254964, Accuracy 22.479%\n",
      "Epoch 2, Batch 665, LR 0.000100 Loss 15.254538, Accuracy 22.487%\n",
      "Epoch 2, Batch 666, LR 0.000100 Loss 15.254457, Accuracy 22.487%\n",
      "Epoch 2, Batch 667, LR 0.000100 Loss 15.253542, Accuracy 22.499%\n",
      "Epoch 2, Batch 668, LR 0.000100 Loss 15.252855, Accuracy 22.508%\n",
      "Epoch 2, Batch 669, LR 0.000100 Loss 15.251796, Accuracy 22.522%\n",
      "Epoch 2, Batch 670, LR 0.000100 Loss 15.250925, Accuracy 22.529%\n",
      "Epoch 2, Batch 671, LR 0.000100 Loss 15.250319, Accuracy 22.534%\n",
      "Epoch 2, Batch 672, LR 0.000100 Loss 15.248931, Accuracy 22.545%\n",
      "Epoch 2, Batch 673, LR 0.000100 Loss 15.248382, Accuracy 22.553%\n",
      "Epoch 2, Batch 674, LR 0.000100 Loss 15.248367, Accuracy 22.547%\n",
      "Epoch 2, Batch 675, LR 0.000100 Loss 15.247931, Accuracy 22.557%\n",
      "Epoch 2, Batch 676, LR 0.000100 Loss 15.247558, Accuracy 22.561%\n",
      "Epoch 2, Batch 677, LR 0.000100 Loss 15.247593, Accuracy 22.563%\n",
      "Epoch 2, Batch 678, LR 0.000100 Loss 15.246873, Accuracy 22.573%\n",
      "Epoch 2, Batch 679, LR 0.000100 Loss 15.246159, Accuracy 22.581%\n",
      "Epoch 2, Batch 680, LR 0.000100 Loss 15.245400, Accuracy 22.595%\n",
      "Epoch 2, Batch 681, LR 0.000100 Loss 15.245120, Accuracy 22.601%\n",
      "Epoch 2, Batch 682, LR 0.000100 Loss 15.244163, Accuracy 22.613%\n",
      "Epoch 2, Batch 683, LR 0.000100 Loss 15.243369, Accuracy 22.621%\n",
      "Epoch 2, Batch 684, LR 0.000100 Loss 15.242569, Accuracy 22.623%\n",
      "Epoch 2, Batch 685, LR 0.000100 Loss 15.241642, Accuracy 22.636%\n",
      "Epoch 2, Batch 686, LR 0.000100 Loss 15.241101, Accuracy 22.639%\n",
      "Epoch 2, Batch 687, LR 0.000100 Loss 15.240985, Accuracy 22.646%\n",
      "Epoch 2, Batch 688, LR 0.000100 Loss 15.240904, Accuracy 22.644%\n",
      "Epoch 2, Batch 689, LR 0.000100 Loss 15.239654, Accuracy 22.657%\n",
      "Epoch 2, Batch 690, LR 0.000100 Loss 15.238794, Accuracy 22.666%\n",
      "Epoch 2, Batch 691, LR 0.000100 Loss 15.238243, Accuracy 22.673%\n",
      "Epoch 2, Batch 692, LR 0.000100 Loss 15.237414, Accuracy 22.680%\n",
      "Epoch 2, Batch 693, LR 0.000100 Loss 15.236010, Accuracy 22.687%\n",
      "Epoch 2, Batch 694, LR 0.000100 Loss 15.235925, Accuracy 22.690%\n",
      "Epoch 2, Batch 695, LR 0.000100 Loss 15.235744, Accuracy 22.689%\n",
      "Epoch 2, Batch 696, LR 0.000100 Loss 15.235012, Accuracy 22.696%\n",
      "Epoch 2, Batch 697, LR 0.000100 Loss 15.233850, Accuracy 22.711%\n",
      "Epoch 2, Batch 698, LR 0.000100 Loss 15.232826, Accuracy 22.717%\n",
      "Epoch 2, Batch 699, LR 0.000100 Loss 15.231978, Accuracy 22.728%\n",
      "Epoch 2, Batch 700, LR 0.000100 Loss 15.231815, Accuracy 22.737%\n",
      "Epoch 2, Batch 701, LR 0.000100 Loss 15.231656, Accuracy 22.730%\n",
      "Epoch 2, Batch 702, LR 0.000100 Loss 15.230750, Accuracy 22.743%\n",
      "Epoch 2, Batch 703, LR 0.000100 Loss 15.229844, Accuracy 22.750%\n",
      "Epoch 2, Batch 704, LR 0.000100 Loss 15.228969, Accuracy 22.762%\n",
      "Epoch 2, Batch 705, LR 0.000100 Loss 15.228464, Accuracy 22.768%\n",
      "Epoch 2, Batch 706, LR 0.000100 Loss 15.228233, Accuracy 22.771%\n",
      "Epoch 2, Batch 707, LR 0.000100 Loss 15.227719, Accuracy 22.776%\n",
      "Epoch 2, Batch 708, LR 0.000100 Loss 15.227404, Accuracy 22.771%\n",
      "Epoch 2, Batch 709, LR 0.000100 Loss 15.226972, Accuracy 22.771%\n",
      "Epoch 2, Batch 710, LR 0.000100 Loss 15.225945, Accuracy 22.784%\n",
      "Epoch 2, Batch 711, LR 0.000100 Loss 15.225771, Accuracy 22.787%\n",
      "Epoch 2, Batch 712, LR 0.000100 Loss 15.225249, Accuracy 22.788%\n",
      "Epoch 2, Batch 713, LR 0.000100 Loss 15.224570, Accuracy 22.791%\n",
      "Epoch 2, Batch 714, LR 0.000100 Loss 15.223883, Accuracy 22.797%\n",
      "Epoch 2, Batch 715, LR 0.000100 Loss 15.223735, Accuracy 22.802%\n",
      "Epoch 2, Batch 716, LR 0.000100 Loss 15.222728, Accuracy 22.810%\n",
      "Epoch 2, Batch 717, LR 0.000100 Loss 15.222042, Accuracy 22.822%\n",
      "Epoch 2, Batch 718, LR 0.000100 Loss 15.220839, Accuracy 22.836%\n",
      "Epoch 2, Batch 719, LR 0.000100 Loss 15.220700, Accuracy 22.838%\n",
      "Epoch 2, Batch 720, LR 0.000100 Loss 15.219618, Accuracy 22.850%\n",
      "Epoch 2, Batch 721, LR 0.000100 Loss 15.218624, Accuracy 22.862%\n",
      "Epoch 2, Batch 722, LR 0.000100 Loss 15.218689, Accuracy 22.860%\n",
      "Epoch 2, Batch 723, LR 0.000100 Loss 15.218565, Accuracy 22.862%\n",
      "Epoch 2, Batch 724, LR 0.000100 Loss 15.217858, Accuracy 22.866%\n",
      "Epoch 2, Batch 725, LR 0.000100 Loss 15.217375, Accuracy 22.869%\n",
      "Epoch 2, Batch 726, LR 0.000100 Loss 15.216861, Accuracy 22.874%\n",
      "Epoch 2, Batch 727, LR 0.000100 Loss 15.216189, Accuracy 22.882%\n",
      "Epoch 2, Batch 728, LR 0.000100 Loss 15.216141, Accuracy 22.885%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 729, LR 0.000100 Loss 15.215049, Accuracy 22.890%\n",
      "Epoch 2, Batch 730, LR 0.000100 Loss 15.214724, Accuracy 22.896%\n",
      "Epoch 2, Batch 731, LR 0.000100 Loss 15.214818, Accuracy 22.890%\n",
      "Epoch 2, Batch 732, LR 0.000100 Loss 15.213621, Accuracy 22.907%\n",
      "Epoch 2, Batch 733, LR 0.000100 Loss 15.213510, Accuracy 22.908%\n",
      "Epoch 2, Batch 734, LR 0.000100 Loss 15.212148, Accuracy 22.926%\n",
      "Epoch 2, Batch 735, LR 0.000100 Loss 15.211725, Accuracy 22.928%\n",
      "Epoch 2, Batch 736, LR 0.000100 Loss 15.210985, Accuracy 22.935%\n",
      "Epoch 2, Batch 737, LR 0.000100 Loss 15.210103, Accuracy 22.938%\n",
      "Epoch 2, Batch 738, LR 0.000100 Loss 15.209810, Accuracy 22.942%\n",
      "Epoch 2, Batch 739, LR 0.000100 Loss 15.208696, Accuracy 22.949%\n",
      "Epoch 2, Batch 740, LR 0.000100 Loss 15.207302, Accuracy 22.957%\n",
      "Epoch 2, Batch 741, LR 0.000100 Loss 15.206211, Accuracy 22.968%\n",
      "Epoch 2, Batch 742, LR 0.000100 Loss 15.205516, Accuracy 22.975%\n",
      "Epoch 2, Batch 743, LR 0.000100 Loss 15.204712, Accuracy 22.985%\n",
      "Epoch 2, Batch 744, LR 0.000100 Loss 15.204501, Accuracy 22.987%\n",
      "Epoch 2, Batch 745, LR 0.000100 Loss 15.204078, Accuracy 22.994%\n",
      "Epoch 2, Batch 746, LR 0.000100 Loss 15.203106, Accuracy 23.004%\n",
      "Epoch 2, Batch 747, LR 0.000100 Loss 15.202976, Accuracy 23.000%\n",
      "Epoch 2, Batch 748, LR 0.000100 Loss 15.202624, Accuracy 23.005%\n",
      "Epoch 2, Batch 749, LR 0.000100 Loss 15.202608, Accuracy 23.004%\n",
      "Epoch 2, Batch 750, LR 0.000100 Loss 15.202853, Accuracy 23.001%\n",
      "Epoch 2, Batch 751, LR 0.000100 Loss 15.201873, Accuracy 23.007%\n",
      "Epoch 2, Batch 752, LR 0.000100 Loss 15.201021, Accuracy 23.015%\n",
      "Epoch 2, Batch 753, LR 0.000100 Loss 15.200463, Accuracy 23.024%\n",
      "Epoch 2, Batch 754, LR 0.000100 Loss 15.200365, Accuracy 23.027%\n",
      "Epoch 2, Batch 755, LR 0.000100 Loss 15.199566, Accuracy 23.032%\n",
      "Epoch 2, Batch 756, LR 0.000100 Loss 15.198829, Accuracy 23.045%\n",
      "Epoch 2, Batch 757, LR 0.000100 Loss 15.198913, Accuracy 23.043%\n",
      "Epoch 2, Batch 758, LR 0.000100 Loss 15.198350, Accuracy 23.058%\n",
      "Epoch 2, Batch 759, LR 0.000100 Loss 15.197225, Accuracy 23.069%\n",
      "Epoch 2, Batch 760, LR 0.000100 Loss 15.197011, Accuracy 23.075%\n",
      "Epoch 2, Batch 761, LR 0.000100 Loss 15.196784, Accuracy 23.082%\n",
      "Epoch 2, Batch 762, LR 0.000100 Loss 15.196012, Accuracy 23.094%\n",
      "Epoch 2, Batch 763, LR 0.000100 Loss 15.195025, Accuracy 23.104%\n",
      "Epoch 2, Batch 764, LR 0.000100 Loss 15.194911, Accuracy 23.102%\n",
      "Epoch 2, Batch 765, LR 0.000100 Loss 15.194270, Accuracy 23.110%\n",
      "Epoch 2, Batch 766, LR 0.000100 Loss 15.193837, Accuracy 23.112%\n",
      "Epoch 2, Batch 767, LR 0.000100 Loss 15.192986, Accuracy 23.115%\n",
      "Epoch 2, Batch 768, LR 0.000100 Loss 15.191932, Accuracy 23.129%\n",
      "Epoch 2, Batch 769, LR 0.000100 Loss 15.191135, Accuracy 23.138%\n",
      "Epoch 2, Batch 770, LR 0.000100 Loss 15.190459, Accuracy 23.145%\n",
      "Epoch 2, Batch 771, LR 0.000100 Loss 15.190348, Accuracy 23.145%\n",
      "Epoch 2, Batch 772, LR 0.000100 Loss 15.189775, Accuracy 23.153%\n",
      "Epoch 2, Batch 773, LR 0.000100 Loss 15.188911, Accuracy 23.165%\n",
      "Epoch 2, Batch 774, LR 0.000100 Loss 15.187673, Accuracy 23.172%\n",
      "Epoch 2, Batch 775, LR 0.000100 Loss 15.186317, Accuracy 23.186%\n",
      "Epoch 2, Batch 776, LR 0.000100 Loss 15.185279, Accuracy 23.198%\n",
      "Epoch 2, Batch 777, LR 0.000100 Loss 15.184416, Accuracy 23.209%\n",
      "Epoch 2, Batch 778, LR 0.000100 Loss 15.183852, Accuracy 23.217%\n",
      "Epoch 2, Batch 779, LR 0.000100 Loss 15.183055, Accuracy 23.222%\n",
      "Epoch 2, Batch 780, LR 0.000100 Loss 15.182373, Accuracy 23.235%\n",
      "Epoch 2, Batch 781, LR 0.000100 Loss 15.181929, Accuracy 23.240%\n",
      "Epoch 2, Batch 782, LR 0.000100 Loss 15.181310, Accuracy 23.244%\n",
      "Epoch 2, Batch 783, LR 0.000100 Loss 15.180490, Accuracy 23.251%\n",
      "Epoch 2, Batch 784, LR 0.000100 Loss 15.180275, Accuracy 23.256%\n",
      "Epoch 2, Batch 785, LR 0.000100 Loss 15.179149, Accuracy 23.265%\n",
      "Epoch 2, Batch 786, LR 0.000100 Loss 15.178298, Accuracy 23.277%\n",
      "Epoch 2, Batch 787, LR 0.000100 Loss 15.177551, Accuracy 23.284%\n",
      "Epoch 2, Batch 788, LR 0.000100 Loss 15.177352, Accuracy 23.285%\n",
      "Epoch 2, Batch 789, LR 0.000100 Loss 15.176711, Accuracy 23.284%\n",
      "Epoch 2, Batch 790, LR 0.000100 Loss 15.176097, Accuracy 23.291%\n",
      "Epoch 2, Batch 791, LR 0.000100 Loss 15.175930, Accuracy 23.290%\n",
      "Epoch 2, Batch 792, LR 0.000100 Loss 15.174981, Accuracy 23.298%\n",
      "Epoch 2, Batch 793, LR 0.000100 Loss 15.173913, Accuracy 23.308%\n",
      "Epoch 2, Batch 794, LR 0.000100 Loss 15.172797, Accuracy 23.318%\n",
      "Epoch 2, Batch 795, LR 0.000100 Loss 15.172447, Accuracy 23.327%\n",
      "Epoch 2, Batch 796, LR 0.000100 Loss 15.171557, Accuracy 23.337%\n",
      "Epoch 2, Batch 797, LR 0.000100 Loss 15.171207, Accuracy 23.344%\n",
      "Epoch 2, Batch 798, LR 0.000100 Loss 15.170532, Accuracy 23.348%\n",
      "Epoch 2, Batch 799, LR 0.000100 Loss 15.169662, Accuracy 23.357%\n",
      "Epoch 2, Batch 800, LR 0.000100 Loss 15.168964, Accuracy 23.364%\n",
      "Epoch 2, Batch 801, LR 0.000100 Loss 15.167430, Accuracy 23.377%\n",
      "Epoch 2, Batch 802, LR 0.000100 Loss 15.165691, Accuracy 23.395%\n",
      "Epoch 2, Batch 803, LR 0.000100 Loss 15.164618, Accuracy 23.402%\n",
      "Epoch 2, Batch 804, LR 0.000100 Loss 15.164216, Accuracy 23.401%\n",
      "Epoch 2, Batch 805, LR 0.000100 Loss 15.163567, Accuracy 23.406%\n",
      "Epoch 2, Batch 806, LR 0.000100 Loss 15.162835, Accuracy 23.418%\n",
      "Epoch 2, Batch 807, LR 0.000100 Loss 15.162292, Accuracy 23.428%\n",
      "Epoch 2, Batch 808, LR 0.000100 Loss 15.161073, Accuracy 23.439%\n",
      "Epoch 2, Batch 809, LR 0.000100 Loss 15.160704, Accuracy 23.442%\n",
      "Epoch 2, Batch 810, LR 0.000100 Loss 15.159721, Accuracy 23.450%\n",
      "Epoch 2, Batch 811, LR 0.000100 Loss 15.159192, Accuracy 23.456%\n",
      "Epoch 2, Batch 812, LR 0.000100 Loss 15.158417, Accuracy 23.467%\n",
      "Epoch 2, Batch 813, LR 0.000100 Loss 15.157528, Accuracy 23.474%\n",
      "Epoch 2, Batch 814, LR 0.000100 Loss 15.156376, Accuracy 23.484%\n",
      "Epoch 2, Batch 815, LR 0.000100 Loss 15.155492, Accuracy 23.489%\n",
      "Epoch 2, Batch 816, LR 0.000100 Loss 15.155110, Accuracy 23.488%\n",
      "Epoch 2, Batch 817, LR 0.000100 Loss 15.154029, Accuracy 23.503%\n",
      "Epoch 2, Batch 818, LR 0.000100 Loss 15.153264, Accuracy 23.514%\n",
      "Epoch 2, Batch 819, LR 0.000100 Loss 15.152502, Accuracy 23.516%\n",
      "Epoch 2, Batch 820, LR 0.000100 Loss 15.152048, Accuracy 23.523%\n",
      "Epoch 2, Batch 821, LR 0.000100 Loss 15.151767, Accuracy 23.524%\n",
      "Epoch 2, Batch 822, LR 0.000100 Loss 15.151193, Accuracy 23.527%\n",
      "Epoch 2, Batch 823, LR 0.000100 Loss 15.150547, Accuracy 23.537%\n",
      "Epoch 2, Batch 824, LR 0.000100 Loss 15.150269, Accuracy 23.533%\n",
      "Epoch 2, Batch 825, LR 0.000100 Loss 15.149515, Accuracy 23.535%\n",
      "Epoch 2, Batch 826, LR 0.000100 Loss 15.149513, Accuracy 23.535%\n",
      "Epoch 2, Batch 827, LR 0.000100 Loss 15.149069, Accuracy 23.539%\n",
      "Epoch 2, Batch 828, LR 0.000100 Loss 15.147812, Accuracy 23.550%\n",
      "Epoch 2, Batch 829, LR 0.000100 Loss 15.147621, Accuracy 23.551%\n",
      "Epoch 2, Batch 830, LR 0.000100 Loss 15.147057, Accuracy 23.556%\n",
      "Epoch 2, Batch 831, LR 0.000100 Loss 15.146149, Accuracy 23.567%\n",
      "Epoch 2, Batch 832, LR 0.000100 Loss 15.146150, Accuracy 23.571%\n",
      "Epoch 2, Batch 833, LR 0.000100 Loss 15.144686, Accuracy 23.585%\n",
      "Epoch 2, Batch 834, LR 0.000100 Loss 15.143761, Accuracy 23.595%\n",
      "Epoch 2, Batch 835, LR 0.000100 Loss 15.143213, Accuracy 23.600%\n",
      "Epoch 2, Batch 836, LR 0.000100 Loss 15.142959, Accuracy 23.604%\n",
      "Epoch 2, Batch 837, LR 0.000100 Loss 15.142750, Accuracy 23.599%\n",
      "Epoch 2, Batch 838, LR 0.000100 Loss 15.142168, Accuracy 23.607%\n",
      "Epoch 2, Batch 839, LR 0.000100 Loss 15.141468, Accuracy 23.615%\n",
      "Epoch 2, Batch 840, LR 0.000100 Loss 15.141043, Accuracy 23.615%\n",
      "Epoch 2, Batch 841, LR 0.000100 Loss 15.140349, Accuracy 23.625%\n",
      "Epoch 2, Batch 842, LR 0.000100 Loss 15.139708, Accuracy 23.631%\n",
      "Epoch 2, Batch 843, LR 0.000100 Loss 15.139035, Accuracy 23.634%\n",
      "Epoch 2, Batch 844, LR 0.000100 Loss 15.138928, Accuracy 23.632%\n",
      "Epoch 2, Batch 845, LR 0.000100 Loss 15.138226, Accuracy 23.636%\n",
      "Epoch 2, Batch 846, LR 0.000100 Loss 15.138577, Accuracy 23.631%\n",
      "Epoch 2, Batch 847, LR 0.000100 Loss 15.138222, Accuracy 23.639%\n",
      "Epoch 2, Batch 848, LR 0.000100 Loss 15.137395, Accuracy 23.649%\n",
      "Epoch 2, Batch 849, LR 0.000100 Loss 15.137083, Accuracy 23.660%\n",
      "Epoch 2, Batch 850, LR 0.000100 Loss 15.136643, Accuracy 23.668%\n",
      "Epoch 2, Batch 851, LR 0.000100 Loss 15.136174, Accuracy 23.673%\n",
      "Epoch 2, Batch 852, LR 0.000100 Loss 15.135662, Accuracy 23.681%\n",
      "Epoch 2, Batch 853, LR 0.000100 Loss 15.135131, Accuracy 23.684%\n",
      "Epoch 2, Batch 854, LR 0.000100 Loss 15.134550, Accuracy 23.692%\n",
      "Epoch 2, Batch 855, LR 0.000100 Loss 15.133525, Accuracy 23.705%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 856, LR 0.000100 Loss 15.132742, Accuracy 23.714%\n",
      "Epoch 2, Batch 857, LR 0.000100 Loss 15.132678, Accuracy 23.716%\n",
      "Epoch 2, Batch 858, LR 0.000100 Loss 15.131854, Accuracy 23.719%\n",
      "Epoch 2, Batch 859, LR 0.000100 Loss 15.131143, Accuracy 23.728%\n",
      "Epoch 2, Batch 860, LR 0.000100 Loss 15.129740, Accuracy 23.743%\n",
      "Epoch 2, Batch 861, LR 0.000100 Loss 15.128966, Accuracy 23.752%\n",
      "Epoch 2, Batch 862, LR 0.000100 Loss 15.128286, Accuracy 23.757%\n",
      "Epoch 2, Batch 863, LR 0.000100 Loss 15.127862, Accuracy 23.763%\n",
      "Epoch 2, Batch 864, LR 0.000100 Loss 15.126904, Accuracy 23.770%\n",
      "Epoch 2, Batch 865, LR 0.000100 Loss 15.125830, Accuracy 23.783%\n",
      "Epoch 2, Batch 866, LR 0.000100 Loss 15.124975, Accuracy 23.796%\n",
      "Epoch 2, Batch 867, LR 0.000100 Loss 15.124056, Accuracy 23.802%\n",
      "Epoch 2, Batch 868, LR 0.000100 Loss 15.123227, Accuracy 23.807%\n",
      "Epoch 2, Batch 869, LR 0.000100 Loss 15.122153, Accuracy 23.818%\n",
      "Epoch 2, Batch 870, LR 0.000100 Loss 15.121951, Accuracy 23.817%\n",
      "Epoch 2, Batch 871, LR 0.000100 Loss 15.120958, Accuracy 23.825%\n",
      "Epoch 2, Batch 872, LR 0.000100 Loss 15.119944, Accuracy 23.838%\n",
      "Epoch 2, Batch 873, LR 0.000100 Loss 15.119417, Accuracy 23.839%\n",
      "Epoch 2, Batch 874, LR 0.000100 Loss 15.118484, Accuracy 23.854%\n",
      "Epoch 2, Batch 875, LR 0.000100 Loss 15.117798, Accuracy 23.861%\n",
      "Epoch 2, Batch 876, LR 0.000100 Loss 15.117100, Accuracy 23.867%\n",
      "Epoch 2, Batch 877, LR 0.000100 Loss 15.116533, Accuracy 23.870%\n",
      "Epoch 2, Batch 878, LR 0.000100 Loss 15.115943, Accuracy 23.878%\n",
      "Epoch 2, Batch 879, LR 0.000100 Loss 15.115708, Accuracy 23.881%\n",
      "Epoch 2, Batch 880, LR 0.000100 Loss 15.114679, Accuracy 23.888%\n",
      "Epoch 2, Batch 881, LR 0.000100 Loss 15.113678, Accuracy 23.894%\n",
      "Epoch 2, Batch 882, LR 0.000100 Loss 15.113129, Accuracy 23.895%\n",
      "Epoch 2, Batch 883, LR 0.000100 Loss 15.112665, Accuracy 23.901%\n",
      "Epoch 2, Batch 884, LR 0.000100 Loss 15.111911, Accuracy 23.912%\n",
      "Epoch 2, Batch 885, LR 0.000100 Loss 15.111202, Accuracy 23.915%\n",
      "Epoch 2, Batch 886, LR 0.000100 Loss 15.110852, Accuracy 23.919%\n",
      "Epoch 2, Batch 887, LR 0.000100 Loss 15.110376, Accuracy 23.925%\n",
      "Epoch 2, Batch 888, LR 0.000100 Loss 15.110158, Accuracy 23.922%\n",
      "Epoch 2, Batch 889, LR 0.000100 Loss 15.109347, Accuracy 23.929%\n",
      "Epoch 2, Batch 890, LR 0.000100 Loss 15.109411, Accuracy 23.925%\n",
      "Epoch 2, Batch 891, LR 0.000100 Loss 15.109273, Accuracy 23.921%\n",
      "Epoch 2, Batch 892, LR 0.000100 Loss 15.108765, Accuracy 23.920%\n",
      "Epoch 2, Batch 893, LR 0.000100 Loss 15.108258, Accuracy 23.924%\n",
      "Epoch 2, Batch 894, LR 0.000100 Loss 15.107808, Accuracy 23.929%\n",
      "Epoch 2, Batch 895, LR 0.000100 Loss 15.107639, Accuracy 23.929%\n",
      "Epoch 2, Batch 896, LR 0.000100 Loss 15.107071, Accuracy 23.930%\n",
      "Epoch 2, Batch 897, LR 0.000100 Loss 15.105881, Accuracy 23.939%\n",
      "Epoch 2, Batch 898, LR 0.000100 Loss 15.105181, Accuracy 23.945%\n",
      "Epoch 2, Batch 899, LR 0.000100 Loss 15.105096, Accuracy 23.945%\n",
      "Epoch 2, Batch 900, LR 0.000100 Loss 15.104688, Accuracy 23.955%\n",
      "Epoch 2, Batch 901, LR 0.000100 Loss 15.103846, Accuracy 23.963%\n",
      "Epoch 2, Batch 902, LR 0.000100 Loss 15.103353, Accuracy 23.964%\n",
      "Epoch 2, Batch 903, LR 0.000100 Loss 15.102869, Accuracy 23.976%\n",
      "Epoch 2, Batch 904, LR 0.000100 Loss 15.102131, Accuracy 23.982%\n",
      "Epoch 2, Batch 905, LR 0.000100 Loss 15.101450, Accuracy 23.985%\n",
      "Epoch 2, Batch 906, LR 0.000100 Loss 15.100614, Accuracy 23.995%\n",
      "Epoch 2, Batch 907, LR 0.000100 Loss 15.100063, Accuracy 24.001%\n",
      "Epoch 2, Batch 908, LR 0.000100 Loss 15.099198, Accuracy 24.010%\n",
      "Epoch 2, Batch 909, LR 0.000100 Loss 15.098334, Accuracy 24.010%\n",
      "Epoch 2, Batch 910, LR 0.000100 Loss 15.098061, Accuracy 24.012%\n",
      "Epoch 2, Batch 911, LR 0.000100 Loss 15.096791, Accuracy 24.023%\n",
      "Epoch 2, Batch 912, LR 0.000100 Loss 15.095405, Accuracy 24.035%\n",
      "Epoch 2, Batch 913, LR 0.000100 Loss 15.094691, Accuracy 24.045%\n",
      "Epoch 2, Batch 914, LR 0.000100 Loss 15.094245, Accuracy 24.045%\n",
      "Epoch 2, Batch 915, LR 0.000100 Loss 15.093263, Accuracy 24.052%\n",
      "Epoch 2, Batch 916, LR 0.000100 Loss 15.093396, Accuracy 24.053%\n",
      "Epoch 2, Batch 917, LR 0.000100 Loss 15.092979, Accuracy 24.056%\n",
      "Epoch 2, Batch 918, LR 0.000100 Loss 15.092348, Accuracy 24.061%\n",
      "Epoch 2, Batch 919, LR 0.000100 Loss 15.091761, Accuracy 24.063%\n",
      "Epoch 2, Batch 920, LR 0.000100 Loss 15.090719, Accuracy 24.078%\n",
      "Epoch 2, Batch 921, LR 0.000100 Loss 15.089955, Accuracy 24.086%\n",
      "Epoch 2, Batch 922, LR 0.000100 Loss 15.088854, Accuracy 24.097%\n",
      "Epoch 2, Batch 923, LR 0.000100 Loss 15.088206, Accuracy 24.100%\n",
      "Epoch 2, Batch 924, LR 0.000100 Loss 15.087322, Accuracy 24.105%\n",
      "Epoch 2, Batch 925, LR 0.000100 Loss 15.086589, Accuracy 24.116%\n",
      "Epoch 2, Batch 926, LR 0.000100 Loss 15.086597, Accuracy 24.116%\n",
      "Epoch 2, Batch 927, LR 0.000100 Loss 15.086014, Accuracy 24.118%\n",
      "Epoch 2, Batch 928, LR 0.000100 Loss 15.085874, Accuracy 24.116%\n",
      "Epoch 2, Batch 929, LR 0.000100 Loss 15.085203, Accuracy 24.122%\n",
      "Epoch 2, Batch 930, LR 0.000100 Loss 15.084752, Accuracy 24.124%\n",
      "Epoch 2, Batch 931, LR 0.000100 Loss 15.084396, Accuracy 24.130%\n",
      "Epoch 2, Batch 932, LR 0.000100 Loss 15.083902, Accuracy 24.135%\n",
      "Epoch 2, Batch 933, LR 0.000100 Loss 15.083042, Accuracy 24.147%\n",
      "Epoch 2, Batch 934, LR 0.000100 Loss 15.082777, Accuracy 24.151%\n",
      "Epoch 2, Batch 935, LR 0.000100 Loss 15.082297, Accuracy 24.154%\n",
      "Epoch 2, Batch 936, LR 0.000100 Loss 15.081378, Accuracy 24.161%\n",
      "Epoch 2, Batch 937, LR 0.000100 Loss 15.080778, Accuracy 24.168%\n",
      "Epoch 2, Batch 938, LR 0.000100 Loss 15.079990, Accuracy 24.175%\n",
      "Epoch 2, Batch 939, LR 0.000100 Loss 15.079956, Accuracy 24.172%\n",
      "Epoch 2, Batch 940, LR 0.000100 Loss 15.079676, Accuracy 24.176%\n",
      "Epoch 2, Batch 941, LR 0.000100 Loss 15.079133, Accuracy 24.179%\n",
      "Epoch 2, Batch 942, LR 0.000100 Loss 15.078623, Accuracy 24.186%\n",
      "Epoch 2, Batch 943, LR 0.000100 Loss 15.077016, Accuracy 24.199%\n",
      "Epoch 2, Batch 944, LR 0.000100 Loss 15.076359, Accuracy 24.208%\n",
      "Epoch 2, Batch 945, LR 0.000100 Loss 15.075723, Accuracy 24.214%\n",
      "Epoch 2, Batch 946, LR 0.000100 Loss 15.074234, Accuracy 24.227%\n",
      "Epoch 2, Batch 947, LR 0.000100 Loss 15.073619, Accuracy 24.230%\n",
      "Epoch 2, Batch 948, LR 0.000100 Loss 15.073155, Accuracy 24.237%\n",
      "Epoch 2, Batch 949, LR 0.000100 Loss 15.072333, Accuracy 24.248%\n",
      "Epoch 2, Batch 950, LR 0.000100 Loss 15.071771, Accuracy 24.251%\n",
      "Epoch 2, Batch 951, LR 0.000100 Loss 15.071290, Accuracy 24.263%\n",
      "Epoch 2, Batch 952, LR 0.000100 Loss 15.071052, Accuracy 24.265%\n",
      "Epoch 2, Batch 953, LR 0.000100 Loss 15.070762, Accuracy 24.272%\n",
      "Epoch 2, Batch 954, LR 0.000100 Loss 15.069857, Accuracy 24.282%\n",
      "Epoch 2, Batch 955, LR 0.000100 Loss 15.069460, Accuracy 24.283%\n",
      "Epoch 2, Batch 956, LR 0.000100 Loss 15.069232, Accuracy 24.285%\n",
      "Epoch 2, Batch 957, LR 0.000100 Loss 15.068729, Accuracy 24.288%\n",
      "Epoch 2, Batch 958, LR 0.000100 Loss 15.068084, Accuracy 24.292%\n",
      "Epoch 2, Batch 959, LR 0.000100 Loss 15.067540, Accuracy 24.301%\n",
      "Epoch 2, Batch 960, LR 0.000100 Loss 15.067196, Accuracy 24.306%\n",
      "Epoch 2, Batch 961, LR 0.000100 Loss 15.066827, Accuracy 24.308%\n",
      "Epoch 2, Batch 962, LR 0.000100 Loss 15.065942, Accuracy 24.319%\n",
      "Epoch 2, Batch 963, LR 0.000100 Loss 15.065743, Accuracy 24.323%\n",
      "Epoch 2, Batch 964, LR 0.000100 Loss 15.064655, Accuracy 24.330%\n",
      "Epoch 2, Batch 965, LR 0.000100 Loss 15.064211, Accuracy 24.340%\n",
      "Epoch 2, Batch 966, LR 0.000100 Loss 15.063482, Accuracy 24.345%\n",
      "Epoch 2, Batch 967, LR 0.000100 Loss 15.063021, Accuracy 24.348%\n",
      "Epoch 2, Batch 968, LR 0.000100 Loss 15.062083, Accuracy 24.358%\n",
      "Epoch 2, Batch 969, LR 0.000100 Loss 15.061312, Accuracy 24.361%\n",
      "Epoch 2, Batch 970, LR 0.000100 Loss 15.060941, Accuracy 24.366%\n",
      "Epoch 2, Batch 971, LR 0.000100 Loss 15.060865, Accuracy 24.369%\n",
      "Epoch 2, Batch 972, LR 0.000100 Loss 15.060680, Accuracy 24.374%\n",
      "Epoch 2, Batch 973, LR 0.000100 Loss 15.060347, Accuracy 24.374%\n",
      "Epoch 2, Batch 974, LR 0.000100 Loss 15.059782, Accuracy 24.377%\n",
      "Epoch 2, Batch 975, LR 0.000100 Loss 15.059421, Accuracy 24.379%\n",
      "Epoch 2, Batch 976, LR 0.000100 Loss 15.059169, Accuracy 24.377%\n",
      "Epoch 2, Batch 977, LR 0.000100 Loss 15.058459, Accuracy 24.384%\n",
      "Epoch 2, Batch 978, LR 0.000100 Loss 15.057890, Accuracy 24.387%\n",
      "Epoch 2, Batch 979, LR 0.000100 Loss 15.057314, Accuracy 24.397%\n",
      "Epoch 2, Batch 980, LR 0.000100 Loss 15.056798, Accuracy 24.409%\n",
      "Epoch 2, Batch 981, LR 0.000100 Loss 15.056630, Accuracy 24.409%\n",
      "Epoch 2, Batch 982, LR 0.000100 Loss 15.056346, Accuracy 24.410%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Batch 983, LR 0.000100 Loss 15.055871, Accuracy 24.416%\n",
      "Epoch 2, Batch 984, LR 0.000100 Loss 15.055695, Accuracy 24.416%\n",
      "Epoch 2, Batch 985, LR 0.000100 Loss 15.054808, Accuracy 24.423%\n",
      "Epoch 2, Batch 986, LR 0.000100 Loss 15.055043, Accuracy 24.418%\n",
      "Epoch 2, Batch 987, LR 0.000100 Loss 15.054955, Accuracy 24.421%\n",
      "Epoch 2, Batch 988, LR 0.000100 Loss 15.054451, Accuracy 24.424%\n",
      "Epoch 2, Batch 989, LR 0.000100 Loss 15.054371, Accuracy 24.423%\n",
      "Epoch 2, Batch 990, LR 0.000100 Loss 15.053176, Accuracy 24.429%\n",
      "Epoch 2, Batch 991, LR 0.000100 Loss 15.052904, Accuracy 24.434%\n",
      "Epoch 2, Batch 992, LR 0.000100 Loss 15.052152, Accuracy 24.435%\n",
      "Epoch 2, Batch 993, LR 0.000100 Loss 15.051391, Accuracy 24.445%\n",
      "Epoch 2, Batch 994, LR 0.000100 Loss 15.050271, Accuracy 24.454%\n",
      "Epoch 2, Batch 995, LR 0.000100 Loss 15.050218, Accuracy 24.454%\n",
      "Epoch 2, Batch 996, LR 0.000100 Loss 15.049403, Accuracy 24.461%\n",
      "Epoch 2, Batch 997, LR 0.000100 Loss 15.048492, Accuracy 24.463%\n",
      "Epoch 2, Batch 998, LR 0.000100 Loss 15.047358, Accuracy 24.468%\n",
      "Epoch 2, Batch 999, LR 0.000100 Loss 15.046961, Accuracy 24.468%\n",
      "Epoch 2, Batch 1000, LR 0.000100 Loss 15.046644, Accuracy 24.471%\n",
      "Epoch 2, Batch 1001, LR 0.000100 Loss 15.046067, Accuracy 24.476%\n",
      "Epoch 2, Batch 1002, LR 0.000100 Loss 15.045617, Accuracy 24.475%\n",
      "Epoch 2, Batch 1003, LR 0.000100 Loss 15.045239, Accuracy 24.476%\n",
      "Epoch 2, Batch 1004, LR 0.000100 Loss 15.044781, Accuracy 24.476%\n",
      "Epoch 2, Batch 1005, LR 0.000100 Loss 15.044227, Accuracy 24.482%\n",
      "Epoch 2, Batch 1006, LR 0.000100 Loss 15.042961, Accuracy 24.494%\n",
      "Epoch 2, Batch 1007, LR 0.000100 Loss 15.042694, Accuracy 24.495%\n",
      "Epoch 2, Batch 1008, LR 0.000100 Loss 15.042148, Accuracy 24.499%\n",
      "Epoch 2, Batch 1009, LR 0.000100 Loss 15.041608, Accuracy 24.502%\n",
      "Epoch 2, Batch 1010, LR 0.000100 Loss 15.041285, Accuracy 24.507%\n",
      "Epoch 2, Batch 1011, LR 0.000100 Loss 15.040342, Accuracy 24.510%\n",
      "Epoch 2, Batch 1012, LR 0.000100 Loss 15.039756, Accuracy 24.518%\n",
      "Epoch 2, Batch 1013, LR 0.000100 Loss 15.039216, Accuracy 24.526%\n",
      "Epoch 2, Batch 1014, LR 0.000100 Loss 15.038689, Accuracy 24.532%\n",
      "Epoch 2, Batch 1015, LR 0.000100 Loss 15.037516, Accuracy 24.547%\n",
      "Epoch 2, Batch 1016, LR 0.000100 Loss 15.036938, Accuracy 24.555%\n",
      "Epoch 2, Batch 1017, LR 0.000100 Loss 15.036893, Accuracy 24.557%\n",
      "Epoch 2, Batch 1018, LR 0.000100 Loss 15.036658, Accuracy 24.559%\n",
      "Epoch 2, Batch 1019, LR 0.000100 Loss 15.036117, Accuracy 24.562%\n",
      "Epoch 2, Batch 1020, LR 0.000100 Loss 15.035575, Accuracy 24.565%\n",
      "Epoch 2, Batch 1021, LR 0.000100 Loss 15.034774, Accuracy 24.568%\n",
      "Epoch 2, Batch 1022, LR 0.000100 Loss 15.034209, Accuracy 24.573%\n",
      "Epoch 2, Batch 1023, LR 0.000100 Loss 15.033527, Accuracy 24.578%\n",
      "Epoch 2, Batch 1024, LR 0.000100 Loss 15.033019, Accuracy 24.580%\n",
      "Epoch 2, Batch 1025, LR 0.000100 Loss 15.032124, Accuracy 24.583%\n",
      "Epoch 2, Batch 1026, LR 0.000100 Loss 15.031772, Accuracy 24.587%\n",
      "Epoch 2, Batch 1027, LR 0.000100 Loss 15.031333, Accuracy 24.593%\n",
      "Epoch 2, Batch 1028, LR 0.000100 Loss 15.031143, Accuracy 24.592%\n",
      "Epoch 2, Batch 1029, LR 0.000100 Loss 15.030649, Accuracy 24.593%\n",
      "Epoch 2, Batch 1030, LR 0.000100 Loss 15.030197, Accuracy 24.599%\n",
      "Epoch 2, Batch 1031, LR 0.000100 Loss 15.029686, Accuracy 24.610%\n",
      "Epoch 2, Batch 1032, LR 0.000100 Loss 15.029593, Accuracy 24.610%\n",
      "Epoch 2, Batch 1033, LR 0.000100 Loss 15.029521, Accuracy 24.611%\n",
      "Epoch 2, Batch 1034, LR 0.000100 Loss 15.029188, Accuracy 24.610%\n",
      "Epoch 2, Batch 1035, LR 0.000100 Loss 15.028752, Accuracy 24.618%\n",
      "Epoch 2, Batch 1036, LR 0.000100 Loss 15.028062, Accuracy 24.626%\n",
      "Epoch 2, Batch 1037, LR 0.000100 Loss 15.027391, Accuracy 24.635%\n",
      "Epoch 2, Batch 1038, LR 0.000100 Loss 15.027110, Accuracy 24.639%\n",
      "Epoch 2, Batch 1039, LR 0.000100 Loss 15.026739, Accuracy 24.641%\n",
      "Epoch 2, Batch 1040, LR 0.000100 Loss 15.026134, Accuracy 24.648%\n",
      "Epoch 2, Batch 1041, LR 0.000100 Loss 15.025393, Accuracy 24.656%\n",
      "Epoch 2, Batch 1042, LR 0.000100 Loss 15.024782, Accuracy 24.663%\n",
      "Epoch 2, Batch 1043, LR 0.000100 Loss 15.023924, Accuracy 24.670%\n",
      "Epoch 2, Batch 1044, LR 0.000100 Loss 15.023702, Accuracy 24.668%\n",
      "Epoch 2, Batch 1045, LR 0.000100 Loss 15.023063, Accuracy 24.673%\n",
      "Epoch 2, Batch 1046, LR 0.000100 Loss 15.022725, Accuracy 24.677%\n",
      "Epoch 2, Batch 1047, LR 0.000100 Loss 15.022520, Accuracy 24.679%\n",
      "Epoch 2, Loss (train set) 15.022520, Accuracy (train set) 24.679%\n",
      "Epoch 3, Batch 1, LR 0.000100 Loss 14.300044, Accuracy 32.812%\n",
      "Epoch 3, Batch 2, LR 0.000100 Loss 14.186650, Accuracy 28.516%\n",
      "Epoch 3, Batch 3, LR 0.000100 Loss 14.196801, Accuracy 30.729%\n",
      "Epoch 3, Batch 4, LR 0.000100 Loss 14.181024, Accuracy 31.055%\n",
      "Epoch 3, Batch 5, LR 0.000100 Loss 14.236581, Accuracy 31.562%\n",
      "Epoch 3, Batch 6, LR 0.000100 Loss 14.200192, Accuracy 31.641%\n",
      "Epoch 3, Batch 7, LR 0.000100 Loss 14.210448, Accuracy 32.701%\n",
      "Epoch 3, Batch 8, LR 0.000100 Loss 14.220010, Accuracy 32.324%\n",
      "Epoch 3, Batch 9, LR 0.000100 Loss 14.240618, Accuracy 32.118%\n",
      "Epoch 3, Batch 10, LR 0.000100 Loss 14.240960, Accuracy 32.031%\n",
      "Epoch 3, Batch 11, LR 0.000100 Loss 14.271605, Accuracy 31.818%\n",
      "Epoch 3, Batch 12, LR 0.000100 Loss 14.284964, Accuracy 31.576%\n",
      "Epoch 3, Batch 13, LR 0.000100 Loss 14.288949, Accuracy 31.550%\n",
      "Epoch 3, Batch 14, LR 0.000100 Loss 14.317612, Accuracy 31.306%\n",
      "Epoch 3, Batch 15, LR 0.000100 Loss 14.306093, Accuracy 31.615%\n",
      "Epoch 3, Batch 16, LR 0.000100 Loss 14.308177, Accuracy 31.689%\n",
      "Epoch 3, Batch 17, LR 0.000100 Loss 14.299501, Accuracy 31.893%\n",
      "Epoch 3, Batch 18, LR 0.000100 Loss 14.293764, Accuracy 31.771%\n",
      "Epoch 3, Batch 19, LR 0.000100 Loss 14.268405, Accuracy 32.319%\n",
      "Epoch 3, Batch 20, LR 0.000100 Loss 14.276770, Accuracy 32.383%\n",
      "Epoch 3, Batch 21, LR 0.000100 Loss 14.267683, Accuracy 32.366%\n",
      "Epoch 3, Batch 22, LR 0.000100 Loss 14.274089, Accuracy 32.280%\n",
      "Epoch 3, Batch 23, LR 0.000100 Loss 14.277428, Accuracy 32.099%\n",
      "Epoch 3, Batch 24, LR 0.000100 Loss 14.252709, Accuracy 32.324%\n",
      "Epoch 3, Batch 25, LR 0.000100 Loss 14.259263, Accuracy 32.125%\n",
      "Epoch 3, Batch 26, LR 0.000100 Loss 14.250794, Accuracy 32.272%\n",
      "Epoch 3, Batch 27, LR 0.000100 Loss 14.240543, Accuracy 32.494%\n",
      "Epoch 3, Batch 28, LR 0.000100 Loss 14.245002, Accuracy 32.394%\n",
      "Epoch 3, Batch 29, LR 0.000100 Loss 14.241091, Accuracy 32.516%\n",
      "Epoch 3, Batch 30, LR 0.000100 Loss 14.243497, Accuracy 32.578%\n",
      "Epoch 3, Batch 31, LR 0.000100 Loss 14.266930, Accuracy 32.283%\n",
      "Epoch 3, Batch 32, LR 0.000100 Loss 14.266318, Accuracy 32.373%\n",
      "Epoch 3, Batch 33, LR 0.000100 Loss 14.264193, Accuracy 32.363%\n",
      "Epoch 3, Batch 34, LR 0.000100 Loss 14.267014, Accuracy 32.353%\n",
      "Epoch 3, Batch 35, LR 0.000100 Loss 14.289037, Accuracy 32.098%\n",
      "Epoch 3, Batch 36, LR 0.000100 Loss 14.292669, Accuracy 31.901%\n",
      "Epoch 3, Batch 37, LR 0.000100 Loss 14.294308, Accuracy 31.947%\n",
      "Epoch 3, Batch 38, LR 0.000100 Loss 14.299020, Accuracy 31.887%\n",
      "Epoch 3, Batch 39, LR 0.000100 Loss 14.287602, Accuracy 31.971%\n",
      "Epoch 3, Batch 40, LR 0.000100 Loss 14.299732, Accuracy 31.758%\n",
      "Epoch 3, Batch 41, LR 0.000100 Loss 14.301027, Accuracy 31.822%\n",
      "Epoch 3, Batch 42, LR 0.000100 Loss 14.309691, Accuracy 31.715%\n",
      "Epoch 3, Batch 43, LR 0.000100 Loss 14.302316, Accuracy 31.759%\n",
      "Epoch 3, Batch 44, LR 0.000100 Loss 14.305978, Accuracy 31.694%\n",
      "Epoch 3, Batch 45, LR 0.000100 Loss 14.303979, Accuracy 31.649%\n",
      "Epoch 3, Batch 46, LR 0.000100 Loss 14.318671, Accuracy 31.556%\n",
      "Epoch 3, Batch 47, LR 0.000100 Loss 14.333093, Accuracy 31.300%\n",
      "Epoch 3, Batch 48, LR 0.000100 Loss 14.329905, Accuracy 31.364%\n",
      "Epoch 3, Batch 49, LR 0.000100 Loss 14.314633, Accuracy 31.617%\n",
      "Epoch 3, Batch 50, LR 0.000100 Loss 14.317221, Accuracy 31.562%\n",
      "Epoch 3, Batch 51, LR 0.000100 Loss 14.326783, Accuracy 31.357%\n",
      "Epoch 3, Batch 52, LR 0.000100 Loss 14.327669, Accuracy 31.370%\n",
      "Epoch 3, Batch 53, LR 0.000100 Loss 14.330300, Accuracy 31.383%\n",
      "Epoch 3, Batch 54, LR 0.000100 Loss 14.335720, Accuracy 31.380%\n",
      "Epoch 3, Batch 55, LR 0.000100 Loss 14.342942, Accuracy 31.293%\n",
      "Epoch 3, Batch 56, LR 0.000100 Loss 14.343811, Accuracy 31.320%\n",
      "Epoch 3, Batch 57, LR 0.000100 Loss 14.341586, Accuracy 31.360%\n",
      "Epoch 3, Batch 58, LR 0.000100 Loss 14.342359, Accuracy 31.344%\n",
      "Epoch 3, Batch 59, LR 0.000100 Loss 14.335030, Accuracy 31.449%\n",
      "Epoch 3, Batch 60, LR 0.000100 Loss 14.339518, Accuracy 31.380%\n",
      "Epoch 3, Batch 61, LR 0.000100 Loss 14.335474, Accuracy 31.429%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 62, LR 0.000100 Loss 14.328984, Accuracy 31.439%\n",
      "Epoch 3, Batch 63, LR 0.000100 Loss 14.327081, Accuracy 31.461%\n",
      "Epoch 3, Batch 64, LR 0.000100 Loss 14.323775, Accuracy 31.433%\n",
      "Epoch 3, Batch 65, LR 0.000100 Loss 14.330855, Accuracy 31.322%\n",
      "Epoch 3, Batch 66, LR 0.000100 Loss 14.334445, Accuracy 31.309%\n",
      "Epoch 3, Batch 67, LR 0.000100 Loss 14.321720, Accuracy 31.483%\n",
      "Epoch 3, Batch 68, LR 0.000100 Loss 14.315083, Accuracy 31.549%\n",
      "Epoch 3, Batch 69, LR 0.000100 Loss 14.312488, Accuracy 31.612%\n",
      "Epoch 3, Batch 70, LR 0.000100 Loss 14.317182, Accuracy 31.562%\n",
      "Epoch 3, Batch 71, LR 0.000100 Loss 14.316629, Accuracy 31.591%\n",
      "Epoch 3, Batch 72, LR 0.000100 Loss 14.314087, Accuracy 31.576%\n",
      "Epoch 3, Batch 73, LR 0.000100 Loss 14.312656, Accuracy 31.646%\n",
      "Epoch 3, Batch 74, LR 0.000100 Loss 14.310887, Accuracy 31.683%\n",
      "Epoch 3, Batch 75, LR 0.000100 Loss 14.306141, Accuracy 31.719%\n",
      "Epoch 3, Batch 76, LR 0.000100 Loss 14.313121, Accuracy 31.610%\n",
      "Epoch 3, Batch 77, LR 0.000100 Loss 14.307892, Accuracy 31.615%\n",
      "Epoch 3, Batch 78, LR 0.000100 Loss 14.299323, Accuracy 31.761%\n",
      "Epoch 3, Batch 79, LR 0.000100 Loss 14.292869, Accuracy 31.873%\n",
      "Epoch 3, Batch 80, LR 0.000100 Loss 14.295475, Accuracy 31.855%\n",
      "Epoch 3, Batch 81, LR 0.000100 Loss 14.283409, Accuracy 32.002%\n",
      "Epoch 3, Batch 82, LR 0.000100 Loss 14.282767, Accuracy 32.003%\n",
      "Epoch 3, Batch 83, LR 0.000100 Loss 14.275967, Accuracy 32.022%\n",
      "Epoch 3, Batch 84, LR 0.000100 Loss 14.272010, Accuracy 32.059%\n",
      "Epoch 3, Batch 85, LR 0.000100 Loss 14.267969, Accuracy 32.142%\n",
      "Epoch 3, Batch 86, LR 0.000100 Loss 14.270946, Accuracy 32.049%\n",
      "Epoch 3, Batch 87, LR 0.000100 Loss 14.273346, Accuracy 32.049%\n",
      "Epoch 3, Batch 88, LR 0.000100 Loss 14.269389, Accuracy 32.111%\n",
      "Epoch 3, Batch 89, LR 0.000100 Loss 14.265521, Accuracy 32.163%\n",
      "Epoch 3, Batch 90, LR 0.000100 Loss 14.270683, Accuracy 32.170%\n",
      "Epoch 3, Batch 91, LR 0.000100 Loss 14.272635, Accuracy 32.091%\n",
      "Epoch 3, Batch 92, LR 0.000100 Loss 14.271436, Accuracy 32.023%\n",
      "Epoch 3, Batch 93, LR 0.000100 Loss 14.272649, Accuracy 32.014%\n",
      "Epoch 3, Batch 94, LR 0.000100 Loss 14.275341, Accuracy 31.990%\n",
      "Epoch 3, Batch 95, LR 0.000100 Loss 14.276078, Accuracy 31.957%\n",
      "Epoch 3, Batch 96, LR 0.000100 Loss 14.274579, Accuracy 31.966%\n",
      "Epoch 3, Batch 97, LR 0.000100 Loss 14.279128, Accuracy 31.967%\n",
      "Epoch 3, Batch 98, LR 0.000100 Loss 14.277103, Accuracy 31.975%\n",
      "Epoch 3, Batch 99, LR 0.000100 Loss 14.275704, Accuracy 32.023%\n",
      "Epoch 3, Batch 100, LR 0.000100 Loss 14.275487, Accuracy 32.008%\n",
      "Epoch 3, Batch 101, LR 0.000100 Loss 14.273057, Accuracy 32.016%\n",
      "Epoch 3, Batch 102, LR 0.000100 Loss 14.271322, Accuracy 32.031%\n",
      "Epoch 3, Batch 103, LR 0.000100 Loss 14.267757, Accuracy 32.062%\n",
      "Epoch 3, Batch 104, LR 0.000100 Loss 14.261821, Accuracy 32.084%\n",
      "Epoch 3, Batch 105, LR 0.000100 Loss 14.262256, Accuracy 32.113%\n",
      "Epoch 3, Batch 106, LR 0.000100 Loss 14.257899, Accuracy 32.149%\n",
      "Epoch 3, Batch 107, LR 0.000100 Loss 14.258309, Accuracy 32.221%\n",
      "Epoch 3, Batch 108, LR 0.000100 Loss 14.253015, Accuracy 32.263%\n",
      "Epoch 3, Batch 109, LR 0.000100 Loss 14.250677, Accuracy 32.289%\n",
      "Epoch 3, Batch 110, LR 0.000100 Loss 14.246359, Accuracy 32.315%\n",
      "Epoch 3, Batch 111, LR 0.000100 Loss 14.245334, Accuracy 32.334%\n",
      "Epoch 3, Batch 112, LR 0.000100 Loss 14.249620, Accuracy 32.254%\n",
      "Epoch 3, Batch 113, LR 0.000100 Loss 14.249958, Accuracy 32.252%\n",
      "Epoch 3, Batch 114, LR 0.000100 Loss 14.246546, Accuracy 32.319%\n",
      "Epoch 3, Batch 115, LR 0.000100 Loss 14.246567, Accuracy 32.269%\n",
      "Epoch 3, Batch 116, LR 0.000100 Loss 14.245874, Accuracy 32.260%\n",
      "Epoch 3, Batch 117, LR 0.000100 Loss 14.245021, Accuracy 32.245%\n",
      "Epoch 3, Batch 118, LR 0.000100 Loss 14.246754, Accuracy 32.197%\n",
      "Epoch 3, Batch 119, LR 0.000100 Loss 14.244475, Accuracy 32.215%\n",
      "Epoch 3, Batch 120, LR 0.000100 Loss 14.244231, Accuracy 32.220%\n",
      "Epoch 3, Batch 121, LR 0.000100 Loss 14.241512, Accuracy 32.264%\n",
      "Epoch 3, Batch 122, LR 0.000100 Loss 14.246396, Accuracy 32.204%\n",
      "Epoch 3, Batch 123, LR 0.000100 Loss 14.249221, Accuracy 32.152%\n",
      "Epoch 3, Batch 124, LR 0.000100 Loss 14.250364, Accuracy 32.132%\n",
      "Epoch 3, Batch 125, LR 0.000100 Loss 14.249758, Accuracy 32.112%\n",
      "Epoch 3, Batch 126, LR 0.000100 Loss 14.250171, Accuracy 32.155%\n",
      "Epoch 3, Batch 127, LR 0.000100 Loss 14.252135, Accuracy 32.124%\n",
      "Epoch 3, Batch 128, LR 0.000100 Loss 14.252220, Accuracy 32.074%\n",
      "Epoch 3, Batch 129, LR 0.000100 Loss 14.250569, Accuracy 32.098%\n",
      "Epoch 3, Batch 130, LR 0.000100 Loss 14.252966, Accuracy 32.109%\n",
      "Epoch 3, Batch 131, LR 0.000100 Loss 14.253671, Accuracy 32.103%\n",
      "Epoch 3, Batch 132, LR 0.000100 Loss 14.253841, Accuracy 32.108%\n",
      "Epoch 3, Batch 133, LR 0.000100 Loss 14.253314, Accuracy 32.137%\n",
      "Epoch 3, Batch 134, LR 0.000100 Loss 14.252298, Accuracy 32.142%\n",
      "Epoch 3, Batch 135, LR 0.000100 Loss 14.250630, Accuracy 32.147%\n",
      "Epoch 3, Batch 136, LR 0.000100 Loss 14.250821, Accuracy 32.112%\n",
      "Epoch 3, Batch 137, LR 0.000100 Loss 14.250947, Accuracy 32.117%\n",
      "Epoch 3, Batch 138, LR 0.000100 Loss 14.253348, Accuracy 32.071%\n",
      "Epoch 3, Batch 139, LR 0.000100 Loss 14.249273, Accuracy 32.121%\n",
      "Epoch 3, Batch 140, LR 0.000100 Loss 14.249691, Accuracy 32.104%\n",
      "Epoch 3, Batch 141, LR 0.000100 Loss 14.251646, Accuracy 32.098%\n",
      "Epoch 3, Batch 142, LR 0.000100 Loss 14.249285, Accuracy 32.108%\n",
      "Epoch 3, Batch 143, LR 0.000100 Loss 14.249916, Accuracy 32.108%\n",
      "Epoch 3, Batch 144, LR 0.000100 Loss 14.248221, Accuracy 32.107%\n",
      "Epoch 3, Batch 145, LR 0.000100 Loss 14.248643, Accuracy 32.085%\n",
      "Epoch 3, Batch 146, LR 0.000100 Loss 14.245919, Accuracy 32.138%\n",
      "Epoch 3, Batch 147, LR 0.000100 Loss 14.243598, Accuracy 32.175%\n",
      "Epoch 3, Batch 148, LR 0.000100 Loss 14.244742, Accuracy 32.158%\n",
      "Epoch 3, Batch 149, LR 0.000100 Loss 14.242935, Accuracy 32.173%\n",
      "Epoch 3, Batch 150, LR 0.000100 Loss 14.245588, Accuracy 32.167%\n",
      "Epoch 3, Batch 151, LR 0.000100 Loss 14.245337, Accuracy 32.135%\n",
      "Epoch 3, Batch 152, LR 0.000100 Loss 14.247091, Accuracy 32.144%\n",
      "Epoch 3, Batch 153, LR 0.000100 Loss 14.249834, Accuracy 32.108%\n",
      "Epoch 3, Batch 154, LR 0.000100 Loss 14.248473, Accuracy 32.138%\n",
      "Epoch 3, Batch 155, LR 0.000100 Loss 14.248102, Accuracy 32.132%\n",
      "Epoch 3, Batch 156, LR 0.000100 Loss 14.247616, Accuracy 32.156%\n",
      "Epoch 3, Batch 157, LR 0.000100 Loss 14.242520, Accuracy 32.225%\n",
      "Epoch 3, Batch 158, LR 0.000100 Loss 14.240169, Accuracy 32.224%\n",
      "Epoch 3, Batch 159, LR 0.000100 Loss 14.242461, Accuracy 32.174%\n",
      "Epoch 3, Batch 160, LR 0.000100 Loss 14.237029, Accuracy 32.217%\n",
      "Epoch 3, Batch 161, LR 0.000100 Loss 14.233921, Accuracy 32.288%\n",
      "Epoch 3, Batch 162, LR 0.000100 Loss 14.232578, Accuracy 32.335%\n",
      "Epoch 3, Batch 163, LR 0.000100 Loss 14.234832, Accuracy 32.295%\n",
      "Epoch 3, Batch 164, LR 0.000100 Loss 14.234258, Accuracy 32.279%\n",
      "Epoch 3, Batch 165, LR 0.000100 Loss 14.236202, Accuracy 32.268%\n",
      "Epoch 3, Batch 166, LR 0.000100 Loss 14.238817, Accuracy 32.238%\n",
      "Epoch 3, Batch 167, LR 0.000100 Loss 14.238065, Accuracy 32.265%\n",
      "Epoch 3, Batch 168, LR 0.000100 Loss 14.237268, Accuracy 32.278%\n",
      "Epoch 3, Batch 169, LR 0.000100 Loss 14.235725, Accuracy 32.295%\n",
      "Epoch 3, Batch 170, LR 0.000100 Loss 14.234078, Accuracy 32.312%\n",
      "Epoch 3, Batch 171, LR 0.000100 Loss 14.227325, Accuracy 32.360%\n",
      "Epoch 3, Batch 172, LR 0.000100 Loss 14.225563, Accuracy 32.358%\n",
      "Epoch 3, Batch 173, LR 0.000100 Loss 14.226098, Accuracy 32.347%\n",
      "Epoch 3, Batch 174, LR 0.000100 Loss 14.229345, Accuracy 32.314%\n",
      "Epoch 3, Batch 175, LR 0.000100 Loss 14.230607, Accuracy 32.281%\n",
      "Epoch 3, Batch 176, LR 0.000100 Loss 14.230790, Accuracy 32.280%\n",
      "Epoch 3, Batch 177, LR 0.000100 Loss 14.229419, Accuracy 32.300%\n",
      "Epoch 3, Batch 178, LR 0.000100 Loss 14.228751, Accuracy 32.338%\n",
      "Epoch 3, Batch 179, LR 0.000100 Loss 14.229247, Accuracy 32.311%\n",
      "Epoch 3, Batch 180, LR 0.000100 Loss 14.231320, Accuracy 32.292%\n",
      "Epoch 3, Batch 181, LR 0.000100 Loss 14.229583, Accuracy 32.299%\n",
      "Epoch 3, Batch 182, LR 0.000100 Loss 14.228838, Accuracy 32.327%\n",
      "Epoch 3, Batch 183, LR 0.000100 Loss 14.226794, Accuracy 32.326%\n",
      "Epoch 3, Batch 184, LR 0.000100 Loss 14.227261, Accuracy 32.337%\n",
      "Epoch 3, Batch 185, LR 0.000100 Loss 14.224706, Accuracy 32.335%\n",
      "Epoch 3, Batch 186, LR 0.000100 Loss 14.223370, Accuracy 32.342%\n",
      "Epoch 3, Batch 187, LR 0.000100 Loss 14.221987, Accuracy 32.349%\n",
      "Epoch 3, Batch 188, LR 0.000100 Loss 14.222618, Accuracy 32.355%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 189, LR 0.000100 Loss 14.222495, Accuracy 32.378%\n",
      "Epoch 3, Batch 190, LR 0.000100 Loss 14.219019, Accuracy 32.410%\n",
      "Epoch 3, Batch 191, LR 0.000100 Loss 14.218197, Accuracy 32.440%\n",
      "Epoch 3, Batch 192, LR 0.000100 Loss 14.216489, Accuracy 32.442%\n",
      "Epoch 3, Batch 193, LR 0.000100 Loss 14.216116, Accuracy 32.428%\n",
      "Epoch 3, Batch 194, LR 0.000100 Loss 14.215065, Accuracy 32.414%\n",
      "Epoch 3, Batch 195, LR 0.000100 Loss 14.212503, Accuracy 32.452%\n",
      "Epoch 3, Batch 196, LR 0.000100 Loss 14.211232, Accuracy 32.478%\n",
      "Epoch 3, Batch 197, LR 0.000100 Loss 14.209414, Accuracy 32.495%\n",
      "Epoch 3, Batch 198, LR 0.000100 Loss 14.207320, Accuracy 32.509%\n",
      "Epoch 3, Batch 199, LR 0.000100 Loss 14.205653, Accuracy 32.502%\n",
      "Epoch 3, Batch 200, LR 0.000100 Loss 14.205805, Accuracy 32.512%\n",
      "Epoch 3, Batch 201, LR 0.000100 Loss 14.207374, Accuracy 32.505%\n",
      "Epoch 3, Batch 202, LR 0.000100 Loss 14.207055, Accuracy 32.507%\n",
      "Epoch 3, Batch 203, LR 0.000100 Loss 14.206284, Accuracy 32.535%\n",
      "Epoch 3, Batch 204, LR 0.000100 Loss 14.207805, Accuracy 32.510%\n",
      "Epoch 3, Batch 205, LR 0.000100 Loss 14.204654, Accuracy 32.523%\n",
      "Epoch 3, Batch 206, LR 0.000100 Loss 14.203811, Accuracy 32.517%\n",
      "Epoch 3, Batch 207, LR 0.000100 Loss 14.203698, Accuracy 32.507%\n",
      "Epoch 3, Batch 208, LR 0.000100 Loss 14.200038, Accuracy 32.531%\n",
      "Epoch 3, Batch 209, LR 0.000100 Loss 14.201085, Accuracy 32.510%\n",
      "Epoch 3, Batch 210, LR 0.000100 Loss 14.197523, Accuracy 32.537%\n",
      "Epoch 3, Batch 211, LR 0.000100 Loss 14.198872, Accuracy 32.524%\n",
      "Epoch 3, Batch 212, LR 0.000100 Loss 14.201076, Accuracy 32.470%\n",
      "Epoch 3, Batch 213, LR 0.000100 Loss 14.200579, Accuracy 32.475%\n",
      "Epoch 3, Batch 214, LR 0.000100 Loss 14.200119, Accuracy 32.484%\n",
      "Epoch 3, Batch 215, LR 0.000100 Loss 14.200094, Accuracy 32.471%\n",
      "Epoch 3, Batch 216, LR 0.000100 Loss 14.198982, Accuracy 32.494%\n",
      "Epoch 3, Batch 217, LR 0.000100 Loss 14.200215, Accuracy 32.481%\n",
      "Epoch 3, Batch 218, LR 0.000100 Loss 14.198092, Accuracy 32.504%\n",
      "Epoch 3, Batch 219, LR 0.000100 Loss 14.195759, Accuracy 32.499%\n",
      "Epoch 3, Batch 220, LR 0.000100 Loss 14.196785, Accuracy 32.472%\n",
      "Epoch 3, Batch 221, LR 0.000100 Loss 14.196785, Accuracy 32.438%\n",
      "Epoch 3, Batch 222, LR 0.000100 Loss 14.196717, Accuracy 32.408%\n",
      "Epoch 3, Batch 223, LR 0.000100 Loss 14.194699, Accuracy 32.413%\n",
      "Epoch 3, Batch 224, LR 0.000100 Loss 14.194051, Accuracy 32.415%\n",
      "Epoch 3, Batch 225, LR 0.000100 Loss 14.194449, Accuracy 32.406%\n",
      "Epoch 3, Batch 226, LR 0.000100 Loss 14.193115, Accuracy 32.415%\n",
      "Epoch 3, Batch 227, LR 0.000100 Loss 14.191399, Accuracy 32.451%\n",
      "Epoch 3, Batch 228, LR 0.000100 Loss 14.190579, Accuracy 32.439%\n",
      "Epoch 3, Batch 229, LR 0.000100 Loss 14.191637, Accuracy 32.437%\n",
      "Epoch 3, Batch 230, LR 0.000100 Loss 14.187942, Accuracy 32.466%\n",
      "Epoch 3, Batch 231, LR 0.000100 Loss 14.188562, Accuracy 32.484%\n",
      "Epoch 3, Batch 232, LR 0.000100 Loss 14.188362, Accuracy 32.489%\n",
      "Epoch 3, Batch 233, LR 0.000100 Loss 14.189897, Accuracy 32.481%\n",
      "Epoch 3, Batch 234, LR 0.000100 Loss 14.188940, Accuracy 32.492%\n",
      "Epoch 3, Batch 235, LR 0.000100 Loss 14.188727, Accuracy 32.493%\n",
      "Epoch 3, Batch 236, LR 0.000100 Loss 14.189753, Accuracy 32.478%\n",
      "Epoch 3, Batch 237, LR 0.000100 Loss 14.188956, Accuracy 32.483%\n",
      "Epoch 3, Batch 238, LR 0.000100 Loss 14.190436, Accuracy 32.474%\n",
      "Epoch 3, Batch 239, LR 0.000100 Loss 14.188855, Accuracy 32.492%\n",
      "Epoch 3, Batch 240, LR 0.000100 Loss 14.189051, Accuracy 32.471%\n",
      "Epoch 3, Batch 241, LR 0.000100 Loss 14.188051, Accuracy 32.475%\n",
      "Epoch 3, Batch 242, LR 0.000100 Loss 14.188034, Accuracy 32.486%\n",
      "Epoch 3, Batch 243, LR 0.000100 Loss 14.188934, Accuracy 32.488%\n",
      "Epoch 3, Batch 244, LR 0.000100 Loss 14.189935, Accuracy 32.489%\n",
      "Epoch 3, Batch 245, LR 0.000100 Loss 14.187534, Accuracy 32.522%\n",
      "Epoch 3, Batch 246, LR 0.000100 Loss 14.187593, Accuracy 32.527%\n",
      "Epoch 3, Batch 247, LR 0.000100 Loss 14.187097, Accuracy 32.540%\n",
      "Epoch 3, Batch 248, LR 0.000100 Loss 14.185590, Accuracy 32.551%\n",
      "Epoch 3, Batch 249, LR 0.000100 Loss 14.184626, Accuracy 32.565%\n",
      "Epoch 3, Batch 250, LR 0.000100 Loss 14.185287, Accuracy 32.525%\n",
      "Epoch 3, Batch 251, LR 0.000100 Loss 14.182572, Accuracy 32.548%\n",
      "Epoch 3, Batch 252, LR 0.000100 Loss 14.179875, Accuracy 32.561%\n",
      "Epoch 3, Batch 253, LR 0.000100 Loss 14.180111, Accuracy 32.562%\n",
      "Epoch 3, Batch 254, LR 0.000100 Loss 14.181249, Accuracy 32.551%\n",
      "Epoch 3, Batch 255, LR 0.000100 Loss 14.180392, Accuracy 32.561%\n",
      "Epoch 3, Batch 256, LR 0.000100 Loss 14.179701, Accuracy 32.562%\n",
      "Epoch 3, Batch 257, LR 0.000100 Loss 14.180690, Accuracy 32.572%\n",
      "Epoch 3, Batch 258, LR 0.000100 Loss 14.179394, Accuracy 32.570%\n",
      "Epoch 3, Batch 259, LR 0.000100 Loss 14.178195, Accuracy 32.580%\n",
      "Epoch 3, Batch 260, LR 0.000100 Loss 14.177905, Accuracy 32.581%\n",
      "Epoch 3, Batch 261, LR 0.000100 Loss 14.177227, Accuracy 32.594%\n",
      "Epoch 3, Batch 262, LR 0.000100 Loss 14.178357, Accuracy 32.568%\n",
      "Epoch 3, Batch 263, LR 0.000100 Loss 14.179181, Accuracy 32.557%\n",
      "Epoch 3, Batch 264, LR 0.000100 Loss 14.178246, Accuracy 32.588%\n",
      "Epoch 3, Batch 265, LR 0.000100 Loss 14.177396, Accuracy 32.591%\n",
      "Epoch 3, Batch 266, LR 0.000100 Loss 14.176944, Accuracy 32.598%\n",
      "Epoch 3, Batch 267, LR 0.000100 Loss 14.177187, Accuracy 32.599%\n",
      "Epoch 3, Batch 268, LR 0.000100 Loss 14.174310, Accuracy 32.635%\n",
      "Epoch 3, Batch 269, LR 0.000100 Loss 14.174497, Accuracy 32.632%\n",
      "Epoch 3, Batch 270, LR 0.000100 Loss 14.174973, Accuracy 32.633%\n",
      "Epoch 3, Batch 271, LR 0.000100 Loss 14.174050, Accuracy 32.640%\n",
      "Epoch 3, Batch 272, LR 0.000100 Loss 14.172908, Accuracy 32.652%\n",
      "Epoch 3, Batch 273, LR 0.000100 Loss 14.171263, Accuracy 32.658%\n",
      "Epoch 3, Batch 274, LR 0.000100 Loss 14.170286, Accuracy 32.664%\n",
      "Epoch 3, Batch 275, LR 0.000100 Loss 14.169839, Accuracy 32.690%\n",
      "Epoch 3, Batch 276, LR 0.000100 Loss 14.170249, Accuracy 32.674%\n",
      "Epoch 3, Batch 277, LR 0.000100 Loss 14.172102, Accuracy 32.646%\n",
      "Epoch 3, Batch 278, LR 0.000100 Loss 14.170089, Accuracy 32.680%\n",
      "Epoch 3, Batch 279, LR 0.000100 Loss 14.169018, Accuracy 32.672%\n",
      "Epoch 3, Batch 280, LR 0.000100 Loss 14.168658, Accuracy 32.679%\n",
      "Epoch 3, Batch 281, LR 0.000100 Loss 14.167681, Accuracy 32.687%\n",
      "Epoch 3, Batch 282, LR 0.000100 Loss 14.168026, Accuracy 32.682%\n",
      "Epoch 3, Batch 283, LR 0.000100 Loss 14.166610, Accuracy 32.680%\n",
      "Epoch 3, Batch 284, LR 0.000100 Loss 14.167047, Accuracy 32.664%\n",
      "Epoch 3, Batch 285, LR 0.000100 Loss 14.167127, Accuracy 32.673%\n",
      "Epoch 3, Batch 286, LR 0.000100 Loss 14.169547, Accuracy 32.654%\n",
      "Epoch 3, Batch 287, LR 0.000100 Loss 14.168380, Accuracy 32.666%\n",
      "Epoch 3, Batch 288, LR 0.000100 Loss 14.169052, Accuracy 32.658%\n",
      "Epoch 3, Batch 289, LR 0.000100 Loss 14.170479, Accuracy 32.648%\n",
      "Epoch 3, Batch 290, LR 0.000100 Loss 14.169803, Accuracy 32.637%\n",
      "Epoch 3, Batch 291, LR 0.000100 Loss 14.169124, Accuracy 32.649%\n",
      "Epoch 3, Batch 292, LR 0.000100 Loss 14.170045, Accuracy 32.641%\n",
      "Epoch 3, Batch 293, LR 0.000100 Loss 14.171225, Accuracy 32.623%\n",
      "Epoch 3, Batch 294, LR 0.000100 Loss 14.171173, Accuracy 32.624%\n",
      "Epoch 3, Batch 295, LR 0.000100 Loss 14.168619, Accuracy 32.651%\n",
      "Epoch 3, Batch 296, LR 0.000100 Loss 14.169397, Accuracy 32.625%\n",
      "Epoch 3, Batch 297, LR 0.000100 Loss 14.168270, Accuracy 32.644%\n",
      "Epoch 3, Batch 298, LR 0.000100 Loss 14.166457, Accuracy 32.658%\n",
      "Epoch 3, Batch 299, LR 0.000100 Loss 14.166564, Accuracy 32.666%\n",
      "Epoch 3, Batch 300, LR 0.000100 Loss 14.165710, Accuracy 32.664%\n",
      "Epoch 3, Batch 301, LR 0.000100 Loss 14.165854, Accuracy 32.659%\n",
      "Epoch 3, Batch 302, LR 0.000100 Loss 14.165220, Accuracy 32.660%\n",
      "Epoch 3, Batch 303, LR 0.000100 Loss 14.161361, Accuracy 32.696%\n",
      "Epoch 3, Batch 304, LR 0.000100 Loss 14.161408, Accuracy 32.697%\n",
      "Epoch 3, Batch 305, LR 0.000100 Loss 14.160369, Accuracy 32.700%\n",
      "Epoch 3, Batch 306, LR 0.000100 Loss 14.161910, Accuracy 32.680%\n",
      "Epoch 3, Batch 307, LR 0.000100 Loss 14.162562, Accuracy 32.657%\n",
      "Epoch 3, Batch 308, LR 0.000100 Loss 14.163706, Accuracy 32.648%\n",
      "Epoch 3, Batch 309, LR 0.000100 Loss 14.163808, Accuracy 32.648%\n",
      "Epoch 3, Batch 310, LR 0.000100 Loss 14.163358, Accuracy 32.656%\n",
      "Epoch 3, Batch 311, LR 0.000100 Loss 14.162565, Accuracy 32.672%\n",
      "Epoch 3, Batch 312, LR 0.000100 Loss 14.162517, Accuracy 32.670%\n",
      "Epoch 3, Batch 313, LR 0.000100 Loss 14.162621, Accuracy 32.653%\n",
      "Epoch 3, Batch 314, LR 0.000100 Loss 14.161749, Accuracy 32.656%\n",
      "Epoch 3, Batch 315, LR 0.000100 Loss 14.161382, Accuracy 32.669%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 316, LR 0.000100 Loss 14.160241, Accuracy 32.659%\n",
      "Epoch 3, Batch 317, LR 0.000100 Loss 14.159794, Accuracy 32.662%\n",
      "Epoch 3, Batch 318, LR 0.000100 Loss 14.160079, Accuracy 32.685%\n",
      "Epoch 3, Batch 319, LR 0.000100 Loss 14.160396, Accuracy 32.697%\n",
      "Epoch 3, Batch 320, LR 0.000100 Loss 14.159742, Accuracy 32.717%\n",
      "Epoch 3, Batch 321, LR 0.000100 Loss 14.159851, Accuracy 32.720%\n",
      "Epoch 3, Batch 322, LR 0.000100 Loss 14.160824, Accuracy 32.713%\n",
      "Epoch 3, Batch 323, LR 0.000100 Loss 14.159017, Accuracy 32.733%\n",
      "Epoch 3, Batch 324, LR 0.000100 Loss 14.157563, Accuracy 32.755%\n",
      "Epoch 3, Batch 325, LR 0.000100 Loss 14.157311, Accuracy 32.740%\n",
      "Epoch 3, Batch 326, LR 0.000100 Loss 14.156591, Accuracy 32.741%\n",
      "Epoch 3, Batch 327, LR 0.000100 Loss 14.158910, Accuracy 32.715%\n",
      "Epoch 3, Batch 328, LR 0.000100 Loss 14.159661, Accuracy 32.696%\n",
      "Epoch 3, Batch 329, LR 0.000100 Loss 14.158898, Accuracy 32.699%\n",
      "Epoch 3, Batch 330, LR 0.000100 Loss 14.157844, Accuracy 32.720%\n",
      "Epoch 3, Batch 331, LR 0.000100 Loss 14.156936, Accuracy 32.735%\n",
      "Epoch 3, Batch 332, LR 0.000100 Loss 14.157290, Accuracy 32.740%\n",
      "Epoch 3, Batch 333, LR 0.000100 Loss 14.156646, Accuracy 32.740%\n",
      "Epoch 3, Batch 334, LR 0.000100 Loss 14.156559, Accuracy 32.747%\n",
      "Epoch 3, Batch 335, LR 0.000100 Loss 14.154591, Accuracy 32.766%\n",
      "Epoch 3, Batch 336, LR 0.000100 Loss 14.154998, Accuracy 32.761%\n",
      "Epoch 3, Batch 337, LR 0.000100 Loss 14.156470, Accuracy 32.745%\n",
      "Epoch 3, Batch 338, LR 0.000100 Loss 14.155753, Accuracy 32.748%\n",
      "Epoch 3, Batch 339, LR 0.000100 Loss 14.154908, Accuracy 32.757%\n",
      "Epoch 3, Batch 340, LR 0.000100 Loss 14.154369, Accuracy 32.764%\n",
      "Epoch 3, Batch 341, LR 0.000100 Loss 14.156057, Accuracy 32.737%\n",
      "Epoch 3, Batch 342, LR 0.000100 Loss 14.154395, Accuracy 32.760%\n",
      "Epoch 3, Batch 343, LR 0.000100 Loss 14.151812, Accuracy 32.790%\n",
      "Epoch 3, Batch 344, LR 0.000100 Loss 14.150942, Accuracy 32.801%\n",
      "Epoch 3, Batch 345, LR 0.000100 Loss 14.150202, Accuracy 32.803%\n",
      "Epoch 3, Batch 346, LR 0.000100 Loss 14.151079, Accuracy 32.806%\n",
      "Epoch 3, Batch 347, LR 0.000100 Loss 14.152961, Accuracy 32.783%\n",
      "Epoch 3, Batch 348, LR 0.000100 Loss 14.153122, Accuracy 32.779%\n",
      "Epoch 3, Batch 349, LR 0.000100 Loss 14.153302, Accuracy 32.792%\n",
      "Epoch 3, Batch 350, LR 0.000100 Loss 14.151238, Accuracy 32.801%\n",
      "Epoch 3, Batch 351, LR 0.000100 Loss 14.152252, Accuracy 32.799%\n",
      "Epoch 3, Batch 352, LR 0.000100 Loss 14.151335, Accuracy 32.806%\n",
      "Epoch 3, Batch 353, LR 0.000100 Loss 14.152478, Accuracy 32.801%\n",
      "Epoch 3, Batch 354, LR 0.000100 Loss 14.151335, Accuracy 32.819%\n",
      "Epoch 3, Batch 355, LR 0.000100 Loss 14.151808, Accuracy 32.819%\n",
      "Epoch 3, Batch 356, LR 0.000100 Loss 14.151697, Accuracy 32.806%\n",
      "Epoch 3, Batch 357, LR 0.000100 Loss 14.152038, Accuracy 32.793%\n",
      "Epoch 3, Batch 358, LR 0.000100 Loss 14.149833, Accuracy 32.808%\n",
      "Epoch 3, Batch 359, LR 0.000100 Loss 14.148325, Accuracy 32.826%\n",
      "Epoch 3, Batch 360, LR 0.000100 Loss 14.147581, Accuracy 32.821%\n",
      "Epoch 3, Batch 361, LR 0.000100 Loss 14.145551, Accuracy 32.830%\n",
      "Epoch 3, Batch 362, LR 0.000100 Loss 14.144725, Accuracy 32.834%\n",
      "Epoch 3, Batch 363, LR 0.000100 Loss 14.143984, Accuracy 32.840%\n",
      "Epoch 3, Batch 364, LR 0.000100 Loss 14.143706, Accuracy 32.834%\n",
      "Epoch 3, Batch 365, LR 0.000100 Loss 14.143845, Accuracy 32.845%\n",
      "Epoch 3, Batch 366, LR 0.000100 Loss 14.144916, Accuracy 32.823%\n",
      "Epoch 3, Batch 367, LR 0.000100 Loss 14.144969, Accuracy 32.838%\n",
      "Epoch 3, Batch 368, LR 0.000100 Loss 14.142927, Accuracy 32.844%\n",
      "Epoch 3, Batch 369, LR 0.000100 Loss 14.142474, Accuracy 32.857%\n",
      "Epoch 3, Batch 370, LR 0.000100 Loss 14.141402, Accuracy 32.865%\n",
      "Epoch 3, Batch 371, LR 0.000100 Loss 14.139366, Accuracy 32.893%\n",
      "Epoch 3, Batch 372, LR 0.000100 Loss 14.138651, Accuracy 32.890%\n",
      "Epoch 3, Batch 373, LR 0.000100 Loss 14.138058, Accuracy 32.900%\n",
      "Epoch 3, Batch 374, LR 0.000100 Loss 14.139067, Accuracy 32.890%\n",
      "Epoch 3, Batch 375, LR 0.000100 Loss 14.137219, Accuracy 32.904%\n",
      "Epoch 3, Batch 376, LR 0.000100 Loss 14.136764, Accuracy 32.914%\n",
      "Epoch 3, Batch 377, LR 0.000100 Loss 14.136212, Accuracy 32.926%\n",
      "Epoch 3, Batch 378, LR 0.000100 Loss 14.134878, Accuracy 32.945%\n",
      "Epoch 3, Batch 379, LR 0.000100 Loss 14.135118, Accuracy 32.928%\n",
      "Epoch 3, Batch 380, LR 0.000100 Loss 14.133172, Accuracy 32.940%\n",
      "Epoch 3, Batch 381, LR 0.000100 Loss 14.131776, Accuracy 32.948%\n",
      "Epoch 3, Batch 382, LR 0.000100 Loss 14.132028, Accuracy 32.947%\n",
      "Epoch 3, Batch 383, LR 0.000100 Loss 14.132602, Accuracy 32.933%\n",
      "Epoch 3, Batch 384, LR 0.000100 Loss 14.131271, Accuracy 32.947%\n",
      "Epoch 3, Batch 385, LR 0.000100 Loss 14.131601, Accuracy 32.946%\n",
      "Epoch 3, Batch 386, LR 0.000100 Loss 14.132243, Accuracy 32.940%\n",
      "Epoch 3, Batch 387, LR 0.000100 Loss 14.130784, Accuracy 32.952%\n",
      "Epoch 3, Batch 388, LR 0.000100 Loss 14.129870, Accuracy 32.949%\n",
      "Epoch 3, Batch 389, LR 0.000100 Loss 14.129961, Accuracy 32.939%\n",
      "Epoch 3, Batch 390, LR 0.000100 Loss 14.128709, Accuracy 32.951%\n",
      "Epoch 3, Batch 391, LR 0.000100 Loss 14.128985, Accuracy 32.956%\n",
      "Epoch 3, Batch 392, LR 0.000100 Loss 14.129817, Accuracy 32.942%\n",
      "Epoch 3, Batch 393, LR 0.000100 Loss 14.129504, Accuracy 32.960%\n",
      "Epoch 3, Batch 394, LR 0.000100 Loss 14.128579, Accuracy 32.967%\n",
      "Epoch 3, Batch 395, LR 0.000100 Loss 14.128819, Accuracy 32.969%\n",
      "Epoch 3, Batch 396, LR 0.000100 Loss 14.128481, Accuracy 32.968%\n",
      "Epoch 3, Batch 397, LR 0.000100 Loss 14.129173, Accuracy 32.954%\n",
      "Epoch 3, Batch 398, LR 0.000100 Loss 14.128314, Accuracy 32.962%\n",
      "Epoch 3, Batch 399, LR 0.000100 Loss 14.126539, Accuracy 32.977%\n",
      "Epoch 3, Batch 400, LR 0.000100 Loss 14.125710, Accuracy 32.977%\n",
      "Epoch 3, Batch 401, LR 0.000100 Loss 14.124959, Accuracy 32.988%\n",
      "Epoch 3, Batch 402, LR 0.000100 Loss 14.124546, Accuracy 32.989%\n",
      "Epoch 3, Batch 403, LR 0.000100 Loss 14.122952, Accuracy 33.006%\n",
      "Epoch 3, Batch 404, LR 0.000100 Loss 14.123154, Accuracy 33.002%\n",
      "Epoch 3, Batch 405, LR 0.000100 Loss 14.123167, Accuracy 32.996%\n",
      "Epoch 3, Batch 406, LR 0.000100 Loss 14.123559, Accuracy 32.984%\n",
      "Epoch 3, Batch 407, LR 0.000100 Loss 14.122826, Accuracy 32.995%\n",
      "Epoch 3, Batch 408, LR 0.000100 Loss 14.121791, Accuracy 33.002%\n",
      "Epoch 3, Batch 409, LR 0.000100 Loss 14.120781, Accuracy 33.015%\n",
      "Epoch 3, Batch 410, LR 0.000100 Loss 14.121370, Accuracy 33.014%\n",
      "Epoch 3, Batch 411, LR 0.000100 Loss 14.120984, Accuracy 33.027%\n",
      "Epoch 3, Batch 412, LR 0.000100 Loss 14.121114, Accuracy 33.021%\n",
      "Epoch 3, Batch 413, LR 0.000100 Loss 14.119376, Accuracy 33.028%\n",
      "Epoch 3, Batch 414, LR 0.000100 Loss 14.120376, Accuracy 33.033%\n",
      "Epoch 3, Batch 415, LR 0.000100 Loss 14.119564, Accuracy 33.042%\n",
      "Epoch 3, Batch 416, LR 0.000100 Loss 14.119055, Accuracy 33.057%\n",
      "Epoch 3, Batch 417, LR 0.000100 Loss 14.118192, Accuracy 33.071%\n",
      "Epoch 3, Batch 418, LR 0.000100 Loss 14.117444, Accuracy 33.070%\n",
      "Epoch 3, Batch 419, LR 0.000100 Loss 14.117196, Accuracy 33.068%\n",
      "Epoch 3, Batch 420, LR 0.000100 Loss 14.116808, Accuracy 33.077%\n",
      "Epoch 3, Batch 421, LR 0.000100 Loss 14.115437, Accuracy 33.098%\n",
      "Epoch 3, Batch 422, LR 0.000100 Loss 14.115622, Accuracy 33.088%\n",
      "Epoch 3, Batch 423, LR 0.000100 Loss 14.114809, Accuracy 33.088%\n",
      "Epoch 3, Batch 424, LR 0.000100 Loss 14.114483, Accuracy 33.096%\n",
      "Epoch 3, Batch 425, LR 0.000100 Loss 14.113775, Accuracy 33.107%\n",
      "Epoch 3, Batch 426, LR 0.000100 Loss 14.113376, Accuracy 33.119%\n",
      "Epoch 3, Batch 427, LR 0.000100 Loss 14.113301, Accuracy 33.114%\n",
      "Epoch 3, Batch 428, LR 0.000100 Loss 14.112610, Accuracy 33.126%\n",
      "Epoch 3, Batch 429, LR 0.000100 Loss 14.110405, Accuracy 33.144%\n",
      "Epoch 3, Batch 430, LR 0.000100 Loss 14.108613, Accuracy 33.152%\n",
      "Epoch 3, Batch 431, LR 0.000100 Loss 14.109012, Accuracy 33.150%\n",
      "Epoch 3, Batch 432, LR 0.000100 Loss 14.107739, Accuracy 33.143%\n",
      "Epoch 3, Batch 433, LR 0.000100 Loss 14.106537, Accuracy 33.157%\n",
      "Epoch 3, Batch 434, LR 0.000100 Loss 14.107908, Accuracy 33.138%\n",
      "Epoch 3, Batch 435, LR 0.000100 Loss 14.106334, Accuracy 33.145%\n",
      "Epoch 3, Batch 436, LR 0.000100 Loss 14.105666, Accuracy 33.155%\n",
      "Epoch 3, Batch 437, LR 0.000100 Loss 14.104943, Accuracy 33.165%\n",
      "Epoch 3, Batch 438, LR 0.000100 Loss 14.103244, Accuracy 33.184%\n",
      "Epoch 3, Batch 439, LR 0.000100 Loss 14.101497, Accuracy 33.195%\n",
      "Epoch 3, Batch 440, LR 0.000100 Loss 14.101653, Accuracy 33.200%\n",
      "Epoch 3, Batch 441, LR 0.000100 Loss 14.101814, Accuracy 33.193%\n",
      "Epoch 3, Batch 442, LR 0.000100 Loss 14.101004, Accuracy 33.198%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 443, LR 0.000100 Loss 14.099914, Accuracy 33.197%\n",
      "Epoch 3, Batch 444, LR 0.000100 Loss 14.098648, Accuracy 33.207%\n",
      "Epoch 3, Batch 445, LR 0.000100 Loss 14.098793, Accuracy 33.211%\n",
      "Epoch 3, Batch 446, LR 0.000100 Loss 14.098669, Accuracy 33.212%\n",
      "Epoch 3, Batch 447, LR 0.000100 Loss 14.099202, Accuracy 33.209%\n",
      "Epoch 3, Batch 448, LR 0.000100 Loss 14.098954, Accuracy 33.207%\n",
      "Epoch 3, Batch 449, LR 0.000100 Loss 14.098199, Accuracy 33.211%\n",
      "Epoch 3, Batch 450, LR 0.000100 Loss 14.098728, Accuracy 33.207%\n",
      "Epoch 3, Batch 451, LR 0.000100 Loss 14.097270, Accuracy 33.232%\n",
      "Epoch 3, Batch 452, LR 0.000100 Loss 14.096660, Accuracy 33.238%\n",
      "Epoch 3, Batch 453, LR 0.000100 Loss 14.096943, Accuracy 33.242%\n",
      "Epoch 3, Batch 454, LR 0.000100 Loss 14.095332, Accuracy 33.251%\n",
      "Epoch 3, Batch 455, LR 0.000100 Loss 14.094613, Accuracy 33.266%\n",
      "Epoch 3, Batch 456, LR 0.000100 Loss 14.093901, Accuracy 33.268%\n",
      "Epoch 3, Batch 457, LR 0.000100 Loss 14.093957, Accuracy 33.271%\n",
      "Epoch 3, Batch 458, LR 0.000100 Loss 14.094708, Accuracy 33.259%\n",
      "Epoch 3, Batch 459, LR 0.000100 Loss 14.095357, Accuracy 33.253%\n",
      "Epoch 3, Batch 460, LR 0.000100 Loss 14.094431, Accuracy 33.254%\n",
      "Epoch 3, Batch 461, LR 0.000100 Loss 14.094406, Accuracy 33.250%\n",
      "Epoch 3, Batch 462, LR 0.000100 Loss 14.094712, Accuracy 33.249%\n",
      "Epoch 3, Batch 463, LR 0.000100 Loss 14.094005, Accuracy 33.261%\n",
      "Epoch 3, Batch 464, LR 0.000100 Loss 14.093419, Accuracy 33.264%\n",
      "Epoch 3, Batch 465, LR 0.000100 Loss 14.092585, Accuracy 33.266%\n",
      "Epoch 3, Batch 466, LR 0.000100 Loss 14.091886, Accuracy 33.270%\n",
      "Epoch 3, Batch 467, LR 0.000100 Loss 14.092267, Accuracy 33.271%\n",
      "Epoch 3, Batch 468, LR 0.000100 Loss 14.092524, Accuracy 33.262%\n",
      "Epoch 3, Batch 469, LR 0.000100 Loss 14.092288, Accuracy 33.271%\n",
      "Epoch 3, Batch 470, LR 0.000100 Loss 14.090865, Accuracy 33.288%\n",
      "Epoch 3, Batch 471, LR 0.000100 Loss 14.089619, Accuracy 33.300%\n",
      "Epoch 3, Batch 472, LR 0.000100 Loss 14.089220, Accuracy 33.301%\n",
      "Epoch 3, Batch 473, LR 0.000100 Loss 14.089032, Accuracy 33.295%\n",
      "Epoch 3, Batch 474, LR 0.000100 Loss 14.088932, Accuracy 33.292%\n",
      "Epoch 3, Batch 475, LR 0.000100 Loss 14.089110, Accuracy 33.289%\n",
      "Epoch 3, Batch 476, LR 0.000100 Loss 14.089033, Accuracy 33.290%\n",
      "Epoch 3, Batch 477, LR 0.000100 Loss 14.087803, Accuracy 33.304%\n",
      "Epoch 3, Batch 478, LR 0.000100 Loss 14.087339, Accuracy 33.318%\n",
      "Epoch 3, Batch 479, LR 0.000100 Loss 14.087069, Accuracy 33.320%\n",
      "Epoch 3, Batch 480, LR 0.000100 Loss 14.086964, Accuracy 33.319%\n",
      "Epoch 3, Batch 481, LR 0.000100 Loss 14.087161, Accuracy 33.305%\n",
      "Epoch 3, Batch 482, LR 0.000100 Loss 14.086799, Accuracy 33.312%\n",
      "Epoch 3, Batch 483, LR 0.000100 Loss 14.086474, Accuracy 33.312%\n",
      "Epoch 3, Batch 484, LR 0.000100 Loss 14.085135, Accuracy 33.311%\n",
      "Epoch 3, Batch 485, LR 0.000100 Loss 14.084995, Accuracy 33.307%\n",
      "Epoch 3, Batch 486, LR 0.000100 Loss 14.083922, Accuracy 33.312%\n",
      "Epoch 3, Batch 487, LR 0.000100 Loss 14.083907, Accuracy 33.307%\n",
      "Epoch 3, Batch 488, LR 0.000100 Loss 14.083684, Accuracy 33.312%\n",
      "Epoch 3, Batch 489, LR 0.000100 Loss 14.083315, Accuracy 33.316%\n",
      "Epoch 3, Batch 490, LR 0.000100 Loss 14.082418, Accuracy 33.329%\n",
      "Epoch 3, Batch 491, LR 0.000100 Loss 14.080724, Accuracy 33.339%\n",
      "Epoch 3, Batch 492, LR 0.000100 Loss 14.077259, Accuracy 33.376%\n",
      "Epoch 3, Batch 493, LR 0.000100 Loss 14.076776, Accuracy 33.383%\n",
      "Epoch 3, Batch 494, LR 0.000100 Loss 14.074466, Accuracy 33.401%\n",
      "Epoch 3, Batch 495, LR 0.000100 Loss 14.073682, Accuracy 33.392%\n",
      "Epoch 3, Batch 496, LR 0.000100 Loss 14.073959, Accuracy 33.384%\n",
      "Epoch 3, Batch 497, LR 0.000100 Loss 14.073269, Accuracy 33.391%\n",
      "Epoch 3, Batch 498, LR 0.000100 Loss 14.073169, Accuracy 33.384%\n",
      "Epoch 3, Batch 499, LR 0.000100 Loss 14.073101, Accuracy 33.389%\n",
      "Epoch 3, Batch 500, LR 0.000100 Loss 14.071405, Accuracy 33.409%\n",
      "Epoch 3, Batch 501, LR 0.000100 Loss 14.072432, Accuracy 33.400%\n",
      "Epoch 3, Batch 502, LR 0.000100 Loss 14.071070, Accuracy 33.409%\n",
      "Epoch 3, Batch 503, LR 0.000100 Loss 14.070618, Accuracy 33.412%\n",
      "Epoch 3, Batch 504, LR 0.000100 Loss 14.071242, Accuracy 33.395%\n",
      "Epoch 3, Batch 505, LR 0.000100 Loss 14.071123, Accuracy 33.382%\n",
      "Epoch 3, Batch 506, LR 0.000100 Loss 14.071251, Accuracy 33.376%\n",
      "Epoch 3, Batch 507, LR 0.000100 Loss 14.071769, Accuracy 33.372%\n",
      "Epoch 3, Batch 508, LR 0.000100 Loss 14.069827, Accuracy 33.391%\n",
      "Epoch 3, Batch 509, LR 0.000100 Loss 14.069158, Accuracy 33.394%\n",
      "Epoch 3, Batch 510, LR 0.000100 Loss 14.067537, Accuracy 33.415%\n",
      "Epoch 3, Batch 511, LR 0.000100 Loss 14.067081, Accuracy 33.423%\n",
      "Epoch 3, Batch 512, LR 0.000100 Loss 14.066987, Accuracy 33.423%\n",
      "Epoch 3, Batch 513, LR 0.000100 Loss 14.066352, Accuracy 33.429%\n",
      "Epoch 3, Batch 514, LR 0.000100 Loss 14.064938, Accuracy 33.455%\n",
      "Epoch 3, Batch 515, LR 0.000100 Loss 14.063685, Accuracy 33.462%\n",
      "Epoch 3, Batch 516, LR 0.000100 Loss 14.062959, Accuracy 33.471%\n",
      "Epoch 3, Batch 517, LR 0.000100 Loss 14.062028, Accuracy 33.482%\n",
      "Epoch 3, Batch 518, LR 0.000100 Loss 14.062513, Accuracy 33.469%\n",
      "Epoch 3, Batch 519, LR 0.000100 Loss 14.062173, Accuracy 33.476%\n",
      "Epoch 3, Batch 520, LR 0.000100 Loss 14.062924, Accuracy 33.460%\n",
      "Epoch 3, Batch 521, LR 0.000100 Loss 14.063180, Accuracy 33.453%\n",
      "Epoch 3, Batch 522, LR 0.000100 Loss 14.062815, Accuracy 33.459%\n",
      "Epoch 3, Batch 523, LR 0.000100 Loss 14.062896, Accuracy 33.458%\n",
      "Epoch 3, Batch 524, LR 0.000100 Loss 14.061963, Accuracy 33.464%\n",
      "Epoch 3, Batch 525, LR 0.000100 Loss 14.061685, Accuracy 33.476%\n",
      "Epoch 3, Batch 526, LR 0.000100 Loss 14.061115, Accuracy 33.487%\n",
      "Epoch 3, Batch 527, LR 0.000100 Loss 14.059239, Accuracy 33.499%\n",
      "Epoch 3, Batch 528, LR 0.000100 Loss 14.059131, Accuracy 33.501%\n",
      "Epoch 3, Batch 529, LR 0.000100 Loss 14.059197, Accuracy 33.521%\n",
      "Epoch 3, Batch 530, LR 0.000100 Loss 14.059054, Accuracy 33.523%\n",
      "Epoch 3, Batch 531, LR 0.000100 Loss 14.057829, Accuracy 33.528%\n",
      "Epoch 3, Batch 532, LR 0.000100 Loss 14.056604, Accuracy 33.538%\n",
      "Epoch 3, Batch 533, LR 0.000100 Loss 14.053594, Accuracy 33.564%\n",
      "Epoch 3, Batch 534, LR 0.000100 Loss 14.053414, Accuracy 33.570%\n",
      "Epoch 3, Batch 535, LR 0.000100 Loss 14.053237, Accuracy 33.563%\n",
      "Epoch 3, Batch 536, LR 0.000100 Loss 14.050798, Accuracy 33.588%\n",
      "Epoch 3, Batch 537, LR 0.000100 Loss 14.051231, Accuracy 33.581%\n",
      "Epoch 3, Batch 538, LR 0.000100 Loss 14.050821, Accuracy 33.585%\n",
      "Epoch 3, Batch 539, LR 0.000100 Loss 14.050286, Accuracy 33.591%\n",
      "Epoch 3, Batch 540, LR 0.000100 Loss 14.050361, Accuracy 33.598%\n",
      "Epoch 3, Batch 541, LR 0.000100 Loss 14.049163, Accuracy 33.608%\n",
      "Epoch 3, Batch 542, LR 0.000100 Loss 14.049038, Accuracy 33.614%\n",
      "Epoch 3, Batch 543, LR 0.000100 Loss 14.049660, Accuracy 33.608%\n",
      "Epoch 3, Batch 544, LR 0.000100 Loss 14.049640, Accuracy 33.608%\n",
      "Epoch 3, Batch 545, LR 0.000100 Loss 14.049015, Accuracy 33.612%\n",
      "Epoch 3, Batch 546, LR 0.000100 Loss 14.049462, Accuracy 33.611%\n",
      "Epoch 3, Batch 547, LR 0.000100 Loss 14.049474, Accuracy 33.605%\n",
      "Epoch 3, Batch 548, LR 0.000100 Loss 14.048605, Accuracy 33.611%\n",
      "Epoch 3, Batch 549, LR 0.000100 Loss 14.048406, Accuracy 33.608%\n",
      "Epoch 3, Batch 550, LR 0.000100 Loss 14.047974, Accuracy 33.599%\n",
      "Epoch 3, Batch 551, LR 0.000100 Loss 14.047267, Accuracy 33.604%\n",
      "Epoch 3, Batch 552, LR 0.000100 Loss 14.047381, Accuracy 33.597%\n",
      "Epoch 3, Batch 553, LR 0.000100 Loss 14.046415, Accuracy 33.612%\n",
      "Epoch 3, Batch 554, LR 0.000100 Loss 14.046027, Accuracy 33.609%\n",
      "Epoch 3, Batch 555, LR 0.000100 Loss 14.045883, Accuracy 33.611%\n",
      "Epoch 3, Batch 556, LR 0.000100 Loss 14.045598, Accuracy 33.612%\n",
      "Epoch 3, Batch 557, LR 0.000100 Loss 14.045948, Accuracy 33.609%\n",
      "Epoch 3, Batch 558, LR 0.000100 Loss 14.045247, Accuracy 33.619%\n",
      "Epoch 3, Batch 559, LR 0.000100 Loss 14.044716, Accuracy 33.624%\n",
      "Epoch 3, Batch 560, LR 0.000100 Loss 14.043851, Accuracy 33.633%\n",
      "Epoch 3, Batch 561, LR 0.000100 Loss 14.044571, Accuracy 33.631%\n",
      "Epoch 3, Batch 562, LR 0.000100 Loss 14.043734, Accuracy 33.641%\n",
      "Epoch 3, Batch 563, LR 0.000100 Loss 14.042990, Accuracy 33.648%\n",
      "Epoch 3, Batch 564, LR 0.000100 Loss 14.042593, Accuracy 33.657%\n",
      "Epoch 3, Batch 565, LR 0.000100 Loss 14.042299, Accuracy 33.663%\n",
      "Epoch 3, Batch 566, LR 0.000100 Loss 14.041220, Accuracy 33.674%\n",
      "Epoch 3, Batch 567, LR 0.000100 Loss 14.040523, Accuracy 33.686%\n",
      "Epoch 3, Batch 568, LR 0.000100 Loss 14.040557, Accuracy 33.683%\n",
      "Epoch 3, Batch 569, LR 0.000100 Loss 14.038893, Accuracy 33.705%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 570, LR 0.000100 Loss 14.038666, Accuracy 33.702%\n",
      "Epoch 3, Batch 571, LR 0.000100 Loss 14.037941, Accuracy 33.714%\n",
      "Epoch 3, Batch 572, LR 0.000100 Loss 14.036961, Accuracy 33.718%\n",
      "Epoch 3, Batch 573, LR 0.000100 Loss 14.036068, Accuracy 33.731%\n",
      "Epoch 3, Batch 574, LR 0.000100 Loss 14.034924, Accuracy 33.743%\n",
      "Epoch 3, Batch 575, LR 0.000100 Loss 14.034809, Accuracy 33.750%\n",
      "Epoch 3, Batch 576, LR 0.000100 Loss 14.035144, Accuracy 33.747%\n",
      "Epoch 3, Batch 577, LR 0.000100 Loss 14.034446, Accuracy 33.759%\n",
      "Epoch 3, Batch 578, LR 0.000100 Loss 14.033237, Accuracy 33.768%\n",
      "Epoch 3, Batch 579, LR 0.000100 Loss 14.033458, Accuracy 33.757%\n",
      "Epoch 3, Batch 580, LR 0.000100 Loss 14.033008, Accuracy 33.772%\n",
      "Epoch 3, Batch 581, LR 0.000100 Loss 14.031331, Accuracy 33.797%\n",
      "Epoch 3, Batch 582, LR 0.000100 Loss 14.030770, Accuracy 33.800%\n",
      "Epoch 3, Batch 583, LR 0.000100 Loss 14.030331, Accuracy 33.797%\n",
      "Epoch 3, Batch 584, LR 0.000100 Loss 14.030264, Accuracy 33.793%\n",
      "Epoch 3, Batch 585, LR 0.000100 Loss 14.030243, Accuracy 33.799%\n",
      "Epoch 3, Batch 586, LR 0.000100 Loss 14.029616, Accuracy 33.810%\n",
      "Epoch 3, Batch 587, LR 0.000100 Loss 14.028651, Accuracy 33.819%\n",
      "Epoch 3, Batch 588, LR 0.000100 Loss 14.028085, Accuracy 33.821%\n",
      "Epoch 3, Batch 589, LR 0.000100 Loss 14.027290, Accuracy 33.817%\n",
      "Epoch 3, Batch 590, LR 0.000100 Loss 14.025956, Accuracy 33.831%\n",
      "Epoch 3, Batch 591, LR 0.000100 Loss 14.024767, Accuracy 33.841%\n",
      "Epoch 3, Batch 592, LR 0.000100 Loss 14.023187, Accuracy 33.856%\n",
      "Epoch 3, Batch 593, LR 0.000100 Loss 14.022353, Accuracy 33.869%\n",
      "Epoch 3, Batch 594, LR 0.000100 Loss 14.021485, Accuracy 33.874%\n",
      "Epoch 3, Batch 595, LR 0.000100 Loss 14.020601, Accuracy 33.888%\n",
      "Epoch 3, Batch 596, LR 0.000100 Loss 14.020572, Accuracy 33.893%\n",
      "Epoch 3, Batch 597, LR 0.000100 Loss 14.018683, Accuracy 33.908%\n",
      "Epoch 3, Batch 598, LR 0.000100 Loss 14.018768, Accuracy 33.913%\n",
      "Epoch 3, Batch 599, LR 0.000100 Loss 14.018836, Accuracy 33.911%\n",
      "Epoch 3, Batch 600, LR 0.000100 Loss 14.019063, Accuracy 33.906%\n",
      "Epoch 3, Batch 601, LR 0.000100 Loss 14.018918, Accuracy 33.907%\n",
      "Epoch 3, Batch 602, LR 0.000100 Loss 14.018043, Accuracy 33.913%\n",
      "Epoch 3, Batch 603, LR 0.000100 Loss 14.018032, Accuracy 33.915%\n",
      "Epoch 3, Batch 604, LR 0.000100 Loss 14.017806, Accuracy 33.916%\n",
      "Epoch 3, Batch 605, LR 0.000100 Loss 14.017185, Accuracy 33.924%\n",
      "Epoch 3, Batch 606, LR 0.000100 Loss 14.016365, Accuracy 33.933%\n",
      "Epoch 3, Batch 607, LR 0.000100 Loss 14.015218, Accuracy 33.934%\n",
      "Epoch 3, Batch 608, LR 0.000100 Loss 14.015512, Accuracy 33.930%\n",
      "Epoch 3, Batch 609, LR 0.000100 Loss 14.014145, Accuracy 33.935%\n",
      "Epoch 3, Batch 610, LR 0.000100 Loss 14.013136, Accuracy 33.951%\n",
      "Epoch 3, Batch 611, LR 0.000100 Loss 14.012607, Accuracy 33.961%\n",
      "Epoch 3, Batch 612, LR 0.000100 Loss 14.012078, Accuracy 33.967%\n",
      "Epoch 3, Batch 613, LR 0.000100 Loss 14.011090, Accuracy 33.982%\n",
      "Epoch 3, Batch 614, LR 0.000100 Loss 14.011245, Accuracy 33.975%\n",
      "Epoch 3, Batch 615, LR 0.000100 Loss 14.010862, Accuracy 33.971%\n",
      "Epoch 3, Batch 616, LR 0.000100 Loss 14.010251, Accuracy 33.979%\n",
      "Epoch 3, Batch 617, LR 0.000100 Loss 14.008979, Accuracy 33.991%\n",
      "Epoch 3, Batch 618, LR 0.000100 Loss 14.008015, Accuracy 34.002%\n",
      "Epoch 3, Batch 619, LR 0.000100 Loss 14.007357, Accuracy 34.010%\n",
      "Epoch 3, Batch 620, LR 0.000100 Loss 14.006567, Accuracy 34.016%\n",
      "Epoch 3, Batch 621, LR 0.000100 Loss 14.006368, Accuracy 34.021%\n",
      "Epoch 3, Batch 622, LR 0.000100 Loss 14.006501, Accuracy 34.016%\n",
      "Epoch 3, Batch 623, LR 0.000100 Loss 14.005075, Accuracy 34.031%\n",
      "Epoch 3, Batch 624, LR 0.000100 Loss 14.003991, Accuracy 34.048%\n",
      "Epoch 3, Batch 625, LR 0.000100 Loss 14.003425, Accuracy 34.047%\n",
      "Epoch 3, Batch 626, LR 0.000100 Loss 14.003022, Accuracy 34.052%\n",
      "Epoch 3, Batch 627, LR 0.000100 Loss 14.003109, Accuracy 34.051%\n",
      "Epoch 3, Batch 628, LR 0.000100 Loss 14.002764, Accuracy 34.053%\n",
      "Epoch 3, Batch 629, LR 0.000100 Loss 14.001761, Accuracy 34.067%\n",
      "Epoch 3, Batch 630, LR 0.000100 Loss 14.001853, Accuracy 34.067%\n",
      "Epoch 3, Batch 631, LR 0.000100 Loss 14.001375, Accuracy 34.067%\n",
      "Epoch 3, Batch 632, LR 0.000100 Loss 14.000915, Accuracy 34.067%\n",
      "Epoch 3, Batch 633, LR 0.000100 Loss 14.000435, Accuracy 34.071%\n",
      "Epoch 3, Batch 634, LR 0.000100 Loss 14.000234, Accuracy 34.071%\n",
      "Epoch 3, Batch 635, LR 0.000100 Loss 14.000389, Accuracy 34.066%\n",
      "Epoch 3, Batch 636, LR 0.000100 Loss 13.999441, Accuracy 34.078%\n",
      "Epoch 3, Batch 637, LR 0.000100 Loss 13.998576, Accuracy 34.088%\n",
      "Epoch 3, Batch 638, LR 0.000100 Loss 13.997842, Accuracy 34.098%\n",
      "Epoch 3, Batch 639, LR 0.000100 Loss 13.996848, Accuracy 34.115%\n",
      "Epoch 3, Batch 640, LR 0.000100 Loss 13.996140, Accuracy 34.117%\n",
      "Epoch 3, Batch 641, LR 0.000100 Loss 13.995411, Accuracy 34.118%\n",
      "Epoch 3, Batch 642, LR 0.000100 Loss 13.994875, Accuracy 34.123%\n",
      "Epoch 3, Batch 643, LR 0.000100 Loss 13.995201, Accuracy 34.119%\n",
      "Epoch 3, Batch 644, LR 0.000100 Loss 13.994369, Accuracy 34.126%\n",
      "Epoch 3, Batch 645, LR 0.000100 Loss 13.993318, Accuracy 34.133%\n",
      "Epoch 3, Batch 646, LR 0.000100 Loss 13.992714, Accuracy 34.137%\n",
      "Epoch 3, Batch 647, LR 0.000100 Loss 13.992626, Accuracy 34.136%\n",
      "Epoch 3, Batch 648, LR 0.000100 Loss 13.991918, Accuracy 34.145%\n",
      "Epoch 3, Batch 649, LR 0.000100 Loss 13.992831, Accuracy 34.137%\n",
      "Epoch 3, Batch 650, LR 0.000100 Loss 13.992363, Accuracy 34.138%\n",
      "Epoch 3, Batch 651, LR 0.000100 Loss 13.991250, Accuracy 34.149%\n",
      "Epoch 3, Batch 652, LR 0.000100 Loss 13.991050, Accuracy 34.158%\n",
      "Epoch 3, Batch 653, LR 0.000100 Loss 13.990399, Accuracy 34.178%\n",
      "Epoch 3, Batch 654, LR 0.000100 Loss 13.989123, Accuracy 34.184%\n",
      "Epoch 3, Batch 655, LR 0.000100 Loss 13.988458, Accuracy 34.185%\n",
      "Epoch 3, Batch 656, LR 0.000100 Loss 13.988334, Accuracy 34.189%\n",
      "Epoch 3, Batch 657, LR 0.000100 Loss 13.987775, Accuracy 34.191%\n",
      "Epoch 3, Batch 658, LR 0.000100 Loss 13.988045, Accuracy 34.181%\n",
      "Epoch 3, Batch 659, LR 0.000100 Loss 13.987939, Accuracy 34.179%\n",
      "Epoch 3, Batch 660, LR 0.000100 Loss 13.988771, Accuracy 34.169%\n",
      "Epoch 3, Batch 661, LR 0.000100 Loss 13.987836, Accuracy 34.179%\n",
      "Epoch 3, Batch 662, LR 0.000100 Loss 13.987430, Accuracy 34.185%\n",
      "Epoch 3, Batch 663, LR 0.000100 Loss 13.986327, Accuracy 34.195%\n",
      "Epoch 3, Batch 664, LR 0.000100 Loss 13.985793, Accuracy 34.199%\n",
      "Epoch 3, Batch 665, LR 0.000100 Loss 13.984439, Accuracy 34.205%\n",
      "Epoch 3, Batch 666, LR 0.000100 Loss 13.984103, Accuracy 34.204%\n",
      "Epoch 3, Batch 667, LR 0.000100 Loss 13.983873, Accuracy 34.200%\n",
      "Epoch 3, Batch 668, LR 0.000100 Loss 13.983490, Accuracy 34.205%\n",
      "Epoch 3, Batch 669, LR 0.000100 Loss 13.983685, Accuracy 34.202%\n",
      "Epoch 3, Batch 670, LR 0.000100 Loss 13.982592, Accuracy 34.215%\n",
      "Epoch 3, Batch 671, LR 0.000100 Loss 13.982852, Accuracy 34.212%\n",
      "Epoch 3, Batch 672, LR 0.000100 Loss 13.982362, Accuracy 34.220%\n",
      "Epoch 3, Batch 673, LR 0.000100 Loss 13.981956, Accuracy 34.226%\n",
      "Epoch 3, Batch 674, LR 0.000100 Loss 13.981688, Accuracy 34.230%\n",
      "Epoch 3, Batch 675, LR 0.000100 Loss 13.982154, Accuracy 34.227%\n",
      "Epoch 3, Batch 676, LR 0.000100 Loss 13.982008, Accuracy 34.227%\n",
      "Epoch 3, Batch 677, LR 0.000100 Loss 13.980530, Accuracy 34.233%\n",
      "Epoch 3, Batch 678, LR 0.000100 Loss 13.979669, Accuracy 34.246%\n",
      "Epoch 3, Batch 679, LR 0.000100 Loss 13.978886, Accuracy 34.248%\n",
      "Epoch 3, Batch 680, LR 0.000100 Loss 13.977947, Accuracy 34.251%\n",
      "Epoch 3, Batch 681, LR 0.000100 Loss 13.978415, Accuracy 34.244%\n",
      "Epoch 3, Batch 682, LR 0.000100 Loss 13.978240, Accuracy 34.244%\n",
      "Epoch 3, Batch 683, LR 0.000100 Loss 13.977598, Accuracy 34.246%\n",
      "Epoch 3, Batch 684, LR 0.000100 Loss 13.976738, Accuracy 34.252%\n",
      "Epoch 3, Batch 685, LR 0.000100 Loss 13.975296, Accuracy 34.258%\n",
      "Epoch 3, Batch 686, LR 0.000100 Loss 13.974058, Accuracy 34.259%\n",
      "Epoch 3, Batch 687, LR 0.000100 Loss 13.973286, Accuracy 34.257%\n",
      "Epoch 3, Batch 688, LR 0.000100 Loss 13.972162, Accuracy 34.265%\n",
      "Epoch 3, Batch 689, LR 0.000100 Loss 13.971879, Accuracy 34.264%\n",
      "Epoch 3, Batch 690, LR 0.000100 Loss 13.971322, Accuracy 34.271%\n",
      "Epoch 3, Batch 691, LR 0.000100 Loss 13.970743, Accuracy 34.273%\n",
      "Epoch 3, Batch 692, LR 0.000100 Loss 13.970675, Accuracy 34.271%\n",
      "Epoch 3, Batch 693, LR 0.000100 Loss 13.970948, Accuracy 34.260%\n",
      "Epoch 3, Batch 694, LR 0.000100 Loss 13.970865, Accuracy 34.255%\n",
      "Epoch 3, Batch 695, LR 0.000100 Loss 13.970657, Accuracy 34.252%\n",
      "Epoch 3, Batch 696, LR 0.000100 Loss 13.970367, Accuracy 34.258%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 697, LR 0.000100 Loss 13.969006, Accuracy 34.272%\n",
      "Epoch 3, Batch 698, LR 0.000100 Loss 13.967947, Accuracy 34.280%\n",
      "Epoch 3, Batch 699, LR 0.000100 Loss 13.967142, Accuracy 34.283%\n",
      "Epoch 3, Batch 700, LR 0.000100 Loss 13.967012, Accuracy 34.282%\n",
      "Epoch 3, Batch 701, LR 0.000100 Loss 13.966816, Accuracy 34.289%\n",
      "Epoch 3, Batch 702, LR 0.000100 Loss 13.965669, Accuracy 34.298%\n",
      "Epoch 3, Batch 703, LR 0.000100 Loss 13.965688, Accuracy 34.296%\n",
      "Epoch 3, Batch 704, LR 0.000100 Loss 13.965133, Accuracy 34.298%\n",
      "Epoch 3, Batch 705, LR 0.000100 Loss 13.964590, Accuracy 34.301%\n",
      "Epoch 3, Batch 706, LR 0.000100 Loss 13.964188, Accuracy 34.303%\n",
      "Epoch 3, Batch 707, LR 0.000100 Loss 13.964508, Accuracy 34.295%\n",
      "Epoch 3, Batch 708, LR 0.000100 Loss 13.963904, Accuracy 34.304%\n",
      "Epoch 3, Batch 709, LR 0.000100 Loss 13.963277, Accuracy 34.310%\n",
      "Epoch 3, Batch 710, LR 0.000100 Loss 13.963963, Accuracy 34.297%\n",
      "Epoch 3, Batch 711, LR 0.000100 Loss 13.963445, Accuracy 34.302%\n",
      "Epoch 3, Batch 712, LR 0.000100 Loss 13.963167, Accuracy 34.306%\n",
      "Epoch 3, Batch 713, LR 0.000100 Loss 13.962397, Accuracy 34.311%\n",
      "Epoch 3, Batch 714, LR 0.000100 Loss 13.962269, Accuracy 34.309%\n",
      "Epoch 3, Batch 715, LR 0.000100 Loss 13.962095, Accuracy 34.307%\n",
      "Epoch 3, Batch 716, LR 0.000100 Loss 13.961830, Accuracy 34.316%\n",
      "Epoch 3, Batch 717, LR 0.000100 Loss 13.961519, Accuracy 34.326%\n",
      "Epoch 3, Batch 718, LR 0.000100 Loss 13.961118, Accuracy 34.325%\n",
      "Epoch 3, Batch 719, LR 0.000100 Loss 13.960576, Accuracy 34.322%\n",
      "Epoch 3, Batch 720, LR 0.000100 Loss 13.959807, Accuracy 34.324%\n",
      "Epoch 3, Batch 721, LR 0.000100 Loss 13.959272, Accuracy 34.328%\n",
      "Epoch 3, Batch 722, LR 0.000100 Loss 13.958571, Accuracy 34.330%\n",
      "Epoch 3, Batch 723, LR 0.000100 Loss 13.958051, Accuracy 34.338%\n",
      "Epoch 3, Batch 724, LR 0.000100 Loss 13.958144, Accuracy 34.336%\n",
      "Epoch 3, Batch 725, LR 0.000100 Loss 13.957855, Accuracy 34.344%\n",
      "Epoch 3, Batch 726, LR 0.000100 Loss 13.958120, Accuracy 34.338%\n",
      "Epoch 3, Batch 727, LR 0.000100 Loss 13.958341, Accuracy 34.334%\n",
      "Epoch 3, Batch 728, LR 0.000100 Loss 13.957418, Accuracy 34.340%\n",
      "Epoch 3, Batch 729, LR 0.000100 Loss 13.957119, Accuracy 34.341%\n",
      "Epoch 3, Batch 730, LR 0.000100 Loss 13.957105, Accuracy 34.344%\n",
      "Epoch 3, Batch 731, LR 0.000100 Loss 13.957207, Accuracy 34.341%\n",
      "Epoch 3, Batch 732, LR 0.000100 Loss 13.957075, Accuracy 34.341%\n",
      "Epoch 3, Batch 733, LR 0.000100 Loss 13.956617, Accuracy 34.348%\n",
      "Epoch 3, Batch 734, LR 0.000100 Loss 13.956666, Accuracy 34.347%\n",
      "Epoch 3, Batch 735, LR 0.000100 Loss 13.956238, Accuracy 34.354%\n",
      "Epoch 3, Batch 736, LR 0.000100 Loss 13.955516, Accuracy 34.359%\n",
      "Epoch 3, Batch 737, LR 0.000100 Loss 13.955950, Accuracy 34.358%\n",
      "Epoch 3, Batch 738, LR 0.000100 Loss 13.955676, Accuracy 34.364%\n",
      "Epoch 3, Batch 739, LR 0.000100 Loss 13.955500, Accuracy 34.368%\n",
      "Epoch 3, Batch 740, LR 0.000100 Loss 13.954509, Accuracy 34.379%\n",
      "Epoch 3, Batch 741, LR 0.000100 Loss 13.954795, Accuracy 34.376%\n",
      "Epoch 3, Batch 742, LR 0.000100 Loss 13.954001, Accuracy 34.392%\n",
      "Epoch 3, Batch 743, LR 0.000100 Loss 13.953229, Accuracy 34.398%\n",
      "Epoch 3, Batch 744, LR 0.000100 Loss 13.952483, Accuracy 34.402%\n",
      "Epoch 3, Batch 745, LR 0.000100 Loss 13.951922, Accuracy 34.408%\n",
      "Epoch 3, Batch 746, LR 0.000099 Loss 13.951847, Accuracy 34.409%\n",
      "Epoch 3, Batch 747, LR 0.000099 Loss 13.952032, Accuracy 34.410%\n",
      "Epoch 3, Batch 748, LR 0.000099 Loss 13.950959, Accuracy 34.421%\n",
      "Epoch 3, Batch 749, LR 0.000099 Loss 13.950982, Accuracy 34.422%\n",
      "Epoch 3, Batch 750, LR 0.000099 Loss 13.950775, Accuracy 34.421%\n",
      "Epoch 3, Batch 751, LR 0.000099 Loss 13.950080, Accuracy 34.424%\n",
      "Epoch 3, Batch 752, LR 0.000099 Loss 13.950085, Accuracy 34.431%\n",
      "Epoch 3, Batch 753, LR 0.000099 Loss 13.950153, Accuracy 34.433%\n",
      "Epoch 3, Batch 754, LR 0.000099 Loss 13.949375, Accuracy 34.437%\n",
      "Epoch 3, Batch 755, LR 0.000099 Loss 13.948912, Accuracy 34.446%\n",
      "Epoch 3, Batch 756, LR 0.000099 Loss 13.948361, Accuracy 34.447%\n",
      "Epoch 3, Batch 757, LR 0.000099 Loss 13.947922, Accuracy 34.450%\n",
      "Epoch 3, Batch 758, LR 0.000099 Loss 13.947224, Accuracy 34.456%\n",
      "Epoch 3, Batch 759, LR 0.000099 Loss 13.947063, Accuracy 34.457%\n",
      "Epoch 3, Batch 760, LR 0.000099 Loss 13.947309, Accuracy 34.452%\n",
      "Epoch 3, Batch 761, LR 0.000099 Loss 13.946930, Accuracy 34.457%\n",
      "Epoch 3, Batch 762, LR 0.000099 Loss 13.946405, Accuracy 34.463%\n",
      "Epoch 3, Batch 763, LR 0.000099 Loss 13.945267, Accuracy 34.479%\n",
      "Epoch 3, Batch 764, LR 0.000099 Loss 13.944902, Accuracy 34.484%\n",
      "Epoch 3, Batch 765, LR 0.000099 Loss 13.944415, Accuracy 34.491%\n",
      "Epoch 3, Batch 766, LR 0.000099 Loss 13.944641, Accuracy 34.491%\n",
      "Epoch 3, Batch 767, LR 0.000099 Loss 13.944915, Accuracy 34.487%\n",
      "Epoch 3, Batch 768, LR 0.000099 Loss 13.944229, Accuracy 34.498%\n",
      "Epoch 3, Batch 769, LR 0.000099 Loss 13.943449, Accuracy 34.509%\n",
      "Epoch 3, Batch 770, LR 0.000099 Loss 13.942962, Accuracy 34.509%\n",
      "Epoch 3, Batch 771, LR 0.000099 Loss 13.942688, Accuracy 34.512%\n",
      "Epoch 3, Batch 772, LR 0.000099 Loss 13.940978, Accuracy 34.525%\n",
      "Epoch 3, Batch 773, LR 0.000099 Loss 13.940635, Accuracy 34.515%\n",
      "Epoch 3, Batch 774, LR 0.000099 Loss 13.939901, Accuracy 34.517%\n",
      "Epoch 3, Batch 775, LR 0.000099 Loss 13.938977, Accuracy 34.523%\n",
      "Epoch 3, Batch 776, LR 0.000099 Loss 13.938320, Accuracy 34.532%\n",
      "Epoch 3, Batch 777, LR 0.000099 Loss 13.937999, Accuracy 34.533%\n",
      "Epoch 3, Batch 778, LR 0.000099 Loss 13.938296, Accuracy 34.534%\n",
      "Epoch 3, Batch 779, LR 0.000099 Loss 13.938180, Accuracy 34.530%\n",
      "Epoch 3, Batch 780, LR 0.000099 Loss 13.936987, Accuracy 34.537%\n",
      "Epoch 3, Batch 781, LR 0.000099 Loss 13.936110, Accuracy 34.547%\n",
      "Epoch 3, Batch 782, LR 0.000099 Loss 13.935003, Accuracy 34.559%\n",
      "Epoch 3, Batch 783, LR 0.000099 Loss 13.934557, Accuracy 34.559%\n",
      "Epoch 3, Batch 784, LR 0.000099 Loss 13.934481, Accuracy 34.561%\n",
      "Epoch 3, Batch 785, LR 0.000099 Loss 13.934597, Accuracy 34.559%\n",
      "Epoch 3, Batch 786, LR 0.000099 Loss 13.934338, Accuracy 34.563%\n",
      "Epoch 3, Batch 787, LR 0.000099 Loss 13.934128, Accuracy 34.568%\n",
      "Epoch 3, Batch 788, LR 0.000099 Loss 13.933797, Accuracy 34.568%\n",
      "Epoch 3, Batch 789, LR 0.000099 Loss 13.933175, Accuracy 34.571%\n",
      "Epoch 3, Batch 790, LR 0.000099 Loss 13.932183, Accuracy 34.578%\n",
      "Epoch 3, Batch 791, LR 0.000099 Loss 13.932209, Accuracy 34.576%\n",
      "Epoch 3, Batch 792, LR 0.000099 Loss 13.932326, Accuracy 34.575%\n",
      "Epoch 3, Batch 793, LR 0.000099 Loss 13.932670, Accuracy 34.574%\n",
      "Epoch 3, Batch 794, LR 0.000099 Loss 13.931831, Accuracy 34.589%\n",
      "Epoch 3, Batch 795, LR 0.000099 Loss 13.931582, Accuracy 34.587%\n",
      "Epoch 3, Batch 796, LR 0.000099 Loss 13.930483, Accuracy 34.605%\n",
      "Epoch 3, Batch 797, LR 0.000099 Loss 13.930417, Accuracy 34.598%\n",
      "Epoch 3, Batch 798, LR 0.000099 Loss 13.929596, Accuracy 34.605%\n",
      "Epoch 3, Batch 799, LR 0.000099 Loss 13.929155, Accuracy 34.603%\n",
      "Epoch 3, Batch 800, LR 0.000099 Loss 13.928702, Accuracy 34.604%\n",
      "Epoch 3, Batch 801, LR 0.000099 Loss 13.928553, Accuracy 34.608%\n",
      "Epoch 3, Batch 802, LR 0.000099 Loss 13.928580, Accuracy 34.603%\n",
      "Epoch 3, Batch 803, LR 0.000099 Loss 13.928219, Accuracy 34.608%\n",
      "Epoch 3, Batch 804, LR 0.000099 Loss 13.927642, Accuracy 34.610%\n",
      "Epoch 3, Batch 805, LR 0.000099 Loss 13.927913, Accuracy 34.610%\n",
      "Epoch 3, Batch 806, LR 0.000099 Loss 13.927353, Accuracy 34.618%\n",
      "Epoch 3, Batch 807, LR 0.000099 Loss 13.927593, Accuracy 34.617%\n",
      "Epoch 3, Batch 808, LR 0.000099 Loss 13.926030, Accuracy 34.629%\n",
      "Epoch 3, Batch 809, LR 0.000099 Loss 13.925404, Accuracy 34.635%\n",
      "Epoch 3, Batch 810, LR 0.000099 Loss 13.924893, Accuracy 34.640%\n",
      "Epoch 3, Batch 811, LR 0.000099 Loss 13.924527, Accuracy 34.640%\n",
      "Epoch 3, Batch 812, LR 0.000099 Loss 13.923759, Accuracy 34.645%\n",
      "Epoch 3, Batch 813, LR 0.000099 Loss 13.923222, Accuracy 34.650%\n",
      "Epoch 3, Batch 814, LR 0.000099 Loss 13.922519, Accuracy 34.653%\n",
      "Epoch 3, Batch 815, LR 0.000099 Loss 13.921291, Accuracy 34.664%\n",
      "Epoch 3, Batch 816, LR 0.000099 Loss 13.920654, Accuracy 34.672%\n",
      "Epoch 3, Batch 817, LR 0.000099 Loss 13.920242, Accuracy 34.673%\n",
      "Epoch 3, Batch 818, LR 0.000099 Loss 13.919745, Accuracy 34.678%\n",
      "Epoch 3, Batch 819, LR 0.000099 Loss 13.918413, Accuracy 34.690%\n",
      "Epoch 3, Batch 820, LR 0.000099 Loss 13.918085, Accuracy 34.691%\n",
      "Epoch 3, Batch 821, LR 0.000099 Loss 13.917736, Accuracy 34.695%\n",
      "Epoch 3, Batch 822, LR 0.000099 Loss 13.917369, Accuracy 34.700%\n",
      "Epoch 3, Batch 823, LR 0.000099 Loss 13.916700, Accuracy 34.703%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 824, LR 0.000099 Loss 13.915715, Accuracy 34.713%\n",
      "Epoch 3, Batch 825, LR 0.000099 Loss 13.915424, Accuracy 34.720%\n",
      "Epoch 3, Batch 826, LR 0.000099 Loss 13.914963, Accuracy 34.723%\n",
      "Epoch 3, Batch 827, LR 0.000099 Loss 13.914444, Accuracy 34.723%\n",
      "Epoch 3, Batch 828, LR 0.000099 Loss 13.913831, Accuracy 34.729%\n",
      "Epoch 3, Batch 829, LR 0.000099 Loss 13.913124, Accuracy 34.736%\n",
      "Epoch 3, Batch 830, LR 0.000099 Loss 13.912806, Accuracy 34.732%\n",
      "Epoch 3, Batch 831, LR 0.000099 Loss 13.912499, Accuracy 34.731%\n",
      "Epoch 3, Batch 832, LR 0.000099 Loss 13.911892, Accuracy 34.738%\n",
      "Epoch 3, Batch 833, LR 0.000099 Loss 13.911733, Accuracy 34.739%\n",
      "Epoch 3, Batch 834, LR 0.000099 Loss 13.911703, Accuracy 34.742%\n",
      "Epoch 3, Batch 835, LR 0.000099 Loss 13.911243, Accuracy 34.742%\n",
      "Epoch 3, Batch 836, LR 0.000099 Loss 13.910016, Accuracy 34.754%\n",
      "Epoch 3, Batch 837, LR 0.000099 Loss 13.909957, Accuracy 34.756%\n",
      "Epoch 3, Batch 838, LR 0.000099 Loss 13.909929, Accuracy 34.754%\n",
      "Epoch 3, Batch 839, LR 0.000099 Loss 13.909032, Accuracy 34.761%\n",
      "Epoch 3, Batch 840, LR 0.000099 Loss 13.908256, Accuracy 34.767%\n",
      "Epoch 3, Batch 841, LR 0.000099 Loss 13.907398, Accuracy 34.769%\n",
      "Epoch 3, Batch 842, LR 0.000099 Loss 13.906000, Accuracy 34.780%\n",
      "Epoch 3, Batch 843, LR 0.000099 Loss 13.905294, Accuracy 34.788%\n",
      "Epoch 3, Batch 844, LR 0.000099 Loss 13.905080, Accuracy 34.790%\n",
      "Epoch 3, Batch 845, LR 0.000099 Loss 13.904564, Accuracy 34.795%\n",
      "Epoch 3, Batch 846, LR 0.000099 Loss 13.904181, Accuracy 34.798%\n",
      "Epoch 3, Batch 847, LR 0.000099 Loss 13.904462, Accuracy 34.797%\n",
      "Epoch 3, Batch 848, LR 0.000099 Loss 13.903117, Accuracy 34.807%\n",
      "Epoch 3, Batch 849, LR 0.000099 Loss 13.902445, Accuracy 34.815%\n",
      "Epoch 3, Batch 850, LR 0.000099 Loss 13.902158, Accuracy 34.821%\n",
      "Epoch 3, Batch 851, LR 0.000099 Loss 13.901917, Accuracy 34.824%\n",
      "Epoch 3, Batch 852, LR 0.000099 Loss 13.901063, Accuracy 34.824%\n",
      "Epoch 3, Batch 853, LR 0.000099 Loss 13.900499, Accuracy 34.826%\n",
      "Epoch 3, Batch 854, LR 0.000099 Loss 13.900079, Accuracy 34.828%\n",
      "Epoch 3, Batch 855, LR 0.000099 Loss 13.900063, Accuracy 34.829%\n",
      "Epoch 3, Batch 856, LR 0.000099 Loss 13.900218, Accuracy 34.829%\n",
      "Epoch 3, Batch 857, LR 0.000099 Loss 13.899830, Accuracy 34.835%\n",
      "Epoch 3, Batch 858, LR 0.000099 Loss 13.899351, Accuracy 34.843%\n",
      "Epoch 3, Batch 859, LR 0.000099 Loss 13.899292, Accuracy 34.844%\n",
      "Epoch 3, Batch 860, LR 0.000099 Loss 13.898688, Accuracy 34.849%\n",
      "Epoch 3, Batch 861, LR 0.000099 Loss 13.897209, Accuracy 34.871%\n",
      "Epoch 3, Batch 862, LR 0.000099 Loss 13.896900, Accuracy 34.867%\n",
      "Epoch 3, Batch 863, LR 0.000099 Loss 13.896695, Accuracy 34.864%\n",
      "Epoch 3, Batch 864, LR 0.000099 Loss 13.896146, Accuracy 34.874%\n",
      "Epoch 3, Batch 865, LR 0.000099 Loss 13.896171, Accuracy 34.876%\n",
      "Epoch 3, Batch 866, LR 0.000099 Loss 13.896070, Accuracy 34.881%\n",
      "Epoch 3, Batch 867, LR 0.000099 Loss 13.895595, Accuracy 34.882%\n",
      "Epoch 3, Batch 868, LR 0.000099 Loss 13.895406, Accuracy 34.886%\n",
      "Epoch 3, Batch 869, LR 0.000099 Loss 13.894281, Accuracy 34.897%\n",
      "Epoch 3, Batch 870, LR 0.000099 Loss 13.893636, Accuracy 34.900%\n",
      "Epoch 3, Batch 871, LR 0.000099 Loss 13.893339, Accuracy 34.905%\n",
      "Epoch 3, Batch 872, LR 0.000099 Loss 13.892296, Accuracy 34.911%\n",
      "Epoch 3, Batch 873, LR 0.000099 Loss 13.892100, Accuracy 34.914%\n",
      "Epoch 3, Batch 874, LR 0.000099 Loss 13.892338, Accuracy 34.911%\n",
      "Epoch 3, Batch 875, LR 0.000099 Loss 13.892443, Accuracy 34.907%\n",
      "Epoch 3, Batch 876, LR 0.000099 Loss 13.891868, Accuracy 34.912%\n",
      "Epoch 3, Batch 877, LR 0.000099 Loss 13.891021, Accuracy 34.919%\n",
      "Epoch 3, Batch 878, LR 0.000099 Loss 13.890997, Accuracy 34.924%\n",
      "Epoch 3, Batch 879, LR 0.000099 Loss 13.891303, Accuracy 34.922%\n",
      "Epoch 3, Batch 880, LR 0.000099 Loss 13.890743, Accuracy 34.928%\n",
      "Epoch 3, Batch 881, LR 0.000099 Loss 13.889863, Accuracy 34.937%\n",
      "Epoch 3, Batch 882, LR 0.000099 Loss 13.889559, Accuracy 34.945%\n",
      "Epoch 3, Batch 883, LR 0.000099 Loss 13.889455, Accuracy 34.945%\n",
      "Epoch 3, Batch 884, LR 0.000099 Loss 13.889081, Accuracy 34.946%\n",
      "Epoch 3, Batch 885, LR 0.000099 Loss 13.889036, Accuracy 34.944%\n",
      "Epoch 3, Batch 886, LR 0.000099 Loss 13.887743, Accuracy 34.958%\n",
      "Epoch 3, Batch 887, LR 0.000099 Loss 13.886961, Accuracy 34.966%\n",
      "Epoch 3, Batch 888, LR 0.000099 Loss 13.886376, Accuracy 34.972%\n",
      "Epoch 3, Batch 889, LR 0.000099 Loss 13.885946, Accuracy 34.980%\n",
      "Epoch 3, Batch 890, LR 0.000099 Loss 13.885023, Accuracy 34.989%\n",
      "Epoch 3, Batch 891, LR 0.000099 Loss 13.884419, Accuracy 34.992%\n",
      "Epoch 3, Batch 892, LR 0.000099 Loss 13.884054, Accuracy 34.999%\n",
      "Epoch 3, Batch 893, LR 0.000099 Loss 13.883488, Accuracy 35.005%\n",
      "Epoch 3, Batch 894, LR 0.000099 Loss 13.882972, Accuracy 35.011%\n",
      "Epoch 3, Batch 895, LR 0.000099 Loss 13.882223, Accuracy 35.014%\n",
      "Epoch 3, Batch 896, LR 0.000099 Loss 13.881828, Accuracy 35.017%\n",
      "Epoch 3, Batch 897, LR 0.000099 Loss 13.881495, Accuracy 35.019%\n",
      "Epoch 3, Batch 898, LR 0.000099 Loss 13.881216, Accuracy 35.021%\n",
      "Epoch 3, Batch 899, LR 0.000099 Loss 13.881128, Accuracy 35.021%\n",
      "Epoch 3, Batch 900, LR 0.000099 Loss 13.881207, Accuracy 35.017%\n",
      "Epoch 3, Batch 901, LR 0.000099 Loss 13.880623, Accuracy 35.026%\n",
      "Epoch 3, Batch 902, LR 0.000099 Loss 13.880375, Accuracy 35.025%\n",
      "Epoch 3, Batch 903, LR 0.000099 Loss 13.879168, Accuracy 35.033%\n",
      "Epoch 3, Batch 904, LR 0.000099 Loss 13.878634, Accuracy 35.038%\n",
      "Epoch 3, Batch 905, LR 0.000099 Loss 13.877999, Accuracy 35.044%\n",
      "Epoch 3, Batch 906, LR 0.000099 Loss 13.877941, Accuracy 35.046%\n",
      "Epoch 3, Batch 907, LR 0.000099 Loss 13.877315, Accuracy 35.048%\n",
      "Epoch 3, Batch 908, LR 0.000099 Loss 13.877062, Accuracy 35.049%\n",
      "Epoch 3, Batch 909, LR 0.000099 Loss 13.877395, Accuracy 35.048%\n",
      "Epoch 3, Batch 910, LR 0.000099 Loss 13.876849, Accuracy 35.050%\n",
      "Epoch 3, Batch 911, LR 0.000099 Loss 13.875656, Accuracy 35.060%\n",
      "Epoch 3, Batch 912, LR 0.000099 Loss 13.874075, Accuracy 35.075%\n",
      "Epoch 3, Batch 913, LR 0.000099 Loss 13.873411, Accuracy 35.074%\n",
      "Epoch 3, Batch 914, LR 0.000099 Loss 13.873302, Accuracy 35.079%\n",
      "Epoch 3, Batch 915, LR 0.000099 Loss 13.872368, Accuracy 35.091%\n",
      "Epoch 3, Batch 916, LR 0.000099 Loss 13.871496, Accuracy 35.097%\n",
      "Epoch 3, Batch 917, LR 0.000099 Loss 13.871540, Accuracy 35.095%\n",
      "Epoch 3, Batch 918, LR 0.000099 Loss 13.870882, Accuracy 35.098%\n",
      "Epoch 3, Batch 919, LR 0.000099 Loss 13.870015, Accuracy 35.098%\n",
      "Epoch 3, Batch 920, LR 0.000099 Loss 13.869643, Accuracy 35.098%\n",
      "Epoch 3, Batch 921, LR 0.000099 Loss 13.868970, Accuracy 35.106%\n",
      "Epoch 3, Batch 922, LR 0.000099 Loss 13.868285, Accuracy 35.114%\n",
      "Epoch 3, Batch 923, LR 0.000099 Loss 13.866760, Accuracy 35.122%\n",
      "Epoch 3, Batch 924, LR 0.000099 Loss 13.866297, Accuracy 35.126%\n",
      "Epoch 3, Batch 925, LR 0.000099 Loss 13.866180, Accuracy 35.121%\n",
      "Epoch 3, Batch 926, LR 0.000099 Loss 13.865489, Accuracy 35.130%\n",
      "Epoch 3, Batch 927, LR 0.000099 Loss 13.864949, Accuracy 35.139%\n",
      "Epoch 3, Batch 928, LR 0.000099 Loss 13.864304, Accuracy 35.150%\n",
      "Epoch 3, Batch 929, LR 0.000099 Loss 13.864513, Accuracy 35.152%\n",
      "Epoch 3, Batch 930, LR 0.000099 Loss 13.863741, Accuracy 35.164%\n",
      "Epoch 3, Batch 931, LR 0.000099 Loss 13.862949, Accuracy 35.171%\n",
      "Epoch 3, Batch 932, LR 0.000099 Loss 13.862338, Accuracy 35.182%\n",
      "Epoch 3, Batch 933, LR 0.000099 Loss 13.862496, Accuracy 35.180%\n",
      "Epoch 3, Batch 934, LR 0.000099 Loss 13.862020, Accuracy 35.184%\n",
      "Epoch 3, Batch 935, LR 0.000099 Loss 13.861323, Accuracy 35.186%\n",
      "Epoch 3, Batch 936, LR 0.000099 Loss 13.860798, Accuracy 35.191%\n",
      "Epoch 3, Batch 937, LR 0.000099 Loss 13.860082, Accuracy 35.203%\n",
      "Epoch 3, Batch 938, LR 0.000099 Loss 13.859622, Accuracy 35.210%\n",
      "Epoch 3, Batch 939, LR 0.000099 Loss 13.858868, Accuracy 35.214%\n",
      "Epoch 3, Batch 940, LR 0.000099 Loss 13.859229, Accuracy 35.209%\n",
      "Epoch 3, Batch 941, LR 0.000099 Loss 13.859525, Accuracy 35.204%\n",
      "Epoch 3, Batch 942, LR 0.000099 Loss 13.859818, Accuracy 35.200%\n",
      "Epoch 3, Batch 943, LR 0.000099 Loss 13.859217, Accuracy 35.203%\n",
      "Epoch 3, Batch 944, LR 0.000099 Loss 13.858836, Accuracy 35.208%\n",
      "Epoch 3, Batch 945, LR 0.000099 Loss 13.858607, Accuracy 35.213%\n",
      "Epoch 3, Batch 946, LR 0.000099 Loss 13.857847, Accuracy 35.222%\n",
      "Epoch 3, Batch 947, LR 0.000099 Loss 13.856756, Accuracy 35.232%\n",
      "Epoch 3, Batch 948, LR 0.000099 Loss 13.855683, Accuracy 35.240%\n",
      "Epoch 3, Batch 949, LR 0.000099 Loss 13.855517, Accuracy 35.244%\n",
      "Epoch 3, Batch 950, LR 0.000099 Loss 13.855207, Accuracy 35.245%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Batch 951, LR 0.000099 Loss 13.855200, Accuracy 35.243%\n",
      "Epoch 3, Batch 952, LR 0.000099 Loss 13.854188, Accuracy 35.251%\n",
      "Epoch 3, Batch 953, LR 0.000099 Loss 13.854527, Accuracy 35.247%\n",
      "Epoch 3, Batch 954, LR 0.000099 Loss 13.854200, Accuracy 35.250%\n",
      "Epoch 3, Batch 955, LR 0.000099 Loss 13.853852, Accuracy 35.255%\n",
      "Epoch 3, Batch 956, LR 0.000099 Loss 13.853503, Accuracy 35.256%\n",
      "Epoch 3, Batch 957, LR 0.000099 Loss 13.853398, Accuracy 35.259%\n",
      "Epoch 3, Batch 958, LR 0.000099 Loss 13.853645, Accuracy 35.253%\n",
      "Epoch 3, Batch 959, LR 0.000099 Loss 13.853123, Accuracy 35.259%\n",
      "Epoch 3, Batch 960, LR 0.000099 Loss 13.852158, Accuracy 35.269%\n",
      "Epoch 3, Batch 961, LR 0.000099 Loss 13.851391, Accuracy 35.279%\n",
      "Epoch 3, Batch 962, LR 0.000099 Loss 13.850796, Accuracy 35.289%\n",
      "Epoch 3, Batch 963, LR 0.000099 Loss 13.850442, Accuracy 35.292%\n",
      "Epoch 3, Batch 964, LR 0.000099 Loss 13.850039, Accuracy 35.295%\n",
      "Epoch 3, Batch 965, LR 0.000099 Loss 13.849785, Accuracy 35.298%\n",
      "Epoch 3, Batch 966, LR 0.000099 Loss 13.848541, Accuracy 35.313%\n",
      "Epoch 3, Batch 967, LR 0.000099 Loss 13.847827, Accuracy 35.320%\n",
      "Epoch 3, Batch 968, LR 0.000099 Loss 13.847258, Accuracy 35.331%\n",
      "Epoch 3, Batch 969, LR 0.000099 Loss 13.846490, Accuracy 35.340%\n",
      "Epoch 3, Batch 970, LR 0.000099 Loss 13.846234, Accuracy 35.344%\n",
      "Epoch 3, Batch 971, LR 0.000099 Loss 13.846202, Accuracy 35.345%\n",
      "Epoch 3, Batch 972, LR 0.000099 Loss 13.845753, Accuracy 35.352%\n",
      "Epoch 3, Batch 973, LR 0.000099 Loss 13.845250, Accuracy 35.359%\n",
      "Epoch 3, Batch 974, LR 0.000099 Loss 13.844589, Accuracy 35.362%\n",
      "Epoch 3, Batch 975, LR 0.000099 Loss 13.844263, Accuracy 35.361%\n",
      "Epoch 3, Batch 976, LR 0.000099 Loss 13.843653, Accuracy 35.365%\n",
      "Epoch 3, Batch 977, LR 0.000099 Loss 13.843461, Accuracy 35.372%\n",
      "Epoch 3, Batch 978, LR 0.000099 Loss 13.842967, Accuracy 35.379%\n",
      "Epoch 3, Batch 979, LR 0.000099 Loss 13.842529, Accuracy 35.385%\n",
      "Epoch 3, Batch 980, LR 0.000099 Loss 13.841553, Accuracy 35.394%\n",
      "Epoch 3, Batch 981, LR 0.000099 Loss 13.840960, Accuracy 35.395%\n",
      "Epoch 3, Batch 982, LR 0.000099 Loss 13.840594, Accuracy 35.398%\n",
      "Epoch 3, Batch 983, LR 0.000099 Loss 13.839954, Accuracy 35.407%\n",
      "Epoch 3, Batch 984, LR 0.000099 Loss 13.839361, Accuracy 35.409%\n",
      "Epoch 3, Batch 985, LR 0.000099 Loss 13.838568, Accuracy 35.413%\n",
      "Epoch 3, Batch 986, LR 0.000099 Loss 13.837608, Accuracy 35.426%\n",
      "Epoch 3, Batch 987, LR 0.000099 Loss 13.836848, Accuracy 35.429%\n",
      "Epoch 3, Batch 988, LR 0.000099 Loss 13.836162, Accuracy 35.436%\n",
      "Epoch 3, Batch 989, LR 0.000099 Loss 13.835882, Accuracy 35.438%\n",
      "Epoch 3, Batch 990, LR 0.000099 Loss 13.835516, Accuracy 35.451%\n",
      "Epoch 3, Batch 991, LR 0.000099 Loss 13.834746, Accuracy 35.461%\n",
      "Epoch 3, Batch 992, LR 0.000099 Loss 13.833867, Accuracy 35.468%\n",
      "Epoch 3, Batch 993, LR 0.000099 Loss 13.833522, Accuracy 35.472%\n",
      "Epoch 3, Batch 994, LR 0.000099 Loss 13.833219, Accuracy 35.475%\n",
      "Epoch 3, Batch 995, LR 0.000099 Loss 13.832816, Accuracy 35.480%\n",
      "Epoch 3, Batch 996, LR 0.000099 Loss 13.832709, Accuracy 35.481%\n",
      "Epoch 3, Batch 997, LR 0.000099 Loss 13.832457, Accuracy 35.481%\n",
      "Epoch 3, Batch 998, LR 0.000099 Loss 13.831778, Accuracy 35.487%\n",
      "Epoch 3, Batch 999, LR 0.000099 Loss 13.831628, Accuracy 35.486%\n",
      "Epoch 3, Batch 1000, LR 0.000099 Loss 13.831202, Accuracy 35.487%\n",
      "Epoch 3, Batch 1001, LR 0.000099 Loss 13.830184, Accuracy 35.500%\n",
      "Epoch 3, Batch 1002, LR 0.000099 Loss 13.829386, Accuracy 35.506%\n",
      "Epoch 3, Batch 1003, LR 0.000099 Loss 13.828447, Accuracy 35.509%\n",
      "Epoch 3, Batch 1004, LR 0.000099 Loss 13.827624, Accuracy 35.520%\n",
      "Epoch 3, Batch 1005, LR 0.000099 Loss 13.827378, Accuracy 35.519%\n",
      "Epoch 3, Batch 1006, LR 0.000099 Loss 13.826496, Accuracy 35.526%\n",
      "Epoch 3, Batch 1007, LR 0.000099 Loss 13.825645, Accuracy 35.534%\n",
      "Epoch 3, Batch 1008, LR 0.000099 Loss 13.824731, Accuracy 35.546%\n",
      "Epoch 3, Batch 1009, LR 0.000099 Loss 13.824747, Accuracy 35.547%\n",
      "Epoch 3, Batch 1010, LR 0.000099 Loss 13.824261, Accuracy 35.551%\n",
      "Epoch 3, Batch 1011, LR 0.000099 Loss 13.824143, Accuracy 35.555%\n",
      "Epoch 3, Batch 1012, LR 0.000099 Loss 13.823325, Accuracy 35.559%\n",
      "Epoch 3, Batch 1013, LR 0.000099 Loss 13.823138, Accuracy 35.563%\n",
      "Epoch 3, Batch 1014, LR 0.000099 Loss 13.823189, Accuracy 35.559%\n",
      "Epoch 3, Batch 1015, LR 0.000099 Loss 13.823407, Accuracy 35.559%\n",
      "Epoch 3, Batch 1016, LR 0.000099 Loss 13.823581, Accuracy 35.559%\n",
      "Epoch 3, Batch 1017, LR 0.000099 Loss 13.823994, Accuracy 35.555%\n",
      "Epoch 3, Batch 1018, LR 0.000099 Loss 13.823026, Accuracy 35.563%\n",
      "Epoch 3, Batch 1019, LR 0.000099 Loss 13.822083, Accuracy 35.576%\n",
      "Epoch 3, Batch 1020, LR 0.000099 Loss 13.821375, Accuracy 35.590%\n",
      "Epoch 3, Batch 1021, LR 0.000099 Loss 13.820855, Accuracy 35.599%\n",
      "Epoch 3, Batch 1022, LR 0.000099 Loss 13.821022, Accuracy 35.599%\n",
      "Epoch 3, Batch 1023, LR 0.000099 Loss 13.820171, Accuracy 35.605%\n",
      "Epoch 3, Batch 1024, LR 0.000099 Loss 13.819988, Accuracy 35.606%\n",
      "Epoch 3, Batch 1025, LR 0.000099 Loss 13.819252, Accuracy 35.611%\n",
      "Epoch 3, Batch 1026, LR 0.000099 Loss 13.818771, Accuracy 35.612%\n",
      "Epoch 3, Batch 1027, LR 0.000099 Loss 13.817833, Accuracy 35.618%\n",
      "Epoch 3, Batch 1028, LR 0.000099 Loss 13.816997, Accuracy 35.625%\n",
      "Epoch 3, Batch 1029, LR 0.000099 Loss 13.816719, Accuracy 35.622%\n",
      "Epoch 3, Batch 1030, LR 0.000099 Loss 13.816414, Accuracy 35.624%\n",
      "Epoch 3, Batch 1031, LR 0.000099 Loss 13.816610, Accuracy 35.622%\n",
      "Epoch 3, Batch 1032, LR 0.000099 Loss 13.815970, Accuracy 35.630%\n",
      "Epoch 3, Batch 1033, LR 0.000099 Loss 13.815283, Accuracy 35.640%\n",
      "Epoch 3, Batch 1034, LR 0.000099 Loss 13.814573, Accuracy 35.645%\n",
      "Epoch 3, Batch 1035, LR 0.000099 Loss 13.813742, Accuracy 35.653%\n",
      "Epoch 3, Batch 1036, LR 0.000099 Loss 13.812797, Accuracy 35.660%\n",
      "Epoch 3, Batch 1037, LR 0.000099 Loss 13.812005, Accuracy 35.663%\n",
      "Epoch 3, Batch 1038, LR 0.000099 Loss 13.811495, Accuracy 35.670%\n",
      "Epoch 3, Batch 1039, LR 0.000099 Loss 13.811162, Accuracy 35.674%\n",
      "Epoch 3, Batch 1040, LR 0.000099 Loss 13.810791, Accuracy 35.675%\n",
      "Epoch 3, Batch 1041, LR 0.000099 Loss 13.810736, Accuracy 35.674%\n",
      "Epoch 3, Batch 1042, LR 0.000099 Loss 13.810181, Accuracy 35.677%\n",
      "Epoch 3, Batch 1043, LR 0.000099 Loss 13.810096, Accuracy 35.679%\n",
      "Epoch 3, Batch 1044, LR 0.000099 Loss 13.809847, Accuracy 35.686%\n",
      "Epoch 3, Batch 1045, LR 0.000099 Loss 13.809832, Accuracy 35.684%\n",
      "Epoch 3, Batch 1046, LR 0.000099 Loss 13.809613, Accuracy 35.686%\n",
      "Epoch 3, Batch 1047, LR 0.000099 Loss 13.809447, Accuracy 35.690%\n",
      "Epoch 3, Loss (train set) 13.809447, Accuracy (train set) 35.690%\n",
      "Epoch 4, Batch 1, LR 0.000099 Loss 13.498815, Accuracy 43.750%\n",
      "Epoch 4, Batch 2, LR 0.000099 Loss 13.208840, Accuracy 40.625%\n",
      "Epoch 4, Batch 3, LR 0.000099 Loss 13.140096, Accuracy 40.104%\n",
      "Epoch 4, Batch 4, LR 0.000099 Loss 13.097070, Accuracy 40.625%\n",
      "Epoch 4, Batch 5, LR 0.000099 Loss 13.229754, Accuracy 40.156%\n",
      "Epoch 4, Batch 6, LR 0.000099 Loss 13.208660, Accuracy 40.104%\n",
      "Epoch 4, Batch 7, LR 0.000099 Loss 13.303047, Accuracy 39.286%\n",
      "Epoch 4, Batch 8, LR 0.000099 Loss 13.296329, Accuracy 39.062%\n",
      "Epoch 4, Batch 9, LR 0.000099 Loss 13.298191, Accuracy 39.062%\n",
      "Epoch 4, Batch 10, LR 0.000099 Loss 13.293039, Accuracy 39.453%\n",
      "Epoch 4, Batch 11, LR 0.000099 Loss 13.287693, Accuracy 39.773%\n",
      "Epoch 4, Batch 12, LR 0.000099 Loss 13.310709, Accuracy 39.714%\n",
      "Epoch 4, Batch 13, LR 0.000099 Loss 13.324501, Accuracy 39.483%\n",
      "Epoch 4, Batch 14, LR 0.000099 Loss 13.317603, Accuracy 39.509%\n",
      "Epoch 4, Batch 15, LR 0.000099 Loss 13.289758, Accuracy 39.792%\n",
      "Epoch 4, Batch 16, LR 0.000099 Loss 13.276438, Accuracy 39.990%\n",
      "Epoch 4, Batch 17, LR 0.000099 Loss 13.268041, Accuracy 40.303%\n",
      "Epoch 4, Batch 18, LR 0.000099 Loss 13.246626, Accuracy 40.278%\n",
      "Epoch 4, Batch 19, LR 0.000099 Loss 13.223527, Accuracy 40.625%\n",
      "Epoch 4, Batch 20, LR 0.000099 Loss 13.217532, Accuracy 40.859%\n",
      "Epoch 4, Batch 21, LR 0.000099 Loss 13.185509, Accuracy 41.183%\n",
      "Epoch 4, Batch 22, LR 0.000099 Loss 13.174415, Accuracy 41.371%\n",
      "Epoch 4, Batch 23, LR 0.000099 Loss 13.175279, Accuracy 41.202%\n",
      "Epoch 4, Batch 24, LR 0.000099 Loss 13.176924, Accuracy 40.918%\n",
      "Epoch 4, Batch 25, LR 0.000099 Loss 13.182662, Accuracy 40.938%\n",
      "Epoch 4, Batch 26, LR 0.000099 Loss 13.181254, Accuracy 40.925%\n",
      "Epoch 4, Batch 27, LR 0.000099 Loss 13.178547, Accuracy 40.799%\n",
      "Epoch 4, Batch 28, LR 0.000099 Loss 13.172630, Accuracy 40.876%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 29, LR 0.000099 Loss 13.181527, Accuracy 40.814%\n",
      "Epoch 4, Batch 30, LR 0.000099 Loss 13.198867, Accuracy 40.625%\n",
      "Epoch 4, Batch 31, LR 0.000099 Loss 13.199248, Accuracy 40.751%\n",
      "Epoch 4, Batch 32, LR 0.000099 Loss 13.211024, Accuracy 40.747%\n",
      "Epoch 4, Batch 33, LR 0.000099 Loss 13.200691, Accuracy 40.838%\n",
      "Epoch 4, Batch 34, LR 0.000099 Loss 13.216364, Accuracy 40.786%\n",
      "Epoch 4, Batch 35, LR 0.000099 Loss 13.218141, Accuracy 40.871%\n",
      "Epoch 4, Batch 36, LR 0.000099 Loss 13.218144, Accuracy 40.842%\n",
      "Epoch 4, Batch 37, LR 0.000099 Loss 13.230979, Accuracy 40.752%\n",
      "Epoch 4, Batch 38, LR 0.000099 Loss 13.227042, Accuracy 40.769%\n",
      "Epoch 4, Batch 39, LR 0.000099 Loss 13.235859, Accuracy 40.705%\n",
      "Epoch 4, Batch 40, LR 0.000099 Loss 13.235382, Accuracy 40.547%\n",
      "Epoch 4, Batch 41, LR 0.000099 Loss 13.230026, Accuracy 40.587%\n",
      "Epoch 4, Batch 42, LR 0.000099 Loss 13.235716, Accuracy 40.569%\n",
      "Epoch 4, Batch 43, LR 0.000099 Loss 13.234661, Accuracy 40.643%\n",
      "Epoch 4, Batch 44, LR 0.000099 Loss 13.216647, Accuracy 40.749%\n",
      "Epoch 4, Batch 45, LR 0.000099 Loss 13.217310, Accuracy 40.833%\n",
      "Epoch 4, Batch 46, LR 0.000099 Loss 13.204962, Accuracy 40.948%\n",
      "Epoch 4, Batch 47, LR 0.000099 Loss 13.211349, Accuracy 40.891%\n",
      "Epoch 4, Batch 48, LR 0.000099 Loss 13.199404, Accuracy 41.097%\n",
      "Epoch 4, Batch 49, LR 0.000099 Loss 13.200040, Accuracy 40.944%\n",
      "Epoch 4, Batch 50, LR 0.000099 Loss 13.202685, Accuracy 40.938%\n",
      "Epoch 4, Batch 51, LR 0.000099 Loss 13.202955, Accuracy 40.916%\n",
      "Epoch 4, Batch 52, LR 0.000099 Loss 13.205214, Accuracy 40.910%\n",
      "Epoch 4, Batch 53, LR 0.000099 Loss 13.213012, Accuracy 40.861%\n",
      "Epoch 4, Batch 54, LR 0.000099 Loss 13.209957, Accuracy 40.856%\n",
      "Epoch 4, Batch 55, LR 0.000099 Loss 13.206526, Accuracy 40.923%\n",
      "Epoch 4, Batch 56, LR 0.000099 Loss 13.204829, Accuracy 40.960%\n",
      "Epoch 4, Batch 57, LR 0.000099 Loss 13.199943, Accuracy 40.968%\n",
      "Epoch 4, Batch 58, LR 0.000099 Loss 13.204284, Accuracy 40.921%\n",
      "Epoch 4, Batch 59, LR 0.000099 Loss 13.199179, Accuracy 40.930%\n",
      "Epoch 4, Batch 60, LR 0.000099 Loss 13.205354, Accuracy 40.872%\n",
      "Epoch 4, Batch 61, LR 0.000099 Loss 13.205442, Accuracy 40.868%\n",
      "Epoch 4, Batch 62, LR 0.000099 Loss 13.202905, Accuracy 40.864%\n",
      "Epoch 4, Batch 63, LR 0.000099 Loss 13.196498, Accuracy 40.923%\n",
      "Epoch 4, Batch 64, LR 0.000099 Loss 13.188670, Accuracy 40.967%\n",
      "Epoch 4, Batch 65, LR 0.000099 Loss 13.191505, Accuracy 40.962%\n",
      "Epoch 4, Batch 66, LR 0.000099 Loss 13.177656, Accuracy 41.087%\n",
      "Epoch 4, Batch 67, LR 0.000099 Loss 13.169819, Accuracy 41.173%\n",
      "Epoch 4, Batch 68, LR 0.000099 Loss 13.167160, Accuracy 41.199%\n",
      "Epoch 4, Batch 69, LR 0.000099 Loss 13.158087, Accuracy 41.304%\n",
      "Epoch 4, Batch 70, LR 0.000099 Loss 13.157831, Accuracy 41.317%\n",
      "Epoch 4, Batch 71, LR 0.000099 Loss 13.160736, Accuracy 41.285%\n",
      "Epoch 4, Batch 72, LR 0.000099 Loss 13.158394, Accuracy 41.276%\n",
      "Epoch 4, Batch 73, LR 0.000099 Loss 13.155729, Accuracy 41.256%\n",
      "Epoch 4, Batch 74, LR 0.000099 Loss 13.162661, Accuracy 41.216%\n",
      "Epoch 4, Batch 75, LR 0.000099 Loss 13.169717, Accuracy 41.135%\n",
      "Epoch 4, Batch 76, LR 0.000099 Loss 13.169438, Accuracy 41.026%\n",
      "Epoch 4, Batch 77, LR 0.000099 Loss 13.168086, Accuracy 41.051%\n",
      "Epoch 4, Batch 78, LR 0.000099 Loss 13.170706, Accuracy 40.986%\n",
      "Epoch 4, Batch 79, LR 0.000099 Loss 13.174506, Accuracy 40.902%\n",
      "Epoch 4, Batch 80, LR 0.000099 Loss 13.170534, Accuracy 40.957%\n",
      "Epoch 4, Batch 81, LR 0.000099 Loss 13.166746, Accuracy 40.953%\n",
      "Epoch 4, Batch 82, LR 0.000099 Loss 13.161661, Accuracy 41.082%\n",
      "Epoch 4, Batch 83, LR 0.000099 Loss 13.162099, Accuracy 41.077%\n",
      "Epoch 4, Batch 84, LR 0.000099 Loss 13.162043, Accuracy 41.118%\n",
      "Epoch 4, Batch 85, LR 0.000099 Loss 13.160945, Accuracy 41.149%\n",
      "Epoch 4, Batch 86, LR 0.000099 Loss 13.156533, Accuracy 41.215%\n",
      "Epoch 4, Batch 87, LR 0.000099 Loss 13.157609, Accuracy 41.173%\n",
      "Epoch 4, Batch 88, LR 0.000099 Loss 13.163168, Accuracy 41.202%\n",
      "Epoch 4, Batch 89, LR 0.000099 Loss 13.162221, Accuracy 41.196%\n",
      "Epoch 4, Batch 90, LR 0.000099 Loss 13.166660, Accuracy 41.128%\n",
      "Epoch 4, Batch 91, LR 0.000099 Loss 13.161825, Accuracy 41.097%\n",
      "Epoch 4, Batch 92, LR 0.000099 Loss 13.153428, Accuracy 41.109%\n",
      "Epoch 4, Batch 93, LR 0.000099 Loss 13.157277, Accuracy 41.045%\n",
      "Epoch 4, Batch 94, LR 0.000099 Loss 13.153426, Accuracy 41.082%\n",
      "Epoch 4, Batch 95, LR 0.000099 Loss 13.139618, Accuracy 41.176%\n",
      "Epoch 4, Batch 96, LR 0.000099 Loss 13.147323, Accuracy 41.105%\n",
      "Epoch 4, Batch 97, LR 0.000099 Loss 13.146443, Accuracy 41.092%\n",
      "Epoch 4, Batch 98, LR 0.000099 Loss 13.142110, Accuracy 41.111%\n",
      "Epoch 4, Batch 99, LR 0.000099 Loss 13.141247, Accuracy 41.106%\n",
      "Epoch 4, Batch 100, LR 0.000099 Loss 13.144785, Accuracy 41.055%\n",
      "Epoch 4, Batch 101, LR 0.000099 Loss 13.149479, Accuracy 41.035%\n",
      "Epoch 4, Batch 102, LR 0.000099 Loss 13.148104, Accuracy 41.046%\n",
      "Epoch 4, Batch 103, LR 0.000099 Loss 13.142087, Accuracy 41.050%\n",
      "Epoch 4, Batch 104, LR 0.000099 Loss 13.142535, Accuracy 41.076%\n",
      "Epoch 4, Batch 105, LR 0.000099 Loss 13.138634, Accuracy 41.101%\n",
      "Epoch 4, Batch 106, LR 0.000099 Loss 13.135924, Accuracy 41.119%\n",
      "Epoch 4, Batch 107, LR 0.000099 Loss 13.127770, Accuracy 41.195%\n",
      "Epoch 4, Batch 108, LR 0.000099 Loss 13.135400, Accuracy 41.146%\n",
      "Epoch 4, Batch 109, LR 0.000099 Loss 13.137350, Accuracy 41.177%\n",
      "Epoch 4, Batch 110, LR 0.000099 Loss 13.141755, Accuracy 41.143%\n",
      "Epoch 4, Batch 111, LR 0.000099 Loss 13.139988, Accuracy 41.139%\n",
      "Epoch 4, Batch 112, LR 0.000099 Loss 13.136256, Accuracy 41.155%\n",
      "Epoch 4, Batch 113, LR 0.000099 Loss 13.137722, Accuracy 41.150%\n",
      "Epoch 4, Batch 114, LR 0.000099 Loss 13.137131, Accuracy 41.173%\n",
      "Epoch 4, Batch 115, LR 0.000099 Loss 13.140172, Accuracy 41.135%\n",
      "Epoch 4, Batch 116, LR 0.000099 Loss 13.141823, Accuracy 41.117%\n",
      "Epoch 4, Batch 117, LR 0.000099 Loss 13.142724, Accuracy 41.119%\n",
      "Epoch 4, Batch 118, LR 0.000099 Loss 13.137810, Accuracy 41.135%\n",
      "Epoch 4, Batch 119, LR 0.000099 Loss 13.139469, Accuracy 41.144%\n",
      "Epoch 4, Batch 120, LR 0.000099 Loss 13.141799, Accuracy 41.165%\n",
      "Epoch 4, Batch 121, LR 0.000099 Loss 13.140368, Accuracy 41.219%\n",
      "Epoch 4, Batch 122, LR 0.000099 Loss 13.144406, Accuracy 41.124%\n",
      "Epoch 4, Batch 123, LR 0.000099 Loss 13.146833, Accuracy 41.044%\n",
      "Epoch 4, Batch 124, LR 0.000099 Loss 13.144384, Accuracy 41.091%\n",
      "Epoch 4, Batch 125, LR 0.000099 Loss 13.145751, Accuracy 41.050%\n",
      "Epoch 4, Batch 126, LR 0.000099 Loss 13.145074, Accuracy 41.022%\n",
      "Epoch 4, Batch 127, LR 0.000099 Loss 13.145900, Accuracy 41.019%\n",
      "Epoch 4, Batch 128, LR 0.000099 Loss 13.145356, Accuracy 41.058%\n",
      "Epoch 4, Batch 129, LR 0.000099 Loss 13.154165, Accuracy 40.994%\n",
      "Epoch 4, Batch 130, LR 0.000099 Loss 13.150706, Accuracy 41.046%\n",
      "Epoch 4, Batch 131, LR 0.000099 Loss 13.146415, Accuracy 41.096%\n",
      "Epoch 4, Batch 132, LR 0.000099 Loss 13.144447, Accuracy 41.063%\n",
      "Epoch 4, Batch 133, LR 0.000099 Loss 13.142787, Accuracy 41.066%\n",
      "Epoch 4, Batch 134, LR 0.000099 Loss 13.147678, Accuracy 41.016%\n",
      "Epoch 4, Batch 135, LR 0.000099 Loss 13.147882, Accuracy 41.019%\n",
      "Epoch 4, Batch 136, LR 0.000099 Loss 13.151311, Accuracy 40.970%\n",
      "Epoch 4, Batch 137, LR 0.000099 Loss 13.155153, Accuracy 40.922%\n",
      "Epoch 4, Batch 138, LR 0.000099 Loss 13.152822, Accuracy 40.948%\n",
      "Epoch 4, Batch 139, LR 0.000099 Loss 13.150810, Accuracy 41.007%\n",
      "Epoch 4, Batch 140, LR 0.000099 Loss 13.152148, Accuracy 41.027%\n",
      "Epoch 4, Batch 141, LR 0.000099 Loss 13.150782, Accuracy 41.074%\n",
      "Epoch 4, Batch 142, LR 0.000099 Loss 13.151060, Accuracy 41.071%\n",
      "Epoch 4, Batch 143, LR 0.000099 Loss 13.150386, Accuracy 41.122%\n",
      "Epoch 4, Batch 144, LR 0.000099 Loss 13.151330, Accuracy 41.102%\n",
      "Epoch 4, Batch 145, LR 0.000099 Loss 13.153406, Accuracy 41.056%\n",
      "Epoch 4, Batch 146, LR 0.000099 Loss 13.151474, Accuracy 41.101%\n",
      "Epoch 4, Batch 147, LR 0.000099 Loss 13.148866, Accuracy 41.119%\n",
      "Epoch 4, Batch 148, LR 0.000099 Loss 13.145164, Accuracy 41.163%\n",
      "Epoch 4, Batch 149, LR 0.000099 Loss 13.143144, Accuracy 41.202%\n",
      "Epoch 4, Batch 150, LR 0.000099 Loss 13.142466, Accuracy 41.188%\n",
      "Epoch 4, Batch 151, LR 0.000099 Loss 13.141625, Accuracy 41.173%\n",
      "Epoch 4, Batch 152, LR 0.000099 Loss 13.137216, Accuracy 41.211%\n",
      "Epoch 4, Batch 153, LR 0.000099 Loss 13.136980, Accuracy 41.217%\n",
      "Epoch 4, Batch 154, LR 0.000099 Loss 13.135613, Accuracy 41.229%\n",
      "Epoch 4, Batch 155, LR 0.000099 Loss 13.135190, Accuracy 41.265%\n",
      "Epoch 4, Batch 156, LR 0.000099 Loss 13.139359, Accuracy 41.246%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 157, LR 0.000099 Loss 13.139494, Accuracy 41.237%\n",
      "Epoch 4, Batch 158, LR 0.000099 Loss 13.133654, Accuracy 41.288%\n",
      "Epoch 4, Batch 159, LR 0.000099 Loss 13.131533, Accuracy 41.283%\n",
      "Epoch 4, Batch 160, LR 0.000099 Loss 13.132534, Accuracy 41.270%\n",
      "Epoch 4, Batch 161, LR 0.000099 Loss 13.130803, Accuracy 41.256%\n",
      "Epoch 4, Batch 162, LR 0.000099 Loss 13.134300, Accuracy 41.204%\n",
      "Epoch 4, Batch 163, LR 0.000099 Loss 13.132126, Accuracy 41.234%\n",
      "Epoch 4, Batch 164, LR 0.000099 Loss 13.131472, Accuracy 41.230%\n",
      "Epoch 4, Batch 165, LR 0.000099 Loss 13.131849, Accuracy 41.217%\n",
      "Epoch 4, Batch 166, LR 0.000099 Loss 13.126924, Accuracy 41.260%\n",
      "Epoch 4, Batch 167, LR 0.000099 Loss 13.125474, Accuracy 41.252%\n",
      "Epoch 4, Batch 168, LR 0.000099 Loss 13.125093, Accuracy 41.234%\n",
      "Epoch 4, Batch 169, LR 0.000099 Loss 13.127468, Accuracy 41.207%\n",
      "Epoch 4, Batch 170, LR 0.000099 Loss 13.128444, Accuracy 41.195%\n",
      "Epoch 4, Batch 171, LR 0.000099 Loss 13.129808, Accuracy 41.169%\n",
      "Epoch 4, Batch 172, LR 0.000099 Loss 13.131493, Accuracy 41.156%\n",
      "Epoch 4, Batch 173, LR 0.000099 Loss 13.131267, Accuracy 41.135%\n",
      "Epoch 4, Batch 174, LR 0.000099 Loss 13.131423, Accuracy 41.132%\n",
      "Epoch 4, Batch 175, LR 0.000099 Loss 13.130445, Accuracy 41.125%\n",
      "Epoch 4, Batch 176, LR 0.000099 Loss 13.132355, Accuracy 41.096%\n",
      "Epoch 4, Batch 177, LR 0.000099 Loss 13.131023, Accuracy 41.066%\n",
      "Epoch 4, Batch 178, LR 0.000099 Loss 13.128727, Accuracy 41.090%\n",
      "Epoch 4, Batch 179, LR 0.000099 Loss 13.126288, Accuracy 41.101%\n",
      "Epoch 4, Batch 180, LR 0.000099 Loss 13.128715, Accuracy 41.085%\n",
      "Epoch 4, Batch 181, LR 0.000099 Loss 13.129187, Accuracy 41.057%\n",
      "Epoch 4, Batch 182, LR 0.000099 Loss 13.129803, Accuracy 41.050%\n",
      "Epoch 4, Batch 183, LR 0.000099 Loss 13.127705, Accuracy 41.060%\n",
      "Epoch 4, Batch 184, LR 0.000099 Loss 13.126168, Accuracy 41.075%\n",
      "Epoch 4, Batch 185, LR 0.000099 Loss 13.129178, Accuracy 41.043%\n",
      "Epoch 4, Batch 186, LR 0.000099 Loss 13.126736, Accuracy 41.049%\n",
      "Epoch 4, Batch 187, LR 0.000099 Loss 13.127855, Accuracy 41.030%\n",
      "Epoch 4, Batch 188, LR 0.000099 Loss 13.127254, Accuracy 41.028%\n",
      "Epoch 4, Batch 189, LR 0.000099 Loss 13.125101, Accuracy 41.051%\n",
      "Epoch 4, Batch 190, LR 0.000099 Loss 13.121254, Accuracy 41.061%\n",
      "Epoch 4, Batch 191, LR 0.000099 Loss 13.123995, Accuracy 41.059%\n",
      "Epoch 4, Batch 192, LR 0.000099 Loss 13.123601, Accuracy 41.069%\n",
      "Epoch 4, Batch 193, LR 0.000099 Loss 13.123526, Accuracy 41.074%\n",
      "Epoch 4, Batch 194, LR 0.000099 Loss 13.124522, Accuracy 41.064%\n",
      "Epoch 4, Batch 195, LR 0.000099 Loss 13.122623, Accuracy 41.074%\n",
      "Epoch 4, Batch 196, LR 0.000099 Loss 13.122933, Accuracy 41.059%\n",
      "Epoch 4, Batch 197, LR 0.000099 Loss 13.125977, Accuracy 41.026%\n",
      "Epoch 4, Batch 198, LR 0.000099 Loss 13.125633, Accuracy 41.024%\n",
      "Epoch 4, Batch 199, LR 0.000099 Loss 13.124927, Accuracy 41.014%\n",
      "Epoch 4, Batch 200, LR 0.000099 Loss 13.126944, Accuracy 40.988%\n",
      "Epoch 4, Batch 201, LR 0.000099 Loss 13.124646, Accuracy 41.021%\n",
      "Epoch 4, Batch 202, LR 0.000099 Loss 13.124923, Accuracy 41.047%\n",
      "Epoch 4, Batch 203, LR 0.000099 Loss 13.122634, Accuracy 41.068%\n",
      "Epoch 4, Batch 204, LR 0.000099 Loss 13.123639, Accuracy 41.035%\n",
      "Epoch 4, Batch 205, LR 0.000099 Loss 13.118741, Accuracy 41.071%\n",
      "Epoch 4, Batch 206, LR 0.000099 Loss 13.120369, Accuracy 41.091%\n",
      "Epoch 4, Batch 207, LR 0.000099 Loss 13.120475, Accuracy 41.097%\n",
      "Epoch 4, Batch 208, LR 0.000099 Loss 13.119705, Accuracy 41.098%\n",
      "Epoch 4, Batch 209, LR 0.000099 Loss 13.118449, Accuracy 41.092%\n",
      "Epoch 4, Batch 210, LR 0.000099 Loss 13.117247, Accuracy 41.101%\n",
      "Epoch 4, Batch 211, LR 0.000099 Loss 13.116715, Accuracy 41.125%\n",
      "Epoch 4, Batch 212, LR 0.000099 Loss 13.118936, Accuracy 41.104%\n",
      "Epoch 4, Batch 213, LR 0.000099 Loss 13.117888, Accuracy 41.116%\n",
      "Epoch 4, Batch 214, LR 0.000099 Loss 13.115823, Accuracy 41.111%\n",
      "Epoch 4, Batch 215, LR 0.000099 Loss 13.116168, Accuracy 41.108%\n",
      "Epoch 4, Batch 216, LR 0.000099 Loss 13.114290, Accuracy 41.117%\n",
      "Epoch 4, Batch 217, LR 0.000099 Loss 13.115378, Accuracy 41.111%\n",
      "Epoch 4, Batch 218, LR 0.000099 Loss 13.115722, Accuracy 41.098%\n",
      "Epoch 4, Batch 219, LR 0.000099 Loss 13.116653, Accuracy 41.103%\n",
      "Epoch 4, Batch 220, LR 0.000099 Loss 13.118246, Accuracy 41.108%\n",
      "Epoch 4, Batch 221, LR 0.000099 Loss 13.119366, Accuracy 41.092%\n",
      "Epoch 4, Batch 222, LR 0.000099 Loss 13.116976, Accuracy 41.135%\n",
      "Epoch 4, Batch 223, LR 0.000099 Loss 13.115290, Accuracy 41.158%\n",
      "Epoch 4, Batch 224, LR 0.000099 Loss 13.116405, Accuracy 41.145%\n",
      "Epoch 4, Batch 225, LR 0.000099 Loss 13.114647, Accuracy 41.149%\n",
      "Epoch 4, Batch 226, LR 0.000099 Loss 13.115850, Accuracy 41.144%\n",
      "Epoch 4, Batch 227, LR 0.000099 Loss 13.117800, Accuracy 41.124%\n",
      "Epoch 4, Batch 228, LR 0.000099 Loss 13.117549, Accuracy 41.136%\n",
      "Epoch 4, Batch 229, LR 0.000099 Loss 13.115369, Accuracy 41.147%\n",
      "Epoch 4, Batch 230, LR 0.000099 Loss 13.114897, Accuracy 41.175%\n",
      "Epoch 4, Batch 231, LR 0.000099 Loss 13.113105, Accuracy 41.190%\n",
      "Epoch 4, Batch 232, LR 0.000099 Loss 13.114491, Accuracy 41.167%\n",
      "Epoch 4, Batch 233, LR 0.000099 Loss 13.114615, Accuracy 41.155%\n",
      "Epoch 4, Batch 234, LR 0.000099 Loss 13.114099, Accuracy 41.166%\n",
      "Epoch 4, Batch 235, LR 0.000099 Loss 13.115664, Accuracy 41.157%\n",
      "Epoch 4, Batch 236, LR 0.000099 Loss 13.115586, Accuracy 41.138%\n",
      "Epoch 4, Batch 237, LR 0.000099 Loss 13.115176, Accuracy 41.113%\n",
      "Epoch 4, Batch 238, LR 0.000099 Loss 13.112024, Accuracy 41.127%\n",
      "Epoch 4, Batch 239, LR 0.000099 Loss 13.111848, Accuracy 41.135%\n",
      "Epoch 4, Batch 240, LR 0.000099 Loss 13.114257, Accuracy 41.117%\n",
      "Epoch 4, Batch 241, LR 0.000099 Loss 13.114655, Accuracy 41.108%\n",
      "Epoch 4, Batch 242, LR 0.000099 Loss 13.114000, Accuracy 41.106%\n",
      "Epoch 4, Batch 243, LR 0.000099 Loss 13.113446, Accuracy 41.104%\n",
      "Epoch 4, Batch 244, LR 0.000099 Loss 13.113795, Accuracy 41.102%\n",
      "Epoch 4, Batch 245, LR 0.000099 Loss 13.115469, Accuracy 41.091%\n",
      "Epoch 4, Batch 246, LR 0.000099 Loss 13.113670, Accuracy 41.114%\n",
      "Epoch 4, Batch 247, LR 0.000099 Loss 13.114799, Accuracy 41.122%\n",
      "Epoch 4, Batch 248, LR 0.000099 Loss 13.116351, Accuracy 41.132%\n",
      "Epoch 4, Batch 249, LR 0.000099 Loss 13.114631, Accuracy 41.152%\n",
      "Epoch 4, Batch 250, LR 0.000099 Loss 13.113655, Accuracy 41.159%\n",
      "Epoch 4, Batch 251, LR 0.000099 Loss 13.112022, Accuracy 41.176%\n",
      "Epoch 4, Batch 252, LR 0.000099 Loss 13.110441, Accuracy 41.180%\n",
      "Epoch 4, Batch 253, LR 0.000099 Loss 13.110566, Accuracy 41.193%\n",
      "Epoch 4, Batch 254, LR 0.000099 Loss 13.106230, Accuracy 41.216%\n",
      "Epoch 4, Batch 255, LR 0.000099 Loss 13.106531, Accuracy 41.213%\n",
      "Epoch 4, Batch 256, LR 0.000099 Loss 13.104786, Accuracy 41.217%\n",
      "Epoch 4, Batch 257, LR 0.000099 Loss 13.104633, Accuracy 41.215%\n",
      "Epoch 4, Batch 258, LR 0.000099 Loss 13.103887, Accuracy 41.215%\n",
      "Epoch 4, Batch 259, LR 0.000099 Loss 13.101811, Accuracy 41.228%\n",
      "Epoch 4, Batch 260, LR 0.000099 Loss 13.101415, Accuracy 41.244%\n",
      "Epoch 4, Batch 261, LR 0.000099 Loss 13.102819, Accuracy 41.242%\n",
      "Epoch 4, Batch 262, LR 0.000099 Loss 13.101455, Accuracy 41.275%\n",
      "Epoch 4, Batch 263, LR 0.000099 Loss 13.100042, Accuracy 41.293%\n",
      "Epoch 4, Batch 264, LR 0.000099 Loss 13.100716, Accuracy 41.288%\n",
      "Epoch 4, Batch 265, LR 0.000099 Loss 13.098920, Accuracy 41.303%\n",
      "Epoch 4, Batch 266, LR 0.000099 Loss 13.098264, Accuracy 41.295%\n",
      "Epoch 4, Batch 267, LR 0.000099 Loss 13.096827, Accuracy 41.327%\n",
      "Epoch 4, Batch 268, LR 0.000099 Loss 13.095495, Accuracy 41.322%\n",
      "Epoch 4, Batch 269, LR 0.000099 Loss 13.097192, Accuracy 41.308%\n",
      "Epoch 4, Batch 270, LR 0.000099 Loss 13.096452, Accuracy 41.314%\n",
      "Epoch 4, Batch 271, LR 0.000099 Loss 13.092801, Accuracy 41.340%\n",
      "Epoch 4, Batch 272, LR 0.000099 Loss 13.092681, Accuracy 41.334%\n",
      "Epoch 4, Batch 273, LR 0.000099 Loss 13.091216, Accuracy 41.358%\n",
      "Epoch 4, Batch 274, LR 0.000099 Loss 13.089448, Accuracy 41.375%\n",
      "Epoch 4, Batch 275, LR 0.000099 Loss 13.089925, Accuracy 41.369%\n",
      "Epoch 4, Batch 276, LR 0.000099 Loss 13.089587, Accuracy 41.375%\n",
      "Epoch 4, Batch 277, LR 0.000099 Loss 13.089343, Accuracy 41.387%\n",
      "Epoch 4, Batch 278, LR 0.000099 Loss 13.089909, Accuracy 41.392%\n",
      "Epoch 4, Batch 279, LR 0.000099 Loss 13.091879, Accuracy 41.373%\n",
      "Epoch 4, Batch 280, LR 0.000099 Loss 13.092713, Accuracy 41.367%\n",
      "Epoch 4, Batch 281, LR 0.000099 Loss 13.092108, Accuracy 41.387%\n",
      "Epoch 4, Batch 282, LR 0.000099 Loss 13.091251, Accuracy 41.392%\n",
      "Epoch 4, Batch 283, LR 0.000099 Loss 13.090094, Accuracy 41.401%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 284, LR 0.000099 Loss 13.088167, Accuracy 41.420%\n",
      "Epoch 4, Batch 285, LR 0.000099 Loss 13.086441, Accuracy 41.458%\n",
      "Epoch 4, Batch 286, LR 0.000099 Loss 13.085515, Accuracy 41.464%\n",
      "Epoch 4, Batch 287, LR 0.000099 Loss 13.085945, Accuracy 41.453%\n",
      "Epoch 4, Batch 288, LR 0.000099 Loss 13.086869, Accuracy 41.425%\n",
      "Epoch 4, Batch 289, LR 0.000099 Loss 13.088195, Accuracy 41.425%\n",
      "Epoch 4, Batch 290, LR 0.000099 Loss 13.085568, Accuracy 41.444%\n",
      "Epoch 4, Batch 291, LR 0.000099 Loss 13.086705, Accuracy 41.436%\n",
      "Epoch 4, Batch 292, LR 0.000099 Loss 13.087567, Accuracy 41.441%\n",
      "Epoch 4, Batch 293, LR 0.000099 Loss 13.087483, Accuracy 41.441%\n",
      "Epoch 4, Batch 294, LR 0.000099 Loss 13.085138, Accuracy 41.443%\n",
      "Epoch 4, Batch 295, LR 0.000099 Loss 13.083407, Accuracy 41.449%\n",
      "Epoch 4, Batch 296, LR 0.000099 Loss 13.081543, Accuracy 41.459%\n",
      "Epoch 4, Batch 297, LR 0.000099 Loss 13.080022, Accuracy 41.448%\n",
      "Epoch 4, Batch 298, LR 0.000099 Loss 13.079310, Accuracy 41.443%\n",
      "Epoch 4, Batch 299, LR 0.000099 Loss 13.077311, Accuracy 41.477%\n",
      "Epoch 4, Batch 300, LR 0.000099 Loss 13.077442, Accuracy 41.479%\n",
      "Epoch 4, Batch 301, LR 0.000099 Loss 13.077122, Accuracy 41.502%\n",
      "Epoch 4, Batch 302, LR 0.000099 Loss 13.075858, Accuracy 41.528%\n",
      "Epoch 4, Batch 303, LR 0.000099 Loss 13.075918, Accuracy 41.530%\n",
      "Epoch 4, Batch 304, LR 0.000099 Loss 13.076172, Accuracy 41.553%\n",
      "Epoch 4, Batch 305, LR 0.000099 Loss 13.075861, Accuracy 41.562%\n",
      "Epoch 4, Batch 306, LR 0.000099 Loss 13.075805, Accuracy 41.554%\n",
      "Epoch 4, Batch 307, LR 0.000099 Loss 13.073753, Accuracy 41.592%\n",
      "Epoch 4, Batch 308, LR 0.000099 Loss 13.073540, Accuracy 41.599%\n",
      "Epoch 4, Batch 309, LR 0.000099 Loss 13.073519, Accuracy 41.588%\n",
      "Epoch 4, Batch 310, LR 0.000099 Loss 13.071806, Accuracy 41.618%\n",
      "Epoch 4, Batch 311, LR 0.000099 Loss 13.070555, Accuracy 41.642%\n",
      "Epoch 4, Batch 312, LR 0.000099 Loss 13.069181, Accuracy 41.639%\n",
      "Epoch 4, Batch 313, LR 0.000099 Loss 13.068105, Accuracy 41.636%\n",
      "Epoch 4, Batch 314, LR 0.000099 Loss 13.068552, Accuracy 41.628%\n",
      "Epoch 4, Batch 315, LR 0.000099 Loss 13.068368, Accuracy 41.639%\n",
      "Epoch 4, Batch 316, LR 0.000099 Loss 13.066184, Accuracy 41.649%\n",
      "Epoch 4, Batch 317, LR 0.000099 Loss 13.065177, Accuracy 41.658%\n",
      "Epoch 4, Batch 318, LR 0.000099 Loss 13.065373, Accuracy 41.652%\n",
      "Epoch 4, Batch 319, LR 0.000099 Loss 13.065427, Accuracy 41.656%\n",
      "Epoch 4, Batch 320, LR 0.000099 Loss 13.065526, Accuracy 41.658%\n",
      "Epoch 4, Batch 321, LR 0.000099 Loss 13.063719, Accuracy 41.691%\n",
      "Epoch 4, Batch 322, LR 0.000099 Loss 13.062245, Accuracy 41.714%\n",
      "Epoch 4, Batch 323, LR 0.000099 Loss 13.063561, Accuracy 41.711%\n",
      "Epoch 4, Batch 324, LR 0.000099 Loss 13.064369, Accuracy 41.703%\n",
      "Epoch 4, Batch 325, LR 0.000099 Loss 13.063857, Accuracy 41.712%\n",
      "Epoch 4, Batch 326, LR 0.000099 Loss 13.064486, Accuracy 41.715%\n",
      "Epoch 4, Batch 327, LR 0.000099 Loss 13.064005, Accuracy 41.722%\n",
      "Epoch 4, Batch 328, LR 0.000099 Loss 13.062696, Accuracy 41.744%\n",
      "Epoch 4, Batch 329, LR 0.000099 Loss 13.061155, Accuracy 41.755%\n",
      "Epoch 4, Batch 330, LR 0.000099 Loss 13.061168, Accuracy 41.754%\n",
      "Epoch 4, Batch 331, LR 0.000099 Loss 13.060393, Accuracy 41.753%\n",
      "Epoch 4, Batch 332, LR 0.000099 Loss 13.057974, Accuracy 41.780%\n",
      "Epoch 4, Batch 333, LR 0.000099 Loss 13.058977, Accuracy 41.768%\n",
      "Epoch 4, Batch 334, LR 0.000099 Loss 13.059913, Accuracy 41.741%\n",
      "Epoch 4, Batch 335, LR 0.000099 Loss 13.059706, Accuracy 41.744%\n",
      "Epoch 4, Batch 336, LR 0.000099 Loss 13.059256, Accuracy 41.732%\n",
      "Epoch 4, Batch 337, LR 0.000099 Loss 13.057611, Accuracy 41.747%\n",
      "Epoch 4, Batch 338, LR 0.000099 Loss 13.056601, Accuracy 41.751%\n",
      "Epoch 4, Batch 339, LR 0.000099 Loss 13.057709, Accuracy 41.724%\n",
      "Epoch 4, Batch 340, LR 0.000099 Loss 13.057188, Accuracy 41.719%\n",
      "Epoch 4, Batch 341, LR 0.000099 Loss 13.055827, Accuracy 41.736%\n",
      "Epoch 4, Batch 342, LR 0.000099 Loss 13.056113, Accuracy 41.737%\n",
      "Epoch 4, Batch 343, LR 0.000099 Loss 13.054106, Accuracy 41.752%\n",
      "Epoch 4, Batch 344, LR 0.000099 Loss 13.054664, Accuracy 41.742%\n",
      "Epoch 4, Batch 345, LR 0.000099 Loss 13.052817, Accuracy 41.760%\n",
      "Epoch 4, Batch 346, LR 0.000099 Loss 13.049891, Accuracy 41.786%\n",
      "Epoch 4, Batch 347, LR 0.000099 Loss 13.047755, Accuracy 41.800%\n",
      "Epoch 4, Batch 348, LR 0.000099 Loss 13.048526, Accuracy 41.797%\n",
      "Epoch 4, Batch 349, LR 0.000099 Loss 13.046804, Accuracy 41.818%\n",
      "Epoch 4, Batch 350, LR 0.000099 Loss 13.047176, Accuracy 41.817%\n",
      "Epoch 4, Batch 351, LR 0.000099 Loss 13.044552, Accuracy 41.838%\n",
      "Epoch 4, Batch 352, LR 0.000099 Loss 13.044635, Accuracy 41.841%\n",
      "Epoch 4, Batch 353, LR 0.000099 Loss 13.044706, Accuracy 41.831%\n",
      "Epoch 4, Batch 354, LR 0.000099 Loss 13.043349, Accuracy 41.845%\n",
      "Epoch 4, Batch 355, LR 0.000099 Loss 13.043312, Accuracy 41.844%\n",
      "Epoch 4, Batch 356, LR 0.000099 Loss 13.042841, Accuracy 41.863%\n",
      "Epoch 4, Batch 357, LR 0.000099 Loss 13.040019, Accuracy 41.899%\n",
      "Epoch 4, Batch 358, LR 0.000099 Loss 13.042326, Accuracy 41.873%\n",
      "Epoch 4, Batch 359, LR 0.000099 Loss 13.042342, Accuracy 41.876%\n",
      "Epoch 4, Batch 360, LR 0.000099 Loss 13.043320, Accuracy 41.864%\n",
      "Epoch 4, Batch 361, LR 0.000099 Loss 13.043564, Accuracy 41.854%\n",
      "Epoch 4, Batch 362, LR 0.000099 Loss 13.045049, Accuracy 41.842%\n",
      "Epoch 4, Batch 363, LR 0.000099 Loss 13.044744, Accuracy 41.858%\n",
      "Epoch 4, Batch 364, LR 0.000099 Loss 13.044668, Accuracy 41.863%\n",
      "Epoch 4, Batch 365, LR 0.000099 Loss 13.045659, Accuracy 41.871%\n",
      "Epoch 4, Batch 366, LR 0.000099 Loss 13.045730, Accuracy 41.876%\n",
      "Epoch 4, Batch 367, LR 0.000099 Loss 13.045247, Accuracy 41.877%\n",
      "Epoch 4, Batch 368, LR 0.000099 Loss 13.044372, Accuracy 41.884%\n",
      "Epoch 4, Batch 369, LR 0.000099 Loss 13.044856, Accuracy 41.872%\n",
      "Epoch 4, Batch 370, LR 0.000099 Loss 13.040908, Accuracy 41.902%\n",
      "Epoch 4, Batch 371, LR 0.000099 Loss 13.039802, Accuracy 41.916%\n",
      "Epoch 4, Batch 372, LR 0.000099 Loss 13.040107, Accuracy 41.908%\n",
      "Epoch 4, Batch 373, LR 0.000099 Loss 13.036480, Accuracy 41.936%\n",
      "Epoch 4, Batch 374, LR 0.000099 Loss 13.035285, Accuracy 41.947%\n",
      "Epoch 4, Batch 375, LR 0.000099 Loss 13.035767, Accuracy 41.933%\n",
      "Epoch 4, Batch 376, LR 0.000099 Loss 13.034896, Accuracy 41.946%\n",
      "Epoch 4, Batch 377, LR 0.000099 Loss 13.035299, Accuracy 41.941%\n",
      "Epoch 4, Batch 378, LR 0.000099 Loss 13.036120, Accuracy 41.942%\n",
      "Epoch 4, Batch 379, LR 0.000099 Loss 13.034658, Accuracy 41.946%\n",
      "Epoch 4, Batch 380, LR 0.000099 Loss 13.033701, Accuracy 41.949%\n",
      "Epoch 4, Batch 381, LR 0.000099 Loss 13.034018, Accuracy 41.935%\n",
      "Epoch 4, Batch 382, LR 0.000099 Loss 13.032985, Accuracy 41.946%\n",
      "Epoch 4, Batch 383, LR 0.000099 Loss 13.029699, Accuracy 41.961%\n",
      "Epoch 4, Batch 384, LR 0.000099 Loss 13.028262, Accuracy 41.970%\n",
      "Epoch 4, Batch 385, LR 0.000099 Loss 13.025898, Accuracy 41.993%\n",
      "Epoch 4, Batch 386, LR 0.000099 Loss 13.025436, Accuracy 41.977%\n",
      "Epoch 4, Batch 387, LR 0.000099 Loss 13.026146, Accuracy 41.971%\n",
      "Epoch 4, Batch 388, LR 0.000099 Loss 13.026578, Accuracy 41.974%\n",
      "Epoch 4, Batch 389, LR 0.000099 Loss 13.026303, Accuracy 41.991%\n",
      "Epoch 4, Batch 390, LR 0.000099 Loss 13.025386, Accuracy 41.991%\n",
      "Epoch 4, Batch 391, LR 0.000099 Loss 13.025673, Accuracy 41.980%\n",
      "Epoch 4, Batch 392, LR 0.000099 Loss 13.025642, Accuracy 41.970%\n",
      "Epoch 4, Batch 393, LR 0.000099 Loss 13.024465, Accuracy 41.985%\n",
      "Epoch 4, Batch 394, LR 0.000099 Loss 13.022983, Accuracy 41.997%\n",
      "Epoch 4, Batch 395, LR 0.000099 Loss 13.023963, Accuracy 41.982%\n",
      "Epoch 4, Batch 396, LR 0.000099 Loss 13.024054, Accuracy 41.967%\n",
      "Epoch 4, Batch 397, LR 0.000099 Loss 13.024364, Accuracy 41.959%\n",
      "Epoch 4, Batch 398, LR 0.000099 Loss 13.024637, Accuracy 41.954%\n",
      "Epoch 4, Batch 399, LR 0.000099 Loss 13.025414, Accuracy 41.953%\n",
      "Epoch 4, Batch 400, LR 0.000099 Loss 13.025652, Accuracy 41.955%\n",
      "Epoch 4, Batch 401, LR 0.000099 Loss 13.025592, Accuracy 41.946%\n",
      "Epoch 4, Batch 402, LR 0.000099 Loss 13.024493, Accuracy 41.960%\n",
      "Epoch 4, Batch 403, LR 0.000099 Loss 13.023810, Accuracy 41.961%\n",
      "Epoch 4, Batch 404, LR 0.000099 Loss 13.024380, Accuracy 41.959%\n",
      "Epoch 4, Batch 405, LR 0.000099 Loss 13.024564, Accuracy 41.954%\n",
      "Epoch 4, Batch 406, LR 0.000099 Loss 13.022793, Accuracy 41.968%\n",
      "Epoch 4, Batch 407, LR 0.000099 Loss 13.023142, Accuracy 41.961%\n",
      "Epoch 4, Batch 408, LR 0.000099 Loss 13.023058, Accuracy 41.969%\n",
      "Epoch 4, Batch 409, LR 0.000099 Loss 13.022773, Accuracy 41.977%\n",
      "Epoch 4, Batch 410, LR 0.000099 Loss 13.022175, Accuracy 41.980%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 411, LR 0.000099 Loss 13.022628, Accuracy 41.977%\n",
      "Epoch 4, Batch 412, LR 0.000099 Loss 13.024025, Accuracy 41.964%\n",
      "Epoch 4, Batch 413, LR 0.000099 Loss 13.024760, Accuracy 41.968%\n",
      "Epoch 4, Batch 414, LR 0.000099 Loss 13.023262, Accuracy 41.982%\n",
      "Epoch 4, Batch 415, LR 0.000099 Loss 13.023312, Accuracy 41.984%\n",
      "Epoch 4, Batch 416, LR 0.000099 Loss 13.021630, Accuracy 41.994%\n",
      "Epoch 4, Batch 417, LR 0.000099 Loss 13.020469, Accuracy 42.015%\n",
      "Epoch 4, Batch 418, LR 0.000099 Loss 13.017737, Accuracy 42.042%\n",
      "Epoch 4, Batch 419, LR 0.000099 Loss 13.019097, Accuracy 42.035%\n",
      "Epoch 4, Batch 420, LR 0.000099 Loss 13.017265, Accuracy 42.048%\n",
      "Epoch 4, Batch 421, LR 0.000099 Loss 13.016897, Accuracy 42.045%\n",
      "Epoch 4, Batch 422, LR 0.000099 Loss 13.013613, Accuracy 42.067%\n",
      "Epoch 4, Batch 423, LR 0.000099 Loss 13.013821, Accuracy 42.062%\n",
      "Epoch 4, Batch 424, LR 0.000099 Loss 13.012749, Accuracy 42.075%\n",
      "Epoch 4, Batch 425, LR 0.000099 Loss 13.012335, Accuracy 42.074%\n",
      "Epoch 4, Batch 426, LR 0.000099 Loss 13.012544, Accuracy 42.074%\n",
      "Epoch 4, Batch 427, LR 0.000099 Loss 13.013433, Accuracy 42.061%\n",
      "Epoch 4, Batch 428, LR 0.000099 Loss 13.011694, Accuracy 42.074%\n",
      "Epoch 4, Batch 429, LR 0.000099 Loss 13.010418, Accuracy 42.091%\n",
      "Epoch 4, Batch 430, LR 0.000099 Loss 13.007628, Accuracy 42.108%\n",
      "Epoch 4, Batch 431, LR 0.000099 Loss 13.007090, Accuracy 42.106%\n",
      "Epoch 4, Batch 432, LR 0.000099 Loss 13.006001, Accuracy 42.119%\n",
      "Epoch 4, Batch 433, LR 0.000099 Loss 13.004794, Accuracy 42.132%\n",
      "Epoch 4, Batch 434, LR 0.000099 Loss 13.004943, Accuracy 42.139%\n",
      "Epoch 4, Batch 435, LR 0.000099 Loss 13.004857, Accuracy 42.143%\n",
      "Epoch 4, Batch 436, LR 0.000099 Loss 13.003456, Accuracy 42.159%\n",
      "Epoch 4, Batch 437, LR 0.000099 Loss 13.004316, Accuracy 42.154%\n",
      "Epoch 4, Batch 438, LR 0.000099 Loss 13.004002, Accuracy 42.163%\n",
      "Epoch 4, Batch 439, LR 0.000099 Loss 13.002247, Accuracy 42.184%\n",
      "Epoch 4, Batch 440, LR 0.000099 Loss 13.000466, Accuracy 42.200%\n",
      "Epoch 4, Batch 441, LR 0.000099 Loss 13.000521, Accuracy 42.200%\n",
      "Epoch 4, Batch 442, LR 0.000099 Loss 12.999941, Accuracy 42.195%\n",
      "Epoch 4, Batch 443, LR 0.000099 Loss 12.998656, Accuracy 42.216%\n",
      "Epoch 4, Batch 444, LR 0.000099 Loss 12.998944, Accuracy 42.205%\n",
      "Epoch 4, Batch 445, LR 0.000099 Loss 12.999927, Accuracy 42.202%\n",
      "Epoch 4, Batch 446, LR 0.000099 Loss 13.000565, Accuracy 42.209%\n",
      "Epoch 4, Batch 447, LR 0.000099 Loss 13.001449, Accuracy 42.214%\n",
      "Epoch 4, Batch 448, LR 0.000099 Loss 13.000630, Accuracy 42.224%\n",
      "Epoch 4, Batch 449, LR 0.000099 Loss 13.000006, Accuracy 42.234%\n",
      "Epoch 4, Batch 450, LR 0.000099 Loss 12.997906, Accuracy 42.248%\n",
      "Epoch 4, Batch 451, LR 0.000099 Loss 12.998131, Accuracy 42.234%\n",
      "Epoch 4, Batch 452, LR 0.000099 Loss 12.998088, Accuracy 42.245%\n",
      "Epoch 4, Batch 453, LR 0.000099 Loss 12.998100, Accuracy 42.243%\n",
      "Epoch 4, Batch 454, LR 0.000099 Loss 12.998144, Accuracy 42.236%\n",
      "Epoch 4, Batch 455, LR 0.000099 Loss 12.998647, Accuracy 42.229%\n",
      "Epoch 4, Batch 456, LR 0.000099 Loss 12.997938, Accuracy 42.229%\n",
      "Epoch 4, Batch 457, LR 0.000099 Loss 12.997509, Accuracy 42.227%\n",
      "Epoch 4, Batch 458, LR 0.000099 Loss 12.997834, Accuracy 42.225%\n",
      "Epoch 4, Batch 459, LR 0.000099 Loss 12.998266, Accuracy 42.218%\n",
      "Epoch 4, Batch 460, LR 0.000099 Loss 12.997579, Accuracy 42.213%\n",
      "Epoch 4, Batch 461, LR 0.000099 Loss 12.996284, Accuracy 42.228%\n",
      "Epoch 4, Batch 462, LR 0.000099 Loss 12.995892, Accuracy 42.230%\n",
      "Epoch 4, Batch 463, LR 0.000099 Loss 12.995188, Accuracy 42.236%\n",
      "Epoch 4, Batch 464, LR 0.000099 Loss 12.996791, Accuracy 42.214%\n",
      "Epoch 4, Batch 465, LR 0.000099 Loss 12.997380, Accuracy 42.209%\n",
      "Epoch 4, Batch 466, LR 0.000099 Loss 12.997842, Accuracy 42.206%\n",
      "Epoch 4, Batch 467, LR 0.000099 Loss 12.999130, Accuracy 42.189%\n",
      "Epoch 4, Batch 468, LR 0.000099 Loss 12.999101, Accuracy 42.179%\n",
      "Epoch 4, Batch 469, LR 0.000099 Loss 12.996223, Accuracy 42.199%\n",
      "Epoch 4, Batch 470, LR 0.000099 Loss 12.995324, Accuracy 42.212%\n",
      "Epoch 4, Batch 471, LR 0.000099 Loss 12.995560, Accuracy 42.216%\n",
      "Epoch 4, Batch 472, LR 0.000099 Loss 12.994435, Accuracy 42.222%\n",
      "Epoch 4, Batch 473, LR 0.000099 Loss 12.993401, Accuracy 42.227%\n",
      "Epoch 4, Batch 474, LR 0.000099 Loss 12.992438, Accuracy 42.235%\n",
      "Epoch 4, Batch 475, LR 0.000099 Loss 12.993422, Accuracy 42.227%\n",
      "Epoch 4, Batch 476, LR 0.000099 Loss 12.993358, Accuracy 42.232%\n",
      "Epoch 4, Batch 477, LR 0.000099 Loss 12.992465, Accuracy 42.242%\n",
      "Epoch 4, Batch 478, LR 0.000099 Loss 12.993106, Accuracy 42.225%\n",
      "Epoch 4, Batch 479, LR 0.000099 Loss 12.992498, Accuracy 42.230%\n",
      "Epoch 4, Batch 480, LR 0.000099 Loss 12.991550, Accuracy 42.233%\n",
      "Epoch 4, Batch 481, LR 0.000099 Loss 12.992660, Accuracy 42.217%\n",
      "Epoch 4, Batch 482, LR 0.000099 Loss 12.993119, Accuracy 42.217%\n",
      "Epoch 4, Batch 483, LR 0.000099 Loss 12.992371, Accuracy 42.215%\n",
      "Epoch 4, Batch 484, LR 0.000099 Loss 12.993020, Accuracy 42.217%\n",
      "Epoch 4, Batch 485, LR 0.000099 Loss 12.992679, Accuracy 42.218%\n",
      "Epoch 4, Batch 486, LR 0.000099 Loss 12.991419, Accuracy 42.233%\n",
      "Epoch 4, Batch 487, LR 0.000099 Loss 12.992170, Accuracy 42.231%\n",
      "Epoch 4, Batch 488, LR 0.000099 Loss 12.993085, Accuracy 42.216%\n",
      "Epoch 4, Batch 489, LR 0.000099 Loss 12.993476, Accuracy 42.208%\n",
      "Epoch 4, Batch 490, LR 0.000099 Loss 12.992489, Accuracy 42.227%\n",
      "Epoch 4, Batch 491, LR 0.000099 Loss 12.993513, Accuracy 42.224%\n",
      "Epoch 4, Batch 492, LR 0.000099 Loss 12.992533, Accuracy 42.234%\n",
      "Epoch 4, Batch 493, LR 0.000099 Loss 12.992211, Accuracy 42.235%\n",
      "Epoch 4, Batch 494, LR 0.000099 Loss 12.991966, Accuracy 42.225%\n",
      "Epoch 4, Batch 495, LR 0.000099 Loss 12.991136, Accuracy 42.230%\n",
      "Epoch 4, Batch 496, LR 0.000099 Loss 12.990414, Accuracy 42.243%\n",
      "Epoch 4, Batch 497, LR 0.000099 Loss 12.990518, Accuracy 42.250%\n",
      "Epoch 4, Batch 498, LR 0.000099 Loss 12.988716, Accuracy 42.258%\n",
      "Epoch 4, Batch 499, LR 0.000099 Loss 12.986823, Accuracy 42.280%\n",
      "Epoch 4, Batch 500, LR 0.000099 Loss 12.986382, Accuracy 42.283%\n",
      "Epoch 4, Batch 501, LR 0.000099 Loss 12.985148, Accuracy 42.284%\n",
      "Epoch 4, Batch 502, LR 0.000099 Loss 12.985457, Accuracy 42.281%\n",
      "Epoch 4, Batch 503, LR 0.000099 Loss 12.985307, Accuracy 42.279%\n",
      "Epoch 4, Batch 504, LR 0.000099 Loss 12.984471, Accuracy 42.287%\n",
      "Epoch 4, Batch 505, LR 0.000099 Loss 12.983008, Accuracy 42.299%\n",
      "Epoch 4, Batch 506, LR 0.000099 Loss 12.982958, Accuracy 42.297%\n",
      "Epoch 4, Batch 507, LR 0.000099 Loss 12.983192, Accuracy 42.285%\n",
      "Epoch 4, Batch 508, LR 0.000099 Loss 12.983533, Accuracy 42.281%\n",
      "Epoch 4, Batch 509, LR 0.000099 Loss 12.983933, Accuracy 42.277%\n",
      "Epoch 4, Batch 510, LR 0.000099 Loss 12.984993, Accuracy 42.269%\n",
      "Epoch 4, Batch 511, LR 0.000099 Loss 12.985145, Accuracy 42.269%\n",
      "Epoch 4, Batch 512, LR 0.000099 Loss 12.984454, Accuracy 42.281%\n",
      "Epoch 4, Batch 513, LR 0.000099 Loss 12.985731, Accuracy 42.271%\n",
      "Epoch 4, Batch 514, LR 0.000099 Loss 12.984379, Accuracy 42.286%\n",
      "Epoch 4, Batch 515, LR 0.000099 Loss 12.983845, Accuracy 42.295%\n",
      "Epoch 4, Batch 516, LR 0.000099 Loss 12.983950, Accuracy 42.293%\n",
      "Epoch 4, Batch 517, LR 0.000099 Loss 12.984574, Accuracy 42.281%\n",
      "Epoch 4, Batch 518, LR 0.000099 Loss 12.983365, Accuracy 42.298%\n",
      "Epoch 4, Batch 519, LR 0.000099 Loss 12.983560, Accuracy 42.296%\n",
      "Epoch 4, Batch 520, LR 0.000099 Loss 12.983153, Accuracy 42.305%\n",
      "Epoch 4, Batch 521, LR 0.000099 Loss 12.980317, Accuracy 42.334%\n",
      "Epoch 4, Batch 522, LR 0.000099 Loss 12.980700, Accuracy 42.319%\n",
      "Epoch 4, Batch 523, LR 0.000099 Loss 12.981109, Accuracy 42.319%\n",
      "Epoch 4, Batch 524, LR 0.000099 Loss 12.981354, Accuracy 42.308%\n",
      "Epoch 4, Batch 525, LR 0.000099 Loss 12.980697, Accuracy 42.311%\n",
      "Epoch 4, Batch 526, LR 0.000099 Loss 12.979105, Accuracy 42.314%\n",
      "Epoch 4, Batch 527, LR 0.000099 Loss 12.979845, Accuracy 42.300%\n",
      "Epoch 4, Batch 528, LR 0.000099 Loss 12.977338, Accuracy 42.321%\n",
      "Epoch 4, Batch 529, LR 0.000099 Loss 12.978077, Accuracy 42.317%\n",
      "Epoch 4, Batch 530, LR 0.000099 Loss 12.978092, Accuracy 42.329%\n",
      "Epoch 4, Batch 531, LR 0.000099 Loss 12.977044, Accuracy 42.348%\n",
      "Epoch 4, Batch 532, LR 0.000099 Loss 12.975701, Accuracy 42.364%\n",
      "Epoch 4, Batch 533, LR 0.000099 Loss 12.976271, Accuracy 42.369%\n",
      "Epoch 4, Batch 534, LR 0.000099 Loss 12.975932, Accuracy 42.375%\n",
      "Epoch 4, Batch 535, LR 0.000099 Loss 12.973741, Accuracy 42.390%\n",
      "Epoch 4, Batch 536, LR 0.000099 Loss 12.972219, Accuracy 42.403%\n",
      "Epoch 4, Batch 537, LR 0.000099 Loss 12.971866, Accuracy 42.410%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 538, LR 0.000099 Loss 12.972593, Accuracy 42.407%\n",
      "Epoch 4, Batch 539, LR 0.000099 Loss 12.972583, Accuracy 42.411%\n",
      "Epoch 4, Batch 540, LR 0.000099 Loss 12.970788, Accuracy 42.433%\n",
      "Epoch 4, Batch 541, LR 0.000099 Loss 12.971870, Accuracy 42.424%\n",
      "Epoch 4, Batch 542, LR 0.000099 Loss 12.970871, Accuracy 42.431%\n",
      "Epoch 4, Batch 543, LR 0.000099 Loss 12.970094, Accuracy 42.444%\n",
      "Epoch 4, Batch 544, LR 0.000099 Loss 12.969492, Accuracy 42.449%\n",
      "Epoch 4, Batch 545, LR 0.000099 Loss 12.969618, Accuracy 42.450%\n",
      "Epoch 4, Batch 546, LR 0.000099 Loss 12.968789, Accuracy 42.458%\n",
      "Epoch 4, Batch 547, LR 0.000099 Loss 12.967890, Accuracy 42.462%\n",
      "Epoch 4, Batch 548, LR 0.000099 Loss 12.967933, Accuracy 42.471%\n",
      "Epoch 4, Batch 549, LR 0.000099 Loss 12.967724, Accuracy 42.481%\n",
      "Epoch 4, Batch 550, LR 0.000099 Loss 12.969349, Accuracy 42.462%\n",
      "Epoch 4, Batch 551, LR 0.000099 Loss 12.968477, Accuracy 42.477%\n",
      "Epoch 4, Batch 552, LR 0.000099 Loss 12.968833, Accuracy 42.479%\n",
      "Epoch 4, Batch 553, LR 0.000099 Loss 12.968791, Accuracy 42.476%\n",
      "Epoch 4, Batch 554, LR 0.000099 Loss 12.967924, Accuracy 42.484%\n",
      "Epoch 4, Batch 555, LR 0.000099 Loss 12.967035, Accuracy 42.494%\n",
      "Epoch 4, Batch 556, LR 0.000099 Loss 12.966456, Accuracy 42.502%\n",
      "Epoch 4, Batch 557, LR 0.000099 Loss 12.966671, Accuracy 42.503%\n",
      "Epoch 4, Batch 558, LR 0.000099 Loss 12.966270, Accuracy 42.508%\n",
      "Epoch 4, Batch 559, LR 0.000099 Loss 12.966297, Accuracy 42.503%\n",
      "Epoch 4, Batch 560, LR 0.000099 Loss 12.965317, Accuracy 42.511%\n",
      "Epoch 4, Batch 561, LR 0.000099 Loss 12.964142, Accuracy 42.519%\n",
      "Epoch 4, Batch 562, LR 0.000099 Loss 12.964113, Accuracy 42.521%\n",
      "Epoch 4, Batch 563, LR 0.000099 Loss 12.963557, Accuracy 42.521%\n",
      "Epoch 4, Batch 564, LR 0.000099 Loss 12.963259, Accuracy 42.523%\n",
      "Epoch 4, Batch 565, LR 0.000099 Loss 12.962562, Accuracy 42.528%\n",
      "Epoch 4, Batch 566, LR 0.000099 Loss 12.961129, Accuracy 42.537%\n",
      "Epoch 4, Batch 567, LR 0.000099 Loss 12.960159, Accuracy 42.544%\n",
      "Epoch 4, Batch 568, LR 0.000099 Loss 12.959260, Accuracy 42.546%\n",
      "Epoch 4, Batch 569, LR 0.000099 Loss 12.958232, Accuracy 42.558%\n",
      "Epoch 4, Batch 570, LR 0.000099 Loss 12.958084, Accuracy 42.558%\n",
      "Epoch 4, Batch 571, LR 0.000099 Loss 12.958382, Accuracy 42.554%\n",
      "Epoch 4, Batch 572, LR 0.000099 Loss 12.957336, Accuracy 42.566%\n",
      "Epoch 4, Batch 573, LR 0.000099 Loss 12.957045, Accuracy 42.572%\n",
      "Epoch 4, Batch 574, LR 0.000099 Loss 12.956228, Accuracy 42.579%\n",
      "Epoch 4, Batch 575, LR 0.000099 Loss 12.955655, Accuracy 42.583%\n",
      "Epoch 4, Batch 576, LR 0.000099 Loss 12.956239, Accuracy 42.575%\n",
      "Epoch 4, Batch 577, LR 0.000099 Loss 12.956571, Accuracy 42.575%\n",
      "Epoch 4, Batch 578, LR 0.000099 Loss 12.956718, Accuracy 42.566%\n",
      "Epoch 4, Batch 579, LR 0.000099 Loss 12.955972, Accuracy 42.567%\n",
      "Epoch 4, Batch 580, LR 0.000099 Loss 12.955436, Accuracy 42.569%\n",
      "Epoch 4, Batch 581, LR 0.000099 Loss 12.955210, Accuracy 42.563%\n",
      "Epoch 4, Batch 582, LR 0.000099 Loss 12.954560, Accuracy 42.569%\n",
      "Epoch 4, Batch 583, LR 0.000099 Loss 12.954882, Accuracy 42.568%\n",
      "Epoch 4, Batch 584, LR 0.000099 Loss 12.954587, Accuracy 42.575%\n",
      "Epoch 4, Batch 585, LR 0.000099 Loss 12.954621, Accuracy 42.576%\n",
      "Epoch 4, Batch 586, LR 0.000099 Loss 12.953355, Accuracy 42.595%\n",
      "Epoch 4, Batch 587, LR 0.000099 Loss 12.954402, Accuracy 42.591%\n",
      "Epoch 4, Batch 588, LR 0.000099 Loss 12.953913, Accuracy 42.590%\n",
      "Epoch 4, Batch 589, LR 0.000099 Loss 12.953583, Accuracy 42.587%\n",
      "Epoch 4, Batch 590, LR 0.000099 Loss 12.952960, Accuracy 42.594%\n",
      "Epoch 4, Batch 591, LR 0.000099 Loss 12.952845, Accuracy 42.593%\n",
      "Epoch 4, Batch 592, LR 0.000099 Loss 12.953231, Accuracy 42.585%\n",
      "Epoch 4, Batch 593, LR 0.000099 Loss 12.951825, Accuracy 42.601%\n",
      "Epoch 4, Batch 594, LR 0.000099 Loss 12.951861, Accuracy 42.591%\n",
      "Epoch 4, Batch 595, LR 0.000099 Loss 12.951497, Accuracy 42.593%\n",
      "Epoch 4, Batch 596, LR 0.000099 Loss 12.951553, Accuracy 42.595%\n",
      "Epoch 4, Batch 597, LR 0.000099 Loss 12.950256, Accuracy 42.605%\n",
      "Epoch 4, Batch 598, LR 0.000099 Loss 12.949615, Accuracy 42.611%\n",
      "Epoch 4, Batch 599, LR 0.000099 Loss 12.950566, Accuracy 42.600%\n",
      "Epoch 4, Batch 600, LR 0.000099 Loss 12.950164, Accuracy 42.603%\n",
      "Epoch 4, Batch 601, LR 0.000099 Loss 12.949176, Accuracy 42.606%\n",
      "Epoch 4, Batch 602, LR 0.000099 Loss 12.948260, Accuracy 42.613%\n",
      "Epoch 4, Batch 603, LR 0.000099 Loss 12.948692, Accuracy 42.606%\n",
      "Epoch 4, Batch 604, LR 0.000099 Loss 12.947503, Accuracy 42.614%\n",
      "Epoch 4, Batch 605, LR 0.000099 Loss 12.946502, Accuracy 42.621%\n",
      "Epoch 4, Batch 606, LR 0.000099 Loss 12.945507, Accuracy 42.632%\n",
      "Epoch 4, Batch 607, LR 0.000099 Loss 12.945237, Accuracy 42.639%\n",
      "Epoch 4, Batch 608, LR 0.000099 Loss 12.944678, Accuracy 42.653%\n",
      "Epoch 4, Batch 609, LR 0.000099 Loss 12.943919, Accuracy 42.644%\n",
      "Epoch 4, Batch 610, LR 0.000099 Loss 12.943235, Accuracy 42.641%\n",
      "Epoch 4, Batch 611, LR 0.000099 Loss 12.942135, Accuracy 42.652%\n",
      "Epoch 4, Batch 612, LR 0.000099 Loss 12.941675, Accuracy 42.661%\n",
      "Epoch 4, Batch 613, LR 0.000099 Loss 12.941352, Accuracy 42.663%\n",
      "Epoch 4, Batch 614, LR 0.000099 Loss 12.940877, Accuracy 42.668%\n",
      "Epoch 4, Batch 615, LR 0.000099 Loss 12.940389, Accuracy 42.677%\n",
      "Epoch 4, Batch 616, LR 0.000099 Loss 12.939100, Accuracy 42.695%\n",
      "Epoch 4, Batch 617, LR 0.000099 Loss 12.939467, Accuracy 42.698%\n",
      "Epoch 4, Batch 618, LR 0.000099 Loss 12.938556, Accuracy 42.713%\n",
      "Epoch 4, Batch 619, LR 0.000099 Loss 12.938246, Accuracy 42.709%\n",
      "Epoch 4, Batch 620, LR 0.000099 Loss 12.937273, Accuracy 42.718%\n",
      "Epoch 4, Batch 621, LR 0.000099 Loss 12.936383, Accuracy 42.730%\n",
      "Epoch 4, Batch 622, LR 0.000099 Loss 12.935983, Accuracy 42.725%\n",
      "Epoch 4, Batch 623, LR 0.000099 Loss 12.936350, Accuracy 42.719%\n",
      "Epoch 4, Batch 624, LR 0.000099 Loss 12.935957, Accuracy 42.722%\n",
      "Epoch 4, Batch 625, LR 0.000099 Loss 12.934666, Accuracy 42.740%\n",
      "Epoch 4, Batch 626, LR 0.000099 Loss 12.934282, Accuracy 42.737%\n",
      "Epoch 4, Batch 627, LR 0.000099 Loss 12.934072, Accuracy 42.743%\n",
      "Epoch 4, Batch 628, LR 0.000099 Loss 12.933616, Accuracy 42.741%\n",
      "Epoch 4, Batch 629, LR 0.000099 Loss 12.934186, Accuracy 42.734%\n",
      "Epoch 4, Batch 630, LR 0.000099 Loss 12.934600, Accuracy 42.722%\n",
      "Epoch 4, Batch 631, LR 0.000099 Loss 12.934355, Accuracy 42.730%\n",
      "Epoch 4, Batch 632, LR 0.000099 Loss 12.932788, Accuracy 42.740%\n",
      "Epoch 4, Batch 633, LR 0.000099 Loss 12.932267, Accuracy 42.743%\n",
      "Epoch 4, Batch 634, LR 0.000099 Loss 12.932768, Accuracy 42.740%\n",
      "Epoch 4, Batch 635, LR 0.000099 Loss 12.932909, Accuracy 42.736%\n",
      "Epoch 4, Batch 636, LR 0.000099 Loss 12.932512, Accuracy 42.738%\n",
      "Epoch 4, Batch 637, LR 0.000099 Loss 12.933080, Accuracy 42.734%\n",
      "Epoch 4, Batch 638, LR 0.000099 Loss 12.932990, Accuracy 42.740%\n",
      "Epoch 4, Batch 639, LR 0.000099 Loss 12.932587, Accuracy 42.744%\n",
      "Epoch 4, Batch 640, LR 0.000099 Loss 12.932322, Accuracy 42.750%\n",
      "Epoch 4, Batch 641, LR 0.000099 Loss 12.931396, Accuracy 42.758%\n",
      "Epoch 4, Batch 642, LR 0.000099 Loss 12.929656, Accuracy 42.764%\n",
      "Epoch 4, Batch 643, LR 0.000099 Loss 12.929296, Accuracy 42.760%\n",
      "Epoch 4, Batch 644, LR 0.000099 Loss 12.928632, Accuracy 42.761%\n",
      "Epoch 4, Batch 645, LR 0.000099 Loss 12.928087, Accuracy 42.764%\n",
      "Epoch 4, Batch 646, LR 0.000099 Loss 12.927810, Accuracy 42.767%\n",
      "Epoch 4, Batch 647, LR 0.000099 Loss 12.926922, Accuracy 42.777%\n",
      "Epoch 4, Batch 648, LR 0.000099 Loss 12.927324, Accuracy 42.764%\n",
      "Epoch 4, Batch 649, LR 0.000099 Loss 12.926247, Accuracy 42.767%\n",
      "Epoch 4, Batch 650, LR 0.000099 Loss 12.925972, Accuracy 42.773%\n",
      "Epoch 4, Batch 651, LR 0.000099 Loss 12.925355, Accuracy 42.779%\n",
      "Epoch 4, Batch 652, LR 0.000099 Loss 12.924874, Accuracy 42.782%\n",
      "Epoch 4, Batch 653, LR 0.000099 Loss 12.925131, Accuracy 42.773%\n",
      "Epoch 4, Batch 654, LR 0.000099 Loss 12.924450, Accuracy 42.780%\n",
      "Epoch 4, Batch 655, LR 0.000099 Loss 12.924272, Accuracy 42.781%\n",
      "Epoch 4, Batch 656, LR 0.000099 Loss 12.923716, Accuracy 42.787%\n",
      "Epoch 4, Batch 657, LR 0.000099 Loss 12.923368, Accuracy 42.796%\n",
      "Epoch 4, Batch 658, LR 0.000099 Loss 12.922765, Accuracy 42.808%\n",
      "Epoch 4, Batch 659, LR 0.000099 Loss 12.922749, Accuracy 42.809%\n",
      "Epoch 4, Batch 660, LR 0.000099 Loss 12.921826, Accuracy 42.810%\n",
      "Epoch 4, Batch 661, LR 0.000099 Loss 12.921188, Accuracy 42.819%\n",
      "Epoch 4, Batch 662, LR 0.000099 Loss 12.920772, Accuracy 42.817%\n",
      "Epoch 4, Batch 663, LR 0.000099 Loss 12.920728, Accuracy 42.819%\n",
      "Epoch 4, Batch 664, LR 0.000099 Loss 12.920319, Accuracy 42.829%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 665, LR 0.000099 Loss 12.919653, Accuracy 42.837%\n",
      "Epoch 4, Batch 666, LR 0.000099 Loss 12.919079, Accuracy 42.843%\n",
      "Epoch 4, Batch 667, LR 0.000099 Loss 12.918172, Accuracy 42.855%\n",
      "Epoch 4, Batch 668, LR 0.000099 Loss 12.917627, Accuracy 42.867%\n",
      "Epoch 4, Batch 669, LR 0.000099 Loss 12.917935, Accuracy 42.869%\n",
      "Epoch 4, Batch 670, LR 0.000099 Loss 12.918244, Accuracy 42.867%\n",
      "Epoch 4, Batch 671, LR 0.000099 Loss 12.917581, Accuracy 42.871%\n",
      "Epoch 4, Batch 672, LR 0.000099 Loss 12.917080, Accuracy 42.871%\n",
      "Epoch 4, Batch 673, LR 0.000099 Loss 12.915986, Accuracy 42.876%\n",
      "Epoch 4, Batch 674, LR 0.000099 Loss 12.914721, Accuracy 42.886%\n",
      "Epoch 4, Batch 675, LR 0.000099 Loss 12.914898, Accuracy 42.888%\n",
      "Epoch 4, Batch 676, LR 0.000099 Loss 12.913746, Accuracy 42.895%\n",
      "Epoch 4, Batch 677, LR 0.000099 Loss 12.914692, Accuracy 42.891%\n",
      "Epoch 4, Batch 678, LR 0.000099 Loss 12.915582, Accuracy 42.886%\n",
      "Epoch 4, Batch 679, LR 0.000099 Loss 12.914253, Accuracy 42.904%\n",
      "Epoch 4, Batch 680, LR 0.000099 Loss 12.914528, Accuracy 42.895%\n",
      "Epoch 4, Batch 681, LR 0.000099 Loss 12.914012, Accuracy 42.903%\n",
      "Epoch 4, Batch 682, LR 0.000099 Loss 12.913235, Accuracy 42.911%\n",
      "Epoch 4, Batch 683, LR 0.000099 Loss 12.913128, Accuracy 42.920%\n",
      "Epoch 4, Batch 684, LR 0.000099 Loss 12.913308, Accuracy 42.918%\n",
      "Epoch 4, Batch 685, LR 0.000099 Loss 12.912490, Accuracy 42.921%\n",
      "Epoch 4, Batch 686, LR 0.000099 Loss 12.911845, Accuracy 42.928%\n",
      "Epoch 4, Batch 687, LR 0.000099 Loss 12.911452, Accuracy 42.938%\n",
      "Epoch 4, Batch 688, LR 0.000099 Loss 12.912178, Accuracy 42.936%\n",
      "Epoch 4, Batch 689, LR 0.000099 Loss 12.912137, Accuracy 42.940%\n",
      "Epoch 4, Batch 690, LR 0.000099 Loss 12.912062, Accuracy 42.944%\n",
      "Epoch 4, Batch 691, LR 0.000099 Loss 12.911292, Accuracy 42.954%\n",
      "Epoch 4, Batch 692, LR 0.000099 Loss 12.910794, Accuracy 42.963%\n",
      "Epoch 4, Batch 693, LR 0.000099 Loss 12.910911, Accuracy 42.961%\n",
      "Epoch 4, Batch 694, LR 0.000099 Loss 12.910319, Accuracy 42.961%\n",
      "Epoch 4, Batch 695, LR 0.000099 Loss 12.909666, Accuracy 42.969%\n",
      "Epoch 4, Batch 696, LR 0.000099 Loss 12.908560, Accuracy 42.983%\n",
      "Epoch 4, Batch 697, LR 0.000099 Loss 12.907607, Accuracy 42.982%\n",
      "Epoch 4, Batch 698, LR 0.000099 Loss 12.907412, Accuracy 42.982%\n",
      "Epoch 4, Batch 699, LR 0.000099 Loss 12.907217, Accuracy 42.986%\n",
      "Epoch 4, Batch 700, LR 0.000099 Loss 12.906581, Accuracy 42.987%\n",
      "Epoch 4, Batch 701, LR 0.000099 Loss 12.906198, Accuracy 42.994%\n",
      "Epoch 4, Batch 702, LR 0.000099 Loss 12.905669, Accuracy 43.000%\n",
      "Epoch 4, Batch 703, LR 0.000099 Loss 12.905765, Accuracy 42.993%\n",
      "Epoch 4, Batch 704, LR 0.000099 Loss 12.905327, Accuracy 43.000%\n",
      "Epoch 4, Batch 705, LR 0.000099 Loss 12.905179, Accuracy 43.004%\n",
      "Epoch 4, Batch 706, LR 0.000099 Loss 12.905260, Accuracy 43.007%\n",
      "Epoch 4, Batch 707, LR 0.000099 Loss 12.905860, Accuracy 43.003%\n",
      "Epoch 4, Batch 708, LR 0.000099 Loss 12.905103, Accuracy 43.014%\n",
      "Epoch 4, Batch 709, LR 0.000099 Loss 12.904805, Accuracy 43.016%\n",
      "Epoch 4, Batch 710, LR 0.000099 Loss 12.905346, Accuracy 43.012%\n",
      "Epoch 4, Batch 711, LR 0.000099 Loss 12.905808, Accuracy 43.002%\n",
      "Epoch 4, Batch 712, LR 0.000099 Loss 12.904911, Accuracy 43.010%\n",
      "Epoch 4, Batch 713, LR 0.000099 Loss 12.904672, Accuracy 43.017%\n",
      "Epoch 4, Batch 714, LR 0.000099 Loss 12.905476, Accuracy 43.011%\n",
      "Epoch 4, Batch 715, LR 0.000099 Loss 12.904249, Accuracy 43.019%\n",
      "Epoch 4, Batch 716, LR 0.000099 Loss 12.904215, Accuracy 43.027%\n",
      "Epoch 4, Batch 717, LR 0.000099 Loss 12.903823, Accuracy 43.023%\n",
      "Epoch 4, Batch 718, LR 0.000099 Loss 12.903558, Accuracy 43.024%\n",
      "Epoch 4, Batch 719, LR 0.000099 Loss 12.903763, Accuracy 43.031%\n",
      "Epoch 4, Batch 720, LR 0.000099 Loss 12.903775, Accuracy 43.040%\n",
      "Epoch 4, Batch 721, LR 0.000099 Loss 12.903583, Accuracy 43.045%\n",
      "Epoch 4, Batch 722, LR 0.000099 Loss 12.903451, Accuracy 43.041%\n",
      "Epoch 4, Batch 723, LR 0.000099 Loss 12.903268, Accuracy 43.047%\n",
      "Epoch 4, Batch 724, LR 0.000099 Loss 12.904290, Accuracy 43.039%\n",
      "Epoch 4, Batch 725, LR 0.000099 Loss 12.903705, Accuracy 43.047%\n",
      "Epoch 4, Batch 726, LR 0.000099 Loss 12.903205, Accuracy 43.048%\n",
      "Epoch 4, Batch 727, LR 0.000099 Loss 12.903727, Accuracy 43.043%\n",
      "Epoch 4, Batch 728, LR 0.000099 Loss 12.904871, Accuracy 43.036%\n",
      "Epoch 4, Batch 729, LR 0.000099 Loss 12.903582, Accuracy 43.048%\n",
      "Epoch 4, Batch 730, LR 0.000099 Loss 12.903205, Accuracy 43.044%\n",
      "Epoch 4, Batch 731, LR 0.000099 Loss 12.902626, Accuracy 43.052%\n",
      "Epoch 4, Batch 732, LR 0.000099 Loss 12.903493, Accuracy 43.045%\n",
      "Epoch 4, Batch 733, LR 0.000099 Loss 12.903251, Accuracy 43.052%\n",
      "Epoch 4, Batch 734, LR 0.000099 Loss 12.902939, Accuracy 43.059%\n",
      "Epoch 4, Batch 735, LR 0.000099 Loss 12.903528, Accuracy 43.054%\n",
      "Epoch 4, Batch 736, LR 0.000099 Loss 12.904447, Accuracy 43.043%\n",
      "Epoch 4, Batch 737, LR 0.000099 Loss 12.905299, Accuracy 43.030%\n",
      "Epoch 4, Batch 738, LR 0.000099 Loss 12.905233, Accuracy 43.029%\n",
      "Epoch 4, Batch 739, LR 0.000099 Loss 12.904505, Accuracy 43.034%\n",
      "Epoch 4, Batch 740, LR 0.000099 Loss 12.904167, Accuracy 43.041%\n",
      "Epoch 4, Batch 741, LR 0.000099 Loss 12.903580, Accuracy 43.047%\n",
      "Epoch 4, Batch 742, LR 0.000099 Loss 12.902400, Accuracy 43.055%\n",
      "Epoch 4, Batch 743, LR 0.000099 Loss 12.902296, Accuracy 43.061%\n",
      "Epoch 4, Batch 744, LR 0.000099 Loss 12.901035, Accuracy 43.073%\n",
      "Epoch 4, Batch 745, LR 0.000099 Loss 12.899160, Accuracy 43.086%\n",
      "Epoch 4, Batch 746, LR 0.000099 Loss 12.899589, Accuracy 43.081%\n",
      "Epoch 4, Batch 747, LR 0.000099 Loss 12.899278, Accuracy 43.080%\n",
      "Epoch 4, Batch 748, LR 0.000099 Loss 12.898364, Accuracy 43.090%\n",
      "Epoch 4, Batch 749, LR 0.000099 Loss 12.897685, Accuracy 43.101%\n",
      "Epoch 4, Batch 750, LR 0.000099 Loss 12.897388, Accuracy 43.101%\n",
      "Epoch 4, Batch 751, LR 0.000099 Loss 12.896786, Accuracy 43.107%\n",
      "Epoch 4, Batch 752, LR 0.000099 Loss 12.896533, Accuracy 43.100%\n",
      "Epoch 4, Batch 753, LR 0.000099 Loss 12.896442, Accuracy 43.108%\n",
      "Epoch 4, Batch 754, LR 0.000099 Loss 12.896259, Accuracy 43.107%\n",
      "Epoch 4, Batch 755, LR 0.000099 Loss 12.895870, Accuracy 43.104%\n",
      "Epoch 4, Batch 756, LR 0.000099 Loss 12.895017, Accuracy 43.112%\n",
      "Epoch 4, Batch 757, LR 0.000099 Loss 12.895051, Accuracy 43.105%\n",
      "Epoch 4, Batch 758, LR 0.000099 Loss 12.893518, Accuracy 43.123%\n",
      "Epoch 4, Batch 759, LR 0.000099 Loss 12.893770, Accuracy 43.126%\n",
      "Epoch 4, Batch 760, LR 0.000099 Loss 12.893318, Accuracy 43.130%\n",
      "Epoch 4, Batch 761, LR 0.000099 Loss 12.893324, Accuracy 43.133%\n",
      "Epoch 4, Batch 762, LR 0.000099 Loss 12.893251, Accuracy 43.132%\n",
      "Epoch 4, Batch 763, LR 0.000099 Loss 12.893185, Accuracy 43.130%\n",
      "Epoch 4, Batch 764, LR 0.000099 Loss 12.893366, Accuracy 43.133%\n",
      "Epoch 4, Batch 765, LR 0.000099 Loss 12.893601, Accuracy 43.138%\n",
      "Epoch 4, Batch 766, LR 0.000099 Loss 12.893041, Accuracy 43.142%\n",
      "Epoch 4, Batch 767, LR 0.000099 Loss 12.893065, Accuracy 43.138%\n",
      "Epoch 4, Batch 768, LR 0.000099 Loss 12.892309, Accuracy 43.150%\n",
      "Epoch 4, Batch 769, LR 0.000099 Loss 12.891203, Accuracy 43.161%\n",
      "Epoch 4, Batch 770, LR 0.000099 Loss 12.891746, Accuracy 43.154%\n",
      "Epoch 4, Batch 771, LR 0.000099 Loss 12.891738, Accuracy 43.148%\n",
      "Epoch 4, Batch 772, LR 0.000099 Loss 12.892279, Accuracy 43.140%\n",
      "Epoch 4, Batch 773, LR 0.000099 Loss 12.890790, Accuracy 43.150%\n",
      "Epoch 4, Batch 774, LR 0.000099 Loss 12.890649, Accuracy 43.146%\n",
      "Epoch 4, Batch 775, LR 0.000099 Loss 12.890879, Accuracy 43.138%\n",
      "Epoch 4, Batch 776, LR 0.000099 Loss 12.891066, Accuracy 43.138%\n",
      "Epoch 4, Batch 777, LR 0.000099 Loss 12.890621, Accuracy 43.138%\n",
      "Epoch 4, Batch 778, LR 0.000099 Loss 12.890877, Accuracy 43.138%\n",
      "Epoch 4, Batch 779, LR 0.000099 Loss 12.890541, Accuracy 43.138%\n",
      "Epoch 4, Batch 780, LR 0.000099 Loss 12.890003, Accuracy 43.145%\n",
      "Epoch 4, Batch 781, LR 0.000099 Loss 12.889999, Accuracy 43.152%\n",
      "Epoch 4, Batch 782, LR 0.000099 Loss 12.889250, Accuracy 43.161%\n",
      "Epoch 4, Batch 783, LR 0.000099 Loss 12.889173, Accuracy 43.166%\n",
      "Epoch 4, Batch 784, LR 0.000099 Loss 12.887512, Accuracy 43.174%\n",
      "Epoch 4, Batch 785, LR 0.000099 Loss 12.886677, Accuracy 43.184%\n",
      "Epoch 4, Batch 786, LR 0.000099 Loss 12.885852, Accuracy 43.190%\n",
      "Epoch 4, Batch 787, LR 0.000099 Loss 12.885058, Accuracy 43.195%\n",
      "Epoch 4, Batch 788, LR 0.000099 Loss 12.885155, Accuracy 43.193%\n",
      "Epoch 4, Batch 789, LR 0.000099 Loss 12.884969, Accuracy 43.194%\n",
      "Epoch 4, Batch 790, LR 0.000099 Loss 12.885303, Accuracy 43.185%\n",
      "Epoch 4, Batch 791, LR 0.000099 Loss 12.884597, Accuracy 43.192%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 792, LR 0.000099 Loss 12.883868, Accuracy 43.192%\n",
      "Epoch 4, Batch 793, LR 0.000099 Loss 12.882877, Accuracy 43.200%\n",
      "Epoch 4, Batch 794, LR 0.000099 Loss 12.882619, Accuracy 43.195%\n",
      "Epoch 4, Batch 795, LR 0.000099 Loss 12.882774, Accuracy 43.193%\n",
      "Epoch 4, Batch 796, LR 0.000099 Loss 12.882017, Accuracy 43.194%\n",
      "Epoch 4, Batch 797, LR 0.000099 Loss 12.881763, Accuracy 43.189%\n",
      "Epoch 4, Batch 798, LR 0.000099 Loss 12.882000, Accuracy 43.189%\n",
      "Epoch 4, Batch 799, LR 0.000099 Loss 12.881231, Accuracy 43.190%\n",
      "Epoch 4, Batch 800, LR 0.000099 Loss 12.881145, Accuracy 43.191%\n",
      "Epoch 4, Batch 801, LR 0.000099 Loss 12.880214, Accuracy 43.199%\n",
      "Epoch 4, Batch 802, LR 0.000099 Loss 12.880271, Accuracy 43.197%\n",
      "Epoch 4, Batch 803, LR 0.000099 Loss 12.880290, Accuracy 43.197%\n",
      "Epoch 4, Batch 804, LR 0.000099 Loss 12.880526, Accuracy 43.195%\n",
      "Epoch 4, Batch 805, LR 0.000099 Loss 12.879773, Accuracy 43.202%\n",
      "Epoch 4, Batch 806, LR 0.000099 Loss 12.879272, Accuracy 43.203%\n",
      "Epoch 4, Batch 807, LR 0.000099 Loss 12.879087, Accuracy 43.203%\n",
      "Epoch 4, Batch 808, LR 0.000099 Loss 12.878778, Accuracy 43.206%\n",
      "Epoch 4, Batch 809, LR 0.000099 Loss 12.877929, Accuracy 43.214%\n",
      "Epoch 4, Batch 810, LR 0.000099 Loss 12.876850, Accuracy 43.224%\n",
      "Epoch 4, Batch 811, LR 0.000099 Loss 12.877270, Accuracy 43.219%\n",
      "Epoch 4, Batch 812, LR 0.000099 Loss 12.876716, Accuracy 43.218%\n",
      "Epoch 4, Batch 813, LR 0.000099 Loss 12.876460, Accuracy 43.215%\n",
      "Epoch 4, Batch 814, LR 0.000099 Loss 12.875776, Accuracy 43.216%\n",
      "Epoch 4, Batch 815, LR 0.000099 Loss 12.875271, Accuracy 43.227%\n",
      "Epoch 4, Batch 816, LR 0.000099 Loss 12.875190, Accuracy 43.222%\n",
      "Epoch 4, Batch 817, LR 0.000099 Loss 12.874106, Accuracy 43.224%\n",
      "Epoch 4, Batch 818, LR 0.000099 Loss 12.873608, Accuracy 43.238%\n",
      "Epoch 4, Batch 819, LR 0.000099 Loss 12.873120, Accuracy 43.243%\n",
      "Epoch 4, Batch 820, LR 0.000099 Loss 12.873395, Accuracy 43.235%\n",
      "Epoch 4, Batch 821, LR 0.000099 Loss 12.873294, Accuracy 43.232%\n",
      "Epoch 4, Batch 822, LR 0.000099 Loss 12.873760, Accuracy 43.229%\n",
      "Epoch 4, Batch 823, LR 0.000099 Loss 12.873072, Accuracy 43.234%\n",
      "Epoch 4, Batch 824, LR 0.000099 Loss 12.872061, Accuracy 43.238%\n",
      "Epoch 4, Batch 825, LR 0.000099 Loss 12.872501, Accuracy 43.235%\n",
      "Epoch 4, Batch 826, LR 0.000099 Loss 12.870924, Accuracy 43.249%\n",
      "Epoch 4, Batch 827, LR 0.000099 Loss 12.870406, Accuracy 43.256%\n",
      "Epoch 4, Batch 828, LR 0.000099 Loss 12.869766, Accuracy 43.258%\n",
      "Epoch 4, Batch 829, LR 0.000099 Loss 12.869812, Accuracy 43.265%\n",
      "Epoch 4, Batch 830, LR 0.000099 Loss 12.869535, Accuracy 43.262%\n",
      "Epoch 4, Batch 831, LR 0.000099 Loss 12.868671, Accuracy 43.267%\n",
      "Epoch 4, Batch 832, LR 0.000099 Loss 12.867733, Accuracy 43.273%\n",
      "Epoch 4, Batch 833, LR 0.000099 Loss 12.867301, Accuracy 43.271%\n",
      "Epoch 4, Batch 834, LR 0.000099 Loss 12.866740, Accuracy 43.269%\n",
      "Epoch 4, Batch 835, LR 0.000099 Loss 12.866271, Accuracy 43.272%\n",
      "Epoch 4, Batch 836, LR 0.000099 Loss 12.865946, Accuracy 43.269%\n",
      "Epoch 4, Batch 837, LR 0.000099 Loss 12.865239, Accuracy 43.274%\n",
      "Epoch 4, Batch 838, LR 0.000099 Loss 12.864190, Accuracy 43.285%\n",
      "Epoch 4, Batch 839, LR 0.000099 Loss 12.863944, Accuracy 43.280%\n",
      "Epoch 4, Batch 840, LR 0.000099 Loss 12.864187, Accuracy 43.271%\n",
      "Epoch 4, Batch 841, LR 0.000099 Loss 12.864782, Accuracy 43.267%\n",
      "Epoch 4, Batch 842, LR 0.000099 Loss 12.864639, Accuracy 43.265%\n",
      "Epoch 4, Batch 843, LR 0.000099 Loss 12.863931, Accuracy 43.267%\n",
      "Epoch 4, Batch 844, LR 0.000099 Loss 12.863451, Accuracy 43.270%\n",
      "Epoch 4, Batch 845, LR 0.000099 Loss 12.862692, Accuracy 43.275%\n",
      "Epoch 4, Batch 846, LR 0.000099 Loss 12.862030, Accuracy 43.278%\n",
      "Epoch 4, Batch 847, LR 0.000099 Loss 12.861041, Accuracy 43.293%\n",
      "Epoch 4, Batch 848, LR 0.000099 Loss 12.860641, Accuracy 43.298%\n",
      "Epoch 4, Batch 849, LR 0.000099 Loss 12.860585, Accuracy 43.294%\n",
      "Epoch 4, Batch 850, LR 0.000099 Loss 12.860513, Accuracy 43.290%\n",
      "Epoch 4, Batch 851, LR 0.000099 Loss 12.860110, Accuracy 43.295%\n",
      "Epoch 4, Batch 852, LR 0.000099 Loss 12.860088, Accuracy 43.292%\n",
      "Epoch 4, Batch 853, LR 0.000099 Loss 12.859377, Accuracy 43.292%\n",
      "Epoch 4, Batch 854, LR 0.000099 Loss 12.859645, Accuracy 43.293%\n",
      "Epoch 4, Batch 855, LR 0.000099 Loss 12.859605, Accuracy 43.297%\n",
      "Epoch 4, Batch 856, LR 0.000099 Loss 12.859512, Accuracy 43.296%\n",
      "Epoch 4, Batch 857, LR 0.000099 Loss 12.859314, Accuracy 43.304%\n",
      "Epoch 4, Batch 858, LR 0.000099 Loss 12.859248, Accuracy 43.303%\n",
      "Epoch 4, Batch 859, LR 0.000099 Loss 12.858312, Accuracy 43.313%\n",
      "Epoch 4, Batch 860, LR 0.000099 Loss 12.857630, Accuracy 43.313%\n",
      "Epoch 4, Batch 861, LR 0.000099 Loss 12.857002, Accuracy 43.315%\n",
      "Epoch 4, Batch 862, LR 0.000099 Loss 12.856530, Accuracy 43.318%\n",
      "Epoch 4, Batch 863, LR 0.000099 Loss 12.856258, Accuracy 43.321%\n",
      "Epoch 4, Batch 864, LR 0.000099 Loss 12.855630, Accuracy 43.326%\n",
      "Epoch 4, Batch 865, LR 0.000099 Loss 12.855174, Accuracy 43.326%\n",
      "Epoch 4, Batch 866, LR 0.000099 Loss 12.855060, Accuracy 43.325%\n",
      "Epoch 4, Batch 867, LR 0.000099 Loss 12.855239, Accuracy 43.326%\n",
      "Epoch 4, Batch 868, LR 0.000099 Loss 12.855377, Accuracy 43.319%\n",
      "Epoch 4, Batch 869, LR 0.000099 Loss 12.855105, Accuracy 43.321%\n",
      "Epoch 4, Batch 870, LR 0.000099 Loss 12.854225, Accuracy 43.331%\n",
      "Epoch 4, Batch 871, LR 0.000099 Loss 12.853734, Accuracy 43.337%\n",
      "Epoch 4, Batch 872, LR 0.000099 Loss 12.853795, Accuracy 43.338%\n",
      "Epoch 4, Batch 873, LR 0.000099 Loss 12.852982, Accuracy 43.343%\n",
      "Epoch 4, Batch 874, LR 0.000099 Loss 12.851882, Accuracy 43.353%\n",
      "Epoch 4, Batch 875, LR 0.000099 Loss 12.851161, Accuracy 43.362%\n",
      "Epoch 4, Batch 876, LR 0.000099 Loss 12.850647, Accuracy 43.363%\n",
      "Epoch 4, Batch 877, LR 0.000099 Loss 12.850094, Accuracy 43.370%\n",
      "Epoch 4, Batch 878, LR 0.000099 Loss 12.850867, Accuracy 43.358%\n",
      "Epoch 4, Batch 879, LR 0.000099 Loss 12.851525, Accuracy 43.352%\n",
      "Epoch 4, Batch 880, LR 0.000099 Loss 12.851579, Accuracy 43.350%\n",
      "Epoch 4, Batch 881, LR 0.000099 Loss 12.851543, Accuracy 43.353%\n",
      "Epoch 4, Batch 882, LR 0.000099 Loss 12.851780, Accuracy 43.348%\n",
      "Epoch 4, Batch 883, LR 0.000099 Loss 12.850640, Accuracy 43.353%\n",
      "Epoch 4, Batch 884, LR 0.000099 Loss 12.850041, Accuracy 43.361%\n",
      "Epoch 4, Batch 885, LR 0.000099 Loss 12.849382, Accuracy 43.368%\n",
      "Epoch 4, Batch 886, LR 0.000099 Loss 12.848652, Accuracy 43.377%\n",
      "Epoch 4, Batch 887, LR 0.000099 Loss 12.847506, Accuracy 43.388%\n",
      "Epoch 4, Batch 888, LR 0.000099 Loss 12.847653, Accuracy 43.389%\n",
      "Epoch 4, Batch 889, LR 0.000099 Loss 12.847341, Accuracy 43.392%\n",
      "Epoch 4, Batch 890, LR 0.000099 Loss 12.847709, Accuracy 43.392%\n",
      "Epoch 4, Batch 891, LR 0.000099 Loss 12.846981, Accuracy 43.397%\n",
      "Epoch 4, Batch 892, LR 0.000099 Loss 12.846366, Accuracy 43.408%\n",
      "Epoch 4, Batch 893, LR 0.000099 Loss 12.845108, Accuracy 43.418%\n",
      "Epoch 4, Batch 894, LR 0.000099 Loss 12.844408, Accuracy 43.425%\n",
      "Epoch 4, Batch 895, LR 0.000099 Loss 12.844083, Accuracy 43.427%\n",
      "Epoch 4, Batch 896, LR 0.000099 Loss 12.843772, Accuracy 43.425%\n",
      "Epoch 4, Batch 897, LR 0.000099 Loss 12.843224, Accuracy 43.435%\n",
      "Epoch 4, Batch 898, LR 0.000099 Loss 12.842795, Accuracy 43.437%\n",
      "Epoch 4, Batch 899, LR 0.000099 Loss 12.842228, Accuracy 43.443%\n",
      "Epoch 4, Batch 900, LR 0.000099 Loss 12.841181, Accuracy 43.456%\n",
      "Epoch 4, Batch 901, LR 0.000099 Loss 12.840539, Accuracy 43.460%\n",
      "Epoch 4, Batch 902, LR 0.000099 Loss 12.840007, Accuracy 43.464%\n",
      "Epoch 4, Batch 903, LR 0.000099 Loss 12.840154, Accuracy 43.463%\n",
      "Epoch 4, Batch 904, LR 0.000099 Loss 12.840170, Accuracy 43.466%\n",
      "Epoch 4, Batch 905, LR 0.000099 Loss 12.840274, Accuracy 43.460%\n",
      "Epoch 4, Batch 906, LR 0.000099 Loss 12.839609, Accuracy 43.465%\n",
      "Epoch 4, Batch 907, LR 0.000099 Loss 12.838908, Accuracy 43.473%\n",
      "Epoch 4, Batch 908, LR 0.000099 Loss 12.838197, Accuracy 43.477%\n",
      "Epoch 4, Batch 909, LR 0.000099 Loss 12.837542, Accuracy 43.484%\n",
      "Epoch 4, Batch 910, LR 0.000099 Loss 12.837558, Accuracy 43.483%\n",
      "Epoch 4, Batch 911, LR 0.000099 Loss 12.837028, Accuracy 43.487%\n",
      "Epoch 4, Batch 912, LR 0.000099 Loss 12.836123, Accuracy 43.489%\n",
      "Epoch 4, Batch 913, LR 0.000099 Loss 12.836044, Accuracy 43.492%\n",
      "Epoch 4, Batch 914, LR 0.000099 Loss 12.835680, Accuracy 43.493%\n",
      "Epoch 4, Batch 915, LR 0.000099 Loss 12.834879, Accuracy 43.502%\n",
      "Epoch 4, Batch 916, LR 0.000099 Loss 12.833893, Accuracy 43.506%\n",
      "Epoch 4, Batch 917, LR 0.000099 Loss 12.833677, Accuracy 43.508%\n",
      "Epoch 4, Batch 918, LR 0.000099 Loss 12.832967, Accuracy 43.512%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 919, LR 0.000099 Loss 12.831846, Accuracy 43.520%\n",
      "Epoch 4, Batch 920, LR 0.000099 Loss 12.831303, Accuracy 43.519%\n",
      "Epoch 4, Batch 921, LR 0.000099 Loss 12.831280, Accuracy 43.513%\n",
      "Epoch 4, Batch 922, LR 0.000099 Loss 12.830824, Accuracy 43.515%\n",
      "Epoch 4, Batch 923, LR 0.000099 Loss 12.829393, Accuracy 43.528%\n",
      "Epoch 4, Batch 924, LR 0.000099 Loss 12.828593, Accuracy 43.532%\n",
      "Epoch 4, Batch 925, LR 0.000099 Loss 12.828443, Accuracy 43.531%\n",
      "Epoch 4, Batch 926, LR 0.000099 Loss 12.828092, Accuracy 43.533%\n",
      "Epoch 4, Batch 927, LR 0.000099 Loss 12.827920, Accuracy 43.538%\n",
      "Epoch 4, Batch 928, LR 0.000099 Loss 12.828344, Accuracy 43.531%\n",
      "Epoch 4, Batch 929, LR 0.000099 Loss 12.828068, Accuracy 43.533%\n",
      "Epoch 4, Batch 930, LR 0.000099 Loss 12.827040, Accuracy 43.545%\n",
      "Epoch 4, Batch 931, LR 0.000099 Loss 12.825827, Accuracy 43.554%\n",
      "Epoch 4, Batch 932, LR 0.000099 Loss 12.825361, Accuracy 43.558%\n",
      "Epoch 4, Batch 933, LR 0.000099 Loss 12.824497, Accuracy 43.561%\n",
      "Epoch 4, Batch 934, LR 0.000099 Loss 12.824112, Accuracy 43.570%\n",
      "Epoch 4, Batch 935, LR 0.000099 Loss 12.823804, Accuracy 43.566%\n",
      "Epoch 4, Batch 936, LR 0.000099 Loss 12.823451, Accuracy 43.571%\n",
      "Epoch 4, Batch 937, LR 0.000099 Loss 12.822418, Accuracy 43.578%\n",
      "Epoch 4, Batch 938, LR 0.000099 Loss 12.822812, Accuracy 43.578%\n",
      "Epoch 4, Batch 939, LR 0.000099 Loss 12.822110, Accuracy 43.584%\n",
      "Epoch 4, Batch 940, LR 0.000099 Loss 12.821950, Accuracy 43.587%\n",
      "Epoch 4, Batch 941, LR 0.000099 Loss 12.821452, Accuracy 43.593%\n",
      "Epoch 4, Batch 942, LR 0.000099 Loss 12.821030, Accuracy 43.598%\n",
      "Epoch 4, Batch 943, LR 0.000099 Loss 12.819682, Accuracy 43.610%\n",
      "Epoch 4, Batch 944, LR 0.000099 Loss 12.819772, Accuracy 43.604%\n",
      "Epoch 4, Batch 945, LR 0.000099 Loss 12.819993, Accuracy 43.601%\n",
      "Epoch 4, Batch 946, LR 0.000099 Loss 12.819584, Accuracy 43.601%\n",
      "Epoch 4, Batch 947, LR 0.000099 Loss 12.819471, Accuracy 43.606%\n",
      "Epoch 4, Batch 948, LR 0.000099 Loss 12.819025, Accuracy 43.610%\n",
      "Epoch 4, Batch 949, LR 0.000099 Loss 12.819415, Accuracy 43.607%\n",
      "Epoch 4, Batch 950, LR 0.000099 Loss 12.818313, Accuracy 43.610%\n",
      "Epoch 4, Batch 951, LR 0.000099 Loss 12.818301, Accuracy 43.609%\n",
      "Epoch 4, Batch 952, LR 0.000099 Loss 12.817936, Accuracy 43.616%\n",
      "Epoch 4, Batch 953, LR 0.000099 Loss 12.817373, Accuracy 43.620%\n",
      "Epoch 4, Batch 954, LR 0.000099 Loss 12.817407, Accuracy 43.626%\n",
      "Epoch 4, Batch 955, LR 0.000099 Loss 12.817348, Accuracy 43.627%\n",
      "Epoch 4, Batch 956, LR 0.000099 Loss 12.816966, Accuracy 43.631%\n",
      "Epoch 4, Batch 957, LR 0.000099 Loss 12.816217, Accuracy 43.635%\n",
      "Epoch 4, Batch 958, LR 0.000099 Loss 12.815840, Accuracy 43.636%\n",
      "Epoch 4, Batch 959, LR 0.000099 Loss 12.816286, Accuracy 43.627%\n",
      "Epoch 4, Batch 960, LR 0.000099 Loss 12.816236, Accuracy 43.627%\n",
      "Epoch 4, Batch 961, LR 0.000099 Loss 12.815745, Accuracy 43.628%\n",
      "Epoch 4, Batch 962, LR 0.000099 Loss 12.815894, Accuracy 43.624%\n",
      "Epoch 4, Batch 963, LR 0.000099 Loss 12.815852, Accuracy 43.622%\n",
      "Epoch 4, Batch 964, LR 0.000099 Loss 12.815725, Accuracy 43.622%\n",
      "Epoch 4, Batch 965, LR 0.000099 Loss 12.814964, Accuracy 43.629%\n",
      "Epoch 4, Batch 966, LR 0.000099 Loss 12.814493, Accuracy 43.637%\n",
      "Epoch 4, Batch 967, LR 0.000099 Loss 12.813864, Accuracy 43.641%\n",
      "Epoch 4, Batch 968, LR 0.000099 Loss 12.812995, Accuracy 43.643%\n",
      "Epoch 4, Batch 969, LR 0.000099 Loss 12.812235, Accuracy 43.649%\n",
      "Epoch 4, Batch 970, LR 0.000099 Loss 12.811527, Accuracy 43.653%\n",
      "Epoch 4, Batch 971, LR 0.000099 Loss 12.811159, Accuracy 43.661%\n",
      "Epoch 4, Batch 972, LR 0.000099 Loss 12.811010, Accuracy 43.661%\n",
      "Epoch 4, Batch 973, LR 0.000099 Loss 12.810801, Accuracy 43.661%\n",
      "Epoch 4, Batch 974, LR 0.000099 Loss 12.810217, Accuracy 43.663%\n",
      "Epoch 4, Batch 975, LR 0.000099 Loss 12.810150, Accuracy 43.663%\n",
      "Epoch 4, Batch 976, LR 0.000099 Loss 12.809548, Accuracy 43.663%\n",
      "Epoch 4, Batch 977, LR 0.000099 Loss 12.809526, Accuracy 43.668%\n",
      "Epoch 4, Batch 978, LR 0.000099 Loss 12.809364, Accuracy 43.669%\n",
      "Epoch 4, Batch 979, LR 0.000099 Loss 12.809087, Accuracy 43.668%\n",
      "Epoch 4, Batch 980, LR 0.000099 Loss 12.808538, Accuracy 43.671%\n",
      "Epoch 4, Batch 981, LR 0.000099 Loss 12.808000, Accuracy 43.675%\n",
      "Epoch 4, Batch 982, LR 0.000099 Loss 12.807879, Accuracy 43.681%\n",
      "Epoch 4, Batch 983, LR 0.000099 Loss 12.807597, Accuracy 43.686%\n",
      "Epoch 4, Batch 984, LR 0.000099 Loss 12.807258, Accuracy 43.684%\n",
      "Epoch 4, Batch 985, LR 0.000099 Loss 12.806253, Accuracy 43.692%\n",
      "Epoch 4, Batch 986, LR 0.000099 Loss 12.804985, Accuracy 43.702%\n",
      "Epoch 4, Batch 987, LR 0.000099 Loss 12.803982, Accuracy 43.716%\n",
      "Epoch 4, Batch 988, LR 0.000099 Loss 12.803720, Accuracy 43.714%\n",
      "Epoch 4, Batch 989, LR 0.000099 Loss 12.803586, Accuracy 43.715%\n",
      "Epoch 4, Batch 990, LR 0.000099 Loss 12.803048, Accuracy 43.721%\n",
      "Epoch 4, Batch 991, LR 0.000099 Loss 12.801957, Accuracy 43.730%\n",
      "Epoch 4, Batch 992, LR 0.000099 Loss 12.802251, Accuracy 43.730%\n",
      "Epoch 4, Batch 993, LR 0.000099 Loss 12.802200, Accuracy 43.740%\n",
      "Epoch 4, Batch 994, LR 0.000099 Loss 12.801917, Accuracy 43.746%\n",
      "Epoch 4, Batch 995, LR 0.000099 Loss 12.800787, Accuracy 43.752%\n",
      "Epoch 4, Batch 996, LR 0.000099 Loss 12.800086, Accuracy 43.752%\n",
      "Epoch 4, Batch 997, LR 0.000099 Loss 12.798466, Accuracy 43.768%\n",
      "Epoch 4, Batch 998, LR 0.000099 Loss 12.798306, Accuracy 43.768%\n",
      "Epoch 4, Batch 999, LR 0.000099 Loss 12.798073, Accuracy 43.773%\n",
      "Epoch 4, Batch 1000, LR 0.000099 Loss 12.797879, Accuracy 43.774%\n",
      "Epoch 4, Batch 1001, LR 0.000099 Loss 12.797667, Accuracy 43.774%\n",
      "Epoch 4, Batch 1002, LR 0.000099 Loss 12.797023, Accuracy 43.777%\n",
      "Epoch 4, Batch 1003, LR 0.000099 Loss 12.796792, Accuracy 43.783%\n",
      "Epoch 4, Batch 1004, LR 0.000099 Loss 12.796265, Accuracy 43.781%\n",
      "Epoch 4, Batch 1005, LR 0.000099 Loss 12.796115, Accuracy 43.782%\n",
      "Epoch 4, Batch 1006, LR 0.000099 Loss 12.795390, Accuracy 43.783%\n",
      "Epoch 4, Batch 1007, LR 0.000099 Loss 12.794308, Accuracy 43.793%\n",
      "Epoch 4, Batch 1008, LR 0.000099 Loss 12.793585, Accuracy 43.796%\n",
      "Epoch 4, Batch 1009, LR 0.000099 Loss 12.793338, Accuracy 43.801%\n",
      "Epoch 4, Batch 1010, LR 0.000099 Loss 12.792696, Accuracy 43.802%\n",
      "Epoch 4, Batch 1011, LR 0.000099 Loss 12.791779, Accuracy 43.811%\n",
      "Epoch 4, Batch 1012, LR 0.000099 Loss 12.791966, Accuracy 43.812%\n",
      "Epoch 4, Batch 1013, LR 0.000099 Loss 12.791734, Accuracy 43.819%\n",
      "Epoch 4, Batch 1014, LR 0.000099 Loss 12.791633, Accuracy 43.825%\n",
      "Epoch 4, Batch 1015, LR 0.000099 Loss 12.791416, Accuracy 43.830%\n",
      "Epoch 4, Batch 1016, LR 0.000098 Loss 12.791473, Accuracy 43.824%\n",
      "Epoch 4, Batch 1017, LR 0.000098 Loss 12.790174, Accuracy 43.835%\n",
      "Epoch 4, Batch 1018, LR 0.000098 Loss 12.789598, Accuracy 43.840%\n",
      "Epoch 4, Batch 1019, LR 0.000098 Loss 12.789231, Accuracy 43.845%\n",
      "Epoch 4, Batch 1020, LR 0.000098 Loss 12.789547, Accuracy 43.847%\n",
      "Epoch 4, Batch 1021, LR 0.000098 Loss 12.788965, Accuracy 43.854%\n",
      "Epoch 4, Batch 1022, LR 0.000098 Loss 12.788769, Accuracy 43.853%\n",
      "Epoch 4, Batch 1023, LR 0.000098 Loss 12.788134, Accuracy 43.862%\n",
      "Epoch 4, Batch 1024, LR 0.000098 Loss 12.787907, Accuracy 43.861%\n",
      "Epoch 4, Batch 1025, LR 0.000098 Loss 12.787961, Accuracy 43.861%\n",
      "Epoch 4, Batch 1026, LR 0.000098 Loss 12.787194, Accuracy 43.870%\n",
      "Epoch 4, Batch 1027, LR 0.000098 Loss 12.786861, Accuracy 43.873%\n",
      "Epoch 4, Batch 1028, LR 0.000098 Loss 12.785885, Accuracy 43.878%\n",
      "Epoch 4, Batch 1029, LR 0.000098 Loss 12.786076, Accuracy 43.878%\n",
      "Epoch 4, Batch 1030, LR 0.000098 Loss 12.785656, Accuracy 43.874%\n",
      "Epoch 4, Batch 1031, LR 0.000098 Loss 12.785705, Accuracy 43.874%\n",
      "Epoch 4, Batch 1032, LR 0.000098 Loss 12.785497, Accuracy 43.874%\n",
      "Epoch 4, Batch 1033, LR 0.000098 Loss 12.785371, Accuracy 43.871%\n",
      "Epoch 4, Batch 1034, LR 0.000098 Loss 12.785474, Accuracy 43.872%\n",
      "Epoch 4, Batch 1035, LR 0.000098 Loss 12.785548, Accuracy 43.874%\n",
      "Epoch 4, Batch 1036, LR 0.000098 Loss 12.785309, Accuracy 43.874%\n",
      "Epoch 4, Batch 1037, LR 0.000098 Loss 12.785246, Accuracy 43.873%\n",
      "Epoch 4, Batch 1038, LR 0.000098 Loss 12.784865, Accuracy 43.876%\n",
      "Epoch 4, Batch 1039, LR 0.000098 Loss 12.784096, Accuracy 43.883%\n",
      "Epoch 4, Batch 1040, LR 0.000098 Loss 12.784077, Accuracy 43.881%\n",
      "Epoch 4, Batch 1041, LR 0.000098 Loss 12.783777, Accuracy 43.887%\n",
      "Epoch 4, Batch 1042, LR 0.000098 Loss 12.783623, Accuracy 43.883%\n",
      "Epoch 4, Batch 1043, LR 0.000098 Loss 12.783007, Accuracy 43.887%\n",
      "Epoch 4, Batch 1044, LR 0.000098 Loss 12.782308, Accuracy 43.890%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Batch 1045, LR 0.000098 Loss 12.781872, Accuracy 43.894%\n",
      "Epoch 4, Batch 1046, LR 0.000098 Loss 12.781267, Accuracy 43.898%\n",
      "Epoch 4, Batch 1047, LR 0.000098 Loss 12.780856, Accuracy 43.905%\n",
      "Epoch 4, Loss (train set) 12.780856, Accuracy (train set) 43.905%\n",
      "Epoch 4, Accuracy (validation set) 25.581%\n",
      "Epoch 4, EER (test set) 8.000%\n",
      "Epoch 5, Batch 1, LR 0.000098 Loss 11.774927, Accuracy 54.688%\n",
      "Epoch 5, Batch 2, LR 0.000098 Loss 11.740908, Accuracy 54.297%\n",
      "Epoch 5, Batch 3, LR 0.000098 Loss 12.236637, Accuracy 51.042%\n",
      "Epoch 5, Batch 4, LR 0.000098 Loss 12.125268, Accuracy 50.195%\n",
      "Epoch 5, Batch 5, LR 0.000098 Loss 12.262029, Accuracy 49.219%\n",
      "Epoch 5, Batch 6, LR 0.000098 Loss 12.223421, Accuracy 48.958%\n",
      "Epoch 5, Batch 7, LR 0.000098 Loss 12.211614, Accuracy 48.661%\n",
      "Epoch 5, Batch 8, LR 0.000098 Loss 12.210759, Accuracy 48.535%\n",
      "Epoch 5, Batch 9, LR 0.000098 Loss 12.253386, Accuracy 48.003%\n",
      "Epoch 5, Batch 10, LR 0.000098 Loss 12.225994, Accuracy 48.828%\n",
      "Epoch 5, Batch 11, LR 0.000098 Loss 12.266497, Accuracy 48.011%\n",
      "Epoch 5, Batch 12, LR 0.000098 Loss 12.234430, Accuracy 48.177%\n",
      "Epoch 5, Batch 13, LR 0.000098 Loss 12.201710, Accuracy 48.798%\n",
      "Epoch 5, Batch 14, LR 0.000098 Loss 12.200402, Accuracy 48.996%\n",
      "Epoch 5, Batch 15, LR 0.000098 Loss 12.168067, Accuracy 49.167%\n",
      "Epoch 5, Batch 16, LR 0.000098 Loss 12.159176, Accuracy 49.414%\n",
      "Epoch 5, Batch 17, LR 0.000098 Loss 12.183119, Accuracy 48.943%\n",
      "Epoch 5, Batch 18, LR 0.000098 Loss 12.201434, Accuracy 49.002%\n",
      "Epoch 5, Batch 19, LR 0.000098 Loss 12.177758, Accuracy 49.219%\n",
      "Epoch 5, Batch 20, LR 0.000098 Loss 12.200462, Accuracy 48.945%\n",
      "Epoch 5, Batch 21, LR 0.000098 Loss 12.181706, Accuracy 49.293%\n",
      "Epoch 5, Batch 22, LR 0.000098 Loss 12.202824, Accuracy 49.219%\n",
      "Epoch 5, Batch 23, LR 0.000098 Loss 12.192496, Accuracy 49.015%\n",
      "Epoch 5, Batch 24, LR 0.000098 Loss 12.193137, Accuracy 49.089%\n",
      "Epoch 5, Batch 25, LR 0.000098 Loss 12.206904, Accuracy 49.125%\n",
      "Epoch 5, Batch 26, LR 0.000098 Loss 12.207536, Accuracy 49.219%\n",
      "Epoch 5, Batch 27, LR 0.000098 Loss 12.216121, Accuracy 49.132%\n",
      "Epoch 5, Batch 28, LR 0.000098 Loss 12.223885, Accuracy 49.135%\n",
      "Epoch 5, Batch 29, LR 0.000098 Loss 12.239452, Accuracy 49.138%\n",
      "Epoch 5, Batch 30, LR 0.000098 Loss 12.231891, Accuracy 49.193%\n",
      "Epoch 5, Batch 31, LR 0.000098 Loss 12.260631, Accuracy 48.891%\n",
      "Epoch 5, Batch 32, LR 0.000098 Loss 12.256514, Accuracy 48.999%\n",
      "Epoch 5, Batch 33, LR 0.000098 Loss 12.249535, Accuracy 49.077%\n",
      "Epoch 5, Batch 34, LR 0.000098 Loss 12.260895, Accuracy 49.173%\n",
      "Epoch 5, Batch 35, LR 0.000098 Loss 12.261335, Accuracy 49.085%\n",
      "Epoch 5, Batch 36, LR 0.000098 Loss 12.279346, Accuracy 48.828%\n",
      "Epoch 5, Batch 37, LR 0.000098 Loss 12.263513, Accuracy 48.881%\n",
      "Epoch 5, Batch 38, LR 0.000098 Loss 12.260698, Accuracy 48.828%\n",
      "Epoch 5, Batch 39, LR 0.000098 Loss 12.284180, Accuracy 48.538%\n",
      "Epoch 5, Batch 40, LR 0.000098 Loss 12.283046, Accuracy 48.457%\n",
      "Epoch 5, Batch 41, LR 0.000098 Loss 12.282960, Accuracy 48.514%\n",
      "Epoch 5, Batch 42, LR 0.000098 Loss 12.273137, Accuracy 48.698%\n",
      "Epoch 5, Batch 43, LR 0.000098 Loss 12.256485, Accuracy 49.001%\n",
      "Epoch 5, Batch 44, LR 0.000098 Loss 12.273854, Accuracy 48.846%\n",
      "Epoch 5, Batch 45, LR 0.000098 Loss 12.265719, Accuracy 48.889%\n",
      "Epoch 5, Batch 46, LR 0.000098 Loss 12.247787, Accuracy 49.100%\n",
      "Epoch 5, Batch 47, LR 0.000098 Loss 12.241883, Accuracy 49.036%\n",
      "Epoch 5, Batch 48, LR 0.000098 Loss 12.245117, Accuracy 48.991%\n",
      "Epoch 5, Batch 49, LR 0.000098 Loss 12.245507, Accuracy 48.820%\n",
      "Epoch 5, Batch 50, LR 0.000098 Loss 12.236110, Accuracy 48.812%\n",
      "Epoch 5, Batch 51, LR 0.000098 Loss 12.234782, Accuracy 48.713%\n",
      "Epoch 5, Batch 52, LR 0.000098 Loss 12.231463, Accuracy 48.723%\n",
      "Epoch 5, Batch 53, LR 0.000098 Loss 12.238456, Accuracy 48.673%\n",
      "Epoch 5, Batch 54, LR 0.000098 Loss 12.237152, Accuracy 48.553%\n",
      "Epoch 5, Batch 55, LR 0.000098 Loss 12.249948, Accuracy 48.352%\n",
      "Epoch 5, Batch 56, LR 0.000098 Loss 12.253196, Accuracy 48.354%\n",
      "Epoch 5, Batch 57, LR 0.000098 Loss 12.248748, Accuracy 48.342%\n",
      "Epoch 5, Batch 58, LR 0.000098 Loss 12.248827, Accuracy 48.330%\n",
      "Epoch 5, Batch 59, LR 0.000098 Loss 12.243426, Accuracy 48.292%\n",
      "Epoch 5, Batch 60, LR 0.000098 Loss 12.236823, Accuracy 48.385%\n",
      "Epoch 5, Batch 61, LR 0.000098 Loss 12.232874, Accuracy 48.425%\n",
      "Epoch 5, Batch 62, LR 0.000098 Loss 12.222487, Accuracy 48.463%\n",
      "Epoch 5, Batch 63, LR 0.000098 Loss 12.213957, Accuracy 48.586%\n",
      "Epoch 5, Batch 64, LR 0.000098 Loss 12.209506, Accuracy 48.621%\n",
      "Epoch 5, Batch 65, LR 0.000098 Loss 12.210054, Accuracy 48.666%\n",
      "Epoch 5, Batch 66, LR 0.000098 Loss 12.200749, Accuracy 48.769%\n",
      "Epoch 5, Batch 67, LR 0.000098 Loss 12.200957, Accuracy 48.764%\n",
      "Epoch 5, Batch 68, LR 0.000098 Loss 12.199278, Accuracy 48.736%\n",
      "Epoch 5, Batch 69, LR 0.000098 Loss 12.202339, Accuracy 48.573%\n",
      "Epoch 5, Batch 70, LR 0.000098 Loss 12.202988, Accuracy 48.560%\n",
      "Epoch 5, Batch 71, LR 0.000098 Loss 12.210519, Accuracy 48.548%\n",
      "Epoch 5, Batch 72, LR 0.000098 Loss 12.209680, Accuracy 48.568%\n",
      "Epoch 5, Batch 73, LR 0.000098 Loss 12.212720, Accuracy 48.555%\n",
      "Epoch 5, Batch 74, LR 0.000098 Loss 12.210403, Accuracy 48.554%\n",
      "Epoch 5, Batch 75, LR 0.000098 Loss 12.216179, Accuracy 48.448%\n",
      "Epoch 5, Batch 76, LR 0.000098 Loss 12.220969, Accuracy 48.417%\n",
      "Epoch 5, Batch 77, LR 0.000098 Loss 12.219430, Accuracy 48.427%\n",
      "Epoch 5, Batch 78, LR 0.000098 Loss 12.223854, Accuracy 48.448%\n",
      "Epoch 5, Batch 79, LR 0.000098 Loss 12.228082, Accuracy 48.329%\n",
      "Epoch 5, Batch 80, LR 0.000098 Loss 12.228745, Accuracy 48.340%\n",
      "Epoch 5, Batch 81, LR 0.000098 Loss 12.232989, Accuracy 48.322%\n",
      "Epoch 5, Batch 82, LR 0.000098 Loss 12.233653, Accuracy 48.333%\n",
      "Epoch 5, Batch 83, LR 0.000098 Loss 12.235733, Accuracy 48.287%\n",
      "Epoch 5, Batch 84, LR 0.000098 Loss 12.230307, Accuracy 48.354%\n",
      "Epoch 5, Batch 85, LR 0.000098 Loss 12.232541, Accuracy 48.309%\n",
      "Epoch 5, Batch 86, LR 0.000098 Loss 12.230854, Accuracy 48.301%\n",
      "Epoch 5, Batch 87, LR 0.000098 Loss 12.229647, Accuracy 48.330%\n",
      "Epoch 5, Batch 88, LR 0.000098 Loss 12.236668, Accuracy 48.242%\n",
      "Epoch 5, Batch 89, LR 0.000098 Loss 12.239737, Accuracy 48.192%\n",
      "Epoch 5, Batch 90, LR 0.000098 Loss 12.248730, Accuracy 48.177%\n",
      "Epoch 5, Batch 91, LR 0.000098 Loss 12.239683, Accuracy 48.240%\n",
      "Epoch 5, Batch 92, LR 0.000098 Loss 12.244570, Accuracy 48.140%\n",
      "Epoch 5, Batch 93, LR 0.000098 Loss 12.246338, Accuracy 48.185%\n",
      "Epoch 5, Batch 94, LR 0.000098 Loss 12.250156, Accuracy 48.155%\n",
      "Epoch 5, Batch 95, LR 0.000098 Loss 12.253357, Accuracy 48.150%\n",
      "Epoch 5, Batch 96, LR 0.000098 Loss 12.245937, Accuracy 48.218%\n",
      "Epoch 5, Batch 97, LR 0.000098 Loss 12.247897, Accuracy 48.236%\n",
      "Epoch 5, Batch 98, LR 0.000098 Loss 12.248628, Accuracy 48.206%\n",
      "Epoch 5, Batch 99, LR 0.000098 Loss 12.249086, Accuracy 48.193%\n",
      "Epoch 5, Batch 100, LR 0.000098 Loss 12.252884, Accuracy 48.156%\n",
      "Epoch 5, Batch 101, LR 0.000098 Loss 12.254844, Accuracy 48.136%\n",
      "Epoch 5, Batch 102, LR 0.000098 Loss 12.255212, Accuracy 48.062%\n",
      "Epoch 5, Batch 103, LR 0.000098 Loss 12.252468, Accuracy 48.081%\n",
      "Epoch 5, Batch 104, LR 0.000098 Loss 12.247063, Accuracy 48.062%\n",
      "Epoch 5, Batch 105, LR 0.000098 Loss 12.244588, Accuracy 48.110%\n",
      "Epoch 5, Batch 106, LR 0.000098 Loss 12.245732, Accuracy 48.098%\n",
      "Epoch 5, Batch 107, LR 0.000098 Loss 12.253098, Accuracy 48.007%\n",
      "Epoch 5, Batch 108, LR 0.000098 Loss 12.253147, Accuracy 47.996%\n",
      "Epoch 5, Batch 109, LR 0.000098 Loss 12.256674, Accuracy 48.007%\n",
      "Epoch 5, Batch 110, LR 0.000098 Loss 12.249314, Accuracy 48.061%\n",
      "Epoch 5, Batch 111, LR 0.000098 Loss 12.254887, Accuracy 48.029%\n",
      "Epoch 5, Batch 112, LR 0.000098 Loss 12.249462, Accuracy 48.089%\n",
      "Epoch 5, Batch 113, LR 0.000098 Loss 12.241777, Accuracy 48.182%\n",
      "Epoch 5, Batch 114, LR 0.000098 Loss 12.243223, Accuracy 48.143%\n",
      "Epoch 5, Batch 115, LR 0.000098 Loss 12.242248, Accuracy 48.152%\n",
      "Epoch 5, Batch 116, LR 0.000098 Loss 12.242120, Accuracy 48.134%\n",
      "Epoch 5, Batch 117, LR 0.000098 Loss 12.237599, Accuracy 48.170%\n",
      "Epoch 5, Batch 118, LR 0.000098 Loss 12.232679, Accuracy 48.206%\n",
      "Epoch 5, Batch 119, LR 0.000098 Loss 12.229715, Accuracy 48.201%\n",
      "Epoch 5, Batch 120, LR 0.000098 Loss 12.237230, Accuracy 48.118%\n",
      "Epoch 5, Batch 121, LR 0.000098 Loss 12.233097, Accuracy 48.166%\n",
      "Epoch 5, Batch 122, LR 0.000098 Loss 12.228373, Accuracy 48.207%\n",
      "Epoch 5, Batch 123, LR 0.000098 Loss 12.230565, Accuracy 48.177%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 124, LR 0.000098 Loss 12.227203, Accuracy 48.198%\n",
      "Epoch 5, Batch 125, LR 0.000098 Loss 12.221697, Accuracy 48.212%\n",
      "Epoch 5, Batch 126, LR 0.000098 Loss 12.227221, Accuracy 48.158%\n",
      "Epoch 5, Batch 127, LR 0.000098 Loss 12.224361, Accuracy 48.161%\n",
      "Epoch 5, Batch 128, LR 0.000098 Loss 12.221761, Accuracy 48.175%\n",
      "Epoch 5, Batch 129, LR 0.000098 Loss 12.220380, Accuracy 48.183%\n",
      "Epoch 5, Batch 130, LR 0.000098 Loss 12.213845, Accuracy 48.233%\n",
      "Epoch 5, Batch 131, LR 0.000098 Loss 12.213684, Accuracy 48.253%\n",
      "Epoch 5, Batch 132, LR 0.000098 Loss 12.202894, Accuracy 48.313%\n",
      "Epoch 5, Batch 133, LR 0.000098 Loss 12.205194, Accuracy 48.302%\n",
      "Epoch 5, Batch 134, LR 0.000098 Loss 12.207535, Accuracy 48.268%\n",
      "Epoch 5, Batch 135, LR 0.000098 Loss 12.207255, Accuracy 48.281%\n",
      "Epoch 5, Batch 136, LR 0.000098 Loss 12.207648, Accuracy 48.277%\n",
      "Epoch 5, Batch 137, LR 0.000098 Loss 12.209471, Accuracy 48.249%\n",
      "Epoch 5, Batch 138, LR 0.000098 Loss 12.206321, Accuracy 48.290%\n",
      "Epoch 5, Batch 139, LR 0.000098 Loss 12.206416, Accuracy 48.241%\n",
      "Epoch 5, Batch 140, LR 0.000098 Loss 12.207526, Accuracy 48.253%\n",
      "Epoch 5, Batch 141, LR 0.000098 Loss 12.207619, Accuracy 48.260%\n",
      "Epoch 5, Batch 142, LR 0.000098 Loss 12.206659, Accuracy 48.250%\n",
      "Epoch 5, Batch 143, LR 0.000098 Loss 12.202690, Accuracy 48.252%\n",
      "Epoch 5, Batch 144, LR 0.000098 Loss 12.203264, Accuracy 48.220%\n",
      "Epoch 5, Batch 145, LR 0.000098 Loss 12.201906, Accuracy 48.233%\n",
      "Epoch 5, Batch 146, LR 0.000098 Loss 12.202549, Accuracy 48.266%\n",
      "Epoch 5, Batch 147, LR 0.000098 Loss 12.202634, Accuracy 48.230%\n",
      "Epoch 5, Batch 148, LR 0.000098 Loss 12.198205, Accuracy 48.274%\n",
      "Epoch 5, Batch 149, LR 0.000098 Loss 12.195593, Accuracy 48.312%\n",
      "Epoch 5, Batch 150, LR 0.000098 Loss 12.197785, Accuracy 48.328%\n",
      "Epoch 5, Batch 151, LR 0.000098 Loss 12.195475, Accuracy 48.350%\n",
      "Epoch 5, Batch 152, LR 0.000098 Loss 12.195786, Accuracy 48.350%\n",
      "Epoch 5, Batch 153, LR 0.000098 Loss 12.194795, Accuracy 48.361%\n",
      "Epoch 5, Batch 154, LR 0.000098 Loss 12.195113, Accuracy 48.361%\n",
      "Epoch 5, Batch 155, LR 0.000098 Loss 12.194580, Accuracy 48.322%\n",
      "Epoch 5, Batch 156, LR 0.000098 Loss 12.194859, Accuracy 48.352%\n",
      "Epoch 5, Batch 157, LR 0.000098 Loss 12.191972, Accuracy 48.373%\n",
      "Epoch 5, Batch 158, LR 0.000098 Loss 12.187319, Accuracy 48.408%\n",
      "Epoch 5, Batch 159, LR 0.000098 Loss 12.185966, Accuracy 48.457%\n",
      "Epoch 5, Batch 160, LR 0.000098 Loss 12.185095, Accuracy 48.452%\n",
      "Epoch 5, Batch 161, LR 0.000098 Loss 12.178772, Accuracy 48.539%\n",
      "Epoch 5, Batch 162, LR 0.000098 Loss 12.182623, Accuracy 48.510%\n",
      "Epoch 5, Batch 163, LR 0.000098 Loss 12.185578, Accuracy 48.466%\n",
      "Epoch 5, Batch 164, LR 0.000098 Loss 12.188039, Accuracy 48.457%\n",
      "Epoch 5, Batch 165, LR 0.000098 Loss 12.186318, Accuracy 48.490%\n",
      "Epoch 5, Batch 166, LR 0.000098 Loss 12.184887, Accuracy 48.503%\n",
      "Epoch 5, Batch 167, LR 0.000098 Loss 12.187431, Accuracy 48.489%\n",
      "Epoch 5, Batch 168, LR 0.000098 Loss 12.185269, Accuracy 48.507%\n",
      "Epoch 5, Batch 169, LR 0.000098 Loss 12.187812, Accuracy 48.511%\n",
      "Epoch 5, Batch 170, LR 0.000098 Loss 12.189158, Accuracy 48.502%\n",
      "Epoch 5, Batch 171, LR 0.000098 Loss 12.193909, Accuracy 48.460%\n",
      "Epoch 5, Batch 172, LR 0.000098 Loss 12.194893, Accuracy 48.442%\n",
      "Epoch 5, Batch 173, LR 0.000098 Loss 12.196423, Accuracy 48.469%\n",
      "Epoch 5, Batch 174, LR 0.000098 Loss 12.192982, Accuracy 48.496%\n",
      "Epoch 5, Batch 175, LR 0.000098 Loss 12.191922, Accuracy 48.469%\n",
      "Epoch 5, Batch 176, LR 0.000098 Loss 12.191063, Accuracy 48.438%\n",
      "Epoch 5, Batch 177, LR 0.000098 Loss 12.189656, Accuracy 48.446%\n",
      "Epoch 5, Batch 178, LR 0.000098 Loss 12.186985, Accuracy 48.459%\n",
      "Epoch 5, Batch 179, LR 0.000098 Loss 12.187122, Accuracy 48.442%\n",
      "Epoch 5, Batch 180, LR 0.000098 Loss 12.187169, Accuracy 48.424%\n",
      "Epoch 5, Batch 181, LR 0.000098 Loss 12.186745, Accuracy 48.438%\n",
      "Epoch 5, Batch 182, LR 0.000098 Loss 12.187523, Accuracy 48.438%\n",
      "Epoch 5, Batch 183, LR 0.000098 Loss 12.189063, Accuracy 48.416%\n",
      "Epoch 5, Batch 184, LR 0.000098 Loss 12.190255, Accuracy 48.412%\n",
      "Epoch 5, Batch 185, LR 0.000098 Loss 12.190041, Accuracy 48.442%\n",
      "Epoch 5, Batch 186, LR 0.000098 Loss 12.192298, Accuracy 48.446%\n",
      "Epoch 5, Batch 187, LR 0.000098 Loss 12.192385, Accuracy 48.408%\n",
      "Epoch 5, Batch 188, LR 0.000098 Loss 12.193397, Accuracy 48.388%\n",
      "Epoch 5, Batch 189, LR 0.000098 Loss 12.189559, Accuracy 48.392%\n",
      "Epoch 5, Batch 190, LR 0.000098 Loss 12.193180, Accuracy 48.376%\n",
      "Epoch 5, Batch 191, LR 0.000098 Loss 12.193148, Accuracy 48.356%\n",
      "Epoch 5, Batch 192, LR 0.000098 Loss 12.188367, Accuracy 48.376%\n",
      "Epoch 5, Batch 193, LR 0.000098 Loss 12.191076, Accuracy 48.377%\n",
      "Epoch 5, Batch 194, LR 0.000098 Loss 12.191761, Accuracy 48.361%\n",
      "Epoch 5, Batch 195, LR 0.000098 Loss 12.186099, Accuracy 48.425%\n",
      "Epoch 5, Batch 196, LR 0.000098 Loss 12.184836, Accuracy 48.449%\n",
      "Epoch 5, Batch 197, LR 0.000098 Loss 12.183035, Accuracy 48.485%\n",
      "Epoch 5, Batch 198, LR 0.000098 Loss 12.183819, Accuracy 48.477%\n",
      "Epoch 5, Batch 199, LR 0.000098 Loss 12.179012, Accuracy 48.528%\n",
      "Epoch 5, Batch 200, LR 0.000098 Loss 12.179495, Accuracy 48.520%\n",
      "Epoch 5, Batch 201, LR 0.000098 Loss 12.181210, Accuracy 48.504%\n",
      "Epoch 5, Batch 202, LR 0.000098 Loss 12.178805, Accuracy 48.515%\n",
      "Epoch 5, Batch 203, LR 0.000098 Loss 12.178684, Accuracy 48.549%\n",
      "Epoch 5, Batch 204, LR 0.000098 Loss 12.177626, Accuracy 48.560%\n",
      "Epoch 5, Batch 205, LR 0.000098 Loss 12.176376, Accuracy 48.579%\n",
      "Epoch 5, Batch 206, LR 0.000098 Loss 12.178535, Accuracy 48.532%\n",
      "Epoch 5, Batch 207, LR 0.000098 Loss 12.173438, Accuracy 48.581%\n",
      "Epoch 5, Batch 208, LR 0.000098 Loss 12.173642, Accuracy 48.573%\n",
      "Epoch 5, Batch 209, LR 0.000098 Loss 12.174971, Accuracy 48.531%\n",
      "Epoch 5, Batch 210, LR 0.000098 Loss 12.175818, Accuracy 48.493%\n",
      "Epoch 5, Batch 211, LR 0.000098 Loss 12.176038, Accuracy 48.489%\n",
      "Epoch 5, Batch 212, LR 0.000098 Loss 12.176864, Accuracy 48.460%\n",
      "Epoch 5, Batch 213, LR 0.000098 Loss 12.173598, Accuracy 48.511%\n",
      "Epoch 5, Batch 214, LR 0.000098 Loss 12.172956, Accuracy 48.514%\n",
      "Epoch 5, Batch 215, LR 0.000098 Loss 12.169465, Accuracy 48.561%\n",
      "Epoch 5, Batch 216, LR 0.000098 Loss 12.167094, Accuracy 48.571%\n",
      "Epoch 5, Batch 217, LR 0.000098 Loss 12.167044, Accuracy 48.564%\n",
      "Epoch 5, Batch 218, LR 0.000098 Loss 12.169551, Accuracy 48.549%\n",
      "Epoch 5, Batch 219, LR 0.000098 Loss 12.170685, Accuracy 48.530%\n",
      "Epoch 5, Batch 220, LR 0.000098 Loss 12.170864, Accuracy 48.519%\n",
      "Epoch 5, Batch 221, LR 0.000098 Loss 12.167557, Accuracy 48.544%\n",
      "Epoch 5, Batch 222, LR 0.000098 Loss 12.165725, Accuracy 48.547%\n",
      "Epoch 5, Batch 223, LR 0.000098 Loss 12.166841, Accuracy 48.532%\n",
      "Epoch 5, Batch 224, LR 0.000098 Loss 12.165380, Accuracy 48.535%\n",
      "Epoch 5, Batch 225, LR 0.000098 Loss 12.164475, Accuracy 48.535%\n",
      "Epoch 5, Batch 226, LR 0.000098 Loss 12.163347, Accuracy 48.524%\n",
      "Epoch 5, Batch 227, LR 0.000098 Loss 12.163681, Accuracy 48.524%\n",
      "Epoch 5, Batch 228, LR 0.000098 Loss 12.162353, Accuracy 48.564%\n",
      "Epoch 5, Batch 229, LR 0.000098 Loss 12.159872, Accuracy 48.591%\n",
      "Epoch 5, Batch 230, LR 0.000098 Loss 12.160176, Accuracy 48.597%\n",
      "Epoch 5, Batch 231, LR 0.000098 Loss 12.160233, Accuracy 48.600%\n",
      "Epoch 5, Batch 232, LR 0.000098 Loss 12.159496, Accuracy 48.592%\n",
      "Epoch 5, Batch 233, LR 0.000098 Loss 12.162306, Accuracy 48.568%\n",
      "Epoch 5, Batch 234, LR 0.000098 Loss 12.163525, Accuracy 48.561%\n",
      "Epoch 5, Batch 235, LR 0.000098 Loss 12.165496, Accuracy 48.534%\n",
      "Epoch 5, Batch 236, LR 0.000098 Loss 12.164484, Accuracy 48.567%\n",
      "Epoch 5, Batch 237, LR 0.000098 Loss 12.162327, Accuracy 48.573%\n",
      "Epoch 5, Batch 238, LR 0.000098 Loss 12.163992, Accuracy 48.556%\n",
      "Epoch 5, Batch 239, LR 0.000098 Loss 12.162809, Accuracy 48.581%\n",
      "Epoch 5, Batch 240, LR 0.000098 Loss 12.163613, Accuracy 48.571%\n",
      "Epoch 5, Batch 241, LR 0.000098 Loss 12.162622, Accuracy 48.567%\n",
      "Epoch 5, Batch 242, LR 0.000098 Loss 12.156675, Accuracy 48.596%\n",
      "Epoch 5, Batch 243, LR 0.000098 Loss 12.158049, Accuracy 48.601%\n",
      "Epoch 5, Batch 244, LR 0.000098 Loss 12.158433, Accuracy 48.594%\n",
      "Epoch 5, Batch 245, LR 0.000098 Loss 12.160815, Accuracy 48.571%\n",
      "Epoch 5, Batch 246, LR 0.000098 Loss 12.163124, Accuracy 48.533%\n",
      "Epoch 5, Batch 247, LR 0.000098 Loss 12.160966, Accuracy 48.551%\n",
      "Epoch 5, Batch 248, LR 0.000098 Loss 12.162026, Accuracy 48.548%\n",
      "Epoch 5, Batch 249, LR 0.000098 Loss 12.161185, Accuracy 48.547%\n",
      "Epoch 5, Batch 250, LR 0.000098 Loss 12.162950, Accuracy 48.534%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 251, LR 0.000098 Loss 12.159866, Accuracy 48.550%\n",
      "Epoch 5, Batch 252, LR 0.000098 Loss 12.160589, Accuracy 48.537%\n",
      "Epoch 5, Batch 253, LR 0.000098 Loss 12.162141, Accuracy 48.530%\n",
      "Epoch 5, Batch 254, LR 0.000098 Loss 12.162278, Accuracy 48.527%\n",
      "Epoch 5, Batch 255, LR 0.000098 Loss 12.159300, Accuracy 48.551%\n",
      "Epoch 5, Batch 256, LR 0.000098 Loss 12.157599, Accuracy 48.563%\n",
      "Epoch 5, Batch 257, LR 0.000098 Loss 12.155394, Accuracy 48.583%\n",
      "Epoch 5, Batch 258, LR 0.000098 Loss 12.154933, Accuracy 48.580%\n",
      "Epoch 5, Batch 259, LR 0.000098 Loss 12.152873, Accuracy 48.597%\n",
      "Epoch 5, Batch 260, LR 0.000098 Loss 12.153280, Accuracy 48.597%\n",
      "Epoch 5, Batch 261, LR 0.000098 Loss 12.153412, Accuracy 48.602%\n",
      "Epoch 5, Batch 262, LR 0.000098 Loss 12.152835, Accuracy 48.607%\n",
      "Epoch 5, Batch 263, LR 0.000098 Loss 12.152651, Accuracy 48.616%\n",
      "Epoch 5, Batch 264, LR 0.000098 Loss 12.154364, Accuracy 48.585%\n",
      "Epoch 5, Batch 265, LR 0.000098 Loss 12.153565, Accuracy 48.597%\n",
      "Epoch 5, Batch 266, LR 0.000098 Loss 12.152564, Accuracy 48.581%\n",
      "Epoch 5, Batch 267, LR 0.000098 Loss 12.152624, Accuracy 48.584%\n",
      "Epoch 5, Batch 268, LR 0.000098 Loss 12.150690, Accuracy 48.612%\n",
      "Epoch 5, Batch 269, LR 0.000098 Loss 12.149194, Accuracy 48.632%\n",
      "Epoch 5, Batch 270, LR 0.000098 Loss 12.150133, Accuracy 48.617%\n",
      "Epoch 5, Batch 271, LR 0.000098 Loss 12.152613, Accuracy 48.579%\n",
      "Epoch 5, Batch 272, LR 0.000098 Loss 12.152595, Accuracy 48.572%\n",
      "Epoch 5, Batch 273, LR 0.000098 Loss 12.152237, Accuracy 48.592%\n",
      "Epoch 5, Batch 274, LR 0.000098 Loss 12.152585, Accuracy 48.580%\n",
      "Epoch 5, Batch 275, LR 0.000098 Loss 12.154038, Accuracy 48.557%\n",
      "Epoch 5, Batch 276, LR 0.000098 Loss 12.157340, Accuracy 48.522%\n",
      "Epoch 5, Batch 277, LR 0.000098 Loss 12.158676, Accuracy 48.505%\n",
      "Epoch 5, Batch 278, LR 0.000098 Loss 12.157486, Accuracy 48.522%\n",
      "Epoch 5, Batch 279, LR 0.000098 Loss 12.158209, Accuracy 48.508%\n",
      "Epoch 5, Batch 280, LR 0.000098 Loss 12.160629, Accuracy 48.479%\n",
      "Epoch 5, Batch 281, LR 0.000098 Loss 12.159030, Accuracy 48.493%\n",
      "Epoch 5, Batch 282, LR 0.000098 Loss 12.158106, Accuracy 48.515%\n",
      "Epoch 5, Batch 283, LR 0.000098 Loss 12.157900, Accuracy 48.518%\n",
      "Epoch 5, Batch 284, LR 0.000098 Loss 12.158394, Accuracy 48.501%\n",
      "Epoch 5, Batch 285, LR 0.000098 Loss 12.156379, Accuracy 48.503%\n",
      "Epoch 5, Batch 286, LR 0.000098 Loss 12.156072, Accuracy 48.500%\n",
      "Epoch 5, Batch 287, LR 0.000098 Loss 12.156944, Accuracy 48.495%\n",
      "Epoch 5, Batch 288, LR 0.000098 Loss 12.157120, Accuracy 48.511%\n",
      "Epoch 5, Batch 289, LR 0.000098 Loss 12.157367, Accuracy 48.513%\n",
      "Epoch 5, Batch 290, LR 0.000098 Loss 12.156040, Accuracy 48.499%\n",
      "Epoch 5, Batch 291, LR 0.000098 Loss 12.155458, Accuracy 48.510%\n",
      "Epoch 5, Batch 292, LR 0.000098 Loss 12.156402, Accuracy 48.502%\n",
      "Epoch 5, Batch 293, LR 0.000098 Loss 12.158368, Accuracy 48.485%\n",
      "Epoch 5, Batch 294, LR 0.000098 Loss 12.155260, Accuracy 48.499%\n",
      "Epoch 5, Batch 295, LR 0.000098 Loss 12.154108, Accuracy 48.488%\n",
      "Epoch 5, Batch 296, LR 0.000098 Loss 12.154863, Accuracy 48.496%\n",
      "Epoch 5, Batch 297, LR 0.000098 Loss 12.154008, Accuracy 48.503%\n",
      "Epoch 5, Batch 298, LR 0.000098 Loss 12.155089, Accuracy 48.493%\n",
      "Epoch 5, Batch 299, LR 0.000098 Loss 12.154565, Accuracy 48.513%\n",
      "Epoch 5, Batch 300, LR 0.000098 Loss 12.154516, Accuracy 48.518%\n",
      "Epoch 5, Batch 301, LR 0.000098 Loss 12.153019, Accuracy 48.521%\n",
      "Epoch 5, Batch 302, LR 0.000098 Loss 12.154463, Accuracy 48.520%\n",
      "Epoch 5, Batch 303, LR 0.000098 Loss 12.154737, Accuracy 48.507%\n",
      "Epoch 5, Batch 304, LR 0.000098 Loss 12.154325, Accuracy 48.509%\n",
      "Epoch 5, Batch 305, LR 0.000098 Loss 12.153518, Accuracy 48.517%\n",
      "Epoch 5, Batch 306, LR 0.000098 Loss 12.155410, Accuracy 48.501%\n",
      "Epoch 5, Batch 307, LR 0.000098 Loss 12.155487, Accuracy 48.483%\n",
      "Epoch 5, Batch 308, LR 0.000098 Loss 12.153823, Accuracy 48.488%\n",
      "Epoch 5, Batch 309, LR 0.000098 Loss 12.152709, Accuracy 48.493%\n",
      "Epoch 5, Batch 310, LR 0.000098 Loss 12.152593, Accuracy 48.488%\n",
      "Epoch 5, Batch 311, LR 0.000098 Loss 12.150104, Accuracy 48.498%\n",
      "Epoch 5, Batch 312, LR 0.000098 Loss 12.151002, Accuracy 48.490%\n",
      "Epoch 5, Batch 313, LR 0.000098 Loss 12.150811, Accuracy 48.482%\n",
      "Epoch 5, Batch 314, LR 0.000098 Loss 12.150559, Accuracy 48.485%\n",
      "Epoch 5, Batch 315, LR 0.000098 Loss 12.152758, Accuracy 48.460%\n",
      "Epoch 5, Batch 316, LR 0.000098 Loss 12.151884, Accuracy 48.467%\n",
      "Epoch 5, Batch 317, LR 0.000098 Loss 12.149431, Accuracy 48.507%\n",
      "Epoch 5, Batch 318, LR 0.000098 Loss 12.147759, Accuracy 48.519%\n",
      "Epoch 5, Batch 319, LR 0.000098 Loss 12.145381, Accuracy 48.531%\n",
      "Epoch 5, Batch 320, LR 0.000098 Loss 12.144934, Accuracy 48.538%\n",
      "Epoch 5, Batch 321, LR 0.000098 Loss 12.144685, Accuracy 48.545%\n",
      "Epoch 5, Batch 322, LR 0.000098 Loss 12.141695, Accuracy 48.559%\n",
      "Epoch 5, Batch 323, LR 0.000098 Loss 12.140026, Accuracy 48.573%\n",
      "Epoch 5, Batch 324, LR 0.000098 Loss 12.140540, Accuracy 48.570%\n",
      "Epoch 5, Batch 325, LR 0.000098 Loss 12.139956, Accuracy 48.575%\n",
      "Epoch 5, Batch 326, LR 0.000098 Loss 12.140340, Accuracy 48.588%\n",
      "Epoch 5, Batch 327, LR 0.000098 Loss 12.141007, Accuracy 48.576%\n",
      "Epoch 5, Batch 328, LR 0.000098 Loss 12.140462, Accuracy 48.583%\n",
      "Epoch 5, Batch 329, LR 0.000098 Loss 12.138446, Accuracy 48.585%\n",
      "Epoch 5, Batch 330, LR 0.000098 Loss 12.137109, Accuracy 48.610%\n",
      "Epoch 5, Batch 331, LR 0.000098 Loss 12.136760, Accuracy 48.603%\n",
      "Epoch 5, Batch 332, LR 0.000098 Loss 12.134290, Accuracy 48.630%\n",
      "Epoch 5, Batch 333, LR 0.000098 Loss 12.134379, Accuracy 48.637%\n",
      "Epoch 5, Batch 334, LR 0.000098 Loss 12.133912, Accuracy 48.646%\n",
      "Epoch 5, Batch 335, LR 0.000098 Loss 12.134436, Accuracy 48.661%\n",
      "Epoch 5, Batch 336, LR 0.000098 Loss 12.134525, Accuracy 48.670%\n",
      "Epoch 5, Batch 337, LR 0.000098 Loss 12.134775, Accuracy 48.658%\n",
      "Epoch 5, Batch 338, LR 0.000098 Loss 12.134004, Accuracy 48.652%\n",
      "Epoch 5, Batch 339, LR 0.000098 Loss 12.132286, Accuracy 48.675%\n",
      "Epoch 5, Batch 340, LR 0.000098 Loss 12.132042, Accuracy 48.681%\n",
      "Epoch 5, Batch 341, LR 0.000098 Loss 12.131294, Accuracy 48.673%\n",
      "Epoch 5, Batch 342, LR 0.000098 Loss 12.131523, Accuracy 48.684%\n",
      "Epoch 5, Batch 343, LR 0.000098 Loss 12.131337, Accuracy 48.693%\n",
      "Epoch 5, Batch 344, LR 0.000098 Loss 12.131346, Accuracy 48.724%\n",
      "Epoch 5, Batch 345, LR 0.000098 Loss 12.130362, Accuracy 48.734%\n",
      "Epoch 5, Batch 346, LR 0.000098 Loss 12.130297, Accuracy 48.745%\n",
      "Epoch 5, Batch 347, LR 0.000098 Loss 12.130192, Accuracy 48.762%\n",
      "Epoch 5, Batch 348, LR 0.000098 Loss 12.128879, Accuracy 48.761%\n",
      "Epoch 5, Batch 349, LR 0.000098 Loss 12.129011, Accuracy 48.746%\n",
      "Epoch 5, Batch 350, LR 0.000098 Loss 12.129226, Accuracy 48.748%\n",
      "Epoch 5, Batch 351, LR 0.000098 Loss 12.128849, Accuracy 48.751%\n",
      "Epoch 5, Batch 352, LR 0.000098 Loss 12.126879, Accuracy 48.757%\n",
      "Epoch 5, Batch 353, LR 0.000098 Loss 12.125500, Accuracy 48.767%\n",
      "Epoch 5, Batch 354, LR 0.000098 Loss 12.126921, Accuracy 48.746%\n",
      "Epoch 5, Batch 355, LR 0.000098 Loss 12.125639, Accuracy 48.761%\n",
      "Epoch 5, Batch 356, LR 0.000098 Loss 12.124695, Accuracy 48.758%\n",
      "Epoch 5, Batch 357, LR 0.000098 Loss 12.124984, Accuracy 48.764%\n",
      "Epoch 5, Batch 358, LR 0.000098 Loss 12.126470, Accuracy 48.752%\n",
      "Epoch 5, Batch 359, LR 0.000098 Loss 12.126420, Accuracy 48.736%\n",
      "Epoch 5, Batch 360, LR 0.000098 Loss 12.127425, Accuracy 48.743%\n",
      "Epoch 5, Batch 361, LR 0.000098 Loss 12.127539, Accuracy 48.751%\n",
      "Epoch 5, Batch 362, LR 0.000098 Loss 12.125988, Accuracy 48.774%\n",
      "Epoch 5, Batch 363, LR 0.000098 Loss 12.124884, Accuracy 48.780%\n",
      "Epoch 5, Batch 364, LR 0.000098 Loss 12.125867, Accuracy 48.764%\n",
      "Epoch 5, Batch 365, LR 0.000098 Loss 12.124240, Accuracy 48.793%\n",
      "Epoch 5, Batch 366, LR 0.000098 Loss 12.124909, Accuracy 48.792%\n",
      "Epoch 5, Batch 367, LR 0.000098 Loss 12.126771, Accuracy 48.789%\n",
      "Epoch 5, Batch 368, LR 0.000098 Loss 12.127929, Accuracy 48.767%\n",
      "Epoch 5, Batch 369, LR 0.000098 Loss 12.126386, Accuracy 48.783%\n",
      "Epoch 5, Batch 370, LR 0.000098 Loss 12.127393, Accuracy 48.788%\n",
      "Epoch 5, Batch 371, LR 0.000098 Loss 12.127002, Accuracy 48.793%\n",
      "Epoch 5, Batch 372, LR 0.000098 Loss 12.128199, Accuracy 48.790%\n",
      "Epoch 5, Batch 373, LR 0.000098 Loss 12.127457, Accuracy 48.789%\n",
      "Epoch 5, Batch 374, LR 0.000098 Loss 12.126471, Accuracy 48.805%\n",
      "Epoch 5, Batch 375, LR 0.000098 Loss 12.126010, Accuracy 48.815%\n",
      "Epoch 5, Batch 376, LR 0.000098 Loss 12.124143, Accuracy 48.822%\n",
      "Epoch 5, Batch 377, LR 0.000098 Loss 12.123455, Accuracy 48.819%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 378, LR 0.000098 Loss 12.120459, Accuracy 48.838%\n",
      "Epoch 5, Batch 379, LR 0.000098 Loss 12.120775, Accuracy 48.839%\n",
      "Epoch 5, Batch 380, LR 0.000098 Loss 12.120076, Accuracy 48.851%\n",
      "Epoch 5, Batch 381, LR 0.000098 Loss 12.121050, Accuracy 48.837%\n",
      "Epoch 5, Batch 382, LR 0.000098 Loss 12.118427, Accuracy 48.857%\n",
      "Epoch 5, Batch 383, LR 0.000098 Loss 12.118792, Accuracy 48.839%\n",
      "Epoch 5, Batch 384, LR 0.000098 Loss 12.118481, Accuracy 48.848%\n",
      "Epoch 5, Batch 385, LR 0.000098 Loss 12.118986, Accuracy 48.837%\n",
      "Epoch 5, Batch 386, LR 0.000098 Loss 12.119249, Accuracy 48.838%\n",
      "Epoch 5, Batch 387, LR 0.000098 Loss 12.118762, Accuracy 48.847%\n",
      "Epoch 5, Batch 388, LR 0.000098 Loss 12.118225, Accuracy 48.864%\n",
      "Epoch 5, Batch 389, LR 0.000098 Loss 12.117235, Accuracy 48.871%\n",
      "Epoch 5, Batch 390, LR 0.000098 Loss 12.117766, Accuracy 48.862%\n",
      "Epoch 5, Batch 391, LR 0.000098 Loss 12.118294, Accuracy 48.863%\n",
      "Epoch 5, Batch 392, LR 0.000098 Loss 12.118844, Accuracy 48.862%\n",
      "Epoch 5, Batch 393, LR 0.000098 Loss 12.118234, Accuracy 48.869%\n",
      "Epoch 5, Batch 394, LR 0.000098 Loss 12.116923, Accuracy 48.868%\n",
      "Epoch 5, Batch 395, LR 0.000098 Loss 12.117060, Accuracy 48.879%\n",
      "Epoch 5, Batch 396, LR 0.000098 Loss 12.117183, Accuracy 48.877%\n",
      "Epoch 5, Batch 397, LR 0.000098 Loss 12.115840, Accuracy 48.888%\n",
      "Epoch 5, Batch 398, LR 0.000098 Loss 12.116439, Accuracy 48.889%\n",
      "Epoch 5, Batch 399, LR 0.000098 Loss 12.114559, Accuracy 48.909%\n",
      "Epoch 5, Batch 400, LR 0.000098 Loss 12.113480, Accuracy 48.912%\n",
      "Epoch 5, Batch 401, LR 0.000098 Loss 12.110394, Accuracy 48.944%\n",
      "Epoch 5, Batch 402, LR 0.000098 Loss 12.111008, Accuracy 48.931%\n",
      "Epoch 5, Batch 403, LR 0.000098 Loss 12.111479, Accuracy 48.920%\n",
      "Epoch 5, Batch 404, LR 0.000098 Loss 12.111729, Accuracy 48.921%\n",
      "Epoch 5, Batch 405, LR 0.000098 Loss 12.112072, Accuracy 48.927%\n",
      "Epoch 5, Batch 406, LR 0.000098 Loss 12.111394, Accuracy 48.930%\n",
      "Epoch 5, Batch 407, LR 0.000098 Loss 12.109792, Accuracy 48.929%\n",
      "Epoch 5, Batch 408, LR 0.000098 Loss 12.109932, Accuracy 48.930%\n",
      "Epoch 5, Batch 409, LR 0.000098 Loss 12.110001, Accuracy 48.923%\n",
      "Epoch 5, Batch 410, LR 0.000098 Loss 12.109995, Accuracy 48.927%\n",
      "Epoch 5, Batch 411, LR 0.000098 Loss 12.110619, Accuracy 48.930%\n",
      "Epoch 5, Batch 412, LR 0.000098 Loss 12.110696, Accuracy 48.919%\n",
      "Epoch 5, Batch 413, LR 0.000098 Loss 12.111774, Accuracy 48.912%\n",
      "Epoch 5, Batch 414, LR 0.000098 Loss 12.111772, Accuracy 48.919%\n",
      "Epoch 5, Batch 415, LR 0.000098 Loss 12.111608, Accuracy 48.929%\n",
      "Epoch 5, Batch 416, LR 0.000098 Loss 12.112001, Accuracy 48.924%\n",
      "Epoch 5, Batch 417, LR 0.000098 Loss 12.113177, Accuracy 48.926%\n",
      "Epoch 5, Batch 418, LR 0.000098 Loss 12.112282, Accuracy 48.923%\n",
      "Epoch 5, Batch 419, LR 0.000098 Loss 12.110698, Accuracy 48.952%\n",
      "Epoch 5, Batch 420, LR 0.000098 Loss 12.110942, Accuracy 48.956%\n",
      "Epoch 5, Batch 421, LR 0.000098 Loss 12.110851, Accuracy 48.957%\n",
      "Epoch 5, Batch 422, LR 0.000098 Loss 12.111542, Accuracy 48.958%\n",
      "Epoch 5, Batch 423, LR 0.000098 Loss 12.111923, Accuracy 48.953%\n",
      "Epoch 5, Batch 424, LR 0.000098 Loss 12.112201, Accuracy 48.941%\n",
      "Epoch 5, Batch 425, LR 0.000098 Loss 12.112040, Accuracy 48.934%\n",
      "Epoch 5, Batch 426, LR 0.000098 Loss 12.112138, Accuracy 48.925%\n",
      "Epoch 5, Batch 427, LR 0.000098 Loss 12.111544, Accuracy 48.935%\n",
      "Epoch 5, Batch 428, LR 0.000098 Loss 12.111458, Accuracy 48.939%\n",
      "Epoch 5, Batch 429, LR 0.000098 Loss 12.109787, Accuracy 48.949%\n",
      "Epoch 5, Batch 430, LR 0.000098 Loss 12.109384, Accuracy 48.943%\n",
      "Epoch 5, Batch 431, LR 0.000098 Loss 12.108378, Accuracy 48.967%\n",
      "Epoch 5, Batch 432, LR 0.000098 Loss 12.105933, Accuracy 48.998%\n",
      "Epoch 5, Batch 433, LR 0.000098 Loss 12.105885, Accuracy 48.995%\n",
      "Epoch 5, Batch 434, LR 0.000098 Loss 12.104875, Accuracy 49.005%\n",
      "Epoch 5, Batch 435, LR 0.000098 Loss 12.103839, Accuracy 49.009%\n",
      "Epoch 5, Batch 436, LR 0.000098 Loss 12.103369, Accuracy 49.016%\n",
      "Epoch 5, Batch 437, LR 0.000098 Loss 12.103982, Accuracy 49.002%\n",
      "Epoch 5, Batch 438, LR 0.000098 Loss 12.103308, Accuracy 49.003%\n",
      "Epoch 5, Batch 439, LR 0.000098 Loss 12.104590, Accuracy 48.986%\n",
      "Epoch 5, Batch 440, LR 0.000098 Loss 12.105880, Accuracy 48.977%\n",
      "Epoch 5, Batch 441, LR 0.000098 Loss 12.104177, Accuracy 48.999%\n",
      "Epoch 5, Batch 442, LR 0.000098 Loss 12.104627, Accuracy 49.003%\n",
      "Epoch 5, Batch 443, LR 0.000098 Loss 12.104447, Accuracy 49.012%\n",
      "Epoch 5, Batch 444, LR 0.000098 Loss 12.104257, Accuracy 49.009%\n",
      "Epoch 5, Batch 445, LR 0.000098 Loss 12.102816, Accuracy 49.013%\n",
      "Epoch 5, Batch 446, LR 0.000098 Loss 12.101958, Accuracy 49.019%\n",
      "Epoch 5, Batch 447, LR 0.000098 Loss 12.102277, Accuracy 49.028%\n",
      "Epoch 5, Batch 448, LR 0.000098 Loss 12.100629, Accuracy 49.034%\n",
      "Epoch 5, Batch 449, LR 0.000098 Loss 12.097551, Accuracy 49.050%\n",
      "Epoch 5, Batch 450, LR 0.000098 Loss 12.095997, Accuracy 49.069%\n",
      "Epoch 5, Batch 451, LR 0.000098 Loss 12.094344, Accuracy 49.080%\n",
      "Epoch 5, Batch 452, LR 0.000098 Loss 12.095067, Accuracy 49.067%\n",
      "Epoch 5, Batch 453, LR 0.000098 Loss 12.094609, Accuracy 49.070%\n",
      "Epoch 5, Batch 454, LR 0.000098 Loss 12.095283, Accuracy 49.074%\n",
      "Epoch 5, Batch 455, LR 0.000098 Loss 12.094797, Accuracy 49.100%\n",
      "Epoch 5, Batch 456, LR 0.000098 Loss 12.094416, Accuracy 49.107%\n",
      "Epoch 5, Batch 457, LR 0.000098 Loss 12.093968, Accuracy 49.106%\n",
      "Epoch 5, Batch 458, LR 0.000098 Loss 12.093276, Accuracy 49.111%\n",
      "Epoch 5, Batch 459, LR 0.000098 Loss 12.092909, Accuracy 49.123%\n",
      "Epoch 5, Batch 460, LR 0.000098 Loss 12.093370, Accuracy 49.112%\n",
      "Epoch 5, Batch 461, LR 0.000098 Loss 12.093938, Accuracy 49.119%\n",
      "Epoch 5, Batch 462, LR 0.000098 Loss 12.093675, Accuracy 49.121%\n",
      "Epoch 5, Batch 463, LR 0.000098 Loss 12.092272, Accuracy 49.145%\n",
      "Epoch 5, Batch 464, LR 0.000098 Loss 12.093265, Accuracy 49.128%\n",
      "Epoch 5, Batch 465, LR 0.000098 Loss 12.092775, Accuracy 49.121%\n",
      "Epoch 5, Batch 466, LR 0.000098 Loss 12.093441, Accuracy 49.110%\n",
      "Epoch 5, Batch 467, LR 0.000098 Loss 12.091947, Accuracy 49.117%\n",
      "Epoch 5, Batch 468, LR 0.000098 Loss 12.091654, Accuracy 49.122%\n",
      "Epoch 5, Batch 469, LR 0.000098 Loss 12.092614, Accuracy 49.125%\n",
      "Epoch 5, Batch 470, LR 0.000098 Loss 12.092071, Accuracy 49.129%\n",
      "Epoch 5, Batch 471, LR 0.000098 Loss 12.092880, Accuracy 49.121%\n",
      "Epoch 5, Batch 472, LR 0.000098 Loss 12.092775, Accuracy 49.118%\n",
      "Epoch 5, Batch 473, LR 0.000098 Loss 12.093848, Accuracy 49.111%\n",
      "Epoch 5, Batch 474, LR 0.000098 Loss 12.092464, Accuracy 49.130%\n",
      "Epoch 5, Batch 475, LR 0.000098 Loss 12.092256, Accuracy 49.128%\n",
      "Epoch 5, Batch 476, LR 0.000098 Loss 12.092606, Accuracy 49.128%\n",
      "Epoch 5, Batch 477, LR 0.000098 Loss 12.092770, Accuracy 49.120%\n",
      "Epoch 5, Batch 478, LR 0.000098 Loss 12.093232, Accuracy 49.114%\n",
      "Epoch 5, Batch 479, LR 0.000098 Loss 12.093851, Accuracy 49.114%\n",
      "Epoch 5, Batch 480, LR 0.000098 Loss 12.093655, Accuracy 49.118%\n",
      "Epoch 5, Batch 481, LR 0.000098 Loss 12.092097, Accuracy 49.125%\n",
      "Epoch 5, Batch 482, LR 0.000098 Loss 12.090753, Accuracy 49.138%\n",
      "Epoch 5, Batch 483, LR 0.000098 Loss 12.090157, Accuracy 49.148%\n",
      "Epoch 5, Batch 484, LR 0.000098 Loss 12.090616, Accuracy 49.144%\n",
      "Epoch 5, Batch 485, LR 0.000098 Loss 12.089842, Accuracy 49.151%\n",
      "Epoch 5, Batch 486, LR 0.000098 Loss 12.087696, Accuracy 49.174%\n",
      "Epoch 5, Batch 487, LR 0.000098 Loss 12.085687, Accuracy 49.196%\n",
      "Epoch 5, Batch 488, LR 0.000098 Loss 12.085258, Accuracy 49.196%\n",
      "Epoch 5, Batch 489, LR 0.000098 Loss 12.084626, Accuracy 49.203%\n",
      "Epoch 5, Batch 490, LR 0.000098 Loss 12.082740, Accuracy 49.217%\n",
      "Epoch 5, Batch 491, LR 0.000098 Loss 12.082182, Accuracy 49.230%\n",
      "Epoch 5, Batch 492, LR 0.000098 Loss 12.081941, Accuracy 49.247%\n",
      "Epoch 5, Batch 493, LR 0.000098 Loss 12.081998, Accuracy 49.238%\n",
      "Epoch 5, Batch 494, LR 0.000098 Loss 12.082352, Accuracy 49.236%\n",
      "Epoch 5, Batch 495, LR 0.000098 Loss 12.080215, Accuracy 49.257%\n",
      "Epoch 5, Batch 496, LR 0.000098 Loss 12.081235, Accuracy 49.252%\n",
      "Epoch 5, Batch 497, LR 0.000098 Loss 12.082062, Accuracy 49.244%\n",
      "Epoch 5, Batch 498, LR 0.000098 Loss 12.081228, Accuracy 49.250%\n",
      "Epoch 5, Batch 499, LR 0.000098 Loss 12.079787, Accuracy 49.261%\n",
      "Epoch 5, Batch 500, LR 0.000098 Loss 12.080825, Accuracy 49.253%\n",
      "Epoch 5, Batch 501, LR 0.000098 Loss 12.080626, Accuracy 49.247%\n",
      "Epoch 5, Batch 502, LR 0.000098 Loss 12.078824, Accuracy 49.258%\n",
      "Epoch 5, Batch 503, LR 0.000098 Loss 12.077763, Accuracy 49.265%\n",
      "Epoch 5, Batch 504, LR 0.000098 Loss 12.076438, Accuracy 49.275%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 505, LR 0.000098 Loss 12.075569, Accuracy 49.278%\n",
      "Epoch 5, Batch 506, LR 0.000098 Loss 12.076320, Accuracy 49.262%\n",
      "Epoch 5, Batch 507, LR 0.000098 Loss 12.076822, Accuracy 49.256%\n",
      "Epoch 5, Batch 508, LR 0.000098 Loss 12.076052, Accuracy 49.257%\n",
      "Epoch 5, Batch 509, LR 0.000098 Loss 12.076115, Accuracy 49.249%\n",
      "Epoch 5, Batch 510, LR 0.000098 Loss 12.076040, Accuracy 49.252%\n",
      "Epoch 5, Batch 511, LR 0.000098 Loss 12.075938, Accuracy 49.252%\n",
      "Epoch 5, Batch 512, LR 0.000098 Loss 12.074850, Accuracy 49.258%\n",
      "Epoch 5, Batch 513, LR 0.000098 Loss 12.074233, Accuracy 49.255%\n",
      "Epoch 5, Batch 514, LR 0.000098 Loss 12.074319, Accuracy 49.261%\n",
      "Epoch 5, Batch 515, LR 0.000098 Loss 12.074324, Accuracy 49.266%\n",
      "Epoch 5, Batch 516, LR 0.000098 Loss 12.074330, Accuracy 49.264%\n",
      "Epoch 5, Batch 517, LR 0.000098 Loss 12.074500, Accuracy 49.257%\n",
      "Epoch 5, Batch 518, LR 0.000098 Loss 12.075020, Accuracy 49.256%\n",
      "Epoch 5, Batch 519, LR 0.000098 Loss 12.075650, Accuracy 49.256%\n",
      "Epoch 5, Batch 520, LR 0.000098 Loss 12.074521, Accuracy 49.264%\n",
      "Epoch 5, Batch 521, LR 0.000098 Loss 12.074965, Accuracy 49.259%\n",
      "Epoch 5, Batch 522, LR 0.000098 Loss 12.074131, Accuracy 49.268%\n",
      "Epoch 5, Batch 523, LR 0.000098 Loss 12.075127, Accuracy 49.261%\n",
      "Epoch 5, Batch 524, LR 0.000098 Loss 12.074951, Accuracy 49.262%\n",
      "Epoch 5, Batch 525, LR 0.000098 Loss 12.073267, Accuracy 49.277%\n",
      "Epoch 5, Batch 526, LR 0.000098 Loss 12.072407, Accuracy 49.296%\n",
      "Epoch 5, Batch 527, LR 0.000098 Loss 12.070670, Accuracy 49.309%\n",
      "Epoch 5, Batch 528, LR 0.000098 Loss 12.070234, Accuracy 49.309%\n",
      "Epoch 5, Batch 529, LR 0.000098 Loss 12.069247, Accuracy 49.324%\n",
      "Epoch 5, Batch 530, LR 0.000098 Loss 12.069678, Accuracy 49.318%\n",
      "Epoch 5, Batch 531, LR 0.000098 Loss 12.070488, Accuracy 49.310%\n",
      "Epoch 5, Batch 532, LR 0.000098 Loss 12.070242, Accuracy 49.310%\n",
      "Epoch 5, Batch 533, LR 0.000098 Loss 12.070032, Accuracy 49.314%\n",
      "Epoch 5, Batch 534, LR 0.000098 Loss 12.069066, Accuracy 49.315%\n",
      "Epoch 5, Batch 535, LR 0.000098 Loss 12.069453, Accuracy 49.312%\n",
      "Epoch 5, Batch 536, LR 0.000098 Loss 12.068719, Accuracy 49.319%\n",
      "Epoch 5, Batch 537, LR 0.000098 Loss 12.068330, Accuracy 49.321%\n",
      "Epoch 5, Batch 538, LR 0.000098 Loss 12.067776, Accuracy 49.333%\n",
      "Epoch 5, Batch 539, LR 0.000098 Loss 12.069748, Accuracy 49.309%\n",
      "Epoch 5, Batch 540, LR 0.000098 Loss 12.069765, Accuracy 49.308%\n",
      "Epoch 5, Batch 541, LR 0.000098 Loss 12.069737, Accuracy 49.307%\n",
      "Epoch 5, Batch 542, LR 0.000098 Loss 12.068914, Accuracy 49.317%\n",
      "Epoch 5, Batch 543, LR 0.000098 Loss 12.068569, Accuracy 49.321%\n",
      "Epoch 5, Batch 544, LR 0.000098 Loss 12.068731, Accuracy 49.308%\n",
      "Epoch 5, Batch 545, LR 0.000098 Loss 12.070307, Accuracy 49.299%\n",
      "Epoch 5, Batch 546, LR 0.000098 Loss 12.071078, Accuracy 49.303%\n",
      "Epoch 5, Batch 547, LR 0.000098 Loss 12.071540, Accuracy 49.294%\n",
      "Epoch 5, Batch 548, LR 0.000098 Loss 12.071418, Accuracy 49.289%\n",
      "Epoch 5, Batch 549, LR 0.000098 Loss 12.071186, Accuracy 49.294%\n",
      "Epoch 5, Batch 550, LR 0.000098 Loss 12.070225, Accuracy 49.288%\n",
      "Epoch 5, Batch 551, LR 0.000098 Loss 12.069168, Accuracy 49.297%\n",
      "Epoch 5, Batch 552, LR 0.000098 Loss 12.068069, Accuracy 49.309%\n",
      "Epoch 5, Batch 553, LR 0.000098 Loss 12.067935, Accuracy 49.316%\n",
      "Epoch 5, Batch 554, LR 0.000098 Loss 12.068965, Accuracy 49.298%\n",
      "Epoch 5, Batch 555, LR 0.000098 Loss 12.068147, Accuracy 49.295%\n",
      "Epoch 5, Batch 556, LR 0.000098 Loss 12.067765, Accuracy 49.297%\n",
      "Epoch 5, Batch 557, LR 0.000098 Loss 12.067064, Accuracy 49.303%\n",
      "Epoch 5, Batch 558, LR 0.000098 Loss 12.067610, Accuracy 49.301%\n",
      "Epoch 5, Batch 559, LR 0.000098 Loss 12.068080, Accuracy 49.312%\n",
      "Epoch 5, Batch 560, LR 0.000098 Loss 12.067100, Accuracy 49.323%\n",
      "Epoch 5, Batch 561, LR 0.000098 Loss 12.065628, Accuracy 49.337%\n",
      "Epoch 5, Batch 562, LR 0.000098 Loss 12.066939, Accuracy 49.330%\n",
      "Epoch 5, Batch 563, LR 0.000098 Loss 12.065479, Accuracy 49.337%\n",
      "Epoch 5, Batch 564, LR 0.000098 Loss 12.066102, Accuracy 49.327%\n",
      "Epoch 5, Batch 565, LR 0.000098 Loss 12.066425, Accuracy 49.321%\n",
      "Epoch 5, Batch 566, LR 0.000098 Loss 12.065603, Accuracy 49.321%\n",
      "Epoch 5, Batch 567, LR 0.000098 Loss 12.064484, Accuracy 49.329%\n",
      "Epoch 5, Batch 568, LR 0.000098 Loss 12.064055, Accuracy 49.336%\n",
      "Epoch 5, Batch 569, LR 0.000098 Loss 12.064105, Accuracy 49.342%\n",
      "Epoch 5, Batch 570, LR 0.000098 Loss 12.064104, Accuracy 49.333%\n",
      "Epoch 5, Batch 571, LR 0.000098 Loss 12.063862, Accuracy 49.320%\n",
      "Epoch 5, Batch 572, LR 0.000098 Loss 12.063179, Accuracy 49.323%\n",
      "Epoch 5, Batch 573, LR 0.000098 Loss 12.063070, Accuracy 49.322%\n",
      "Epoch 5, Batch 574, LR 0.000098 Loss 12.063116, Accuracy 49.322%\n",
      "Epoch 5, Batch 575, LR 0.000098 Loss 12.062185, Accuracy 49.336%\n",
      "Epoch 5, Batch 576, LR 0.000098 Loss 12.062739, Accuracy 49.330%\n",
      "Epoch 5, Batch 577, LR 0.000098 Loss 12.063230, Accuracy 49.332%\n",
      "Epoch 5, Batch 578, LR 0.000098 Loss 12.062250, Accuracy 49.351%\n",
      "Epoch 5, Batch 579, LR 0.000098 Loss 12.063108, Accuracy 49.342%\n",
      "Epoch 5, Batch 580, LR 0.000098 Loss 12.062073, Accuracy 49.353%\n",
      "Epoch 5, Batch 581, LR 0.000098 Loss 12.061534, Accuracy 49.361%\n",
      "Epoch 5, Batch 582, LR 0.000098 Loss 12.060842, Accuracy 49.368%\n",
      "Epoch 5, Batch 583, LR 0.000098 Loss 12.059367, Accuracy 49.385%\n",
      "Epoch 5, Batch 584, LR 0.000098 Loss 12.058966, Accuracy 49.381%\n",
      "Epoch 5, Batch 585, LR 0.000098 Loss 12.057562, Accuracy 49.402%\n",
      "Epoch 5, Batch 586, LR 0.000098 Loss 12.058047, Accuracy 49.397%\n",
      "Epoch 5, Batch 587, LR 0.000098 Loss 12.056522, Accuracy 49.409%\n",
      "Epoch 5, Batch 588, LR 0.000098 Loss 12.056826, Accuracy 49.409%\n",
      "Epoch 5, Batch 589, LR 0.000098 Loss 12.056586, Accuracy 49.411%\n",
      "Epoch 5, Batch 590, LR 0.000098 Loss 12.056101, Accuracy 49.415%\n",
      "Epoch 5, Batch 591, LR 0.000098 Loss 12.055363, Accuracy 49.417%\n",
      "Epoch 5, Batch 592, LR 0.000098 Loss 12.054818, Accuracy 49.417%\n",
      "Epoch 5, Batch 593, LR 0.000098 Loss 12.054839, Accuracy 49.410%\n",
      "Epoch 5, Batch 594, LR 0.000098 Loss 12.054945, Accuracy 49.413%\n",
      "Epoch 5, Batch 595, LR 0.000098 Loss 12.055117, Accuracy 49.410%\n",
      "Epoch 5, Batch 596, LR 0.000098 Loss 12.053653, Accuracy 49.426%\n",
      "Epoch 5, Batch 597, LR 0.000098 Loss 12.053952, Accuracy 49.418%\n",
      "Epoch 5, Batch 598, LR 0.000098 Loss 12.054115, Accuracy 49.420%\n",
      "Epoch 5, Batch 599, LR 0.000098 Loss 12.053168, Accuracy 49.430%\n",
      "Epoch 5, Batch 600, LR 0.000098 Loss 12.053190, Accuracy 49.424%\n",
      "Epoch 5, Batch 601, LR 0.000098 Loss 12.052815, Accuracy 49.435%\n",
      "Epoch 5, Batch 602, LR 0.000098 Loss 12.052348, Accuracy 49.434%\n",
      "Epoch 5, Batch 603, LR 0.000098 Loss 12.052491, Accuracy 49.434%\n",
      "Epoch 5, Batch 604, LR 0.000098 Loss 12.052797, Accuracy 49.437%\n",
      "Epoch 5, Batch 605, LR 0.000098 Loss 12.052549, Accuracy 49.441%\n",
      "Epoch 5, Batch 606, LR 0.000098 Loss 12.053110, Accuracy 49.433%\n",
      "Epoch 5, Batch 607, LR 0.000098 Loss 12.052168, Accuracy 49.447%\n",
      "Epoch 5, Batch 608, LR 0.000098 Loss 12.051674, Accuracy 49.451%\n",
      "Epoch 5, Batch 609, LR 0.000098 Loss 12.050778, Accuracy 49.468%\n",
      "Epoch 5, Batch 610, LR 0.000098 Loss 12.051546, Accuracy 49.463%\n",
      "Epoch 5, Batch 611, LR 0.000098 Loss 12.049177, Accuracy 49.485%\n",
      "Epoch 5, Batch 612, LR 0.000098 Loss 12.048006, Accuracy 49.500%\n",
      "Epoch 5, Batch 613, LR 0.000098 Loss 12.047520, Accuracy 49.504%\n",
      "Epoch 5, Batch 614, LR 0.000098 Loss 12.045999, Accuracy 49.518%\n",
      "Epoch 5, Batch 615, LR 0.000098 Loss 12.046899, Accuracy 49.511%\n",
      "Epoch 5, Batch 616, LR 0.000098 Loss 12.046202, Accuracy 49.516%\n",
      "Epoch 5, Batch 617, LR 0.000098 Loss 12.044978, Accuracy 49.523%\n",
      "Epoch 5, Batch 618, LR 0.000098 Loss 12.044179, Accuracy 49.532%\n",
      "Epoch 5, Batch 619, LR 0.000098 Loss 12.044993, Accuracy 49.519%\n",
      "Epoch 5, Batch 620, LR 0.000098 Loss 12.044946, Accuracy 49.515%\n",
      "Epoch 5, Batch 621, LR 0.000098 Loss 12.043546, Accuracy 49.523%\n",
      "Epoch 5, Batch 622, LR 0.000098 Loss 12.042049, Accuracy 49.537%\n",
      "Epoch 5, Batch 623, LR 0.000098 Loss 12.041530, Accuracy 49.540%\n",
      "Epoch 5, Batch 624, LR 0.000098 Loss 12.041410, Accuracy 49.541%\n",
      "Epoch 5, Batch 625, LR 0.000098 Loss 12.041476, Accuracy 49.540%\n",
      "Epoch 5, Batch 626, LR 0.000098 Loss 12.041196, Accuracy 49.543%\n",
      "Epoch 5, Batch 627, LR 0.000098 Loss 12.040976, Accuracy 49.553%\n",
      "Epoch 5, Batch 628, LR 0.000098 Loss 12.041052, Accuracy 49.548%\n",
      "Epoch 5, Batch 629, LR 0.000098 Loss 12.041090, Accuracy 49.555%\n",
      "Epoch 5, Batch 630, LR 0.000098 Loss 12.040704, Accuracy 49.561%\n",
      "Epoch 5, Batch 631, LR 0.000098 Loss 12.040968, Accuracy 49.557%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 632, LR 0.000098 Loss 12.040229, Accuracy 49.557%\n",
      "Epoch 5, Batch 633, LR 0.000098 Loss 12.039423, Accuracy 49.562%\n",
      "Epoch 5, Batch 634, LR 0.000098 Loss 12.038344, Accuracy 49.567%\n",
      "Epoch 5, Batch 635, LR 0.000098 Loss 12.037862, Accuracy 49.571%\n",
      "Epoch 5, Batch 636, LR 0.000098 Loss 12.036961, Accuracy 49.575%\n",
      "Epoch 5, Batch 637, LR 0.000098 Loss 12.037489, Accuracy 49.571%\n",
      "Epoch 5, Batch 638, LR 0.000098 Loss 12.037645, Accuracy 49.569%\n",
      "Epoch 5, Batch 639, LR 0.000098 Loss 12.037562, Accuracy 49.575%\n",
      "Epoch 5, Batch 640, LR 0.000098 Loss 12.036657, Accuracy 49.587%\n",
      "Epoch 5, Batch 641, LR 0.000098 Loss 12.036569, Accuracy 49.594%\n",
      "Epoch 5, Batch 642, LR 0.000098 Loss 12.035364, Accuracy 49.608%\n",
      "Epoch 5, Batch 643, LR 0.000098 Loss 12.035138, Accuracy 49.612%\n",
      "Epoch 5, Batch 644, LR 0.000098 Loss 12.034601, Accuracy 49.619%\n",
      "Epoch 5, Batch 645, LR 0.000098 Loss 12.034840, Accuracy 49.608%\n",
      "Epoch 5, Batch 646, LR 0.000098 Loss 12.034951, Accuracy 49.600%\n",
      "Epoch 5, Batch 647, LR 0.000098 Loss 12.035858, Accuracy 49.597%\n",
      "Epoch 5, Batch 648, LR 0.000098 Loss 12.035394, Accuracy 49.596%\n",
      "Epoch 5, Batch 649, LR 0.000098 Loss 12.034743, Accuracy 49.600%\n",
      "Epoch 5, Batch 650, LR 0.000098 Loss 12.035074, Accuracy 49.603%\n",
      "Epoch 5, Batch 651, LR 0.000098 Loss 12.035422, Accuracy 49.600%\n",
      "Epoch 5, Batch 652, LR 0.000098 Loss 12.033997, Accuracy 49.614%\n",
      "Epoch 5, Batch 653, LR 0.000098 Loss 12.034199, Accuracy 49.611%\n",
      "Epoch 5, Batch 654, LR 0.000098 Loss 12.033431, Accuracy 49.619%\n",
      "Epoch 5, Batch 655, LR 0.000098 Loss 12.033153, Accuracy 49.620%\n",
      "Epoch 5, Batch 656, LR 0.000098 Loss 12.033105, Accuracy 49.626%\n",
      "Epoch 5, Batch 657, LR 0.000098 Loss 12.033055, Accuracy 49.622%\n",
      "Epoch 5, Batch 658, LR 0.000098 Loss 12.031855, Accuracy 49.634%\n",
      "Epoch 5, Batch 659, LR 0.000098 Loss 12.031658, Accuracy 49.630%\n",
      "Epoch 5, Batch 660, LR 0.000098 Loss 12.030514, Accuracy 49.644%\n",
      "Epoch 5, Batch 661, LR 0.000098 Loss 12.029281, Accuracy 49.656%\n",
      "Epoch 5, Batch 662, LR 0.000098 Loss 12.028446, Accuracy 49.668%\n",
      "Epoch 5, Batch 663, LR 0.000098 Loss 12.027633, Accuracy 49.669%\n",
      "Epoch 5, Batch 664, LR 0.000098 Loss 12.027881, Accuracy 49.667%\n",
      "Epoch 5, Batch 665, LR 0.000098 Loss 12.028245, Accuracy 49.668%\n",
      "Epoch 5, Batch 666, LR 0.000098 Loss 12.028750, Accuracy 49.661%\n",
      "Epoch 5, Batch 667, LR 0.000098 Loss 12.028492, Accuracy 49.664%\n",
      "Epoch 5, Batch 668, LR 0.000098 Loss 12.027714, Accuracy 49.668%\n",
      "Epoch 5, Batch 669, LR 0.000098 Loss 12.026738, Accuracy 49.672%\n",
      "Epoch 5, Batch 670, LR 0.000098 Loss 12.026482, Accuracy 49.676%\n",
      "Epoch 5, Batch 671, LR 0.000098 Loss 12.026243, Accuracy 49.680%\n",
      "Epoch 5, Batch 672, LR 0.000098 Loss 12.026949, Accuracy 49.663%\n",
      "Epoch 5, Batch 673, LR 0.000098 Loss 12.025374, Accuracy 49.674%\n",
      "Epoch 5, Batch 674, LR 0.000098 Loss 12.025974, Accuracy 49.663%\n",
      "Epoch 5, Batch 675, LR 0.000098 Loss 12.025335, Accuracy 49.666%\n",
      "Epoch 5, Batch 676, LR 0.000098 Loss 12.025643, Accuracy 49.654%\n",
      "Epoch 5, Batch 677, LR 0.000098 Loss 12.024335, Accuracy 49.665%\n",
      "Epoch 5, Batch 678, LR 0.000098 Loss 12.025041, Accuracy 49.662%\n",
      "Epoch 5, Batch 679, LR 0.000098 Loss 12.024718, Accuracy 49.664%\n",
      "Epoch 5, Batch 680, LR 0.000098 Loss 12.024101, Accuracy 49.663%\n",
      "Epoch 5, Batch 681, LR 0.000098 Loss 12.023913, Accuracy 49.659%\n",
      "Epoch 5, Batch 682, LR 0.000098 Loss 12.024852, Accuracy 49.647%\n",
      "Epoch 5, Batch 683, LR 0.000098 Loss 12.025292, Accuracy 49.642%\n",
      "Epoch 5, Batch 684, LR 0.000098 Loss 12.025297, Accuracy 49.641%\n",
      "Epoch 5, Batch 685, LR 0.000098 Loss 12.024201, Accuracy 49.653%\n",
      "Epoch 5, Batch 686, LR 0.000098 Loss 12.022829, Accuracy 49.670%\n",
      "Epoch 5, Batch 687, LR 0.000098 Loss 12.021551, Accuracy 49.684%\n",
      "Epoch 5, Batch 688, LR 0.000098 Loss 12.021705, Accuracy 49.682%\n",
      "Epoch 5, Batch 689, LR 0.000098 Loss 12.021034, Accuracy 49.684%\n",
      "Epoch 5, Batch 690, LR 0.000098 Loss 12.021113, Accuracy 49.684%\n",
      "Epoch 5, Batch 691, LR 0.000098 Loss 12.020817, Accuracy 49.686%\n",
      "Epoch 5, Batch 692, LR 0.000098 Loss 12.020134, Accuracy 49.691%\n",
      "Epoch 5, Batch 693, LR 0.000098 Loss 12.019477, Accuracy 49.705%\n",
      "Epoch 5, Batch 694, LR 0.000098 Loss 12.019136, Accuracy 49.706%\n",
      "Epoch 5, Batch 695, LR 0.000098 Loss 12.019294, Accuracy 49.704%\n",
      "Epoch 5, Batch 696, LR 0.000098 Loss 12.018350, Accuracy 49.708%\n",
      "Epoch 5, Batch 697, LR 0.000098 Loss 12.017841, Accuracy 49.712%\n",
      "Epoch 5, Batch 698, LR 0.000098 Loss 12.018189, Accuracy 49.707%\n",
      "Epoch 5, Batch 699, LR 0.000098 Loss 12.017808, Accuracy 49.700%\n",
      "Epoch 5, Batch 700, LR 0.000098 Loss 12.018654, Accuracy 49.696%\n",
      "Epoch 5, Batch 701, LR 0.000098 Loss 12.018661, Accuracy 49.695%\n",
      "Epoch 5, Batch 702, LR 0.000098 Loss 12.018728, Accuracy 49.683%\n",
      "Epoch 5, Batch 703, LR 0.000098 Loss 12.018433, Accuracy 49.687%\n",
      "Epoch 5, Batch 704, LR 0.000098 Loss 12.017586, Accuracy 49.691%\n",
      "Epoch 5, Batch 705, LR 0.000098 Loss 12.016801, Accuracy 49.700%\n",
      "Epoch 5, Batch 706, LR 0.000098 Loss 12.016063, Accuracy 49.708%\n",
      "Epoch 5, Batch 707, LR 0.000098 Loss 12.015710, Accuracy 49.710%\n",
      "Epoch 5, Batch 708, LR 0.000098 Loss 12.015272, Accuracy 49.713%\n",
      "Epoch 5, Batch 709, LR 0.000098 Loss 12.013418, Accuracy 49.732%\n",
      "Epoch 5, Batch 710, LR 0.000098 Loss 12.013383, Accuracy 49.730%\n",
      "Epoch 5, Batch 711, LR 0.000098 Loss 12.012642, Accuracy 49.738%\n",
      "Epoch 5, Batch 712, LR 0.000098 Loss 12.012008, Accuracy 49.737%\n",
      "Epoch 5, Batch 713, LR 0.000098 Loss 12.011049, Accuracy 49.738%\n",
      "Epoch 5, Batch 714, LR 0.000098 Loss 12.010899, Accuracy 49.737%\n",
      "Epoch 5, Batch 715, LR 0.000098 Loss 12.010491, Accuracy 49.733%\n",
      "Epoch 5, Batch 716, LR 0.000098 Loss 12.009791, Accuracy 49.735%\n",
      "Epoch 5, Batch 717, LR 0.000098 Loss 12.008738, Accuracy 49.745%\n",
      "Epoch 5, Batch 718, LR 0.000098 Loss 12.007512, Accuracy 49.751%\n",
      "Epoch 5, Batch 719, LR 0.000098 Loss 12.007511, Accuracy 49.751%\n",
      "Epoch 5, Batch 720, LR 0.000098 Loss 12.007965, Accuracy 49.740%\n",
      "Epoch 5, Batch 721, LR 0.000098 Loss 12.007981, Accuracy 49.735%\n",
      "Epoch 5, Batch 722, LR 0.000098 Loss 12.008218, Accuracy 49.731%\n",
      "Epoch 5, Batch 723, LR 0.000098 Loss 12.008092, Accuracy 49.724%\n",
      "Epoch 5, Batch 724, LR 0.000098 Loss 12.008441, Accuracy 49.721%\n",
      "Epoch 5, Batch 725, LR 0.000098 Loss 12.008441, Accuracy 49.721%\n",
      "Epoch 5, Batch 726, LR 0.000098 Loss 12.008205, Accuracy 49.728%\n",
      "Epoch 5, Batch 727, LR 0.000098 Loss 12.007306, Accuracy 49.738%\n",
      "Epoch 5, Batch 728, LR 0.000098 Loss 12.007315, Accuracy 49.735%\n",
      "Epoch 5, Batch 729, LR 0.000098 Loss 12.006748, Accuracy 49.740%\n",
      "Epoch 5, Batch 730, LR 0.000098 Loss 12.005763, Accuracy 49.738%\n",
      "Epoch 5, Batch 731, LR 0.000098 Loss 12.005955, Accuracy 49.737%\n",
      "Epoch 5, Batch 732, LR 0.000098 Loss 12.006481, Accuracy 49.735%\n",
      "Epoch 5, Batch 733, LR 0.000098 Loss 12.004927, Accuracy 49.746%\n",
      "Epoch 5, Batch 734, LR 0.000098 Loss 12.005370, Accuracy 49.735%\n",
      "Epoch 5, Batch 735, LR 0.000098 Loss 12.004593, Accuracy 49.747%\n",
      "Epoch 5, Batch 736, LR 0.000098 Loss 12.004591, Accuracy 49.755%\n",
      "Epoch 5, Batch 737, LR 0.000098 Loss 12.004215, Accuracy 49.752%\n",
      "Epoch 5, Batch 738, LR 0.000098 Loss 12.003233, Accuracy 49.769%\n",
      "Epoch 5, Batch 739, LR 0.000098 Loss 12.003738, Accuracy 49.766%\n",
      "Epoch 5, Batch 740, LR 0.000098 Loss 12.003042, Accuracy 49.776%\n",
      "Epoch 5, Batch 741, LR 0.000098 Loss 12.003073, Accuracy 49.778%\n",
      "Epoch 5, Batch 742, LR 0.000098 Loss 12.002756, Accuracy 49.779%\n",
      "Epoch 5, Batch 743, LR 0.000098 Loss 12.001949, Accuracy 49.781%\n",
      "Epoch 5, Batch 744, LR 0.000098 Loss 12.001066, Accuracy 49.785%\n",
      "Epoch 5, Batch 745, LR 0.000098 Loss 12.000529, Accuracy 49.797%\n",
      "Epoch 5, Batch 746, LR 0.000098 Loss 11.999171, Accuracy 49.804%\n",
      "Epoch 5, Batch 747, LR 0.000098 Loss 11.998302, Accuracy 49.818%\n",
      "Epoch 5, Batch 748, LR 0.000098 Loss 11.998116, Accuracy 49.820%\n",
      "Epoch 5, Batch 749, LR 0.000098 Loss 11.997848, Accuracy 49.822%\n",
      "Epoch 5, Batch 750, LR 0.000098 Loss 11.997670, Accuracy 49.827%\n",
      "Epoch 5, Batch 751, LR 0.000098 Loss 11.996335, Accuracy 49.836%\n",
      "Epoch 5, Batch 752, LR 0.000098 Loss 11.995289, Accuracy 49.842%\n",
      "Epoch 5, Batch 753, LR 0.000098 Loss 11.994796, Accuracy 49.838%\n",
      "Epoch 5, Batch 754, LR 0.000098 Loss 11.994343, Accuracy 49.841%\n",
      "Epoch 5, Batch 755, LR 0.000098 Loss 11.994340, Accuracy 49.838%\n",
      "Epoch 5, Batch 756, LR 0.000098 Loss 11.994932, Accuracy 49.843%\n",
      "Epoch 5, Batch 757, LR 0.000098 Loss 11.993473, Accuracy 49.851%\n",
      "Epoch 5, Batch 758, LR 0.000098 Loss 11.992877, Accuracy 49.855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 759, LR 0.000098 Loss 11.991739, Accuracy 49.865%\n",
      "Epoch 5, Batch 760, LR 0.000098 Loss 11.991782, Accuracy 49.865%\n",
      "Epoch 5, Batch 761, LR 0.000098 Loss 11.990406, Accuracy 49.879%\n",
      "Epoch 5, Batch 762, LR 0.000098 Loss 11.989855, Accuracy 49.893%\n",
      "Epoch 5, Batch 763, LR 0.000098 Loss 11.989280, Accuracy 49.897%\n",
      "Epoch 5, Batch 764, LR 0.000098 Loss 11.988801, Accuracy 49.894%\n",
      "Epoch 5, Batch 765, LR 0.000098 Loss 11.988959, Accuracy 49.888%\n",
      "Epoch 5, Batch 766, LR 0.000098 Loss 11.988353, Accuracy 49.890%\n",
      "Epoch 5, Batch 767, LR 0.000098 Loss 11.988204, Accuracy 49.888%\n",
      "Epoch 5, Batch 768, LR 0.000098 Loss 11.988111, Accuracy 49.883%\n",
      "Epoch 5, Batch 769, LR 0.000098 Loss 11.988397, Accuracy 49.884%\n",
      "Epoch 5, Batch 770, LR 0.000098 Loss 11.988513, Accuracy 49.884%\n",
      "Epoch 5, Batch 771, LR 0.000098 Loss 11.988448, Accuracy 49.879%\n",
      "Epoch 5, Batch 772, LR 0.000098 Loss 11.987978, Accuracy 49.882%\n",
      "Epoch 5, Batch 773, LR 0.000098 Loss 11.988101, Accuracy 49.878%\n",
      "Epoch 5, Batch 774, LR 0.000098 Loss 11.987852, Accuracy 49.882%\n",
      "Epoch 5, Batch 775, LR 0.000098 Loss 11.987603, Accuracy 49.892%\n",
      "Epoch 5, Batch 776, LR 0.000098 Loss 11.986498, Accuracy 49.908%\n",
      "Epoch 5, Batch 777, LR 0.000098 Loss 11.986781, Accuracy 49.903%\n",
      "Epoch 5, Batch 778, LR 0.000098 Loss 11.986538, Accuracy 49.907%\n",
      "Epoch 5, Batch 779, LR 0.000098 Loss 11.986547, Accuracy 49.904%\n",
      "Epoch 5, Batch 780, LR 0.000098 Loss 11.986192, Accuracy 49.910%\n",
      "Epoch 5, Batch 781, LR 0.000098 Loss 11.984525, Accuracy 49.918%\n",
      "Epoch 5, Batch 782, LR 0.000098 Loss 11.984529, Accuracy 49.912%\n",
      "Epoch 5, Batch 783, LR 0.000098 Loss 11.983918, Accuracy 49.917%\n",
      "Epoch 5, Batch 784, LR 0.000098 Loss 11.983170, Accuracy 49.926%\n",
      "Epoch 5, Batch 785, LR 0.000098 Loss 11.982624, Accuracy 49.930%\n",
      "Epoch 5, Batch 786, LR 0.000098 Loss 11.982175, Accuracy 49.929%\n",
      "Epoch 5, Batch 787, LR 0.000098 Loss 11.981142, Accuracy 49.930%\n",
      "Epoch 5, Batch 788, LR 0.000098 Loss 11.980314, Accuracy 49.933%\n",
      "Epoch 5, Batch 789, LR 0.000098 Loss 11.980246, Accuracy 49.931%\n",
      "Epoch 5, Batch 790, LR 0.000098 Loss 11.980616, Accuracy 49.924%\n",
      "Epoch 5, Batch 791, LR 0.000098 Loss 11.980045, Accuracy 49.925%\n",
      "Epoch 5, Batch 792, LR 0.000098 Loss 11.978831, Accuracy 49.937%\n",
      "Epoch 5, Batch 793, LR 0.000098 Loss 11.978052, Accuracy 49.942%\n",
      "Epoch 5, Batch 794, LR 0.000098 Loss 11.978323, Accuracy 49.939%\n",
      "Epoch 5, Batch 795, LR 0.000098 Loss 11.977971, Accuracy 49.939%\n",
      "Epoch 5, Batch 796, LR 0.000098 Loss 11.978138, Accuracy 49.935%\n",
      "Epoch 5, Batch 797, LR 0.000098 Loss 11.978294, Accuracy 49.929%\n",
      "Epoch 5, Batch 798, LR 0.000098 Loss 11.977154, Accuracy 49.934%\n",
      "Epoch 5, Batch 799, LR 0.000098 Loss 11.976391, Accuracy 49.937%\n",
      "Epoch 5, Batch 800, LR 0.000098 Loss 11.977072, Accuracy 49.930%\n",
      "Epoch 5, Batch 801, LR 0.000098 Loss 11.976328, Accuracy 49.934%\n",
      "Epoch 5, Batch 802, LR 0.000098 Loss 11.976071, Accuracy 49.931%\n",
      "Epoch 5, Batch 803, LR 0.000098 Loss 11.975031, Accuracy 49.932%\n",
      "Epoch 5, Batch 804, LR 0.000098 Loss 11.975013, Accuracy 49.930%\n",
      "Epoch 5, Batch 805, LR 0.000098 Loss 11.975476, Accuracy 49.923%\n",
      "Epoch 5, Batch 806, LR 0.000098 Loss 11.975426, Accuracy 49.922%\n",
      "Epoch 5, Batch 807, LR 0.000098 Loss 11.974313, Accuracy 49.927%\n",
      "Epoch 5, Batch 808, LR 0.000098 Loss 11.972466, Accuracy 49.943%\n",
      "Epoch 5, Batch 809, LR 0.000098 Loss 11.972801, Accuracy 49.942%\n",
      "Epoch 5, Batch 810, LR 0.000098 Loss 11.972478, Accuracy 49.943%\n",
      "Epoch 5, Batch 811, LR 0.000098 Loss 11.972593, Accuracy 49.940%\n",
      "Epoch 5, Batch 812, LR 0.000098 Loss 11.971634, Accuracy 49.949%\n",
      "Epoch 5, Batch 813, LR 0.000098 Loss 11.970057, Accuracy 49.963%\n",
      "Epoch 5, Batch 814, LR 0.000098 Loss 11.969103, Accuracy 49.977%\n",
      "Epoch 5, Batch 815, LR 0.000098 Loss 11.969634, Accuracy 49.973%\n",
      "Epoch 5, Batch 816, LR 0.000098 Loss 11.969863, Accuracy 49.972%\n",
      "Epoch 5, Batch 817, LR 0.000098 Loss 11.969413, Accuracy 49.977%\n",
      "Epoch 5, Batch 818, LR 0.000098 Loss 11.969000, Accuracy 49.981%\n",
      "Epoch 5, Batch 819, LR 0.000098 Loss 11.969173, Accuracy 49.985%\n",
      "Epoch 5, Batch 820, LR 0.000098 Loss 11.969400, Accuracy 49.985%\n",
      "Epoch 5, Batch 821, LR 0.000098 Loss 11.969370, Accuracy 49.984%\n",
      "Epoch 5, Batch 822, LR 0.000098 Loss 11.969062, Accuracy 49.986%\n",
      "Epoch 5, Batch 823, LR 0.000098 Loss 11.968764, Accuracy 49.988%\n",
      "Epoch 5, Batch 824, LR 0.000098 Loss 11.968664, Accuracy 49.992%\n",
      "Epoch 5, Batch 825, LR 0.000098 Loss 11.968764, Accuracy 49.990%\n",
      "Epoch 5, Batch 826, LR 0.000098 Loss 11.967968, Accuracy 49.997%\n",
      "Epoch 5, Batch 827, LR 0.000098 Loss 11.967182, Accuracy 50.007%\n",
      "Epoch 5, Batch 828, LR 0.000098 Loss 11.967667, Accuracy 50.003%\n",
      "Epoch 5, Batch 829, LR 0.000098 Loss 11.967069, Accuracy 50.007%\n",
      "Epoch 5, Batch 830, LR 0.000098 Loss 11.967245, Accuracy 50.008%\n",
      "Epoch 5, Batch 831, LR 0.000098 Loss 11.966524, Accuracy 50.017%\n",
      "Epoch 5, Batch 832, LR 0.000098 Loss 11.964867, Accuracy 50.027%\n",
      "Epoch 5, Batch 833, LR 0.000098 Loss 11.964204, Accuracy 50.036%\n",
      "Epoch 5, Batch 834, LR 0.000098 Loss 11.963298, Accuracy 50.043%\n",
      "Epoch 5, Batch 835, LR 0.000098 Loss 11.963247, Accuracy 50.043%\n",
      "Epoch 5, Batch 836, LR 0.000098 Loss 11.962027, Accuracy 50.050%\n",
      "Epoch 5, Batch 837, LR 0.000098 Loss 11.961756, Accuracy 50.051%\n",
      "Epoch 5, Batch 838, LR 0.000098 Loss 11.961243, Accuracy 50.062%\n",
      "Epoch 5, Batch 839, LR 0.000098 Loss 11.959998, Accuracy 50.073%\n",
      "Epoch 5, Batch 840, LR 0.000098 Loss 11.960294, Accuracy 50.060%\n",
      "Epoch 5, Batch 841, LR 0.000098 Loss 11.960405, Accuracy 50.064%\n",
      "Epoch 5, Batch 842, LR 0.000098 Loss 11.959146, Accuracy 50.072%\n",
      "Epoch 5, Batch 843, LR 0.000098 Loss 11.959055, Accuracy 50.075%\n",
      "Epoch 5, Batch 844, LR 0.000098 Loss 11.958655, Accuracy 50.078%\n",
      "Epoch 5, Batch 845, LR 0.000098 Loss 11.957605, Accuracy 50.081%\n",
      "Epoch 5, Batch 846, LR 0.000098 Loss 11.957635, Accuracy 50.082%\n",
      "Epoch 5, Batch 847, LR 0.000098 Loss 11.957777, Accuracy 50.078%\n",
      "Epoch 5, Batch 848, LR 0.000098 Loss 11.957078, Accuracy 50.087%\n",
      "Epoch 5, Batch 849, LR 0.000098 Loss 11.957333, Accuracy 50.087%\n",
      "Epoch 5, Batch 850, LR 0.000098 Loss 11.956879, Accuracy 50.092%\n",
      "Epoch 5, Batch 851, LR 0.000098 Loss 11.956878, Accuracy 50.095%\n",
      "Epoch 5, Batch 852, LR 0.000098 Loss 11.956439, Accuracy 50.093%\n",
      "Epoch 5, Batch 853, LR 0.000098 Loss 11.956581, Accuracy 50.093%\n",
      "Epoch 5, Batch 854, LR 0.000098 Loss 11.955770, Accuracy 50.097%\n",
      "Epoch 5, Batch 855, LR 0.000098 Loss 11.955236, Accuracy 50.102%\n",
      "Epoch 5, Batch 856, LR 0.000098 Loss 11.954696, Accuracy 50.114%\n",
      "Epoch 5, Batch 857, LR 0.000098 Loss 11.954235, Accuracy 50.119%\n",
      "Epoch 5, Batch 858, LR 0.000098 Loss 11.953694, Accuracy 50.119%\n",
      "Epoch 5, Batch 859, LR 0.000098 Loss 11.953626, Accuracy 50.114%\n",
      "Epoch 5, Batch 860, LR 0.000098 Loss 11.953856, Accuracy 50.115%\n",
      "Epoch 5, Batch 861, LR 0.000098 Loss 11.953623, Accuracy 50.113%\n",
      "Epoch 5, Batch 862, LR 0.000098 Loss 11.953476, Accuracy 50.118%\n",
      "Epoch 5, Batch 863, LR 0.000098 Loss 11.952727, Accuracy 50.124%\n",
      "Epoch 5, Batch 864, LR 0.000098 Loss 11.953060, Accuracy 50.121%\n",
      "Epoch 5, Batch 865, LR 0.000098 Loss 11.952319, Accuracy 50.129%\n",
      "Epoch 5, Batch 866, LR 0.000098 Loss 11.951517, Accuracy 50.141%\n",
      "Epoch 5, Batch 867, LR 0.000098 Loss 11.950686, Accuracy 50.146%\n",
      "Epoch 5, Batch 868, LR 0.000098 Loss 11.949636, Accuracy 50.154%\n",
      "Epoch 5, Batch 869, LR 0.000098 Loss 11.948786, Accuracy 50.161%\n",
      "Epoch 5, Batch 870, LR 0.000098 Loss 11.947608, Accuracy 50.169%\n",
      "Epoch 5, Batch 871, LR 0.000098 Loss 11.948441, Accuracy 50.169%\n",
      "Epoch 5, Batch 872, LR 0.000098 Loss 11.948025, Accuracy 50.172%\n",
      "Epoch 5, Batch 873, LR 0.000098 Loss 11.946351, Accuracy 50.184%\n",
      "Epoch 5, Batch 874, LR 0.000098 Loss 11.946073, Accuracy 50.178%\n",
      "Epoch 5, Batch 875, LR 0.000098 Loss 11.945413, Accuracy 50.185%\n",
      "Epoch 5, Batch 876, LR 0.000098 Loss 11.944874, Accuracy 50.187%\n",
      "Epoch 5, Batch 877, LR 0.000098 Loss 11.945283, Accuracy 50.183%\n",
      "Epoch 5, Batch 878, LR 0.000098 Loss 11.944872, Accuracy 50.186%\n",
      "Epoch 5, Batch 879, LR 0.000098 Loss 11.945026, Accuracy 50.183%\n",
      "Epoch 5, Batch 880, LR 0.000098 Loss 11.944875, Accuracy 50.186%\n",
      "Epoch 5, Batch 881, LR 0.000097 Loss 11.944981, Accuracy 50.184%\n",
      "Epoch 5, Batch 882, LR 0.000097 Loss 11.944342, Accuracy 50.185%\n",
      "Epoch 5, Batch 883, LR 0.000097 Loss 11.943977, Accuracy 50.198%\n",
      "Epoch 5, Batch 884, LR 0.000097 Loss 11.944353, Accuracy 50.197%\n",
      "Epoch 5, Batch 885, LR 0.000097 Loss 11.944228, Accuracy 50.200%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 886, LR 0.000097 Loss 11.943836, Accuracy 50.206%\n",
      "Epoch 5, Batch 887, LR 0.000097 Loss 11.943421, Accuracy 50.206%\n",
      "Epoch 5, Batch 888, LR 0.000097 Loss 11.943861, Accuracy 50.201%\n",
      "Epoch 5, Batch 889, LR 0.000097 Loss 11.943636, Accuracy 50.203%\n",
      "Epoch 5, Batch 890, LR 0.000097 Loss 11.943490, Accuracy 50.202%\n",
      "Epoch 5, Batch 891, LR 0.000097 Loss 11.943874, Accuracy 50.193%\n",
      "Epoch 5, Batch 892, LR 0.000097 Loss 11.943083, Accuracy 50.199%\n",
      "Epoch 5, Batch 893, LR 0.000097 Loss 11.941920, Accuracy 50.211%\n",
      "Epoch 5, Batch 894, LR 0.000097 Loss 11.941131, Accuracy 50.212%\n",
      "Epoch 5, Batch 895, LR 0.000097 Loss 11.941475, Accuracy 50.206%\n",
      "Epoch 5, Batch 896, LR 0.000097 Loss 11.941244, Accuracy 50.203%\n",
      "Epoch 5, Batch 897, LR 0.000097 Loss 11.940746, Accuracy 50.210%\n",
      "Epoch 5, Batch 898, LR 0.000097 Loss 11.939564, Accuracy 50.214%\n",
      "Epoch 5, Batch 899, LR 0.000097 Loss 11.938909, Accuracy 50.221%\n",
      "Epoch 5, Batch 900, LR 0.000097 Loss 11.939383, Accuracy 50.215%\n",
      "Epoch 5, Batch 901, LR 0.000097 Loss 11.940031, Accuracy 50.209%\n",
      "Epoch 5, Batch 902, LR 0.000097 Loss 11.940109, Accuracy 50.211%\n",
      "Epoch 5, Batch 903, LR 0.000097 Loss 11.939475, Accuracy 50.217%\n",
      "Epoch 5, Batch 904, LR 0.000097 Loss 11.938659, Accuracy 50.220%\n",
      "Epoch 5, Batch 905, LR 0.000097 Loss 11.938878, Accuracy 50.218%\n",
      "Epoch 5, Batch 906, LR 0.000097 Loss 11.938738, Accuracy 50.215%\n",
      "Epoch 5, Batch 907, LR 0.000097 Loss 11.938233, Accuracy 50.218%\n",
      "Epoch 5, Batch 908, LR 0.000097 Loss 11.938453, Accuracy 50.210%\n",
      "Epoch 5, Batch 909, LR 0.000097 Loss 11.938012, Accuracy 50.211%\n",
      "Epoch 5, Batch 910, LR 0.000097 Loss 11.937776, Accuracy 50.216%\n",
      "Epoch 5, Batch 911, LR 0.000097 Loss 11.937356, Accuracy 50.219%\n",
      "Epoch 5, Batch 912, LR 0.000097 Loss 11.936968, Accuracy 50.226%\n",
      "Epoch 5, Batch 913, LR 0.000097 Loss 11.936381, Accuracy 50.225%\n",
      "Epoch 5, Batch 914, LR 0.000097 Loss 11.935726, Accuracy 50.227%\n",
      "Epoch 5, Batch 915, LR 0.000097 Loss 11.935966, Accuracy 50.221%\n",
      "Epoch 5, Batch 916, LR 0.000097 Loss 11.934841, Accuracy 50.230%\n",
      "Epoch 5, Batch 917, LR 0.000097 Loss 11.933953, Accuracy 50.235%\n",
      "Epoch 5, Batch 918, LR 0.000097 Loss 11.933818, Accuracy 50.238%\n",
      "Epoch 5, Batch 919, LR 0.000097 Loss 11.933165, Accuracy 50.248%\n",
      "Epoch 5, Batch 920, LR 0.000097 Loss 11.932732, Accuracy 50.253%\n",
      "Epoch 5, Batch 921, LR 0.000097 Loss 11.932824, Accuracy 50.256%\n",
      "Epoch 5, Batch 922, LR 0.000097 Loss 11.932394, Accuracy 50.263%\n",
      "Epoch 5, Batch 923, LR 0.000097 Loss 11.931557, Accuracy 50.271%\n",
      "Epoch 5, Batch 924, LR 0.000097 Loss 11.931235, Accuracy 50.272%\n",
      "Epoch 5, Batch 925, LR 0.000097 Loss 11.931451, Accuracy 50.270%\n",
      "Epoch 5, Batch 926, LR 0.000097 Loss 11.930576, Accuracy 50.277%\n",
      "Epoch 5, Batch 927, LR 0.000097 Loss 11.929275, Accuracy 50.290%\n",
      "Epoch 5, Batch 928, LR 0.000097 Loss 11.928586, Accuracy 50.295%\n",
      "Epoch 5, Batch 929, LR 0.000097 Loss 11.927877, Accuracy 50.299%\n",
      "Epoch 5, Batch 930, LR 0.000097 Loss 11.928170, Accuracy 50.299%\n",
      "Epoch 5, Batch 931, LR 0.000097 Loss 11.927813, Accuracy 50.301%\n",
      "Epoch 5, Batch 932, LR 0.000097 Loss 11.927952, Accuracy 50.301%\n",
      "Epoch 5, Batch 933, LR 0.000097 Loss 11.927039, Accuracy 50.302%\n",
      "Epoch 5, Batch 934, LR 0.000097 Loss 11.927215, Accuracy 50.299%\n",
      "Epoch 5, Batch 935, LR 0.000097 Loss 11.926224, Accuracy 50.306%\n",
      "Epoch 5, Batch 936, LR 0.000097 Loss 11.926558, Accuracy 50.299%\n",
      "Epoch 5, Batch 937, LR 0.000097 Loss 11.925487, Accuracy 50.304%\n",
      "Epoch 5, Batch 938, LR 0.000097 Loss 11.923650, Accuracy 50.316%\n",
      "Epoch 5, Batch 939, LR 0.000097 Loss 11.924059, Accuracy 50.314%\n",
      "Epoch 5, Batch 940, LR 0.000097 Loss 11.923518, Accuracy 50.317%\n",
      "Epoch 5, Batch 941, LR 0.000097 Loss 11.923261, Accuracy 50.318%\n",
      "Epoch 5, Batch 942, LR 0.000097 Loss 11.922858, Accuracy 50.320%\n",
      "Epoch 5, Batch 943, LR 0.000097 Loss 11.922734, Accuracy 50.326%\n",
      "Epoch 5, Batch 944, LR 0.000097 Loss 11.921786, Accuracy 50.329%\n",
      "Epoch 5, Batch 945, LR 0.000097 Loss 11.921358, Accuracy 50.330%\n",
      "Epoch 5, Batch 946, LR 0.000097 Loss 11.921611, Accuracy 50.330%\n",
      "Epoch 5, Batch 947, LR 0.000097 Loss 11.921697, Accuracy 50.332%\n",
      "Epoch 5, Batch 948, LR 0.000097 Loss 11.921285, Accuracy 50.340%\n",
      "Epoch 5, Batch 949, LR 0.000097 Loss 11.920961, Accuracy 50.345%\n",
      "Epoch 5, Batch 950, LR 0.000097 Loss 11.920391, Accuracy 50.349%\n",
      "Epoch 5, Batch 951, LR 0.000097 Loss 11.919932, Accuracy 50.348%\n",
      "Epoch 5, Batch 952, LR 0.000097 Loss 11.920220, Accuracy 50.349%\n",
      "Epoch 5, Batch 953, LR 0.000097 Loss 11.919700, Accuracy 50.353%\n",
      "Epoch 5, Batch 954, LR 0.000097 Loss 11.919288, Accuracy 50.352%\n",
      "Epoch 5, Batch 955, LR 0.000097 Loss 11.918116, Accuracy 50.359%\n",
      "Epoch 5, Batch 956, LR 0.000097 Loss 11.917470, Accuracy 50.364%\n",
      "Epoch 5, Batch 957, LR 0.000097 Loss 11.917382, Accuracy 50.362%\n",
      "Epoch 5, Batch 958, LR 0.000097 Loss 11.915705, Accuracy 50.372%\n",
      "Epoch 5, Batch 959, LR 0.000097 Loss 11.916151, Accuracy 50.370%\n",
      "Epoch 5, Batch 960, LR 0.000097 Loss 11.915298, Accuracy 50.376%\n",
      "Epoch 5, Batch 961, LR 0.000097 Loss 11.915487, Accuracy 50.373%\n",
      "Epoch 5, Batch 962, LR 0.000097 Loss 11.915220, Accuracy 50.376%\n",
      "Epoch 5, Batch 963, LR 0.000097 Loss 11.914626, Accuracy 50.379%\n",
      "Epoch 5, Batch 964, LR 0.000097 Loss 11.913980, Accuracy 50.388%\n",
      "Epoch 5, Batch 965, LR 0.000097 Loss 11.913269, Accuracy 50.392%\n",
      "Epoch 5, Batch 966, LR 0.000097 Loss 11.912384, Accuracy 50.395%\n",
      "Epoch 5, Batch 967, LR 0.000097 Loss 11.911647, Accuracy 50.398%\n",
      "Epoch 5, Batch 968, LR 0.000097 Loss 11.911661, Accuracy 50.392%\n",
      "Epoch 5, Batch 969, LR 0.000097 Loss 11.912079, Accuracy 50.389%\n",
      "Epoch 5, Batch 970, LR 0.000097 Loss 11.911799, Accuracy 50.383%\n",
      "Epoch 5, Batch 971, LR 0.000097 Loss 11.910866, Accuracy 50.389%\n",
      "Epoch 5, Batch 972, LR 0.000097 Loss 11.910774, Accuracy 50.387%\n",
      "Epoch 5, Batch 973, LR 0.000097 Loss 11.910598, Accuracy 50.393%\n",
      "Epoch 5, Batch 974, LR 0.000097 Loss 11.910480, Accuracy 50.394%\n",
      "Epoch 5, Batch 975, LR 0.000097 Loss 11.910166, Accuracy 50.395%\n",
      "Epoch 5, Batch 976, LR 0.000097 Loss 11.909292, Accuracy 50.394%\n",
      "Epoch 5, Batch 977, LR 0.000097 Loss 11.908607, Accuracy 50.403%\n",
      "Epoch 5, Batch 978, LR 0.000097 Loss 11.907939, Accuracy 50.404%\n",
      "Epoch 5, Batch 979, LR 0.000097 Loss 11.907134, Accuracy 50.413%\n",
      "Epoch 5, Batch 980, LR 0.000097 Loss 11.906961, Accuracy 50.416%\n",
      "Epoch 5, Batch 981, LR 0.000097 Loss 11.907097, Accuracy 50.414%\n",
      "Epoch 5, Batch 982, LR 0.000097 Loss 11.906672, Accuracy 50.418%\n",
      "Epoch 5, Batch 983, LR 0.000097 Loss 11.905489, Accuracy 50.432%\n",
      "Epoch 5, Batch 984, LR 0.000097 Loss 11.905279, Accuracy 50.437%\n",
      "Epoch 5, Batch 985, LR 0.000097 Loss 11.906195, Accuracy 50.424%\n",
      "Epoch 5, Batch 986, LR 0.000097 Loss 11.906192, Accuracy 50.429%\n",
      "Epoch 5, Batch 987, LR 0.000097 Loss 11.906497, Accuracy 50.428%\n",
      "Epoch 5, Batch 988, LR 0.000097 Loss 11.906681, Accuracy 50.425%\n",
      "Epoch 5, Batch 989, LR 0.000097 Loss 11.906288, Accuracy 50.431%\n",
      "Epoch 5, Batch 990, LR 0.000097 Loss 11.906867, Accuracy 50.430%\n",
      "Epoch 5, Batch 991, LR 0.000097 Loss 11.906924, Accuracy 50.431%\n",
      "Epoch 5, Batch 992, LR 0.000097 Loss 11.906483, Accuracy 50.431%\n",
      "Epoch 5, Batch 993, LR 0.000097 Loss 11.905939, Accuracy 50.437%\n",
      "Epoch 5, Batch 994, LR 0.000097 Loss 11.905070, Accuracy 50.440%\n",
      "Epoch 5, Batch 995, LR 0.000097 Loss 11.905792, Accuracy 50.427%\n",
      "Epoch 5, Batch 996, LR 0.000097 Loss 11.905782, Accuracy 50.424%\n",
      "Epoch 5, Batch 997, LR 0.000097 Loss 11.905058, Accuracy 50.430%\n",
      "Epoch 5, Batch 998, LR 0.000097 Loss 11.904151, Accuracy 50.435%\n",
      "Epoch 5, Batch 999, LR 0.000097 Loss 11.903562, Accuracy 50.441%\n",
      "Epoch 5, Batch 1000, LR 0.000097 Loss 11.903209, Accuracy 50.443%\n",
      "Epoch 5, Batch 1001, LR 0.000097 Loss 11.902536, Accuracy 50.444%\n",
      "Epoch 5, Batch 1002, LR 0.000097 Loss 11.902569, Accuracy 50.447%\n",
      "Epoch 5, Batch 1003, LR 0.000097 Loss 11.902071, Accuracy 50.450%\n",
      "Epoch 5, Batch 1004, LR 0.000097 Loss 11.901311, Accuracy 50.458%\n",
      "Epoch 5, Batch 1005, LR 0.000097 Loss 11.900246, Accuracy 50.467%\n",
      "Epoch 5, Batch 1006, LR 0.000097 Loss 11.900321, Accuracy 50.464%\n",
      "Epoch 5, Batch 1007, LR 0.000097 Loss 11.900314, Accuracy 50.458%\n",
      "Epoch 5, Batch 1008, LR 0.000097 Loss 11.899140, Accuracy 50.460%\n",
      "Epoch 5, Batch 1009, LR 0.000097 Loss 11.898469, Accuracy 50.468%\n",
      "Epoch 5, Batch 1010, LR 0.000097 Loss 11.898182, Accuracy 50.467%\n",
      "Epoch 5, Batch 1011, LR 0.000097 Loss 11.897750, Accuracy 50.467%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Batch 1012, LR 0.000097 Loss 11.897880, Accuracy 50.469%\n",
      "Epoch 5, Batch 1013, LR 0.000097 Loss 11.897346, Accuracy 50.479%\n",
      "Epoch 5, Batch 1014, LR 0.000097 Loss 11.897022, Accuracy 50.480%\n",
      "Epoch 5, Batch 1015, LR 0.000097 Loss 11.896319, Accuracy 50.483%\n",
      "Epoch 5, Batch 1016, LR 0.000097 Loss 11.896179, Accuracy 50.486%\n",
      "Epoch 5, Batch 1017, LR 0.000097 Loss 11.896838, Accuracy 50.483%\n",
      "Epoch 5, Batch 1018, LR 0.000097 Loss 11.896327, Accuracy 50.487%\n",
      "Epoch 5, Batch 1019, LR 0.000097 Loss 11.897422, Accuracy 50.477%\n",
      "Epoch 5, Batch 1020, LR 0.000097 Loss 11.897530, Accuracy 50.476%\n",
      "Epoch 5, Batch 1021, LR 0.000097 Loss 11.897388, Accuracy 50.481%\n",
      "Epoch 5, Batch 1022, LR 0.000097 Loss 11.897716, Accuracy 50.479%\n",
      "Epoch 5, Batch 1023, LR 0.000097 Loss 11.896903, Accuracy 50.486%\n",
      "Epoch 5, Batch 1024, LR 0.000097 Loss 11.896114, Accuracy 50.488%\n",
      "Epoch 5, Batch 1025, LR 0.000097 Loss 11.896103, Accuracy 50.491%\n",
      "Epoch 5, Batch 1026, LR 0.000097 Loss 11.894954, Accuracy 50.497%\n",
      "Epoch 5, Batch 1027, LR 0.000097 Loss 11.894825, Accuracy 50.501%\n",
      "Epoch 5, Batch 1028, LR 0.000097 Loss 11.894918, Accuracy 50.502%\n",
      "Epoch 5, Batch 1029, LR 0.000097 Loss 11.894751, Accuracy 50.503%\n",
      "Epoch 5, Batch 1030, LR 0.000097 Loss 11.893275, Accuracy 50.512%\n",
      "Epoch 5, Batch 1031, LR 0.000097 Loss 11.892617, Accuracy 50.515%\n",
      "Epoch 5, Batch 1032, LR 0.000097 Loss 11.892449, Accuracy 50.519%\n",
      "Epoch 5, Batch 1033, LR 0.000097 Loss 11.892221, Accuracy 50.521%\n",
      "Epoch 5, Batch 1034, LR 0.000097 Loss 11.891984, Accuracy 50.522%\n",
      "Epoch 5, Batch 1035, LR 0.000097 Loss 11.890841, Accuracy 50.537%\n",
      "Epoch 5, Batch 1036, LR 0.000097 Loss 11.891116, Accuracy 50.537%\n",
      "Epoch 5, Batch 1037, LR 0.000097 Loss 11.890823, Accuracy 50.543%\n",
      "Epoch 5, Batch 1038, LR 0.000097 Loss 11.890028, Accuracy 50.547%\n",
      "Epoch 5, Batch 1039, LR 0.000097 Loss 11.889859, Accuracy 50.545%\n",
      "Epoch 5, Batch 1040, LR 0.000097 Loss 11.889652, Accuracy 50.546%\n",
      "Epoch 5, Batch 1041, LR 0.000097 Loss 11.889298, Accuracy 50.550%\n",
      "Epoch 5, Batch 1042, LR 0.000097 Loss 11.887851, Accuracy 50.562%\n",
      "Epoch 5, Batch 1043, LR 0.000097 Loss 11.887591, Accuracy 50.563%\n",
      "Epoch 5, Batch 1044, LR 0.000097 Loss 11.887544, Accuracy 50.564%\n",
      "Epoch 5, Batch 1045, LR 0.000097 Loss 11.887080, Accuracy 50.566%\n",
      "Epoch 5, Batch 1046, LR 0.000097 Loss 11.886245, Accuracy 50.571%\n",
      "Epoch 5, Batch 1047, LR 0.000097 Loss 11.886419, Accuracy 50.569%\n",
      "Epoch 5, Loss (train set) 11.886419, Accuracy (train set) 50.569%\n",
      "Epoch 6, Batch 1, LR 0.000097 Loss 11.971880, Accuracy 50.781%\n",
      "Epoch 6, Batch 2, LR 0.000097 Loss 11.949599, Accuracy 51.172%\n",
      "Epoch 6, Batch 3, LR 0.000097 Loss 11.732947, Accuracy 53.125%\n",
      "Epoch 6, Batch 4, LR 0.000097 Loss 11.550584, Accuracy 53.711%\n",
      "Epoch 6, Batch 5, LR 0.000097 Loss 11.536134, Accuracy 53.750%\n",
      "Epoch 6, Batch 6, LR 0.000097 Loss 11.565232, Accuracy 53.516%\n",
      "Epoch 6, Batch 7, LR 0.000097 Loss 11.553683, Accuracy 53.683%\n",
      "Epoch 6, Batch 8, LR 0.000097 Loss 11.464097, Accuracy 54.199%\n",
      "Epoch 6, Batch 9, LR 0.000097 Loss 11.495889, Accuracy 54.340%\n",
      "Epoch 6, Batch 10, LR 0.000097 Loss 11.492284, Accuracy 54.531%\n",
      "Epoch 6, Batch 11, LR 0.000097 Loss 11.498884, Accuracy 54.403%\n",
      "Epoch 6, Batch 12, LR 0.000097 Loss 11.548483, Accuracy 53.776%\n",
      "Epoch 6, Batch 13, LR 0.000097 Loss 11.592582, Accuracy 53.125%\n",
      "Epoch 6, Batch 14, LR 0.000097 Loss 11.549582, Accuracy 53.683%\n",
      "Epoch 6, Batch 15, LR 0.000097 Loss 11.570696, Accuracy 53.698%\n",
      "Epoch 6, Batch 16, LR 0.000097 Loss 11.532365, Accuracy 53.857%\n",
      "Epoch 6, Batch 17, LR 0.000097 Loss 11.563796, Accuracy 53.539%\n",
      "Epoch 6, Batch 18, LR 0.000097 Loss 11.583922, Accuracy 53.472%\n",
      "Epoch 6, Batch 19, LR 0.000097 Loss 11.579310, Accuracy 53.495%\n",
      "Epoch 6, Batch 20, LR 0.000097 Loss 11.553791, Accuracy 53.711%\n",
      "Epoch 6, Batch 21, LR 0.000097 Loss 11.564486, Accuracy 53.869%\n",
      "Epoch 6, Batch 22, LR 0.000097 Loss 11.554283, Accuracy 54.048%\n",
      "Epoch 6, Batch 23, LR 0.000097 Loss 11.523894, Accuracy 54.348%\n",
      "Epoch 6, Batch 24, LR 0.000097 Loss 11.526482, Accuracy 54.297%\n",
      "Epoch 6, Batch 25, LR 0.000097 Loss 11.535057, Accuracy 54.094%\n",
      "Epoch 6, Batch 26, LR 0.000097 Loss 11.533295, Accuracy 54.117%\n",
      "Epoch 6, Batch 27, LR 0.000097 Loss 11.515106, Accuracy 54.167%\n",
      "Epoch 6, Batch 28, LR 0.000097 Loss 11.522214, Accuracy 54.129%\n",
      "Epoch 6, Batch 29, LR 0.000097 Loss 11.500971, Accuracy 54.283%\n",
      "Epoch 6, Batch 30, LR 0.000097 Loss 11.493798, Accuracy 54.271%\n",
      "Epoch 6, Batch 31, LR 0.000097 Loss 11.479243, Accuracy 54.309%\n",
      "Epoch 6, Batch 32, LR 0.000097 Loss 11.466325, Accuracy 54.443%\n",
      "Epoch 6, Batch 33, LR 0.000097 Loss 11.467340, Accuracy 54.498%\n",
      "Epoch 6, Batch 34, LR 0.000097 Loss 11.468610, Accuracy 54.458%\n",
      "Epoch 6, Batch 35, LR 0.000097 Loss 11.474008, Accuracy 54.330%\n",
      "Epoch 6, Batch 36, LR 0.000097 Loss 11.468743, Accuracy 54.340%\n",
      "Epoch 6, Batch 37, LR 0.000097 Loss 11.471866, Accuracy 54.160%\n",
      "Epoch 6, Batch 38, LR 0.000097 Loss 11.464077, Accuracy 54.235%\n",
      "Epoch 6, Batch 39, LR 0.000097 Loss 11.467737, Accuracy 54.247%\n",
      "Epoch 6, Batch 40, LR 0.000097 Loss 11.443620, Accuracy 54.492%\n",
      "Epoch 6, Batch 41, LR 0.000097 Loss 11.461601, Accuracy 54.345%\n",
      "Epoch 6, Batch 42, LR 0.000097 Loss 11.463671, Accuracy 54.297%\n",
      "Epoch 6, Batch 43, LR 0.000097 Loss 11.462504, Accuracy 54.324%\n",
      "Epoch 6, Batch 44, LR 0.000097 Loss 11.456014, Accuracy 54.368%\n",
      "Epoch 6, Batch 45, LR 0.000097 Loss 11.459194, Accuracy 54.410%\n",
      "Epoch 6, Batch 46, LR 0.000097 Loss 11.445824, Accuracy 54.484%\n",
      "Epoch 6, Batch 47, LR 0.000097 Loss 11.433983, Accuracy 54.538%\n",
      "Epoch 6, Batch 48, LR 0.000097 Loss 11.434835, Accuracy 54.622%\n",
      "Epoch 6, Batch 49, LR 0.000097 Loss 11.422535, Accuracy 54.703%\n",
      "Epoch 6, Batch 50, LR 0.000097 Loss 11.409332, Accuracy 54.844%\n",
      "Epoch 6, Batch 51, LR 0.000097 Loss 11.406893, Accuracy 54.856%\n",
      "Epoch 6, Batch 52, LR 0.000097 Loss 11.408907, Accuracy 54.853%\n",
      "Epoch 6, Batch 53, LR 0.000097 Loss 11.410244, Accuracy 54.835%\n",
      "Epoch 6, Batch 54, LR 0.000097 Loss 11.400503, Accuracy 54.876%\n",
      "Epoch 6, Batch 55, LR 0.000097 Loss 11.397656, Accuracy 54.943%\n",
      "Epoch 6, Batch 56, LR 0.000097 Loss 11.401307, Accuracy 54.939%\n",
      "Epoch 6, Batch 57, LR 0.000097 Loss 11.397631, Accuracy 54.879%\n",
      "Epoch 6, Batch 58, LR 0.000097 Loss 11.395275, Accuracy 54.863%\n",
      "Epoch 6, Batch 59, LR 0.000097 Loss 11.396756, Accuracy 54.846%\n",
      "Epoch 6, Batch 60, LR 0.000097 Loss 11.398979, Accuracy 54.792%\n",
      "Epoch 6, Batch 61, LR 0.000097 Loss 11.398289, Accuracy 54.803%\n",
      "Epoch 6, Batch 62, LR 0.000097 Loss 11.411861, Accuracy 54.801%\n",
      "Epoch 6, Batch 63, LR 0.000097 Loss 11.403851, Accuracy 54.836%\n",
      "Epoch 6, Batch 64, LR 0.000097 Loss 11.388695, Accuracy 54.944%\n",
      "Epoch 6, Batch 65, LR 0.000097 Loss 11.388120, Accuracy 55.012%\n",
      "Epoch 6, Batch 66, LR 0.000097 Loss 11.384828, Accuracy 54.995%\n",
      "Epoch 6, Batch 67, LR 0.000097 Loss 11.378629, Accuracy 55.119%\n",
      "Epoch 6, Batch 68, LR 0.000097 Loss 11.388878, Accuracy 54.986%\n",
      "Epoch 6, Batch 69, LR 0.000097 Loss 11.381257, Accuracy 55.072%\n",
      "Epoch 6, Batch 70, LR 0.000097 Loss 11.380953, Accuracy 55.112%\n",
      "Epoch 6, Batch 71, LR 0.000097 Loss 11.378183, Accuracy 55.117%\n",
      "Epoch 6, Batch 72, LR 0.000097 Loss 11.383024, Accuracy 55.024%\n",
      "Epoch 6, Batch 73, LR 0.000097 Loss 11.378633, Accuracy 55.083%\n",
      "Epoch 6, Batch 74, LR 0.000097 Loss 11.375623, Accuracy 55.057%\n",
      "Epoch 6, Batch 75, LR 0.000097 Loss 11.377102, Accuracy 55.010%\n",
      "Epoch 6, Batch 76, LR 0.000097 Loss 11.375226, Accuracy 54.975%\n",
      "Epoch 6, Batch 77, LR 0.000097 Loss 11.378437, Accuracy 54.941%\n",
      "Epoch 6, Batch 78, LR 0.000097 Loss 11.377549, Accuracy 54.928%\n",
      "Epoch 6, Batch 79, LR 0.000097 Loss 11.373090, Accuracy 54.945%\n",
      "Epoch 6, Batch 80, LR 0.000097 Loss 11.368418, Accuracy 55.049%\n",
      "Epoch 6, Batch 81, LR 0.000097 Loss 11.362415, Accuracy 55.093%\n",
      "Epoch 6, Batch 82, LR 0.000097 Loss 11.358404, Accuracy 55.097%\n",
      "Epoch 6, Batch 83, LR 0.000097 Loss 11.355987, Accuracy 55.102%\n",
      "Epoch 6, Batch 84, LR 0.000097 Loss 11.349843, Accuracy 55.115%\n",
      "Epoch 6, Batch 85, LR 0.000097 Loss 11.340314, Accuracy 55.221%\n",
      "Epoch 6, Batch 86, LR 0.000097 Loss 11.344075, Accuracy 55.223%\n",
      "Epoch 6, Batch 87, LR 0.000097 Loss 11.349342, Accuracy 55.145%\n",
      "Epoch 6, Batch 88, LR 0.000097 Loss 11.338811, Accuracy 55.238%\n",
      "Epoch 6, Batch 89, LR 0.000097 Loss 11.337916, Accuracy 55.258%\n",
      "Epoch 6, Batch 90, LR 0.000097 Loss 11.337827, Accuracy 55.252%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 91, LR 0.000097 Loss 11.346181, Accuracy 55.203%\n",
      "Epoch 6, Batch 92, LR 0.000097 Loss 11.352114, Accuracy 55.112%\n",
      "Epoch 6, Batch 93, LR 0.000097 Loss 11.350166, Accuracy 55.150%\n",
      "Epoch 6, Batch 94, LR 0.000097 Loss 11.347618, Accuracy 55.136%\n",
      "Epoch 6, Batch 95, LR 0.000097 Loss 11.344013, Accuracy 55.156%\n",
      "Epoch 6, Batch 96, LR 0.000097 Loss 11.343902, Accuracy 55.168%\n",
      "Epoch 6, Batch 97, LR 0.000097 Loss 11.349539, Accuracy 55.090%\n",
      "Epoch 6, Batch 98, LR 0.000097 Loss 11.348873, Accuracy 55.166%\n",
      "Epoch 6, Batch 99, LR 0.000097 Loss 11.342818, Accuracy 55.177%\n",
      "Epoch 6, Batch 100, LR 0.000097 Loss 11.343398, Accuracy 55.172%\n",
      "Epoch 6, Batch 101, LR 0.000097 Loss 11.345379, Accuracy 55.144%\n",
      "Epoch 6, Batch 102, LR 0.000097 Loss 11.352582, Accuracy 55.132%\n",
      "Epoch 6, Batch 103, LR 0.000097 Loss 11.353240, Accuracy 55.097%\n",
      "Epoch 6, Batch 104, LR 0.000097 Loss 11.358245, Accuracy 55.056%\n",
      "Epoch 6, Batch 105, LR 0.000097 Loss 11.360158, Accuracy 55.000%\n",
      "Epoch 6, Batch 106, LR 0.000097 Loss 11.364541, Accuracy 54.938%\n",
      "Epoch 6, Batch 107, LR 0.000097 Loss 11.361773, Accuracy 54.936%\n",
      "Epoch 6, Batch 108, LR 0.000097 Loss 11.364674, Accuracy 54.912%\n",
      "Epoch 6, Batch 109, LR 0.000097 Loss 11.368447, Accuracy 54.867%\n",
      "Epoch 6, Batch 110, LR 0.000097 Loss 11.371190, Accuracy 54.858%\n",
      "Epoch 6, Batch 111, LR 0.000097 Loss 11.373424, Accuracy 54.849%\n",
      "Epoch 6, Batch 112, LR 0.000097 Loss 11.370971, Accuracy 54.785%\n",
      "Epoch 6, Batch 113, LR 0.000097 Loss 11.364681, Accuracy 54.853%\n",
      "Epoch 6, Batch 114, LR 0.000097 Loss 11.365551, Accuracy 54.886%\n",
      "Epoch 6, Batch 115, LR 0.000097 Loss 11.361540, Accuracy 54.857%\n",
      "Epoch 6, Batch 116, LR 0.000097 Loss 11.359989, Accuracy 54.863%\n",
      "Epoch 6, Batch 117, LR 0.000097 Loss 11.355800, Accuracy 54.874%\n",
      "Epoch 6, Batch 118, LR 0.000097 Loss 11.357448, Accuracy 54.860%\n",
      "Epoch 6, Batch 119, LR 0.000097 Loss 11.356807, Accuracy 54.865%\n",
      "Epoch 6, Batch 120, LR 0.000097 Loss 11.360390, Accuracy 54.889%\n",
      "Epoch 6, Batch 121, LR 0.000097 Loss 11.355138, Accuracy 54.952%\n",
      "Epoch 6, Batch 122, LR 0.000097 Loss 11.351111, Accuracy 54.995%\n",
      "Epoch 6, Batch 123, LR 0.000097 Loss 11.345839, Accuracy 55.069%\n",
      "Epoch 6, Batch 124, LR 0.000097 Loss 11.350265, Accuracy 55.021%\n",
      "Epoch 6, Batch 125, LR 0.000097 Loss 11.355973, Accuracy 54.950%\n",
      "Epoch 6, Batch 126, LR 0.000097 Loss 11.361603, Accuracy 54.880%\n",
      "Epoch 6, Batch 127, LR 0.000097 Loss 11.364914, Accuracy 54.811%\n",
      "Epoch 6, Batch 128, LR 0.000097 Loss 11.366162, Accuracy 54.797%\n",
      "Epoch 6, Batch 129, LR 0.000097 Loss 11.366803, Accuracy 54.797%\n",
      "Epoch 6, Batch 130, LR 0.000097 Loss 11.370684, Accuracy 54.742%\n",
      "Epoch 6, Batch 131, LR 0.000097 Loss 11.372517, Accuracy 54.753%\n",
      "Epoch 6, Batch 132, LR 0.000097 Loss 11.369883, Accuracy 54.764%\n",
      "Epoch 6, Batch 133, LR 0.000097 Loss 11.370987, Accuracy 54.770%\n",
      "Epoch 6, Batch 134, LR 0.000097 Loss 11.362157, Accuracy 54.810%\n",
      "Epoch 6, Batch 135, LR 0.000097 Loss 11.358946, Accuracy 54.826%\n",
      "Epoch 6, Batch 136, LR 0.000097 Loss 11.360602, Accuracy 54.785%\n",
      "Epoch 6, Batch 137, LR 0.000097 Loss 11.363711, Accuracy 54.756%\n",
      "Epoch 6, Batch 138, LR 0.000097 Loss 11.364699, Accuracy 54.772%\n",
      "Epoch 6, Batch 139, LR 0.000097 Loss 11.361706, Accuracy 54.766%\n",
      "Epoch 6, Batch 140, LR 0.000097 Loss 11.360319, Accuracy 54.760%\n",
      "Epoch 6, Batch 141, LR 0.000097 Loss 11.360740, Accuracy 54.710%\n",
      "Epoch 6, Batch 142, LR 0.000097 Loss 11.361131, Accuracy 54.693%\n",
      "Epoch 6, Batch 143, LR 0.000097 Loss 11.359212, Accuracy 54.688%\n",
      "Epoch 6, Batch 144, LR 0.000097 Loss 11.361354, Accuracy 54.655%\n",
      "Epoch 6, Batch 145, LR 0.000097 Loss 11.359869, Accuracy 54.661%\n",
      "Epoch 6, Batch 146, LR 0.000097 Loss 11.360025, Accuracy 54.650%\n",
      "Epoch 6, Batch 147, LR 0.000097 Loss 11.359061, Accuracy 54.688%\n",
      "Epoch 6, Batch 148, LR 0.000097 Loss 11.357907, Accuracy 54.682%\n",
      "Epoch 6, Batch 149, LR 0.000097 Loss 11.356368, Accuracy 54.698%\n",
      "Epoch 6, Batch 150, LR 0.000097 Loss 11.356117, Accuracy 54.703%\n",
      "Epoch 6, Batch 151, LR 0.000097 Loss 11.360551, Accuracy 54.662%\n",
      "Epoch 6, Batch 152, LR 0.000097 Loss 11.357032, Accuracy 54.636%\n",
      "Epoch 6, Batch 153, LR 0.000097 Loss 11.358443, Accuracy 54.626%\n",
      "Epoch 6, Batch 154, LR 0.000097 Loss 11.356967, Accuracy 54.652%\n",
      "Epoch 6, Batch 155, LR 0.000097 Loss 11.359159, Accuracy 54.617%\n",
      "Epoch 6, Batch 156, LR 0.000097 Loss 11.358872, Accuracy 54.587%\n",
      "Epoch 6, Batch 157, LR 0.000097 Loss 11.356021, Accuracy 54.603%\n",
      "Epoch 6, Batch 158, LR 0.000097 Loss 11.356678, Accuracy 54.623%\n",
      "Epoch 6, Batch 159, LR 0.000097 Loss 11.355181, Accuracy 54.619%\n",
      "Epoch 6, Batch 160, LR 0.000097 Loss 11.354665, Accuracy 54.604%\n",
      "Epoch 6, Batch 161, LR 0.000097 Loss 11.354228, Accuracy 54.620%\n",
      "Epoch 6, Batch 162, LR 0.000097 Loss 11.354007, Accuracy 54.615%\n",
      "Epoch 6, Batch 163, LR 0.000097 Loss 11.354782, Accuracy 54.572%\n",
      "Epoch 6, Batch 164, LR 0.000097 Loss 11.355682, Accuracy 54.564%\n",
      "Epoch 6, Batch 165, LR 0.000097 Loss 11.352500, Accuracy 54.607%\n",
      "Epoch 6, Batch 166, LR 0.000097 Loss 11.348794, Accuracy 54.622%\n",
      "Epoch 6, Batch 167, LR 0.000097 Loss 11.344970, Accuracy 54.636%\n",
      "Epoch 6, Batch 168, LR 0.000097 Loss 11.341145, Accuracy 54.660%\n",
      "Epoch 6, Batch 169, LR 0.000097 Loss 11.340872, Accuracy 54.674%\n",
      "Epoch 6, Batch 170, LR 0.000097 Loss 11.342031, Accuracy 54.646%\n",
      "Epoch 6, Batch 171, LR 0.000097 Loss 11.336752, Accuracy 54.692%\n",
      "Epoch 6, Batch 172, LR 0.000097 Loss 11.335834, Accuracy 54.701%\n",
      "Epoch 6, Batch 173, LR 0.000097 Loss 11.335813, Accuracy 54.692%\n",
      "Epoch 6, Batch 174, LR 0.000097 Loss 11.338237, Accuracy 54.670%\n",
      "Epoch 6, Batch 175, LR 0.000097 Loss 11.332832, Accuracy 54.683%\n",
      "Epoch 6, Batch 176, LR 0.000097 Loss 11.329328, Accuracy 54.696%\n",
      "Epoch 6, Batch 177, LR 0.000097 Loss 11.325247, Accuracy 54.714%\n",
      "Epoch 6, Batch 178, LR 0.000097 Loss 11.323413, Accuracy 54.731%\n",
      "Epoch 6, Batch 179, LR 0.000097 Loss 11.322922, Accuracy 54.753%\n",
      "Epoch 6, Batch 180, LR 0.000097 Loss 11.322548, Accuracy 54.761%\n",
      "Epoch 6, Batch 181, LR 0.000097 Loss 11.322876, Accuracy 54.791%\n",
      "Epoch 6, Batch 182, LR 0.000097 Loss 11.324064, Accuracy 54.791%\n",
      "Epoch 6, Batch 183, LR 0.000097 Loss 11.326797, Accuracy 54.752%\n",
      "Epoch 6, Batch 184, LR 0.000097 Loss 11.328335, Accuracy 54.738%\n",
      "Epoch 6, Batch 185, LR 0.000097 Loss 11.330065, Accuracy 54.747%\n",
      "Epoch 6, Batch 186, LR 0.000097 Loss 11.328460, Accuracy 54.742%\n",
      "Epoch 6, Batch 187, LR 0.000097 Loss 11.326023, Accuracy 54.742%\n",
      "Epoch 6, Batch 188, LR 0.000097 Loss 11.324666, Accuracy 54.746%\n",
      "Epoch 6, Batch 189, LR 0.000097 Loss 11.326177, Accuracy 54.745%\n",
      "Epoch 6, Batch 190, LR 0.000097 Loss 11.324203, Accuracy 54.757%\n",
      "Epoch 6, Batch 191, LR 0.000097 Loss 11.325518, Accuracy 54.741%\n",
      "Epoch 6, Batch 192, LR 0.000097 Loss 11.322501, Accuracy 54.773%\n",
      "Epoch 6, Batch 193, LR 0.000097 Loss 11.320660, Accuracy 54.801%\n",
      "Epoch 6, Batch 194, LR 0.000097 Loss 11.320624, Accuracy 54.796%\n",
      "Epoch 6, Batch 195, LR 0.000097 Loss 11.318119, Accuracy 54.816%\n",
      "Epoch 6, Batch 196, LR 0.000097 Loss 11.320084, Accuracy 54.775%\n",
      "Epoch 6, Batch 197, LR 0.000097 Loss 11.317417, Accuracy 54.787%\n",
      "Epoch 6, Batch 198, LR 0.000097 Loss 11.318512, Accuracy 54.790%\n",
      "Epoch 6, Batch 199, LR 0.000097 Loss 11.316375, Accuracy 54.821%\n",
      "Epoch 6, Batch 200, LR 0.000097 Loss 11.319915, Accuracy 54.797%\n",
      "Epoch 6, Batch 201, LR 0.000097 Loss 11.323162, Accuracy 54.769%\n",
      "Epoch 6, Batch 202, LR 0.000097 Loss 11.323834, Accuracy 54.784%\n",
      "Epoch 6, Batch 203, LR 0.000097 Loss 11.322986, Accuracy 54.780%\n",
      "Epoch 6, Batch 204, LR 0.000097 Loss 11.323877, Accuracy 54.768%\n",
      "Epoch 6, Batch 205, LR 0.000097 Loss 11.323863, Accuracy 54.764%\n",
      "Epoch 6, Batch 206, LR 0.000097 Loss 11.326573, Accuracy 54.733%\n",
      "Epoch 6, Batch 207, LR 0.000097 Loss 11.323867, Accuracy 54.748%\n",
      "Epoch 6, Batch 208, LR 0.000097 Loss 11.324684, Accuracy 54.733%\n",
      "Epoch 6, Batch 209, LR 0.000097 Loss 11.328322, Accuracy 54.691%\n",
      "Epoch 6, Batch 210, LR 0.000097 Loss 11.332229, Accuracy 54.654%\n",
      "Epoch 6, Batch 211, LR 0.000097 Loss 11.331756, Accuracy 54.643%\n",
      "Epoch 6, Batch 212, LR 0.000097 Loss 11.336359, Accuracy 54.617%\n",
      "Epoch 6, Batch 213, LR 0.000097 Loss 11.339208, Accuracy 54.577%\n",
      "Epoch 6, Batch 214, LR 0.000097 Loss 11.336930, Accuracy 54.596%\n",
      "Epoch 6, Batch 215, LR 0.000097 Loss 11.336307, Accuracy 54.600%\n",
      "Epoch 6, Batch 216, LR 0.000097 Loss 11.333188, Accuracy 54.637%\n",
      "Epoch 6, Batch 217, LR 0.000097 Loss 11.329516, Accuracy 54.666%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 218, LR 0.000097 Loss 11.328395, Accuracy 54.677%\n",
      "Epoch 6, Batch 219, LR 0.000097 Loss 11.324569, Accuracy 54.709%\n",
      "Epoch 6, Batch 220, LR 0.000097 Loss 11.325257, Accuracy 54.705%\n",
      "Epoch 6, Batch 221, LR 0.000097 Loss 11.327034, Accuracy 54.677%\n",
      "Epoch 6, Batch 222, LR 0.000097 Loss 11.327064, Accuracy 54.666%\n",
      "Epoch 6, Batch 223, LR 0.000097 Loss 11.325814, Accuracy 54.691%\n",
      "Epoch 6, Batch 224, LR 0.000097 Loss 11.323035, Accuracy 54.712%\n",
      "Epoch 6, Batch 225, LR 0.000097 Loss 11.324985, Accuracy 54.701%\n",
      "Epoch 6, Batch 226, LR 0.000097 Loss 11.324684, Accuracy 54.691%\n",
      "Epoch 6, Batch 227, LR 0.000097 Loss 11.318712, Accuracy 54.732%\n",
      "Epoch 6, Batch 228, LR 0.000097 Loss 11.317906, Accuracy 54.735%\n",
      "Epoch 6, Batch 229, LR 0.000097 Loss 11.319188, Accuracy 54.763%\n",
      "Epoch 6, Batch 230, LR 0.000097 Loss 11.319382, Accuracy 54.752%\n",
      "Epoch 6, Batch 231, LR 0.000097 Loss 11.319459, Accuracy 54.745%\n",
      "Epoch 6, Batch 232, LR 0.000097 Loss 11.317578, Accuracy 54.751%\n",
      "Epoch 6, Batch 233, LR 0.000097 Loss 11.319229, Accuracy 54.771%\n",
      "Epoch 6, Batch 234, LR 0.000097 Loss 11.320456, Accuracy 54.754%\n",
      "Epoch 6, Batch 235, LR 0.000097 Loss 11.320139, Accuracy 54.767%\n",
      "Epoch 6, Batch 236, LR 0.000097 Loss 11.318078, Accuracy 54.780%\n",
      "Epoch 6, Batch 237, LR 0.000097 Loss 11.317347, Accuracy 54.770%\n",
      "Epoch 6, Batch 238, LR 0.000097 Loss 11.316564, Accuracy 54.799%\n",
      "Epoch 6, Batch 239, LR 0.000097 Loss 11.316159, Accuracy 54.808%\n",
      "Epoch 6, Batch 240, LR 0.000097 Loss 11.316785, Accuracy 54.798%\n",
      "Epoch 6, Batch 241, LR 0.000097 Loss 11.314151, Accuracy 54.811%\n",
      "Epoch 6, Batch 242, LR 0.000097 Loss 11.315070, Accuracy 54.807%\n",
      "Epoch 6, Batch 243, LR 0.000097 Loss 11.313795, Accuracy 54.829%\n",
      "Epoch 6, Batch 244, LR 0.000097 Loss 11.313175, Accuracy 54.822%\n",
      "Epoch 6, Batch 245, LR 0.000097 Loss 11.311754, Accuracy 54.802%\n",
      "Epoch 6, Batch 246, LR 0.000097 Loss 11.310562, Accuracy 54.802%\n",
      "Epoch 6, Batch 247, LR 0.000097 Loss 11.311627, Accuracy 54.782%\n",
      "Epoch 6, Batch 248, LR 0.000097 Loss 11.311217, Accuracy 54.782%\n",
      "Epoch 6, Batch 249, LR 0.000097 Loss 11.310123, Accuracy 54.778%\n",
      "Epoch 6, Batch 250, LR 0.000097 Loss 11.312710, Accuracy 54.769%\n",
      "Epoch 6, Batch 251, LR 0.000097 Loss 11.316585, Accuracy 54.747%\n",
      "Epoch 6, Batch 252, LR 0.000097 Loss 11.316908, Accuracy 54.731%\n",
      "Epoch 6, Batch 253, LR 0.000097 Loss 11.317421, Accuracy 54.737%\n",
      "Epoch 6, Batch 254, LR 0.000097 Loss 11.320159, Accuracy 54.712%\n",
      "Epoch 6, Batch 255, LR 0.000097 Loss 11.318292, Accuracy 54.724%\n",
      "Epoch 6, Batch 256, LR 0.000097 Loss 11.319049, Accuracy 54.730%\n",
      "Epoch 6, Batch 257, LR 0.000097 Loss 11.318386, Accuracy 54.730%\n",
      "Epoch 6, Batch 258, LR 0.000097 Loss 11.313373, Accuracy 54.769%\n",
      "Epoch 6, Batch 259, LR 0.000097 Loss 11.315306, Accuracy 54.769%\n",
      "Epoch 6, Batch 260, LR 0.000097 Loss 11.314805, Accuracy 54.760%\n",
      "Epoch 6, Batch 261, LR 0.000097 Loss 11.313624, Accuracy 54.765%\n",
      "Epoch 6, Batch 262, LR 0.000097 Loss 11.311714, Accuracy 54.765%\n",
      "Epoch 6, Batch 263, LR 0.000097 Loss 11.311738, Accuracy 54.768%\n",
      "Epoch 6, Batch 264, LR 0.000097 Loss 11.312199, Accuracy 54.764%\n",
      "Epoch 6, Batch 265, LR 0.000097 Loss 11.312953, Accuracy 54.744%\n",
      "Epoch 6, Batch 266, LR 0.000097 Loss 11.314137, Accuracy 54.732%\n",
      "Epoch 6, Batch 267, LR 0.000097 Loss 11.312976, Accuracy 54.728%\n",
      "Epoch 6, Batch 268, LR 0.000097 Loss 11.312898, Accuracy 54.728%\n",
      "Epoch 6, Batch 269, LR 0.000097 Loss 11.314802, Accuracy 54.725%\n",
      "Epoch 6, Batch 270, LR 0.000097 Loss 11.315059, Accuracy 54.734%\n",
      "Epoch 6, Batch 271, LR 0.000097 Loss 11.316466, Accuracy 54.731%\n",
      "Epoch 6, Batch 272, LR 0.000097 Loss 11.313682, Accuracy 54.751%\n",
      "Epoch 6, Batch 273, LR 0.000097 Loss 11.313495, Accuracy 54.759%\n",
      "Epoch 6, Batch 274, LR 0.000097 Loss 11.311370, Accuracy 54.762%\n",
      "Epoch 6, Batch 275, LR 0.000097 Loss 11.311376, Accuracy 54.767%\n",
      "Epoch 6, Batch 276, LR 0.000097 Loss 11.313322, Accuracy 54.755%\n",
      "Epoch 6, Batch 277, LR 0.000097 Loss 11.314702, Accuracy 54.719%\n",
      "Epoch 6, Batch 278, LR 0.000097 Loss 11.316173, Accuracy 54.702%\n",
      "Epoch 6, Batch 279, LR 0.000097 Loss 11.317290, Accuracy 54.699%\n",
      "Epoch 6, Batch 280, LR 0.000097 Loss 11.317510, Accuracy 54.685%\n",
      "Epoch 6, Batch 281, LR 0.000097 Loss 11.316334, Accuracy 54.693%\n",
      "Epoch 6, Batch 282, LR 0.000097 Loss 11.315895, Accuracy 54.710%\n",
      "Epoch 6, Batch 283, LR 0.000097 Loss 11.318086, Accuracy 54.676%\n",
      "Epoch 6, Batch 284, LR 0.000097 Loss 11.319292, Accuracy 54.649%\n",
      "Epoch 6, Batch 285, LR 0.000097 Loss 11.319452, Accuracy 54.644%\n",
      "Epoch 6, Batch 286, LR 0.000097 Loss 11.320879, Accuracy 54.625%\n",
      "Epoch 6, Batch 287, LR 0.000097 Loss 11.322594, Accuracy 54.609%\n",
      "Epoch 6, Batch 288, LR 0.000097 Loss 11.319818, Accuracy 54.641%\n",
      "Epoch 6, Batch 289, LR 0.000097 Loss 11.318937, Accuracy 54.655%\n",
      "Epoch 6, Batch 290, LR 0.000097 Loss 11.318429, Accuracy 54.661%\n",
      "Epoch 6, Batch 291, LR 0.000097 Loss 11.318364, Accuracy 54.666%\n",
      "Epoch 6, Batch 292, LR 0.000097 Loss 11.319416, Accuracy 54.663%\n",
      "Epoch 6, Batch 293, LR 0.000097 Loss 11.320789, Accuracy 54.658%\n",
      "Epoch 6, Batch 294, LR 0.000097 Loss 11.321997, Accuracy 54.640%\n",
      "Epoch 6, Batch 295, LR 0.000097 Loss 11.324855, Accuracy 54.619%\n",
      "Epoch 6, Batch 296, LR 0.000097 Loss 11.324568, Accuracy 54.603%\n",
      "Epoch 6, Batch 297, LR 0.000097 Loss 11.326630, Accuracy 54.588%\n",
      "Epoch 6, Batch 298, LR 0.000097 Loss 11.325654, Accuracy 54.596%\n",
      "Epoch 6, Batch 299, LR 0.000097 Loss 11.323723, Accuracy 54.612%\n",
      "Epoch 6, Batch 300, LR 0.000097 Loss 11.323281, Accuracy 54.635%\n",
      "Epoch 6, Batch 301, LR 0.000097 Loss 11.325520, Accuracy 54.620%\n",
      "Epoch 6, Batch 302, LR 0.000097 Loss 11.324329, Accuracy 54.636%\n",
      "Epoch 6, Batch 303, LR 0.000097 Loss 11.326424, Accuracy 54.620%\n",
      "Epoch 6, Batch 304, LR 0.000097 Loss 11.326385, Accuracy 54.618%\n",
      "Epoch 6, Batch 305, LR 0.000097 Loss 11.325800, Accuracy 54.613%\n",
      "Epoch 6, Batch 306, LR 0.000097 Loss 11.328149, Accuracy 54.593%\n",
      "Epoch 6, Batch 307, LR 0.000097 Loss 11.327276, Accuracy 54.601%\n",
      "Epoch 6, Batch 308, LR 0.000097 Loss 11.327579, Accuracy 54.594%\n",
      "Epoch 6, Batch 309, LR 0.000097 Loss 11.328493, Accuracy 54.581%\n",
      "Epoch 6, Batch 310, LR 0.000097 Loss 11.327293, Accuracy 54.602%\n",
      "Epoch 6, Batch 311, LR 0.000097 Loss 11.327414, Accuracy 54.610%\n",
      "Epoch 6, Batch 312, LR 0.000097 Loss 11.328268, Accuracy 54.597%\n",
      "Epoch 6, Batch 313, LR 0.000097 Loss 11.329998, Accuracy 54.578%\n",
      "Epoch 6, Batch 314, LR 0.000097 Loss 11.329908, Accuracy 54.583%\n",
      "Epoch 6, Batch 315, LR 0.000097 Loss 11.331883, Accuracy 54.563%\n",
      "Epoch 6, Batch 316, LR 0.000097 Loss 11.332766, Accuracy 54.549%\n",
      "Epoch 6, Batch 317, LR 0.000097 Loss 11.332405, Accuracy 54.562%\n",
      "Epoch 6, Batch 318, LR 0.000097 Loss 11.330845, Accuracy 54.579%\n",
      "Epoch 6, Batch 319, LR 0.000097 Loss 11.332304, Accuracy 54.582%\n",
      "Epoch 6, Batch 320, LR 0.000097 Loss 11.332450, Accuracy 54.580%\n",
      "Epoch 6, Batch 321, LR 0.000097 Loss 11.330678, Accuracy 54.593%\n",
      "Epoch 6, Batch 322, LR 0.000097 Loss 11.330077, Accuracy 54.600%\n",
      "Epoch 6, Batch 323, LR 0.000097 Loss 11.327816, Accuracy 54.625%\n",
      "Epoch 6, Batch 324, LR 0.000097 Loss 11.328220, Accuracy 54.615%\n",
      "Epoch 6, Batch 325, LR 0.000097 Loss 11.326569, Accuracy 54.632%\n",
      "Epoch 6, Batch 326, LR 0.000097 Loss 11.325908, Accuracy 54.632%\n",
      "Epoch 6, Batch 327, LR 0.000097 Loss 11.324689, Accuracy 54.649%\n",
      "Epoch 6, Batch 328, LR 0.000097 Loss 11.323738, Accuracy 54.659%\n",
      "Epoch 6, Batch 329, LR 0.000097 Loss 11.322932, Accuracy 54.661%\n",
      "Epoch 6, Batch 330, LR 0.000097 Loss 11.324600, Accuracy 54.654%\n",
      "Epoch 6, Batch 331, LR 0.000097 Loss 11.326083, Accuracy 54.654%\n",
      "Epoch 6, Batch 332, LR 0.000097 Loss 11.325584, Accuracy 54.659%\n",
      "Epoch 6, Batch 333, LR 0.000097 Loss 11.326461, Accuracy 54.657%\n",
      "Epoch 6, Batch 334, LR 0.000097 Loss 11.326659, Accuracy 54.662%\n",
      "Epoch 6, Batch 335, LR 0.000097 Loss 11.327286, Accuracy 54.657%\n",
      "Epoch 6, Batch 336, LR 0.000097 Loss 11.326005, Accuracy 54.657%\n",
      "Epoch 6, Batch 337, LR 0.000097 Loss 11.326011, Accuracy 54.660%\n",
      "Epoch 6, Batch 338, LR 0.000097 Loss 11.326181, Accuracy 54.651%\n",
      "Epoch 6, Batch 339, LR 0.000097 Loss 11.322893, Accuracy 54.674%\n",
      "Epoch 6, Batch 340, LR 0.000097 Loss 11.323279, Accuracy 54.655%\n",
      "Epoch 6, Batch 341, LR 0.000097 Loss 11.322309, Accuracy 54.678%\n",
      "Epoch 6, Batch 342, LR 0.000097 Loss 11.319712, Accuracy 54.694%\n",
      "Epoch 6, Batch 343, LR 0.000097 Loss 11.321237, Accuracy 54.681%\n",
      "Epoch 6, Batch 344, LR 0.000097 Loss 11.322958, Accuracy 54.667%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 345, LR 0.000097 Loss 11.321816, Accuracy 54.678%\n",
      "Epoch 6, Batch 346, LR 0.000097 Loss 11.322422, Accuracy 54.685%\n",
      "Epoch 6, Batch 347, LR 0.000097 Loss 11.323486, Accuracy 54.667%\n",
      "Epoch 6, Batch 348, LR 0.000097 Loss 11.323413, Accuracy 54.667%\n",
      "Epoch 6, Batch 349, LR 0.000097 Loss 11.322081, Accuracy 54.672%\n",
      "Epoch 6, Batch 350, LR 0.000097 Loss 11.322180, Accuracy 54.663%\n",
      "Epoch 6, Batch 351, LR 0.000097 Loss 11.321478, Accuracy 54.650%\n",
      "Epoch 6, Batch 352, LR 0.000097 Loss 11.319098, Accuracy 54.663%\n",
      "Epoch 6, Batch 353, LR 0.000097 Loss 11.319526, Accuracy 54.657%\n",
      "Epoch 6, Batch 354, LR 0.000097 Loss 11.320900, Accuracy 54.630%\n",
      "Epoch 6, Batch 355, LR 0.000097 Loss 11.322085, Accuracy 54.630%\n",
      "Epoch 6, Batch 356, LR 0.000097 Loss 11.321424, Accuracy 54.639%\n",
      "Epoch 6, Batch 357, LR 0.000097 Loss 11.320226, Accuracy 54.646%\n",
      "Epoch 6, Batch 358, LR 0.000097 Loss 11.318901, Accuracy 54.650%\n",
      "Epoch 6, Batch 359, LR 0.000097 Loss 11.318573, Accuracy 54.651%\n",
      "Epoch 6, Batch 360, LR 0.000097 Loss 11.317352, Accuracy 54.666%\n",
      "Epoch 6, Batch 361, LR 0.000097 Loss 11.317382, Accuracy 54.675%\n",
      "Epoch 6, Batch 362, LR 0.000097 Loss 11.316223, Accuracy 54.681%\n",
      "Epoch 6, Batch 363, LR 0.000097 Loss 11.315347, Accuracy 54.675%\n",
      "Epoch 6, Batch 364, LR 0.000097 Loss 11.315682, Accuracy 54.664%\n",
      "Epoch 6, Batch 365, LR 0.000097 Loss 11.315887, Accuracy 54.658%\n",
      "Epoch 6, Batch 366, LR 0.000097 Loss 11.315031, Accuracy 54.660%\n",
      "Epoch 6, Batch 367, LR 0.000097 Loss 11.315318, Accuracy 54.656%\n",
      "Epoch 6, Batch 368, LR 0.000097 Loss 11.314439, Accuracy 54.673%\n",
      "Epoch 6, Batch 369, LR 0.000097 Loss 11.313143, Accuracy 54.683%\n",
      "Epoch 6, Batch 370, LR 0.000097 Loss 11.314136, Accuracy 54.660%\n",
      "Epoch 6, Batch 371, LR 0.000097 Loss 11.312991, Accuracy 54.664%\n",
      "Epoch 6, Batch 372, LR 0.000097 Loss 11.310966, Accuracy 54.679%\n",
      "Epoch 6, Batch 373, LR 0.000097 Loss 11.311984, Accuracy 54.667%\n",
      "Epoch 6, Batch 374, LR 0.000097 Loss 11.311090, Accuracy 54.671%\n",
      "Epoch 6, Batch 375, LR 0.000097 Loss 11.313015, Accuracy 54.667%\n",
      "Epoch 6, Batch 376, LR 0.000097 Loss 11.313161, Accuracy 54.656%\n",
      "Epoch 6, Batch 377, LR 0.000097 Loss 11.314326, Accuracy 54.638%\n",
      "Epoch 6, Batch 378, LR 0.000097 Loss 11.313334, Accuracy 54.648%\n",
      "Epoch 6, Batch 379, LR 0.000097 Loss 11.312181, Accuracy 54.644%\n",
      "Epoch 6, Batch 380, LR 0.000097 Loss 11.310685, Accuracy 54.646%\n",
      "Epoch 6, Batch 381, LR 0.000097 Loss 11.309606, Accuracy 54.659%\n",
      "Epoch 6, Batch 382, LR 0.000097 Loss 11.311432, Accuracy 54.643%\n",
      "Epoch 6, Batch 383, LR 0.000097 Loss 11.312889, Accuracy 54.628%\n",
      "Epoch 6, Batch 384, LR 0.000097 Loss 11.314436, Accuracy 54.602%\n",
      "Epoch 6, Batch 385, LR 0.000097 Loss 11.314142, Accuracy 54.604%\n",
      "Epoch 6, Batch 386, LR 0.000097 Loss 11.313278, Accuracy 54.613%\n",
      "Epoch 6, Batch 387, LR 0.000097 Loss 11.315046, Accuracy 54.587%\n",
      "Epoch 6, Batch 388, LR 0.000097 Loss 11.316113, Accuracy 54.585%\n",
      "Epoch 6, Batch 389, LR 0.000097 Loss 11.315233, Accuracy 54.595%\n",
      "Epoch 6, Batch 390, LR 0.000097 Loss 11.315973, Accuracy 54.581%\n",
      "Epoch 6, Batch 391, LR 0.000097 Loss 11.316303, Accuracy 54.574%\n",
      "Epoch 6, Batch 392, LR 0.000097 Loss 11.316557, Accuracy 54.576%\n",
      "Epoch 6, Batch 393, LR 0.000097 Loss 11.315449, Accuracy 54.594%\n",
      "Epoch 6, Batch 394, LR 0.000097 Loss 11.315276, Accuracy 54.602%\n",
      "Epoch 6, Batch 395, LR 0.000097 Loss 11.314159, Accuracy 54.606%\n",
      "Epoch 6, Batch 396, LR 0.000097 Loss 11.314471, Accuracy 54.609%\n",
      "Epoch 6, Batch 397, LR 0.000097 Loss 11.315969, Accuracy 54.593%\n",
      "Epoch 6, Batch 398, LR 0.000097 Loss 11.316024, Accuracy 54.595%\n",
      "Epoch 6, Batch 399, LR 0.000097 Loss 11.316345, Accuracy 54.594%\n",
      "Epoch 6, Batch 400, LR 0.000097 Loss 11.316070, Accuracy 54.596%\n",
      "Epoch 6, Batch 401, LR 0.000097 Loss 11.315076, Accuracy 54.612%\n",
      "Epoch 6, Batch 402, LR 0.000097 Loss 11.314969, Accuracy 54.618%\n",
      "Epoch 6, Batch 403, LR 0.000097 Loss 11.314642, Accuracy 54.620%\n",
      "Epoch 6, Batch 404, LR 0.000097 Loss 11.315465, Accuracy 54.616%\n",
      "Epoch 6, Batch 405, LR 0.000097 Loss 11.315720, Accuracy 54.620%\n",
      "Epoch 6, Batch 406, LR 0.000097 Loss 11.318026, Accuracy 54.607%\n",
      "Epoch 6, Batch 407, LR 0.000097 Loss 11.317465, Accuracy 54.620%\n",
      "Epoch 6, Batch 408, LR 0.000097 Loss 11.317011, Accuracy 54.613%\n",
      "Epoch 6, Batch 409, LR 0.000097 Loss 11.316244, Accuracy 54.617%\n",
      "Epoch 6, Batch 410, LR 0.000097 Loss 11.316823, Accuracy 54.613%\n",
      "Epoch 6, Batch 411, LR 0.000097 Loss 11.317687, Accuracy 54.604%\n",
      "Epoch 6, Batch 412, LR 0.000097 Loss 11.316197, Accuracy 54.612%\n",
      "Epoch 6, Batch 413, LR 0.000097 Loss 11.314011, Accuracy 54.629%\n",
      "Epoch 6, Batch 414, LR 0.000097 Loss 11.314626, Accuracy 54.627%\n",
      "Epoch 6, Batch 415, LR 0.000097 Loss 11.314618, Accuracy 54.629%\n",
      "Epoch 6, Batch 416, LR 0.000097 Loss 11.312601, Accuracy 54.627%\n",
      "Epoch 6, Batch 417, LR 0.000097 Loss 11.312079, Accuracy 54.635%\n",
      "Epoch 6, Batch 418, LR 0.000097 Loss 11.312307, Accuracy 54.637%\n",
      "Epoch 6, Batch 419, LR 0.000097 Loss 11.312561, Accuracy 54.632%\n",
      "Epoch 6, Batch 420, LR 0.000097 Loss 11.312867, Accuracy 54.617%\n",
      "Epoch 6, Batch 421, LR 0.000097 Loss 11.311146, Accuracy 54.630%\n",
      "Epoch 6, Batch 422, LR 0.000097 Loss 11.310373, Accuracy 54.639%\n",
      "Epoch 6, Batch 423, LR 0.000097 Loss 11.310635, Accuracy 54.641%\n",
      "Epoch 6, Batch 424, LR 0.000097 Loss 11.311838, Accuracy 54.636%\n",
      "Epoch 6, Batch 425, LR 0.000097 Loss 11.313554, Accuracy 54.623%\n",
      "Epoch 6, Batch 426, LR 0.000097 Loss 11.312535, Accuracy 54.632%\n",
      "Epoch 6, Batch 427, LR 0.000097 Loss 11.311541, Accuracy 54.645%\n",
      "Epoch 6, Batch 428, LR 0.000097 Loss 11.310044, Accuracy 54.662%\n",
      "Epoch 6, Batch 429, LR 0.000097 Loss 11.309638, Accuracy 54.660%\n",
      "Epoch 6, Batch 430, LR 0.000097 Loss 11.308680, Accuracy 54.673%\n",
      "Epoch 6, Batch 431, LR 0.000097 Loss 11.309816, Accuracy 54.653%\n",
      "Epoch 6, Batch 432, LR 0.000097 Loss 11.311153, Accuracy 54.644%\n",
      "Epoch 6, Batch 433, LR 0.000097 Loss 11.310912, Accuracy 54.637%\n",
      "Epoch 6, Batch 434, LR 0.000097 Loss 11.309412, Accuracy 54.651%\n",
      "Epoch 6, Batch 435, LR 0.000097 Loss 11.309893, Accuracy 54.641%\n",
      "Epoch 6, Batch 436, LR 0.000097 Loss 11.307638, Accuracy 54.664%\n",
      "Epoch 6, Batch 437, LR 0.000097 Loss 11.308819, Accuracy 54.657%\n",
      "Epoch 6, Batch 438, LR 0.000097 Loss 11.309362, Accuracy 54.645%\n",
      "Epoch 6, Batch 439, LR 0.000097 Loss 11.309846, Accuracy 54.636%\n",
      "Epoch 6, Batch 440, LR 0.000097 Loss 11.309833, Accuracy 54.634%\n",
      "Epoch 6, Batch 441, LR 0.000097 Loss 11.307527, Accuracy 54.647%\n",
      "Epoch 6, Batch 442, LR 0.000097 Loss 11.305691, Accuracy 54.652%\n",
      "Epoch 6, Batch 443, LR 0.000097 Loss 11.304904, Accuracy 54.668%\n",
      "Epoch 6, Batch 444, LR 0.000097 Loss 11.304784, Accuracy 54.670%\n",
      "Epoch 6, Batch 445, LR 0.000097 Loss 11.305304, Accuracy 54.666%\n",
      "Epoch 6, Batch 446, LR 0.000097 Loss 11.305487, Accuracy 54.652%\n",
      "Epoch 6, Batch 447, LR 0.000097 Loss 11.304054, Accuracy 54.665%\n",
      "Epoch 6, Batch 448, LR 0.000097 Loss 11.304482, Accuracy 54.663%\n",
      "Epoch 6, Batch 449, LR 0.000097 Loss 11.303708, Accuracy 54.677%\n",
      "Epoch 6, Batch 450, LR 0.000097 Loss 11.303583, Accuracy 54.677%\n",
      "Epoch 6, Batch 451, LR 0.000097 Loss 11.303336, Accuracy 54.684%\n",
      "Epoch 6, Batch 452, LR 0.000097 Loss 11.302170, Accuracy 54.696%\n",
      "Epoch 6, Batch 453, LR 0.000097 Loss 11.303016, Accuracy 54.686%\n",
      "Epoch 6, Batch 454, LR 0.000097 Loss 11.302619, Accuracy 54.684%\n",
      "Epoch 6, Batch 455, LR 0.000097 Loss 11.303143, Accuracy 54.694%\n",
      "Epoch 6, Batch 456, LR 0.000097 Loss 11.303212, Accuracy 54.688%\n",
      "Epoch 6, Batch 457, LR 0.000097 Loss 11.303661, Accuracy 54.679%\n",
      "Epoch 6, Batch 458, LR 0.000097 Loss 11.303830, Accuracy 54.667%\n",
      "Epoch 6, Batch 459, LR 0.000097 Loss 11.303488, Accuracy 54.665%\n",
      "Epoch 6, Batch 460, LR 0.000097 Loss 11.303653, Accuracy 54.671%\n",
      "Epoch 6, Batch 461, LR 0.000097 Loss 11.303825, Accuracy 54.669%\n",
      "Epoch 6, Batch 462, LR 0.000097 Loss 11.302441, Accuracy 54.679%\n",
      "Epoch 6, Batch 463, LR 0.000097 Loss 11.302449, Accuracy 54.679%\n",
      "Epoch 6, Batch 464, LR 0.000097 Loss 11.302834, Accuracy 54.679%\n",
      "Epoch 6, Batch 465, LR 0.000097 Loss 11.303334, Accuracy 54.667%\n",
      "Epoch 6, Batch 466, LR 0.000097 Loss 11.300859, Accuracy 54.681%\n",
      "Epoch 6, Batch 467, LR 0.000097 Loss 11.302176, Accuracy 54.666%\n",
      "Epoch 6, Batch 468, LR 0.000097 Loss 11.302960, Accuracy 54.642%\n",
      "Epoch 6, Batch 469, LR 0.000097 Loss 11.301615, Accuracy 54.644%\n",
      "Epoch 6, Batch 470, LR 0.000097 Loss 11.300554, Accuracy 54.659%\n",
      "Epoch 6, Batch 471, LR 0.000097 Loss 11.300433, Accuracy 54.654%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 472, LR 0.000097 Loss 11.301048, Accuracy 54.654%\n",
      "Epoch 6, Batch 473, LR 0.000097 Loss 11.300330, Accuracy 54.648%\n",
      "Epoch 6, Batch 474, LR 0.000097 Loss 11.301102, Accuracy 54.641%\n",
      "Epoch 6, Batch 475, LR 0.000097 Loss 11.300413, Accuracy 54.658%\n",
      "Epoch 6, Batch 476, LR 0.000097 Loss 11.300793, Accuracy 54.661%\n",
      "Epoch 6, Batch 477, LR 0.000097 Loss 11.300173, Accuracy 54.668%\n",
      "Epoch 6, Batch 478, LR 0.000097 Loss 11.300127, Accuracy 54.674%\n",
      "Epoch 6, Batch 479, LR 0.000097 Loss 11.299230, Accuracy 54.676%\n",
      "Epoch 6, Batch 480, LR 0.000097 Loss 11.300289, Accuracy 54.666%\n",
      "Epoch 6, Batch 481, LR 0.000097 Loss 11.300632, Accuracy 54.673%\n",
      "Epoch 6, Batch 482, LR 0.000097 Loss 11.300538, Accuracy 54.670%\n",
      "Epoch 6, Batch 483, LR 0.000097 Loss 11.301859, Accuracy 54.644%\n",
      "Epoch 6, Batch 484, LR 0.000097 Loss 11.300235, Accuracy 54.650%\n",
      "Epoch 6, Batch 485, LR 0.000097 Loss 11.300142, Accuracy 54.657%\n",
      "Epoch 6, Batch 486, LR 0.000097 Loss 11.299406, Accuracy 54.662%\n",
      "Epoch 6, Batch 487, LR 0.000097 Loss 11.297917, Accuracy 54.673%\n",
      "Epoch 6, Batch 488, LR 0.000097 Loss 11.298014, Accuracy 54.668%\n",
      "Epoch 6, Batch 489, LR 0.000097 Loss 11.297437, Accuracy 54.665%\n",
      "Epoch 6, Batch 490, LR 0.000097 Loss 11.298443, Accuracy 54.652%\n",
      "Epoch 6, Batch 491, LR 0.000097 Loss 11.299278, Accuracy 54.643%\n",
      "Epoch 6, Batch 492, LR 0.000097 Loss 11.300035, Accuracy 54.640%\n",
      "Epoch 6, Batch 493, LR 0.000097 Loss 11.298209, Accuracy 54.651%\n",
      "Epoch 6, Batch 494, LR 0.000097 Loss 11.298710, Accuracy 54.650%\n",
      "Epoch 6, Batch 495, LR 0.000097 Loss 11.297521, Accuracy 54.659%\n",
      "Epoch 6, Batch 496, LR 0.000097 Loss 11.297433, Accuracy 54.650%\n",
      "Epoch 6, Batch 497, LR 0.000097 Loss 11.297738, Accuracy 54.640%\n",
      "Epoch 6, Batch 498, LR 0.000097 Loss 11.295330, Accuracy 54.651%\n",
      "Epoch 6, Batch 499, LR 0.000097 Loss 11.295914, Accuracy 54.650%\n",
      "Epoch 6, Batch 500, LR 0.000097 Loss 11.295501, Accuracy 54.645%\n",
      "Epoch 6, Batch 501, LR 0.000097 Loss 11.294873, Accuracy 54.645%\n",
      "Epoch 6, Batch 502, LR 0.000097 Loss 11.293402, Accuracy 54.659%\n",
      "Epoch 6, Batch 503, LR 0.000097 Loss 11.292677, Accuracy 54.667%\n",
      "Epoch 6, Batch 504, LR 0.000097 Loss 11.292217, Accuracy 54.672%\n",
      "Epoch 6, Batch 505, LR 0.000097 Loss 11.291151, Accuracy 54.681%\n",
      "Epoch 6, Batch 506, LR 0.000097 Loss 11.290766, Accuracy 54.694%\n",
      "Epoch 6, Batch 507, LR 0.000097 Loss 11.290977, Accuracy 54.694%\n",
      "Epoch 6, Batch 508, LR 0.000097 Loss 11.291089, Accuracy 54.684%\n",
      "Epoch 6, Batch 509, LR 0.000097 Loss 11.293194, Accuracy 54.675%\n",
      "Epoch 6, Batch 510, LR 0.000097 Loss 11.292064, Accuracy 54.695%\n",
      "Epoch 6, Batch 511, LR 0.000097 Loss 11.292152, Accuracy 54.703%\n",
      "Epoch 6, Batch 512, LR 0.000097 Loss 11.291148, Accuracy 54.713%\n",
      "Epoch 6, Batch 513, LR 0.000097 Loss 11.292082, Accuracy 54.704%\n",
      "Epoch 6, Batch 514, LR 0.000097 Loss 11.291254, Accuracy 54.710%\n",
      "Epoch 6, Batch 515, LR 0.000097 Loss 11.291024, Accuracy 54.709%\n",
      "Epoch 6, Batch 516, LR 0.000097 Loss 11.290111, Accuracy 54.704%\n",
      "Epoch 6, Batch 517, LR 0.000097 Loss 11.289765, Accuracy 54.704%\n",
      "Epoch 6, Batch 518, LR 0.000097 Loss 11.289943, Accuracy 54.700%\n",
      "Epoch 6, Batch 519, LR 0.000097 Loss 11.288688, Accuracy 54.707%\n",
      "Epoch 6, Batch 520, LR 0.000097 Loss 11.286756, Accuracy 54.721%\n",
      "Epoch 6, Batch 521, LR 0.000097 Loss 11.288627, Accuracy 54.711%\n",
      "Epoch 6, Batch 522, LR 0.000097 Loss 11.287538, Accuracy 54.720%\n",
      "Epoch 6, Batch 523, LR 0.000097 Loss 11.287090, Accuracy 54.725%\n",
      "Epoch 6, Batch 524, LR 0.000097 Loss 11.285071, Accuracy 54.737%\n",
      "Epoch 6, Batch 525, LR 0.000097 Loss 11.284177, Accuracy 54.751%\n",
      "Epoch 6, Batch 526, LR 0.000097 Loss 11.283861, Accuracy 54.759%\n",
      "Epoch 6, Batch 527, LR 0.000097 Loss 11.285443, Accuracy 54.747%\n",
      "Epoch 6, Batch 528, LR 0.000097 Loss 11.284785, Accuracy 54.747%\n",
      "Epoch 6, Batch 529, LR 0.000097 Loss 11.284714, Accuracy 54.748%\n",
      "Epoch 6, Batch 530, LR 0.000097 Loss 11.284190, Accuracy 54.742%\n",
      "Epoch 6, Batch 531, LR 0.000097 Loss 11.285215, Accuracy 54.736%\n",
      "Epoch 6, Batch 532, LR 0.000097 Loss 11.285294, Accuracy 54.740%\n",
      "Epoch 6, Batch 533, LR 0.000097 Loss 11.286611, Accuracy 54.724%\n",
      "Epoch 6, Batch 534, LR 0.000097 Loss 11.285584, Accuracy 54.724%\n",
      "Epoch 6, Batch 535, LR 0.000097 Loss 11.284991, Accuracy 54.733%\n",
      "Epoch 6, Batch 536, LR 0.000097 Loss 11.285472, Accuracy 54.727%\n",
      "Epoch 6, Batch 537, LR 0.000097 Loss 11.286466, Accuracy 54.721%\n",
      "Epoch 6, Batch 538, LR 0.000097 Loss 11.287399, Accuracy 54.703%\n",
      "Epoch 6, Batch 539, LR 0.000097 Loss 11.286435, Accuracy 54.711%\n",
      "Epoch 6, Batch 540, LR 0.000097 Loss 11.286836, Accuracy 54.715%\n",
      "Epoch 6, Batch 541, LR 0.000097 Loss 11.286963, Accuracy 54.711%\n",
      "Epoch 6, Batch 542, LR 0.000097 Loss 11.285610, Accuracy 54.724%\n",
      "Epoch 6, Batch 543, LR 0.000097 Loss 11.284599, Accuracy 54.732%\n",
      "Epoch 6, Batch 544, LR 0.000097 Loss 11.285467, Accuracy 54.718%\n",
      "Epoch 6, Batch 545, LR 0.000097 Loss 11.285960, Accuracy 54.702%\n",
      "Epoch 6, Batch 546, LR 0.000097 Loss 11.284597, Accuracy 54.705%\n",
      "Epoch 6, Batch 547, LR 0.000097 Loss 11.282290, Accuracy 54.723%\n",
      "Epoch 6, Batch 548, LR 0.000097 Loss 11.283115, Accuracy 54.719%\n",
      "Epoch 6, Batch 549, LR 0.000097 Loss 11.282066, Accuracy 54.732%\n",
      "Epoch 6, Batch 550, LR 0.000097 Loss 11.282199, Accuracy 54.726%\n",
      "Epoch 6, Batch 551, LR 0.000097 Loss 11.279618, Accuracy 54.746%\n",
      "Epoch 6, Batch 552, LR 0.000097 Loss 11.278958, Accuracy 54.743%\n",
      "Epoch 6, Batch 553, LR 0.000097 Loss 11.278597, Accuracy 54.752%\n",
      "Epoch 6, Batch 554, LR 0.000097 Loss 11.279290, Accuracy 54.740%\n",
      "Epoch 6, Batch 555, LR 0.000097 Loss 11.279675, Accuracy 54.742%\n",
      "Epoch 6, Batch 556, LR 0.000097 Loss 11.279609, Accuracy 54.744%\n",
      "Epoch 6, Batch 557, LR 0.000097 Loss 11.280004, Accuracy 54.737%\n",
      "Epoch 6, Batch 558, LR 0.000097 Loss 11.278862, Accuracy 54.744%\n",
      "Epoch 6, Batch 559, LR 0.000097 Loss 11.278911, Accuracy 54.752%\n",
      "Epoch 6, Batch 560, LR 0.000097 Loss 11.278108, Accuracy 54.749%\n",
      "Epoch 6, Batch 561, LR 0.000097 Loss 11.278720, Accuracy 54.743%\n",
      "Epoch 6, Batch 562, LR 0.000097 Loss 11.278267, Accuracy 54.743%\n",
      "Epoch 6, Batch 563, LR 0.000097 Loss 11.277418, Accuracy 54.746%\n",
      "Epoch 6, Batch 564, LR 0.000097 Loss 11.277334, Accuracy 54.733%\n",
      "Epoch 6, Batch 565, LR 0.000097 Loss 11.277103, Accuracy 54.732%\n",
      "Epoch 6, Batch 566, LR 0.000097 Loss 11.276397, Accuracy 54.732%\n",
      "Epoch 6, Batch 567, LR 0.000097 Loss 11.276990, Accuracy 54.726%\n",
      "Epoch 6, Batch 568, LR 0.000097 Loss 11.276201, Accuracy 54.726%\n",
      "Epoch 6, Batch 569, LR 0.000097 Loss 11.274948, Accuracy 54.723%\n",
      "Epoch 6, Batch 570, LR 0.000097 Loss 11.274367, Accuracy 54.731%\n",
      "Epoch 6, Batch 571, LR 0.000097 Loss 11.274930, Accuracy 54.722%\n",
      "Epoch 6, Batch 572, LR 0.000097 Loss 11.274475, Accuracy 54.726%\n",
      "Epoch 6, Batch 573, LR 0.000097 Loss 11.274889, Accuracy 54.723%\n",
      "Epoch 6, Batch 574, LR 0.000097 Loss 11.273881, Accuracy 54.727%\n",
      "Epoch 6, Batch 575, LR 0.000097 Loss 11.274437, Accuracy 54.730%\n",
      "Epoch 6, Batch 576, LR 0.000097 Loss 11.274753, Accuracy 54.719%\n",
      "Epoch 6, Batch 577, LR 0.000097 Loss 11.275448, Accuracy 54.715%\n",
      "Epoch 6, Batch 578, LR 0.000097 Loss 11.275687, Accuracy 54.705%\n",
      "Epoch 6, Batch 579, LR 0.000096 Loss 11.275109, Accuracy 54.708%\n",
      "Epoch 6, Batch 580, LR 0.000096 Loss 11.274984, Accuracy 54.705%\n",
      "Epoch 6, Batch 581, LR 0.000096 Loss 11.274795, Accuracy 54.704%\n",
      "Epoch 6, Batch 582, LR 0.000096 Loss 11.273833, Accuracy 54.709%\n",
      "Epoch 6, Batch 583, LR 0.000096 Loss 11.272977, Accuracy 54.713%\n",
      "Epoch 6, Batch 584, LR 0.000096 Loss 11.272369, Accuracy 54.712%\n",
      "Epoch 6, Batch 585, LR 0.000096 Loss 11.271141, Accuracy 54.716%\n",
      "Epoch 6, Batch 586, LR 0.000096 Loss 11.271158, Accuracy 54.717%\n",
      "Epoch 6, Batch 587, LR 0.000096 Loss 11.269995, Accuracy 54.726%\n",
      "Epoch 6, Batch 588, LR 0.000096 Loss 11.270609, Accuracy 54.727%\n",
      "Epoch 6, Batch 589, LR 0.000096 Loss 11.270750, Accuracy 54.721%\n",
      "Epoch 6, Batch 590, LR 0.000096 Loss 11.269698, Accuracy 54.727%\n",
      "Epoch 6, Batch 591, LR 0.000096 Loss 11.269418, Accuracy 54.732%\n",
      "Epoch 6, Batch 592, LR 0.000096 Loss 11.268925, Accuracy 54.736%\n",
      "Epoch 6, Batch 593, LR 0.000096 Loss 11.268193, Accuracy 54.739%\n",
      "Epoch 6, Batch 594, LR 0.000096 Loss 11.268078, Accuracy 54.737%\n",
      "Epoch 6, Batch 595, LR 0.000096 Loss 11.268250, Accuracy 54.735%\n",
      "Epoch 6, Batch 596, LR 0.000096 Loss 11.267233, Accuracy 54.735%\n",
      "Epoch 6, Batch 597, LR 0.000096 Loss 11.267054, Accuracy 54.742%\n",
      "Epoch 6, Batch 598, LR 0.000096 Loss 11.265355, Accuracy 54.763%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 599, LR 0.000096 Loss 11.265935, Accuracy 54.759%\n",
      "Epoch 6, Batch 600, LR 0.000096 Loss 11.266287, Accuracy 54.754%\n",
      "Epoch 6, Batch 601, LR 0.000096 Loss 11.266036, Accuracy 54.760%\n",
      "Epoch 6, Batch 602, LR 0.000096 Loss 11.264750, Accuracy 54.778%\n",
      "Epoch 6, Batch 603, LR 0.000096 Loss 11.263645, Accuracy 54.787%\n",
      "Epoch 6, Batch 604, LR 0.000096 Loss 11.263342, Accuracy 54.786%\n",
      "Epoch 6, Batch 605, LR 0.000096 Loss 11.262847, Accuracy 54.787%\n",
      "Epoch 6, Batch 606, LR 0.000096 Loss 11.263486, Accuracy 54.782%\n",
      "Epoch 6, Batch 607, LR 0.000096 Loss 11.262792, Accuracy 54.792%\n",
      "Epoch 6, Batch 608, LR 0.000096 Loss 11.262652, Accuracy 54.792%\n",
      "Epoch 6, Batch 609, LR 0.000096 Loss 11.263444, Accuracy 54.790%\n",
      "Epoch 6, Batch 610, LR 0.000096 Loss 11.263656, Accuracy 54.786%\n",
      "Epoch 6, Batch 611, LR 0.000096 Loss 11.263679, Accuracy 54.791%\n",
      "Epoch 6, Batch 612, LR 0.000096 Loss 11.262224, Accuracy 54.795%\n",
      "Epoch 6, Batch 613, LR 0.000096 Loss 11.261307, Accuracy 54.797%\n",
      "Epoch 6, Batch 614, LR 0.000096 Loss 11.258845, Accuracy 54.815%\n",
      "Epoch 6, Batch 615, LR 0.000096 Loss 11.260512, Accuracy 54.802%\n",
      "Epoch 6, Batch 616, LR 0.000096 Loss 11.260354, Accuracy 54.807%\n",
      "Epoch 6, Batch 617, LR 0.000096 Loss 11.260877, Accuracy 54.798%\n",
      "Epoch 6, Batch 618, LR 0.000096 Loss 11.260251, Accuracy 54.806%\n",
      "Epoch 6, Batch 619, LR 0.000096 Loss 11.259649, Accuracy 54.815%\n",
      "Epoch 6, Batch 620, LR 0.000096 Loss 11.259812, Accuracy 54.812%\n",
      "Epoch 6, Batch 621, LR 0.000096 Loss 11.259457, Accuracy 54.813%\n",
      "Epoch 6, Batch 622, LR 0.000096 Loss 11.260058, Accuracy 54.803%\n",
      "Epoch 6, Batch 623, LR 0.000096 Loss 11.260503, Accuracy 54.802%\n",
      "Epoch 6, Batch 624, LR 0.000096 Loss 11.259626, Accuracy 54.813%\n",
      "Epoch 6, Batch 625, LR 0.000096 Loss 11.260018, Accuracy 54.810%\n",
      "Epoch 6, Batch 626, LR 0.000096 Loss 11.260581, Accuracy 54.811%\n",
      "Epoch 6, Batch 627, LR 0.000096 Loss 11.260402, Accuracy 54.808%\n",
      "Epoch 6, Batch 628, LR 0.000096 Loss 11.259693, Accuracy 54.819%\n",
      "Epoch 6, Batch 629, LR 0.000096 Loss 11.261778, Accuracy 54.805%\n",
      "Epoch 6, Batch 630, LR 0.000096 Loss 11.260400, Accuracy 54.809%\n",
      "Epoch 6, Batch 631, LR 0.000096 Loss 11.261209, Accuracy 54.805%\n",
      "Epoch 6, Batch 632, LR 0.000096 Loss 11.260114, Accuracy 54.807%\n",
      "Epoch 6, Batch 633, LR 0.000096 Loss 11.259958, Accuracy 54.806%\n",
      "Epoch 6, Batch 634, LR 0.000096 Loss 11.259627, Accuracy 54.811%\n",
      "Epoch 6, Batch 635, LR 0.000096 Loss 11.259200, Accuracy 54.813%\n",
      "Epoch 6, Batch 636, LR 0.000096 Loss 11.259077, Accuracy 54.815%\n",
      "Epoch 6, Batch 637, LR 0.000096 Loss 11.258725, Accuracy 54.808%\n",
      "Epoch 6, Batch 638, LR 0.000096 Loss 11.258986, Accuracy 54.801%\n",
      "Epoch 6, Batch 639, LR 0.000096 Loss 11.258070, Accuracy 54.804%\n",
      "Epoch 6, Batch 640, LR 0.000096 Loss 11.257992, Accuracy 54.797%\n",
      "Epoch 6, Batch 641, LR 0.000096 Loss 11.257214, Accuracy 54.801%\n",
      "Epoch 6, Batch 642, LR 0.000096 Loss 11.257815, Accuracy 54.798%\n",
      "Epoch 6, Batch 643, LR 0.000096 Loss 11.258001, Accuracy 54.802%\n",
      "Epoch 6, Batch 644, LR 0.000096 Loss 11.258010, Accuracy 54.797%\n",
      "Epoch 6, Batch 645, LR 0.000096 Loss 11.258560, Accuracy 54.793%\n",
      "Epoch 6, Batch 646, LR 0.000096 Loss 11.258184, Accuracy 54.795%\n",
      "Epoch 6, Batch 647, LR 0.000096 Loss 11.258323, Accuracy 54.793%\n",
      "Epoch 6, Batch 648, LR 0.000096 Loss 11.258943, Accuracy 54.791%\n",
      "Epoch 6, Batch 649, LR 0.000096 Loss 11.257851, Accuracy 54.796%\n",
      "Epoch 6, Batch 650, LR 0.000096 Loss 11.256515, Accuracy 54.803%\n",
      "Epoch 6, Batch 651, LR 0.000096 Loss 11.255663, Accuracy 54.806%\n",
      "Epoch 6, Batch 652, LR 0.000096 Loss 11.255398, Accuracy 54.816%\n",
      "Epoch 6, Batch 653, LR 0.000096 Loss 11.254736, Accuracy 54.823%\n",
      "Epoch 6, Batch 654, LR 0.000096 Loss 11.254824, Accuracy 54.831%\n",
      "Epoch 6, Batch 655, LR 0.000096 Loss 11.255209, Accuracy 54.831%\n",
      "Epoch 6, Batch 656, LR 0.000096 Loss 11.255872, Accuracy 54.826%\n",
      "Epoch 6, Batch 657, LR 0.000096 Loss 11.255874, Accuracy 54.837%\n",
      "Epoch 6, Batch 658, LR 0.000096 Loss 11.255175, Accuracy 54.836%\n",
      "Epoch 6, Batch 659, LR 0.000096 Loss 11.254188, Accuracy 54.845%\n",
      "Epoch 6, Batch 660, LR 0.000096 Loss 11.253600, Accuracy 54.848%\n",
      "Epoch 6, Batch 661, LR 0.000096 Loss 11.253241, Accuracy 54.858%\n",
      "Epoch 6, Batch 662, LR 0.000096 Loss 11.251377, Accuracy 54.872%\n",
      "Epoch 6, Batch 663, LR 0.000096 Loss 11.250948, Accuracy 54.874%\n",
      "Epoch 6, Batch 664, LR 0.000096 Loss 11.251013, Accuracy 54.869%\n",
      "Epoch 6, Batch 665, LR 0.000096 Loss 11.249989, Accuracy 54.875%\n",
      "Epoch 6, Batch 666, LR 0.000096 Loss 11.248990, Accuracy 54.885%\n",
      "Epoch 6, Batch 667, LR 0.000096 Loss 11.247012, Accuracy 54.901%\n",
      "Epoch 6, Batch 668, LR 0.000096 Loss 11.245710, Accuracy 54.912%\n",
      "Epoch 6, Batch 669, LR 0.000096 Loss 11.244865, Accuracy 54.927%\n",
      "Epoch 6, Batch 670, LR 0.000096 Loss 11.244506, Accuracy 54.936%\n",
      "Epoch 6, Batch 671, LR 0.000096 Loss 11.243819, Accuracy 54.942%\n",
      "Epoch 6, Batch 672, LR 0.000096 Loss 11.243925, Accuracy 54.943%\n",
      "Epoch 6, Batch 673, LR 0.000096 Loss 11.243706, Accuracy 54.946%\n",
      "Epoch 6, Batch 674, LR 0.000096 Loss 11.243611, Accuracy 54.949%\n",
      "Epoch 6, Batch 675, LR 0.000096 Loss 11.243413, Accuracy 54.947%\n",
      "Epoch 6, Batch 676, LR 0.000096 Loss 11.242936, Accuracy 54.945%\n",
      "Epoch 6, Batch 677, LR 0.000096 Loss 11.242956, Accuracy 54.940%\n",
      "Epoch 6, Batch 678, LR 0.000096 Loss 11.242023, Accuracy 54.942%\n",
      "Epoch 6, Batch 679, LR 0.000096 Loss 11.242581, Accuracy 54.944%\n",
      "Epoch 6, Batch 680, LR 0.000096 Loss 11.243705, Accuracy 54.933%\n",
      "Epoch 6, Batch 681, LR 0.000096 Loss 11.243459, Accuracy 54.930%\n",
      "Epoch 6, Batch 682, LR 0.000096 Loss 11.243659, Accuracy 54.922%\n",
      "Epoch 6, Batch 683, LR 0.000096 Loss 11.243394, Accuracy 54.931%\n",
      "Epoch 6, Batch 684, LR 0.000096 Loss 11.244116, Accuracy 54.930%\n",
      "Epoch 6, Batch 685, LR 0.000096 Loss 11.243645, Accuracy 54.925%\n",
      "Epoch 6, Batch 686, LR 0.000096 Loss 11.244065, Accuracy 54.916%\n",
      "Epoch 6, Batch 687, LR 0.000096 Loss 11.244036, Accuracy 54.916%\n",
      "Epoch 6, Batch 688, LR 0.000096 Loss 11.244610, Accuracy 54.920%\n",
      "Epoch 6, Batch 689, LR 0.000096 Loss 11.244638, Accuracy 54.924%\n",
      "Epoch 6, Batch 690, LR 0.000096 Loss 11.244609, Accuracy 54.926%\n",
      "Epoch 6, Batch 691, LR 0.000096 Loss 11.245124, Accuracy 54.920%\n",
      "Epoch 6, Batch 692, LR 0.000096 Loss 11.245116, Accuracy 54.919%\n",
      "Epoch 6, Batch 693, LR 0.000096 Loss 11.244726, Accuracy 54.924%\n",
      "Epoch 6, Batch 694, LR 0.000096 Loss 11.244627, Accuracy 54.922%\n",
      "Epoch 6, Batch 695, LR 0.000096 Loss 11.243525, Accuracy 54.931%\n",
      "Epoch 6, Batch 696, LR 0.000096 Loss 11.242376, Accuracy 54.941%\n",
      "Epoch 6, Batch 697, LR 0.000096 Loss 11.240891, Accuracy 54.944%\n",
      "Epoch 6, Batch 698, LR 0.000096 Loss 11.240609, Accuracy 54.953%\n",
      "Epoch 6, Batch 699, LR 0.000096 Loss 11.240130, Accuracy 54.951%\n",
      "Epoch 6, Batch 700, LR 0.000096 Loss 11.239824, Accuracy 54.954%\n",
      "Epoch 6, Batch 701, LR 0.000096 Loss 11.239380, Accuracy 54.955%\n",
      "Epoch 6, Batch 702, LR 0.000096 Loss 11.239323, Accuracy 54.952%\n",
      "Epoch 6, Batch 703, LR 0.000096 Loss 11.238592, Accuracy 54.961%\n",
      "Epoch 6, Batch 704, LR 0.000096 Loss 11.238870, Accuracy 54.962%\n",
      "Epoch 6, Batch 705, LR 0.000096 Loss 11.238790, Accuracy 54.965%\n",
      "Epoch 6, Batch 706, LR 0.000096 Loss 11.237883, Accuracy 54.966%\n",
      "Epoch 6, Batch 707, LR 0.000096 Loss 11.238324, Accuracy 54.963%\n",
      "Epoch 6, Batch 708, LR 0.000096 Loss 11.237869, Accuracy 54.967%\n",
      "Epoch 6, Batch 709, LR 0.000096 Loss 11.237068, Accuracy 54.966%\n",
      "Epoch 6, Batch 710, LR 0.000096 Loss 11.236389, Accuracy 54.977%\n",
      "Epoch 6, Batch 711, LR 0.000096 Loss 11.237261, Accuracy 54.968%\n",
      "Epoch 6, Batch 712, LR 0.000096 Loss 11.236270, Accuracy 54.971%\n",
      "Epoch 6, Batch 713, LR 0.000096 Loss 11.236538, Accuracy 54.967%\n",
      "Epoch 6, Batch 714, LR 0.000096 Loss 11.236916, Accuracy 54.965%\n",
      "Epoch 6, Batch 715, LR 0.000096 Loss 11.236265, Accuracy 54.972%\n",
      "Epoch 6, Batch 716, LR 0.000096 Loss 11.233673, Accuracy 54.983%\n",
      "Epoch 6, Batch 717, LR 0.000096 Loss 11.233538, Accuracy 54.982%\n",
      "Epoch 6, Batch 718, LR 0.000096 Loss 11.233175, Accuracy 54.989%\n",
      "Epoch 6, Batch 719, LR 0.000096 Loss 11.232739, Accuracy 54.995%\n",
      "Epoch 6, Batch 720, LR 0.000096 Loss 11.232586, Accuracy 54.990%\n",
      "Epoch 6, Batch 721, LR 0.000096 Loss 11.232590, Accuracy 54.992%\n",
      "Epoch 6, Batch 722, LR 0.000096 Loss 11.230818, Accuracy 54.997%\n",
      "Epoch 6, Batch 723, LR 0.000096 Loss 11.230723, Accuracy 54.992%\n",
      "Epoch 6, Batch 724, LR 0.000096 Loss 11.231032, Accuracy 54.989%\n",
      "Epoch 6, Batch 725, LR 0.000096 Loss 11.231638, Accuracy 54.982%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 726, LR 0.000096 Loss 11.232137, Accuracy 54.977%\n",
      "Epoch 6, Batch 727, LR 0.000096 Loss 11.232087, Accuracy 54.980%\n",
      "Epoch 6, Batch 728, LR 0.000096 Loss 11.232747, Accuracy 54.973%\n",
      "Epoch 6, Batch 729, LR 0.000096 Loss 11.232410, Accuracy 54.973%\n",
      "Epoch 6, Batch 730, LR 0.000096 Loss 11.231597, Accuracy 54.980%\n",
      "Epoch 6, Batch 731, LR 0.000096 Loss 11.231510, Accuracy 54.985%\n",
      "Epoch 6, Batch 732, LR 0.000096 Loss 11.231747, Accuracy 54.984%\n",
      "Epoch 6, Batch 733, LR 0.000096 Loss 11.230072, Accuracy 55.000%\n",
      "Epoch 6, Batch 734, LR 0.000096 Loss 11.229663, Accuracy 55.010%\n",
      "Epoch 6, Batch 735, LR 0.000096 Loss 11.229189, Accuracy 55.014%\n",
      "Epoch 6, Batch 736, LR 0.000096 Loss 11.228363, Accuracy 55.019%\n",
      "Epoch 6, Batch 737, LR 0.000096 Loss 11.228572, Accuracy 55.022%\n",
      "Epoch 6, Batch 738, LR 0.000096 Loss 11.227854, Accuracy 55.033%\n",
      "Epoch 6, Batch 739, LR 0.000096 Loss 11.226472, Accuracy 55.040%\n",
      "Epoch 6, Batch 740, LR 0.000096 Loss 11.225706, Accuracy 55.049%\n",
      "Epoch 6, Batch 741, LR 0.000096 Loss 11.225046, Accuracy 55.055%\n",
      "Epoch 6, Batch 742, LR 0.000096 Loss 11.225795, Accuracy 55.047%\n",
      "Epoch 6, Batch 743, LR 0.000096 Loss 11.224916, Accuracy 55.050%\n",
      "Epoch 6, Batch 744, LR 0.000096 Loss 11.225270, Accuracy 55.038%\n",
      "Epoch 6, Batch 745, LR 0.000096 Loss 11.223722, Accuracy 55.044%\n",
      "Epoch 6, Batch 746, LR 0.000096 Loss 11.223681, Accuracy 55.048%\n",
      "Epoch 6, Batch 747, LR 0.000096 Loss 11.222439, Accuracy 55.057%\n",
      "Epoch 6, Batch 748, LR 0.000096 Loss 11.222312, Accuracy 55.050%\n",
      "Epoch 6, Batch 749, LR 0.000096 Loss 11.221944, Accuracy 55.053%\n",
      "Epoch 6, Batch 750, LR 0.000096 Loss 11.221380, Accuracy 55.054%\n",
      "Epoch 6, Batch 751, LR 0.000096 Loss 11.222020, Accuracy 55.048%\n",
      "Epoch 6, Batch 752, LR 0.000096 Loss 11.221673, Accuracy 55.055%\n",
      "Epoch 6, Batch 753, LR 0.000096 Loss 11.222160, Accuracy 55.052%\n",
      "Epoch 6, Batch 754, LR 0.000096 Loss 11.221629, Accuracy 55.057%\n",
      "Epoch 6, Batch 755, LR 0.000096 Loss 11.220682, Accuracy 55.067%\n",
      "Epoch 6, Batch 756, LR 0.000096 Loss 11.220920, Accuracy 55.067%\n",
      "Epoch 6, Batch 757, LR 0.000096 Loss 11.220682, Accuracy 55.069%\n",
      "Epoch 6, Batch 758, LR 0.000096 Loss 11.220822, Accuracy 55.067%\n",
      "Epoch 6, Batch 759, LR 0.000096 Loss 11.220514, Accuracy 55.070%\n",
      "Epoch 6, Batch 760, LR 0.000096 Loss 11.219785, Accuracy 55.076%\n",
      "Epoch 6, Batch 761, LR 0.000096 Loss 11.220426, Accuracy 55.067%\n",
      "Epoch 6, Batch 762, LR 0.000096 Loss 11.220063, Accuracy 55.074%\n",
      "Epoch 6, Batch 763, LR 0.000096 Loss 11.219559, Accuracy 55.074%\n",
      "Epoch 6, Batch 764, LR 0.000096 Loss 11.219038, Accuracy 55.077%\n",
      "Epoch 6, Batch 765, LR 0.000096 Loss 11.217590, Accuracy 55.080%\n",
      "Epoch 6, Batch 766, LR 0.000096 Loss 11.216026, Accuracy 55.092%\n",
      "Epoch 6, Batch 767, LR 0.000096 Loss 11.215835, Accuracy 55.095%\n",
      "Epoch 6, Batch 768, LR 0.000096 Loss 11.215658, Accuracy 55.091%\n",
      "Epoch 6, Batch 769, LR 0.000096 Loss 11.214734, Accuracy 55.093%\n",
      "Epoch 6, Batch 770, LR 0.000096 Loss 11.214512, Accuracy 55.098%\n",
      "Epoch 6, Batch 771, LR 0.000096 Loss 11.214223, Accuracy 55.111%\n",
      "Epoch 6, Batch 772, LR 0.000096 Loss 11.213676, Accuracy 55.118%\n",
      "Epoch 6, Batch 773, LR 0.000096 Loss 11.214333, Accuracy 55.110%\n",
      "Epoch 6, Batch 774, LR 0.000096 Loss 11.214316, Accuracy 55.111%\n",
      "Epoch 6, Batch 775, LR 0.000096 Loss 11.213735, Accuracy 55.115%\n",
      "Epoch 6, Batch 776, LR 0.000096 Loss 11.214181, Accuracy 55.101%\n",
      "Epoch 6, Batch 777, LR 0.000096 Loss 11.213339, Accuracy 55.106%\n",
      "Epoch 6, Batch 778, LR 0.000096 Loss 11.213529, Accuracy 55.104%\n",
      "Epoch 6, Batch 779, LR 0.000096 Loss 11.213448, Accuracy 55.107%\n",
      "Epoch 6, Batch 780, LR 0.000096 Loss 11.211735, Accuracy 55.118%\n",
      "Epoch 6, Batch 781, LR 0.000096 Loss 11.212534, Accuracy 55.113%\n",
      "Epoch 6, Batch 782, LR 0.000096 Loss 11.212551, Accuracy 55.119%\n",
      "Epoch 6, Batch 783, LR 0.000096 Loss 11.212330, Accuracy 55.125%\n",
      "Epoch 6, Batch 784, LR 0.000096 Loss 11.211574, Accuracy 55.129%\n",
      "Epoch 6, Batch 785, LR 0.000096 Loss 11.211577, Accuracy 55.127%\n",
      "Epoch 6, Batch 786, LR 0.000096 Loss 11.211117, Accuracy 55.131%\n",
      "Epoch 6, Batch 787, LR 0.000096 Loss 11.210420, Accuracy 55.135%\n",
      "Epoch 6, Batch 788, LR 0.000096 Loss 11.209642, Accuracy 55.141%\n",
      "Epoch 6, Batch 789, LR 0.000096 Loss 11.209294, Accuracy 55.145%\n",
      "Epoch 6, Batch 790, LR 0.000096 Loss 11.208938, Accuracy 55.146%\n",
      "Epoch 6, Batch 791, LR 0.000096 Loss 11.207325, Accuracy 55.155%\n",
      "Epoch 6, Batch 792, LR 0.000096 Loss 11.206623, Accuracy 55.156%\n",
      "Epoch 6, Batch 793, LR 0.000096 Loss 11.207024, Accuracy 55.146%\n",
      "Epoch 6, Batch 794, LR 0.000096 Loss 11.206242, Accuracy 55.153%\n",
      "Epoch 6, Batch 795, LR 0.000096 Loss 11.205690, Accuracy 55.154%\n",
      "Epoch 6, Batch 796, LR 0.000096 Loss 11.205393, Accuracy 55.155%\n",
      "Epoch 6, Batch 797, LR 0.000096 Loss 11.204439, Accuracy 55.160%\n",
      "Epoch 6, Batch 798, LR 0.000096 Loss 11.203808, Accuracy 55.165%\n",
      "Epoch 6, Batch 799, LR 0.000096 Loss 11.203246, Accuracy 55.169%\n",
      "Epoch 6, Batch 800, LR 0.000096 Loss 11.203260, Accuracy 55.164%\n",
      "Epoch 6, Batch 801, LR 0.000096 Loss 11.202481, Accuracy 55.172%\n",
      "Epoch 6, Batch 802, LR 0.000096 Loss 11.201771, Accuracy 55.179%\n",
      "Epoch 6, Batch 803, LR 0.000096 Loss 11.201623, Accuracy 55.179%\n",
      "Epoch 6, Batch 804, LR 0.000096 Loss 11.202089, Accuracy 55.176%\n",
      "Epoch 6, Batch 805, LR 0.000096 Loss 11.201467, Accuracy 55.187%\n",
      "Epoch 6, Batch 806, LR 0.000096 Loss 11.201635, Accuracy 55.188%\n",
      "Epoch 6, Batch 807, LR 0.000096 Loss 11.200859, Accuracy 55.190%\n",
      "Epoch 6, Batch 808, LR 0.000096 Loss 11.201202, Accuracy 55.192%\n",
      "Epoch 6, Batch 809, LR 0.000096 Loss 11.200076, Accuracy 55.202%\n",
      "Epoch 6, Batch 810, LR 0.000096 Loss 11.199831, Accuracy 55.197%\n",
      "Epoch 6, Batch 811, LR 0.000096 Loss 11.198714, Accuracy 55.204%\n",
      "Epoch 6, Batch 812, LR 0.000096 Loss 11.198199, Accuracy 55.205%\n",
      "Epoch 6, Batch 813, LR 0.000096 Loss 11.198258, Accuracy 55.207%\n",
      "Epoch 6, Batch 814, LR 0.000096 Loss 11.197111, Accuracy 55.215%\n",
      "Epoch 6, Batch 815, LR 0.000096 Loss 11.196513, Accuracy 55.218%\n",
      "Epoch 6, Batch 816, LR 0.000096 Loss 11.196507, Accuracy 55.223%\n",
      "Epoch 6, Batch 817, LR 0.000096 Loss 11.195493, Accuracy 55.235%\n",
      "Epoch 6, Batch 818, LR 0.000096 Loss 11.195540, Accuracy 55.231%\n",
      "Epoch 6, Batch 819, LR 0.000096 Loss 11.194990, Accuracy 55.233%\n",
      "Epoch 6, Batch 820, LR 0.000096 Loss 11.194289, Accuracy 55.246%\n",
      "Epoch 6, Batch 821, LR 0.000096 Loss 11.193076, Accuracy 55.258%\n",
      "Epoch 6, Batch 822, LR 0.000096 Loss 11.192122, Accuracy 55.258%\n",
      "Epoch 6, Batch 823, LR 0.000096 Loss 11.192008, Accuracy 55.264%\n",
      "Epoch 6, Batch 824, LR 0.000096 Loss 11.190346, Accuracy 55.270%\n",
      "Epoch 6, Batch 825, LR 0.000096 Loss 11.189961, Accuracy 55.277%\n",
      "Epoch 6, Batch 826, LR 0.000096 Loss 11.189526, Accuracy 55.274%\n",
      "Epoch 6, Batch 827, LR 0.000096 Loss 11.189791, Accuracy 55.268%\n",
      "Epoch 6, Batch 828, LR 0.000096 Loss 11.190085, Accuracy 55.270%\n",
      "Epoch 6, Batch 829, LR 0.000096 Loss 11.190862, Accuracy 55.258%\n",
      "Epoch 6, Batch 830, LR 0.000096 Loss 11.189833, Accuracy 55.267%\n",
      "Epoch 6, Batch 831, LR 0.000096 Loss 11.189835, Accuracy 55.266%\n",
      "Epoch 6, Batch 832, LR 0.000096 Loss 11.189312, Accuracy 55.270%\n",
      "Epoch 6, Batch 833, LR 0.000096 Loss 11.188389, Accuracy 55.270%\n",
      "Epoch 6, Batch 834, LR 0.000096 Loss 11.187961, Accuracy 55.273%\n",
      "Epoch 6, Batch 835, LR 0.000096 Loss 11.187636, Accuracy 55.276%\n",
      "Epoch 6, Batch 836, LR 0.000096 Loss 11.188845, Accuracy 55.266%\n",
      "Epoch 6, Batch 837, LR 0.000096 Loss 11.187954, Accuracy 55.270%\n",
      "Epoch 6, Batch 838, LR 0.000096 Loss 11.188308, Accuracy 55.264%\n",
      "Epoch 6, Batch 839, LR 0.000096 Loss 11.188254, Accuracy 55.261%\n",
      "Epoch 6, Batch 840, LR 0.000096 Loss 11.188125, Accuracy 55.268%\n",
      "Epoch 6, Batch 841, LR 0.000096 Loss 11.188355, Accuracy 55.265%\n",
      "Epoch 6, Batch 842, LR 0.000096 Loss 11.188272, Accuracy 55.269%\n",
      "Epoch 6, Batch 843, LR 0.000096 Loss 11.187864, Accuracy 55.270%\n",
      "Epoch 6, Batch 844, LR 0.000096 Loss 11.188295, Accuracy 55.274%\n",
      "Epoch 6, Batch 845, LR 0.000096 Loss 11.188709, Accuracy 55.274%\n",
      "Epoch 6, Batch 846, LR 0.000096 Loss 11.188147, Accuracy 55.283%\n",
      "Epoch 6, Batch 847, LR 0.000096 Loss 11.187965, Accuracy 55.286%\n",
      "Epoch 6, Batch 848, LR 0.000096 Loss 11.187398, Accuracy 55.289%\n",
      "Epoch 6, Batch 849, LR 0.000096 Loss 11.186684, Accuracy 55.292%\n",
      "Epoch 6, Batch 850, LR 0.000096 Loss 11.186645, Accuracy 55.287%\n",
      "Epoch 6, Batch 851, LR 0.000096 Loss 11.186124, Accuracy 55.290%\n",
      "Epoch 6, Batch 852, LR 0.000096 Loss 11.185062, Accuracy 55.291%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 853, LR 0.000096 Loss 11.184129, Accuracy 55.294%\n",
      "Epoch 6, Batch 854, LR 0.000096 Loss 11.183897, Accuracy 55.298%\n",
      "Epoch 6, Batch 855, LR 0.000096 Loss 11.183750, Accuracy 55.301%\n",
      "Epoch 6, Batch 856, LR 0.000096 Loss 11.183877, Accuracy 55.301%\n",
      "Epoch 6, Batch 857, LR 0.000096 Loss 11.183073, Accuracy 55.310%\n",
      "Epoch 6, Batch 858, LR 0.000096 Loss 11.182529, Accuracy 55.309%\n",
      "Epoch 6, Batch 859, LR 0.000096 Loss 11.181883, Accuracy 55.319%\n",
      "Epoch 6, Batch 860, LR 0.000096 Loss 11.181371, Accuracy 55.315%\n",
      "Epoch 6, Batch 861, LR 0.000096 Loss 11.181491, Accuracy 55.313%\n",
      "Epoch 6, Batch 862, LR 0.000096 Loss 11.181171, Accuracy 55.314%\n",
      "Epoch 6, Batch 863, LR 0.000096 Loss 11.181125, Accuracy 55.306%\n",
      "Epoch 6, Batch 864, LR 0.000096 Loss 11.181054, Accuracy 55.311%\n",
      "Epoch 6, Batch 865, LR 0.000096 Loss 11.180881, Accuracy 55.311%\n",
      "Epoch 6, Batch 866, LR 0.000096 Loss 11.179886, Accuracy 55.317%\n",
      "Epoch 6, Batch 867, LR 0.000096 Loss 11.179928, Accuracy 55.315%\n",
      "Epoch 6, Batch 868, LR 0.000096 Loss 11.180146, Accuracy 55.319%\n",
      "Epoch 6, Batch 869, LR 0.000096 Loss 11.180164, Accuracy 55.319%\n",
      "Epoch 6, Batch 870, LR 0.000096 Loss 11.180298, Accuracy 55.321%\n",
      "Epoch 6, Batch 871, LR 0.000096 Loss 11.180647, Accuracy 55.318%\n",
      "Epoch 6, Batch 872, LR 0.000096 Loss 11.180642, Accuracy 55.319%\n",
      "Epoch 6, Batch 873, LR 0.000096 Loss 11.180427, Accuracy 55.323%\n",
      "Epoch 6, Batch 874, LR 0.000096 Loss 11.179738, Accuracy 55.328%\n",
      "Epoch 6, Batch 875, LR 0.000096 Loss 11.178202, Accuracy 55.341%\n",
      "Epoch 6, Batch 876, LR 0.000096 Loss 11.177659, Accuracy 55.339%\n",
      "Epoch 6, Batch 877, LR 0.000096 Loss 11.176753, Accuracy 55.346%\n",
      "Epoch 6, Batch 878, LR 0.000096 Loss 11.176679, Accuracy 55.345%\n",
      "Epoch 6, Batch 879, LR 0.000096 Loss 11.176721, Accuracy 55.350%\n",
      "Epoch 6, Batch 880, LR 0.000096 Loss 11.176431, Accuracy 55.350%\n",
      "Epoch 6, Batch 881, LR 0.000096 Loss 11.175728, Accuracy 55.353%\n",
      "Epoch 6, Batch 882, LR 0.000096 Loss 11.175236, Accuracy 55.357%\n",
      "Epoch 6, Batch 883, LR 0.000096 Loss 11.174539, Accuracy 55.359%\n",
      "Epoch 6, Batch 884, LR 0.000096 Loss 11.174696, Accuracy 55.360%\n",
      "Epoch 6, Batch 885, LR 0.000096 Loss 11.173897, Accuracy 55.365%\n",
      "Epoch 6, Batch 886, LR 0.000096 Loss 11.173020, Accuracy 55.373%\n",
      "Epoch 6, Batch 887, LR 0.000096 Loss 11.172461, Accuracy 55.373%\n",
      "Epoch 6, Batch 888, LR 0.000096 Loss 11.172421, Accuracy 55.370%\n",
      "Epoch 6, Batch 889, LR 0.000096 Loss 11.171188, Accuracy 55.380%\n",
      "Epoch 6, Batch 890, LR 0.000096 Loss 11.169580, Accuracy 55.391%\n",
      "Epoch 6, Batch 891, LR 0.000096 Loss 11.169054, Accuracy 55.393%\n",
      "Epoch 6, Batch 892, LR 0.000096 Loss 11.168697, Accuracy 55.400%\n",
      "Epoch 6, Batch 893, LR 0.000096 Loss 11.169071, Accuracy 55.391%\n",
      "Epoch 6, Batch 894, LR 0.000096 Loss 11.168704, Accuracy 55.391%\n",
      "Epoch 6, Batch 895, LR 0.000096 Loss 11.168461, Accuracy 55.391%\n",
      "Epoch 6, Batch 896, LR 0.000096 Loss 11.168296, Accuracy 55.388%\n",
      "Epoch 6, Batch 897, LR 0.000096 Loss 11.168060, Accuracy 55.393%\n",
      "Epoch 6, Batch 898, LR 0.000096 Loss 11.168147, Accuracy 55.390%\n",
      "Epoch 6, Batch 899, LR 0.000096 Loss 11.168518, Accuracy 55.388%\n",
      "Epoch 6, Batch 900, LR 0.000096 Loss 11.168245, Accuracy 55.391%\n",
      "Epoch 6, Batch 901, LR 0.000096 Loss 11.167958, Accuracy 55.392%\n",
      "Epoch 6, Batch 902, LR 0.000096 Loss 11.168415, Accuracy 55.387%\n",
      "Epoch 6, Batch 903, LR 0.000096 Loss 11.168386, Accuracy 55.386%\n",
      "Epoch 6, Batch 904, LR 0.000096 Loss 11.167782, Accuracy 55.388%\n",
      "Epoch 6, Batch 905, LR 0.000096 Loss 11.167795, Accuracy 55.388%\n",
      "Epoch 6, Batch 906, LR 0.000096 Loss 11.167790, Accuracy 55.385%\n",
      "Epoch 6, Batch 907, LR 0.000096 Loss 11.168277, Accuracy 55.377%\n",
      "Epoch 6, Batch 908, LR 0.000096 Loss 11.168059, Accuracy 55.378%\n",
      "Epoch 6, Batch 909, LR 0.000096 Loss 11.167696, Accuracy 55.378%\n",
      "Epoch 6, Batch 910, LR 0.000096 Loss 11.166414, Accuracy 55.385%\n",
      "Epoch 6, Batch 911, LR 0.000096 Loss 11.166327, Accuracy 55.386%\n",
      "Epoch 6, Batch 912, LR 0.000096 Loss 11.164896, Accuracy 55.396%\n",
      "Epoch 6, Batch 913, LR 0.000096 Loss 11.164493, Accuracy 55.395%\n",
      "Epoch 6, Batch 914, LR 0.000096 Loss 11.164226, Accuracy 55.402%\n",
      "Epoch 6, Batch 915, LR 0.000096 Loss 11.163777, Accuracy 55.404%\n",
      "Epoch 6, Batch 916, LR 0.000096 Loss 11.163287, Accuracy 55.418%\n",
      "Epoch 6, Batch 917, LR 0.000096 Loss 11.163401, Accuracy 55.412%\n",
      "Epoch 6, Batch 918, LR 0.000096 Loss 11.163388, Accuracy 55.411%\n",
      "Epoch 6, Batch 919, LR 0.000096 Loss 11.162981, Accuracy 55.409%\n",
      "Epoch 6, Batch 920, LR 0.000096 Loss 11.163110, Accuracy 55.406%\n",
      "Epoch 6, Batch 921, LR 0.000096 Loss 11.162278, Accuracy 55.412%\n",
      "Epoch 6, Batch 922, LR 0.000096 Loss 11.162024, Accuracy 55.414%\n",
      "Epoch 6, Batch 923, LR 0.000096 Loss 11.161800, Accuracy 55.410%\n",
      "Epoch 6, Batch 924, LR 0.000096 Loss 11.161037, Accuracy 55.419%\n",
      "Epoch 6, Batch 925, LR 0.000096 Loss 11.160814, Accuracy 55.427%\n",
      "Epoch 6, Batch 926, LR 0.000096 Loss 11.160622, Accuracy 55.427%\n",
      "Epoch 6, Batch 927, LR 0.000096 Loss 11.160461, Accuracy 55.425%\n",
      "Epoch 6, Batch 928, LR 0.000096 Loss 11.159866, Accuracy 55.431%\n",
      "Epoch 6, Batch 929, LR 0.000096 Loss 11.159770, Accuracy 55.431%\n",
      "Epoch 6, Batch 930, LR 0.000096 Loss 11.159736, Accuracy 55.430%\n",
      "Epoch 6, Batch 931, LR 0.000096 Loss 11.159893, Accuracy 55.427%\n",
      "Epoch 6, Batch 932, LR 0.000096 Loss 11.159593, Accuracy 55.425%\n",
      "Epoch 6, Batch 933, LR 0.000096 Loss 11.158506, Accuracy 55.434%\n",
      "Epoch 6, Batch 934, LR 0.000096 Loss 11.158717, Accuracy 55.429%\n",
      "Epoch 6, Batch 935, LR 0.000096 Loss 11.158625, Accuracy 55.432%\n",
      "Epoch 6, Batch 936, LR 0.000096 Loss 11.158124, Accuracy 55.437%\n",
      "Epoch 6, Batch 937, LR 0.000096 Loss 11.157500, Accuracy 55.444%\n",
      "Epoch 6, Batch 938, LR 0.000096 Loss 11.157245, Accuracy 55.449%\n",
      "Epoch 6, Batch 939, LR 0.000096 Loss 11.157657, Accuracy 55.445%\n",
      "Epoch 6, Batch 940, LR 0.000096 Loss 11.157756, Accuracy 55.439%\n",
      "Epoch 6, Batch 941, LR 0.000096 Loss 11.157195, Accuracy 55.441%\n",
      "Epoch 6, Batch 942, LR 0.000096 Loss 11.157109, Accuracy 55.441%\n",
      "Epoch 6, Batch 943, LR 0.000096 Loss 11.155751, Accuracy 55.446%\n",
      "Epoch 6, Batch 944, LR 0.000096 Loss 11.154763, Accuracy 55.448%\n",
      "Epoch 6, Batch 945, LR 0.000096 Loss 11.154551, Accuracy 55.455%\n",
      "Epoch 6, Batch 946, LR 0.000096 Loss 11.154061, Accuracy 55.458%\n",
      "Epoch 6, Batch 947, LR 0.000096 Loss 11.154144, Accuracy 55.457%\n",
      "Epoch 6, Batch 948, LR 0.000096 Loss 11.153935, Accuracy 55.459%\n",
      "Epoch 6, Batch 949, LR 0.000096 Loss 11.153706, Accuracy 55.460%\n",
      "Epoch 6, Batch 950, LR 0.000096 Loss 11.154199, Accuracy 55.458%\n",
      "Epoch 6, Batch 951, LR 0.000096 Loss 11.153629, Accuracy 55.466%\n",
      "Epoch 6, Batch 952, LR 0.000096 Loss 11.153341, Accuracy 55.465%\n",
      "Epoch 6, Batch 953, LR 0.000096 Loss 11.153125, Accuracy 55.470%\n",
      "Epoch 6, Batch 954, LR 0.000096 Loss 11.151933, Accuracy 55.483%\n",
      "Epoch 6, Batch 955, LR 0.000096 Loss 11.151642, Accuracy 55.490%\n",
      "Epoch 6, Batch 956, LR 0.000096 Loss 11.150528, Accuracy 55.497%\n",
      "Epoch 6, Batch 957, LR 0.000096 Loss 11.150438, Accuracy 55.501%\n",
      "Epoch 6, Batch 958, LR 0.000096 Loss 11.150115, Accuracy 55.505%\n",
      "Epoch 6, Batch 959, LR 0.000096 Loss 11.149956, Accuracy 55.505%\n",
      "Epoch 6, Batch 960, LR 0.000096 Loss 11.150099, Accuracy 55.505%\n",
      "Epoch 6, Batch 961, LR 0.000096 Loss 11.148670, Accuracy 55.517%\n",
      "Epoch 6, Batch 962, LR 0.000096 Loss 11.148460, Accuracy 55.517%\n",
      "Epoch 6, Batch 963, LR 0.000096 Loss 11.148166, Accuracy 55.517%\n",
      "Epoch 6, Batch 964, LR 0.000096 Loss 11.147981, Accuracy 55.518%\n",
      "Epoch 6, Batch 965, LR 0.000096 Loss 11.147421, Accuracy 55.520%\n",
      "Epoch 6, Batch 966, LR 0.000096 Loss 11.146681, Accuracy 55.526%\n",
      "Epoch 6, Batch 967, LR 0.000096 Loss 11.147071, Accuracy 55.524%\n",
      "Epoch 6, Batch 968, LR 0.000096 Loss 11.146434, Accuracy 55.529%\n",
      "Epoch 6, Batch 969, LR 0.000096 Loss 11.146626, Accuracy 55.524%\n",
      "Epoch 6, Batch 970, LR 0.000096 Loss 11.146524, Accuracy 55.524%\n",
      "Epoch 6, Batch 971, LR 0.000096 Loss 11.147101, Accuracy 55.519%\n",
      "Epoch 6, Batch 972, LR 0.000096 Loss 11.147241, Accuracy 55.515%\n",
      "Epoch 6, Batch 973, LR 0.000096 Loss 11.146323, Accuracy 55.520%\n",
      "Epoch 6, Batch 974, LR 0.000096 Loss 11.146327, Accuracy 55.520%\n",
      "Epoch 6, Batch 975, LR 0.000096 Loss 11.146466, Accuracy 55.517%\n",
      "Epoch 6, Batch 976, LR 0.000096 Loss 11.145482, Accuracy 55.523%\n",
      "Epoch 6, Batch 977, LR 0.000096 Loss 11.145109, Accuracy 55.522%\n",
      "Epoch 6, Batch 978, LR 0.000096 Loss 11.143956, Accuracy 55.530%\n",
      "Epoch 6, Batch 979, LR 0.000096 Loss 11.144201, Accuracy 55.529%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Batch 980, LR 0.000096 Loss 11.144125, Accuracy 55.530%\n",
      "Epoch 6, Batch 981, LR 0.000096 Loss 11.143686, Accuracy 55.534%\n",
      "Epoch 6, Batch 982, LR 0.000096 Loss 11.144262, Accuracy 55.528%\n",
      "Epoch 6, Batch 983, LR 0.000096 Loss 11.143940, Accuracy 55.529%\n",
      "Epoch 6, Batch 984, LR 0.000096 Loss 11.143580, Accuracy 55.532%\n",
      "Epoch 6, Batch 985, LR 0.000096 Loss 11.143768, Accuracy 55.533%\n",
      "Epoch 6, Batch 986, LR 0.000096 Loss 11.143481, Accuracy 55.531%\n",
      "Epoch 6, Batch 987, LR 0.000096 Loss 11.143293, Accuracy 55.535%\n",
      "Epoch 6, Batch 988, LR 0.000096 Loss 11.143028, Accuracy 55.540%\n",
      "Epoch 6, Batch 989, LR 0.000096 Loss 11.142995, Accuracy 55.541%\n",
      "Epoch 6, Batch 990, LR 0.000096 Loss 11.143236, Accuracy 55.541%\n",
      "Epoch 6, Batch 991, LR 0.000096 Loss 11.143110, Accuracy 55.544%\n",
      "Epoch 6, Batch 992, LR 0.000096 Loss 11.143262, Accuracy 55.541%\n",
      "Epoch 6, Batch 993, LR 0.000096 Loss 11.143393, Accuracy 55.545%\n",
      "Epoch 6, Batch 994, LR 0.000096 Loss 11.143293, Accuracy 55.545%\n",
      "Epoch 6, Batch 995, LR 0.000096 Loss 11.143230, Accuracy 55.546%\n",
      "Epoch 6, Batch 996, LR 0.000096 Loss 11.142907, Accuracy 55.550%\n",
      "Epoch 6, Batch 997, LR 0.000096 Loss 11.142302, Accuracy 55.556%\n",
      "Epoch 6, Batch 998, LR 0.000096 Loss 11.142207, Accuracy 55.554%\n",
      "Epoch 6, Batch 999, LR 0.000096 Loss 11.142377, Accuracy 55.547%\n",
      "Epoch 6, Batch 1000, LR 0.000096 Loss 11.142484, Accuracy 55.548%\n",
      "Epoch 6, Batch 1001, LR 0.000096 Loss 11.142798, Accuracy 55.544%\n",
      "Epoch 6, Batch 1002, LR 0.000096 Loss 11.143049, Accuracy 55.543%\n",
      "Epoch 6, Batch 1003, LR 0.000096 Loss 11.142756, Accuracy 55.546%\n",
      "Epoch 6, Batch 1004, LR 0.000096 Loss 11.142538, Accuracy 55.544%\n",
      "Epoch 6, Batch 1005, LR 0.000096 Loss 11.142141, Accuracy 55.546%\n",
      "Epoch 6, Batch 1006, LR 0.000096 Loss 11.141745, Accuracy 55.550%\n",
      "Epoch 6, Batch 1007, LR 0.000096 Loss 11.141031, Accuracy 55.556%\n",
      "Epoch 6, Batch 1008, LR 0.000096 Loss 11.141506, Accuracy 55.553%\n",
      "Epoch 6, Batch 1009, LR 0.000096 Loss 11.140787, Accuracy 55.555%\n",
      "Epoch 6, Batch 1010, LR 0.000096 Loss 11.139801, Accuracy 55.565%\n",
      "Epoch 6, Batch 1011, LR 0.000096 Loss 11.139506, Accuracy 55.567%\n",
      "Epoch 6, Batch 1012, LR 0.000096 Loss 11.139584, Accuracy 55.564%\n",
      "Epoch 6, Batch 1013, LR 0.000096 Loss 11.139132, Accuracy 55.562%\n",
      "Epoch 6, Batch 1014, LR 0.000096 Loss 11.138159, Accuracy 55.568%\n",
      "Epoch 6, Batch 1015, LR 0.000096 Loss 11.137256, Accuracy 55.573%\n",
      "Epoch 6, Batch 1016, LR 0.000096 Loss 11.137451, Accuracy 55.569%\n",
      "Epoch 6, Batch 1017, LR 0.000096 Loss 11.136832, Accuracy 55.573%\n",
      "Epoch 6, Batch 1018, LR 0.000096 Loss 11.136416, Accuracy 55.573%\n",
      "Epoch 6, Batch 1019, LR 0.000096 Loss 11.135783, Accuracy 55.578%\n",
      "Epoch 6, Batch 1020, LR 0.000096 Loss 11.135403, Accuracy 55.581%\n",
      "Epoch 6, Batch 1021, LR 0.000096 Loss 11.134536, Accuracy 55.587%\n",
      "Epoch 6, Batch 1022, LR 0.000096 Loss 11.132741, Accuracy 55.604%\n",
      "Epoch 6, Batch 1023, LR 0.000096 Loss 11.132242, Accuracy 55.608%\n",
      "Epoch 6, Batch 1024, LR 0.000096 Loss 11.131365, Accuracy 55.618%\n",
      "Epoch 6, Batch 1025, LR 0.000096 Loss 11.130847, Accuracy 55.619%\n",
      "Epoch 6, Batch 1026, LR 0.000096 Loss 11.130336, Accuracy 55.629%\n",
      "Epoch 6, Batch 1027, LR 0.000096 Loss 11.130547, Accuracy 55.628%\n",
      "Epoch 6, Batch 1028, LR 0.000096 Loss 11.130442, Accuracy 55.637%\n",
      "Epoch 6, Batch 1029, LR 0.000096 Loss 11.129978, Accuracy 55.635%\n",
      "Epoch 6, Batch 1030, LR 0.000096 Loss 11.129905, Accuracy 55.633%\n",
      "Epoch 6, Batch 1031, LR 0.000096 Loss 11.129279, Accuracy 55.634%\n",
      "Epoch 6, Batch 1032, LR 0.000096 Loss 11.129077, Accuracy 55.638%\n",
      "Epoch 6, Batch 1033, LR 0.000096 Loss 11.128364, Accuracy 55.646%\n",
      "Epoch 6, Batch 1034, LR 0.000096 Loss 11.127668, Accuracy 55.648%\n",
      "Epoch 6, Batch 1035, LR 0.000096 Loss 11.128098, Accuracy 55.646%\n",
      "Epoch 6, Batch 1036, LR 0.000096 Loss 11.128336, Accuracy 55.641%\n",
      "Epoch 6, Batch 1037, LR 0.000096 Loss 11.128304, Accuracy 55.644%\n",
      "Epoch 6, Batch 1038, LR 0.000096 Loss 11.128356, Accuracy 55.642%\n",
      "Epoch 6, Batch 1039, LR 0.000096 Loss 11.128431, Accuracy 55.642%\n",
      "Epoch 6, Batch 1040, LR 0.000096 Loss 11.127757, Accuracy 55.645%\n",
      "Epoch 6, Batch 1041, LR 0.000096 Loss 11.127784, Accuracy 55.644%\n",
      "Epoch 6, Batch 1042, LR 0.000096 Loss 11.127428, Accuracy 55.646%\n",
      "Epoch 6, Batch 1043, LR 0.000096 Loss 11.126881, Accuracy 55.652%\n",
      "Epoch 6, Batch 1044, LR 0.000096 Loss 11.126983, Accuracy 55.649%\n",
      "Epoch 6, Batch 1045, LR 0.000096 Loss 11.126699, Accuracy 55.653%\n",
      "Epoch 6, Batch 1046, LR 0.000096 Loss 11.126037, Accuracy 55.655%\n",
      "Epoch 6, Batch 1047, LR 0.000096 Loss 11.125705, Accuracy 55.654%\n",
      "Epoch 6, Loss (train set) 11.125705, Accuracy (train set) 55.654%\n",
      "Epoch 7, Batch 1, LR 0.000096 Loss 9.934267, Accuracy 58.594%\n",
      "Epoch 7, Batch 2, LR 0.000096 Loss 10.127356, Accuracy 60.938%\n",
      "Epoch 7, Batch 3, LR 0.000096 Loss 10.372012, Accuracy 59.896%\n",
      "Epoch 7, Batch 4, LR 0.000096 Loss 10.584600, Accuracy 58.008%\n",
      "Epoch 7, Batch 5, LR 0.000096 Loss 10.692539, Accuracy 57.500%\n",
      "Epoch 7, Batch 6, LR 0.000096 Loss 10.769406, Accuracy 57.943%\n",
      "Epoch 7, Batch 7, LR 0.000096 Loss 10.714585, Accuracy 59.152%\n",
      "Epoch 7, Batch 8, LR 0.000096 Loss 10.645330, Accuracy 59.375%\n",
      "Epoch 7, Batch 9, LR 0.000096 Loss 10.621523, Accuracy 59.896%\n",
      "Epoch 7, Batch 10, LR 0.000096 Loss 10.665299, Accuracy 59.766%\n",
      "Epoch 7, Batch 11, LR 0.000096 Loss 10.661303, Accuracy 59.162%\n",
      "Epoch 7, Batch 12, LR 0.000096 Loss 10.758049, Accuracy 58.073%\n",
      "Epoch 7, Batch 13, LR 0.000096 Loss 10.749573, Accuracy 57.873%\n",
      "Epoch 7, Batch 14, LR 0.000096 Loss 10.794857, Accuracy 57.087%\n",
      "Epoch 7, Batch 15, LR 0.000096 Loss 10.842208, Accuracy 56.667%\n",
      "Epoch 7, Batch 16, LR 0.000096 Loss 10.877243, Accuracy 56.445%\n",
      "Epoch 7, Batch 17, LR 0.000096 Loss 10.879278, Accuracy 56.434%\n",
      "Epoch 7, Batch 18, LR 0.000096 Loss 10.857006, Accuracy 56.727%\n",
      "Epoch 7, Batch 19, LR 0.000096 Loss 10.809710, Accuracy 57.113%\n",
      "Epoch 7, Batch 20, LR 0.000096 Loss 10.816661, Accuracy 56.758%\n",
      "Epoch 7, Batch 21, LR 0.000096 Loss 10.853207, Accuracy 56.436%\n",
      "Epoch 7, Batch 22, LR 0.000096 Loss 10.835216, Accuracy 56.783%\n",
      "Epoch 7, Batch 23, LR 0.000096 Loss 10.839416, Accuracy 56.827%\n",
      "Epoch 7, Batch 24, LR 0.000096 Loss 10.841868, Accuracy 56.934%\n",
      "Epoch 7, Batch 25, LR 0.000096 Loss 10.812964, Accuracy 57.281%\n",
      "Epoch 7, Batch 26, LR 0.000096 Loss 10.830611, Accuracy 57.272%\n",
      "Epoch 7, Batch 27, LR 0.000096 Loss 10.809514, Accuracy 57.321%\n",
      "Epoch 7, Batch 28, LR 0.000096 Loss 10.824325, Accuracy 57.227%\n",
      "Epoch 7, Batch 29, LR 0.000096 Loss 10.834332, Accuracy 57.112%\n",
      "Epoch 7, Batch 30, LR 0.000096 Loss 10.818500, Accuracy 57.266%\n",
      "Epoch 7, Batch 31, LR 0.000096 Loss 10.808063, Accuracy 57.283%\n",
      "Epoch 7, Batch 32, LR 0.000096 Loss 10.810455, Accuracy 57.178%\n",
      "Epoch 7, Batch 33, LR 0.000096 Loss 10.766819, Accuracy 57.481%\n",
      "Epoch 7, Batch 34, LR 0.000096 Loss 10.762579, Accuracy 57.652%\n",
      "Epoch 7, Batch 35, LR 0.000096 Loss 10.757997, Accuracy 57.812%\n",
      "Epoch 7, Batch 36, LR 0.000096 Loss 10.759409, Accuracy 57.856%\n",
      "Epoch 7, Batch 37, LR 0.000096 Loss 10.752409, Accuracy 58.024%\n",
      "Epoch 7, Batch 38, LR 0.000096 Loss 10.740240, Accuracy 58.018%\n",
      "Epoch 7, Batch 39, LR 0.000096 Loss 10.729529, Accuracy 57.993%\n",
      "Epoch 7, Batch 40, LR 0.000096 Loss 10.737612, Accuracy 57.988%\n",
      "Epoch 7, Batch 41, LR 0.000096 Loss 10.750856, Accuracy 57.736%\n",
      "Epoch 7, Batch 42, LR 0.000096 Loss 10.747905, Accuracy 57.775%\n",
      "Epoch 7, Batch 43, LR 0.000096 Loss 10.745891, Accuracy 57.685%\n",
      "Epoch 7, Batch 44, LR 0.000096 Loss 10.757184, Accuracy 57.653%\n",
      "Epoch 7, Batch 45, LR 0.000096 Loss 10.745365, Accuracy 57.760%\n",
      "Epoch 7, Batch 46, LR 0.000096 Loss 10.752909, Accuracy 57.745%\n",
      "Epoch 7, Batch 47, LR 0.000096 Loss 10.742246, Accuracy 57.879%\n",
      "Epoch 7, Batch 48, LR 0.000096 Loss 10.740633, Accuracy 57.747%\n",
      "Epoch 7, Batch 49, LR 0.000096 Loss 10.740508, Accuracy 57.781%\n",
      "Epoch 7, Batch 50, LR 0.000096 Loss 10.736109, Accuracy 57.734%\n",
      "Epoch 7, Batch 51, LR 0.000096 Loss 10.735299, Accuracy 57.690%\n",
      "Epoch 7, Batch 52, LR 0.000096 Loss 10.735444, Accuracy 57.767%\n",
      "Epoch 7, Batch 53, LR 0.000096 Loss 10.735773, Accuracy 57.916%\n",
      "Epoch 7, Batch 54, LR 0.000096 Loss 10.719607, Accuracy 57.972%\n",
      "Epoch 7, Batch 55, LR 0.000096 Loss 10.712169, Accuracy 58.026%\n",
      "Epoch 7, Batch 56, LR 0.000096 Loss 10.695564, Accuracy 58.022%\n",
      "Epoch 7, Batch 57, LR 0.000096 Loss 10.698846, Accuracy 57.977%\n",
      "Epoch 7, Batch 58, LR 0.000096 Loss 10.688956, Accuracy 58.068%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 59, LR 0.000096 Loss 10.682293, Accuracy 58.144%\n",
      "Epoch 7, Batch 60, LR 0.000096 Loss 10.672267, Accuracy 58.125%\n",
      "Epoch 7, Batch 61, LR 0.000096 Loss 10.662692, Accuracy 58.094%\n",
      "Epoch 7, Batch 62, LR 0.000096 Loss 10.673978, Accuracy 58.027%\n",
      "Epoch 7, Batch 63, LR 0.000096 Loss 10.683998, Accuracy 57.999%\n",
      "Epoch 7, Batch 64, LR 0.000096 Loss 10.684435, Accuracy 58.032%\n",
      "Epoch 7, Batch 65, LR 0.000096 Loss 10.668912, Accuracy 58.101%\n",
      "Epoch 7, Batch 66, LR 0.000096 Loss 10.665324, Accuracy 58.144%\n",
      "Epoch 7, Batch 67, LR 0.000096 Loss 10.658257, Accuracy 58.232%\n",
      "Epoch 7, Batch 68, LR 0.000096 Loss 10.647356, Accuracy 58.364%\n",
      "Epoch 7, Batch 69, LR 0.000096 Loss 10.642892, Accuracy 58.469%\n",
      "Epoch 7, Batch 70, LR 0.000096 Loss 10.644134, Accuracy 58.504%\n",
      "Epoch 7, Batch 71, LR 0.000096 Loss 10.634962, Accuracy 58.594%\n",
      "Epoch 7, Batch 72, LR 0.000096 Loss 10.631369, Accuracy 58.626%\n",
      "Epoch 7, Batch 73, LR 0.000096 Loss 10.636308, Accuracy 58.626%\n",
      "Epoch 7, Batch 74, LR 0.000096 Loss 10.641639, Accuracy 58.625%\n",
      "Epoch 7, Batch 75, LR 0.000096 Loss 10.639035, Accuracy 58.635%\n",
      "Epoch 7, Batch 76, LR 0.000096 Loss 10.639097, Accuracy 58.655%\n",
      "Epoch 7, Batch 77, LR 0.000096 Loss 10.633736, Accuracy 58.655%\n",
      "Epoch 7, Batch 78, LR 0.000096 Loss 10.634979, Accuracy 58.624%\n",
      "Epoch 7, Batch 79, LR 0.000096 Loss 10.636525, Accuracy 58.683%\n",
      "Epoch 7, Batch 80, LR 0.000096 Loss 10.635224, Accuracy 58.701%\n",
      "Epoch 7, Batch 81, LR 0.000096 Loss 10.637728, Accuracy 58.652%\n",
      "Epoch 7, Batch 82, LR 0.000096 Loss 10.640040, Accuracy 58.584%\n",
      "Epoch 7, Batch 83, LR 0.000096 Loss 10.648426, Accuracy 58.471%\n",
      "Epoch 7, Batch 84, LR 0.000096 Loss 10.645379, Accuracy 58.566%\n",
      "Epoch 7, Batch 85, LR 0.000096 Loss 10.651440, Accuracy 58.483%\n",
      "Epoch 7, Batch 86, LR 0.000096 Loss 10.650721, Accuracy 58.494%\n",
      "Epoch 7, Batch 87, LR 0.000096 Loss 10.644661, Accuracy 58.513%\n",
      "Epoch 7, Batch 88, LR 0.000096 Loss 10.634533, Accuracy 58.620%\n",
      "Epoch 7, Batch 89, LR 0.000096 Loss 10.637919, Accuracy 58.559%\n",
      "Epoch 7, Batch 90, LR 0.000096 Loss 10.640466, Accuracy 58.559%\n",
      "Epoch 7, Batch 91, LR 0.000096 Loss 10.637150, Accuracy 58.568%\n",
      "Epoch 7, Batch 92, LR 0.000096 Loss 10.633581, Accuracy 58.585%\n",
      "Epoch 7, Batch 93, LR 0.000096 Loss 10.635078, Accuracy 58.552%\n",
      "Epoch 7, Batch 94, LR 0.000096 Loss 10.641174, Accuracy 58.486%\n",
      "Epoch 7, Batch 95, LR 0.000096 Loss 10.647781, Accuracy 58.487%\n",
      "Epoch 7, Batch 96, LR 0.000096 Loss 10.647090, Accuracy 58.488%\n",
      "Epoch 7, Batch 97, LR 0.000096 Loss 10.653363, Accuracy 58.489%\n",
      "Epoch 7, Batch 98, LR 0.000096 Loss 10.660137, Accuracy 58.426%\n",
      "Epoch 7, Batch 99, LR 0.000096 Loss 10.647114, Accuracy 58.515%\n",
      "Epoch 7, Batch 100, LR 0.000096 Loss 10.646519, Accuracy 58.547%\n",
      "Epoch 7, Batch 101, LR 0.000096 Loss 10.646609, Accuracy 58.578%\n",
      "Epoch 7, Batch 102, LR 0.000096 Loss 10.648784, Accuracy 58.555%\n",
      "Epoch 7, Batch 103, LR 0.000096 Loss 10.652882, Accuracy 58.533%\n",
      "Epoch 7, Batch 104, LR 0.000096 Loss 10.655807, Accuracy 58.466%\n",
      "Epoch 7, Batch 105, LR 0.000096 Loss 10.650075, Accuracy 58.490%\n",
      "Epoch 7, Batch 106, LR 0.000096 Loss 10.649710, Accuracy 58.476%\n",
      "Epoch 7, Batch 107, LR 0.000096 Loss 10.638601, Accuracy 58.572%\n",
      "Epoch 7, Batch 108, LR 0.000096 Loss 10.644108, Accuracy 58.500%\n",
      "Epoch 7, Batch 109, LR 0.000096 Loss 10.650831, Accuracy 58.472%\n",
      "Epoch 7, Batch 110, LR 0.000096 Loss 10.653543, Accuracy 58.466%\n",
      "Epoch 7, Batch 111, LR 0.000096 Loss 10.654329, Accuracy 58.453%\n",
      "Epoch 7, Batch 112, LR 0.000096 Loss 10.655211, Accuracy 58.440%\n",
      "Epoch 7, Batch 113, LR 0.000096 Loss 10.661071, Accuracy 58.414%\n",
      "Epoch 7, Batch 114, LR 0.000096 Loss 10.659621, Accuracy 58.381%\n",
      "Epoch 7, Batch 115, LR 0.000096 Loss 10.656360, Accuracy 58.404%\n",
      "Epoch 7, Batch 116, LR 0.000096 Loss 10.661435, Accuracy 58.385%\n",
      "Epoch 7, Batch 117, LR 0.000096 Loss 10.665223, Accuracy 58.367%\n",
      "Epoch 7, Batch 118, LR 0.000096 Loss 10.667506, Accuracy 58.355%\n",
      "Epoch 7, Batch 119, LR 0.000096 Loss 10.661954, Accuracy 58.390%\n",
      "Epoch 7, Batch 120, LR 0.000096 Loss 10.656552, Accuracy 58.464%\n",
      "Epoch 7, Batch 121, LR 0.000096 Loss 10.656691, Accuracy 58.465%\n",
      "Epoch 7, Batch 122, LR 0.000096 Loss 10.655260, Accuracy 58.459%\n",
      "Epoch 7, Batch 123, LR 0.000096 Loss 10.646320, Accuracy 58.537%\n",
      "Epoch 7, Batch 124, LR 0.000096 Loss 10.648766, Accuracy 58.506%\n",
      "Epoch 7, Batch 125, LR 0.000096 Loss 10.650643, Accuracy 58.556%\n",
      "Epoch 7, Batch 126, LR 0.000096 Loss 10.651368, Accuracy 58.550%\n",
      "Epoch 7, Batch 127, LR 0.000096 Loss 10.648511, Accuracy 58.588%\n",
      "Epoch 7, Batch 128, LR 0.000096 Loss 10.646595, Accuracy 58.557%\n",
      "Epoch 7, Batch 129, LR 0.000096 Loss 10.642101, Accuracy 58.594%\n",
      "Epoch 7, Batch 130, LR 0.000096 Loss 10.643646, Accuracy 58.600%\n",
      "Epoch 7, Batch 131, LR 0.000096 Loss 10.648980, Accuracy 58.570%\n",
      "Epoch 7, Batch 132, LR 0.000096 Loss 10.650031, Accuracy 58.564%\n",
      "Epoch 7, Batch 133, LR 0.000096 Loss 10.651893, Accuracy 58.553%\n",
      "Epoch 7, Batch 134, LR 0.000096 Loss 10.652160, Accuracy 58.611%\n",
      "Epoch 7, Batch 135, LR 0.000096 Loss 10.654875, Accuracy 58.600%\n",
      "Epoch 7, Batch 136, LR 0.000096 Loss 10.656674, Accuracy 58.554%\n",
      "Epoch 7, Batch 137, LR 0.000096 Loss 10.656528, Accuracy 58.571%\n",
      "Epoch 7, Batch 138, LR 0.000096 Loss 10.660914, Accuracy 58.531%\n",
      "Epoch 7, Batch 139, LR 0.000096 Loss 10.661608, Accuracy 58.549%\n",
      "Epoch 7, Batch 140, LR 0.000096 Loss 10.661190, Accuracy 58.549%\n",
      "Epoch 7, Batch 141, LR 0.000096 Loss 10.662781, Accuracy 58.500%\n",
      "Epoch 7, Batch 142, LR 0.000096 Loss 10.658810, Accuracy 58.517%\n",
      "Epoch 7, Batch 143, LR 0.000096 Loss 10.664687, Accuracy 58.463%\n",
      "Epoch 7, Batch 144, LR 0.000096 Loss 10.666951, Accuracy 58.431%\n",
      "Epoch 7, Batch 145, LR 0.000096 Loss 10.669218, Accuracy 58.405%\n",
      "Epoch 7, Batch 146, LR 0.000096 Loss 10.672448, Accuracy 58.369%\n",
      "Epoch 7, Batch 147, LR 0.000096 Loss 10.671905, Accuracy 58.392%\n",
      "Epoch 7, Batch 148, LR 0.000096 Loss 10.674025, Accuracy 58.367%\n",
      "Epoch 7, Batch 149, LR 0.000096 Loss 10.673443, Accuracy 58.379%\n",
      "Epoch 7, Batch 150, LR 0.000096 Loss 10.672397, Accuracy 58.391%\n",
      "Epoch 7, Batch 151, LR 0.000096 Loss 10.675166, Accuracy 58.387%\n",
      "Epoch 7, Batch 152, LR 0.000096 Loss 10.674581, Accuracy 58.362%\n",
      "Epoch 7, Batch 153, LR 0.000096 Loss 10.674736, Accuracy 58.374%\n",
      "Epoch 7, Batch 154, LR 0.000096 Loss 10.674769, Accuracy 58.391%\n",
      "Epoch 7, Batch 155, LR 0.000096 Loss 10.668943, Accuracy 58.453%\n",
      "Epoch 7, Batch 156, LR 0.000096 Loss 10.668408, Accuracy 58.428%\n",
      "Epoch 7, Batch 157, LR 0.000096 Loss 10.664987, Accuracy 58.444%\n",
      "Epoch 7, Batch 158, LR 0.000096 Loss 10.667588, Accuracy 58.431%\n",
      "Epoch 7, Batch 159, LR 0.000096 Loss 10.667565, Accuracy 58.412%\n",
      "Epoch 7, Batch 160, LR 0.000096 Loss 10.672306, Accuracy 58.359%\n",
      "Epoch 7, Batch 161, LR 0.000096 Loss 10.674758, Accuracy 58.341%\n",
      "Epoch 7, Batch 162, LR 0.000096 Loss 10.673383, Accuracy 58.353%\n",
      "Epoch 7, Batch 163, LR 0.000096 Loss 10.680084, Accuracy 58.325%\n",
      "Epoch 7, Batch 164, LR 0.000096 Loss 10.676030, Accuracy 58.332%\n",
      "Epoch 7, Batch 165, LR 0.000096 Loss 10.670975, Accuracy 58.390%\n",
      "Epoch 7, Batch 166, LR 0.000096 Loss 10.672124, Accuracy 58.387%\n",
      "Epoch 7, Batch 167, LR 0.000096 Loss 10.668997, Accuracy 58.393%\n",
      "Epoch 7, Batch 168, LR 0.000096 Loss 10.670742, Accuracy 58.394%\n",
      "Epoch 7, Batch 169, LR 0.000096 Loss 10.671087, Accuracy 58.404%\n",
      "Epoch 7, Batch 170, LR 0.000096 Loss 10.671865, Accuracy 58.415%\n",
      "Epoch 7, Batch 171, LR 0.000096 Loss 10.669290, Accuracy 58.466%\n",
      "Epoch 7, Batch 172, LR 0.000096 Loss 10.669926, Accuracy 58.444%\n",
      "Epoch 7, Batch 173, LR 0.000096 Loss 10.670271, Accuracy 58.440%\n",
      "Epoch 7, Batch 174, LR 0.000096 Loss 10.666774, Accuracy 58.450%\n",
      "Epoch 7, Batch 175, LR 0.000096 Loss 10.668296, Accuracy 58.429%\n",
      "Epoch 7, Batch 176, LR 0.000096 Loss 10.666346, Accuracy 58.456%\n",
      "Epoch 7, Batch 177, LR 0.000096 Loss 10.659772, Accuracy 58.505%\n",
      "Epoch 7, Batch 178, LR 0.000096 Loss 10.660601, Accuracy 58.475%\n",
      "Epoch 7, Batch 179, LR 0.000096 Loss 10.657572, Accuracy 58.489%\n",
      "Epoch 7, Batch 180, LR 0.000095 Loss 10.658757, Accuracy 58.481%\n",
      "Epoch 7, Batch 181, LR 0.000095 Loss 10.657228, Accuracy 58.512%\n",
      "Epoch 7, Batch 182, LR 0.000095 Loss 10.652850, Accuracy 58.542%\n",
      "Epoch 7, Batch 183, LR 0.000095 Loss 10.651498, Accuracy 58.568%\n",
      "Epoch 7, Batch 184, LR 0.000095 Loss 10.655412, Accuracy 58.530%\n",
      "Epoch 7, Batch 185, LR 0.000095 Loss 10.657720, Accuracy 58.505%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 186, LR 0.000095 Loss 10.656501, Accuracy 58.501%\n",
      "Epoch 7, Batch 187, LR 0.000095 Loss 10.656234, Accuracy 58.519%\n",
      "Epoch 7, Batch 188, LR 0.000095 Loss 10.655085, Accuracy 58.536%\n",
      "Epoch 7, Batch 189, LR 0.000095 Loss 10.653785, Accuracy 58.552%\n",
      "Epoch 7, Batch 190, LR 0.000095 Loss 10.651845, Accuracy 58.577%\n",
      "Epoch 7, Batch 191, LR 0.000095 Loss 10.651375, Accuracy 58.577%\n",
      "Epoch 7, Batch 192, LR 0.000095 Loss 10.647828, Accuracy 58.614%\n",
      "Epoch 7, Batch 193, LR 0.000095 Loss 10.648014, Accuracy 58.598%\n",
      "Epoch 7, Batch 194, LR 0.000095 Loss 10.650479, Accuracy 58.537%\n",
      "Epoch 7, Batch 195, LR 0.000095 Loss 10.653789, Accuracy 58.518%\n",
      "Epoch 7, Batch 196, LR 0.000095 Loss 10.653606, Accuracy 58.502%\n",
      "Epoch 7, Batch 197, LR 0.000095 Loss 10.655826, Accuracy 58.467%\n",
      "Epoch 7, Batch 198, LR 0.000095 Loss 10.657455, Accuracy 58.467%\n",
      "Epoch 7, Batch 199, LR 0.000095 Loss 10.663611, Accuracy 58.417%\n",
      "Epoch 7, Batch 200, LR 0.000095 Loss 10.662724, Accuracy 58.422%\n",
      "Epoch 7, Batch 201, LR 0.000095 Loss 10.659923, Accuracy 58.458%\n",
      "Epoch 7, Batch 202, LR 0.000095 Loss 10.658572, Accuracy 58.485%\n",
      "Epoch 7, Batch 203, LR 0.000095 Loss 10.653332, Accuracy 58.505%\n",
      "Epoch 7, Batch 204, LR 0.000095 Loss 10.651353, Accuracy 58.517%\n",
      "Epoch 7, Batch 205, LR 0.000095 Loss 10.650785, Accuracy 58.502%\n",
      "Epoch 7, Batch 206, LR 0.000095 Loss 10.652471, Accuracy 58.480%\n",
      "Epoch 7, Batch 207, LR 0.000095 Loss 10.648874, Accuracy 58.518%\n",
      "Epoch 7, Batch 208, LR 0.000095 Loss 10.651992, Accuracy 58.496%\n",
      "Epoch 7, Batch 209, LR 0.000095 Loss 10.654975, Accuracy 58.478%\n",
      "Epoch 7, Batch 210, LR 0.000095 Loss 10.651788, Accuracy 58.534%\n",
      "Epoch 7, Batch 211, LR 0.000095 Loss 10.652549, Accuracy 58.520%\n",
      "Epoch 7, Batch 212, LR 0.000095 Loss 10.650556, Accuracy 58.542%\n",
      "Epoch 7, Batch 213, LR 0.000095 Loss 10.647256, Accuracy 58.572%\n",
      "Epoch 7, Batch 214, LR 0.000095 Loss 10.645619, Accuracy 58.583%\n",
      "Epoch 7, Batch 215, LR 0.000095 Loss 10.646414, Accuracy 58.557%\n",
      "Epoch 7, Batch 216, LR 0.000095 Loss 10.644672, Accuracy 58.558%\n",
      "Epoch 7, Batch 217, LR 0.000095 Loss 10.645885, Accuracy 58.554%\n",
      "Epoch 7, Batch 218, LR 0.000095 Loss 10.647007, Accuracy 58.511%\n",
      "Epoch 7, Batch 219, LR 0.000095 Loss 10.648261, Accuracy 58.483%\n",
      "Epoch 7, Batch 220, LR 0.000095 Loss 10.647661, Accuracy 58.491%\n",
      "Epoch 7, Batch 221, LR 0.000095 Loss 10.644257, Accuracy 58.509%\n",
      "Epoch 7, Batch 222, LR 0.000095 Loss 10.641325, Accuracy 58.541%\n",
      "Epoch 7, Batch 223, LR 0.000095 Loss 10.641402, Accuracy 58.524%\n",
      "Epoch 7, Batch 224, LR 0.000095 Loss 10.643151, Accuracy 58.493%\n",
      "Epoch 7, Batch 225, LR 0.000095 Loss 10.646173, Accuracy 58.472%\n",
      "Epoch 7, Batch 226, LR 0.000095 Loss 10.644341, Accuracy 58.494%\n",
      "Epoch 7, Batch 227, LR 0.000095 Loss 10.643340, Accuracy 58.494%\n",
      "Epoch 7, Batch 228, LR 0.000095 Loss 10.641525, Accuracy 58.498%\n",
      "Epoch 7, Batch 229, LR 0.000095 Loss 10.642369, Accuracy 58.491%\n",
      "Epoch 7, Batch 230, LR 0.000095 Loss 10.638140, Accuracy 58.512%\n",
      "Epoch 7, Batch 231, LR 0.000095 Loss 10.641237, Accuracy 58.496%\n",
      "Epoch 7, Batch 232, LR 0.000095 Loss 10.640437, Accuracy 58.520%\n",
      "Epoch 7, Batch 233, LR 0.000095 Loss 10.638683, Accuracy 58.533%\n",
      "Epoch 7, Batch 234, LR 0.000095 Loss 10.637729, Accuracy 58.537%\n",
      "Epoch 7, Batch 235, LR 0.000095 Loss 10.636772, Accuracy 58.541%\n",
      "Epoch 7, Batch 236, LR 0.000095 Loss 10.635474, Accuracy 58.537%\n",
      "Epoch 7, Batch 237, LR 0.000095 Loss 10.635930, Accuracy 58.531%\n",
      "Epoch 7, Batch 238, LR 0.000095 Loss 10.637659, Accuracy 58.535%\n",
      "Epoch 7, Batch 239, LR 0.000095 Loss 10.637540, Accuracy 58.555%\n",
      "Epoch 7, Batch 240, LR 0.000095 Loss 10.635368, Accuracy 58.564%\n",
      "Epoch 7, Batch 241, LR 0.000095 Loss 10.639401, Accuracy 58.542%\n",
      "Epoch 7, Batch 242, LR 0.000095 Loss 10.643161, Accuracy 58.516%\n",
      "Epoch 7, Batch 243, LR 0.000095 Loss 10.644945, Accuracy 58.497%\n",
      "Epoch 7, Batch 244, LR 0.000095 Loss 10.640452, Accuracy 58.507%\n",
      "Epoch 7, Batch 245, LR 0.000095 Loss 10.639304, Accuracy 58.527%\n",
      "Epoch 7, Batch 246, LR 0.000095 Loss 10.641022, Accuracy 58.524%\n",
      "Epoch 7, Batch 247, LR 0.000095 Loss 10.646107, Accuracy 58.499%\n",
      "Epoch 7, Batch 248, LR 0.000095 Loss 10.648997, Accuracy 58.465%\n",
      "Epoch 7, Batch 249, LR 0.000095 Loss 10.649286, Accuracy 58.462%\n",
      "Epoch 7, Batch 250, LR 0.000095 Loss 10.646236, Accuracy 58.487%\n",
      "Epoch 7, Batch 251, LR 0.000095 Loss 10.646455, Accuracy 58.472%\n",
      "Epoch 7, Batch 252, LR 0.000095 Loss 10.646406, Accuracy 58.460%\n",
      "Epoch 7, Batch 253, LR 0.000095 Loss 10.647078, Accuracy 58.446%\n",
      "Epoch 7, Batch 254, LR 0.000095 Loss 10.648241, Accuracy 58.449%\n",
      "Epoch 7, Batch 255, LR 0.000095 Loss 10.649406, Accuracy 58.434%\n",
      "Epoch 7, Batch 256, LR 0.000095 Loss 10.651589, Accuracy 58.408%\n",
      "Epoch 7, Batch 257, LR 0.000095 Loss 10.652444, Accuracy 58.408%\n",
      "Epoch 7, Batch 258, LR 0.000095 Loss 10.653043, Accuracy 58.391%\n",
      "Epoch 7, Batch 259, LR 0.000095 Loss 10.652378, Accuracy 58.392%\n",
      "Epoch 7, Batch 260, LR 0.000095 Loss 10.650670, Accuracy 58.389%\n",
      "Epoch 7, Batch 261, LR 0.000095 Loss 10.650294, Accuracy 58.390%\n",
      "Epoch 7, Batch 262, LR 0.000095 Loss 10.652843, Accuracy 58.379%\n",
      "Epoch 7, Batch 263, LR 0.000095 Loss 10.651295, Accuracy 58.377%\n",
      "Epoch 7, Batch 264, LR 0.000095 Loss 10.653273, Accuracy 58.360%\n",
      "Epoch 7, Batch 265, LR 0.000095 Loss 10.651820, Accuracy 58.379%\n",
      "Epoch 7, Batch 266, LR 0.000095 Loss 10.652418, Accuracy 58.373%\n",
      "Epoch 7, Batch 267, LR 0.000095 Loss 10.649220, Accuracy 58.401%\n",
      "Epoch 7, Batch 268, LR 0.000095 Loss 10.647019, Accuracy 58.410%\n",
      "Epoch 7, Batch 269, LR 0.000095 Loss 10.647874, Accuracy 58.414%\n",
      "Epoch 7, Batch 270, LR 0.000095 Loss 10.648602, Accuracy 58.414%\n",
      "Epoch 7, Batch 271, LR 0.000095 Loss 10.647564, Accuracy 58.427%\n",
      "Epoch 7, Batch 272, LR 0.000095 Loss 10.644727, Accuracy 58.444%\n",
      "Epoch 7, Batch 273, LR 0.000095 Loss 10.643188, Accuracy 58.454%\n",
      "Epoch 7, Batch 274, LR 0.000095 Loss 10.640462, Accuracy 58.465%\n",
      "Epoch 7, Batch 275, LR 0.000095 Loss 10.640463, Accuracy 58.466%\n",
      "Epoch 7, Batch 276, LR 0.000095 Loss 10.638366, Accuracy 58.489%\n",
      "Epoch 7, Batch 277, LR 0.000095 Loss 10.637730, Accuracy 58.501%\n",
      "Epoch 7, Batch 278, LR 0.000095 Loss 10.639968, Accuracy 58.476%\n",
      "Epoch 7, Batch 279, LR 0.000095 Loss 10.640395, Accuracy 58.473%\n",
      "Epoch 7, Batch 280, LR 0.000095 Loss 10.642540, Accuracy 58.451%\n",
      "Epoch 7, Batch 281, LR 0.000095 Loss 10.642244, Accuracy 58.444%\n",
      "Epoch 7, Batch 282, LR 0.000095 Loss 10.642669, Accuracy 58.433%\n",
      "Epoch 7, Batch 283, LR 0.000095 Loss 10.642767, Accuracy 58.445%\n",
      "Epoch 7, Batch 284, LR 0.000095 Loss 10.643660, Accuracy 58.418%\n",
      "Epoch 7, Batch 285, LR 0.000095 Loss 10.642535, Accuracy 58.427%\n",
      "Epoch 7, Batch 286, LR 0.000095 Loss 10.643692, Accuracy 58.416%\n",
      "Epoch 7, Batch 287, LR 0.000095 Loss 10.642546, Accuracy 58.436%\n",
      "Epoch 7, Batch 288, LR 0.000095 Loss 10.641174, Accuracy 58.439%\n",
      "Epoch 7, Batch 289, LR 0.000095 Loss 10.638734, Accuracy 58.461%\n",
      "Epoch 7, Batch 290, LR 0.000095 Loss 10.638610, Accuracy 58.459%\n",
      "Epoch 7, Batch 291, LR 0.000095 Loss 10.637098, Accuracy 58.484%\n",
      "Epoch 7, Batch 292, LR 0.000095 Loss 10.637608, Accuracy 58.465%\n",
      "Epoch 7, Batch 293, LR 0.000095 Loss 10.636310, Accuracy 58.479%\n",
      "Epoch 7, Batch 294, LR 0.000095 Loss 10.635510, Accuracy 58.485%\n",
      "Epoch 7, Batch 295, LR 0.000095 Loss 10.632859, Accuracy 58.512%\n",
      "Epoch 7, Batch 296, LR 0.000095 Loss 10.629235, Accuracy 58.528%\n",
      "Epoch 7, Batch 297, LR 0.000095 Loss 10.631212, Accuracy 58.512%\n",
      "Epoch 7, Batch 298, LR 0.000095 Loss 10.631514, Accuracy 58.507%\n",
      "Epoch 7, Batch 299, LR 0.000095 Loss 10.632395, Accuracy 58.494%\n",
      "Epoch 7, Batch 300, LR 0.000095 Loss 10.631571, Accuracy 58.500%\n",
      "Epoch 7, Batch 301, LR 0.000095 Loss 10.630487, Accuracy 58.503%\n",
      "Epoch 7, Batch 302, LR 0.000095 Loss 10.630495, Accuracy 58.485%\n",
      "Epoch 7, Batch 303, LR 0.000095 Loss 10.630535, Accuracy 58.501%\n",
      "Epoch 7, Batch 304, LR 0.000095 Loss 10.629673, Accuracy 58.506%\n",
      "Epoch 7, Batch 305, LR 0.000095 Loss 10.629075, Accuracy 58.509%\n",
      "Epoch 7, Batch 306, LR 0.000095 Loss 10.629530, Accuracy 58.502%\n",
      "Epoch 7, Batch 307, LR 0.000095 Loss 10.628508, Accuracy 58.522%\n",
      "Epoch 7, Batch 308, LR 0.000095 Loss 10.631122, Accuracy 58.510%\n",
      "Epoch 7, Batch 309, LR 0.000095 Loss 10.630638, Accuracy 58.493%\n",
      "Epoch 7, Batch 310, LR 0.000095 Loss 10.628218, Accuracy 58.498%\n",
      "Epoch 7, Batch 311, LR 0.000095 Loss 10.628457, Accuracy 58.486%\n",
      "Epoch 7, Batch 312, LR 0.000095 Loss 10.630799, Accuracy 58.466%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 313, LR 0.000095 Loss 10.628138, Accuracy 58.501%\n",
      "Epoch 7, Batch 314, LR 0.000095 Loss 10.629228, Accuracy 58.507%\n",
      "Epoch 7, Batch 315, LR 0.000095 Loss 10.632760, Accuracy 58.475%\n",
      "Epoch 7, Batch 316, LR 0.000095 Loss 10.633027, Accuracy 58.463%\n",
      "Epoch 7, Batch 317, LR 0.000095 Loss 10.634213, Accuracy 58.461%\n",
      "Epoch 7, Batch 318, LR 0.000095 Loss 10.633333, Accuracy 58.481%\n",
      "Epoch 7, Batch 319, LR 0.000095 Loss 10.632611, Accuracy 58.506%\n",
      "Epoch 7, Batch 320, LR 0.000095 Loss 10.632044, Accuracy 58.518%\n",
      "Epoch 7, Batch 321, LR 0.000095 Loss 10.632637, Accuracy 58.533%\n",
      "Epoch 7, Batch 322, LR 0.000095 Loss 10.631513, Accuracy 58.545%\n",
      "Epoch 7, Batch 323, LR 0.000095 Loss 10.631651, Accuracy 58.545%\n",
      "Epoch 7, Batch 324, LR 0.000095 Loss 10.633960, Accuracy 58.531%\n",
      "Epoch 7, Batch 325, LR 0.000095 Loss 10.631667, Accuracy 58.553%\n",
      "Epoch 7, Batch 326, LR 0.000095 Loss 10.632088, Accuracy 58.543%\n",
      "Epoch 7, Batch 327, LR 0.000095 Loss 10.631905, Accuracy 58.544%\n",
      "Epoch 7, Batch 328, LR 0.000095 Loss 10.631938, Accuracy 58.563%\n",
      "Epoch 7, Batch 329, LR 0.000095 Loss 10.633572, Accuracy 58.561%\n",
      "Epoch 7, Batch 330, LR 0.000095 Loss 10.632495, Accuracy 58.572%\n",
      "Epoch 7, Batch 331, LR 0.000095 Loss 10.632091, Accuracy 58.580%\n",
      "Epoch 7, Batch 332, LR 0.000095 Loss 10.632151, Accuracy 58.584%\n",
      "Epoch 7, Batch 333, LR 0.000095 Loss 10.631822, Accuracy 58.601%\n",
      "Epoch 7, Batch 334, LR 0.000095 Loss 10.629893, Accuracy 58.619%\n",
      "Epoch 7, Batch 335, LR 0.000095 Loss 10.629080, Accuracy 58.638%\n",
      "Epoch 7, Batch 336, LR 0.000095 Loss 10.628985, Accuracy 58.629%\n",
      "Epoch 7, Batch 337, LR 0.000095 Loss 10.627870, Accuracy 58.633%\n",
      "Epoch 7, Batch 338, LR 0.000095 Loss 10.627142, Accuracy 58.638%\n",
      "Epoch 7, Batch 339, LR 0.000095 Loss 10.628408, Accuracy 58.624%\n",
      "Epoch 7, Batch 340, LR 0.000095 Loss 10.626334, Accuracy 58.644%\n",
      "Epoch 7, Batch 341, LR 0.000095 Loss 10.624152, Accuracy 58.658%\n",
      "Epoch 7, Batch 342, LR 0.000095 Loss 10.624590, Accuracy 58.655%\n",
      "Epoch 7, Batch 343, LR 0.000095 Loss 10.622237, Accuracy 58.678%\n",
      "Epoch 7, Batch 344, LR 0.000095 Loss 10.623109, Accuracy 58.669%\n",
      "Epoch 7, Batch 345, LR 0.000095 Loss 10.622488, Accuracy 58.673%\n",
      "Epoch 7, Batch 346, LR 0.000095 Loss 10.621596, Accuracy 58.680%\n",
      "Epoch 7, Batch 347, LR 0.000095 Loss 10.621289, Accuracy 58.693%\n",
      "Epoch 7, Batch 348, LR 0.000095 Loss 10.622223, Accuracy 58.679%\n",
      "Epoch 7, Batch 349, LR 0.000095 Loss 10.621411, Accuracy 58.677%\n",
      "Epoch 7, Batch 350, LR 0.000095 Loss 10.623520, Accuracy 58.670%\n",
      "Epoch 7, Batch 351, LR 0.000095 Loss 10.621822, Accuracy 58.685%\n",
      "Epoch 7, Batch 352, LR 0.000095 Loss 10.624790, Accuracy 58.660%\n",
      "Epoch 7, Batch 353, LR 0.000095 Loss 10.625593, Accuracy 58.638%\n",
      "Epoch 7, Batch 354, LR 0.000095 Loss 10.625937, Accuracy 58.642%\n",
      "Epoch 7, Batch 355, LR 0.000095 Loss 10.625925, Accuracy 58.647%\n",
      "Epoch 7, Batch 356, LR 0.000095 Loss 10.626321, Accuracy 58.642%\n",
      "Epoch 7, Batch 357, LR 0.000095 Loss 10.626895, Accuracy 58.644%\n",
      "Epoch 7, Batch 358, LR 0.000095 Loss 10.627463, Accuracy 58.646%\n",
      "Epoch 7, Batch 359, LR 0.000095 Loss 10.628259, Accuracy 58.637%\n",
      "Epoch 7, Batch 360, LR 0.000095 Loss 10.631197, Accuracy 58.613%\n",
      "Epoch 7, Batch 361, LR 0.000095 Loss 10.632369, Accuracy 58.596%\n",
      "Epoch 7, Batch 362, LR 0.000095 Loss 10.633144, Accuracy 58.592%\n",
      "Epoch 7, Batch 363, LR 0.000095 Loss 10.632801, Accuracy 58.587%\n",
      "Epoch 7, Batch 364, LR 0.000095 Loss 10.635031, Accuracy 58.566%\n",
      "Epoch 7, Batch 365, LR 0.000095 Loss 10.635357, Accuracy 58.560%\n",
      "Epoch 7, Batch 366, LR 0.000095 Loss 10.635288, Accuracy 58.547%\n",
      "Epoch 7, Batch 367, LR 0.000095 Loss 10.635165, Accuracy 58.553%\n",
      "Epoch 7, Batch 368, LR 0.000095 Loss 10.635703, Accuracy 58.553%\n",
      "Epoch 7, Batch 369, LR 0.000095 Loss 10.635053, Accuracy 58.558%\n",
      "Epoch 7, Batch 370, LR 0.000095 Loss 10.633308, Accuracy 58.558%\n",
      "Epoch 7, Batch 371, LR 0.000095 Loss 10.630118, Accuracy 58.579%\n",
      "Epoch 7, Batch 372, LR 0.000095 Loss 10.629034, Accuracy 58.590%\n",
      "Epoch 7, Batch 373, LR 0.000095 Loss 10.626984, Accuracy 58.602%\n",
      "Epoch 7, Batch 374, LR 0.000095 Loss 10.627256, Accuracy 58.610%\n",
      "Epoch 7, Batch 375, LR 0.000095 Loss 10.623837, Accuracy 58.635%\n",
      "Epoch 7, Batch 376, LR 0.000095 Loss 10.623625, Accuracy 58.631%\n",
      "Epoch 7, Batch 377, LR 0.000095 Loss 10.622096, Accuracy 58.633%\n",
      "Epoch 7, Batch 378, LR 0.000095 Loss 10.622868, Accuracy 58.635%\n",
      "Epoch 7, Batch 379, LR 0.000095 Loss 10.623324, Accuracy 58.651%\n",
      "Epoch 7, Batch 380, LR 0.000095 Loss 10.624985, Accuracy 58.649%\n",
      "Epoch 7, Batch 381, LR 0.000095 Loss 10.626047, Accuracy 58.645%\n",
      "Epoch 7, Batch 382, LR 0.000095 Loss 10.625316, Accuracy 58.643%\n",
      "Epoch 7, Batch 383, LR 0.000095 Loss 10.624528, Accuracy 58.639%\n",
      "Epoch 7, Batch 384, LR 0.000095 Loss 10.625638, Accuracy 58.622%\n",
      "Epoch 7, Batch 385, LR 0.000095 Loss 10.623805, Accuracy 58.622%\n",
      "Epoch 7, Batch 386, LR 0.000095 Loss 10.625215, Accuracy 58.614%\n",
      "Epoch 7, Batch 387, LR 0.000095 Loss 10.625021, Accuracy 58.618%\n",
      "Epoch 7, Batch 388, LR 0.000095 Loss 10.624392, Accuracy 58.628%\n",
      "Epoch 7, Batch 389, LR 0.000095 Loss 10.626499, Accuracy 58.622%\n",
      "Epoch 7, Batch 390, LR 0.000095 Loss 10.628309, Accuracy 58.606%\n",
      "Epoch 7, Batch 391, LR 0.000095 Loss 10.627843, Accuracy 58.620%\n",
      "Epoch 7, Batch 392, LR 0.000095 Loss 10.625922, Accuracy 58.642%\n",
      "Epoch 7, Batch 393, LR 0.000095 Loss 10.626695, Accuracy 58.641%\n",
      "Epoch 7, Batch 394, LR 0.000095 Loss 10.622501, Accuracy 58.659%\n",
      "Epoch 7, Batch 395, LR 0.000095 Loss 10.623193, Accuracy 58.661%\n",
      "Epoch 7, Batch 396, LR 0.000095 Loss 10.622782, Accuracy 58.673%\n",
      "Epoch 7, Batch 397, LR 0.000095 Loss 10.621833, Accuracy 58.672%\n",
      "Epoch 7, Batch 398, LR 0.000095 Loss 10.621454, Accuracy 58.684%\n",
      "Epoch 7, Batch 399, LR 0.000095 Loss 10.621862, Accuracy 58.692%\n",
      "Epoch 7, Batch 400, LR 0.000095 Loss 10.620400, Accuracy 58.703%\n",
      "Epoch 7, Batch 401, LR 0.000095 Loss 10.617882, Accuracy 58.718%\n",
      "Epoch 7, Batch 402, LR 0.000095 Loss 10.618545, Accuracy 58.716%\n",
      "Epoch 7, Batch 403, LR 0.000095 Loss 10.615456, Accuracy 58.743%\n",
      "Epoch 7, Batch 404, LR 0.000095 Loss 10.613157, Accuracy 58.762%\n",
      "Epoch 7, Batch 405, LR 0.000095 Loss 10.611058, Accuracy 58.789%\n",
      "Epoch 7, Batch 406, LR 0.000095 Loss 10.612394, Accuracy 58.778%\n",
      "Epoch 7, Batch 407, LR 0.000095 Loss 10.610951, Accuracy 58.784%\n",
      "Epoch 7, Batch 408, LR 0.000095 Loss 10.610470, Accuracy 58.791%\n",
      "Epoch 7, Batch 409, LR 0.000095 Loss 10.609358, Accuracy 58.792%\n",
      "Epoch 7, Batch 410, LR 0.000095 Loss 10.609836, Accuracy 58.775%\n",
      "Epoch 7, Batch 411, LR 0.000095 Loss 10.609456, Accuracy 58.788%\n",
      "Epoch 7, Batch 412, LR 0.000095 Loss 10.611945, Accuracy 58.764%\n",
      "Epoch 7, Batch 413, LR 0.000095 Loss 10.611515, Accuracy 58.779%\n",
      "Epoch 7, Batch 414, LR 0.000095 Loss 10.611830, Accuracy 58.784%\n",
      "Epoch 7, Batch 415, LR 0.000095 Loss 10.611988, Accuracy 58.780%\n",
      "Epoch 7, Batch 416, LR 0.000095 Loss 10.614160, Accuracy 58.763%\n",
      "Epoch 7, Batch 417, LR 0.000095 Loss 10.613984, Accuracy 58.775%\n",
      "Epoch 7, Batch 418, LR 0.000095 Loss 10.614517, Accuracy 58.771%\n",
      "Epoch 7, Batch 419, LR 0.000095 Loss 10.614527, Accuracy 58.778%\n",
      "Epoch 7, Batch 420, LR 0.000095 Loss 10.614986, Accuracy 58.776%\n",
      "Epoch 7, Batch 421, LR 0.000095 Loss 10.615941, Accuracy 58.755%\n",
      "Epoch 7, Batch 422, LR 0.000095 Loss 10.615099, Accuracy 58.766%\n",
      "Epoch 7, Batch 423, LR 0.000095 Loss 10.614604, Accuracy 58.767%\n",
      "Epoch 7, Batch 424, LR 0.000095 Loss 10.614980, Accuracy 58.767%\n",
      "Epoch 7, Batch 425, LR 0.000095 Loss 10.615011, Accuracy 58.757%\n",
      "Epoch 7, Batch 426, LR 0.000095 Loss 10.616422, Accuracy 58.755%\n",
      "Epoch 7, Batch 427, LR 0.000095 Loss 10.614251, Accuracy 58.784%\n",
      "Epoch 7, Batch 428, LR 0.000095 Loss 10.615906, Accuracy 58.769%\n",
      "Epoch 7, Batch 429, LR 0.000095 Loss 10.617895, Accuracy 58.765%\n",
      "Epoch 7, Batch 430, LR 0.000095 Loss 10.616099, Accuracy 58.779%\n",
      "Epoch 7, Batch 431, LR 0.000095 Loss 10.613654, Accuracy 58.804%\n",
      "Epoch 7, Batch 432, LR 0.000095 Loss 10.613502, Accuracy 58.805%\n",
      "Epoch 7, Batch 433, LR 0.000095 Loss 10.610462, Accuracy 58.810%\n",
      "Epoch 7, Batch 434, LR 0.000095 Loss 10.610287, Accuracy 58.812%\n",
      "Epoch 7, Batch 435, LR 0.000095 Loss 10.609886, Accuracy 58.815%\n",
      "Epoch 7, Batch 436, LR 0.000095 Loss 10.610461, Accuracy 58.811%\n",
      "Epoch 7, Batch 437, LR 0.000095 Loss 10.609216, Accuracy 58.806%\n",
      "Epoch 7, Batch 438, LR 0.000095 Loss 10.608291, Accuracy 58.817%\n",
      "Epoch 7, Batch 439, LR 0.000095 Loss 10.606037, Accuracy 58.841%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 440, LR 0.000095 Loss 10.607389, Accuracy 58.835%\n",
      "Epoch 7, Batch 441, LR 0.000095 Loss 10.608631, Accuracy 58.831%\n",
      "Epoch 7, Batch 442, LR 0.000095 Loss 10.608239, Accuracy 58.829%\n",
      "Epoch 7, Batch 443, LR 0.000095 Loss 10.608882, Accuracy 58.811%\n",
      "Epoch 7, Batch 444, LR 0.000095 Loss 10.606566, Accuracy 58.835%\n",
      "Epoch 7, Batch 445, LR 0.000095 Loss 10.605433, Accuracy 58.843%\n",
      "Epoch 7, Batch 446, LR 0.000095 Loss 10.604920, Accuracy 58.841%\n",
      "Epoch 7, Batch 447, LR 0.000095 Loss 10.606930, Accuracy 58.819%\n",
      "Epoch 7, Batch 448, LR 0.000095 Loss 10.606666, Accuracy 58.820%\n",
      "Epoch 7, Batch 449, LR 0.000095 Loss 10.605189, Accuracy 58.827%\n",
      "Epoch 7, Batch 450, LR 0.000095 Loss 10.604503, Accuracy 58.830%\n",
      "Epoch 7, Batch 451, LR 0.000095 Loss 10.604425, Accuracy 58.822%\n",
      "Epoch 7, Batch 452, LR 0.000095 Loss 10.603708, Accuracy 58.822%\n",
      "Epoch 7, Batch 453, LR 0.000095 Loss 10.603732, Accuracy 58.816%\n",
      "Epoch 7, Batch 454, LR 0.000095 Loss 10.603215, Accuracy 58.828%\n",
      "Epoch 7, Batch 455, LR 0.000095 Loss 10.603514, Accuracy 58.815%\n",
      "Epoch 7, Batch 456, LR 0.000095 Loss 10.604595, Accuracy 58.813%\n",
      "Epoch 7, Batch 457, LR 0.000095 Loss 10.605308, Accuracy 58.809%\n",
      "Epoch 7, Batch 458, LR 0.000095 Loss 10.605971, Accuracy 58.800%\n",
      "Epoch 7, Batch 459, LR 0.000095 Loss 10.605115, Accuracy 58.805%\n",
      "Epoch 7, Batch 460, LR 0.000095 Loss 10.605287, Accuracy 58.803%\n",
      "Epoch 7, Batch 461, LR 0.000095 Loss 10.604497, Accuracy 58.809%\n",
      "Epoch 7, Batch 462, LR 0.000095 Loss 10.603349, Accuracy 58.824%\n",
      "Epoch 7, Batch 463, LR 0.000095 Loss 10.602592, Accuracy 58.837%\n",
      "Epoch 7, Batch 464, LR 0.000095 Loss 10.604184, Accuracy 58.831%\n",
      "Epoch 7, Batch 465, LR 0.000095 Loss 10.602451, Accuracy 58.832%\n",
      "Epoch 7, Batch 466, LR 0.000095 Loss 10.604238, Accuracy 58.817%\n",
      "Epoch 7, Batch 467, LR 0.000095 Loss 10.604475, Accuracy 58.818%\n",
      "Epoch 7, Batch 468, LR 0.000095 Loss 10.602685, Accuracy 58.829%\n",
      "Epoch 7, Batch 469, LR 0.000095 Loss 10.601299, Accuracy 58.845%\n",
      "Epoch 7, Batch 470, LR 0.000095 Loss 10.600701, Accuracy 58.856%\n",
      "Epoch 7, Batch 471, LR 0.000095 Loss 10.601405, Accuracy 58.859%\n",
      "Epoch 7, Batch 472, LR 0.000095 Loss 10.601241, Accuracy 58.865%\n",
      "Epoch 7, Batch 473, LR 0.000095 Loss 10.600920, Accuracy 58.856%\n",
      "Epoch 7, Batch 474, LR 0.000095 Loss 10.600634, Accuracy 58.848%\n",
      "Epoch 7, Batch 475, LR 0.000095 Loss 10.600493, Accuracy 58.845%\n",
      "Epoch 7, Batch 476, LR 0.000095 Loss 10.599587, Accuracy 58.850%\n",
      "Epoch 7, Batch 477, LR 0.000095 Loss 10.598981, Accuracy 58.866%\n",
      "Epoch 7, Batch 478, LR 0.000095 Loss 10.597980, Accuracy 58.877%\n",
      "Epoch 7, Batch 479, LR 0.000095 Loss 10.596233, Accuracy 58.891%\n",
      "Epoch 7, Batch 480, LR 0.000095 Loss 10.597271, Accuracy 58.882%\n",
      "Epoch 7, Batch 481, LR 0.000095 Loss 10.596082, Accuracy 58.888%\n",
      "Epoch 7, Batch 482, LR 0.000095 Loss 10.595183, Accuracy 58.895%\n",
      "Epoch 7, Batch 483, LR 0.000095 Loss 10.594689, Accuracy 58.896%\n",
      "Epoch 7, Batch 484, LR 0.000095 Loss 10.595112, Accuracy 58.884%\n",
      "Epoch 7, Batch 485, LR 0.000095 Loss 10.595662, Accuracy 58.882%\n",
      "Epoch 7, Batch 486, LR 0.000095 Loss 10.595780, Accuracy 58.878%\n",
      "Epoch 7, Batch 487, LR 0.000095 Loss 10.595578, Accuracy 58.873%\n",
      "Epoch 7, Batch 488, LR 0.000095 Loss 10.594334, Accuracy 58.882%\n",
      "Epoch 7, Batch 489, LR 0.000095 Loss 10.593950, Accuracy 58.897%\n",
      "Epoch 7, Batch 490, LR 0.000095 Loss 10.593927, Accuracy 58.905%\n",
      "Epoch 7, Batch 491, LR 0.000095 Loss 10.593813, Accuracy 58.906%\n",
      "Epoch 7, Batch 492, LR 0.000095 Loss 10.592967, Accuracy 58.902%\n",
      "Epoch 7, Batch 493, LR 0.000095 Loss 10.593993, Accuracy 58.887%\n",
      "Epoch 7, Batch 494, LR 0.000095 Loss 10.593246, Accuracy 58.883%\n",
      "Epoch 7, Batch 495, LR 0.000095 Loss 10.592671, Accuracy 58.894%\n",
      "Epoch 7, Batch 496, LR 0.000095 Loss 10.593345, Accuracy 58.893%\n",
      "Epoch 7, Batch 497, LR 0.000095 Loss 10.591754, Accuracy 58.908%\n",
      "Epoch 7, Batch 498, LR 0.000095 Loss 10.590109, Accuracy 58.909%\n",
      "Epoch 7, Batch 499, LR 0.000095 Loss 10.589537, Accuracy 58.913%\n",
      "Epoch 7, Batch 500, LR 0.000095 Loss 10.589813, Accuracy 58.909%\n",
      "Epoch 7, Batch 501, LR 0.000095 Loss 10.588062, Accuracy 58.913%\n",
      "Epoch 7, Batch 502, LR 0.000095 Loss 10.588835, Accuracy 58.905%\n",
      "Epoch 7, Batch 503, LR 0.000095 Loss 10.588407, Accuracy 58.900%\n",
      "Epoch 7, Batch 504, LR 0.000095 Loss 10.586739, Accuracy 58.912%\n",
      "Epoch 7, Batch 505, LR 0.000095 Loss 10.586730, Accuracy 58.908%\n",
      "Epoch 7, Batch 506, LR 0.000095 Loss 10.586763, Accuracy 58.904%\n",
      "Epoch 7, Batch 507, LR 0.000095 Loss 10.586387, Accuracy 58.911%\n",
      "Epoch 7, Batch 508, LR 0.000095 Loss 10.585670, Accuracy 58.917%\n",
      "Epoch 7, Batch 509, LR 0.000095 Loss 10.586315, Accuracy 58.915%\n",
      "Epoch 7, Batch 510, LR 0.000095 Loss 10.586962, Accuracy 58.909%\n",
      "Epoch 7, Batch 511, LR 0.000095 Loss 10.587304, Accuracy 58.903%\n",
      "Epoch 7, Batch 512, LR 0.000095 Loss 10.586754, Accuracy 58.914%\n",
      "Epoch 7, Batch 513, LR 0.000095 Loss 10.589449, Accuracy 58.897%\n",
      "Epoch 7, Batch 514, LR 0.000095 Loss 10.589043, Accuracy 58.901%\n",
      "Epoch 7, Batch 515, LR 0.000095 Loss 10.588866, Accuracy 58.908%\n",
      "Epoch 7, Batch 516, LR 0.000095 Loss 10.588080, Accuracy 58.906%\n",
      "Epoch 7, Batch 517, LR 0.000095 Loss 10.588411, Accuracy 58.904%\n",
      "Epoch 7, Batch 518, LR 0.000095 Loss 10.589919, Accuracy 58.883%\n",
      "Epoch 7, Batch 519, LR 0.000095 Loss 10.589526, Accuracy 58.880%\n",
      "Epoch 7, Batch 520, LR 0.000095 Loss 10.590741, Accuracy 58.876%\n",
      "Epoch 7, Batch 521, LR 0.000095 Loss 10.590840, Accuracy 58.865%\n",
      "Epoch 7, Batch 522, LR 0.000095 Loss 10.590604, Accuracy 58.866%\n",
      "Epoch 7, Batch 523, LR 0.000095 Loss 10.589319, Accuracy 58.869%\n",
      "Epoch 7, Batch 524, LR 0.000095 Loss 10.588000, Accuracy 58.879%\n",
      "Epoch 7, Batch 525, LR 0.000095 Loss 10.588605, Accuracy 58.879%\n",
      "Epoch 7, Batch 526, LR 0.000095 Loss 10.587378, Accuracy 58.888%\n",
      "Epoch 7, Batch 527, LR 0.000095 Loss 10.587137, Accuracy 58.901%\n",
      "Epoch 7, Batch 528, LR 0.000095 Loss 10.586943, Accuracy 58.907%\n",
      "Epoch 7, Batch 529, LR 0.000095 Loss 10.587259, Accuracy 58.905%\n",
      "Epoch 7, Batch 530, LR 0.000095 Loss 10.586545, Accuracy 58.918%\n",
      "Epoch 7, Batch 531, LR 0.000095 Loss 10.585512, Accuracy 58.934%\n",
      "Epoch 7, Batch 532, LR 0.000095 Loss 10.584329, Accuracy 58.937%\n",
      "Epoch 7, Batch 533, LR 0.000095 Loss 10.584527, Accuracy 58.931%\n",
      "Epoch 7, Batch 534, LR 0.000095 Loss 10.584495, Accuracy 58.926%\n",
      "Epoch 7, Batch 535, LR 0.000095 Loss 10.584448, Accuracy 58.927%\n",
      "Epoch 7, Batch 536, LR 0.000095 Loss 10.583627, Accuracy 58.942%\n",
      "Epoch 7, Batch 537, LR 0.000095 Loss 10.584100, Accuracy 58.936%\n",
      "Epoch 7, Batch 538, LR 0.000095 Loss 10.584391, Accuracy 58.939%\n",
      "Epoch 7, Batch 539, LR 0.000095 Loss 10.584495, Accuracy 58.942%\n",
      "Epoch 7, Batch 540, LR 0.000095 Loss 10.585492, Accuracy 58.934%\n",
      "Epoch 7, Batch 541, LR 0.000095 Loss 10.585370, Accuracy 58.930%\n",
      "Epoch 7, Batch 542, LR 0.000095 Loss 10.585714, Accuracy 58.928%\n",
      "Epoch 7, Batch 543, LR 0.000095 Loss 10.584800, Accuracy 58.929%\n",
      "Epoch 7, Batch 544, LR 0.000095 Loss 10.583915, Accuracy 58.927%\n",
      "Epoch 7, Batch 545, LR 0.000095 Loss 10.583068, Accuracy 58.932%\n",
      "Epoch 7, Batch 546, LR 0.000095 Loss 10.582507, Accuracy 58.946%\n",
      "Epoch 7, Batch 547, LR 0.000095 Loss 10.582007, Accuracy 58.948%\n",
      "Epoch 7, Batch 548, LR 0.000095 Loss 10.581210, Accuracy 58.953%\n",
      "Epoch 7, Batch 549, LR 0.000095 Loss 10.581401, Accuracy 58.952%\n",
      "Epoch 7, Batch 550, LR 0.000095 Loss 10.581142, Accuracy 58.953%\n",
      "Epoch 7, Batch 551, LR 0.000095 Loss 10.580851, Accuracy 58.948%\n",
      "Epoch 7, Batch 552, LR 0.000095 Loss 10.579423, Accuracy 58.955%\n",
      "Epoch 7, Batch 553, LR 0.000095 Loss 10.578843, Accuracy 58.967%\n",
      "Epoch 7, Batch 554, LR 0.000095 Loss 10.577854, Accuracy 58.977%\n",
      "Epoch 7, Batch 555, LR 0.000095 Loss 10.577459, Accuracy 58.981%\n",
      "Epoch 7, Batch 556, LR 0.000095 Loss 10.576570, Accuracy 58.991%\n",
      "Epoch 7, Batch 557, LR 0.000095 Loss 10.576756, Accuracy 58.981%\n",
      "Epoch 7, Batch 558, LR 0.000095 Loss 10.576816, Accuracy 58.990%\n",
      "Epoch 7, Batch 559, LR 0.000095 Loss 10.577219, Accuracy 58.996%\n",
      "Epoch 7, Batch 560, LR 0.000095 Loss 10.577706, Accuracy 58.997%\n",
      "Epoch 7, Batch 561, LR 0.000095 Loss 10.578378, Accuracy 58.989%\n",
      "Epoch 7, Batch 562, LR 0.000095 Loss 10.576653, Accuracy 59.002%\n",
      "Epoch 7, Batch 563, LR 0.000095 Loss 10.577266, Accuracy 58.995%\n",
      "Epoch 7, Batch 564, LR 0.000095 Loss 10.577741, Accuracy 58.995%\n",
      "Epoch 7, Batch 565, LR 0.000095 Loss 10.577579, Accuracy 58.999%\n",
      "Epoch 7, Batch 566, LR 0.000095 Loss 10.575997, Accuracy 59.019%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 567, LR 0.000095 Loss 10.576643, Accuracy 59.021%\n",
      "Epoch 7, Batch 568, LR 0.000095 Loss 10.575532, Accuracy 59.027%\n",
      "Epoch 7, Batch 569, LR 0.000095 Loss 10.574421, Accuracy 59.037%\n",
      "Epoch 7, Batch 570, LR 0.000095 Loss 10.573345, Accuracy 59.038%\n",
      "Epoch 7, Batch 571, LR 0.000095 Loss 10.572792, Accuracy 59.041%\n",
      "Epoch 7, Batch 572, LR 0.000095 Loss 10.572725, Accuracy 59.036%\n",
      "Epoch 7, Batch 573, LR 0.000095 Loss 10.573643, Accuracy 59.033%\n",
      "Epoch 7, Batch 574, LR 0.000095 Loss 10.575187, Accuracy 59.027%\n",
      "Epoch 7, Batch 575, LR 0.000095 Loss 10.575822, Accuracy 59.029%\n",
      "Epoch 7, Batch 576, LR 0.000095 Loss 10.575172, Accuracy 59.030%\n",
      "Epoch 7, Batch 577, LR 0.000095 Loss 10.574648, Accuracy 59.031%\n",
      "Epoch 7, Batch 578, LR 0.000095 Loss 10.574016, Accuracy 59.041%\n",
      "Epoch 7, Batch 579, LR 0.000095 Loss 10.574385, Accuracy 59.038%\n",
      "Epoch 7, Batch 580, LR 0.000095 Loss 10.574726, Accuracy 59.041%\n",
      "Epoch 7, Batch 581, LR 0.000095 Loss 10.575243, Accuracy 59.036%\n",
      "Epoch 7, Batch 582, LR 0.000095 Loss 10.575317, Accuracy 59.046%\n",
      "Epoch 7, Batch 583, LR 0.000095 Loss 10.575046, Accuracy 59.048%\n",
      "Epoch 7, Batch 584, LR 0.000095 Loss 10.573775, Accuracy 59.061%\n",
      "Epoch 7, Batch 585, LR 0.000095 Loss 10.573659, Accuracy 59.061%\n",
      "Epoch 7, Batch 586, LR 0.000095 Loss 10.574810, Accuracy 59.046%\n",
      "Epoch 7, Batch 587, LR 0.000095 Loss 10.575835, Accuracy 59.052%\n",
      "Epoch 7, Batch 588, LR 0.000095 Loss 10.574205, Accuracy 59.060%\n",
      "Epoch 7, Batch 589, LR 0.000095 Loss 10.573865, Accuracy 59.070%\n",
      "Epoch 7, Batch 590, LR 0.000095 Loss 10.574031, Accuracy 59.065%\n",
      "Epoch 7, Batch 591, LR 0.000095 Loss 10.573216, Accuracy 59.067%\n",
      "Epoch 7, Batch 592, LR 0.000095 Loss 10.574244, Accuracy 59.064%\n",
      "Epoch 7, Batch 593, LR 0.000095 Loss 10.574182, Accuracy 59.064%\n",
      "Epoch 7, Batch 594, LR 0.000095 Loss 10.573158, Accuracy 59.074%\n",
      "Epoch 7, Batch 595, LR 0.000095 Loss 10.572855, Accuracy 59.070%\n",
      "Epoch 7, Batch 596, LR 0.000095 Loss 10.573141, Accuracy 59.068%\n",
      "Epoch 7, Batch 597, LR 0.000095 Loss 10.573539, Accuracy 59.074%\n",
      "Epoch 7, Batch 598, LR 0.000095 Loss 10.572595, Accuracy 59.089%\n",
      "Epoch 7, Batch 599, LR 0.000095 Loss 10.572848, Accuracy 59.089%\n",
      "Epoch 7, Batch 600, LR 0.000095 Loss 10.570997, Accuracy 59.103%\n",
      "Epoch 7, Batch 601, LR 0.000095 Loss 10.570823, Accuracy 59.102%\n",
      "Epoch 7, Batch 602, LR 0.000095 Loss 10.570148, Accuracy 59.109%\n",
      "Epoch 7, Batch 603, LR 0.000095 Loss 10.569041, Accuracy 59.113%\n",
      "Epoch 7, Batch 604, LR 0.000095 Loss 10.569193, Accuracy 59.112%\n",
      "Epoch 7, Batch 605, LR 0.000095 Loss 10.568531, Accuracy 59.121%\n",
      "Epoch 7, Batch 606, LR 0.000095 Loss 10.568246, Accuracy 59.122%\n",
      "Epoch 7, Batch 607, LR 0.000095 Loss 10.568678, Accuracy 59.124%\n",
      "Epoch 7, Batch 608, LR 0.000095 Loss 10.568592, Accuracy 59.122%\n",
      "Epoch 7, Batch 609, LR 0.000095 Loss 10.567837, Accuracy 59.135%\n",
      "Epoch 7, Batch 610, LR 0.000095 Loss 10.567219, Accuracy 59.138%\n",
      "Epoch 7, Batch 611, LR 0.000095 Loss 10.566728, Accuracy 59.145%\n",
      "Epoch 7, Batch 612, LR 0.000095 Loss 10.565834, Accuracy 59.148%\n",
      "Epoch 7, Batch 613, LR 0.000095 Loss 10.564947, Accuracy 59.152%\n",
      "Epoch 7, Batch 614, LR 0.000095 Loss 10.564589, Accuracy 59.150%\n",
      "Epoch 7, Batch 615, LR 0.000095 Loss 10.564937, Accuracy 59.136%\n",
      "Epoch 7, Batch 616, LR 0.000095 Loss 10.563267, Accuracy 59.149%\n",
      "Epoch 7, Batch 617, LR 0.000095 Loss 10.561715, Accuracy 59.160%\n",
      "Epoch 7, Batch 618, LR 0.000095 Loss 10.560250, Accuracy 59.175%\n",
      "Epoch 7, Batch 619, LR 0.000095 Loss 10.559941, Accuracy 59.184%\n",
      "Epoch 7, Batch 620, LR 0.000095 Loss 10.559896, Accuracy 59.192%\n",
      "Epoch 7, Batch 621, LR 0.000095 Loss 10.558933, Accuracy 59.200%\n",
      "Epoch 7, Batch 622, LR 0.000095 Loss 10.557557, Accuracy 59.212%\n",
      "Epoch 7, Batch 623, LR 0.000095 Loss 10.559174, Accuracy 59.201%\n",
      "Epoch 7, Batch 624, LR 0.000095 Loss 10.559993, Accuracy 59.191%\n",
      "Epoch 7, Batch 625, LR 0.000095 Loss 10.560124, Accuracy 59.190%\n",
      "Epoch 7, Batch 626, LR 0.000095 Loss 10.558544, Accuracy 59.200%\n",
      "Epoch 7, Batch 627, LR 0.000095 Loss 10.557936, Accuracy 59.209%\n",
      "Epoch 7, Batch 628, LR 0.000095 Loss 10.557950, Accuracy 59.208%\n",
      "Epoch 7, Batch 629, LR 0.000095 Loss 10.558270, Accuracy 59.206%\n",
      "Epoch 7, Batch 630, LR 0.000095 Loss 10.557377, Accuracy 59.215%\n",
      "Epoch 7, Batch 631, LR 0.000095 Loss 10.557323, Accuracy 59.225%\n",
      "Epoch 7, Batch 632, LR 0.000095 Loss 10.556463, Accuracy 59.232%\n",
      "Epoch 7, Batch 633, LR 0.000095 Loss 10.555259, Accuracy 59.244%\n",
      "Epoch 7, Batch 634, LR 0.000095 Loss 10.554778, Accuracy 59.243%\n",
      "Epoch 7, Batch 635, LR 0.000095 Loss 10.555919, Accuracy 59.231%\n",
      "Epoch 7, Batch 636, LR 0.000095 Loss 10.555246, Accuracy 59.239%\n",
      "Epoch 7, Batch 637, LR 0.000095 Loss 10.553055, Accuracy 59.256%\n",
      "Epoch 7, Batch 638, LR 0.000095 Loss 10.553935, Accuracy 59.248%\n",
      "Epoch 7, Batch 639, LR 0.000095 Loss 10.552879, Accuracy 59.244%\n",
      "Epoch 7, Batch 640, LR 0.000095 Loss 10.554441, Accuracy 59.226%\n",
      "Epoch 7, Batch 641, LR 0.000095 Loss 10.553500, Accuracy 59.240%\n",
      "Epoch 7, Batch 642, LR 0.000095 Loss 10.554325, Accuracy 59.235%\n",
      "Epoch 7, Batch 643, LR 0.000095 Loss 10.553427, Accuracy 59.246%\n",
      "Epoch 7, Batch 644, LR 0.000095 Loss 10.552766, Accuracy 59.256%\n",
      "Epoch 7, Batch 645, LR 0.000095 Loss 10.552887, Accuracy 59.259%\n",
      "Epoch 7, Batch 646, LR 0.000095 Loss 10.553749, Accuracy 59.254%\n",
      "Epoch 7, Batch 647, LR 0.000095 Loss 10.552472, Accuracy 59.264%\n",
      "Epoch 7, Batch 648, LR 0.000095 Loss 10.551875, Accuracy 59.265%\n",
      "Epoch 7, Batch 649, LR 0.000095 Loss 10.550643, Accuracy 59.276%\n",
      "Epoch 7, Batch 650, LR 0.000095 Loss 10.549987, Accuracy 59.284%\n",
      "Epoch 7, Batch 651, LR 0.000095 Loss 10.550165, Accuracy 59.280%\n",
      "Epoch 7, Batch 652, LR 0.000095 Loss 10.549506, Accuracy 59.279%\n",
      "Epoch 7, Batch 653, LR 0.000095 Loss 10.548081, Accuracy 59.285%\n",
      "Epoch 7, Batch 654, LR 0.000095 Loss 10.547191, Accuracy 59.289%\n",
      "Epoch 7, Batch 655, LR 0.000095 Loss 10.546761, Accuracy 59.290%\n",
      "Epoch 7, Batch 656, LR 0.000095 Loss 10.547441, Accuracy 59.280%\n",
      "Epoch 7, Batch 657, LR 0.000095 Loss 10.546857, Accuracy 59.281%\n",
      "Epoch 7, Batch 658, LR 0.000095 Loss 10.545745, Accuracy 59.293%\n",
      "Epoch 7, Batch 659, LR 0.000095 Loss 10.545766, Accuracy 59.296%\n",
      "Epoch 7, Batch 660, LR 0.000095 Loss 10.546480, Accuracy 59.292%\n",
      "Epoch 7, Batch 661, LR 0.000095 Loss 10.546632, Accuracy 59.290%\n",
      "Epoch 7, Batch 662, LR 0.000095 Loss 10.546664, Accuracy 59.286%\n",
      "Epoch 7, Batch 663, LR 0.000095 Loss 10.546552, Accuracy 59.287%\n",
      "Epoch 7, Batch 664, LR 0.000095 Loss 10.546783, Accuracy 59.289%\n",
      "Epoch 7, Batch 665, LR 0.000095 Loss 10.546058, Accuracy 59.300%\n",
      "Epoch 7, Batch 666, LR 0.000095 Loss 10.545060, Accuracy 59.309%\n",
      "Epoch 7, Batch 667, LR 0.000095 Loss 10.545122, Accuracy 59.308%\n",
      "Epoch 7, Batch 668, LR 0.000095 Loss 10.545907, Accuracy 59.302%\n",
      "Epoch 7, Batch 669, LR 0.000095 Loss 10.545531, Accuracy 59.300%\n",
      "Epoch 7, Batch 670, LR 0.000095 Loss 10.545043, Accuracy 59.299%\n",
      "Epoch 7, Batch 671, LR 0.000095 Loss 10.544603, Accuracy 59.300%\n",
      "Epoch 7, Batch 672, LR 0.000095 Loss 10.544625, Accuracy 59.295%\n",
      "Epoch 7, Batch 673, LR 0.000095 Loss 10.544330, Accuracy 59.297%\n",
      "Epoch 7, Batch 674, LR 0.000095 Loss 10.544818, Accuracy 59.292%\n",
      "Epoch 7, Batch 675, LR 0.000095 Loss 10.544576, Accuracy 59.292%\n",
      "Epoch 7, Batch 676, LR 0.000095 Loss 10.544362, Accuracy 59.291%\n",
      "Epoch 7, Batch 677, LR 0.000095 Loss 10.545702, Accuracy 59.282%\n",
      "Epoch 7, Batch 678, LR 0.000095 Loss 10.545685, Accuracy 59.278%\n",
      "Epoch 7, Batch 679, LR 0.000095 Loss 10.543934, Accuracy 59.286%\n",
      "Epoch 7, Batch 680, LR 0.000095 Loss 10.543584, Accuracy 59.293%\n",
      "Epoch 7, Batch 681, LR 0.000095 Loss 10.543499, Accuracy 59.294%\n",
      "Epoch 7, Batch 682, LR 0.000095 Loss 10.543402, Accuracy 59.295%\n",
      "Epoch 7, Batch 683, LR 0.000095 Loss 10.544138, Accuracy 59.294%\n",
      "Epoch 7, Batch 684, LR 0.000095 Loss 10.543009, Accuracy 59.303%\n",
      "Epoch 7, Batch 685, LR 0.000095 Loss 10.542883, Accuracy 59.305%\n",
      "Epoch 7, Batch 686, LR 0.000095 Loss 10.542875, Accuracy 59.301%\n",
      "Epoch 7, Batch 687, LR 0.000095 Loss 10.542106, Accuracy 59.304%\n",
      "Epoch 7, Batch 688, LR 0.000095 Loss 10.542074, Accuracy 59.302%\n",
      "Epoch 7, Batch 689, LR 0.000095 Loss 10.540435, Accuracy 59.309%\n",
      "Epoch 7, Batch 690, LR 0.000095 Loss 10.540092, Accuracy 59.310%\n",
      "Epoch 7, Batch 691, LR 0.000095 Loss 10.539623, Accuracy 59.311%\n",
      "Epoch 7, Batch 692, LR 0.000095 Loss 10.539808, Accuracy 59.310%\n",
      "Epoch 7, Batch 693, LR 0.000095 Loss 10.540320, Accuracy 59.310%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 694, LR 0.000095 Loss 10.539755, Accuracy 59.315%\n",
      "Epoch 7, Batch 695, LR 0.000095 Loss 10.539102, Accuracy 59.319%\n",
      "Epoch 7, Batch 696, LR 0.000095 Loss 10.538456, Accuracy 59.321%\n",
      "Epoch 7, Batch 697, LR 0.000095 Loss 10.539461, Accuracy 59.319%\n",
      "Epoch 7, Batch 698, LR 0.000095 Loss 10.538041, Accuracy 59.322%\n",
      "Epoch 7, Batch 699, LR 0.000095 Loss 10.537554, Accuracy 59.328%\n",
      "Epoch 7, Batch 700, LR 0.000095 Loss 10.536883, Accuracy 59.334%\n",
      "Epoch 7, Batch 701, LR 0.000095 Loss 10.537486, Accuracy 59.329%\n",
      "Epoch 7, Batch 702, LR 0.000095 Loss 10.536741, Accuracy 59.333%\n",
      "Epoch 7, Batch 703, LR 0.000095 Loss 10.537097, Accuracy 59.332%\n",
      "Epoch 7, Batch 704, LR 0.000095 Loss 10.537140, Accuracy 59.336%\n",
      "Epoch 7, Batch 705, LR 0.000095 Loss 10.537320, Accuracy 59.332%\n",
      "Epoch 7, Batch 706, LR 0.000095 Loss 10.536245, Accuracy 59.333%\n",
      "Epoch 7, Batch 707, LR 0.000095 Loss 10.535762, Accuracy 59.336%\n",
      "Epoch 7, Batch 708, LR 0.000095 Loss 10.535118, Accuracy 59.342%\n",
      "Epoch 7, Batch 709, LR 0.000095 Loss 10.535281, Accuracy 59.345%\n",
      "Epoch 7, Batch 710, LR 0.000095 Loss 10.534585, Accuracy 59.349%\n",
      "Epoch 7, Batch 711, LR 0.000095 Loss 10.534804, Accuracy 59.349%\n",
      "Epoch 7, Batch 712, LR 0.000095 Loss 10.534015, Accuracy 59.356%\n",
      "Epoch 7, Batch 713, LR 0.000095 Loss 10.533245, Accuracy 59.359%\n",
      "Epoch 7, Batch 714, LR 0.000095 Loss 10.532964, Accuracy 59.363%\n",
      "Epoch 7, Batch 715, LR 0.000095 Loss 10.532153, Accuracy 59.374%\n",
      "Epoch 7, Batch 716, LR 0.000095 Loss 10.531182, Accuracy 59.380%\n",
      "Epoch 7, Batch 717, LR 0.000095 Loss 10.529661, Accuracy 59.388%\n",
      "Epoch 7, Batch 718, LR 0.000095 Loss 10.529859, Accuracy 59.389%\n",
      "Epoch 7, Batch 719, LR 0.000095 Loss 10.529646, Accuracy 59.393%\n",
      "Epoch 7, Batch 720, LR 0.000095 Loss 10.530535, Accuracy 59.382%\n",
      "Epoch 7, Batch 721, LR 0.000095 Loss 10.530398, Accuracy 59.383%\n",
      "Epoch 7, Batch 722, LR 0.000095 Loss 10.529373, Accuracy 59.390%\n",
      "Epoch 7, Batch 723, LR 0.000095 Loss 10.528688, Accuracy 59.390%\n",
      "Epoch 7, Batch 724, LR 0.000095 Loss 10.528025, Accuracy 59.394%\n",
      "Epoch 7, Batch 725, LR 0.000095 Loss 10.526304, Accuracy 59.411%\n",
      "Epoch 7, Batch 726, LR 0.000095 Loss 10.527007, Accuracy 59.399%\n",
      "Epoch 7, Batch 727, LR 0.000095 Loss 10.527368, Accuracy 59.392%\n",
      "Epoch 7, Batch 728, LR 0.000095 Loss 10.527391, Accuracy 59.392%\n",
      "Epoch 7, Batch 729, LR 0.000095 Loss 10.527300, Accuracy 59.395%\n",
      "Epoch 7, Batch 730, LR 0.000095 Loss 10.527210, Accuracy 59.396%\n",
      "Epoch 7, Batch 731, LR 0.000095 Loss 10.526795, Accuracy 59.396%\n",
      "Epoch 7, Batch 732, LR 0.000095 Loss 10.525573, Accuracy 59.402%\n",
      "Epoch 7, Batch 733, LR 0.000095 Loss 10.525808, Accuracy 59.396%\n",
      "Epoch 7, Batch 734, LR 0.000095 Loss 10.525391, Accuracy 59.401%\n",
      "Epoch 7, Batch 735, LR 0.000095 Loss 10.525078, Accuracy 59.402%\n",
      "Epoch 7, Batch 736, LR 0.000095 Loss 10.524187, Accuracy 59.409%\n",
      "Epoch 7, Batch 737, LR 0.000095 Loss 10.523494, Accuracy 59.415%\n",
      "Epoch 7, Batch 738, LR 0.000095 Loss 10.524080, Accuracy 59.405%\n",
      "Epoch 7, Batch 739, LR 0.000095 Loss 10.523890, Accuracy 59.404%\n",
      "Epoch 7, Batch 740, LR 0.000095 Loss 10.523507, Accuracy 59.406%\n",
      "Epoch 7, Batch 741, LR 0.000095 Loss 10.523317, Accuracy 59.411%\n",
      "Epoch 7, Batch 742, LR 0.000095 Loss 10.523476, Accuracy 59.410%\n",
      "Epoch 7, Batch 743, LR 0.000095 Loss 10.523273, Accuracy 59.412%\n",
      "Epoch 7, Batch 744, LR 0.000095 Loss 10.522015, Accuracy 59.426%\n",
      "Epoch 7, Batch 745, LR 0.000095 Loss 10.522391, Accuracy 59.428%\n",
      "Epoch 7, Batch 746, LR 0.000095 Loss 10.521347, Accuracy 59.433%\n",
      "Epoch 7, Batch 747, LR 0.000095 Loss 10.521813, Accuracy 59.431%\n",
      "Epoch 7, Batch 748, LR 0.000095 Loss 10.520410, Accuracy 59.440%\n",
      "Epoch 7, Batch 749, LR 0.000095 Loss 10.520386, Accuracy 59.439%\n",
      "Epoch 7, Batch 750, LR 0.000095 Loss 10.519453, Accuracy 59.440%\n",
      "Epoch 7, Batch 751, LR 0.000095 Loss 10.518975, Accuracy 59.437%\n",
      "Epoch 7, Batch 752, LR 0.000095 Loss 10.519665, Accuracy 59.431%\n",
      "Epoch 7, Batch 753, LR 0.000095 Loss 10.519040, Accuracy 59.432%\n",
      "Epoch 7, Batch 754, LR 0.000095 Loss 10.519615, Accuracy 59.426%\n",
      "Epoch 7, Batch 755, LR 0.000095 Loss 10.519077, Accuracy 59.432%\n",
      "Epoch 7, Batch 756, LR 0.000095 Loss 10.519560, Accuracy 59.431%\n",
      "Epoch 7, Batch 757, LR 0.000095 Loss 10.519239, Accuracy 59.438%\n",
      "Epoch 7, Batch 758, LR 0.000095 Loss 10.518502, Accuracy 59.437%\n",
      "Epoch 7, Batch 759, LR 0.000095 Loss 10.517568, Accuracy 59.436%\n",
      "Epoch 7, Batch 760, LR 0.000095 Loss 10.516770, Accuracy 59.446%\n",
      "Epoch 7, Batch 761, LR 0.000094 Loss 10.515716, Accuracy 59.456%\n",
      "Epoch 7, Batch 762, LR 0.000094 Loss 10.515428, Accuracy 59.458%\n",
      "Epoch 7, Batch 763, LR 0.000094 Loss 10.515320, Accuracy 59.455%\n",
      "Epoch 7, Batch 764, LR 0.000094 Loss 10.515454, Accuracy 59.450%\n",
      "Epoch 7, Batch 765, LR 0.000094 Loss 10.515473, Accuracy 59.443%\n",
      "Epoch 7, Batch 766, LR 0.000094 Loss 10.515928, Accuracy 59.441%\n",
      "Epoch 7, Batch 767, LR 0.000094 Loss 10.516068, Accuracy 59.441%\n",
      "Epoch 7, Batch 768, LR 0.000094 Loss 10.515314, Accuracy 59.444%\n",
      "Epoch 7, Batch 769, LR 0.000094 Loss 10.515699, Accuracy 59.439%\n",
      "Epoch 7, Batch 770, LR 0.000094 Loss 10.515429, Accuracy 59.446%\n",
      "Epoch 7, Batch 771, LR 0.000094 Loss 10.514938, Accuracy 59.456%\n",
      "Epoch 7, Batch 772, LR 0.000094 Loss 10.515689, Accuracy 59.449%\n",
      "Epoch 7, Batch 773, LR 0.000094 Loss 10.516110, Accuracy 59.443%\n",
      "Epoch 7, Batch 774, LR 0.000094 Loss 10.515159, Accuracy 59.454%\n",
      "Epoch 7, Batch 775, LR 0.000094 Loss 10.514131, Accuracy 59.457%\n",
      "Epoch 7, Batch 776, LR 0.000094 Loss 10.515055, Accuracy 59.452%\n",
      "Epoch 7, Batch 777, LR 0.000094 Loss 10.514710, Accuracy 59.453%\n",
      "Epoch 7, Batch 778, LR 0.000094 Loss 10.513855, Accuracy 59.465%\n",
      "Epoch 7, Batch 779, LR 0.000094 Loss 10.513710, Accuracy 59.466%\n",
      "Epoch 7, Batch 780, LR 0.000094 Loss 10.512436, Accuracy 59.469%\n",
      "Epoch 7, Batch 781, LR 0.000094 Loss 10.511450, Accuracy 59.482%\n",
      "Epoch 7, Batch 782, LR 0.000094 Loss 10.511217, Accuracy 59.478%\n",
      "Epoch 7, Batch 783, LR 0.000094 Loss 10.510711, Accuracy 59.482%\n",
      "Epoch 7, Batch 784, LR 0.000094 Loss 10.509817, Accuracy 59.486%\n",
      "Epoch 7, Batch 785, LR 0.000094 Loss 10.509630, Accuracy 59.487%\n",
      "Epoch 7, Batch 786, LR 0.000094 Loss 10.510485, Accuracy 59.475%\n",
      "Epoch 7, Batch 787, LR 0.000094 Loss 10.509624, Accuracy 59.480%\n",
      "Epoch 7, Batch 788, LR 0.000094 Loss 10.509523, Accuracy 59.485%\n",
      "Epoch 7, Batch 789, LR 0.000094 Loss 10.509251, Accuracy 59.488%\n",
      "Epoch 7, Batch 790, LR 0.000094 Loss 10.509623, Accuracy 59.490%\n",
      "Epoch 7, Batch 791, LR 0.000094 Loss 10.510807, Accuracy 59.481%\n",
      "Epoch 7, Batch 792, LR 0.000094 Loss 10.509755, Accuracy 59.490%\n",
      "Epoch 7, Batch 793, LR 0.000094 Loss 10.509231, Accuracy 59.495%\n",
      "Epoch 7, Batch 794, LR 0.000094 Loss 10.508800, Accuracy 59.502%\n",
      "Epoch 7, Batch 795, LR 0.000094 Loss 10.509360, Accuracy 59.500%\n",
      "Epoch 7, Batch 796, LR 0.000094 Loss 10.509641, Accuracy 59.499%\n",
      "Epoch 7, Batch 797, LR 0.000094 Loss 10.509931, Accuracy 59.498%\n",
      "Epoch 7, Batch 798, LR 0.000094 Loss 10.508993, Accuracy 59.505%\n",
      "Epoch 7, Batch 799, LR 0.000094 Loss 10.507339, Accuracy 59.519%\n",
      "Epoch 7, Batch 800, LR 0.000094 Loss 10.506802, Accuracy 59.521%\n",
      "Epoch 7, Batch 801, LR 0.000094 Loss 10.505839, Accuracy 59.519%\n",
      "Epoch 7, Batch 802, LR 0.000094 Loss 10.505178, Accuracy 59.522%\n",
      "Epoch 7, Batch 803, LR 0.000094 Loss 10.505186, Accuracy 59.518%\n",
      "Epoch 7, Batch 804, LR 0.000094 Loss 10.505094, Accuracy 59.518%\n",
      "Epoch 7, Batch 805, LR 0.000094 Loss 10.505364, Accuracy 59.508%\n",
      "Epoch 7, Batch 806, LR 0.000094 Loss 10.505156, Accuracy 59.508%\n",
      "Epoch 7, Batch 807, LR 0.000094 Loss 10.504352, Accuracy 59.514%\n",
      "Epoch 7, Batch 808, LR 0.000094 Loss 10.504291, Accuracy 59.516%\n",
      "Epoch 7, Batch 809, LR 0.000094 Loss 10.503979, Accuracy 59.517%\n",
      "Epoch 7, Batch 810, LR 0.000094 Loss 10.503214, Accuracy 59.518%\n",
      "Epoch 7, Batch 811, LR 0.000094 Loss 10.502487, Accuracy 59.522%\n",
      "Epoch 7, Batch 812, LR 0.000094 Loss 10.501661, Accuracy 59.528%\n",
      "Epoch 7, Batch 813, LR 0.000094 Loss 10.499897, Accuracy 59.538%\n",
      "Epoch 7, Batch 814, LR 0.000094 Loss 10.499655, Accuracy 59.535%\n",
      "Epoch 7, Batch 815, LR 0.000094 Loss 10.499359, Accuracy 59.545%\n",
      "Epoch 7, Batch 816, LR 0.000094 Loss 10.499915, Accuracy 59.539%\n",
      "Epoch 7, Batch 817, LR 0.000094 Loss 10.500855, Accuracy 59.532%\n",
      "Epoch 7, Batch 818, LR 0.000094 Loss 10.500888, Accuracy 59.536%\n",
      "Epoch 7, Batch 819, LR 0.000094 Loss 10.500673, Accuracy 59.536%\n",
      "Epoch 7, Batch 820, LR 0.000094 Loss 10.500108, Accuracy 59.540%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 821, LR 0.000094 Loss 10.499480, Accuracy 59.543%\n",
      "Epoch 7, Batch 822, LR 0.000094 Loss 10.499909, Accuracy 59.543%\n",
      "Epoch 7, Batch 823, LR 0.000094 Loss 10.499343, Accuracy 59.544%\n",
      "Epoch 7, Batch 824, LR 0.000094 Loss 10.498375, Accuracy 59.551%\n",
      "Epoch 7, Batch 825, LR 0.000094 Loss 10.497532, Accuracy 59.560%\n",
      "Epoch 7, Batch 826, LR 0.000094 Loss 10.496549, Accuracy 59.562%\n",
      "Epoch 7, Batch 827, LR 0.000094 Loss 10.496690, Accuracy 59.556%\n",
      "Epoch 7, Batch 828, LR 0.000094 Loss 10.496895, Accuracy 59.559%\n",
      "Epoch 7, Batch 829, LR 0.000094 Loss 10.496518, Accuracy 59.563%\n",
      "Epoch 7, Batch 830, LR 0.000094 Loss 10.496779, Accuracy 59.560%\n",
      "Epoch 7, Batch 831, LR 0.000094 Loss 10.496665, Accuracy 59.568%\n",
      "Epoch 7, Batch 832, LR 0.000094 Loss 10.496237, Accuracy 59.572%\n",
      "Epoch 7, Batch 833, LR 0.000094 Loss 10.496243, Accuracy 59.574%\n",
      "Epoch 7, Batch 834, LR 0.000094 Loss 10.495886, Accuracy 59.580%\n",
      "Epoch 7, Batch 835, LR 0.000094 Loss 10.495640, Accuracy 59.580%\n",
      "Epoch 7, Batch 836, LR 0.000094 Loss 10.495767, Accuracy 59.582%\n",
      "Epoch 7, Batch 837, LR 0.000094 Loss 10.495081, Accuracy 59.590%\n",
      "Epoch 7, Batch 838, LR 0.000094 Loss 10.495572, Accuracy 59.587%\n",
      "Epoch 7, Batch 839, LR 0.000094 Loss 10.494065, Accuracy 59.597%\n",
      "Epoch 7, Batch 840, LR 0.000094 Loss 10.494054, Accuracy 59.596%\n",
      "Epoch 7, Batch 841, LR 0.000094 Loss 10.495043, Accuracy 59.590%\n",
      "Epoch 7, Batch 842, LR 0.000094 Loss 10.495336, Accuracy 59.588%\n",
      "Epoch 7, Batch 843, LR 0.000094 Loss 10.494789, Accuracy 59.588%\n",
      "Epoch 7, Batch 844, LR 0.000094 Loss 10.494745, Accuracy 59.583%\n",
      "Epoch 7, Batch 845, LR 0.000094 Loss 10.494513, Accuracy 59.592%\n",
      "Epoch 7, Batch 846, LR 0.000094 Loss 10.493093, Accuracy 59.601%\n",
      "Epoch 7, Batch 847, LR 0.000094 Loss 10.493216, Accuracy 59.604%\n",
      "Epoch 7, Batch 848, LR 0.000094 Loss 10.492922, Accuracy 59.610%\n",
      "Epoch 7, Batch 849, LR 0.000094 Loss 10.492494, Accuracy 59.612%\n",
      "Epoch 7, Batch 850, LR 0.000094 Loss 10.492105, Accuracy 59.620%\n",
      "Epoch 7, Batch 851, LR 0.000094 Loss 10.492316, Accuracy 59.618%\n",
      "Epoch 7, Batch 852, LR 0.000094 Loss 10.491220, Accuracy 59.629%\n",
      "Epoch 7, Batch 853, LR 0.000094 Loss 10.490769, Accuracy 59.633%\n",
      "Epoch 7, Batch 854, LR 0.000094 Loss 10.489665, Accuracy 59.634%\n",
      "Epoch 7, Batch 855, LR 0.000094 Loss 10.489377, Accuracy 59.637%\n",
      "Epoch 7, Batch 856, LR 0.000094 Loss 10.489354, Accuracy 59.638%\n",
      "Epoch 7, Batch 857, LR 0.000094 Loss 10.489185, Accuracy 59.642%\n",
      "Epoch 7, Batch 858, LR 0.000094 Loss 10.489655, Accuracy 59.637%\n",
      "Epoch 7, Batch 859, LR 0.000094 Loss 10.488153, Accuracy 59.644%\n",
      "Epoch 7, Batch 860, LR 0.000094 Loss 10.488027, Accuracy 59.648%\n",
      "Epoch 7, Batch 861, LR 0.000094 Loss 10.488023, Accuracy 59.649%\n",
      "Epoch 7, Batch 862, LR 0.000094 Loss 10.487581, Accuracy 59.648%\n",
      "Epoch 7, Batch 863, LR 0.000094 Loss 10.486332, Accuracy 59.657%\n",
      "Epoch 7, Batch 864, LR 0.000094 Loss 10.486536, Accuracy 59.653%\n",
      "Epoch 7, Batch 865, LR 0.000094 Loss 10.486251, Accuracy 59.653%\n",
      "Epoch 7, Batch 866, LR 0.000094 Loss 10.486977, Accuracy 59.653%\n",
      "Epoch 7, Batch 867, LR 0.000094 Loss 10.486618, Accuracy 59.662%\n",
      "Epoch 7, Batch 868, LR 0.000094 Loss 10.486088, Accuracy 59.663%\n",
      "Epoch 7, Batch 869, LR 0.000094 Loss 10.485570, Accuracy 59.665%\n",
      "Epoch 7, Batch 870, LR 0.000094 Loss 10.485592, Accuracy 59.667%\n",
      "Epoch 7, Batch 871, LR 0.000094 Loss 10.485578, Accuracy 59.667%\n",
      "Epoch 7, Batch 872, LR 0.000094 Loss 10.485435, Accuracy 59.667%\n",
      "Epoch 7, Batch 873, LR 0.000094 Loss 10.485680, Accuracy 59.670%\n",
      "Epoch 7, Batch 874, LR 0.000094 Loss 10.486291, Accuracy 59.662%\n",
      "Epoch 7, Batch 875, LR 0.000094 Loss 10.485891, Accuracy 59.665%\n",
      "Epoch 7, Batch 876, LR 0.000094 Loss 10.485489, Accuracy 59.666%\n",
      "Epoch 7, Batch 877, LR 0.000094 Loss 10.485071, Accuracy 59.670%\n",
      "Epoch 7, Batch 878, LR 0.000094 Loss 10.484633, Accuracy 59.671%\n",
      "Epoch 7, Batch 879, LR 0.000094 Loss 10.483737, Accuracy 59.677%\n",
      "Epoch 7, Batch 880, LR 0.000094 Loss 10.483273, Accuracy 59.680%\n",
      "Epoch 7, Batch 881, LR 0.000094 Loss 10.483188, Accuracy 59.681%\n",
      "Epoch 7, Batch 882, LR 0.000094 Loss 10.483115, Accuracy 59.683%\n",
      "Epoch 7, Batch 883, LR 0.000094 Loss 10.481978, Accuracy 59.688%\n",
      "Epoch 7, Batch 884, LR 0.000094 Loss 10.481151, Accuracy 59.690%\n",
      "Epoch 7, Batch 885, LR 0.000094 Loss 10.480916, Accuracy 59.699%\n",
      "Epoch 7, Batch 886, LR 0.000094 Loss 10.481149, Accuracy 59.697%\n",
      "Epoch 7, Batch 887, LR 0.000094 Loss 10.481261, Accuracy 59.698%\n",
      "Epoch 7, Batch 888, LR 0.000094 Loss 10.480641, Accuracy 59.700%\n",
      "Epoch 7, Batch 889, LR 0.000094 Loss 10.480589, Accuracy 59.700%\n",
      "Epoch 7, Batch 890, LR 0.000094 Loss 10.480627, Accuracy 59.698%\n",
      "Epoch 7, Batch 891, LR 0.000094 Loss 10.480155, Accuracy 59.698%\n",
      "Epoch 7, Batch 892, LR 0.000094 Loss 10.480985, Accuracy 59.686%\n",
      "Epoch 7, Batch 893, LR 0.000094 Loss 10.480742, Accuracy 59.683%\n",
      "Epoch 7, Batch 894, LR 0.000094 Loss 10.481365, Accuracy 59.680%\n",
      "Epoch 7, Batch 895, LR 0.000094 Loss 10.481027, Accuracy 59.682%\n",
      "Epoch 7, Batch 896, LR 0.000094 Loss 10.480795, Accuracy 59.681%\n",
      "Epoch 7, Batch 897, LR 0.000094 Loss 10.480730, Accuracy 59.684%\n",
      "Epoch 7, Batch 898, LR 0.000094 Loss 10.480879, Accuracy 59.679%\n",
      "Epoch 7, Batch 899, LR 0.000094 Loss 10.480148, Accuracy 59.677%\n",
      "Epoch 7, Batch 900, LR 0.000094 Loss 10.479090, Accuracy 59.681%\n",
      "Epoch 7, Batch 901, LR 0.000094 Loss 10.479946, Accuracy 59.675%\n",
      "Epoch 7, Batch 902, LR 0.000094 Loss 10.479383, Accuracy 59.680%\n",
      "Epoch 7, Batch 903, LR 0.000094 Loss 10.479469, Accuracy 59.676%\n",
      "Epoch 7, Batch 904, LR 0.000094 Loss 10.480186, Accuracy 59.671%\n",
      "Epoch 7, Batch 905, LR 0.000094 Loss 10.479969, Accuracy 59.672%\n",
      "Epoch 7, Batch 906, LR 0.000094 Loss 10.479684, Accuracy 59.678%\n",
      "Epoch 7, Batch 907, LR 0.000094 Loss 10.479049, Accuracy 59.678%\n",
      "Epoch 7, Batch 908, LR 0.000094 Loss 10.478692, Accuracy 59.682%\n",
      "Epoch 7, Batch 909, LR 0.000094 Loss 10.478524, Accuracy 59.679%\n",
      "Epoch 7, Batch 910, LR 0.000094 Loss 10.478678, Accuracy 59.680%\n",
      "Epoch 7, Batch 911, LR 0.000094 Loss 10.478607, Accuracy 59.674%\n",
      "Epoch 7, Batch 912, LR 0.000094 Loss 10.478394, Accuracy 59.680%\n",
      "Epoch 7, Batch 913, LR 0.000094 Loss 10.478257, Accuracy 59.678%\n",
      "Epoch 7, Batch 914, LR 0.000094 Loss 10.478444, Accuracy 59.678%\n",
      "Epoch 7, Batch 915, LR 0.000094 Loss 10.477787, Accuracy 59.677%\n",
      "Epoch 7, Batch 916, LR 0.000094 Loss 10.477923, Accuracy 59.676%\n",
      "Epoch 7, Batch 917, LR 0.000094 Loss 10.477451, Accuracy 59.680%\n",
      "Epoch 7, Batch 918, LR 0.000094 Loss 10.477442, Accuracy 59.681%\n",
      "Epoch 7, Batch 919, LR 0.000094 Loss 10.476804, Accuracy 59.682%\n",
      "Epoch 7, Batch 920, LR 0.000094 Loss 10.477513, Accuracy 59.682%\n",
      "Epoch 7, Batch 921, LR 0.000094 Loss 10.476721, Accuracy 59.686%\n",
      "Epoch 7, Batch 922, LR 0.000094 Loss 10.476269, Accuracy 59.687%\n",
      "Epoch 7, Batch 923, LR 0.000094 Loss 10.475429, Accuracy 59.694%\n",
      "Epoch 7, Batch 924, LR 0.000094 Loss 10.476031, Accuracy 59.688%\n",
      "Epoch 7, Batch 925, LR 0.000094 Loss 10.475956, Accuracy 59.691%\n",
      "Epoch 7, Batch 926, LR 0.000094 Loss 10.475454, Accuracy 59.697%\n",
      "Epoch 7, Batch 927, LR 0.000094 Loss 10.475703, Accuracy 59.694%\n",
      "Epoch 7, Batch 928, LR 0.000094 Loss 10.474589, Accuracy 59.699%\n",
      "Epoch 7, Batch 929, LR 0.000094 Loss 10.474077, Accuracy 59.701%\n",
      "Epoch 7, Batch 930, LR 0.000094 Loss 10.474014, Accuracy 59.704%\n",
      "Epoch 7, Batch 931, LR 0.000094 Loss 10.472958, Accuracy 59.707%\n",
      "Epoch 7, Batch 932, LR 0.000094 Loss 10.472433, Accuracy 59.711%\n",
      "Epoch 7, Batch 933, LR 0.000094 Loss 10.471168, Accuracy 59.720%\n",
      "Epoch 7, Batch 934, LR 0.000094 Loss 10.470718, Accuracy 59.719%\n",
      "Epoch 7, Batch 935, LR 0.000094 Loss 10.470145, Accuracy 59.722%\n",
      "Epoch 7, Batch 936, LR 0.000094 Loss 10.470629, Accuracy 59.718%\n",
      "Epoch 7, Batch 937, LR 0.000094 Loss 10.470161, Accuracy 59.723%\n",
      "Epoch 7, Batch 938, LR 0.000094 Loss 10.470315, Accuracy 59.726%\n",
      "Epoch 7, Batch 939, LR 0.000094 Loss 10.469217, Accuracy 59.733%\n",
      "Epoch 7, Batch 940, LR 0.000094 Loss 10.469510, Accuracy 59.732%\n",
      "Epoch 7, Batch 941, LR 0.000094 Loss 10.469886, Accuracy 59.730%\n",
      "Epoch 7, Batch 942, LR 0.000094 Loss 10.469539, Accuracy 59.728%\n",
      "Epoch 7, Batch 943, LR 0.000094 Loss 10.469152, Accuracy 59.725%\n",
      "Epoch 7, Batch 944, LR 0.000094 Loss 10.468625, Accuracy 59.723%\n",
      "Epoch 7, Batch 945, LR 0.000094 Loss 10.468304, Accuracy 59.726%\n",
      "Epoch 7, Batch 946, LR 0.000094 Loss 10.468057, Accuracy 59.724%\n",
      "Epoch 7, Batch 947, LR 0.000094 Loss 10.467633, Accuracy 59.722%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Batch 948, LR 0.000094 Loss 10.467779, Accuracy 59.718%\n",
      "Epoch 7, Batch 949, LR 0.000094 Loss 10.467268, Accuracy 59.722%\n",
      "Epoch 7, Batch 950, LR 0.000094 Loss 10.467479, Accuracy 59.720%\n",
      "Epoch 7, Batch 951, LR 0.000094 Loss 10.466706, Accuracy 59.727%\n",
      "Epoch 7, Batch 952, LR 0.000094 Loss 10.466671, Accuracy 59.722%\n",
      "Epoch 7, Batch 953, LR 0.000094 Loss 10.466785, Accuracy 59.722%\n",
      "Epoch 7, Batch 954, LR 0.000094 Loss 10.466144, Accuracy 59.725%\n",
      "Epoch 7, Batch 955, LR 0.000094 Loss 10.465577, Accuracy 59.731%\n",
      "Epoch 7, Batch 956, LR 0.000094 Loss 10.464983, Accuracy 59.733%\n",
      "Epoch 7, Batch 957, LR 0.000094 Loss 10.464371, Accuracy 59.737%\n",
      "Epoch 7, Batch 958, LR 0.000094 Loss 10.464462, Accuracy 59.737%\n",
      "Epoch 7, Batch 959, LR 0.000094 Loss 10.464443, Accuracy 59.737%\n",
      "Epoch 7, Batch 960, LR 0.000094 Loss 10.463747, Accuracy 59.741%\n",
      "Epoch 7, Batch 961, LR 0.000094 Loss 10.464412, Accuracy 59.739%\n",
      "Epoch 7, Batch 962, LR 0.000094 Loss 10.464280, Accuracy 59.743%\n",
      "Epoch 7, Batch 963, LR 0.000094 Loss 10.464134, Accuracy 59.747%\n",
      "Epoch 7, Batch 964, LR 0.000094 Loss 10.462859, Accuracy 59.761%\n",
      "Epoch 7, Batch 965, LR 0.000094 Loss 10.462803, Accuracy 59.760%\n",
      "Epoch 7, Batch 966, LR 0.000094 Loss 10.461750, Accuracy 59.769%\n",
      "Epoch 7, Batch 967, LR 0.000094 Loss 10.461807, Accuracy 59.767%\n",
      "Epoch 7, Batch 968, LR 0.000094 Loss 10.461642, Accuracy 59.766%\n",
      "Epoch 7, Batch 969, LR 0.000094 Loss 10.460999, Accuracy 59.768%\n",
      "Epoch 7, Batch 970, LR 0.000094 Loss 10.460980, Accuracy 59.766%\n",
      "Epoch 7, Batch 971, LR 0.000094 Loss 10.460801, Accuracy 59.770%\n",
      "Epoch 7, Batch 972, LR 0.000094 Loss 10.460418, Accuracy 59.770%\n",
      "Epoch 7, Batch 973, LR 0.000094 Loss 10.459832, Accuracy 59.776%\n",
      "Epoch 7, Batch 974, LR 0.000094 Loss 10.460035, Accuracy 59.774%\n",
      "Epoch 7, Batch 975, LR 0.000094 Loss 10.458749, Accuracy 59.786%\n",
      "Epoch 7, Batch 976, LR 0.000094 Loss 10.458755, Accuracy 59.788%\n",
      "Epoch 7, Batch 977, LR 0.000094 Loss 10.458296, Accuracy 59.788%\n",
      "Epoch 7, Batch 978, LR 0.000094 Loss 10.458549, Accuracy 59.786%\n",
      "Epoch 7, Batch 979, LR 0.000094 Loss 10.458320, Accuracy 59.789%\n",
      "Epoch 7, Batch 980, LR 0.000094 Loss 10.458896, Accuracy 59.786%\n",
      "Epoch 7, Batch 981, LR 0.000094 Loss 10.458229, Accuracy 59.792%\n",
      "Epoch 7, Batch 982, LR 0.000094 Loss 10.457417, Accuracy 59.795%\n",
      "Epoch 7, Batch 983, LR 0.000094 Loss 10.457518, Accuracy 59.795%\n",
      "Epoch 7, Batch 984, LR 0.000094 Loss 10.456614, Accuracy 59.799%\n",
      "Epoch 7, Batch 985, LR 0.000094 Loss 10.456068, Accuracy 59.804%\n",
      "Epoch 7, Batch 986, LR 0.000094 Loss 10.456406, Accuracy 59.804%\n",
      "Epoch 7, Batch 987, LR 0.000094 Loss 10.456380, Accuracy 59.803%\n",
      "Epoch 7, Batch 988, LR 0.000094 Loss 10.455793, Accuracy 59.811%\n",
      "Epoch 7, Batch 989, LR 0.000094 Loss 10.454773, Accuracy 59.820%\n",
      "Epoch 7, Batch 990, LR 0.000094 Loss 10.455204, Accuracy 59.820%\n",
      "Epoch 7, Batch 991, LR 0.000094 Loss 10.454490, Accuracy 59.825%\n",
      "Epoch 7, Batch 992, LR 0.000094 Loss 10.453906, Accuracy 59.829%\n",
      "Epoch 7, Batch 993, LR 0.000094 Loss 10.453462, Accuracy 59.830%\n",
      "Epoch 7, Batch 994, LR 0.000094 Loss 10.453477, Accuracy 59.828%\n",
      "Epoch 7, Batch 995, LR 0.000094 Loss 10.452922, Accuracy 59.836%\n",
      "Epoch 7, Batch 996, LR 0.000094 Loss 10.453392, Accuracy 59.836%\n",
      "Epoch 7, Batch 997, LR 0.000094 Loss 10.453278, Accuracy 59.837%\n",
      "Epoch 7, Batch 998, LR 0.000094 Loss 10.452195, Accuracy 59.844%\n",
      "Epoch 7, Batch 999, LR 0.000094 Loss 10.451950, Accuracy 59.848%\n",
      "Epoch 7, Batch 1000, LR 0.000094 Loss 10.451818, Accuracy 59.849%\n",
      "Epoch 7, Batch 1001, LR 0.000094 Loss 10.451482, Accuracy 59.854%\n",
      "Epoch 7, Batch 1002, LR 0.000094 Loss 10.451749, Accuracy 59.852%\n",
      "Epoch 7, Batch 1003, LR 0.000094 Loss 10.451565, Accuracy 59.852%\n",
      "Epoch 7, Batch 1004, LR 0.000094 Loss 10.451445, Accuracy 59.852%\n",
      "Epoch 7, Batch 1005, LR 0.000094 Loss 10.452217, Accuracy 59.848%\n",
      "Epoch 7, Batch 1006, LR 0.000094 Loss 10.452216, Accuracy 59.849%\n",
      "Epoch 7, Batch 1007, LR 0.000094 Loss 10.452157, Accuracy 59.847%\n",
      "Epoch 7, Batch 1008, LR 0.000094 Loss 10.452451, Accuracy 59.847%\n",
      "Epoch 7, Batch 1009, LR 0.000094 Loss 10.452210, Accuracy 59.843%\n",
      "Epoch 7, Batch 1010, LR 0.000094 Loss 10.452008, Accuracy 59.842%\n",
      "Epoch 7, Batch 1011, LR 0.000094 Loss 10.452396, Accuracy 59.839%\n",
      "Epoch 7, Batch 1012, LR 0.000094 Loss 10.452301, Accuracy 59.839%\n",
      "Epoch 7, Batch 1013, LR 0.000094 Loss 10.451652, Accuracy 59.843%\n",
      "Epoch 7, Batch 1014, LR 0.000094 Loss 10.451174, Accuracy 59.849%\n",
      "Epoch 7, Batch 1015, LR 0.000094 Loss 10.451333, Accuracy 59.848%\n",
      "Epoch 7, Batch 1016, LR 0.000094 Loss 10.450913, Accuracy 59.853%\n",
      "Epoch 7, Batch 1017, LR 0.000094 Loss 10.450950, Accuracy 59.851%\n",
      "Epoch 7, Batch 1018, LR 0.000094 Loss 10.450305, Accuracy 59.858%\n",
      "Epoch 7, Batch 1019, LR 0.000094 Loss 10.450403, Accuracy 59.858%\n",
      "Epoch 7, Batch 1020, LR 0.000094 Loss 10.451602, Accuracy 59.851%\n",
      "Epoch 7, Batch 1021, LR 0.000094 Loss 10.451024, Accuracy 59.857%\n",
      "Epoch 7, Batch 1022, LR 0.000094 Loss 10.450542, Accuracy 59.859%\n",
      "Epoch 7, Batch 1023, LR 0.000094 Loss 10.450143, Accuracy 59.865%\n",
      "Epoch 7, Batch 1024, LR 0.000094 Loss 10.449672, Accuracy 59.870%\n",
      "Epoch 7, Batch 1025, LR 0.000094 Loss 10.449868, Accuracy 59.871%\n",
      "Epoch 7, Batch 1026, LR 0.000094 Loss 10.449456, Accuracy 59.873%\n",
      "Epoch 7, Batch 1027, LR 0.000094 Loss 10.448684, Accuracy 59.879%\n",
      "Epoch 7, Batch 1028, LR 0.000094 Loss 10.449322, Accuracy 59.874%\n",
      "Epoch 7, Batch 1029, LR 0.000094 Loss 10.448554, Accuracy 59.875%\n",
      "Epoch 7, Batch 1030, LR 0.000094 Loss 10.447812, Accuracy 59.883%\n",
      "Epoch 7, Batch 1031, LR 0.000094 Loss 10.447285, Accuracy 59.884%\n",
      "Epoch 7, Batch 1032, LR 0.000094 Loss 10.447754, Accuracy 59.882%\n",
      "Epoch 7, Batch 1033, LR 0.000094 Loss 10.447833, Accuracy 59.878%\n",
      "Epoch 7, Batch 1034, LR 0.000094 Loss 10.447597, Accuracy 59.877%\n",
      "Epoch 7, Batch 1035, LR 0.000094 Loss 10.447460, Accuracy 59.882%\n",
      "Epoch 7, Batch 1036, LR 0.000094 Loss 10.446814, Accuracy 59.883%\n",
      "Epoch 7, Batch 1037, LR 0.000094 Loss 10.447178, Accuracy 59.881%\n",
      "Epoch 7, Batch 1038, LR 0.000094 Loss 10.446288, Accuracy 59.885%\n",
      "Epoch 7, Batch 1039, LR 0.000094 Loss 10.445405, Accuracy 59.892%\n",
      "Epoch 7, Batch 1040, LR 0.000094 Loss 10.444043, Accuracy 59.902%\n",
      "Epoch 7, Batch 1041, LR 0.000094 Loss 10.443748, Accuracy 59.902%\n",
      "Epoch 7, Batch 1042, LR 0.000094 Loss 10.442561, Accuracy 59.909%\n",
      "Epoch 7, Batch 1043, LR 0.000094 Loss 10.441866, Accuracy 59.919%\n",
      "Epoch 7, Batch 1044, LR 0.000094 Loss 10.441846, Accuracy 59.917%\n",
      "Epoch 7, Batch 1045, LR 0.000094 Loss 10.440819, Accuracy 59.923%\n",
      "Epoch 7, Batch 1046, LR 0.000094 Loss 10.440802, Accuracy 59.917%\n",
      "Epoch 7, Batch 1047, LR 0.000094 Loss 10.440466, Accuracy 59.918%\n",
      "Epoch 7, Loss (train set) 10.440466, Accuracy (train set) 59.918%\n",
      "Epoch 8, Batch 1, LR 0.000094 Loss 10.165757, Accuracy 62.500%\n",
      "Epoch 8, Batch 2, LR 0.000094 Loss 9.622604, Accuracy 67.969%\n",
      "Epoch 8, Batch 3, LR 0.000094 Loss 9.810108, Accuracy 65.885%\n",
      "Epoch 8, Batch 4, LR 0.000094 Loss 9.825810, Accuracy 67.188%\n",
      "Epoch 8, Batch 5, LR 0.000094 Loss 9.923188, Accuracy 65.469%\n",
      "Epoch 8, Batch 6, LR 0.000094 Loss 9.970434, Accuracy 64.714%\n",
      "Epoch 8, Batch 7, LR 0.000094 Loss 9.862922, Accuracy 65.067%\n",
      "Epoch 8, Batch 8, LR 0.000094 Loss 9.884770, Accuracy 64.941%\n",
      "Epoch 8, Batch 9, LR 0.000094 Loss 9.865185, Accuracy 65.451%\n",
      "Epoch 8, Batch 10, LR 0.000094 Loss 9.875982, Accuracy 65.234%\n",
      "Epoch 8, Batch 11, LR 0.000094 Loss 9.904270, Accuracy 64.773%\n",
      "Epoch 8, Batch 12, LR 0.000094 Loss 9.954238, Accuracy 64.583%\n",
      "Epoch 8, Batch 13, LR 0.000094 Loss 9.918451, Accuracy 64.964%\n",
      "Epoch 8, Batch 14, LR 0.000094 Loss 9.883685, Accuracy 65.290%\n",
      "Epoch 8, Batch 15, LR 0.000094 Loss 9.932893, Accuracy 64.583%\n",
      "Epoch 8, Batch 16, LR 0.000094 Loss 9.972585, Accuracy 64.111%\n",
      "Epoch 8, Batch 17, LR 0.000094 Loss 9.953152, Accuracy 64.522%\n",
      "Epoch 8, Batch 18, LR 0.000094 Loss 9.958974, Accuracy 64.280%\n",
      "Epoch 8, Batch 19, LR 0.000094 Loss 10.004731, Accuracy 64.104%\n",
      "Epoch 8, Batch 20, LR 0.000094 Loss 9.995321, Accuracy 64.258%\n",
      "Epoch 8, Batch 21, LR 0.000094 Loss 10.026868, Accuracy 63.802%\n",
      "Epoch 8, Batch 22, LR 0.000094 Loss 10.008034, Accuracy 63.920%\n",
      "Epoch 8, Batch 23, LR 0.000094 Loss 10.021078, Accuracy 63.791%\n",
      "Epoch 8, Batch 24, LR 0.000094 Loss 10.050902, Accuracy 63.574%\n",
      "Epoch 8, Batch 25, LR 0.000094 Loss 10.088035, Accuracy 63.219%\n",
      "Epoch 8, Batch 26, LR 0.000094 Loss 10.081840, Accuracy 63.401%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 27, LR 0.000094 Loss 10.080412, Accuracy 63.223%\n",
      "Epoch 8, Batch 28, LR 0.000094 Loss 10.085611, Accuracy 63.142%\n",
      "Epoch 8, Batch 29, LR 0.000094 Loss 10.078210, Accuracy 63.308%\n",
      "Epoch 8, Batch 30, LR 0.000094 Loss 10.079304, Accuracy 63.229%\n",
      "Epoch 8, Batch 31, LR 0.000094 Loss 10.074307, Accuracy 63.054%\n",
      "Epoch 8, Batch 32, LR 0.000094 Loss 10.070931, Accuracy 62.915%\n",
      "Epoch 8, Batch 33, LR 0.000094 Loss 10.068558, Accuracy 62.879%\n",
      "Epoch 8, Batch 34, LR 0.000094 Loss 10.049683, Accuracy 63.028%\n",
      "Epoch 8, Batch 35, LR 0.000094 Loss 10.067446, Accuracy 62.902%\n",
      "Epoch 8, Batch 36, LR 0.000094 Loss 10.063545, Accuracy 62.804%\n",
      "Epoch 8, Batch 37, LR 0.000094 Loss 10.043169, Accuracy 62.965%\n",
      "Epoch 8, Batch 38, LR 0.000094 Loss 10.012820, Accuracy 62.993%\n",
      "Epoch 8, Batch 39, LR 0.000094 Loss 10.028758, Accuracy 62.921%\n",
      "Epoch 8, Batch 40, LR 0.000094 Loss 10.038095, Accuracy 62.754%\n",
      "Epoch 8, Batch 41, LR 0.000094 Loss 10.010004, Accuracy 62.938%\n",
      "Epoch 8, Batch 42, LR 0.000094 Loss 10.016941, Accuracy 62.816%\n",
      "Epoch 8, Batch 43, LR 0.000094 Loss 10.031723, Accuracy 62.627%\n",
      "Epoch 8, Batch 44, LR 0.000094 Loss 10.016610, Accuracy 62.731%\n",
      "Epoch 8, Batch 45, LR 0.000094 Loss 10.009559, Accuracy 62.812%\n",
      "Epoch 8, Batch 46, LR 0.000094 Loss 10.021433, Accuracy 62.704%\n",
      "Epoch 8, Batch 47, LR 0.000094 Loss 10.024773, Accuracy 62.716%\n",
      "Epoch 8, Batch 48, LR 0.000094 Loss 10.023060, Accuracy 62.679%\n",
      "Epoch 8, Batch 49, LR 0.000094 Loss 10.034995, Accuracy 62.580%\n",
      "Epoch 8, Batch 50, LR 0.000094 Loss 10.043974, Accuracy 62.547%\n",
      "Epoch 8, Batch 51, LR 0.000094 Loss 10.038266, Accuracy 62.561%\n",
      "Epoch 8, Batch 52, LR 0.000094 Loss 10.047125, Accuracy 62.530%\n",
      "Epoch 8, Batch 53, LR 0.000094 Loss 10.027815, Accuracy 62.633%\n",
      "Epoch 8, Batch 54, LR 0.000094 Loss 10.031311, Accuracy 62.616%\n",
      "Epoch 8, Batch 55, LR 0.000094 Loss 10.020441, Accuracy 62.727%\n",
      "Epoch 8, Batch 56, LR 0.000094 Loss 10.034261, Accuracy 62.612%\n",
      "Epoch 8, Batch 57, LR 0.000094 Loss 10.033058, Accuracy 62.596%\n",
      "Epoch 8, Batch 58, LR 0.000094 Loss 10.028965, Accuracy 62.608%\n",
      "Epoch 8, Batch 59, LR 0.000094 Loss 10.027353, Accuracy 62.579%\n",
      "Epoch 8, Batch 60, LR 0.000094 Loss 10.040225, Accuracy 62.474%\n",
      "Epoch 8, Batch 61, LR 0.000094 Loss 10.040274, Accuracy 62.449%\n",
      "Epoch 8, Batch 62, LR 0.000094 Loss 10.044090, Accuracy 62.450%\n",
      "Epoch 8, Batch 63, LR 0.000094 Loss 10.042614, Accuracy 62.351%\n",
      "Epoch 8, Batch 64, LR 0.000094 Loss 10.027335, Accuracy 62.451%\n",
      "Epoch 8, Batch 65, LR 0.000094 Loss 10.026297, Accuracy 62.476%\n",
      "Epoch 8, Batch 66, LR 0.000094 Loss 10.030613, Accuracy 62.512%\n",
      "Epoch 8, Batch 67, LR 0.000094 Loss 10.029587, Accuracy 62.512%\n",
      "Epoch 8, Batch 68, LR 0.000094 Loss 10.026036, Accuracy 62.546%\n",
      "Epoch 8, Batch 69, LR 0.000094 Loss 10.030738, Accuracy 62.534%\n",
      "Epoch 8, Batch 70, LR 0.000094 Loss 10.035558, Accuracy 62.522%\n",
      "Epoch 8, Batch 71, LR 0.000094 Loss 10.029962, Accuracy 62.511%\n",
      "Epoch 8, Batch 72, LR 0.000094 Loss 10.021620, Accuracy 62.587%\n",
      "Epoch 8, Batch 73, LR 0.000094 Loss 10.010332, Accuracy 62.703%\n",
      "Epoch 8, Batch 74, LR 0.000094 Loss 10.013060, Accuracy 62.658%\n",
      "Epoch 8, Batch 75, LR 0.000094 Loss 10.016951, Accuracy 62.573%\n",
      "Epoch 8, Batch 76, LR 0.000094 Loss 10.016732, Accuracy 62.521%\n",
      "Epoch 8, Batch 77, LR 0.000094 Loss 10.017224, Accuracy 62.520%\n",
      "Epoch 8, Batch 78, LR 0.000094 Loss 10.021034, Accuracy 62.520%\n",
      "Epoch 8, Batch 79, LR 0.000094 Loss 10.019737, Accuracy 62.549%\n",
      "Epoch 8, Batch 80, LR 0.000094 Loss 10.025983, Accuracy 62.520%\n",
      "Epoch 8, Batch 81, LR 0.000094 Loss 10.034727, Accuracy 62.432%\n",
      "Epoch 8, Batch 82, LR 0.000094 Loss 10.031117, Accuracy 62.433%\n",
      "Epoch 8, Batch 83, LR 0.000094 Loss 10.033953, Accuracy 62.406%\n",
      "Epoch 8, Batch 84, LR 0.000094 Loss 10.019352, Accuracy 62.444%\n",
      "Epoch 8, Batch 85, LR 0.000094 Loss 10.021895, Accuracy 62.436%\n",
      "Epoch 8, Batch 86, LR 0.000094 Loss 10.021370, Accuracy 62.491%\n",
      "Epoch 8, Batch 87, LR 0.000094 Loss 10.024401, Accuracy 62.500%\n",
      "Epoch 8, Batch 88, LR 0.000094 Loss 10.020934, Accuracy 62.473%\n",
      "Epoch 8, Batch 89, LR 0.000094 Loss 10.024969, Accuracy 62.456%\n",
      "Epoch 8, Batch 90, LR 0.000094 Loss 10.030459, Accuracy 62.457%\n",
      "Epoch 8, Batch 91, LR 0.000094 Loss 10.031827, Accuracy 62.397%\n",
      "Epoch 8, Batch 92, LR 0.000094 Loss 10.027438, Accuracy 62.398%\n",
      "Epoch 8, Batch 93, LR 0.000094 Loss 10.031847, Accuracy 62.332%\n",
      "Epoch 8, Batch 94, LR 0.000094 Loss 10.032856, Accuracy 62.317%\n",
      "Epoch 8, Batch 95, LR 0.000094 Loss 10.044124, Accuracy 62.229%\n",
      "Epoch 8, Batch 96, LR 0.000094 Loss 10.044330, Accuracy 62.215%\n",
      "Epoch 8, Batch 97, LR 0.000094 Loss 10.044167, Accuracy 62.258%\n",
      "Epoch 8, Batch 98, LR 0.000094 Loss 10.042129, Accuracy 62.277%\n",
      "Epoch 8, Batch 99, LR 0.000094 Loss 10.036881, Accuracy 62.318%\n",
      "Epoch 8, Batch 100, LR 0.000094 Loss 10.041725, Accuracy 62.289%\n",
      "Epoch 8, Batch 101, LR 0.000094 Loss 10.033790, Accuracy 62.345%\n",
      "Epoch 8, Batch 102, LR 0.000094 Loss 10.027297, Accuracy 62.377%\n",
      "Epoch 8, Batch 103, LR 0.000094 Loss 10.026150, Accuracy 62.386%\n",
      "Epoch 8, Batch 104, LR 0.000094 Loss 10.028899, Accuracy 62.335%\n",
      "Epoch 8, Batch 105, LR 0.000094 Loss 10.029321, Accuracy 62.403%\n",
      "Epoch 8, Batch 106, LR 0.000094 Loss 10.023428, Accuracy 62.434%\n",
      "Epoch 8, Batch 107, LR 0.000094 Loss 10.027503, Accuracy 62.463%\n",
      "Epoch 8, Batch 108, LR 0.000094 Loss 10.034270, Accuracy 62.377%\n",
      "Epoch 8, Batch 109, LR 0.000094 Loss 10.026736, Accuracy 62.421%\n",
      "Epoch 8, Batch 110, LR 0.000094 Loss 10.019184, Accuracy 62.514%\n",
      "Epoch 8, Batch 111, LR 0.000094 Loss 10.024286, Accuracy 62.514%\n",
      "Epoch 8, Batch 112, LR 0.000094 Loss 10.028138, Accuracy 62.479%\n",
      "Epoch 8, Batch 113, LR 0.000094 Loss 10.030117, Accuracy 62.459%\n",
      "Epoch 8, Batch 114, LR 0.000094 Loss 10.032084, Accuracy 62.418%\n",
      "Epoch 8, Batch 115, LR 0.000094 Loss 10.029486, Accuracy 62.452%\n",
      "Epoch 8, Batch 116, LR 0.000094 Loss 10.030115, Accuracy 62.460%\n",
      "Epoch 8, Batch 117, LR 0.000094 Loss 10.022634, Accuracy 62.500%\n",
      "Epoch 8, Batch 118, LR 0.000094 Loss 10.027105, Accuracy 62.454%\n",
      "Epoch 8, Batch 119, LR 0.000094 Loss 10.021716, Accuracy 62.480%\n",
      "Epoch 8, Batch 120, LR 0.000094 Loss 10.022043, Accuracy 62.493%\n",
      "Epoch 8, Batch 121, LR 0.000094 Loss 10.024378, Accuracy 62.481%\n",
      "Epoch 8, Batch 122, LR 0.000094 Loss 10.020192, Accuracy 62.526%\n",
      "Epoch 8, Batch 123, LR 0.000094 Loss 10.022045, Accuracy 62.544%\n",
      "Epoch 8, Batch 124, LR 0.000094 Loss 10.024946, Accuracy 62.500%\n",
      "Epoch 8, Batch 125, LR 0.000094 Loss 10.027222, Accuracy 62.500%\n",
      "Epoch 8, Batch 126, LR 0.000094 Loss 10.029348, Accuracy 62.494%\n",
      "Epoch 8, Batch 127, LR 0.000094 Loss 10.036766, Accuracy 62.475%\n",
      "Epoch 8, Batch 128, LR 0.000094 Loss 10.036023, Accuracy 62.512%\n",
      "Epoch 8, Batch 129, LR 0.000094 Loss 10.042569, Accuracy 62.464%\n",
      "Epoch 8, Batch 130, LR 0.000094 Loss 10.041872, Accuracy 62.458%\n",
      "Epoch 8, Batch 131, LR 0.000094 Loss 10.042323, Accuracy 62.422%\n",
      "Epoch 8, Batch 132, LR 0.000094 Loss 10.043981, Accuracy 62.423%\n",
      "Epoch 8, Batch 133, LR 0.000094 Loss 10.046688, Accuracy 62.388%\n",
      "Epoch 8, Batch 134, LR 0.000094 Loss 10.046935, Accuracy 62.407%\n",
      "Epoch 8, Batch 135, LR 0.000094 Loss 10.041537, Accuracy 62.459%\n",
      "Epoch 8, Batch 136, LR 0.000094 Loss 10.033395, Accuracy 62.523%\n",
      "Epoch 8, Batch 137, LR 0.000094 Loss 10.033653, Accuracy 62.500%\n",
      "Epoch 8, Batch 138, LR 0.000094 Loss 10.031643, Accuracy 62.506%\n",
      "Epoch 8, Batch 139, LR 0.000094 Loss 10.026538, Accuracy 62.590%\n",
      "Epoch 8, Batch 140, LR 0.000094 Loss 10.026113, Accuracy 62.617%\n",
      "Epoch 8, Batch 141, LR 0.000094 Loss 10.026649, Accuracy 62.616%\n",
      "Epoch 8, Batch 142, LR 0.000094 Loss 10.030893, Accuracy 62.610%\n",
      "Epoch 8, Batch 143, LR 0.000094 Loss 10.028026, Accuracy 62.626%\n",
      "Epoch 8, Batch 144, LR 0.000094 Loss 10.029434, Accuracy 62.609%\n",
      "Epoch 8, Batch 145, LR 0.000094 Loss 10.024876, Accuracy 62.672%\n",
      "Epoch 8, Batch 146, LR 0.000094 Loss 10.024504, Accuracy 62.666%\n",
      "Epoch 8, Batch 147, LR 0.000094 Loss 10.030498, Accuracy 62.633%\n",
      "Epoch 8, Batch 148, LR 0.000094 Loss 10.028327, Accuracy 62.664%\n",
      "Epoch 8, Batch 149, LR 0.000094 Loss 10.026311, Accuracy 62.642%\n",
      "Epoch 8, Batch 150, LR 0.000094 Loss 10.020055, Accuracy 62.703%\n",
      "Epoch 8, Batch 151, LR 0.000094 Loss 10.023872, Accuracy 62.666%\n",
      "Epoch 8, Batch 152, LR 0.000094 Loss 10.020427, Accuracy 62.711%\n",
      "Epoch 8, Batch 153, LR 0.000094 Loss 10.020543, Accuracy 62.720%\n",
      "Epoch 8, Batch 154, LR 0.000094 Loss 10.020105, Accuracy 62.738%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 155, LR 0.000094 Loss 10.028924, Accuracy 62.702%\n",
      "Epoch 8, Batch 156, LR 0.000094 Loss 10.030428, Accuracy 62.675%\n",
      "Epoch 8, Batch 157, LR 0.000094 Loss 10.027776, Accuracy 62.699%\n",
      "Epoch 8, Batch 158, LR 0.000094 Loss 10.025555, Accuracy 62.732%\n",
      "Epoch 8, Batch 159, LR 0.000094 Loss 10.026924, Accuracy 62.687%\n",
      "Epoch 8, Batch 160, LR 0.000094 Loss 10.028359, Accuracy 62.690%\n",
      "Epoch 8, Batch 161, LR 0.000094 Loss 10.030981, Accuracy 62.665%\n",
      "Epoch 8, Batch 162, LR 0.000094 Loss 10.030859, Accuracy 62.659%\n",
      "Epoch 8, Batch 163, LR 0.000094 Loss 10.029331, Accuracy 62.653%\n",
      "Epoch 8, Batch 164, LR 0.000094 Loss 10.025972, Accuracy 62.681%\n",
      "Epoch 8, Batch 165, LR 0.000094 Loss 10.021574, Accuracy 62.689%\n",
      "Epoch 8, Batch 166, LR 0.000094 Loss 10.017634, Accuracy 62.702%\n",
      "Epoch 8, Batch 167, LR 0.000094 Loss 10.016710, Accuracy 62.748%\n",
      "Epoch 8, Batch 168, LR 0.000094 Loss 10.018505, Accuracy 62.737%\n",
      "Epoch 8, Batch 169, LR 0.000094 Loss 10.022614, Accuracy 62.727%\n",
      "Epoch 8, Batch 170, LR 0.000094 Loss 10.025657, Accuracy 62.702%\n",
      "Epoch 8, Batch 171, LR 0.000094 Loss 10.027596, Accuracy 62.728%\n",
      "Epoch 8, Batch 172, LR 0.000094 Loss 10.025755, Accuracy 62.768%\n",
      "Epoch 8, Batch 173, LR 0.000094 Loss 10.028573, Accuracy 62.726%\n",
      "Epoch 8, Batch 174, LR 0.000094 Loss 10.025338, Accuracy 62.747%\n",
      "Epoch 8, Batch 175, LR 0.000094 Loss 10.020579, Accuracy 62.786%\n",
      "Epoch 8, Batch 176, LR 0.000094 Loss 10.019593, Accuracy 62.793%\n",
      "Epoch 8, Batch 177, LR 0.000094 Loss 10.019335, Accuracy 62.813%\n",
      "Epoch 8, Batch 178, LR 0.000094 Loss 10.016849, Accuracy 62.798%\n",
      "Epoch 8, Batch 179, LR 0.000094 Loss 10.016145, Accuracy 62.827%\n",
      "Epoch 8, Batch 180, LR 0.000094 Loss 10.021453, Accuracy 62.821%\n",
      "Epoch 8, Batch 181, LR 0.000094 Loss 10.019679, Accuracy 62.811%\n",
      "Epoch 8, Batch 182, LR 0.000094 Loss 10.019187, Accuracy 62.848%\n",
      "Epoch 8, Batch 183, LR 0.000094 Loss 10.020161, Accuracy 62.846%\n",
      "Epoch 8, Batch 184, LR 0.000094 Loss 10.020408, Accuracy 62.852%\n",
      "Epoch 8, Batch 185, LR 0.000094 Loss 10.014666, Accuracy 62.897%\n",
      "Epoch 8, Batch 186, LR 0.000094 Loss 10.015441, Accuracy 62.865%\n",
      "Epoch 8, Batch 187, LR 0.000094 Loss 10.014388, Accuracy 62.872%\n",
      "Epoch 8, Batch 188, LR 0.000094 Loss 10.008291, Accuracy 62.907%\n",
      "Epoch 8, Batch 189, LR 0.000094 Loss 10.011435, Accuracy 62.872%\n",
      "Epoch 8, Batch 190, LR 0.000094 Loss 10.013334, Accuracy 62.850%\n",
      "Epoch 8, Batch 191, LR 0.000094 Loss 10.008804, Accuracy 62.860%\n",
      "Epoch 8, Batch 192, LR 0.000094 Loss 10.007522, Accuracy 62.850%\n",
      "Epoch 8, Batch 193, LR 0.000094 Loss 10.006701, Accuracy 62.856%\n",
      "Epoch 8, Batch 194, LR 0.000094 Loss 10.006182, Accuracy 62.883%\n",
      "Epoch 8, Batch 195, LR 0.000094 Loss 10.008923, Accuracy 62.865%\n",
      "Epoch 8, Batch 196, LR 0.000094 Loss 10.011539, Accuracy 62.835%\n",
      "Epoch 8, Batch 197, LR 0.000094 Loss 10.016140, Accuracy 62.797%\n",
      "Epoch 8, Batch 198, LR 0.000094 Loss 10.014842, Accuracy 62.760%\n",
      "Epoch 8, Batch 199, LR 0.000094 Loss 10.013719, Accuracy 62.783%\n",
      "Epoch 8, Batch 200, LR 0.000094 Loss 10.016227, Accuracy 62.770%\n",
      "Epoch 8, Batch 201, LR 0.000094 Loss 10.016770, Accuracy 62.749%\n",
      "Epoch 8, Batch 202, LR 0.000094 Loss 10.015352, Accuracy 62.778%\n",
      "Epoch 8, Batch 203, LR 0.000094 Loss 10.011727, Accuracy 62.819%\n",
      "Epoch 8, Batch 204, LR 0.000094 Loss 10.007377, Accuracy 62.848%\n",
      "Epoch 8, Batch 205, LR 0.000094 Loss 10.009838, Accuracy 62.828%\n",
      "Epoch 8, Batch 206, LR 0.000094 Loss 10.008056, Accuracy 62.830%\n",
      "Epoch 8, Batch 207, LR 0.000094 Loss 10.008130, Accuracy 62.809%\n",
      "Epoch 8, Batch 208, LR 0.000094 Loss 10.008060, Accuracy 62.819%\n",
      "Epoch 8, Batch 209, LR 0.000094 Loss 10.005998, Accuracy 62.844%\n",
      "Epoch 8, Batch 210, LR 0.000094 Loss 10.005794, Accuracy 62.835%\n",
      "Epoch 8, Batch 211, LR 0.000094 Loss 10.007898, Accuracy 62.815%\n",
      "Epoch 8, Batch 212, LR 0.000094 Loss 10.009365, Accuracy 62.824%\n",
      "Epoch 8, Batch 213, LR 0.000094 Loss 10.016031, Accuracy 62.790%\n",
      "Epoch 8, Batch 214, LR 0.000094 Loss 10.015853, Accuracy 62.770%\n",
      "Epoch 8, Batch 215, LR 0.000094 Loss 10.018553, Accuracy 62.762%\n",
      "Epoch 8, Batch 216, LR 0.000094 Loss 10.019886, Accuracy 62.760%\n",
      "Epoch 8, Batch 217, LR 0.000094 Loss 10.019255, Accuracy 62.774%\n",
      "Epoch 8, Batch 218, LR 0.000094 Loss 10.018243, Accuracy 62.783%\n",
      "Epoch 8, Batch 219, LR 0.000094 Loss 10.019122, Accuracy 62.789%\n",
      "Epoch 8, Batch 220, LR 0.000094 Loss 10.019620, Accuracy 62.798%\n",
      "Epoch 8, Batch 221, LR 0.000094 Loss 10.017078, Accuracy 62.829%\n",
      "Epoch 8, Batch 222, LR 0.000094 Loss 10.014943, Accuracy 62.824%\n",
      "Epoch 8, Batch 223, LR 0.000094 Loss 10.017149, Accuracy 62.805%\n",
      "Epoch 8, Batch 224, LR 0.000094 Loss 10.019893, Accuracy 62.779%\n",
      "Epoch 8, Batch 225, LR 0.000094 Loss 10.019711, Accuracy 62.785%\n",
      "Epoch 8, Batch 226, LR 0.000094 Loss 10.021079, Accuracy 62.777%\n",
      "Epoch 8, Batch 227, LR 0.000094 Loss 10.018070, Accuracy 62.782%\n",
      "Epoch 8, Batch 228, LR 0.000094 Loss 10.013316, Accuracy 62.802%\n",
      "Epoch 8, Batch 229, LR 0.000094 Loss 10.015649, Accuracy 62.780%\n",
      "Epoch 8, Batch 230, LR 0.000094 Loss 10.018302, Accuracy 62.768%\n",
      "Epoch 8, Batch 231, LR 0.000094 Loss 10.017320, Accuracy 62.784%\n",
      "Epoch 8, Batch 232, LR 0.000094 Loss 10.015987, Accuracy 62.803%\n",
      "Epoch 8, Batch 233, LR 0.000094 Loss 10.019894, Accuracy 62.765%\n",
      "Epoch 8, Batch 234, LR 0.000094 Loss 10.022050, Accuracy 62.730%\n",
      "Epoch 8, Batch 235, LR 0.000094 Loss 10.021869, Accuracy 62.723%\n",
      "Epoch 8, Batch 236, LR 0.000094 Loss 10.023117, Accuracy 62.715%\n",
      "Epoch 8, Batch 237, LR 0.000094 Loss 10.022330, Accuracy 62.711%\n",
      "Epoch 8, Batch 238, LR 0.000094 Loss 10.023054, Accuracy 62.713%\n",
      "Epoch 8, Batch 239, LR 0.000094 Loss 10.023015, Accuracy 62.729%\n",
      "Epoch 8, Batch 240, LR 0.000094 Loss 10.022423, Accuracy 62.738%\n",
      "Epoch 8, Batch 241, LR 0.000094 Loss 10.021988, Accuracy 62.720%\n",
      "Epoch 8, Batch 242, LR 0.000094 Loss 10.020554, Accuracy 62.729%\n",
      "Epoch 8, Batch 243, LR 0.000094 Loss 10.019520, Accuracy 62.754%\n",
      "Epoch 8, Batch 244, LR 0.000094 Loss 10.020962, Accuracy 62.737%\n",
      "Epoch 8, Batch 245, LR 0.000094 Loss 10.023222, Accuracy 62.723%\n",
      "Epoch 8, Batch 246, LR 0.000094 Loss 10.023653, Accuracy 62.722%\n",
      "Epoch 8, Batch 247, LR 0.000094 Loss 10.025814, Accuracy 62.693%\n",
      "Epoch 8, Batch 248, LR 0.000093 Loss 10.024158, Accuracy 62.717%\n",
      "Epoch 8, Batch 249, LR 0.000093 Loss 10.018610, Accuracy 62.735%\n",
      "Epoch 8, Batch 250, LR 0.000093 Loss 10.021851, Accuracy 62.719%\n",
      "Epoch 8, Batch 251, LR 0.000093 Loss 10.020536, Accuracy 62.749%\n",
      "Epoch 8, Batch 252, LR 0.000093 Loss 10.021035, Accuracy 62.745%\n",
      "Epoch 8, Batch 253, LR 0.000093 Loss 10.021445, Accuracy 62.735%\n",
      "Epoch 8, Batch 254, LR 0.000093 Loss 10.022056, Accuracy 62.743%\n",
      "Epoch 8, Batch 255, LR 0.000093 Loss 10.021665, Accuracy 62.754%\n",
      "Epoch 8, Batch 256, LR 0.000093 Loss 10.019847, Accuracy 62.775%\n",
      "Epoch 8, Batch 257, LR 0.000093 Loss 10.019493, Accuracy 62.780%\n",
      "Epoch 8, Batch 258, LR 0.000093 Loss 10.017870, Accuracy 62.788%\n",
      "Epoch 8, Batch 259, LR 0.000093 Loss 10.019372, Accuracy 62.781%\n",
      "Epoch 8, Batch 260, LR 0.000093 Loss 10.018729, Accuracy 62.785%\n",
      "Epoch 8, Batch 261, LR 0.000093 Loss 10.017063, Accuracy 62.790%\n",
      "Epoch 8, Batch 262, LR 0.000093 Loss 10.017996, Accuracy 62.789%\n",
      "Epoch 8, Batch 263, LR 0.000093 Loss 10.020420, Accuracy 62.758%\n",
      "Epoch 8, Batch 264, LR 0.000093 Loss 10.018559, Accuracy 62.772%\n",
      "Epoch 8, Batch 265, LR 0.000093 Loss 10.017262, Accuracy 62.780%\n",
      "Epoch 8, Batch 266, LR 0.000093 Loss 10.017557, Accuracy 62.776%\n",
      "Epoch 8, Batch 267, LR 0.000093 Loss 10.017186, Accuracy 62.787%\n",
      "Epoch 8, Batch 268, LR 0.000093 Loss 10.016572, Accuracy 62.792%\n",
      "Epoch 8, Batch 269, LR 0.000093 Loss 10.018576, Accuracy 62.785%\n",
      "Epoch 8, Batch 270, LR 0.000093 Loss 10.016247, Accuracy 62.792%\n",
      "Epoch 8, Batch 271, LR 0.000093 Loss 10.016794, Accuracy 62.788%\n",
      "Epoch 8, Batch 272, LR 0.000093 Loss 10.012399, Accuracy 62.816%\n",
      "Epoch 8, Batch 273, LR 0.000093 Loss 10.010547, Accuracy 62.841%\n",
      "Epoch 8, Batch 274, LR 0.000093 Loss 10.009560, Accuracy 62.848%\n",
      "Epoch 8, Batch 275, LR 0.000093 Loss 10.009324, Accuracy 62.847%\n",
      "Epoch 8, Batch 276, LR 0.000093 Loss 10.009617, Accuracy 62.854%\n",
      "Epoch 8, Batch 277, LR 0.000093 Loss 10.008818, Accuracy 62.864%\n",
      "Epoch 8, Batch 278, LR 0.000093 Loss 10.011428, Accuracy 62.826%\n",
      "Epoch 8, Batch 279, LR 0.000093 Loss 10.012516, Accuracy 62.805%\n",
      "Epoch 8, Batch 280, LR 0.000093 Loss 10.014008, Accuracy 62.782%\n",
      "Epoch 8, Batch 281, LR 0.000093 Loss 10.009716, Accuracy 62.789%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 282, LR 0.000093 Loss 10.009169, Accuracy 62.785%\n",
      "Epoch 8, Batch 283, LR 0.000093 Loss 10.009907, Accuracy 62.757%\n",
      "Epoch 8, Batch 284, LR 0.000093 Loss 10.008376, Accuracy 62.770%\n",
      "Epoch 8, Batch 285, LR 0.000093 Loss 10.010213, Accuracy 62.733%\n",
      "Epoch 8, Batch 286, LR 0.000093 Loss 10.009265, Accuracy 62.735%\n",
      "Epoch 8, Batch 287, LR 0.000093 Loss 10.008969, Accuracy 62.748%\n",
      "Epoch 8, Batch 288, LR 0.000093 Loss 10.006629, Accuracy 62.755%\n",
      "Epoch 8, Batch 289, LR 0.000093 Loss 10.004985, Accuracy 62.765%\n",
      "Epoch 8, Batch 290, LR 0.000093 Loss 10.002149, Accuracy 62.818%\n",
      "Epoch 8, Batch 291, LR 0.000093 Loss 10.000468, Accuracy 62.819%\n",
      "Epoch 8, Batch 292, LR 0.000093 Loss 10.000454, Accuracy 62.805%\n",
      "Epoch 8, Batch 293, LR 0.000093 Loss 10.003473, Accuracy 62.769%\n",
      "Epoch 8, Batch 294, LR 0.000093 Loss 10.003026, Accuracy 62.782%\n",
      "Epoch 8, Batch 295, LR 0.000093 Loss 10.001561, Accuracy 62.783%\n",
      "Epoch 8, Batch 296, LR 0.000093 Loss 10.001142, Accuracy 62.772%\n",
      "Epoch 8, Batch 297, LR 0.000093 Loss 10.002097, Accuracy 62.768%\n",
      "Epoch 8, Batch 298, LR 0.000093 Loss 10.000895, Accuracy 62.773%\n",
      "Epoch 8, Batch 299, LR 0.000093 Loss 10.000903, Accuracy 62.777%\n",
      "Epoch 8, Batch 300, LR 0.000093 Loss 9.998054, Accuracy 62.815%\n",
      "Epoch 8, Batch 301, LR 0.000093 Loss 9.998568, Accuracy 62.804%\n",
      "Epoch 8, Batch 302, LR 0.000093 Loss 9.996805, Accuracy 62.818%\n",
      "Epoch 8, Batch 303, LR 0.000093 Loss 9.996956, Accuracy 62.804%\n",
      "Epoch 8, Batch 304, LR 0.000093 Loss 10.000739, Accuracy 62.780%\n",
      "Epoch 8, Batch 305, LR 0.000093 Loss 9.999109, Accuracy 62.784%\n",
      "Epoch 8, Batch 306, LR 0.000093 Loss 10.000018, Accuracy 62.786%\n",
      "Epoch 8, Batch 307, LR 0.000093 Loss 9.998241, Accuracy 62.790%\n",
      "Epoch 8, Batch 308, LR 0.000093 Loss 9.997953, Accuracy 62.794%\n",
      "Epoch 8, Batch 309, LR 0.000093 Loss 9.996708, Accuracy 62.814%\n",
      "Epoch 8, Batch 310, LR 0.000093 Loss 9.994760, Accuracy 62.835%\n",
      "Epoch 8, Batch 311, LR 0.000093 Loss 9.996261, Accuracy 62.834%\n",
      "Epoch 8, Batch 312, LR 0.000093 Loss 9.995719, Accuracy 62.823%\n",
      "Epoch 8, Batch 313, LR 0.000093 Loss 9.996673, Accuracy 62.812%\n",
      "Epoch 8, Batch 314, LR 0.000093 Loss 9.994498, Accuracy 62.813%\n",
      "Epoch 8, Batch 315, LR 0.000093 Loss 9.993619, Accuracy 62.805%\n",
      "Epoch 8, Batch 316, LR 0.000093 Loss 9.994062, Accuracy 62.792%\n",
      "Epoch 8, Batch 317, LR 0.000093 Loss 9.992053, Accuracy 62.798%\n",
      "Epoch 8, Batch 318, LR 0.000093 Loss 9.993157, Accuracy 62.795%\n",
      "Epoch 8, Batch 319, LR 0.000093 Loss 9.992261, Accuracy 62.799%\n",
      "Epoch 8, Batch 320, LR 0.000093 Loss 9.989860, Accuracy 62.791%\n",
      "Epoch 8, Batch 321, LR 0.000093 Loss 9.988907, Accuracy 62.799%\n",
      "Epoch 8, Batch 322, LR 0.000093 Loss 9.988168, Accuracy 62.801%\n",
      "Epoch 8, Batch 323, LR 0.000093 Loss 9.987476, Accuracy 62.810%\n",
      "Epoch 8, Batch 324, LR 0.000093 Loss 9.989066, Accuracy 62.809%\n",
      "Epoch 8, Batch 325, LR 0.000093 Loss 9.988481, Accuracy 62.822%\n",
      "Epoch 8, Batch 326, LR 0.000093 Loss 9.989384, Accuracy 62.838%\n",
      "Epoch 8, Batch 327, LR 0.000093 Loss 9.990555, Accuracy 62.842%\n",
      "Epoch 8, Batch 328, LR 0.000093 Loss 9.992243, Accuracy 62.819%\n",
      "Epoch 8, Batch 329, LR 0.000093 Loss 9.990570, Accuracy 62.821%\n",
      "Epoch 8, Batch 330, LR 0.000093 Loss 9.990856, Accuracy 62.822%\n",
      "Epoch 8, Batch 331, LR 0.000093 Loss 9.989393, Accuracy 62.842%\n",
      "Epoch 8, Batch 332, LR 0.000093 Loss 9.989099, Accuracy 62.851%\n",
      "Epoch 8, Batch 333, LR 0.000093 Loss 9.987864, Accuracy 62.845%\n",
      "Epoch 8, Batch 334, LR 0.000093 Loss 9.988455, Accuracy 62.846%\n",
      "Epoch 8, Batch 335, LR 0.000093 Loss 9.988463, Accuracy 62.845%\n",
      "Epoch 8, Batch 336, LR 0.000093 Loss 9.988947, Accuracy 62.837%\n",
      "Epoch 8, Batch 337, LR 0.000093 Loss 9.988440, Accuracy 62.841%\n",
      "Epoch 8, Batch 338, LR 0.000093 Loss 9.986411, Accuracy 62.861%\n",
      "Epoch 8, Batch 339, LR 0.000093 Loss 9.984473, Accuracy 62.871%\n",
      "Epoch 8, Batch 340, LR 0.000093 Loss 9.985670, Accuracy 62.852%\n",
      "Epoch 8, Batch 341, LR 0.000093 Loss 9.985044, Accuracy 62.848%\n",
      "Epoch 8, Batch 342, LR 0.000093 Loss 9.985181, Accuracy 62.845%\n",
      "Epoch 8, Batch 343, LR 0.000093 Loss 9.983314, Accuracy 62.846%\n",
      "Epoch 8, Batch 344, LR 0.000093 Loss 9.986457, Accuracy 62.825%\n",
      "Epoch 8, Batch 345, LR 0.000093 Loss 9.985121, Accuracy 62.837%\n",
      "Epoch 8, Batch 346, LR 0.000093 Loss 9.986175, Accuracy 62.816%\n",
      "Epoch 8, Batch 347, LR 0.000093 Loss 9.984625, Accuracy 62.820%\n",
      "Epoch 8, Batch 348, LR 0.000093 Loss 9.985251, Accuracy 62.817%\n",
      "Epoch 8, Batch 349, LR 0.000093 Loss 9.985446, Accuracy 62.798%\n",
      "Epoch 8, Batch 350, LR 0.000093 Loss 9.982591, Accuracy 62.833%\n",
      "Epoch 8, Batch 351, LR 0.000093 Loss 9.980694, Accuracy 62.852%\n",
      "Epoch 8, Batch 352, LR 0.000093 Loss 9.979012, Accuracy 62.868%\n",
      "Epoch 8, Batch 353, LR 0.000093 Loss 9.977429, Accuracy 62.874%\n",
      "Epoch 8, Batch 354, LR 0.000093 Loss 9.977852, Accuracy 62.875%\n",
      "Epoch 8, Batch 355, LR 0.000093 Loss 9.977394, Accuracy 62.887%\n",
      "Epoch 8, Batch 356, LR 0.000093 Loss 9.977804, Accuracy 62.884%\n",
      "Epoch 8, Batch 357, LR 0.000093 Loss 9.978845, Accuracy 62.872%\n",
      "Epoch 8, Batch 358, LR 0.000093 Loss 9.979216, Accuracy 62.875%\n",
      "Epoch 8, Batch 359, LR 0.000093 Loss 9.977697, Accuracy 62.894%\n",
      "Epoch 8, Batch 360, LR 0.000093 Loss 9.978605, Accuracy 62.880%\n",
      "Epoch 8, Batch 361, LR 0.000093 Loss 9.976266, Accuracy 62.916%\n",
      "Epoch 8, Batch 362, LR 0.000093 Loss 9.976138, Accuracy 62.917%\n",
      "Epoch 8, Batch 363, LR 0.000093 Loss 9.977680, Accuracy 62.909%\n",
      "Epoch 8, Batch 364, LR 0.000093 Loss 9.978768, Accuracy 62.910%\n",
      "Epoch 8, Batch 365, LR 0.000093 Loss 9.978796, Accuracy 62.922%\n",
      "Epoch 8, Batch 366, LR 0.000093 Loss 9.977307, Accuracy 62.933%\n",
      "Epoch 8, Batch 367, LR 0.000093 Loss 9.975538, Accuracy 62.951%\n",
      "Epoch 8, Batch 368, LR 0.000093 Loss 9.975038, Accuracy 62.948%\n",
      "Epoch 8, Batch 369, LR 0.000093 Loss 9.976952, Accuracy 62.928%\n",
      "Epoch 8, Batch 370, LR 0.000093 Loss 9.977156, Accuracy 62.920%\n",
      "Epoch 8, Batch 371, LR 0.000093 Loss 9.976732, Accuracy 62.938%\n",
      "Epoch 8, Batch 372, LR 0.000093 Loss 9.975916, Accuracy 62.958%\n",
      "Epoch 8, Batch 373, LR 0.000093 Loss 9.976964, Accuracy 62.942%\n",
      "Epoch 8, Batch 374, LR 0.000093 Loss 9.976768, Accuracy 62.964%\n",
      "Epoch 8, Batch 375, LR 0.000093 Loss 9.973544, Accuracy 62.973%\n",
      "Epoch 8, Batch 376, LR 0.000093 Loss 9.972662, Accuracy 62.974%\n",
      "Epoch 8, Batch 377, LR 0.000093 Loss 9.973181, Accuracy 62.983%\n",
      "Epoch 8, Batch 378, LR 0.000093 Loss 9.974941, Accuracy 62.959%\n",
      "Epoch 8, Batch 379, LR 0.000093 Loss 9.975251, Accuracy 62.956%\n",
      "Epoch 8, Batch 380, LR 0.000093 Loss 9.975512, Accuracy 62.952%\n",
      "Epoch 8, Batch 381, LR 0.000093 Loss 9.975114, Accuracy 62.945%\n",
      "Epoch 8, Batch 382, LR 0.000093 Loss 9.972572, Accuracy 62.964%\n",
      "Epoch 8, Batch 383, LR 0.000093 Loss 9.971530, Accuracy 62.975%\n",
      "Epoch 8, Batch 384, LR 0.000093 Loss 9.970648, Accuracy 62.966%\n",
      "Epoch 8, Batch 385, LR 0.000093 Loss 9.969370, Accuracy 62.973%\n",
      "Epoch 8, Batch 386, LR 0.000093 Loss 9.969956, Accuracy 62.986%\n",
      "Epoch 8, Batch 387, LR 0.000093 Loss 9.969336, Accuracy 62.989%\n",
      "Epoch 8, Batch 388, LR 0.000093 Loss 9.968855, Accuracy 62.993%\n",
      "Epoch 8, Batch 389, LR 0.000093 Loss 9.969746, Accuracy 62.988%\n",
      "Epoch 8, Batch 390, LR 0.000093 Loss 9.968686, Accuracy 62.993%\n",
      "Epoch 8, Batch 391, LR 0.000093 Loss 9.968210, Accuracy 62.992%\n",
      "Epoch 8, Batch 392, LR 0.000093 Loss 9.966011, Accuracy 62.998%\n",
      "Epoch 8, Batch 393, LR 0.000093 Loss 9.965578, Accuracy 62.997%\n",
      "Epoch 8, Batch 394, LR 0.000093 Loss 9.967247, Accuracy 62.996%\n",
      "Epoch 8, Batch 395, LR 0.000093 Loss 9.967277, Accuracy 62.991%\n",
      "Epoch 8, Batch 396, LR 0.000093 Loss 9.964914, Accuracy 62.993%\n",
      "Epoch 8, Batch 397, LR 0.000093 Loss 9.964168, Accuracy 62.988%\n",
      "Epoch 8, Batch 398, LR 0.000093 Loss 9.964029, Accuracy 62.985%\n",
      "Epoch 8, Batch 399, LR 0.000093 Loss 9.962007, Accuracy 62.997%\n",
      "Epoch 8, Batch 400, LR 0.000093 Loss 9.962462, Accuracy 63.006%\n",
      "Epoch 8, Batch 401, LR 0.000093 Loss 9.962767, Accuracy 63.016%\n",
      "Epoch 8, Batch 402, LR 0.000093 Loss 9.963488, Accuracy 63.007%\n",
      "Epoch 8, Batch 403, LR 0.000093 Loss 9.964825, Accuracy 62.998%\n",
      "Epoch 8, Batch 404, LR 0.000093 Loss 9.966355, Accuracy 62.993%\n",
      "Epoch 8, Batch 405, LR 0.000093 Loss 9.966421, Accuracy 62.998%\n",
      "Epoch 8, Batch 406, LR 0.000093 Loss 9.965403, Accuracy 63.002%\n",
      "Epoch 8, Batch 407, LR 0.000093 Loss 9.964099, Accuracy 63.016%\n",
      "Epoch 8, Batch 408, LR 0.000093 Loss 9.961787, Accuracy 63.032%\n",
      "Epoch 8, Batch 409, LR 0.000093 Loss 9.960679, Accuracy 63.056%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 410, LR 0.000093 Loss 9.958322, Accuracy 63.068%\n",
      "Epoch 8, Batch 411, LR 0.000093 Loss 9.959552, Accuracy 63.044%\n",
      "Epoch 8, Batch 412, LR 0.000093 Loss 9.957255, Accuracy 63.065%\n",
      "Epoch 8, Batch 413, LR 0.000093 Loss 9.957540, Accuracy 63.049%\n",
      "Epoch 8, Batch 414, LR 0.000093 Loss 9.957906, Accuracy 63.042%\n",
      "Epoch 8, Batch 415, LR 0.000093 Loss 9.959067, Accuracy 63.027%\n",
      "Epoch 8, Batch 416, LR 0.000093 Loss 9.957900, Accuracy 63.037%\n",
      "Epoch 8, Batch 417, LR 0.000093 Loss 9.956519, Accuracy 63.032%\n",
      "Epoch 8, Batch 418, LR 0.000093 Loss 9.957141, Accuracy 63.021%\n",
      "Epoch 8, Batch 419, LR 0.000093 Loss 9.957783, Accuracy 63.024%\n",
      "Epoch 8, Batch 420, LR 0.000093 Loss 9.956711, Accuracy 63.030%\n",
      "Epoch 8, Batch 421, LR 0.000093 Loss 9.957733, Accuracy 63.012%\n",
      "Epoch 8, Batch 422, LR 0.000093 Loss 9.956393, Accuracy 63.018%\n",
      "Epoch 8, Batch 423, LR 0.000093 Loss 9.955654, Accuracy 63.023%\n",
      "Epoch 8, Batch 424, LR 0.000093 Loss 9.955614, Accuracy 63.009%\n",
      "Epoch 8, Batch 425, LR 0.000093 Loss 9.955860, Accuracy 63.015%\n",
      "Epoch 8, Batch 426, LR 0.000093 Loss 9.957725, Accuracy 63.002%\n",
      "Epoch 8, Batch 427, LR 0.000093 Loss 9.957617, Accuracy 62.985%\n",
      "Epoch 8, Batch 428, LR 0.000093 Loss 9.958461, Accuracy 62.975%\n",
      "Epoch 8, Batch 429, LR 0.000093 Loss 9.957648, Accuracy 62.972%\n",
      "Epoch 8, Batch 430, LR 0.000093 Loss 9.957953, Accuracy 62.976%\n",
      "Epoch 8, Batch 431, LR 0.000093 Loss 9.958668, Accuracy 62.977%\n",
      "Epoch 8, Batch 432, LR 0.000093 Loss 9.957997, Accuracy 62.981%\n",
      "Epoch 8, Batch 433, LR 0.000093 Loss 9.957642, Accuracy 62.993%\n",
      "Epoch 8, Batch 434, LR 0.000093 Loss 9.957380, Accuracy 62.991%\n",
      "Epoch 8, Batch 435, LR 0.000093 Loss 9.957529, Accuracy 62.980%\n",
      "Epoch 8, Batch 436, LR 0.000093 Loss 9.957892, Accuracy 62.971%\n",
      "Epoch 8, Batch 437, LR 0.000093 Loss 9.955882, Accuracy 62.986%\n",
      "Epoch 8, Batch 438, LR 0.000093 Loss 9.954910, Accuracy 62.999%\n",
      "Epoch 8, Batch 439, LR 0.000093 Loss 9.954935, Accuracy 62.993%\n",
      "Epoch 8, Batch 440, LR 0.000093 Loss 9.954745, Accuracy 62.999%\n",
      "Epoch 8, Batch 441, LR 0.000093 Loss 9.955110, Accuracy 63.001%\n",
      "Epoch 8, Batch 442, LR 0.000093 Loss 9.954763, Accuracy 63.009%\n",
      "Epoch 8, Batch 443, LR 0.000093 Loss 9.954746, Accuracy 63.003%\n",
      "Epoch 8, Batch 444, LR 0.000093 Loss 9.955741, Accuracy 62.993%\n",
      "Epoch 8, Batch 445, LR 0.000093 Loss 9.955847, Accuracy 62.990%\n",
      "Epoch 8, Batch 446, LR 0.000093 Loss 9.955831, Accuracy 62.980%\n",
      "Epoch 8, Batch 447, LR 0.000093 Loss 9.955235, Accuracy 62.970%\n",
      "Epoch 8, Batch 448, LR 0.000093 Loss 9.956170, Accuracy 62.974%\n",
      "Epoch 8, Batch 449, LR 0.000093 Loss 9.955950, Accuracy 62.963%\n",
      "Epoch 8, Batch 450, LR 0.000093 Loss 9.956964, Accuracy 62.950%\n",
      "Epoch 8, Batch 451, LR 0.000093 Loss 9.956100, Accuracy 62.973%\n",
      "Epoch 8, Batch 452, LR 0.000093 Loss 9.955602, Accuracy 62.970%\n",
      "Epoch 8, Batch 453, LR 0.000093 Loss 9.956621, Accuracy 62.957%\n",
      "Epoch 8, Batch 454, LR 0.000093 Loss 9.956114, Accuracy 62.958%\n",
      "Epoch 8, Batch 455, LR 0.000093 Loss 9.956241, Accuracy 62.962%\n",
      "Epoch 8, Batch 456, LR 0.000093 Loss 9.954992, Accuracy 62.975%\n",
      "Epoch 8, Batch 457, LR 0.000093 Loss 9.955359, Accuracy 62.975%\n",
      "Epoch 8, Batch 458, LR 0.000093 Loss 9.953415, Accuracy 62.995%\n",
      "Epoch 8, Batch 459, LR 0.000093 Loss 9.952154, Accuracy 63.014%\n",
      "Epoch 8, Batch 460, LR 0.000093 Loss 9.950878, Accuracy 63.020%\n",
      "Epoch 8, Batch 461, LR 0.000093 Loss 9.949874, Accuracy 63.029%\n",
      "Epoch 8, Batch 462, LR 0.000093 Loss 9.948052, Accuracy 63.045%\n",
      "Epoch 8, Batch 463, LR 0.000093 Loss 9.948572, Accuracy 63.033%\n",
      "Epoch 8, Batch 464, LR 0.000093 Loss 9.948268, Accuracy 63.035%\n",
      "Epoch 8, Batch 465, LR 0.000093 Loss 9.949297, Accuracy 63.024%\n",
      "Epoch 8, Batch 466, LR 0.000093 Loss 9.946962, Accuracy 63.045%\n",
      "Epoch 8, Batch 467, LR 0.000093 Loss 9.946272, Accuracy 63.052%\n",
      "Epoch 8, Batch 468, LR 0.000093 Loss 9.945175, Accuracy 63.058%\n",
      "Epoch 8, Batch 469, LR 0.000093 Loss 9.944295, Accuracy 63.068%\n",
      "Epoch 8, Batch 470, LR 0.000093 Loss 9.945512, Accuracy 63.060%\n",
      "Epoch 8, Batch 471, LR 0.000093 Loss 9.944816, Accuracy 63.066%\n",
      "Epoch 8, Batch 472, LR 0.000093 Loss 9.946251, Accuracy 63.058%\n",
      "Epoch 8, Batch 473, LR 0.000093 Loss 9.946478, Accuracy 63.050%\n",
      "Epoch 8, Batch 474, LR 0.000093 Loss 9.946549, Accuracy 63.052%\n",
      "Epoch 8, Batch 475, LR 0.000093 Loss 9.945199, Accuracy 63.053%\n",
      "Epoch 8, Batch 476, LR 0.000093 Loss 9.943535, Accuracy 63.066%\n",
      "Epoch 8, Batch 477, LR 0.000093 Loss 9.943090, Accuracy 63.063%\n",
      "Epoch 8, Batch 478, LR 0.000093 Loss 9.943125, Accuracy 63.061%\n",
      "Epoch 8, Batch 479, LR 0.000093 Loss 9.942086, Accuracy 63.076%\n",
      "Epoch 8, Batch 480, LR 0.000093 Loss 9.943515, Accuracy 63.070%\n",
      "Epoch 8, Batch 481, LR 0.000093 Loss 9.943930, Accuracy 63.060%\n",
      "Epoch 8, Batch 482, LR 0.000093 Loss 9.944550, Accuracy 63.049%\n",
      "Epoch 8, Batch 483, LR 0.000093 Loss 9.942413, Accuracy 63.063%\n",
      "Epoch 8, Batch 484, LR 0.000093 Loss 9.942437, Accuracy 63.065%\n",
      "Epoch 8, Batch 485, LR 0.000093 Loss 9.941704, Accuracy 63.082%\n",
      "Epoch 8, Batch 486, LR 0.000093 Loss 9.942600, Accuracy 63.072%\n",
      "Epoch 8, Batch 487, LR 0.000093 Loss 9.941836, Accuracy 63.073%\n",
      "Epoch 8, Batch 488, LR 0.000093 Loss 9.941036, Accuracy 63.084%\n",
      "Epoch 8, Batch 489, LR 0.000093 Loss 9.940406, Accuracy 63.088%\n",
      "Epoch 8, Batch 490, LR 0.000093 Loss 9.940269, Accuracy 63.092%\n",
      "Epoch 8, Batch 491, LR 0.000093 Loss 9.941519, Accuracy 63.081%\n",
      "Epoch 8, Batch 492, LR 0.000093 Loss 9.942202, Accuracy 63.068%\n",
      "Epoch 8, Batch 493, LR 0.000093 Loss 9.941376, Accuracy 63.074%\n",
      "Epoch 8, Batch 494, LR 0.000093 Loss 9.941247, Accuracy 63.080%\n",
      "Epoch 8, Batch 495, LR 0.000093 Loss 9.942876, Accuracy 63.078%\n",
      "Epoch 8, Batch 496, LR 0.000093 Loss 9.942860, Accuracy 63.078%\n",
      "Epoch 8, Batch 497, LR 0.000093 Loss 9.941637, Accuracy 63.086%\n",
      "Epoch 8, Batch 498, LR 0.000093 Loss 9.941981, Accuracy 63.082%\n",
      "Epoch 8, Batch 499, LR 0.000093 Loss 9.941405, Accuracy 63.092%\n",
      "Epoch 8, Batch 500, LR 0.000093 Loss 9.940287, Accuracy 63.097%\n",
      "Epoch 8, Batch 501, LR 0.000093 Loss 9.937446, Accuracy 63.127%\n",
      "Epoch 8, Batch 502, LR 0.000093 Loss 9.937075, Accuracy 63.129%\n",
      "Epoch 8, Batch 503, LR 0.000093 Loss 9.936694, Accuracy 63.129%\n",
      "Epoch 8, Batch 504, LR 0.000093 Loss 9.936058, Accuracy 63.136%\n",
      "Epoch 8, Batch 505, LR 0.000093 Loss 9.933623, Accuracy 63.151%\n",
      "Epoch 8, Batch 506, LR 0.000093 Loss 9.934290, Accuracy 63.150%\n",
      "Epoch 8, Batch 507, LR 0.000093 Loss 9.933993, Accuracy 63.150%\n",
      "Epoch 8, Batch 508, LR 0.000093 Loss 9.934441, Accuracy 63.146%\n",
      "Epoch 8, Batch 509, LR 0.000093 Loss 9.934932, Accuracy 63.137%\n",
      "Epoch 8, Batch 510, LR 0.000093 Loss 9.935919, Accuracy 63.128%\n",
      "Epoch 8, Batch 511, LR 0.000093 Loss 9.935136, Accuracy 63.127%\n",
      "Epoch 8, Batch 512, LR 0.000093 Loss 9.933857, Accuracy 63.138%\n",
      "Epoch 8, Batch 513, LR 0.000093 Loss 9.933401, Accuracy 63.141%\n",
      "Epoch 8, Batch 514, LR 0.000093 Loss 9.933177, Accuracy 63.138%\n",
      "Epoch 8, Batch 515, LR 0.000093 Loss 9.933235, Accuracy 63.139%\n",
      "Epoch 8, Batch 516, LR 0.000093 Loss 9.933682, Accuracy 63.143%\n",
      "Epoch 8, Batch 517, LR 0.000093 Loss 9.935535, Accuracy 63.133%\n",
      "Epoch 8, Batch 518, LR 0.000093 Loss 9.935246, Accuracy 63.135%\n",
      "Epoch 8, Batch 519, LR 0.000093 Loss 9.935850, Accuracy 63.134%\n",
      "Epoch 8, Batch 520, LR 0.000093 Loss 9.936435, Accuracy 63.127%\n",
      "Epoch 8, Batch 521, LR 0.000093 Loss 9.936549, Accuracy 63.127%\n",
      "Epoch 8, Batch 522, LR 0.000093 Loss 9.935149, Accuracy 63.120%\n",
      "Epoch 8, Batch 523, LR 0.000093 Loss 9.935367, Accuracy 63.111%\n",
      "Epoch 8, Batch 524, LR 0.000093 Loss 9.936065, Accuracy 63.114%\n",
      "Epoch 8, Batch 525, LR 0.000093 Loss 9.937917, Accuracy 63.104%\n",
      "Epoch 8, Batch 526, LR 0.000093 Loss 9.939346, Accuracy 63.091%\n",
      "Epoch 8, Batch 527, LR 0.000093 Loss 9.938638, Accuracy 63.100%\n",
      "Epoch 8, Batch 528, LR 0.000093 Loss 9.938356, Accuracy 63.102%\n",
      "Epoch 8, Batch 529, LR 0.000093 Loss 9.938389, Accuracy 63.098%\n",
      "Epoch 8, Batch 530, LR 0.000093 Loss 9.937912, Accuracy 63.112%\n",
      "Epoch 8, Batch 531, LR 0.000093 Loss 9.938765, Accuracy 63.105%\n",
      "Epoch 8, Batch 532, LR 0.000093 Loss 9.939588, Accuracy 63.093%\n",
      "Epoch 8, Batch 533, LR 0.000093 Loss 9.939037, Accuracy 63.099%\n",
      "Epoch 8, Batch 534, LR 0.000093 Loss 9.938837, Accuracy 63.104%\n",
      "Epoch 8, Batch 535, LR 0.000093 Loss 9.939996, Accuracy 63.090%\n",
      "Epoch 8, Batch 536, LR 0.000093 Loss 9.941127, Accuracy 63.071%\n",
      "Epoch 8, Batch 537, LR 0.000093 Loss 9.941390, Accuracy 63.066%\n",
      "Epoch 8, Batch 538, LR 0.000093 Loss 9.941204, Accuracy 63.072%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 539, LR 0.000093 Loss 9.940344, Accuracy 63.090%\n",
      "Epoch 8, Batch 540, LR 0.000093 Loss 9.940844, Accuracy 63.087%\n",
      "Epoch 8, Batch 541, LR 0.000093 Loss 9.940650, Accuracy 63.083%\n",
      "Epoch 8, Batch 542, LR 0.000093 Loss 9.941231, Accuracy 63.078%\n",
      "Epoch 8, Batch 543, LR 0.000093 Loss 9.940996, Accuracy 63.078%\n",
      "Epoch 8, Batch 544, LR 0.000093 Loss 9.941726, Accuracy 63.077%\n",
      "Epoch 8, Batch 545, LR 0.000093 Loss 9.939729, Accuracy 63.091%\n",
      "Epoch 8, Batch 546, LR 0.000093 Loss 9.938985, Accuracy 63.094%\n",
      "Epoch 8, Batch 547, LR 0.000093 Loss 9.939038, Accuracy 63.103%\n",
      "Epoch 8, Batch 548, LR 0.000093 Loss 9.939116, Accuracy 63.104%\n",
      "Epoch 8, Batch 549, LR 0.000093 Loss 9.939363, Accuracy 63.102%\n",
      "Epoch 8, Batch 550, LR 0.000093 Loss 9.938635, Accuracy 63.107%\n",
      "Epoch 8, Batch 551, LR 0.000093 Loss 9.937183, Accuracy 63.107%\n",
      "Epoch 8, Batch 552, LR 0.000093 Loss 9.937902, Accuracy 63.100%\n",
      "Epoch 8, Batch 553, LR 0.000093 Loss 9.937649, Accuracy 63.103%\n",
      "Epoch 8, Batch 554, LR 0.000093 Loss 9.938128, Accuracy 63.104%\n",
      "Epoch 8, Batch 555, LR 0.000093 Loss 9.939705, Accuracy 63.101%\n",
      "Epoch 8, Batch 556, LR 0.000093 Loss 9.938654, Accuracy 63.110%\n",
      "Epoch 8, Batch 557, LR 0.000093 Loss 9.938997, Accuracy 63.110%\n",
      "Epoch 8, Batch 558, LR 0.000093 Loss 9.938818, Accuracy 63.108%\n",
      "Epoch 8, Batch 559, LR 0.000093 Loss 9.939956, Accuracy 63.097%\n",
      "Epoch 8, Batch 560, LR 0.000093 Loss 9.938359, Accuracy 63.101%\n",
      "Epoch 8, Batch 561, LR 0.000093 Loss 9.937276, Accuracy 63.110%\n",
      "Epoch 8, Batch 562, LR 0.000093 Loss 9.935759, Accuracy 63.126%\n",
      "Epoch 8, Batch 563, LR 0.000093 Loss 9.935351, Accuracy 63.123%\n",
      "Epoch 8, Batch 564, LR 0.000093 Loss 9.934904, Accuracy 63.119%\n",
      "Epoch 8, Batch 565, LR 0.000093 Loss 9.934962, Accuracy 63.117%\n",
      "Epoch 8, Batch 566, LR 0.000093 Loss 9.933515, Accuracy 63.138%\n",
      "Epoch 8, Batch 567, LR 0.000093 Loss 9.932263, Accuracy 63.148%\n",
      "Epoch 8, Batch 568, LR 0.000093 Loss 9.931059, Accuracy 63.149%\n",
      "Epoch 8, Batch 569, LR 0.000093 Loss 9.931746, Accuracy 63.149%\n",
      "Epoch 8, Batch 570, LR 0.000093 Loss 9.931187, Accuracy 63.146%\n",
      "Epoch 8, Batch 571, LR 0.000093 Loss 9.931074, Accuracy 63.149%\n",
      "Epoch 8, Batch 572, LR 0.000093 Loss 9.929681, Accuracy 63.146%\n",
      "Epoch 8, Batch 573, LR 0.000093 Loss 9.929027, Accuracy 63.163%\n",
      "Epoch 8, Batch 574, LR 0.000093 Loss 9.928220, Accuracy 63.176%\n",
      "Epoch 8, Batch 575, LR 0.000093 Loss 9.929066, Accuracy 63.173%\n",
      "Epoch 8, Batch 576, LR 0.000093 Loss 9.928274, Accuracy 63.174%\n",
      "Epoch 8, Batch 577, LR 0.000093 Loss 9.928112, Accuracy 63.182%\n",
      "Epoch 8, Batch 578, LR 0.000093 Loss 9.927081, Accuracy 63.183%\n",
      "Epoch 8, Batch 579, LR 0.000093 Loss 9.927848, Accuracy 63.173%\n",
      "Epoch 8, Batch 580, LR 0.000093 Loss 9.929184, Accuracy 63.165%\n",
      "Epoch 8, Batch 581, LR 0.000093 Loss 9.927578, Accuracy 63.159%\n",
      "Epoch 8, Batch 582, LR 0.000093 Loss 9.927484, Accuracy 63.156%\n",
      "Epoch 8, Batch 583, LR 0.000093 Loss 9.928637, Accuracy 63.151%\n",
      "Epoch 8, Batch 584, LR 0.000093 Loss 9.926944, Accuracy 63.164%\n",
      "Epoch 8, Batch 585, LR 0.000093 Loss 9.927786, Accuracy 63.153%\n",
      "Epoch 8, Batch 586, LR 0.000093 Loss 9.928842, Accuracy 63.148%\n",
      "Epoch 8, Batch 587, LR 0.000093 Loss 9.929039, Accuracy 63.151%\n",
      "Epoch 8, Batch 588, LR 0.000093 Loss 9.928559, Accuracy 63.164%\n",
      "Epoch 8, Batch 589, LR 0.000093 Loss 9.929353, Accuracy 63.162%\n",
      "Epoch 8, Batch 590, LR 0.000093 Loss 9.929350, Accuracy 63.154%\n",
      "Epoch 8, Batch 591, LR 0.000093 Loss 9.928749, Accuracy 63.169%\n",
      "Epoch 8, Batch 592, LR 0.000093 Loss 9.928828, Accuracy 63.178%\n",
      "Epoch 8, Batch 593, LR 0.000093 Loss 9.929205, Accuracy 63.177%\n",
      "Epoch 8, Batch 594, LR 0.000093 Loss 9.929776, Accuracy 63.163%\n",
      "Epoch 8, Batch 595, LR 0.000093 Loss 9.928999, Accuracy 63.172%\n",
      "Epoch 8, Batch 596, LR 0.000093 Loss 9.929651, Accuracy 63.167%\n",
      "Epoch 8, Batch 597, LR 0.000093 Loss 9.929653, Accuracy 63.166%\n",
      "Epoch 8, Batch 598, LR 0.000093 Loss 9.929164, Accuracy 63.172%\n",
      "Epoch 8, Batch 599, LR 0.000093 Loss 9.929354, Accuracy 63.170%\n",
      "Epoch 8, Batch 600, LR 0.000093 Loss 9.930301, Accuracy 63.160%\n",
      "Epoch 8, Batch 601, LR 0.000093 Loss 9.929902, Accuracy 63.168%\n",
      "Epoch 8, Batch 602, LR 0.000093 Loss 9.928575, Accuracy 63.177%\n",
      "Epoch 8, Batch 603, LR 0.000093 Loss 9.928039, Accuracy 63.179%\n",
      "Epoch 8, Batch 604, LR 0.000093 Loss 9.927876, Accuracy 63.184%\n",
      "Epoch 8, Batch 605, LR 0.000093 Loss 9.928267, Accuracy 63.183%\n",
      "Epoch 8, Batch 606, LR 0.000093 Loss 9.928355, Accuracy 63.183%\n",
      "Epoch 8, Batch 607, LR 0.000093 Loss 9.929048, Accuracy 63.178%\n",
      "Epoch 8, Batch 608, LR 0.000093 Loss 9.928924, Accuracy 63.175%\n",
      "Epoch 8, Batch 609, LR 0.000093 Loss 9.927910, Accuracy 63.181%\n",
      "Epoch 8, Batch 610, LR 0.000093 Loss 9.928386, Accuracy 63.174%\n",
      "Epoch 8, Batch 611, LR 0.000093 Loss 9.927861, Accuracy 63.182%\n",
      "Epoch 8, Batch 612, LR 0.000093 Loss 9.926979, Accuracy 63.182%\n",
      "Epoch 8, Batch 613, LR 0.000093 Loss 9.925623, Accuracy 63.182%\n",
      "Epoch 8, Batch 614, LR 0.000093 Loss 9.924122, Accuracy 63.193%\n",
      "Epoch 8, Batch 615, LR 0.000093 Loss 9.925261, Accuracy 63.183%\n",
      "Epoch 8, Batch 616, LR 0.000093 Loss 9.925166, Accuracy 63.177%\n",
      "Epoch 8, Batch 617, LR 0.000093 Loss 9.924913, Accuracy 63.186%\n",
      "Epoch 8, Batch 618, LR 0.000093 Loss 9.925518, Accuracy 63.183%\n",
      "Epoch 8, Batch 619, LR 0.000093 Loss 9.925694, Accuracy 63.187%\n",
      "Epoch 8, Batch 620, LR 0.000093 Loss 9.926147, Accuracy 63.189%\n",
      "Epoch 8, Batch 621, LR 0.000093 Loss 9.926435, Accuracy 63.184%\n",
      "Epoch 8, Batch 622, LR 0.000093 Loss 9.927039, Accuracy 63.174%\n",
      "Epoch 8, Batch 623, LR 0.000093 Loss 9.926363, Accuracy 63.173%\n",
      "Epoch 8, Batch 624, LR 0.000093 Loss 9.925732, Accuracy 63.176%\n",
      "Epoch 8, Batch 625, LR 0.000093 Loss 9.925665, Accuracy 63.179%\n",
      "Epoch 8, Batch 626, LR 0.000093 Loss 9.924781, Accuracy 63.183%\n",
      "Epoch 8, Batch 627, LR 0.000093 Loss 9.925392, Accuracy 63.183%\n",
      "Epoch 8, Batch 628, LR 0.000093 Loss 9.925926, Accuracy 63.184%\n",
      "Epoch 8, Batch 629, LR 0.000093 Loss 9.926619, Accuracy 63.179%\n",
      "Epoch 8, Batch 630, LR 0.000093 Loss 9.926550, Accuracy 63.180%\n",
      "Epoch 8, Batch 631, LR 0.000093 Loss 9.927068, Accuracy 63.178%\n",
      "Epoch 8, Batch 632, LR 0.000093 Loss 9.927369, Accuracy 63.176%\n",
      "Epoch 8, Batch 633, LR 0.000093 Loss 9.926578, Accuracy 63.184%\n",
      "Epoch 8, Batch 634, LR 0.000093 Loss 9.926266, Accuracy 63.190%\n",
      "Epoch 8, Batch 635, LR 0.000093 Loss 9.926872, Accuracy 63.178%\n",
      "Epoch 8, Batch 636, LR 0.000093 Loss 9.927015, Accuracy 63.182%\n",
      "Epoch 8, Batch 637, LR 0.000093 Loss 9.926515, Accuracy 63.178%\n",
      "Epoch 8, Batch 638, LR 0.000093 Loss 9.926173, Accuracy 63.181%\n",
      "Epoch 8, Batch 639, LR 0.000093 Loss 9.924559, Accuracy 63.191%\n",
      "Epoch 8, Batch 640, LR 0.000093 Loss 9.925261, Accuracy 63.188%\n",
      "Epoch 8, Batch 641, LR 0.000093 Loss 9.925791, Accuracy 63.189%\n",
      "Epoch 8, Batch 642, LR 0.000093 Loss 9.925977, Accuracy 63.186%\n",
      "Epoch 8, Batch 643, LR 0.000093 Loss 9.926089, Accuracy 63.194%\n",
      "Epoch 8, Batch 644, LR 0.000093 Loss 9.924724, Accuracy 63.200%\n",
      "Epoch 8, Batch 645, LR 0.000093 Loss 9.924106, Accuracy 63.206%\n",
      "Epoch 8, Batch 646, LR 0.000093 Loss 9.924403, Accuracy 63.209%\n",
      "Epoch 8, Batch 647, LR 0.000093 Loss 9.924577, Accuracy 63.203%\n",
      "Epoch 8, Batch 648, LR 0.000093 Loss 9.924018, Accuracy 63.214%\n",
      "Epoch 8, Batch 649, LR 0.000093 Loss 9.921393, Accuracy 63.229%\n",
      "Epoch 8, Batch 650, LR 0.000093 Loss 9.921375, Accuracy 63.225%\n",
      "Epoch 8, Batch 651, LR 0.000093 Loss 9.922245, Accuracy 63.220%\n",
      "Epoch 8, Batch 652, LR 0.000093 Loss 9.922264, Accuracy 63.220%\n",
      "Epoch 8, Batch 653, LR 0.000093 Loss 9.922055, Accuracy 63.219%\n",
      "Epoch 8, Batch 654, LR 0.000093 Loss 9.921720, Accuracy 63.223%\n",
      "Epoch 8, Batch 655, LR 0.000093 Loss 9.919523, Accuracy 63.235%\n",
      "Epoch 8, Batch 656, LR 0.000093 Loss 9.919140, Accuracy 63.248%\n",
      "Epoch 8, Batch 657, LR 0.000093 Loss 9.920154, Accuracy 63.243%\n",
      "Epoch 8, Batch 658, LR 0.000093 Loss 9.919651, Accuracy 63.246%\n",
      "Epoch 8, Batch 659, LR 0.000093 Loss 9.918209, Accuracy 63.253%\n",
      "Epoch 8, Batch 660, LR 0.000093 Loss 9.917161, Accuracy 63.261%\n",
      "Epoch 8, Batch 661, LR 0.000093 Loss 9.915850, Accuracy 63.271%\n",
      "Epoch 8, Batch 662, LR 0.000093 Loss 9.914749, Accuracy 63.269%\n",
      "Epoch 8, Batch 663, LR 0.000093 Loss 9.914883, Accuracy 63.272%\n",
      "Epoch 8, Batch 664, LR 0.000093 Loss 9.914609, Accuracy 63.278%\n",
      "Epoch 8, Batch 665, LR 0.000093 Loss 9.914781, Accuracy 63.279%\n",
      "Epoch 8, Batch 666, LR 0.000093 Loss 9.915308, Accuracy 63.277%\n",
      "Epoch 8, Batch 667, LR 0.000093 Loss 9.915410, Accuracy 63.278%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 668, LR 0.000093 Loss 9.915318, Accuracy 63.278%\n",
      "Epoch 8, Batch 669, LR 0.000093 Loss 9.914778, Accuracy 63.284%\n",
      "Epoch 8, Batch 670, LR 0.000093 Loss 9.914210, Accuracy 63.285%\n",
      "Epoch 8, Batch 671, LR 0.000093 Loss 9.914216, Accuracy 63.291%\n",
      "Epoch 8, Batch 672, LR 0.000093 Loss 9.913317, Accuracy 63.295%\n",
      "Epoch 8, Batch 673, LR 0.000093 Loss 9.913380, Accuracy 63.294%\n",
      "Epoch 8, Batch 674, LR 0.000093 Loss 9.914186, Accuracy 63.285%\n",
      "Epoch 8, Batch 675, LR 0.000093 Loss 9.914671, Accuracy 63.278%\n",
      "Epoch 8, Batch 676, LR 0.000093 Loss 9.914721, Accuracy 63.277%\n",
      "Epoch 8, Batch 677, LR 0.000093 Loss 9.913977, Accuracy 63.280%\n",
      "Epoch 8, Batch 678, LR 0.000093 Loss 9.913387, Accuracy 63.279%\n",
      "Epoch 8, Batch 679, LR 0.000093 Loss 9.913188, Accuracy 63.282%\n",
      "Epoch 8, Batch 680, LR 0.000093 Loss 9.913431, Accuracy 63.282%\n",
      "Epoch 8, Batch 681, LR 0.000093 Loss 9.914043, Accuracy 63.274%\n",
      "Epoch 8, Batch 682, LR 0.000093 Loss 9.913957, Accuracy 63.279%\n",
      "Epoch 8, Batch 683, LR 0.000093 Loss 9.914139, Accuracy 63.277%\n",
      "Epoch 8, Batch 684, LR 0.000093 Loss 9.915654, Accuracy 63.264%\n",
      "Epoch 8, Batch 685, LR 0.000093 Loss 9.916866, Accuracy 63.257%\n",
      "Epoch 8, Batch 686, LR 0.000093 Loss 9.917098, Accuracy 63.253%\n",
      "Epoch 8, Batch 687, LR 0.000093 Loss 9.917124, Accuracy 63.243%\n",
      "Epoch 8, Batch 688, LR 0.000093 Loss 9.918608, Accuracy 63.231%\n",
      "Epoch 8, Batch 689, LR 0.000093 Loss 9.918581, Accuracy 63.228%\n",
      "Epoch 8, Batch 690, LR 0.000093 Loss 9.917609, Accuracy 63.239%\n",
      "Epoch 8, Batch 691, LR 0.000093 Loss 9.916824, Accuracy 63.238%\n",
      "Epoch 8, Batch 692, LR 0.000093 Loss 9.916885, Accuracy 63.235%\n",
      "Epoch 8, Batch 693, LR 0.000093 Loss 9.916489, Accuracy 63.235%\n",
      "Epoch 8, Batch 694, LR 0.000093 Loss 9.914454, Accuracy 63.242%\n",
      "Epoch 8, Batch 695, LR 0.000093 Loss 9.914845, Accuracy 63.240%\n",
      "Epoch 8, Batch 696, LR 0.000093 Loss 9.915246, Accuracy 63.236%\n",
      "Epoch 8, Batch 697, LR 0.000093 Loss 9.915041, Accuracy 63.234%\n",
      "Epoch 8, Batch 698, LR 0.000093 Loss 9.915596, Accuracy 63.230%\n",
      "Epoch 8, Batch 699, LR 0.000093 Loss 9.915210, Accuracy 63.230%\n",
      "Epoch 8, Batch 700, LR 0.000093 Loss 9.914360, Accuracy 63.246%\n",
      "Epoch 8, Batch 701, LR 0.000093 Loss 9.914946, Accuracy 63.247%\n",
      "Epoch 8, Batch 702, LR 0.000093 Loss 9.914750, Accuracy 63.248%\n",
      "Epoch 8, Batch 703, LR 0.000093 Loss 9.914854, Accuracy 63.251%\n",
      "Epoch 8, Batch 704, LR 0.000093 Loss 9.914869, Accuracy 63.244%\n",
      "Epoch 8, Batch 705, LR 0.000093 Loss 9.914986, Accuracy 63.241%\n",
      "Epoch 8, Batch 706, LR 0.000093 Loss 9.914267, Accuracy 63.247%\n",
      "Epoch 8, Batch 707, LR 0.000093 Loss 9.915015, Accuracy 63.241%\n",
      "Epoch 8, Batch 708, LR 0.000093 Loss 9.914219, Accuracy 63.250%\n",
      "Epoch 8, Batch 709, LR 0.000093 Loss 9.913679, Accuracy 63.250%\n",
      "Epoch 8, Batch 710, LR 0.000093 Loss 9.913725, Accuracy 63.252%\n",
      "Epoch 8, Batch 711, LR 0.000093 Loss 9.913410, Accuracy 63.254%\n",
      "Epoch 8, Batch 712, LR 0.000093 Loss 9.913816, Accuracy 63.254%\n",
      "Epoch 8, Batch 713, LR 0.000093 Loss 9.913557, Accuracy 63.256%\n",
      "Epoch 8, Batch 714, LR 0.000093 Loss 9.913709, Accuracy 63.263%\n",
      "Epoch 8, Batch 715, LR 0.000093 Loss 9.913770, Accuracy 63.263%\n",
      "Epoch 8, Batch 716, LR 0.000093 Loss 9.913226, Accuracy 63.266%\n",
      "Epoch 8, Batch 717, LR 0.000093 Loss 9.912461, Accuracy 63.273%\n",
      "Epoch 8, Batch 718, LR 0.000093 Loss 9.912795, Accuracy 63.264%\n",
      "Epoch 8, Batch 719, LR 0.000093 Loss 9.911109, Accuracy 63.274%\n",
      "Epoch 8, Batch 720, LR 0.000093 Loss 9.910603, Accuracy 63.278%\n",
      "Epoch 8, Batch 721, LR 0.000093 Loss 9.910863, Accuracy 63.275%\n",
      "Epoch 8, Batch 722, LR 0.000093 Loss 9.910769, Accuracy 63.272%\n",
      "Epoch 8, Batch 723, LR 0.000093 Loss 9.910484, Accuracy 63.270%\n",
      "Epoch 8, Batch 724, LR 0.000093 Loss 9.909427, Accuracy 63.276%\n",
      "Epoch 8, Batch 725, LR 0.000093 Loss 9.909389, Accuracy 63.274%\n",
      "Epoch 8, Batch 726, LR 0.000093 Loss 9.908691, Accuracy 63.286%\n",
      "Epoch 8, Batch 727, LR 0.000093 Loss 9.907471, Accuracy 63.293%\n",
      "Epoch 8, Batch 728, LR 0.000093 Loss 9.906782, Accuracy 63.298%\n",
      "Epoch 8, Batch 729, LR 0.000093 Loss 9.906822, Accuracy 63.292%\n",
      "Epoch 8, Batch 730, LR 0.000093 Loss 9.905396, Accuracy 63.305%\n",
      "Epoch 8, Batch 731, LR 0.000093 Loss 9.904514, Accuracy 63.313%\n",
      "Epoch 8, Batch 732, LR 0.000093 Loss 9.905718, Accuracy 63.310%\n",
      "Epoch 8, Batch 733, LR 0.000093 Loss 9.904584, Accuracy 63.321%\n",
      "Epoch 8, Batch 734, LR 0.000093 Loss 9.905367, Accuracy 63.313%\n",
      "Epoch 8, Batch 735, LR 0.000093 Loss 9.905399, Accuracy 63.315%\n",
      "Epoch 8, Batch 736, LR 0.000093 Loss 9.903758, Accuracy 63.326%\n",
      "Epoch 8, Batch 737, LR 0.000093 Loss 9.903820, Accuracy 63.324%\n",
      "Epoch 8, Batch 738, LR 0.000093 Loss 9.903181, Accuracy 63.333%\n",
      "Epoch 8, Batch 739, LR 0.000093 Loss 9.902575, Accuracy 63.340%\n",
      "Epoch 8, Batch 740, LR 0.000093 Loss 9.902636, Accuracy 63.344%\n",
      "Epoch 8, Batch 741, LR 0.000093 Loss 9.902539, Accuracy 63.341%\n",
      "Epoch 8, Batch 742, LR 0.000093 Loss 9.902467, Accuracy 63.338%\n",
      "Epoch 8, Batch 743, LR 0.000093 Loss 9.902316, Accuracy 63.337%\n",
      "Epoch 8, Batch 744, LR 0.000093 Loss 9.902127, Accuracy 63.336%\n",
      "Epoch 8, Batch 745, LR 0.000092 Loss 9.901184, Accuracy 63.341%\n",
      "Epoch 8, Batch 746, LR 0.000092 Loss 9.901145, Accuracy 63.344%\n",
      "Epoch 8, Batch 747, LR 0.000092 Loss 9.900239, Accuracy 63.349%\n",
      "Epoch 8, Batch 748, LR 0.000092 Loss 9.900298, Accuracy 63.349%\n",
      "Epoch 8, Batch 749, LR 0.000092 Loss 9.899078, Accuracy 63.355%\n",
      "Epoch 8, Batch 750, LR 0.000092 Loss 9.898510, Accuracy 63.364%\n",
      "Epoch 8, Batch 751, LR 0.000092 Loss 9.898366, Accuracy 63.366%\n",
      "Epoch 8, Batch 752, LR 0.000092 Loss 9.899079, Accuracy 63.356%\n",
      "Epoch 8, Batch 753, LR 0.000092 Loss 9.898078, Accuracy 63.361%\n",
      "Epoch 8, Batch 754, LR 0.000092 Loss 9.897483, Accuracy 63.369%\n",
      "Epoch 8, Batch 755, LR 0.000092 Loss 9.897333, Accuracy 63.374%\n",
      "Epoch 8, Batch 756, LR 0.000092 Loss 9.897674, Accuracy 63.373%\n",
      "Epoch 8, Batch 757, LR 0.000092 Loss 9.896181, Accuracy 63.380%\n",
      "Epoch 8, Batch 758, LR 0.000092 Loss 9.895782, Accuracy 63.384%\n",
      "Epoch 8, Batch 759, LR 0.000092 Loss 9.895564, Accuracy 63.388%\n",
      "Epoch 8, Batch 760, LR 0.000092 Loss 9.895578, Accuracy 63.382%\n",
      "Epoch 8, Batch 761, LR 0.000092 Loss 9.895767, Accuracy 63.385%\n",
      "Epoch 8, Batch 762, LR 0.000092 Loss 9.895890, Accuracy 63.385%\n",
      "Epoch 8, Batch 763, LR 0.000092 Loss 9.895428, Accuracy 63.387%\n",
      "Epoch 8, Batch 764, LR 0.000092 Loss 9.895391, Accuracy 63.384%\n",
      "Epoch 8, Batch 765, LR 0.000092 Loss 9.895386, Accuracy 63.377%\n",
      "Epoch 8, Batch 766, LR 0.000092 Loss 9.894861, Accuracy 63.376%\n",
      "Epoch 8, Batch 767, LR 0.000092 Loss 9.894588, Accuracy 63.375%\n",
      "Epoch 8, Batch 768, LR 0.000092 Loss 9.894775, Accuracy 63.372%\n",
      "Epoch 8, Batch 769, LR 0.000092 Loss 9.894472, Accuracy 63.373%\n",
      "Epoch 8, Batch 770, LR 0.000092 Loss 9.893652, Accuracy 63.377%\n",
      "Epoch 8, Batch 771, LR 0.000092 Loss 9.894783, Accuracy 63.370%\n",
      "Epoch 8, Batch 772, LR 0.000092 Loss 9.894138, Accuracy 63.373%\n",
      "Epoch 8, Batch 773, LR 0.000092 Loss 9.893318, Accuracy 63.376%\n",
      "Epoch 8, Batch 774, LR 0.000092 Loss 9.893162, Accuracy 63.378%\n",
      "Epoch 8, Batch 775, LR 0.000092 Loss 9.892267, Accuracy 63.385%\n",
      "Epoch 8, Batch 776, LR 0.000092 Loss 9.892461, Accuracy 63.385%\n",
      "Epoch 8, Batch 777, LR 0.000092 Loss 9.893478, Accuracy 63.382%\n",
      "Epoch 8, Batch 778, LR 0.000092 Loss 9.892698, Accuracy 63.390%\n",
      "Epoch 8, Batch 779, LR 0.000092 Loss 9.893576, Accuracy 63.388%\n",
      "Epoch 8, Batch 780, LR 0.000092 Loss 9.893199, Accuracy 63.392%\n",
      "Epoch 8, Batch 781, LR 0.000092 Loss 9.893919, Accuracy 63.393%\n",
      "Epoch 8, Batch 782, LR 0.000092 Loss 9.893521, Accuracy 63.397%\n",
      "Epoch 8, Batch 783, LR 0.000092 Loss 9.893242, Accuracy 63.399%\n",
      "Epoch 8, Batch 784, LR 0.000092 Loss 9.893113, Accuracy 63.397%\n",
      "Epoch 8, Batch 785, LR 0.000092 Loss 9.893147, Accuracy 63.395%\n",
      "Epoch 8, Batch 786, LR 0.000092 Loss 9.893963, Accuracy 63.386%\n",
      "Epoch 8, Batch 787, LR 0.000092 Loss 9.893032, Accuracy 63.395%\n",
      "Epoch 8, Batch 788, LR 0.000092 Loss 9.893211, Accuracy 63.388%\n",
      "Epoch 8, Batch 789, LR 0.000092 Loss 9.892705, Accuracy 63.392%\n",
      "Epoch 8, Batch 790, LR 0.000092 Loss 9.891869, Accuracy 63.396%\n",
      "Epoch 8, Batch 791, LR 0.000092 Loss 9.891585, Accuracy 63.394%\n",
      "Epoch 8, Batch 792, LR 0.000092 Loss 9.892204, Accuracy 63.390%\n",
      "Epoch 8, Batch 793, LR 0.000092 Loss 9.891503, Accuracy 63.395%\n",
      "Epoch 8, Batch 794, LR 0.000092 Loss 9.889966, Accuracy 63.407%\n",
      "Epoch 8, Batch 795, LR 0.000092 Loss 9.890395, Accuracy 63.401%\n",
      "Epoch 8, Batch 796, LR 0.000092 Loss 9.890591, Accuracy 63.401%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 797, LR 0.000092 Loss 9.890271, Accuracy 63.411%\n",
      "Epoch 8, Batch 798, LR 0.000092 Loss 9.891166, Accuracy 63.401%\n",
      "Epoch 8, Batch 799, LR 0.000092 Loss 9.891586, Accuracy 63.402%\n",
      "Epoch 8, Batch 800, LR 0.000092 Loss 9.891679, Accuracy 63.403%\n",
      "Epoch 8, Batch 801, LR 0.000092 Loss 9.891613, Accuracy 63.404%\n",
      "Epoch 8, Batch 802, LR 0.000092 Loss 9.891294, Accuracy 63.408%\n",
      "Epoch 8, Batch 803, LR 0.000092 Loss 9.890441, Accuracy 63.407%\n",
      "Epoch 8, Batch 804, LR 0.000092 Loss 9.890649, Accuracy 63.403%\n",
      "Epoch 8, Batch 805, LR 0.000092 Loss 9.890306, Accuracy 63.402%\n",
      "Epoch 8, Batch 806, LR 0.000092 Loss 9.889943, Accuracy 63.403%\n",
      "Epoch 8, Batch 807, LR 0.000092 Loss 9.890666, Accuracy 63.393%\n",
      "Epoch 8, Batch 808, LR 0.000092 Loss 9.890197, Accuracy 63.399%\n",
      "Epoch 8, Batch 809, LR 0.000092 Loss 9.889881, Accuracy 63.399%\n",
      "Epoch 8, Batch 810, LR 0.000092 Loss 9.889233, Accuracy 63.404%\n",
      "Epoch 8, Batch 811, LR 0.000092 Loss 9.889422, Accuracy 63.405%\n",
      "Epoch 8, Batch 812, LR 0.000092 Loss 9.889357, Accuracy 63.402%\n",
      "Epoch 8, Batch 813, LR 0.000092 Loss 9.889149, Accuracy 63.401%\n",
      "Epoch 8, Batch 814, LR 0.000092 Loss 9.889796, Accuracy 63.399%\n",
      "Epoch 8, Batch 815, LR 0.000092 Loss 9.890255, Accuracy 63.392%\n",
      "Epoch 8, Batch 816, LR 0.000092 Loss 9.890753, Accuracy 63.385%\n",
      "Epoch 8, Batch 817, LR 0.000092 Loss 9.890772, Accuracy 63.385%\n",
      "Epoch 8, Batch 818, LR 0.000092 Loss 9.890717, Accuracy 63.386%\n",
      "Epoch 8, Batch 819, LR 0.000092 Loss 9.890554, Accuracy 63.381%\n",
      "Epoch 8, Batch 820, LR 0.000092 Loss 9.891010, Accuracy 63.374%\n",
      "Epoch 8, Batch 821, LR 0.000092 Loss 9.890545, Accuracy 63.379%\n",
      "Epoch 8, Batch 822, LR 0.000092 Loss 9.890678, Accuracy 63.381%\n",
      "Epoch 8, Batch 823, LR 0.000092 Loss 9.890260, Accuracy 63.379%\n",
      "Epoch 8, Batch 824, LR 0.000092 Loss 9.890042, Accuracy 63.383%\n",
      "Epoch 8, Batch 825, LR 0.000092 Loss 9.889943, Accuracy 63.386%\n",
      "Epoch 8, Batch 826, LR 0.000092 Loss 9.890978, Accuracy 63.376%\n",
      "Epoch 8, Batch 827, LR 0.000092 Loss 9.890981, Accuracy 63.379%\n",
      "Epoch 8, Batch 828, LR 0.000092 Loss 9.889268, Accuracy 63.389%\n",
      "Epoch 8, Batch 829, LR 0.000092 Loss 9.888856, Accuracy 63.392%\n",
      "Epoch 8, Batch 830, LR 0.000092 Loss 9.888502, Accuracy 63.389%\n",
      "Epoch 8, Batch 831, LR 0.000092 Loss 9.888346, Accuracy 63.389%\n",
      "Epoch 8, Batch 832, LR 0.000092 Loss 9.888492, Accuracy 63.385%\n",
      "Epoch 8, Batch 833, LR 0.000092 Loss 9.888378, Accuracy 63.387%\n",
      "Epoch 8, Batch 834, LR 0.000092 Loss 9.887717, Accuracy 63.390%\n",
      "Epoch 8, Batch 835, LR 0.000092 Loss 9.886964, Accuracy 63.387%\n",
      "Epoch 8, Batch 836, LR 0.000092 Loss 9.885629, Accuracy 63.394%\n",
      "Epoch 8, Batch 837, LR 0.000092 Loss 9.885506, Accuracy 63.391%\n",
      "Epoch 8, Batch 838, LR 0.000092 Loss 9.886290, Accuracy 63.387%\n",
      "Epoch 8, Batch 839, LR 0.000092 Loss 9.885076, Accuracy 63.394%\n",
      "Epoch 8, Batch 840, LR 0.000092 Loss 9.883719, Accuracy 63.401%\n",
      "Epoch 8, Batch 841, LR 0.000092 Loss 9.882272, Accuracy 63.408%\n",
      "Epoch 8, Batch 842, LR 0.000092 Loss 9.881797, Accuracy 63.417%\n",
      "Epoch 8, Batch 843, LR 0.000092 Loss 9.882284, Accuracy 63.411%\n",
      "Epoch 8, Batch 844, LR 0.000092 Loss 9.882388, Accuracy 63.415%\n",
      "Epoch 8, Batch 845, LR 0.000092 Loss 9.881849, Accuracy 63.417%\n",
      "Epoch 8, Batch 846, LR 0.000092 Loss 9.881882, Accuracy 63.417%\n",
      "Epoch 8, Batch 847, LR 0.000092 Loss 9.881244, Accuracy 63.423%\n",
      "Epoch 8, Batch 848, LR 0.000092 Loss 9.881906, Accuracy 63.419%\n",
      "Epoch 8, Batch 849, LR 0.000092 Loss 9.881316, Accuracy 63.420%\n",
      "Epoch 8, Batch 850, LR 0.000092 Loss 9.880628, Accuracy 63.425%\n",
      "Epoch 8, Batch 851, LR 0.000092 Loss 9.879576, Accuracy 63.435%\n",
      "Epoch 8, Batch 852, LR 0.000092 Loss 9.877935, Accuracy 63.444%\n",
      "Epoch 8, Batch 853, LR 0.000092 Loss 9.877205, Accuracy 63.451%\n",
      "Epoch 8, Batch 854, LR 0.000092 Loss 9.877732, Accuracy 63.447%\n",
      "Epoch 8, Batch 855, LR 0.000092 Loss 9.877347, Accuracy 63.452%\n",
      "Epoch 8, Batch 856, LR 0.000092 Loss 9.877647, Accuracy 63.447%\n",
      "Epoch 8, Batch 857, LR 0.000092 Loss 9.877677, Accuracy 63.451%\n",
      "Epoch 8, Batch 858, LR 0.000092 Loss 9.877174, Accuracy 63.452%\n",
      "Epoch 8, Batch 859, LR 0.000092 Loss 9.876785, Accuracy 63.450%\n",
      "Epoch 8, Batch 860, LR 0.000092 Loss 9.876516, Accuracy 63.453%\n",
      "Epoch 8, Batch 861, LR 0.000092 Loss 9.876864, Accuracy 63.449%\n",
      "Epoch 8, Batch 862, LR 0.000092 Loss 9.876933, Accuracy 63.446%\n",
      "Epoch 8, Batch 863, LR 0.000092 Loss 9.877182, Accuracy 63.442%\n",
      "Epoch 8, Batch 864, LR 0.000092 Loss 9.876417, Accuracy 63.445%\n",
      "Epoch 8, Batch 865, LR 0.000092 Loss 9.875391, Accuracy 63.456%\n",
      "Epoch 8, Batch 866, LR 0.000092 Loss 9.875096, Accuracy 63.456%\n",
      "Epoch 8, Batch 867, LR 0.000092 Loss 9.874740, Accuracy 63.461%\n",
      "Epoch 8, Batch 868, LR 0.000092 Loss 9.874842, Accuracy 63.463%\n",
      "Epoch 8, Batch 869, LR 0.000092 Loss 9.875714, Accuracy 63.455%\n",
      "Epoch 8, Batch 870, LR 0.000092 Loss 9.874913, Accuracy 63.462%\n",
      "Epoch 8, Batch 871, LR 0.000092 Loss 9.874254, Accuracy 63.465%\n",
      "Epoch 8, Batch 872, LR 0.000092 Loss 9.874704, Accuracy 63.464%\n",
      "Epoch 8, Batch 873, LR 0.000092 Loss 9.875010, Accuracy 63.456%\n",
      "Epoch 8, Batch 874, LR 0.000092 Loss 9.874672, Accuracy 63.457%\n",
      "Epoch 8, Batch 875, LR 0.000092 Loss 9.874662, Accuracy 63.453%\n",
      "Epoch 8, Batch 876, LR 0.000092 Loss 9.874167, Accuracy 63.452%\n",
      "Epoch 8, Batch 877, LR 0.000092 Loss 9.873807, Accuracy 63.454%\n",
      "Epoch 8, Batch 878, LR 0.000092 Loss 9.873352, Accuracy 63.454%\n",
      "Epoch 8, Batch 879, LR 0.000092 Loss 9.873574, Accuracy 63.452%\n",
      "Epoch 8, Batch 880, LR 0.000092 Loss 9.873287, Accuracy 63.455%\n",
      "Epoch 8, Batch 881, LR 0.000092 Loss 9.872761, Accuracy 63.459%\n",
      "Epoch 8, Batch 882, LR 0.000092 Loss 9.873293, Accuracy 63.459%\n",
      "Epoch 8, Batch 883, LR 0.000092 Loss 9.872899, Accuracy 63.464%\n",
      "Epoch 8, Batch 884, LR 0.000092 Loss 9.871809, Accuracy 63.471%\n",
      "Epoch 8, Batch 885, LR 0.000092 Loss 9.871919, Accuracy 63.472%\n",
      "Epoch 8, Batch 886, LR 0.000092 Loss 9.872072, Accuracy 63.472%\n",
      "Epoch 8, Batch 887, LR 0.000092 Loss 9.871877, Accuracy 63.475%\n",
      "Epoch 8, Batch 888, LR 0.000092 Loss 9.871445, Accuracy 63.470%\n",
      "Epoch 8, Batch 889, LR 0.000092 Loss 9.871129, Accuracy 63.470%\n",
      "Epoch 8, Batch 890, LR 0.000092 Loss 9.870997, Accuracy 63.471%\n",
      "Epoch 8, Batch 891, LR 0.000092 Loss 9.871291, Accuracy 63.468%\n",
      "Epoch 8, Batch 892, LR 0.000092 Loss 9.871362, Accuracy 63.469%\n",
      "Epoch 8, Batch 893, LR 0.000092 Loss 9.870372, Accuracy 63.470%\n",
      "Epoch 8, Batch 894, LR 0.000092 Loss 9.868953, Accuracy 63.474%\n",
      "Epoch 8, Batch 895, LR 0.000092 Loss 9.869263, Accuracy 63.468%\n",
      "Epoch 8, Batch 896, LR 0.000092 Loss 9.868705, Accuracy 63.471%\n",
      "Epoch 8, Batch 897, LR 0.000092 Loss 9.868498, Accuracy 63.475%\n",
      "Epoch 8, Batch 898, LR 0.000092 Loss 9.867653, Accuracy 63.482%\n",
      "Epoch 8, Batch 899, LR 0.000092 Loss 9.867072, Accuracy 63.487%\n",
      "Epoch 8, Batch 900, LR 0.000092 Loss 9.865607, Accuracy 63.500%\n",
      "Epoch 8, Batch 901, LR 0.000092 Loss 9.865499, Accuracy 63.501%\n",
      "Epoch 8, Batch 902, LR 0.000092 Loss 9.864282, Accuracy 63.508%\n",
      "Epoch 8, Batch 903, LR 0.000092 Loss 9.863458, Accuracy 63.512%\n",
      "Epoch 8, Batch 904, LR 0.000092 Loss 9.862714, Accuracy 63.521%\n",
      "Epoch 8, Batch 905, LR 0.000092 Loss 9.862552, Accuracy 63.523%\n",
      "Epoch 8, Batch 906, LR 0.000092 Loss 9.861864, Accuracy 63.524%\n",
      "Epoch 8, Batch 907, LR 0.000092 Loss 9.862161, Accuracy 63.522%\n",
      "Epoch 8, Batch 908, LR 0.000092 Loss 9.862745, Accuracy 63.518%\n",
      "Epoch 8, Batch 909, LR 0.000092 Loss 9.862596, Accuracy 63.514%\n",
      "Epoch 8, Batch 910, LR 0.000092 Loss 9.862296, Accuracy 63.514%\n",
      "Epoch 8, Batch 911, LR 0.000092 Loss 9.861884, Accuracy 63.512%\n",
      "Epoch 8, Batch 912, LR 0.000092 Loss 9.861828, Accuracy 63.511%\n",
      "Epoch 8, Batch 913, LR 0.000092 Loss 9.862152, Accuracy 63.507%\n",
      "Epoch 8, Batch 914, LR 0.000092 Loss 9.862317, Accuracy 63.504%\n",
      "Epoch 8, Batch 915, LR 0.000092 Loss 9.861522, Accuracy 63.512%\n",
      "Epoch 8, Batch 916, LR 0.000092 Loss 9.860477, Accuracy 63.514%\n",
      "Epoch 8, Batch 917, LR 0.000092 Loss 9.860247, Accuracy 63.516%\n",
      "Epoch 8, Batch 918, LR 0.000092 Loss 9.860774, Accuracy 63.520%\n",
      "Epoch 8, Batch 919, LR 0.000092 Loss 9.861032, Accuracy 63.514%\n",
      "Epoch 8, Batch 920, LR 0.000092 Loss 9.860948, Accuracy 63.516%\n",
      "Epoch 8, Batch 921, LR 0.000092 Loss 9.860251, Accuracy 63.519%\n",
      "Epoch 8, Batch 922, LR 0.000092 Loss 9.860348, Accuracy 63.513%\n",
      "Epoch 8, Batch 923, LR 0.000092 Loss 9.859718, Accuracy 63.521%\n",
      "Epoch 8, Batch 924, LR 0.000092 Loss 9.858797, Accuracy 63.525%\n",
      "Epoch 8, Batch 925, LR 0.000092 Loss 9.858793, Accuracy 63.523%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Batch 926, LR 0.000092 Loss 9.859414, Accuracy 63.523%\n",
      "Epoch 8, Batch 927, LR 0.000092 Loss 9.858510, Accuracy 63.525%\n",
      "Epoch 8, Batch 928, LR 0.000092 Loss 9.857588, Accuracy 63.525%\n",
      "Epoch 8, Batch 929, LR 0.000092 Loss 9.857131, Accuracy 63.530%\n",
      "Epoch 8, Batch 930, LR 0.000092 Loss 9.856245, Accuracy 63.531%\n",
      "Epoch 8, Batch 931, LR 0.000092 Loss 9.856153, Accuracy 63.531%\n",
      "Epoch 8, Batch 932, LR 0.000092 Loss 9.856217, Accuracy 63.531%\n",
      "Epoch 8, Batch 933, LR 0.000092 Loss 9.855594, Accuracy 63.532%\n",
      "Epoch 8, Batch 934, LR 0.000092 Loss 9.855317, Accuracy 63.536%\n",
      "Epoch 8, Batch 935, LR 0.000092 Loss 9.854982, Accuracy 63.540%\n",
      "Epoch 8, Batch 936, LR 0.000092 Loss 9.854070, Accuracy 63.545%\n",
      "Epoch 8, Batch 937, LR 0.000092 Loss 9.853232, Accuracy 63.549%\n",
      "Epoch 8, Batch 938, LR 0.000092 Loss 9.853087, Accuracy 63.552%\n",
      "Epoch 8, Batch 939, LR 0.000092 Loss 9.853633, Accuracy 63.550%\n",
      "Epoch 8, Batch 940, LR 0.000092 Loss 9.853219, Accuracy 63.551%\n",
      "Epoch 8, Batch 941, LR 0.000092 Loss 9.852995, Accuracy 63.554%\n",
      "Epoch 8, Batch 942, LR 0.000092 Loss 9.853635, Accuracy 63.553%\n",
      "Epoch 8, Batch 943, LR 0.000092 Loss 9.853259, Accuracy 63.558%\n",
      "Epoch 8, Batch 944, LR 0.000092 Loss 9.852498, Accuracy 63.558%\n",
      "Epoch 8, Batch 945, LR 0.000092 Loss 9.852635, Accuracy 63.552%\n",
      "Epoch 8, Batch 946, LR 0.000092 Loss 9.851390, Accuracy 63.560%\n",
      "Epoch 8, Batch 947, LR 0.000092 Loss 9.851784, Accuracy 63.554%\n",
      "Epoch 8, Batch 948, LR 0.000092 Loss 9.851141, Accuracy 63.557%\n",
      "Epoch 8, Batch 949, LR 0.000092 Loss 9.850708, Accuracy 63.556%\n",
      "Epoch 8, Batch 950, LR 0.000092 Loss 9.850628, Accuracy 63.552%\n",
      "Epoch 8, Batch 951, LR 0.000092 Loss 9.850143, Accuracy 63.549%\n",
      "Epoch 8, Batch 952, LR 0.000092 Loss 9.850082, Accuracy 63.552%\n",
      "Epoch 8, Batch 953, LR 0.000092 Loss 9.850204, Accuracy 63.548%\n",
      "Epoch 8, Batch 954, LR 0.000092 Loss 9.849501, Accuracy 63.551%\n",
      "Epoch 8, Batch 955, LR 0.000092 Loss 9.849282, Accuracy 63.554%\n",
      "Epoch 8, Batch 956, LR 0.000092 Loss 9.848992, Accuracy 63.554%\n",
      "Epoch 8, Batch 957, LR 0.000092 Loss 9.849003, Accuracy 63.555%\n",
      "Epoch 8, Batch 958, LR 0.000092 Loss 9.849113, Accuracy 63.552%\n",
      "Epoch 8, Batch 959, LR 0.000092 Loss 9.847949, Accuracy 63.561%\n",
      "Epoch 8, Batch 960, LR 0.000092 Loss 9.846869, Accuracy 63.569%\n",
      "Epoch 8, Batch 961, LR 0.000092 Loss 9.846490, Accuracy 63.573%\n",
      "Epoch 8, Batch 962, LR 0.000092 Loss 9.846322, Accuracy 63.574%\n",
      "Epoch 8, Batch 963, LR 0.000092 Loss 9.845854, Accuracy 63.575%\n",
      "Epoch 8, Batch 964, LR 0.000092 Loss 9.845806, Accuracy 63.572%\n",
      "Epoch 8, Batch 965, LR 0.000092 Loss 9.845446, Accuracy 63.572%\n",
      "Epoch 8, Batch 966, LR 0.000092 Loss 9.845540, Accuracy 63.573%\n",
      "Epoch 8, Batch 967, LR 0.000092 Loss 9.845031, Accuracy 63.579%\n",
      "Epoch 8, Batch 968, LR 0.000092 Loss 9.844788, Accuracy 63.581%\n",
      "Epoch 8, Batch 969, LR 0.000092 Loss 9.844115, Accuracy 63.582%\n",
      "Epoch 8, Batch 970, LR 0.000092 Loss 9.843704, Accuracy 63.588%\n",
      "Epoch 8, Batch 971, LR 0.000092 Loss 9.842639, Accuracy 63.593%\n",
      "Epoch 8, Batch 972, LR 0.000092 Loss 9.843242, Accuracy 63.584%\n",
      "Epoch 8, Batch 973, LR 0.000092 Loss 9.843339, Accuracy 63.580%\n",
      "Epoch 8, Batch 974, LR 0.000092 Loss 9.843747, Accuracy 63.572%\n",
      "Epoch 8, Batch 975, LR 0.000092 Loss 9.842481, Accuracy 63.585%\n",
      "Epoch 8, Batch 976, LR 0.000092 Loss 9.842715, Accuracy 63.586%\n",
      "Epoch 8, Batch 977, LR 0.000092 Loss 9.842083, Accuracy 63.592%\n",
      "Epoch 8, Batch 978, LR 0.000092 Loss 9.841744, Accuracy 63.595%\n",
      "Epoch 8, Batch 979, LR 0.000092 Loss 9.842043, Accuracy 63.592%\n",
      "Epoch 8, Batch 980, LR 0.000092 Loss 9.842067, Accuracy 63.591%\n",
      "Epoch 8, Batch 981, LR 0.000092 Loss 9.841603, Accuracy 63.598%\n",
      "Epoch 8, Batch 982, LR 0.000092 Loss 9.840876, Accuracy 63.603%\n",
      "Epoch 8, Batch 983, LR 0.000092 Loss 9.840260, Accuracy 63.611%\n",
      "Epoch 8, Batch 984, LR 0.000092 Loss 9.839158, Accuracy 63.614%\n",
      "Epoch 8, Batch 985, LR 0.000092 Loss 9.838452, Accuracy 63.618%\n",
      "Epoch 8, Batch 986, LR 0.000092 Loss 9.838364, Accuracy 63.618%\n",
      "Epoch 8, Batch 987, LR 0.000092 Loss 9.838898, Accuracy 63.614%\n",
      "Epoch 8, Batch 988, LR 0.000092 Loss 9.838349, Accuracy 63.615%\n",
      "Epoch 8, Batch 989, LR 0.000092 Loss 9.838366, Accuracy 63.619%\n",
      "Epoch 8, Batch 990, LR 0.000092 Loss 9.838061, Accuracy 63.620%\n",
      "Epoch 8, Batch 991, LR 0.000092 Loss 9.837271, Accuracy 63.630%\n",
      "Epoch 8, Batch 992, LR 0.000092 Loss 9.837402, Accuracy 63.629%\n",
      "Epoch 8, Batch 993, LR 0.000092 Loss 9.837464, Accuracy 63.624%\n",
      "Epoch 8, Batch 994, LR 0.000092 Loss 9.838013, Accuracy 63.622%\n",
      "Epoch 8, Batch 995, LR 0.000092 Loss 9.838189, Accuracy 63.626%\n",
      "Epoch 8, Batch 996, LR 0.000092 Loss 9.838178, Accuracy 63.626%\n",
      "Epoch 8, Batch 997, LR 0.000092 Loss 9.837967, Accuracy 63.625%\n",
      "Epoch 8, Batch 998, LR 0.000092 Loss 9.838302, Accuracy 63.625%\n",
      "Epoch 8, Batch 999, LR 0.000092 Loss 9.837977, Accuracy 63.625%\n",
      "Epoch 8, Batch 1000, LR 0.000092 Loss 9.838931, Accuracy 63.619%\n",
      "Epoch 8, Batch 1001, LR 0.000092 Loss 9.838996, Accuracy 63.617%\n",
      "Epoch 8, Batch 1002, LR 0.000092 Loss 9.838900, Accuracy 63.617%\n",
      "Epoch 8, Batch 1003, LR 0.000092 Loss 9.838471, Accuracy 63.626%\n",
      "Epoch 8, Batch 1004, LR 0.000092 Loss 9.837331, Accuracy 63.632%\n",
      "Epoch 8, Batch 1005, LR 0.000092 Loss 9.837013, Accuracy 63.633%\n",
      "Epoch 8, Batch 1006, LR 0.000092 Loss 9.837018, Accuracy 63.631%\n",
      "Epoch 8, Batch 1007, LR 0.000092 Loss 9.837719, Accuracy 63.626%\n",
      "Epoch 8, Batch 1008, LR 0.000092 Loss 9.836547, Accuracy 63.628%\n",
      "Epoch 8, Batch 1009, LR 0.000092 Loss 9.836032, Accuracy 63.628%\n",
      "Epoch 8, Batch 1010, LR 0.000092 Loss 9.835314, Accuracy 63.633%\n",
      "Epoch 8, Batch 1011, LR 0.000092 Loss 9.834182, Accuracy 63.641%\n",
      "Epoch 8, Batch 1012, LR 0.000092 Loss 9.834020, Accuracy 63.643%\n",
      "Epoch 8, Batch 1013, LR 0.000092 Loss 9.834595, Accuracy 63.638%\n",
      "Epoch 8, Batch 1014, LR 0.000092 Loss 9.834392, Accuracy 63.640%\n",
      "Epoch 8, Batch 1015, LR 0.000092 Loss 9.833796, Accuracy 63.641%\n",
      "Epoch 8, Batch 1016, LR 0.000092 Loss 9.832747, Accuracy 63.647%\n",
      "Epoch 8, Batch 1017, LR 0.000092 Loss 9.832262, Accuracy 63.652%\n",
      "Epoch 8, Batch 1018, LR 0.000092 Loss 9.831802, Accuracy 63.658%\n",
      "Epoch 8, Batch 1019, LR 0.000092 Loss 9.831650, Accuracy 63.657%\n",
      "Epoch 8, Batch 1020, LR 0.000092 Loss 9.831935, Accuracy 63.656%\n",
      "Epoch 8, Batch 1021, LR 0.000092 Loss 9.832173, Accuracy 63.653%\n",
      "Epoch 8, Batch 1022, LR 0.000092 Loss 9.832190, Accuracy 63.650%\n",
      "Epoch 8, Batch 1023, LR 0.000092 Loss 9.832534, Accuracy 63.652%\n",
      "Epoch 8, Batch 1024, LR 0.000092 Loss 9.833207, Accuracy 63.645%\n",
      "Epoch 8, Batch 1025, LR 0.000092 Loss 9.833204, Accuracy 63.650%\n",
      "Epoch 8, Batch 1026, LR 0.000092 Loss 9.833184, Accuracy 63.651%\n",
      "Epoch 8, Batch 1027, LR 0.000092 Loss 9.832522, Accuracy 63.659%\n",
      "Epoch 8, Batch 1028, LR 0.000092 Loss 9.831483, Accuracy 63.663%\n",
      "Epoch 8, Batch 1029, LR 0.000092 Loss 9.830971, Accuracy 63.667%\n",
      "Epoch 8, Batch 1030, LR 0.000092 Loss 9.830680, Accuracy 63.667%\n",
      "Epoch 8, Batch 1031, LR 0.000092 Loss 9.831115, Accuracy 63.664%\n",
      "Epoch 8, Batch 1032, LR 0.000092 Loss 9.830129, Accuracy 63.672%\n",
      "Epoch 8, Batch 1033, LR 0.000092 Loss 9.829885, Accuracy 63.672%\n",
      "Epoch 8, Batch 1034, LR 0.000092 Loss 9.829636, Accuracy 63.673%\n",
      "Epoch 8, Batch 1035, LR 0.000092 Loss 9.829693, Accuracy 63.673%\n",
      "Epoch 8, Batch 1036, LR 0.000092 Loss 9.829372, Accuracy 63.673%\n",
      "Epoch 8, Batch 1037, LR 0.000092 Loss 9.828918, Accuracy 63.675%\n",
      "Epoch 8, Batch 1038, LR 0.000092 Loss 9.829145, Accuracy 63.677%\n",
      "Epoch 8, Batch 1039, LR 0.000092 Loss 9.829341, Accuracy 63.678%\n",
      "Epoch 8, Batch 1040, LR 0.000092 Loss 9.828968, Accuracy 63.680%\n",
      "Epoch 8, Batch 1041, LR 0.000092 Loss 9.829115, Accuracy 63.678%\n",
      "Epoch 8, Batch 1042, LR 0.000092 Loss 9.828654, Accuracy 63.679%\n",
      "Epoch 8, Batch 1043, LR 0.000092 Loss 9.828690, Accuracy 63.674%\n",
      "Epoch 8, Batch 1044, LR 0.000092 Loss 9.828698, Accuracy 63.679%\n",
      "Epoch 8, Batch 1045, LR 0.000092 Loss 9.828168, Accuracy 63.683%\n",
      "Epoch 8, Batch 1046, LR 0.000092 Loss 9.827690, Accuracy 63.685%\n",
      "Epoch 8, Batch 1047, LR 0.000092 Loss 9.827420, Accuracy 63.684%\n",
      "Epoch 8, Loss (train set) 9.827420, Accuracy (train set) 63.684%\n",
      "Epoch 9, Batch 1, LR 0.000092 Loss 9.594043, Accuracy 66.406%\n",
      "Epoch 9, Batch 2, LR 0.000092 Loss 9.600502, Accuracy 64.062%\n",
      "Epoch 9, Batch 3, LR 0.000092 Loss 9.639439, Accuracy 63.802%\n",
      "Epoch 9, Batch 4, LR 0.000092 Loss 9.267569, Accuracy 65.039%\n",
      "Epoch 9, Batch 5, LR 0.000092 Loss 9.224186, Accuracy 66.094%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 6, LR 0.000092 Loss 9.272699, Accuracy 65.755%\n",
      "Epoch 9, Batch 7, LR 0.000092 Loss 9.349443, Accuracy 65.179%\n",
      "Epoch 9, Batch 8, LR 0.000092 Loss 9.205772, Accuracy 66.309%\n",
      "Epoch 9, Batch 9, LR 0.000092 Loss 9.274949, Accuracy 66.319%\n",
      "Epoch 9, Batch 10, LR 0.000092 Loss 9.280754, Accuracy 66.562%\n",
      "Epoch 9, Batch 11, LR 0.000092 Loss 9.287879, Accuracy 66.335%\n",
      "Epoch 9, Batch 12, LR 0.000092 Loss 9.300156, Accuracy 66.341%\n",
      "Epoch 9, Batch 13, LR 0.000092 Loss 9.326829, Accuracy 66.106%\n",
      "Epoch 9, Batch 14, LR 0.000092 Loss 9.391602, Accuracy 65.737%\n",
      "Epoch 9, Batch 15, LR 0.000092 Loss 9.426204, Accuracy 65.781%\n",
      "Epoch 9, Batch 16, LR 0.000092 Loss 9.404419, Accuracy 65.723%\n",
      "Epoch 9, Batch 17, LR 0.000092 Loss 9.410284, Accuracy 65.855%\n",
      "Epoch 9, Batch 18, LR 0.000092 Loss 9.416186, Accuracy 65.538%\n",
      "Epoch 9, Batch 19, LR 0.000092 Loss 9.446645, Accuracy 65.461%\n",
      "Epoch 9, Batch 20, LR 0.000092 Loss 9.397972, Accuracy 65.938%\n",
      "Epoch 9, Batch 21, LR 0.000092 Loss 9.398963, Accuracy 66.071%\n",
      "Epoch 9, Batch 22, LR 0.000092 Loss 9.406211, Accuracy 65.980%\n",
      "Epoch 9, Batch 23, LR 0.000092 Loss 9.434774, Accuracy 65.557%\n",
      "Epoch 9, Batch 24, LR 0.000092 Loss 9.426364, Accuracy 65.495%\n",
      "Epoch 9, Batch 25, LR 0.000092 Loss 9.433334, Accuracy 65.562%\n",
      "Epoch 9, Batch 26, LR 0.000092 Loss 9.458478, Accuracy 65.264%\n",
      "Epoch 9, Batch 27, LR 0.000092 Loss 9.494314, Accuracy 64.873%\n",
      "Epoch 9, Batch 28, LR 0.000092 Loss 9.497255, Accuracy 65.123%\n",
      "Epoch 9, Batch 29, LR 0.000092 Loss 9.491447, Accuracy 65.032%\n",
      "Epoch 9, Batch 30, LR 0.000092 Loss 9.493905, Accuracy 64.870%\n",
      "Epoch 9, Batch 31, LR 0.000092 Loss 9.460923, Accuracy 65.146%\n",
      "Epoch 9, Batch 32, LR 0.000092 Loss 9.462000, Accuracy 65.112%\n",
      "Epoch 9, Batch 33, LR 0.000092 Loss 9.460335, Accuracy 65.080%\n",
      "Epoch 9, Batch 34, LR 0.000092 Loss 9.474829, Accuracy 64.959%\n",
      "Epoch 9, Batch 35, LR 0.000092 Loss 9.455765, Accuracy 65.112%\n",
      "Epoch 9, Batch 36, LR 0.000092 Loss 9.416633, Accuracy 65.451%\n",
      "Epoch 9, Batch 37, LR 0.000092 Loss 9.398471, Accuracy 65.498%\n",
      "Epoch 9, Batch 38, LR 0.000092 Loss 9.427478, Accuracy 65.317%\n",
      "Epoch 9, Batch 39, LR 0.000092 Loss 9.421360, Accuracy 65.365%\n",
      "Epoch 9, Batch 40, LR 0.000092 Loss 9.432633, Accuracy 65.254%\n",
      "Epoch 9, Batch 41, LR 0.000092 Loss 9.430073, Accuracy 65.339%\n",
      "Epoch 9, Batch 42, LR 0.000092 Loss 9.424305, Accuracy 65.365%\n",
      "Epoch 9, Batch 43, LR 0.000092 Loss 9.435070, Accuracy 65.352%\n",
      "Epoch 9, Batch 44, LR 0.000092 Loss 9.436738, Accuracy 65.305%\n",
      "Epoch 9, Batch 45, LR 0.000092 Loss 9.442967, Accuracy 65.330%\n",
      "Epoch 9, Batch 46, LR 0.000092 Loss 9.462169, Accuracy 65.234%\n",
      "Epoch 9, Batch 47, LR 0.000092 Loss 9.466658, Accuracy 65.243%\n",
      "Epoch 9, Batch 48, LR 0.000092 Loss 9.475792, Accuracy 65.137%\n",
      "Epoch 9, Batch 49, LR 0.000092 Loss 9.493892, Accuracy 65.099%\n",
      "Epoch 9, Batch 50, LR 0.000092 Loss 9.479275, Accuracy 65.234%\n",
      "Epoch 9, Batch 51, LR 0.000092 Loss 9.481933, Accuracy 65.150%\n",
      "Epoch 9, Batch 52, LR 0.000092 Loss 9.488884, Accuracy 65.144%\n",
      "Epoch 9, Batch 53, LR 0.000092 Loss 9.489978, Accuracy 65.212%\n",
      "Epoch 9, Batch 54, LR 0.000092 Loss 9.488330, Accuracy 65.278%\n",
      "Epoch 9, Batch 55, LR 0.000092 Loss 9.486989, Accuracy 65.270%\n",
      "Epoch 9, Batch 56, LR 0.000092 Loss 9.477695, Accuracy 65.374%\n",
      "Epoch 9, Batch 57, LR 0.000092 Loss 9.489969, Accuracy 65.269%\n",
      "Epoch 9, Batch 58, LR 0.000092 Loss 9.485213, Accuracy 65.369%\n",
      "Epoch 9, Batch 59, LR 0.000092 Loss 9.489827, Accuracy 65.294%\n",
      "Epoch 9, Batch 60, LR 0.000092 Loss 9.490894, Accuracy 65.299%\n",
      "Epoch 9, Batch 61, LR 0.000092 Loss 9.496788, Accuracy 65.266%\n",
      "Epoch 9, Batch 62, LR 0.000092 Loss 9.507397, Accuracy 65.171%\n",
      "Epoch 9, Batch 63, LR 0.000092 Loss 9.501312, Accuracy 65.290%\n",
      "Epoch 9, Batch 64, LR 0.000092 Loss 9.502532, Accuracy 65.271%\n",
      "Epoch 9, Batch 65, LR 0.000092 Loss 9.510924, Accuracy 65.144%\n",
      "Epoch 9, Batch 66, LR 0.000092 Loss 9.494746, Accuracy 65.246%\n",
      "Epoch 9, Batch 67, LR 0.000092 Loss 9.495263, Accuracy 65.252%\n",
      "Epoch 9, Batch 68, LR 0.000092 Loss 9.509506, Accuracy 65.131%\n",
      "Epoch 9, Batch 69, LR 0.000092 Loss 9.500958, Accuracy 65.172%\n",
      "Epoch 9, Batch 70, LR 0.000092 Loss 9.497663, Accuracy 65.279%\n",
      "Epoch 9, Batch 71, LR 0.000092 Loss 9.496657, Accuracy 65.262%\n",
      "Epoch 9, Batch 72, LR 0.000092 Loss 9.499211, Accuracy 65.289%\n",
      "Epoch 9, Batch 73, LR 0.000092 Loss 9.485894, Accuracy 65.411%\n",
      "Epoch 9, Batch 74, LR 0.000092 Loss 9.472918, Accuracy 65.477%\n",
      "Epoch 9, Batch 75, LR 0.000092 Loss 9.473760, Accuracy 65.438%\n",
      "Epoch 9, Batch 76, LR 0.000092 Loss 9.473994, Accuracy 65.409%\n",
      "Epoch 9, Batch 77, LR 0.000092 Loss 9.481991, Accuracy 65.422%\n",
      "Epoch 9, Batch 78, LR 0.000092 Loss 9.487854, Accuracy 65.395%\n",
      "Epoch 9, Batch 79, LR 0.000092 Loss 9.488914, Accuracy 65.398%\n",
      "Epoch 9, Batch 80, LR 0.000092 Loss 9.508579, Accuracy 65.293%\n",
      "Epoch 9, Batch 81, LR 0.000092 Loss 9.507044, Accuracy 65.316%\n",
      "Epoch 9, Batch 82, LR 0.000092 Loss 9.515761, Accuracy 65.234%\n",
      "Epoch 9, Batch 83, LR 0.000092 Loss 9.516299, Accuracy 65.248%\n",
      "Epoch 9, Batch 84, LR 0.000092 Loss 9.509041, Accuracy 65.365%\n",
      "Epoch 9, Batch 85, LR 0.000092 Loss 9.507425, Accuracy 65.331%\n",
      "Epoch 9, Batch 86, LR 0.000092 Loss 9.502593, Accuracy 65.362%\n",
      "Epoch 9, Batch 87, LR 0.000092 Loss 9.502555, Accuracy 65.409%\n",
      "Epoch 9, Batch 88, LR 0.000092 Loss 9.507458, Accuracy 65.350%\n",
      "Epoch 9, Batch 89, LR 0.000092 Loss 9.505790, Accuracy 65.291%\n",
      "Epoch 9, Batch 90, LR 0.000092 Loss 9.503853, Accuracy 65.312%\n",
      "Epoch 9, Batch 91, LR 0.000092 Loss 9.506215, Accuracy 65.290%\n",
      "Epoch 9, Batch 92, LR 0.000092 Loss 9.507195, Accuracy 65.353%\n",
      "Epoch 9, Batch 93, LR 0.000092 Loss 9.504984, Accuracy 65.390%\n",
      "Epoch 9, Batch 94, LR 0.000092 Loss 9.499638, Accuracy 65.426%\n",
      "Epoch 9, Batch 95, LR 0.000092 Loss 9.494312, Accuracy 65.452%\n",
      "Epoch 9, Batch 96, LR 0.000092 Loss 9.499722, Accuracy 65.389%\n",
      "Epoch 9, Batch 97, LR 0.000092 Loss 9.504427, Accuracy 65.367%\n",
      "Epoch 9, Batch 98, LR 0.000092 Loss 9.498540, Accuracy 65.402%\n",
      "Epoch 9, Batch 99, LR 0.000092 Loss 9.506034, Accuracy 65.301%\n",
      "Epoch 9, Batch 100, LR 0.000092 Loss 9.503171, Accuracy 65.359%\n",
      "Epoch 9, Batch 101, LR 0.000092 Loss 9.503557, Accuracy 65.385%\n",
      "Epoch 9, Batch 102, LR 0.000092 Loss 9.498108, Accuracy 65.464%\n",
      "Epoch 9, Batch 103, LR 0.000092 Loss 9.492215, Accuracy 65.473%\n",
      "Epoch 9, Batch 104, LR 0.000092 Loss 9.494794, Accuracy 65.460%\n",
      "Epoch 9, Batch 105, LR 0.000092 Loss 9.492634, Accuracy 65.394%\n",
      "Epoch 9, Batch 106, LR 0.000092 Loss 9.486927, Accuracy 65.404%\n",
      "Epoch 9, Batch 107, LR 0.000092 Loss 9.489703, Accuracy 65.399%\n",
      "Epoch 9, Batch 108, LR 0.000092 Loss 9.501415, Accuracy 65.328%\n",
      "Epoch 9, Batch 109, LR 0.000092 Loss 9.496889, Accuracy 65.381%\n",
      "Epoch 9, Batch 110, LR 0.000092 Loss 9.498641, Accuracy 65.369%\n",
      "Epoch 9, Batch 111, LR 0.000092 Loss 9.509641, Accuracy 65.287%\n",
      "Epoch 9, Batch 112, LR 0.000092 Loss 9.512187, Accuracy 65.290%\n",
      "Epoch 9, Batch 113, LR 0.000092 Loss 9.508447, Accuracy 65.362%\n",
      "Epoch 9, Batch 114, LR 0.000092 Loss 9.508719, Accuracy 65.392%\n",
      "Epoch 9, Batch 115, LR 0.000092 Loss 9.514315, Accuracy 65.306%\n",
      "Epoch 9, Batch 116, LR 0.000092 Loss 9.514145, Accuracy 65.315%\n",
      "Epoch 9, Batch 117, LR 0.000092 Loss 9.515470, Accuracy 65.331%\n",
      "Epoch 9, Batch 118, LR 0.000092 Loss 9.516634, Accuracy 65.307%\n",
      "Epoch 9, Batch 119, LR 0.000092 Loss 9.515832, Accuracy 65.316%\n",
      "Epoch 9, Batch 120, LR 0.000092 Loss 9.514415, Accuracy 65.339%\n",
      "Epoch 9, Batch 121, LR 0.000092 Loss 9.517000, Accuracy 65.283%\n",
      "Epoch 9, Batch 122, LR 0.000092 Loss 9.513430, Accuracy 65.324%\n",
      "Epoch 9, Batch 123, LR 0.000092 Loss 9.511797, Accuracy 65.358%\n",
      "Epoch 9, Batch 124, LR 0.000092 Loss 9.514979, Accuracy 65.316%\n",
      "Epoch 9, Batch 125, LR 0.000092 Loss 9.517270, Accuracy 65.281%\n",
      "Epoch 9, Batch 126, LR 0.000092 Loss 9.520467, Accuracy 65.253%\n",
      "Epoch 9, Batch 127, LR 0.000092 Loss 9.523205, Accuracy 65.244%\n",
      "Epoch 9, Batch 128, LR 0.000092 Loss 9.525241, Accuracy 65.216%\n",
      "Epoch 9, Batch 129, LR 0.000092 Loss 9.524113, Accuracy 65.213%\n",
      "Epoch 9, Batch 130, LR 0.000092 Loss 9.511104, Accuracy 65.306%\n",
      "Epoch 9, Batch 131, LR 0.000092 Loss 9.517048, Accuracy 65.285%\n",
      "Epoch 9, Batch 132, LR 0.000092 Loss 9.512162, Accuracy 65.305%\n",
      "Epoch 9, Batch 133, LR 0.000092 Loss 9.505891, Accuracy 65.378%\n",
      "Epoch 9, Batch 134, LR 0.000092 Loss 9.502110, Accuracy 65.374%\n",
      "Epoch 9, Batch 135, LR 0.000092 Loss 9.496775, Accuracy 65.399%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 136, LR 0.000092 Loss 9.491361, Accuracy 65.435%\n",
      "Epoch 9, Batch 137, LR 0.000092 Loss 9.488379, Accuracy 65.465%\n",
      "Epoch 9, Batch 138, LR 0.000092 Loss 9.488736, Accuracy 65.455%\n",
      "Epoch 9, Batch 139, LR 0.000092 Loss 9.483476, Accuracy 65.490%\n",
      "Epoch 9, Batch 140, LR 0.000092 Loss 9.481900, Accuracy 65.513%\n",
      "Epoch 9, Batch 141, LR 0.000092 Loss 9.475796, Accuracy 65.559%\n",
      "Epoch 9, Batch 142, LR 0.000092 Loss 9.476654, Accuracy 65.559%\n",
      "Epoch 9, Batch 143, LR 0.000092 Loss 9.476299, Accuracy 65.587%\n",
      "Epoch 9, Batch 144, LR 0.000092 Loss 9.475451, Accuracy 65.587%\n",
      "Epoch 9, Batch 145, LR 0.000092 Loss 9.474786, Accuracy 65.582%\n",
      "Epoch 9, Batch 146, LR 0.000092 Loss 9.475453, Accuracy 65.593%\n",
      "Epoch 9, Batch 147, LR 0.000092 Loss 9.471512, Accuracy 65.598%\n",
      "Epoch 9, Batch 148, LR 0.000092 Loss 9.466596, Accuracy 65.646%\n",
      "Epoch 9, Batch 149, LR 0.000092 Loss 9.468833, Accuracy 65.620%\n",
      "Epoch 9, Batch 150, LR 0.000092 Loss 9.465836, Accuracy 65.641%\n",
      "Epoch 9, Batch 151, LR 0.000092 Loss 9.460714, Accuracy 65.682%\n",
      "Epoch 9, Batch 152, LR 0.000092 Loss 9.460551, Accuracy 65.666%\n",
      "Epoch 9, Batch 153, LR 0.000092 Loss 9.459423, Accuracy 65.666%\n",
      "Epoch 9, Batch 154, LR 0.000092 Loss 9.458408, Accuracy 65.676%\n",
      "Epoch 9, Batch 155, LR 0.000092 Loss 9.460585, Accuracy 65.670%\n",
      "Epoch 9, Batch 156, LR 0.000092 Loss 9.460436, Accuracy 65.685%\n",
      "Epoch 9, Batch 157, LR 0.000092 Loss 9.460995, Accuracy 65.680%\n",
      "Epoch 9, Batch 158, LR 0.000092 Loss 9.460790, Accuracy 65.694%\n",
      "Epoch 9, Batch 159, LR 0.000092 Loss 9.463955, Accuracy 65.654%\n",
      "Epoch 9, Batch 160, LR 0.000092 Loss 9.467821, Accuracy 65.645%\n",
      "Epoch 9, Batch 161, LR 0.000092 Loss 9.468870, Accuracy 65.640%\n",
      "Epoch 9, Batch 162, LR 0.000092 Loss 9.471049, Accuracy 65.615%\n",
      "Epoch 9, Batch 163, LR 0.000092 Loss 9.473998, Accuracy 65.630%\n",
      "Epoch 9, Batch 164, LR 0.000092 Loss 9.472241, Accuracy 65.658%\n",
      "Epoch 9, Batch 165, LR 0.000091 Loss 9.473255, Accuracy 65.639%\n",
      "Epoch 9, Batch 166, LR 0.000091 Loss 9.474113, Accuracy 65.639%\n",
      "Epoch 9, Batch 167, LR 0.000091 Loss 9.473222, Accuracy 65.662%\n",
      "Epoch 9, Batch 168, LR 0.000091 Loss 9.472025, Accuracy 65.662%\n",
      "Epoch 9, Batch 169, LR 0.000091 Loss 9.472583, Accuracy 65.653%\n",
      "Epoch 9, Batch 170, LR 0.000091 Loss 9.475831, Accuracy 65.630%\n",
      "Epoch 9, Batch 171, LR 0.000091 Loss 9.476997, Accuracy 65.643%\n",
      "Epoch 9, Batch 172, LR 0.000091 Loss 9.473192, Accuracy 65.689%\n",
      "Epoch 9, Batch 173, LR 0.000091 Loss 9.472286, Accuracy 65.684%\n",
      "Epoch 9, Batch 174, LR 0.000091 Loss 9.473728, Accuracy 65.647%\n",
      "Epoch 9, Batch 175, LR 0.000091 Loss 9.472736, Accuracy 65.661%\n",
      "Epoch 9, Batch 176, LR 0.000091 Loss 9.470434, Accuracy 65.674%\n",
      "Epoch 9, Batch 177, LR 0.000091 Loss 9.463622, Accuracy 65.718%\n",
      "Epoch 9, Batch 178, LR 0.000091 Loss 9.465524, Accuracy 65.695%\n",
      "Epoch 9, Batch 179, LR 0.000091 Loss 9.464364, Accuracy 65.704%\n",
      "Epoch 9, Batch 180, LR 0.000091 Loss 9.465383, Accuracy 65.707%\n",
      "Epoch 9, Batch 181, LR 0.000091 Loss 9.464937, Accuracy 65.716%\n",
      "Epoch 9, Batch 182, LR 0.000091 Loss 9.462019, Accuracy 65.728%\n",
      "Epoch 9, Batch 183, LR 0.000091 Loss 9.463368, Accuracy 65.710%\n",
      "Epoch 9, Batch 184, LR 0.000091 Loss 9.460576, Accuracy 65.718%\n",
      "Epoch 9, Batch 185, LR 0.000091 Loss 9.463444, Accuracy 65.693%\n",
      "Epoch 9, Batch 186, LR 0.000091 Loss 9.463646, Accuracy 65.701%\n",
      "Epoch 9, Batch 187, LR 0.000091 Loss 9.463676, Accuracy 65.717%\n",
      "Epoch 9, Batch 188, LR 0.000091 Loss 9.465685, Accuracy 65.700%\n",
      "Epoch 9, Batch 189, LR 0.000091 Loss 9.465676, Accuracy 65.720%\n",
      "Epoch 9, Batch 190, LR 0.000091 Loss 9.461914, Accuracy 65.748%\n",
      "Epoch 9, Batch 191, LR 0.000091 Loss 9.461947, Accuracy 65.735%\n",
      "Epoch 9, Batch 192, LR 0.000091 Loss 9.464158, Accuracy 65.735%\n",
      "Epoch 9, Batch 193, LR 0.000091 Loss 9.467835, Accuracy 65.726%\n",
      "Epoch 9, Batch 194, LR 0.000091 Loss 9.467691, Accuracy 65.722%\n",
      "Epoch 9, Batch 195, LR 0.000091 Loss 9.470078, Accuracy 65.693%\n",
      "Epoch 9, Batch 196, LR 0.000091 Loss 9.472527, Accuracy 65.681%\n",
      "Epoch 9, Batch 197, LR 0.000091 Loss 9.473269, Accuracy 65.669%\n",
      "Epoch 9, Batch 198, LR 0.000091 Loss 9.475551, Accuracy 65.657%\n",
      "Epoch 9, Batch 199, LR 0.000091 Loss 9.473609, Accuracy 65.668%\n",
      "Epoch 9, Batch 200, LR 0.000091 Loss 9.468047, Accuracy 65.707%\n",
      "Epoch 9, Batch 201, LR 0.000091 Loss 9.473112, Accuracy 65.679%\n",
      "Epoch 9, Batch 202, LR 0.000091 Loss 9.471992, Accuracy 65.683%\n",
      "Epoch 9, Batch 203, LR 0.000091 Loss 9.473011, Accuracy 65.663%\n",
      "Epoch 9, Batch 204, LR 0.000091 Loss 9.473890, Accuracy 65.648%\n",
      "Epoch 9, Batch 205, LR 0.000091 Loss 9.470459, Accuracy 65.671%\n",
      "Epoch 9, Batch 206, LR 0.000091 Loss 9.467640, Accuracy 65.682%\n",
      "Epoch 9, Batch 207, LR 0.000091 Loss 9.467547, Accuracy 65.693%\n",
      "Epoch 9, Batch 208, LR 0.000091 Loss 9.467928, Accuracy 65.674%\n",
      "Epoch 9, Batch 209, LR 0.000091 Loss 9.466828, Accuracy 65.674%\n",
      "Epoch 9, Batch 210, LR 0.000091 Loss 9.469419, Accuracy 65.655%\n",
      "Epoch 9, Batch 211, LR 0.000091 Loss 9.470031, Accuracy 65.640%\n",
      "Epoch 9, Batch 212, LR 0.000091 Loss 9.473026, Accuracy 65.621%\n",
      "Epoch 9, Batch 213, LR 0.000091 Loss 9.469021, Accuracy 65.643%\n",
      "Epoch 9, Batch 214, LR 0.000091 Loss 9.470979, Accuracy 65.621%\n",
      "Epoch 9, Batch 215, LR 0.000091 Loss 9.470356, Accuracy 65.640%\n",
      "Epoch 9, Batch 216, LR 0.000091 Loss 9.468476, Accuracy 65.650%\n",
      "Epoch 9, Batch 217, LR 0.000091 Loss 9.470657, Accuracy 65.657%\n",
      "Epoch 9, Batch 218, LR 0.000091 Loss 9.469576, Accuracy 65.686%\n",
      "Epoch 9, Batch 219, LR 0.000091 Loss 9.467622, Accuracy 65.693%\n",
      "Epoch 9, Batch 220, LR 0.000091 Loss 9.465237, Accuracy 65.703%\n",
      "Epoch 9, Batch 221, LR 0.000091 Loss 9.467172, Accuracy 65.696%\n",
      "Epoch 9, Batch 222, LR 0.000091 Loss 9.467291, Accuracy 65.706%\n",
      "Epoch 9, Batch 223, LR 0.000091 Loss 9.470281, Accuracy 65.713%\n",
      "Epoch 9, Batch 224, LR 0.000091 Loss 9.470986, Accuracy 65.719%\n",
      "Epoch 9, Batch 225, LR 0.000091 Loss 9.474029, Accuracy 65.694%\n",
      "Epoch 9, Batch 226, LR 0.000091 Loss 9.471220, Accuracy 65.732%\n",
      "Epoch 9, Batch 227, LR 0.000091 Loss 9.473109, Accuracy 65.714%\n",
      "Epoch 9, Batch 228, LR 0.000091 Loss 9.473819, Accuracy 65.714%\n",
      "Epoch 9, Batch 229, LR 0.000091 Loss 9.473334, Accuracy 65.734%\n",
      "Epoch 9, Batch 230, LR 0.000091 Loss 9.474277, Accuracy 65.727%\n",
      "Epoch 9, Batch 231, LR 0.000091 Loss 9.472498, Accuracy 65.740%\n",
      "Epoch 9, Batch 232, LR 0.000091 Loss 9.472327, Accuracy 65.753%\n",
      "Epoch 9, Batch 233, LR 0.000091 Loss 9.471245, Accuracy 65.773%\n",
      "Epoch 9, Batch 234, LR 0.000091 Loss 9.473010, Accuracy 65.759%\n",
      "Epoch 9, Batch 235, LR 0.000091 Loss 9.469228, Accuracy 65.775%\n",
      "Epoch 9, Batch 236, LR 0.000091 Loss 9.468765, Accuracy 65.777%\n",
      "Epoch 9, Batch 237, LR 0.000091 Loss 9.466703, Accuracy 65.796%\n",
      "Epoch 9, Batch 238, LR 0.000091 Loss 9.470153, Accuracy 65.783%\n",
      "Epoch 9, Batch 239, LR 0.000091 Loss 9.466843, Accuracy 65.798%\n",
      "Epoch 9, Batch 240, LR 0.000091 Loss 9.466431, Accuracy 65.814%\n",
      "Epoch 9, Batch 241, LR 0.000091 Loss 9.465549, Accuracy 65.823%\n",
      "Epoch 9, Batch 242, LR 0.000091 Loss 9.463710, Accuracy 65.819%\n",
      "Epoch 9, Batch 243, LR 0.000091 Loss 9.465566, Accuracy 65.818%\n",
      "Epoch 9, Batch 244, LR 0.000091 Loss 9.464338, Accuracy 65.846%\n",
      "Epoch 9, Batch 245, LR 0.000091 Loss 9.465051, Accuracy 65.842%\n",
      "Epoch 9, Batch 246, LR 0.000091 Loss 9.465405, Accuracy 65.825%\n",
      "Epoch 9, Batch 247, LR 0.000091 Loss 9.467720, Accuracy 65.831%\n",
      "Epoch 9, Batch 248, LR 0.000091 Loss 9.467192, Accuracy 65.842%\n",
      "Epoch 9, Batch 249, LR 0.000091 Loss 9.470389, Accuracy 65.807%\n",
      "Epoch 9, Batch 250, LR 0.000091 Loss 9.472886, Accuracy 65.800%\n",
      "Epoch 9, Batch 251, LR 0.000091 Loss 9.471911, Accuracy 65.787%\n",
      "Epoch 9, Batch 252, LR 0.000091 Loss 9.472388, Accuracy 65.799%\n",
      "Epoch 9, Batch 253, LR 0.000091 Loss 9.472074, Accuracy 65.813%\n",
      "Epoch 9, Batch 254, LR 0.000091 Loss 9.467720, Accuracy 65.850%\n",
      "Epoch 9, Batch 255, LR 0.000091 Loss 9.465080, Accuracy 65.870%\n",
      "Epoch 9, Batch 256, LR 0.000091 Loss 9.466964, Accuracy 65.851%\n",
      "Epoch 9, Batch 257, LR 0.000091 Loss 9.466720, Accuracy 65.856%\n",
      "Epoch 9, Batch 258, LR 0.000091 Loss 9.468050, Accuracy 65.843%\n",
      "Epoch 9, Batch 259, LR 0.000091 Loss 9.462608, Accuracy 65.872%\n",
      "Epoch 9, Batch 260, LR 0.000091 Loss 9.461974, Accuracy 65.886%\n",
      "Epoch 9, Batch 261, LR 0.000091 Loss 9.461505, Accuracy 65.900%\n",
      "Epoch 9, Batch 262, LR 0.000091 Loss 9.459579, Accuracy 65.914%\n",
      "Epoch 9, Batch 263, LR 0.000091 Loss 9.460318, Accuracy 65.925%\n",
      "Epoch 9, Batch 264, LR 0.000091 Loss 9.456155, Accuracy 65.959%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 265, LR 0.000091 Loss 9.452453, Accuracy 65.970%\n",
      "Epoch 9, Batch 266, LR 0.000091 Loss 9.453753, Accuracy 65.969%\n",
      "Epoch 9, Batch 267, LR 0.000091 Loss 9.452413, Accuracy 65.976%\n",
      "Epoch 9, Batch 268, LR 0.000091 Loss 9.452067, Accuracy 65.984%\n",
      "Epoch 9, Batch 269, LR 0.000091 Loss 9.453711, Accuracy 65.982%\n",
      "Epoch 9, Batch 270, LR 0.000091 Loss 9.453915, Accuracy 65.992%\n",
      "Epoch 9, Batch 271, LR 0.000091 Loss 9.453475, Accuracy 66.000%\n",
      "Epoch 9, Batch 272, LR 0.000091 Loss 9.450803, Accuracy 66.018%\n",
      "Epoch 9, Batch 273, LR 0.000091 Loss 9.448666, Accuracy 66.026%\n",
      "Epoch 9, Batch 274, LR 0.000091 Loss 9.448639, Accuracy 66.038%\n",
      "Epoch 9, Batch 275, LR 0.000091 Loss 9.448958, Accuracy 66.043%\n",
      "Epoch 9, Batch 276, LR 0.000091 Loss 9.447192, Accuracy 66.058%\n",
      "Epoch 9, Batch 277, LR 0.000091 Loss 9.445042, Accuracy 66.068%\n",
      "Epoch 9, Batch 278, LR 0.000091 Loss 9.447374, Accuracy 66.061%\n",
      "Epoch 9, Batch 279, LR 0.000091 Loss 9.448003, Accuracy 66.062%\n",
      "Epoch 9, Batch 280, LR 0.000091 Loss 9.447785, Accuracy 66.063%\n",
      "Epoch 9, Batch 281, LR 0.000091 Loss 9.444121, Accuracy 66.070%\n",
      "Epoch 9, Batch 282, LR 0.000091 Loss 9.444211, Accuracy 66.068%\n",
      "Epoch 9, Batch 283, LR 0.000091 Loss 9.444242, Accuracy 66.056%\n",
      "Epoch 9, Batch 284, LR 0.000091 Loss 9.445000, Accuracy 66.049%\n",
      "Epoch 9, Batch 285, LR 0.000091 Loss 9.446227, Accuracy 66.042%\n",
      "Epoch 9, Batch 286, LR 0.000091 Loss 9.444994, Accuracy 66.051%\n",
      "Epoch 9, Batch 287, LR 0.000091 Loss 9.447586, Accuracy 66.044%\n",
      "Epoch 9, Batch 288, LR 0.000091 Loss 9.447512, Accuracy 66.051%\n",
      "Epoch 9, Batch 289, LR 0.000091 Loss 9.446657, Accuracy 66.058%\n",
      "Epoch 9, Batch 290, LR 0.000091 Loss 9.446206, Accuracy 66.059%\n",
      "Epoch 9, Batch 291, LR 0.000091 Loss 9.447756, Accuracy 66.049%\n",
      "Epoch 9, Batch 292, LR 0.000091 Loss 9.444244, Accuracy 66.080%\n",
      "Epoch 9, Batch 293, LR 0.000091 Loss 9.448580, Accuracy 66.025%\n",
      "Epoch 9, Batch 294, LR 0.000091 Loss 9.449064, Accuracy 66.042%\n",
      "Epoch 9, Batch 295, LR 0.000091 Loss 9.450229, Accuracy 66.025%\n",
      "Epoch 9, Batch 296, LR 0.000091 Loss 9.445865, Accuracy 66.053%\n",
      "Epoch 9, Batch 297, LR 0.000091 Loss 9.443906, Accuracy 66.075%\n",
      "Epoch 9, Batch 298, LR 0.000091 Loss 9.443272, Accuracy 66.094%\n",
      "Epoch 9, Batch 299, LR 0.000091 Loss 9.442021, Accuracy 66.101%\n",
      "Epoch 9, Batch 300, LR 0.000091 Loss 9.444018, Accuracy 66.081%\n",
      "Epoch 9, Batch 301, LR 0.000091 Loss 9.447675, Accuracy 66.056%\n",
      "Epoch 9, Batch 302, LR 0.000091 Loss 9.448391, Accuracy 66.054%\n",
      "Epoch 9, Batch 303, LR 0.000091 Loss 9.444271, Accuracy 66.079%\n",
      "Epoch 9, Batch 304, LR 0.000091 Loss 9.444960, Accuracy 66.093%\n",
      "Epoch 9, Batch 305, LR 0.000091 Loss 9.447432, Accuracy 66.091%\n",
      "Epoch 9, Batch 306, LR 0.000091 Loss 9.448246, Accuracy 66.092%\n",
      "Epoch 9, Batch 307, LR 0.000091 Loss 9.448858, Accuracy 66.081%\n",
      "Epoch 9, Batch 308, LR 0.000091 Loss 9.450164, Accuracy 66.084%\n",
      "Epoch 9, Batch 309, LR 0.000091 Loss 9.449703, Accuracy 66.075%\n",
      "Epoch 9, Batch 310, LR 0.000091 Loss 9.451980, Accuracy 66.053%\n",
      "Epoch 9, Batch 311, LR 0.000091 Loss 9.450195, Accuracy 66.060%\n",
      "Epoch 9, Batch 312, LR 0.000091 Loss 9.450034, Accuracy 66.053%\n",
      "Epoch 9, Batch 313, LR 0.000091 Loss 9.450547, Accuracy 66.047%\n",
      "Epoch 9, Batch 314, LR 0.000091 Loss 9.449891, Accuracy 66.043%\n",
      "Epoch 9, Batch 315, LR 0.000091 Loss 9.450673, Accuracy 66.032%\n",
      "Epoch 9, Batch 316, LR 0.000091 Loss 9.450822, Accuracy 66.028%\n",
      "Epoch 9, Batch 317, LR 0.000091 Loss 9.452302, Accuracy 66.024%\n",
      "Epoch 9, Batch 318, LR 0.000091 Loss 9.451362, Accuracy 66.048%\n",
      "Epoch 9, Batch 319, LR 0.000091 Loss 9.451316, Accuracy 66.054%\n",
      "Epoch 9, Batch 320, LR 0.000091 Loss 9.453095, Accuracy 66.040%\n",
      "Epoch 9, Batch 321, LR 0.000091 Loss 9.452748, Accuracy 66.031%\n",
      "Epoch 9, Batch 322, LR 0.000091 Loss 9.453044, Accuracy 66.016%\n",
      "Epoch 9, Batch 323, LR 0.000091 Loss 9.451726, Accuracy 66.022%\n",
      "Epoch 9, Batch 324, LR 0.000091 Loss 9.450951, Accuracy 66.028%\n",
      "Epoch 9, Batch 325, LR 0.000091 Loss 9.450204, Accuracy 66.026%\n",
      "Epoch 9, Batch 326, LR 0.000091 Loss 9.451996, Accuracy 66.013%\n",
      "Epoch 9, Batch 327, LR 0.000091 Loss 9.453967, Accuracy 66.000%\n",
      "Epoch 9, Batch 328, LR 0.000091 Loss 9.453740, Accuracy 66.008%\n",
      "Epoch 9, Batch 329, LR 0.000091 Loss 9.453051, Accuracy 66.007%\n",
      "Epoch 9, Batch 330, LR 0.000091 Loss 9.453644, Accuracy 66.009%\n",
      "Epoch 9, Batch 331, LR 0.000091 Loss 9.451766, Accuracy 66.010%\n",
      "Epoch 9, Batch 332, LR 0.000091 Loss 9.452204, Accuracy 66.011%\n",
      "Epoch 9, Batch 333, LR 0.000091 Loss 9.450339, Accuracy 66.031%\n",
      "Epoch 9, Batch 334, LR 0.000091 Loss 9.449165, Accuracy 66.041%\n",
      "Epoch 9, Batch 335, LR 0.000091 Loss 9.447360, Accuracy 66.038%\n",
      "Epoch 9, Batch 336, LR 0.000091 Loss 9.447225, Accuracy 66.037%\n",
      "Epoch 9, Batch 337, LR 0.000091 Loss 9.445741, Accuracy 66.045%\n",
      "Epoch 9, Batch 338, LR 0.000091 Loss 9.444713, Accuracy 66.050%\n",
      "Epoch 9, Batch 339, LR 0.000091 Loss 9.441368, Accuracy 66.086%\n",
      "Epoch 9, Batch 340, LR 0.000091 Loss 9.441079, Accuracy 66.082%\n",
      "Epoch 9, Batch 341, LR 0.000091 Loss 9.438995, Accuracy 66.095%\n",
      "Epoch 9, Batch 342, LR 0.000091 Loss 9.440314, Accuracy 66.086%\n",
      "Epoch 9, Batch 343, LR 0.000091 Loss 9.442516, Accuracy 66.069%\n",
      "Epoch 9, Batch 344, LR 0.000091 Loss 9.440292, Accuracy 66.097%\n",
      "Epoch 9, Batch 345, LR 0.000091 Loss 9.439010, Accuracy 66.110%\n",
      "Epoch 9, Batch 346, LR 0.000091 Loss 9.439428, Accuracy 66.097%\n",
      "Epoch 9, Batch 347, LR 0.000091 Loss 9.437132, Accuracy 66.123%\n",
      "Epoch 9, Batch 348, LR 0.000091 Loss 9.433875, Accuracy 66.146%\n",
      "Epoch 9, Batch 349, LR 0.000091 Loss 9.433324, Accuracy 66.153%\n",
      "Epoch 9, Batch 350, LR 0.000091 Loss 9.431362, Accuracy 66.150%\n",
      "Epoch 9, Batch 351, LR 0.000091 Loss 9.430109, Accuracy 66.153%\n",
      "Epoch 9, Batch 352, LR 0.000091 Loss 9.428014, Accuracy 66.171%\n",
      "Epoch 9, Batch 353, LR 0.000091 Loss 9.429955, Accuracy 66.154%\n",
      "Epoch 9, Batch 354, LR 0.000091 Loss 9.428699, Accuracy 66.159%\n",
      "Epoch 9, Batch 355, LR 0.000091 Loss 9.427464, Accuracy 66.160%\n",
      "Epoch 9, Batch 356, LR 0.000091 Loss 9.425400, Accuracy 66.171%\n",
      "Epoch 9, Batch 357, LR 0.000091 Loss 9.425318, Accuracy 66.172%\n",
      "Epoch 9, Batch 358, LR 0.000091 Loss 9.426730, Accuracy 66.162%\n",
      "Epoch 9, Batch 359, LR 0.000091 Loss 9.426809, Accuracy 66.158%\n",
      "Epoch 9, Batch 360, LR 0.000091 Loss 9.424625, Accuracy 66.161%\n",
      "Epoch 9, Batch 361, LR 0.000091 Loss 9.424725, Accuracy 66.149%\n",
      "Epoch 9, Batch 362, LR 0.000091 Loss 9.424199, Accuracy 66.169%\n",
      "Epoch 9, Batch 363, LR 0.000091 Loss 9.422791, Accuracy 66.185%\n",
      "Epoch 9, Batch 364, LR 0.000091 Loss 9.421103, Accuracy 66.209%\n",
      "Epoch 9, Batch 365, LR 0.000091 Loss 9.420145, Accuracy 66.226%\n",
      "Epoch 9, Batch 366, LR 0.000091 Loss 9.421867, Accuracy 66.203%\n",
      "Epoch 9, Batch 367, LR 0.000091 Loss 9.421387, Accuracy 66.193%\n",
      "Epoch 9, Batch 368, LR 0.000091 Loss 9.419722, Accuracy 66.219%\n",
      "Epoch 9, Batch 369, LR 0.000091 Loss 9.421971, Accuracy 66.214%\n",
      "Epoch 9, Batch 370, LR 0.000091 Loss 9.421539, Accuracy 66.216%\n",
      "Epoch 9, Batch 371, LR 0.000091 Loss 9.419080, Accuracy 66.238%\n",
      "Epoch 9, Batch 372, LR 0.000091 Loss 9.418299, Accuracy 66.247%\n",
      "Epoch 9, Batch 373, LR 0.000091 Loss 9.416760, Accuracy 66.255%\n",
      "Epoch 9, Batch 374, LR 0.000091 Loss 9.415767, Accuracy 66.264%\n",
      "Epoch 9, Batch 375, LR 0.000091 Loss 9.412272, Accuracy 66.281%\n",
      "Epoch 9, Batch 376, LR 0.000091 Loss 9.411863, Accuracy 66.290%\n",
      "Epoch 9, Batch 377, LR 0.000091 Loss 9.412541, Accuracy 66.286%\n",
      "Epoch 9, Batch 378, LR 0.000091 Loss 9.413860, Accuracy 66.278%\n",
      "Epoch 9, Batch 379, LR 0.000091 Loss 9.414777, Accuracy 66.262%\n",
      "Epoch 9, Batch 380, LR 0.000091 Loss 9.412393, Accuracy 66.262%\n",
      "Epoch 9, Batch 381, LR 0.000091 Loss 9.409548, Accuracy 66.263%\n",
      "Epoch 9, Batch 382, LR 0.000091 Loss 9.410099, Accuracy 66.273%\n",
      "Epoch 9, Batch 383, LR 0.000091 Loss 9.409710, Accuracy 66.270%\n",
      "Epoch 9, Batch 384, LR 0.000091 Loss 9.410053, Accuracy 66.262%\n",
      "Epoch 9, Batch 385, LR 0.000091 Loss 9.408045, Accuracy 66.270%\n",
      "Epoch 9, Batch 386, LR 0.000091 Loss 9.405934, Accuracy 66.281%\n",
      "Epoch 9, Batch 387, LR 0.000091 Loss 9.406145, Accuracy 66.275%\n",
      "Epoch 9, Batch 388, LR 0.000091 Loss 9.407373, Accuracy 66.271%\n",
      "Epoch 9, Batch 389, LR 0.000091 Loss 9.408796, Accuracy 66.256%\n",
      "Epoch 9, Batch 390, LR 0.000091 Loss 9.411212, Accuracy 66.244%\n",
      "Epoch 9, Batch 391, LR 0.000091 Loss 9.412995, Accuracy 66.232%\n",
      "Epoch 9, Batch 392, LR 0.000091 Loss 9.414499, Accuracy 66.217%\n",
      "Epoch 9, Batch 393, LR 0.000091 Loss 9.414962, Accuracy 66.213%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 394, LR 0.000091 Loss 9.413790, Accuracy 66.228%\n",
      "Epoch 9, Batch 395, LR 0.000091 Loss 9.410492, Accuracy 66.248%\n",
      "Epoch 9, Batch 396, LR 0.000091 Loss 9.409587, Accuracy 66.260%\n",
      "Epoch 9, Batch 397, LR 0.000091 Loss 9.409048, Accuracy 66.263%\n",
      "Epoch 9, Batch 398, LR 0.000091 Loss 9.410387, Accuracy 66.261%\n",
      "Epoch 9, Batch 399, LR 0.000091 Loss 9.408865, Accuracy 66.261%\n",
      "Epoch 9, Batch 400, LR 0.000091 Loss 9.408413, Accuracy 66.264%\n",
      "Epoch 9, Batch 401, LR 0.000091 Loss 9.408317, Accuracy 66.272%\n",
      "Epoch 9, Batch 402, LR 0.000091 Loss 9.411061, Accuracy 66.266%\n",
      "Epoch 9, Batch 403, LR 0.000091 Loss 9.411396, Accuracy 66.265%\n",
      "Epoch 9, Batch 404, LR 0.000091 Loss 9.409259, Accuracy 66.271%\n",
      "Epoch 9, Batch 405, LR 0.000091 Loss 9.408242, Accuracy 66.281%\n",
      "Epoch 9, Batch 406, LR 0.000091 Loss 9.408008, Accuracy 66.285%\n",
      "Epoch 9, Batch 407, LR 0.000091 Loss 9.407003, Accuracy 66.287%\n",
      "Epoch 9, Batch 408, LR 0.000091 Loss 9.409060, Accuracy 66.259%\n",
      "Epoch 9, Batch 409, LR 0.000091 Loss 9.409074, Accuracy 66.259%\n",
      "Epoch 9, Batch 410, LR 0.000091 Loss 9.410063, Accuracy 66.252%\n",
      "Epoch 9, Batch 411, LR 0.000091 Loss 9.410869, Accuracy 66.247%\n",
      "Epoch 9, Batch 412, LR 0.000091 Loss 9.412185, Accuracy 66.239%\n",
      "Epoch 9, Batch 413, LR 0.000091 Loss 9.412193, Accuracy 66.236%\n",
      "Epoch 9, Batch 414, LR 0.000091 Loss 9.412614, Accuracy 66.231%\n",
      "Epoch 9, Batch 415, LR 0.000091 Loss 9.411743, Accuracy 66.246%\n",
      "Epoch 9, Batch 416, LR 0.000091 Loss 9.410744, Accuracy 66.252%\n",
      "Epoch 9, Batch 417, LR 0.000091 Loss 9.411095, Accuracy 66.251%\n",
      "Epoch 9, Batch 418, LR 0.000091 Loss 9.412219, Accuracy 66.246%\n",
      "Epoch 9, Batch 419, LR 0.000091 Loss 9.413234, Accuracy 66.237%\n",
      "Epoch 9, Batch 420, LR 0.000091 Loss 9.414298, Accuracy 66.233%\n",
      "Epoch 9, Batch 421, LR 0.000091 Loss 9.414020, Accuracy 66.239%\n",
      "Epoch 9, Batch 422, LR 0.000091 Loss 9.414174, Accuracy 66.243%\n",
      "Epoch 9, Batch 423, LR 0.000091 Loss 9.414302, Accuracy 66.246%\n",
      "Epoch 9, Batch 424, LR 0.000091 Loss 9.413912, Accuracy 66.253%\n",
      "Epoch 9, Batch 425, LR 0.000091 Loss 9.414191, Accuracy 66.257%\n",
      "Epoch 9, Batch 426, LR 0.000091 Loss 9.414617, Accuracy 66.247%\n",
      "Epoch 9, Batch 427, LR 0.000091 Loss 9.414161, Accuracy 66.254%\n",
      "Epoch 9, Batch 428, LR 0.000091 Loss 9.412517, Accuracy 66.268%\n",
      "Epoch 9, Batch 429, LR 0.000091 Loss 9.411853, Accuracy 66.268%\n",
      "Epoch 9, Batch 430, LR 0.000091 Loss 9.412312, Accuracy 66.270%\n",
      "Epoch 9, Batch 431, LR 0.000091 Loss 9.411934, Accuracy 66.272%\n",
      "Epoch 9, Batch 432, LR 0.000091 Loss 9.411486, Accuracy 66.274%\n",
      "Epoch 9, Batch 433, LR 0.000091 Loss 9.410698, Accuracy 66.269%\n",
      "Epoch 9, Batch 434, LR 0.000091 Loss 9.410696, Accuracy 66.269%\n",
      "Epoch 9, Batch 435, LR 0.000091 Loss 9.410236, Accuracy 66.270%\n",
      "Epoch 9, Batch 436, LR 0.000091 Loss 9.409884, Accuracy 66.283%\n",
      "Epoch 9, Batch 437, LR 0.000091 Loss 9.409706, Accuracy 66.286%\n",
      "Epoch 9, Batch 438, LR 0.000091 Loss 9.408310, Accuracy 66.290%\n",
      "Epoch 9, Batch 439, LR 0.000091 Loss 9.406781, Accuracy 66.294%\n",
      "Epoch 9, Batch 440, LR 0.000091 Loss 9.405416, Accuracy 66.303%\n",
      "Epoch 9, Batch 441, LR 0.000091 Loss 9.406949, Accuracy 66.289%\n",
      "Epoch 9, Batch 442, LR 0.000091 Loss 9.407547, Accuracy 66.290%\n",
      "Epoch 9, Batch 443, LR 0.000091 Loss 9.407211, Accuracy 66.302%\n",
      "Epoch 9, Batch 444, LR 0.000091 Loss 9.407009, Accuracy 66.304%\n",
      "Epoch 9, Batch 445, LR 0.000091 Loss 9.406841, Accuracy 66.310%\n",
      "Epoch 9, Batch 446, LR 0.000091 Loss 9.406835, Accuracy 66.308%\n",
      "Epoch 9, Batch 447, LR 0.000091 Loss 9.405890, Accuracy 66.298%\n",
      "Epoch 9, Batch 448, LR 0.000091 Loss 9.406899, Accuracy 66.291%\n",
      "Epoch 9, Batch 449, LR 0.000091 Loss 9.407779, Accuracy 66.281%\n",
      "Epoch 9, Batch 450, LR 0.000091 Loss 9.406668, Accuracy 66.299%\n",
      "Epoch 9, Batch 451, LR 0.000091 Loss 9.404845, Accuracy 66.308%\n",
      "Epoch 9, Batch 452, LR 0.000091 Loss 9.404477, Accuracy 66.306%\n",
      "Epoch 9, Batch 453, LR 0.000091 Loss 9.402591, Accuracy 66.313%\n",
      "Epoch 9, Batch 454, LR 0.000091 Loss 9.402569, Accuracy 66.317%\n",
      "Epoch 9, Batch 455, LR 0.000091 Loss 9.402936, Accuracy 66.326%\n",
      "Epoch 9, Batch 456, LR 0.000091 Loss 9.401883, Accuracy 66.331%\n",
      "Epoch 9, Batch 457, LR 0.000091 Loss 9.400911, Accuracy 66.336%\n",
      "Epoch 9, Batch 458, LR 0.000091 Loss 9.402088, Accuracy 66.324%\n",
      "Epoch 9, Batch 459, LR 0.000091 Loss 9.400651, Accuracy 66.335%\n",
      "Epoch 9, Batch 460, LR 0.000091 Loss 9.401636, Accuracy 66.330%\n",
      "Epoch 9, Batch 461, LR 0.000091 Loss 9.399692, Accuracy 66.342%\n",
      "Epoch 9, Batch 462, LR 0.000091 Loss 9.398099, Accuracy 66.361%\n",
      "Epoch 9, Batch 463, LR 0.000091 Loss 9.396826, Accuracy 66.364%\n",
      "Epoch 9, Batch 464, LR 0.000091 Loss 9.396560, Accuracy 66.356%\n",
      "Epoch 9, Batch 465, LR 0.000091 Loss 9.394620, Accuracy 66.371%\n",
      "Epoch 9, Batch 466, LR 0.000091 Loss 9.396763, Accuracy 66.354%\n",
      "Epoch 9, Batch 467, LR 0.000091 Loss 9.396916, Accuracy 66.356%\n",
      "Epoch 9, Batch 468, LR 0.000091 Loss 9.396686, Accuracy 66.356%\n",
      "Epoch 9, Batch 469, LR 0.000091 Loss 9.395392, Accuracy 66.360%\n",
      "Epoch 9, Batch 470, LR 0.000091 Loss 9.395153, Accuracy 66.361%\n",
      "Epoch 9, Batch 471, LR 0.000091 Loss 9.395181, Accuracy 66.361%\n",
      "Epoch 9, Batch 472, LR 0.000091 Loss 9.396791, Accuracy 66.350%\n",
      "Epoch 9, Batch 473, LR 0.000091 Loss 9.397252, Accuracy 66.340%\n",
      "Epoch 9, Batch 474, LR 0.000091 Loss 9.396587, Accuracy 66.342%\n",
      "Epoch 9, Batch 475, LR 0.000091 Loss 9.396780, Accuracy 66.342%\n",
      "Epoch 9, Batch 476, LR 0.000091 Loss 9.396633, Accuracy 66.349%\n",
      "Epoch 9, Batch 477, LR 0.000091 Loss 9.397512, Accuracy 66.347%\n",
      "Epoch 9, Batch 478, LR 0.000091 Loss 9.398733, Accuracy 66.336%\n",
      "Epoch 9, Batch 479, LR 0.000091 Loss 9.399189, Accuracy 66.333%\n",
      "Epoch 9, Batch 480, LR 0.000091 Loss 9.398688, Accuracy 66.336%\n",
      "Epoch 9, Batch 481, LR 0.000091 Loss 9.397021, Accuracy 66.346%\n",
      "Epoch 9, Batch 482, LR 0.000091 Loss 9.396100, Accuracy 66.351%\n",
      "Epoch 9, Batch 483, LR 0.000091 Loss 9.394668, Accuracy 66.354%\n",
      "Epoch 9, Batch 484, LR 0.000091 Loss 9.394337, Accuracy 66.363%\n",
      "Epoch 9, Batch 485, LR 0.000091 Loss 9.394252, Accuracy 66.358%\n",
      "Epoch 9, Batch 486, LR 0.000091 Loss 9.393397, Accuracy 66.363%\n",
      "Epoch 9, Batch 487, LR 0.000091 Loss 9.393813, Accuracy 66.358%\n",
      "Epoch 9, Batch 488, LR 0.000091 Loss 9.394362, Accuracy 66.353%\n",
      "Epoch 9, Batch 489, LR 0.000091 Loss 9.394060, Accuracy 66.354%\n",
      "Epoch 9, Batch 490, LR 0.000091 Loss 9.393798, Accuracy 66.355%\n",
      "Epoch 9, Batch 491, LR 0.000091 Loss 9.392378, Accuracy 66.368%\n",
      "Epoch 9, Batch 492, LR 0.000091 Loss 9.391307, Accuracy 66.379%\n",
      "Epoch 9, Batch 493, LR 0.000091 Loss 9.391826, Accuracy 66.368%\n",
      "Epoch 9, Batch 494, LR 0.000091 Loss 9.391648, Accuracy 66.365%\n",
      "Epoch 9, Batch 495, LR 0.000091 Loss 9.390502, Accuracy 66.376%\n",
      "Epoch 9, Batch 496, LR 0.000091 Loss 9.391712, Accuracy 66.367%\n",
      "Epoch 9, Batch 497, LR 0.000091 Loss 9.391235, Accuracy 66.365%\n",
      "Epoch 9, Batch 498, LR 0.000091 Loss 9.392136, Accuracy 66.364%\n",
      "Epoch 9, Batch 499, LR 0.000091 Loss 9.393589, Accuracy 66.359%\n",
      "Epoch 9, Batch 500, LR 0.000091 Loss 9.393211, Accuracy 66.362%\n",
      "Epoch 9, Batch 501, LR 0.000091 Loss 9.390802, Accuracy 66.377%\n",
      "Epoch 9, Batch 502, LR 0.000091 Loss 9.390937, Accuracy 66.374%\n",
      "Epoch 9, Batch 503, LR 0.000091 Loss 9.389899, Accuracy 66.377%\n",
      "Epoch 9, Batch 504, LR 0.000091 Loss 9.390429, Accuracy 66.369%\n",
      "Epoch 9, Batch 505, LR 0.000091 Loss 9.390505, Accuracy 66.375%\n",
      "Epoch 9, Batch 506, LR 0.000091 Loss 9.390824, Accuracy 66.377%\n",
      "Epoch 9, Batch 507, LR 0.000091 Loss 9.390256, Accuracy 66.388%\n",
      "Epoch 9, Batch 508, LR 0.000091 Loss 9.390974, Accuracy 66.392%\n",
      "Epoch 9, Batch 509, LR 0.000091 Loss 9.389463, Accuracy 66.402%\n",
      "Epoch 9, Batch 510, LR 0.000091 Loss 9.389844, Accuracy 66.399%\n",
      "Epoch 9, Batch 511, LR 0.000091 Loss 9.389647, Accuracy 66.399%\n",
      "Epoch 9, Batch 512, LR 0.000091 Loss 9.390709, Accuracy 66.396%\n",
      "Epoch 9, Batch 513, LR 0.000091 Loss 9.389300, Accuracy 66.403%\n",
      "Epoch 9, Batch 514, LR 0.000091 Loss 9.390115, Accuracy 66.400%\n",
      "Epoch 9, Batch 515, LR 0.000091 Loss 9.390641, Accuracy 66.397%\n",
      "Epoch 9, Batch 516, LR 0.000091 Loss 9.389498, Accuracy 66.405%\n",
      "Epoch 9, Batch 517, LR 0.000091 Loss 9.389388, Accuracy 66.397%\n",
      "Epoch 9, Batch 518, LR 0.000091 Loss 9.388922, Accuracy 66.405%\n",
      "Epoch 9, Batch 519, LR 0.000091 Loss 9.388090, Accuracy 66.417%\n",
      "Epoch 9, Batch 520, LR 0.000091 Loss 9.388756, Accuracy 66.411%\n",
      "Epoch 9, Batch 521, LR 0.000091 Loss 9.389254, Accuracy 66.406%\n",
      "Epoch 9, Batch 522, LR 0.000091 Loss 9.388991, Accuracy 66.400%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 523, LR 0.000091 Loss 9.387482, Accuracy 66.394%\n",
      "Epoch 9, Batch 524, LR 0.000091 Loss 9.388995, Accuracy 66.382%\n",
      "Epoch 9, Batch 525, LR 0.000091 Loss 9.388683, Accuracy 66.374%\n",
      "Epoch 9, Batch 526, LR 0.000091 Loss 9.388354, Accuracy 66.380%\n",
      "Epoch 9, Batch 527, LR 0.000091 Loss 9.387050, Accuracy 66.385%\n",
      "Epoch 9, Batch 528, LR 0.000091 Loss 9.385913, Accuracy 66.393%\n",
      "Epoch 9, Batch 529, LR 0.000091 Loss 9.383738, Accuracy 66.396%\n",
      "Epoch 9, Batch 530, LR 0.000091 Loss 9.383583, Accuracy 66.393%\n",
      "Epoch 9, Batch 531, LR 0.000091 Loss 9.382338, Accuracy 66.396%\n",
      "Epoch 9, Batch 532, LR 0.000091 Loss 9.381278, Accuracy 66.399%\n",
      "Epoch 9, Batch 533, LR 0.000091 Loss 9.380459, Accuracy 66.411%\n",
      "Epoch 9, Batch 534, LR 0.000091 Loss 9.381298, Accuracy 66.418%\n",
      "Epoch 9, Batch 535, LR 0.000091 Loss 9.380236, Accuracy 66.419%\n",
      "Epoch 9, Batch 536, LR 0.000091 Loss 9.379916, Accuracy 66.416%\n",
      "Epoch 9, Batch 537, LR 0.000091 Loss 9.380503, Accuracy 66.415%\n",
      "Epoch 9, Batch 538, LR 0.000091 Loss 9.380469, Accuracy 66.412%\n",
      "Epoch 9, Batch 539, LR 0.000091 Loss 9.379627, Accuracy 66.418%\n",
      "Epoch 9, Batch 540, LR 0.000091 Loss 9.379529, Accuracy 66.422%\n",
      "Epoch 9, Batch 541, LR 0.000091 Loss 9.378925, Accuracy 66.431%\n",
      "Epoch 9, Batch 542, LR 0.000091 Loss 9.377983, Accuracy 66.439%\n",
      "Epoch 9, Batch 543, LR 0.000091 Loss 9.377270, Accuracy 66.444%\n",
      "Epoch 9, Batch 544, LR 0.000091 Loss 9.376961, Accuracy 66.445%\n",
      "Epoch 9, Batch 545, LR 0.000091 Loss 9.377646, Accuracy 66.432%\n",
      "Epoch 9, Batch 546, LR 0.000091 Loss 9.377278, Accuracy 66.442%\n",
      "Epoch 9, Batch 547, LR 0.000091 Loss 9.378471, Accuracy 66.431%\n",
      "Epoch 9, Batch 548, LR 0.000091 Loss 9.378200, Accuracy 66.430%\n",
      "Epoch 9, Batch 549, LR 0.000091 Loss 9.378711, Accuracy 66.428%\n",
      "Epoch 9, Batch 550, LR 0.000091 Loss 9.378750, Accuracy 66.430%\n",
      "Epoch 9, Batch 551, LR 0.000091 Loss 9.378019, Accuracy 66.449%\n",
      "Epoch 9, Batch 552, LR 0.000091 Loss 9.375304, Accuracy 66.454%\n",
      "Epoch 9, Batch 553, LR 0.000091 Loss 9.376484, Accuracy 66.449%\n",
      "Epoch 9, Batch 554, LR 0.000091 Loss 9.376644, Accuracy 66.447%\n",
      "Epoch 9, Batch 555, LR 0.000091 Loss 9.375944, Accuracy 66.450%\n",
      "Epoch 9, Batch 556, LR 0.000091 Loss 9.376281, Accuracy 66.443%\n",
      "Epoch 9, Batch 557, LR 0.000091 Loss 9.377311, Accuracy 66.440%\n",
      "Epoch 9, Batch 558, LR 0.000091 Loss 9.378287, Accuracy 66.429%\n",
      "Epoch 9, Batch 559, LR 0.000091 Loss 9.377913, Accuracy 66.433%\n",
      "Epoch 9, Batch 560, LR 0.000091 Loss 9.377939, Accuracy 66.445%\n",
      "Epoch 9, Batch 561, LR 0.000091 Loss 9.376435, Accuracy 66.459%\n",
      "Epoch 9, Batch 562, LR 0.000091 Loss 9.377246, Accuracy 66.451%\n",
      "Epoch 9, Batch 563, LR 0.000091 Loss 9.376167, Accuracy 66.458%\n",
      "Epoch 9, Batch 564, LR 0.000091 Loss 9.375422, Accuracy 66.453%\n",
      "Epoch 9, Batch 565, LR 0.000091 Loss 9.375536, Accuracy 66.450%\n",
      "Epoch 9, Batch 566, LR 0.000091 Loss 9.374684, Accuracy 66.457%\n",
      "Epoch 9, Batch 567, LR 0.000091 Loss 9.374443, Accuracy 66.460%\n",
      "Epoch 9, Batch 568, LR 0.000091 Loss 9.374135, Accuracy 66.461%\n",
      "Epoch 9, Batch 569, LR 0.000091 Loss 9.376456, Accuracy 66.449%\n",
      "Epoch 9, Batch 570, LR 0.000091 Loss 9.376222, Accuracy 66.450%\n",
      "Epoch 9, Batch 571, LR 0.000091 Loss 9.376227, Accuracy 66.449%\n",
      "Epoch 9, Batch 572, LR 0.000091 Loss 9.376600, Accuracy 66.446%\n",
      "Epoch 9, Batch 573, LR 0.000091 Loss 9.377081, Accuracy 66.449%\n",
      "Epoch 9, Batch 574, LR 0.000091 Loss 9.376472, Accuracy 66.457%\n",
      "Epoch 9, Batch 575, LR 0.000091 Loss 9.377733, Accuracy 66.452%\n",
      "Epoch 9, Batch 576, LR 0.000091 Loss 9.377519, Accuracy 66.454%\n",
      "Epoch 9, Batch 577, LR 0.000091 Loss 9.378191, Accuracy 66.443%\n",
      "Epoch 9, Batch 578, LR 0.000091 Loss 9.377881, Accuracy 66.450%\n",
      "Epoch 9, Batch 579, LR 0.000091 Loss 9.377344, Accuracy 66.462%\n",
      "Epoch 9, Batch 580, LR 0.000091 Loss 9.376295, Accuracy 66.467%\n",
      "Epoch 9, Batch 581, LR 0.000091 Loss 9.374993, Accuracy 66.480%\n",
      "Epoch 9, Batch 582, LR 0.000091 Loss 9.374913, Accuracy 66.484%\n",
      "Epoch 9, Batch 583, LR 0.000091 Loss 9.374739, Accuracy 66.479%\n",
      "Epoch 9, Batch 584, LR 0.000091 Loss 9.374650, Accuracy 66.491%\n",
      "Epoch 9, Batch 585, LR 0.000091 Loss 9.375072, Accuracy 66.492%\n",
      "Epoch 9, Batch 586, LR 0.000091 Loss 9.374795, Accuracy 66.494%\n",
      "Epoch 9, Batch 587, LR 0.000091 Loss 9.374393, Accuracy 66.487%\n",
      "Epoch 9, Batch 588, LR 0.000091 Loss 9.374564, Accuracy 66.483%\n",
      "Epoch 9, Batch 589, LR 0.000091 Loss 9.374003, Accuracy 66.488%\n",
      "Epoch 9, Batch 590, LR 0.000091 Loss 9.373674, Accuracy 66.494%\n",
      "Epoch 9, Batch 591, LR 0.000091 Loss 9.373238, Accuracy 66.495%\n",
      "Epoch 9, Batch 592, LR 0.000091 Loss 9.372621, Accuracy 66.504%\n",
      "Epoch 9, Batch 593, LR 0.000091 Loss 9.372927, Accuracy 66.502%\n",
      "Epoch 9, Batch 594, LR 0.000091 Loss 9.372765, Accuracy 66.497%\n",
      "Epoch 9, Batch 595, LR 0.000091 Loss 9.373147, Accuracy 66.496%\n",
      "Epoch 9, Batch 596, LR 0.000091 Loss 9.372575, Accuracy 66.505%\n",
      "Epoch 9, Batch 597, LR 0.000091 Loss 9.372506, Accuracy 66.503%\n",
      "Epoch 9, Batch 598, LR 0.000091 Loss 9.372290, Accuracy 66.511%\n",
      "Epoch 9, Batch 599, LR 0.000091 Loss 9.372077, Accuracy 66.520%\n",
      "Epoch 9, Batch 600, LR 0.000091 Loss 9.372666, Accuracy 66.520%\n",
      "Epoch 9, Batch 601, LR 0.000091 Loss 9.373093, Accuracy 66.521%\n",
      "Epoch 9, Batch 602, LR 0.000091 Loss 9.373772, Accuracy 66.514%\n",
      "Epoch 9, Batch 603, LR 0.000091 Loss 9.373651, Accuracy 66.519%\n",
      "Epoch 9, Batch 604, LR 0.000091 Loss 9.373764, Accuracy 66.524%\n",
      "Epoch 9, Batch 605, LR 0.000091 Loss 9.374449, Accuracy 66.521%\n",
      "Epoch 9, Batch 606, LR 0.000091 Loss 9.372998, Accuracy 66.533%\n",
      "Epoch 9, Batch 607, LR 0.000091 Loss 9.373907, Accuracy 66.525%\n",
      "Epoch 9, Batch 608, LR 0.000090 Loss 9.372590, Accuracy 66.537%\n",
      "Epoch 9, Batch 609, LR 0.000090 Loss 9.371788, Accuracy 66.542%\n",
      "Epoch 9, Batch 610, LR 0.000090 Loss 9.370759, Accuracy 66.554%\n",
      "Epoch 9, Batch 611, LR 0.000090 Loss 9.368770, Accuracy 66.565%\n",
      "Epoch 9, Batch 612, LR 0.000090 Loss 9.368620, Accuracy 66.575%\n",
      "Epoch 9, Batch 613, LR 0.000090 Loss 9.368600, Accuracy 66.571%\n",
      "Epoch 9, Batch 614, LR 0.000090 Loss 9.367936, Accuracy 66.575%\n",
      "Epoch 9, Batch 615, LR 0.000090 Loss 9.367458, Accuracy 66.571%\n",
      "Epoch 9, Batch 616, LR 0.000090 Loss 9.367167, Accuracy 66.567%\n",
      "Epoch 9, Batch 617, LR 0.000090 Loss 9.367371, Accuracy 66.570%\n",
      "Epoch 9, Batch 618, LR 0.000090 Loss 9.367343, Accuracy 66.571%\n",
      "Epoch 9, Batch 619, LR 0.000090 Loss 9.365590, Accuracy 66.584%\n",
      "Epoch 9, Batch 620, LR 0.000090 Loss 9.366311, Accuracy 66.586%\n",
      "Epoch 9, Batch 621, LR 0.000090 Loss 9.366635, Accuracy 66.582%\n",
      "Epoch 9, Batch 622, LR 0.000090 Loss 9.366312, Accuracy 66.583%\n",
      "Epoch 9, Batch 623, LR 0.000090 Loss 9.366752, Accuracy 66.573%\n",
      "Epoch 9, Batch 624, LR 0.000090 Loss 9.366596, Accuracy 66.570%\n",
      "Epoch 9, Batch 625, LR 0.000090 Loss 9.366552, Accuracy 66.569%\n",
      "Epoch 9, Batch 626, LR 0.000090 Loss 9.367144, Accuracy 66.566%\n",
      "Epoch 9, Batch 627, LR 0.000090 Loss 9.367508, Accuracy 66.562%\n",
      "Epoch 9, Batch 628, LR 0.000090 Loss 9.367987, Accuracy 66.556%\n",
      "Epoch 9, Batch 629, LR 0.000090 Loss 9.367915, Accuracy 66.550%\n",
      "Epoch 9, Batch 630, LR 0.000090 Loss 9.368470, Accuracy 66.541%\n",
      "Epoch 9, Batch 631, LR 0.000090 Loss 9.367723, Accuracy 66.549%\n",
      "Epoch 9, Batch 632, LR 0.000090 Loss 9.367653, Accuracy 66.545%\n",
      "Epoch 9, Batch 633, LR 0.000090 Loss 9.367507, Accuracy 66.548%\n",
      "Epoch 9, Batch 634, LR 0.000090 Loss 9.366805, Accuracy 66.560%\n",
      "Epoch 9, Batch 635, LR 0.000090 Loss 9.366716, Accuracy 66.560%\n",
      "Epoch 9, Batch 636, LR 0.000090 Loss 9.364763, Accuracy 66.567%\n",
      "Epoch 9, Batch 637, LR 0.000090 Loss 9.363566, Accuracy 66.577%\n",
      "Epoch 9, Batch 638, LR 0.000090 Loss 9.363409, Accuracy 66.574%\n",
      "Epoch 9, Batch 639, LR 0.000090 Loss 9.362496, Accuracy 66.576%\n",
      "Epoch 9, Batch 640, LR 0.000090 Loss 9.362806, Accuracy 66.571%\n",
      "Epoch 9, Batch 641, LR 0.000090 Loss 9.364191, Accuracy 66.560%\n",
      "Epoch 9, Batch 642, LR 0.000090 Loss 9.365784, Accuracy 66.556%\n",
      "Epoch 9, Batch 643, LR 0.000090 Loss 9.365074, Accuracy 66.565%\n",
      "Epoch 9, Batch 644, LR 0.000090 Loss 9.363669, Accuracy 66.580%\n",
      "Epoch 9, Batch 645, LR 0.000090 Loss 9.362785, Accuracy 66.583%\n",
      "Epoch 9, Batch 646, LR 0.000090 Loss 9.362402, Accuracy 66.580%\n",
      "Epoch 9, Batch 647, LR 0.000090 Loss 9.361468, Accuracy 66.587%\n",
      "Epoch 9, Batch 648, LR 0.000090 Loss 9.361428, Accuracy 66.587%\n",
      "Epoch 9, Batch 649, LR 0.000090 Loss 9.360568, Accuracy 66.595%\n",
      "Epoch 9, Batch 650, LR 0.000090 Loss 9.360328, Accuracy 66.591%\n",
      "Epoch 9, Batch 651, LR 0.000090 Loss 9.359709, Accuracy 66.593%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 652, LR 0.000090 Loss 9.359553, Accuracy 66.594%\n",
      "Epoch 9, Batch 653, LR 0.000090 Loss 9.359243, Accuracy 66.594%\n",
      "Epoch 9, Batch 654, LR 0.000090 Loss 9.359461, Accuracy 66.587%\n",
      "Epoch 9, Batch 655, LR 0.000090 Loss 9.359573, Accuracy 66.582%\n",
      "Epoch 9, Batch 656, LR 0.000090 Loss 9.360156, Accuracy 66.567%\n",
      "Epoch 9, Batch 657, LR 0.000090 Loss 9.359071, Accuracy 66.569%\n",
      "Epoch 9, Batch 658, LR 0.000090 Loss 9.359504, Accuracy 66.562%\n",
      "Epoch 9, Batch 659, LR 0.000090 Loss 9.359234, Accuracy 66.557%\n",
      "Epoch 9, Batch 660, LR 0.000090 Loss 9.359573, Accuracy 66.555%\n",
      "Epoch 9, Batch 661, LR 0.000090 Loss 9.359761, Accuracy 66.562%\n",
      "Epoch 9, Batch 662, LR 0.000090 Loss 9.359750, Accuracy 66.568%\n",
      "Epoch 9, Batch 663, LR 0.000090 Loss 9.360163, Accuracy 66.567%\n",
      "Epoch 9, Batch 664, LR 0.000090 Loss 9.360729, Accuracy 66.564%\n",
      "Epoch 9, Batch 665, LR 0.000090 Loss 9.360725, Accuracy 66.562%\n",
      "Epoch 9, Batch 666, LR 0.000090 Loss 9.359774, Accuracy 66.563%\n",
      "Epoch 9, Batch 667, LR 0.000090 Loss 9.359667, Accuracy 66.562%\n",
      "Epoch 9, Batch 668, LR 0.000090 Loss 9.359067, Accuracy 66.570%\n",
      "Epoch 9, Batch 669, LR 0.000090 Loss 9.357691, Accuracy 66.586%\n",
      "Epoch 9, Batch 670, LR 0.000090 Loss 9.356507, Accuracy 66.593%\n",
      "Epoch 9, Batch 671, LR 0.000090 Loss 9.357236, Accuracy 66.586%\n",
      "Epoch 9, Batch 672, LR 0.000090 Loss 9.356155, Accuracy 66.588%\n",
      "Epoch 9, Batch 673, LR 0.000090 Loss 9.356234, Accuracy 66.584%\n",
      "Epoch 9, Batch 674, LR 0.000090 Loss 9.356239, Accuracy 66.573%\n",
      "Epoch 9, Batch 675, LR 0.000090 Loss 9.356756, Accuracy 66.569%\n",
      "Epoch 9, Batch 676, LR 0.000090 Loss 9.356722, Accuracy 66.563%\n",
      "Epoch 9, Batch 677, LR 0.000090 Loss 9.357845, Accuracy 66.552%\n",
      "Epoch 9, Batch 678, LR 0.000090 Loss 9.356781, Accuracy 66.562%\n",
      "Epoch 9, Batch 679, LR 0.000090 Loss 9.357336, Accuracy 66.562%\n",
      "Epoch 9, Batch 680, LR 0.000090 Loss 9.357739, Accuracy 66.560%\n",
      "Epoch 9, Batch 681, LR 0.000090 Loss 9.356820, Accuracy 66.562%\n",
      "Epoch 9, Batch 682, LR 0.000090 Loss 9.356985, Accuracy 66.559%\n",
      "Epoch 9, Batch 683, LR 0.000090 Loss 9.356906, Accuracy 66.555%\n",
      "Epoch 9, Batch 684, LR 0.000090 Loss 9.357458, Accuracy 66.548%\n",
      "Epoch 9, Batch 685, LR 0.000090 Loss 9.356969, Accuracy 66.550%\n",
      "Epoch 9, Batch 686, LR 0.000090 Loss 9.356417, Accuracy 66.547%\n",
      "Epoch 9, Batch 687, LR 0.000090 Loss 9.356609, Accuracy 66.550%\n",
      "Epoch 9, Batch 688, LR 0.000090 Loss 9.356409, Accuracy 66.545%\n",
      "Epoch 9, Batch 689, LR 0.000090 Loss 9.356440, Accuracy 66.548%\n",
      "Epoch 9, Batch 690, LR 0.000090 Loss 9.356274, Accuracy 66.548%\n",
      "Epoch 9, Batch 691, LR 0.000090 Loss 9.356980, Accuracy 66.548%\n",
      "Epoch 9, Batch 692, LR 0.000090 Loss 9.356768, Accuracy 66.556%\n",
      "Epoch 9, Batch 693, LR 0.000090 Loss 9.356255, Accuracy 66.557%\n",
      "Epoch 9, Batch 694, LR 0.000090 Loss 9.356549, Accuracy 66.553%\n",
      "Epoch 9, Batch 695, LR 0.000090 Loss 9.356492, Accuracy 66.548%\n",
      "Epoch 9, Batch 696, LR 0.000090 Loss 9.356517, Accuracy 66.554%\n",
      "Epoch 9, Batch 697, LR 0.000090 Loss 9.356809, Accuracy 66.550%\n",
      "Epoch 9, Batch 698, LR 0.000090 Loss 9.356716, Accuracy 66.553%\n",
      "Epoch 9, Batch 699, LR 0.000090 Loss 9.357028, Accuracy 66.553%\n",
      "Epoch 9, Batch 700, LR 0.000090 Loss 9.356042, Accuracy 66.565%\n",
      "Epoch 9, Batch 701, LR 0.000090 Loss 9.355579, Accuracy 66.568%\n",
      "Epoch 9, Batch 702, LR 0.000090 Loss 9.354944, Accuracy 66.572%\n",
      "Epoch 9, Batch 703, LR 0.000090 Loss 9.354949, Accuracy 66.572%\n",
      "Epoch 9, Batch 704, LR 0.000090 Loss 9.356045, Accuracy 66.564%\n",
      "Epoch 9, Batch 705, LR 0.000090 Loss 9.355083, Accuracy 66.575%\n",
      "Epoch 9, Batch 706, LR 0.000090 Loss 9.354402, Accuracy 66.582%\n",
      "Epoch 9, Batch 707, LR 0.000090 Loss 9.354552, Accuracy 66.580%\n",
      "Epoch 9, Batch 708, LR 0.000090 Loss 9.354972, Accuracy 66.574%\n",
      "Epoch 9, Batch 709, LR 0.000090 Loss 9.354709, Accuracy 66.580%\n",
      "Epoch 9, Batch 710, LR 0.000090 Loss 9.355272, Accuracy 66.572%\n",
      "Epoch 9, Batch 711, LR 0.000090 Loss 9.355889, Accuracy 66.568%\n",
      "Epoch 9, Batch 712, LR 0.000090 Loss 9.354752, Accuracy 66.577%\n",
      "Epoch 9, Batch 713, LR 0.000090 Loss 9.353831, Accuracy 66.584%\n",
      "Epoch 9, Batch 714, LR 0.000090 Loss 9.353305, Accuracy 66.590%\n",
      "Epoch 9, Batch 715, LR 0.000090 Loss 9.353553, Accuracy 66.590%\n",
      "Epoch 9, Batch 716, LR 0.000090 Loss 9.353854, Accuracy 66.580%\n",
      "Epoch 9, Batch 717, LR 0.000090 Loss 9.352909, Accuracy 66.593%\n",
      "Epoch 9, Batch 718, LR 0.000090 Loss 9.353126, Accuracy 66.587%\n",
      "Epoch 9, Batch 719, LR 0.000090 Loss 9.353876, Accuracy 66.584%\n",
      "Epoch 9, Batch 720, LR 0.000090 Loss 9.354654, Accuracy 66.580%\n",
      "Epoch 9, Batch 721, LR 0.000090 Loss 9.355308, Accuracy 66.581%\n",
      "Epoch 9, Batch 722, LR 0.000090 Loss 9.354716, Accuracy 66.585%\n",
      "Epoch 9, Batch 723, LR 0.000090 Loss 9.353973, Accuracy 66.590%\n",
      "Epoch 9, Batch 724, LR 0.000090 Loss 9.353390, Accuracy 66.591%\n",
      "Epoch 9, Batch 725, LR 0.000090 Loss 9.352685, Accuracy 66.601%\n",
      "Epoch 9, Batch 726, LR 0.000090 Loss 9.352335, Accuracy 66.598%\n",
      "Epoch 9, Batch 727, LR 0.000090 Loss 9.353062, Accuracy 66.592%\n",
      "Epoch 9, Batch 728, LR 0.000090 Loss 9.351871, Accuracy 66.600%\n",
      "Epoch 9, Batch 729, LR 0.000090 Loss 9.351952, Accuracy 66.603%\n",
      "Epoch 9, Batch 730, LR 0.000090 Loss 9.352579, Accuracy 66.609%\n",
      "Epoch 9, Batch 731, LR 0.000090 Loss 9.352993, Accuracy 66.601%\n",
      "Epoch 9, Batch 732, LR 0.000090 Loss 9.351454, Accuracy 66.600%\n",
      "Epoch 9, Batch 733, LR 0.000090 Loss 9.351543, Accuracy 66.602%\n",
      "Epoch 9, Batch 734, LR 0.000090 Loss 9.351747, Accuracy 66.600%\n",
      "Epoch 9, Batch 735, LR 0.000090 Loss 9.351789, Accuracy 66.603%\n",
      "Epoch 9, Batch 736, LR 0.000090 Loss 9.351240, Accuracy 66.606%\n",
      "Epoch 9, Batch 737, LR 0.000090 Loss 9.351223, Accuracy 66.606%\n",
      "Epoch 9, Batch 738, LR 0.000090 Loss 9.351502, Accuracy 66.607%\n",
      "Epoch 9, Batch 739, LR 0.000090 Loss 9.351578, Accuracy 66.611%\n",
      "Epoch 9, Batch 740, LR 0.000090 Loss 9.351283, Accuracy 66.612%\n",
      "Epoch 9, Batch 741, LR 0.000090 Loss 9.352168, Accuracy 66.607%\n",
      "Epoch 9, Batch 742, LR 0.000090 Loss 9.352839, Accuracy 66.598%\n",
      "Epoch 9, Batch 743, LR 0.000090 Loss 9.352207, Accuracy 66.599%\n",
      "Epoch 9, Batch 744, LR 0.000090 Loss 9.351344, Accuracy 66.611%\n",
      "Epoch 9, Batch 745, LR 0.000090 Loss 9.350992, Accuracy 66.619%\n",
      "Epoch 9, Batch 746, LR 0.000090 Loss 9.350891, Accuracy 66.623%\n",
      "Epoch 9, Batch 747, LR 0.000090 Loss 9.350777, Accuracy 66.619%\n",
      "Epoch 9, Batch 748, LR 0.000090 Loss 9.350379, Accuracy 66.626%\n",
      "Epoch 9, Batch 749, LR 0.000090 Loss 9.349473, Accuracy 66.632%\n",
      "Epoch 9, Batch 750, LR 0.000090 Loss 9.347664, Accuracy 66.641%\n",
      "Epoch 9, Batch 751, LR 0.000090 Loss 9.347520, Accuracy 66.644%\n",
      "Epoch 9, Batch 752, LR 0.000090 Loss 9.346804, Accuracy 66.646%\n",
      "Epoch 9, Batch 753, LR 0.000090 Loss 9.346609, Accuracy 66.652%\n",
      "Epoch 9, Batch 754, LR 0.000090 Loss 9.347321, Accuracy 66.640%\n",
      "Epoch 9, Batch 755, LR 0.000090 Loss 9.346748, Accuracy 66.642%\n",
      "Epoch 9, Batch 756, LR 0.000090 Loss 9.345638, Accuracy 66.651%\n",
      "Epoch 9, Batch 757, LR 0.000090 Loss 9.346544, Accuracy 66.645%\n",
      "Epoch 9, Batch 758, LR 0.000090 Loss 9.345649, Accuracy 66.649%\n",
      "Epoch 9, Batch 759, LR 0.000090 Loss 9.345357, Accuracy 66.650%\n",
      "Epoch 9, Batch 760, LR 0.000090 Loss 9.344525, Accuracy 66.648%\n",
      "Epoch 9, Batch 761, LR 0.000090 Loss 9.343467, Accuracy 66.652%\n",
      "Epoch 9, Batch 762, LR 0.000090 Loss 9.342832, Accuracy 66.648%\n",
      "Epoch 9, Batch 763, LR 0.000090 Loss 9.342396, Accuracy 66.655%\n",
      "Epoch 9, Batch 764, LR 0.000090 Loss 9.342416, Accuracy 66.650%\n",
      "Epoch 9, Batch 765, LR 0.000090 Loss 9.342488, Accuracy 66.651%\n",
      "Epoch 9, Batch 766, LR 0.000090 Loss 9.343796, Accuracy 66.641%\n",
      "Epoch 9, Batch 767, LR 0.000090 Loss 9.345199, Accuracy 66.634%\n",
      "Epoch 9, Batch 768, LR 0.000090 Loss 9.344228, Accuracy 66.640%\n",
      "Epoch 9, Batch 769, LR 0.000090 Loss 9.344018, Accuracy 66.638%\n",
      "Epoch 9, Batch 770, LR 0.000090 Loss 9.343609, Accuracy 66.644%\n",
      "Epoch 9, Batch 771, LR 0.000090 Loss 9.343159, Accuracy 66.645%\n",
      "Epoch 9, Batch 772, LR 0.000090 Loss 9.343241, Accuracy 66.646%\n",
      "Epoch 9, Batch 773, LR 0.000090 Loss 9.342498, Accuracy 66.651%\n",
      "Epoch 9, Batch 774, LR 0.000090 Loss 9.343626, Accuracy 66.645%\n",
      "Epoch 9, Batch 775, LR 0.000090 Loss 9.343285, Accuracy 66.646%\n",
      "Epoch 9, Batch 776, LR 0.000090 Loss 9.343654, Accuracy 66.645%\n",
      "Epoch 9, Batch 777, LR 0.000090 Loss 9.344387, Accuracy 66.639%\n",
      "Epoch 9, Batch 778, LR 0.000090 Loss 9.343604, Accuracy 66.639%\n",
      "Epoch 9, Batch 779, LR 0.000090 Loss 9.343920, Accuracy 66.638%\n",
      "Epoch 9, Batch 780, LR 0.000090 Loss 9.344182, Accuracy 66.630%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 781, LR 0.000090 Loss 9.343914, Accuracy 66.629%\n",
      "Epoch 9, Batch 782, LR 0.000090 Loss 9.343680, Accuracy 66.631%\n",
      "Epoch 9, Batch 783, LR 0.000090 Loss 9.343101, Accuracy 66.637%\n",
      "Epoch 9, Batch 784, LR 0.000090 Loss 9.342529, Accuracy 66.643%\n",
      "Epoch 9, Batch 785, LR 0.000090 Loss 9.342597, Accuracy 66.646%\n",
      "Epoch 9, Batch 786, LR 0.000090 Loss 9.342709, Accuracy 66.647%\n",
      "Epoch 9, Batch 787, LR 0.000090 Loss 9.342685, Accuracy 66.649%\n",
      "Epoch 9, Batch 788, LR 0.000090 Loss 9.343304, Accuracy 66.653%\n",
      "Epoch 9, Batch 789, LR 0.000090 Loss 9.343032, Accuracy 66.658%\n",
      "Epoch 9, Batch 790, LR 0.000090 Loss 9.343014, Accuracy 66.660%\n",
      "Epoch 9, Batch 791, LR 0.000090 Loss 9.341528, Accuracy 66.674%\n",
      "Epoch 9, Batch 792, LR 0.000090 Loss 9.341032, Accuracy 66.675%\n",
      "Epoch 9, Batch 793, LR 0.000090 Loss 9.340824, Accuracy 66.672%\n",
      "Epoch 9, Batch 794, LR 0.000090 Loss 9.339708, Accuracy 66.683%\n",
      "Epoch 9, Batch 795, LR 0.000090 Loss 9.339409, Accuracy 66.689%\n",
      "Epoch 9, Batch 796, LR 0.000090 Loss 9.340457, Accuracy 66.683%\n",
      "Epoch 9, Batch 797, LR 0.000090 Loss 9.340272, Accuracy 66.685%\n",
      "Epoch 9, Batch 798, LR 0.000090 Loss 9.339622, Accuracy 66.684%\n",
      "Epoch 9, Batch 799, LR 0.000090 Loss 9.340341, Accuracy 66.681%\n",
      "Epoch 9, Batch 800, LR 0.000090 Loss 9.339623, Accuracy 66.683%\n",
      "Epoch 9, Batch 801, LR 0.000090 Loss 9.340079, Accuracy 66.681%\n",
      "Epoch 9, Batch 802, LR 0.000090 Loss 9.339783, Accuracy 66.685%\n",
      "Epoch 9, Batch 803, LR 0.000090 Loss 9.339545, Accuracy 66.685%\n",
      "Epoch 9, Batch 804, LR 0.000090 Loss 9.339904, Accuracy 66.689%\n",
      "Epoch 9, Batch 805, LR 0.000090 Loss 9.339289, Accuracy 66.689%\n",
      "Epoch 9, Batch 806, LR 0.000090 Loss 9.338743, Accuracy 66.690%\n",
      "Epoch 9, Batch 807, LR 0.000090 Loss 9.339136, Accuracy 66.690%\n",
      "Epoch 9, Batch 808, LR 0.000090 Loss 9.338281, Accuracy 66.694%\n",
      "Epoch 9, Batch 809, LR 0.000090 Loss 9.337335, Accuracy 66.701%\n",
      "Epoch 9, Batch 810, LR 0.000090 Loss 9.337165, Accuracy 66.701%\n",
      "Epoch 9, Batch 811, LR 0.000090 Loss 9.336249, Accuracy 66.713%\n",
      "Epoch 9, Batch 812, LR 0.000090 Loss 9.335885, Accuracy 66.715%\n",
      "Epoch 9, Batch 813, LR 0.000090 Loss 9.334664, Accuracy 66.727%\n",
      "Epoch 9, Batch 814, LR 0.000090 Loss 9.334771, Accuracy 66.725%\n",
      "Epoch 9, Batch 815, LR 0.000090 Loss 9.333828, Accuracy 66.731%\n",
      "Epoch 9, Batch 816, LR 0.000090 Loss 9.333400, Accuracy 66.730%\n",
      "Epoch 9, Batch 817, LR 0.000090 Loss 9.333414, Accuracy 66.727%\n",
      "Epoch 9, Batch 818, LR 0.000090 Loss 9.334177, Accuracy 66.724%\n",
      "Epoch 9, Batch 819, LR 0.000090 Loss 9.334671, Accuracy 66.722%\n",
      "Epoch 9, Batch 820, LR 0.000090 Loss 9.333969, Accuracy 66.722%\n",
      "Epoch 9, Batch 821, LR 0.000090 Loss 9.333615, Accuracy 66.724%\n",
      "Epoch 9, Batch 822, LR 0.000090 Loss 9.334076, Accuracy 66.717%\n",
      "Epoch 9, Batch 823, LR 0.000090 Loss 9.332539, Accuracy 66.728%\n",
      "Epoch 9, Batch 824, LR 0.000090 Loss 9.331620, Accuracy 66.738%\n",
      "Epoch 9, Batch 825, LR 0.000090 Loss 9.331509, Accuracy 66.740%\n",
      "Epoch 9, Batch 826, LR 0.000090 Loss 9.330903, Accuracy 66.748%\n",
      "Epoch 9, Batch 827, LR 0.000090 Loss 9.330797, Accuracy 66.753%\n",
      "Epoch 9, Batch 828, LR 0.000090 Loss 9.329204, Accuracy 66.761%\n",
      "Epoch 9, Batch 829, LR 0.000090 Loss 9.328038, Accuracy 66.767%\n",
      "Epoch 9, Batch 830, LR 0.000090 Loss 9.328356, Accuracy 66.762%\n",
      "Epoch 9, Batch 831, LR 0.000090 Loss 9.327836, Accuracy 66.765%\n",
      "Epoch 9, Batch 832, LR 0.000090 Loss 9.327976, Accuracy 66.761%\n",
      "Epoch 9, Batch 833, LR 0.000090 Loss 9.328519, Accuracy 66.759%\n",
      "Epoch 9, Batch 834, LR 0.000090 Loss 9.328068, Accuracy 66.759%\n",
      "Epoch 9, Batch 835, LR 0.000090 Loss 9.327118, Accuracy 66.766%\n",
      "Epoch 9, Batch 836, LR 0.000090 Loss 9.327399, Accuracy 66.770%\n",
      "Epoch 9, Batch 837, LR 0.000090 Loss 9.326111, Accuracy 66.780%\n",
      "Epoch 9, Batch 838, LR 0.000090 Loss 9.325526, Accuracy 66.785%\n",
      "Epoch 9, Batch 839, LR 0.000090 Loss 9.324753, Accuracy 66.790%\n",
      "Epoch 9, Batch 840, LR 0.000090 Loss 9.324029, Accuracy 66.791%\n",
      "Epoch 9, Batch 841, LR 0.000090 Loss 9.323204, Accuracy 66.794%\n",
      "Epoch 9, Batch 842, LR 0.000090 Loss 9.323446, Accuracy 66.791%\n",
      "Epoch 9, Batch 843, LR 0.000090 Loss 9.321517, Accuracy 66.802%\n",
      "Epoch 9, Batch 844, LR 0.000090 Loss 9.321585, Accuracy 66.798%\n",
      "Epoch 9, Batch 845, LR 0.000090 Loss 9.321677, Accuracy 66.795%\n",
      "Epoch 9, Batch 846, LR 0.000090 Loss 9.321324, Accuracy 66.793%\n",
      "Epoch 9, Batch 847, LR 0.000090 Loss 9.320921, Accuracy 66.797%\n",
      "Epoch 9, Batch 848, LR 0.000090 Loss 9.321208, Accuracy 66.802%\n",
      "Epoch 9, Batch 849, LR 0.000090 Loss 9.321582, Accuracy 66.795%\n",
      "Epoch 9, Batch 850, LR 0.000090 Loss 9.320801, Accuracy 66.801%\n",
      "Epoch 9, Batch 851, LR 0.000090 Loss 9.321290, Accuracy 66.798%\n",
      "Epoch 9, Batch 852, LR 0.000090 Loss 9.321375, Accuracy 66.795%\n",
      "Epoch 9, Batch 853, LR 0.000090 Loss 9.320947, Accuracy 66.796%\n",
      "Epoch 9, Batch 854, LR 0.000090 Loss 9.321209, Accuracy 66.799%\n",
      "Epoch 9, Batch 855, LR 0.000090 Loss 9.321065, Accuracy 66.801%\n",
      "Epoch 9, Batch 856, LR 0.000090 Loss 9.321185, Accuracy 66.804%\n",
      "Epoch 9, Batch 857, LR 0.000090 Loss 9.320523, Accuracy 66.815%\n",
      "Epoch 9, Batch 858, LR 0.000090 Loss 9.320437, Accuracy 66.816%\n",
      "Epoch 9, Batch 859, LR 0.000090 Loss 9.319823, Accuracy 66.815%\n",
      "Epoch 9, Batch 860, LR 0.000090 Loss 9.319106, Accuracy 66.813%\n",
      "Epoch 9, Batch 861, LR 0.000090 Loss 9.318421, Accuracy 66.820%\n",
      "Epoch 9, Batch 862, LR 0.000090 Loss 9.318438, Accuracy 66.820%\n",
      "Epoch 9, Batch 863, LR 0.000090 Loss 9.318553, Accuracy 66.822%\n",
      "Epoch 9, Batch 864, LR 0.000090 Loss 9.317115, Accuracy 66.826%\n",
      "Epoch 9, Batch 865, LR 0.000090 Loss 9.318112, Accuracy 66.814%\n",
      "Epoch 9, Batch 866, LR 0.000090 Loss 9.318446, Accuracy 66.815%\n",
      "Epoch 9, Batch 867, LR 0.000090 Loss 9.318070, Accuracy 66.819%\n",
      "Epoch 9, Batch 868, LR 0.000090 Loss 9.318378, Accuracy 66.815%\n",
      "Epoch 9, Batch 869, LR 0.000090 Loss 9.318457, Accuracy 66.812%\n",
      "Epoch 9, Batch 870, LR 0.000090 Loss 9.317938, Accuracy 66.811%\n",
      "Epoch 9, Batch 871, LR 0.000090 Loss 9.318138, Accuracy 66.815%\n",
      "Epoch 9, Batch 872, LR 0.000090 Loss 9.318343, Accuracy 66.809%\n",
      "Epoch 9, Batch 873, LR 0.000090 Loss 9.319032, Accuracy 66.801%\n",
      "Epoch 9, Batch 874, LR 0.000090 Loss 9.319009, Accuracy 66.804%\n",
      "Epoch 9, Batch 875, LR 0.000090 Loss 9.319063, Accuracy 66.800%\n",
      "Epoch 9, Batch 876, LR 0.000090 Loss 9.319188, Accuracy 66.801%\n",
      "Epoch 9, Batch 877, LR 0.000090 Loss 9.319203, Accuracy 66.803%\n",
      "Epoch 9, Batch 878, LR 0.000090 Loss 9.319093, Accuracy 66.799%\n",
      "Epoch 9, Batch 879, LR 0.000090 Loss 9.318249, Accuracy 66.802%\n",
      "Epoch 9, Batch 880, LR 0.000090 Loss 9.317660, Accuracy 66.809%\n",
      "Epoch 9, Batch 881, LR 0.000090 Loss 9.317361, Accuracy 66.811%\n",
      "Epoch 9, Batch 882, LR 0.000090 Loss 9.315964, Accuracy 66.818%\n",
      "Epoch 9, Batch 883, LR 0.000090 Loss 9.315947, Accuracy 66.813%\n",
      "Epoch 9, Batch 884, LR 0.000090 Loss 9.316008, Accuracy 66.818%\n",
      "Epoch 9, Batch 885, LR 0.000090 Loss 9.315460, Accuracy 66.822%\n",
      "Epoch 9, Batch 886, LR 0.000090 Loss 9.315434, Accuracy 66.820%\n",
      "Epoch 9, Batch 887, LR 0.000090 Loss 9.314799, Accuracy 66.824%\n",
      "Epoch 9, Batch 888, LR 0.000090 Loss 9.314955, Accuracy 66.818%\n",
      "Epoch 9, Batch 889, LR 0.000090 Loss 9.314555, Accuracy 66.821%\n",
      "Epoch 9, Batch 890, LR 0.000090 Loss 9.314397, Accuracy 66.821%\n",
      "Epoch 9, Batch 891, LR 0.000090 Loss 9.314676, Accuracy 66.820%\n",
      "Epoch 9, Batch 892, LR 0.000090 Loss 9.315111, Accuracy 66.816%\n",
      "Epoch 9, Batch 893, LR 0.000090 Loss 9.314716, Accuracy 66.820%\n",
      "Epoch 9, Batch 894, LR 0.000090 Loss 9.313497, Accuracy 66.828%\n",
      "Epoch 9, Batch 895, LR 0.000090 Loss 9.313343, Accuracy 66.830%\n",
      "Epoch 9, Batch 896, LR 0.000090 Loss 9.313311, Accuracy 66.831%\n",
      "Epoch 9, Batch 897, LR 0.000090 Loss 9.313088, Accuracy 66.828%\n",
      "Epoch 9, Batch 898, LR 0.000090 Loss 9.313116, Accuracy 66.823%\n",
      "Epoch 9, Batch 899, LR 0.000090 Loss 9.312898, Accuracy 66.824%\n",
      "Epoch 9, Batch 900, LR 0.000090 Loss 9.311698, Accuracy 66.829%\n",
      "Epoch 9, Batch 901, LR 0.000090 Loss 9.311549, Accuracy 66.830%\n",
      "Epoch 9, Batch 902, LR 0.000090 Loss 9.311557, Accuracy 66.832%\n",
      "Epoch 9, Batch 903, LR 0.000090 Loss 9.311484, Accuracy 66.838%\n",
      "Epoch 9, Batch 904, LR 0.000090 Loss 9.312022, Accuracy 66.838%\n",
      "Epoch 9, Batch 905, LR 0.000090 Loss 9.311992, Accuracy 66.840%\n",
      "Epoch 9, Batch 906, LR 0.000090 Loss 9.312015, Accuracy 66.841%\n",
      "Epoch 9, Batch 907, LR 0.000090 Loss 9.311726, Accuracy 66.840%\n",
      "Epoch 9, Batch 908, LR 0.000090 Loss 9.311010, Accuracy 66.844%\n",
      "Epoch 9, Batch 909, LR 0.000090 Loss 9.309906, Accuracy 66.855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 910, LR 0.000090 Loss 9.308231, Accuracy 66.866%\n",
      "Epoch 9, Batch 911, LR 0.000090 Loss 9.307891, Accuracy 66.868%\n",
      "Epoch 9, Batch 912, LR 0.000090 Loss 9.307202, Accuracy 66.874%\n",
      "Epoch 9, Batch 913, LR 0.000090 Loss 9.306908, Accuracy 66.876%\n",
      "Epoch 9, Batch 914, LR 0.000090 Loss 9.306095, Accuracy 66.880%\n",
      "Epoch 9, Batch 915, LR 0.000090 Loss 9.306590, Accuracy 66.874%\n",
      "Epoch 9, Batch 916, LR 0.000090 Loss 9.306713, Accuracy 66.875%\n",
      "Epoch 9, Batch 917, LR 0.000090 Loss 9.306045, Accuracy 66.882%\n",
      "Epoch 9, Batch 918, LR 0.000090 Loss 9.306306, Accuracy 66.877%\n",
      "Epoch 9, Batch 919, LR 0.000090 Loss 9.305423, Accuracy 66.885%\n",
      "Epoch 9, Batch 920, LR 0.000090 Loss 9.304598, Accuracy 66.888%\n",
      "Epoch 9, Batch 921, LR 0.000090 Loss 9.304370, Accuracy 66.889%\n",
      "Epoch 9, Batch 922, LR 0.000090 Loss 9.305238, Accuracy 66.877%\n",
      "Epoch 9, Batch 923, LR 0.000090 Loss 9.304472, Accuracy 66.882%\n",
      "Epoch 9, Batch 924, LR 0.000090 Loss 9.303855, Accuracy 66.881%\n",
      "Epoch 9, Batch 925, LR 0.000090 Loss 9.302757, Accuracy 66.883%\n",
      "Epoch 9, Batch 926, LR 0.000090 Loss 9.302283, Accuracy 66.883%\n",
      "Epoch 9, Batch 927, LR 0.000090 Loss 9.301834, Accuracy 66.885%\n",
      "Epoch 9, Batch 928, LR 0.000090 Loss 9.301672, Accuracy 66.887%\n",
      "Epoch 9, Batch 929, LR 0.000090 Loss 9.301796, Accuracy 66.883%\n",
      "Epoch 9, Batch 930, LR 0.000090 Loss 9.301789, Accuracy 66.888%\n",
      "Epoch 9, Batch 931, LR 0.000090 Loss 9.302309, Accuracy 66.887%\n",
      "Epoch 9, Batch 932, LR 0.000090 Loss 9.302281, Accuracy 66.882%\n",
      "Epoch 9, Batch 933, LR 0.000090 Loss 9.302282, Accuracy 66.875%\n",
      "Epoch 9, Batch 934, LR 0.000090 Loss 9.302452, Accuracy 66.877%\n",
      "Epoch 9, Batch 935, LR 0.000090 Loss 9.302464, Accuracy 66.881%\n",
      "Epoch 9, Batch 936, LR 0.000090 Loss 9.301767, Accuracy 66.887%\n",
      "Epoch 9, Batch 937, LR 0.000090 Loss 9.301223, Accuracy 66.891%\n",
      "Epoch 9, Batch 938, LR 0.000090 Loss 9.301698, Accuracy 66.887%\n",
      "Epoch 9, Batch 939, LR 0.000090 Loss 9.301902, Accuracy 66.885%\n",
      "Epoch 9, Batch 940, LR 0.000090 Loss 9.301636, Accuracy 66.887%\n",
      "Epoch 9, Batch 941, LR 0.000090 Loss 9.301504, Accuracy 66.890%\n",
      "Epoch 9, Batch 942, LR 0.000090 Loss 9.300947, Accuracy 66.891%\n",
      "Epoch 9, Batch 943, LR 0.000090 Loss 9.300783, Accuracy 66.890%\n",
      "Epoch 9, Batch 944, LR 0.000090 Loss 9.301967, Accuracy 66.886%\n",
      "Epoch 9, Batch 945, LR 0.000090 Loss 9.300980, Accuracy 66.894%\n",
      "Epoch 9, Batch 946, LR 0.000090 Loss 9.299621, Accuracy 66.901%\n",
      "Epoch 9, Batch 947, LR 0.000090 Loss 9.298360, Accuracy 66.908%\n",
      "Epoch 9, Batch 948, LR 0.000090 Loss 9.297870, Accuracy 66.908%\n",
      "Epoch 9, Batch 949, LR 0.000090 Loss 9.297688, Accuracy 66.907%\n",
      "Epoch 9, Batch 950, LR 0.000090 Loss 9.296628, Accuracy 66.914%\n",
      "Epoch 9, Batch 951, LR 0.000090 Loss 9.296620, Accuracy 66.913%\n",
      "Epoch 9, Batch 952, LR 0.000090 Loss 9.296920, Accuracy 66.907%\n",
      "Epoch 9, Batch 953, LR 0.000090 Loss 9.296905, Accuracy 66.910%\n",
      "Epoch 9, Batch 954, LR 0.000090 Loss 9.295972, Accuracy 66.910%\n",
      "Epoch 9, Batch 955, LR 0.000090 Loss 9.296082, Accuracy 66.904%\n",
      "Epoch 9, Batch 956, LR 0.000090 Loss 9.294522, Accuracy 66.915%\n",
      "Epoch 9, Batch 957, LR 0.000090 Loss 9.295045, Accuracy 66.915%\n",
      "Epoch 9, Batch 958, LR 0.000090 Loss 9.295460, Accuracy 66.913%\n",
      "Epoch 9, Batch 959, LR 0.000090 Loss 9.294961, Accuracy 66.915%\n",
      "Epoch 9, Batch 960, LR 0.000090 Loss 9.295776, Accuracy 66.907%\n",
      "Epoch 9, Batch 961, LR 0.000090 Loss 9.295002, Accuracy 66.910%\n",
      "Epoch 9, Batch 962, LR 0.000090 Loss 9.294728, Accuracy 66.908%\n",
      "Epoch 9, Batch 963, LR 0.000090 Loss 9.294545, Accuracy 66.911%\n",
      "Epoch 9, Batch 964, LR 0.000090 Loss 9.294443, Accuracy 66.910%\n",
      "Epoch 9, Batch 965, LR 0.000090 Loss 9.293602, Accuracy 66.911%\n",
      "Epoch 9, Batch 966, LR 0.000090 Loss 9.293600, Accuracy 66.911%\n",
      "Epoch 9, Batch 967, LR 0.000090 Loss 9.293433, Accuracy 66.911%\n",
      "Epoch 9, Batch 968, LR 0.000090 Loss 9.293473, Accuracy 66.912%\n",
      "Epoch 9, Batch 969, LR 0.000090 Loss 9.292458, Accuracy 66.917%\n",
      "Epoch 9, Batch 970, LR 0.000090 Loss 9.292259, Accuracy 66.915%\n",
      "Epoch 9, Batch 971, LR 0.000090 Loss 9.291191, Accuracy 66.927%\n",
      "Epoch 9, Batch 972, LR 0.000090 Loss 9.291700, Accuracy 66.929%\n",
      "Epoch 9, Batch 973, LR 0.000090 Loss 9.291719, Accuracy 66.933%\n",
      "Epoch 9, Batch 974, LR 0.000090 Loss 9.291997, Accuracy 66.936%\n",
      "Epoch 9, Batch 975, LR 0.000090 Loss 9.291065, Accuracy 66.947%\n",
      "Epoch 9, Batch 976, LR 0.000090 Loss 9.291403, Accuracy 66.947%\n",
      "Epoch 9, Batch 977, LR 0.000090 Loss 9.290002, Accuracy 66.958%\n",
      "Epoch 9, Batch 978, LR 0.000090 Loss 9.289160, Accuracy 66.960%\n",
      "Epoch 9, Batch 979, LR 0.000090 Loss 9.289099, Accuracy 66.964%\n",
      "Epoch 9, Batch 980, LR 0.000090 Loss 9.289456, Accuracy 66.962%\n",
      "Epoch 9, Batch 981, LR 0.000090 Loss 9.288643, Accuracy 66.968%\n",
      "Epoch 9, Batch 982, LR 0.000090 Loss 9.287987, Accuracy 66.975%\n",
      "Epoch 9, Batch 983, LR 0.000090 Loss 9.287435, Accuracy 66.976%\n",
      "Epoch 9, Batch 984, LR 0.000090 Loss 9.287687, Accuracy 66.970%\n",
      "Epoch 9, Batch 985, LR 0.000090 Loss 9.287443, Accuracy 66.970%\n",
      "Epoch 9, Batch 986, LR 0.000090 Loss 9.287011, Accuracy 66.972%\n",
      "Epoch 9, Batch 987, LR 0.000090 Loss 9.286830, Accuracy 66.969%\n",
      "Epoch 9, Batch 988, LR 0.000090 Loss 9.286437, Accuracy 66.971%\n",
      "Epoch 9, Batch 989, LR 0.000090 Loss 9.285814, Accuracy 66.972%\n",
      "Epoch 9, Batch 990, LR 0.000090 Loss 9.285253, Accuracy 66.971%\n",
      "Epoch 9, Batch 991, LR 0.000090 Loss 9.285287, Accuracy 66.968%\n",
      "Epoch 9, Batch 992, LR 0.000090 Loss 9.284884, Accuracy 66.969%\n",
      "Epoch 9, Batch 993, LR 0.000090 Loss 9.284761, Accuracy 66.967%\n",
      "Epoch 9, Batch 994, LR 0.000090 Loss 9.284028, Accuracy 66.971%\n",
      "Epoch 9, Batch 995, LR 0.000090 Loss 9.283429, Accuracy 66.975%\n",
      "Epoch 9, Batch 996, LR 0.000090 Loss 9.282898, Accuracy 66.975%\n",
      "Epoch 9, Batch 997, LR 0.000090 Loss 9.282635, Accuracy 66.978%\n",
      "Epoch 9, Batch 998, LR 0.000090 Loss 9.282722, Accuracy 66.979%\n",
      "Epoch 9, Batch 999, LR 0.000090 Loss 9.283100, Accuracy 66.979%\n",
      "Epoch 9, Batch 1000, LR 0.000090 Loss 9.282449, Accuracy 66.981%\n",
      "Epoch 9, Batch 1001, LR 0.000090 Loss 9.282097, Accuracy 66.986%\n",
      "Epoch 9, Batch 1002, LR 0.000090 Loss 9.281891, Accuracy 66.983%\n",
      "Epoch 9, Batch 1003, LR 0.000090 Loss 9.281074, Accuracy 66.985%\n",
      "Epoch 9, Batch 1004, LR 0.000090 Loss 9.280551, Accuracy 66.988%\n",
      "Epoch 9, Batch 1005, LR 0.000090 Loss 9.281333, Accuracy 66.981%\n",
      "Epoch 9, Batch 1006, LR 0.000090 Loss 9.280600, Accuracy 66.982%\n",
      "Epoch 9, Batch 1007, LR 0.000090 Loss 9.280599, Accuracy 66.983%\n",
      "Epoch 9, Batch 1008, LR 0.000090 Loss 9.280464, Accuracy 66.982%\n",
      "Epoch 9, Batch 1009, LR 0.000090 Loss 9.279702, Accuracy 66.987%\n",
      "Epoch 9, Batch 1010, LR 0.000090 Loss 9.279615, Accuracy 66.983%\n",
      "Epoch 9, Batch 1011, LR 0.000090 Loss 9.278674, Accuracy 66.988%\n",
      "Epoch 9, Batch 1012, LR 0.000090 Loss 9.278742, Accuracy 66.983%\n",
      "Epoch 9, Batch 1013, LR 0.000090 Loss 9.279242, Accuracy 66.978%\n",
      "Epoch 9, Batch 1014, LR 0.000090 Loss 9.278715, Accuracy 66.981%\n",
      "Epoch 9, Batch 1015, LR 0.000090 Loss 9.278178, Accuracy 66.987%\n",
      "Epoch 9, Batch 1016, LR 0.000090 Loss 9.277937, Accuracy 66.988%\n",
      "Epoch 9, Batch 1017, LR 0.000090 Loss 9.278260, Accuracy 66.990%\n",
      "Epoch 9, Batch 1018, LR 0.000090 Loss 9.277957, Accuracy 66.991%\n",
      "Epoch 9, Batch 1019, LR 0.000090 Loss 9.277831, Accuracy 66.987%\n",
      "Epoch 9, Batch 1020, LR 0.000090 Loss 9.277401, Accuracy 66.988%\n",
      "Epoch 9, Batch 1021, LR 0.000090 Loss 9.276731, Accuracy 66.995%\n",
      "Epoch 9, Batch 1022, LR 0.000090 Loss 9.277110, Accuracy 66.992%\n",
      "Epoch 9, Batch 1023, LR 0.000090 Loss 9.276857, Accuracy 66.991%\n",
      "Epoch 9, Batch 1024, LR 0.000090 Loss 9.276881, Accuracy 66.996%\n",
      "Epoch 9, Batch 1025, LR 0.000090 Loss 9.276458, Accuracy 67.003%\n",
      "Epoch 9, Batch 1026, LR 0.000090 Loss 9.276679, Accuracy 67.000%\n",
      "Epoch 9, Batch 1027, LR 0.000090 Loss 9.276897, Accuracy 66.998%\n",
      "Epoch 9, Batch 1028, LR 0.000090 Loss 9.276272, Accuracy 67.001%\n",
      "Epoch 9, Batch 1029, LR 0.000090 Loss 9.275166, Accuracy 67.009%\n",
      "Epoch 9, Batch 1030, LR 0.000089 Loss 9.276045, Accuracy 67.004%\n",
      "Epoch 9, Batch 1031, LR 0.000089 Loss 9.276046, Accuracy 67.003%\n",
      "Epoch 9, Batch 1032, LR 0.000089 Loss 9.275708, Accuracy 67.001%\n",
      "Epoch 9, Batch 1033, LR 0.000089 Loss 9.275227, Accuracy 67.008%\n",
      "Epoch 9, Batch 1034, LR 0.000089 Loss 9.275038, Accuracy 67.005%\n",
      "Epoch 9, Batch 1035, LR 0.000089 Loss 9.275250, Accuracy 67.003%\n",
      "Epoch 9, Batch 1036, LR 0.000089 Loss 9.274835, Accuracy 67.004%\n",
      "Epoch 9, Batch 1037, LR 0.000089 Loss 9.274112, Accuracy 67.009%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Batch 1038, LR 0.000089 Loss 9.274192, Accuracy 67.008%\n",
      "Epoch 9, Batch 1039, LR 0.000089 Loss 9.273603, Accuracy 67.016%\n",
      "Epoch 9, Batch 1040, LR 0.000089 Loss 9.273343, Accuracy 67.016%\n",
      "Epoch 9, Batch 1041, LR 0.000089 Loss 9.273267, Accuracy 67.015%\n",
      "Epoch 9, Batch 1042, LR 0.000089 Loss 9.273024, Accuracy 67.017%\n",
      "Epoch 9, Batch 1043, LR 0.000089 Loss 9.272614, Accuracy 67.017%\n",
      "Epoch 9, Batch 1044, LR 0.000089 Loss 9.272503, Accuracy 67.012%\n",
      "Epoch 9, Batch 1045, LR 0.000089 Loss 9.271832, Accuracy 67.022%\n",
      "Epoch 9, Batch 1046, LR 0.000089 Loss 9.272030, Accuracy 67.024%\n",
      "Epoch 9, Batch 1047, LR 0.000089 Loss 9.272506, Accuracy 67.019%\n",
      "Epoch 9, Loss (train set) 9.272506, Accuracy (train set) 67.019%\n",
      "Epoch 9, Accuracy (validation set) 45.028%\n",
      "Epoch 9, EER (test set) 6.010%\n",
      "Epoch 10, Batch 1, LR 0.000089 Loss 8.212362, Accuracy 71.875%\n",
      "Epoch 10, Batch 2, LR 0.000089 Loss 8.568502, Accuracy 70.703%\n",
      "Epoch 10, Batch 3, LR 0.000089 Loss 8.556846, Accuracy 70.833%\n",
      "Epoch 10, Batch 4, LR 0.000089 Loss 8.693015, Accuracy 69.531%\n",
      "Epoch 10, Batch 5, LR 0.000089 Loss 8.655783, Accuracy 70.000%\n",
      "Epoch 10, Batch 6, LR 0.000089 Loss 8.646235, Accuracy 70.182%\n",
      "Epoch 10, Batch 7, LR 0.000089 Loss 8.679399, Accuracy 69.308%\n",
      "Epoch 10, Batch 8, LR 0.000089 Loss 8.760148, Accuracy 68.652%\n",
      "Epoch 10, Batch 9, LR 0.000089 Loss 8.791925, Accuracy 68.056%\n",
      "Epoch 10, Batch 10, LR 0.000089 Loss 8.808406, Accuracy 68.359%\n",
      "Epoch 10, Batch 11, LR 0.000089 Loss 8.816728, Accuracy 68.537%\n",
      "Epoch 10, Batch 12, LR 0.000089 Loss 8.840911, Accuracy 68.490%\n",
      "Epoch 10, Batch 13, LR 0.000089 Loss 8.832548, Accuracy 68.389%\n",
      "Epoch 10, Batch 14, LR 0.000089 Loss 8.817670, Accuracy 68.471%\n",
      "Epoch 10, Batch 15, LR 0.000089 Loss 8.780927, Accuracy 69.010%\n",
      "Epoch 10, Batch 16, LR 0.000089 Loss 8.792371, Accuracy 68.945%\n",
      "Epoch 10, Batch 17, LR 0.000089 Loss 8.753235, Accuracy 69.210%\n",
      "Epoch 10, Batch 18, LR 0.000089 Loss 8.790688, Accuracy 68.837%\n",
      "Epoch 10, Batch 19, LR 0.000089 Loss 8.768929, Accuracy 69.202%\n",
      "Epoch 10, Batch 20, LR 0.000089 Loss 8.754260, Accuracy 69.609%\n",
      "Epoch 10, Batch 21, LR 0.000089 Loss 8.771090, Accuracy 69.643%\n",
      "Epoch 10, Batch 22, LR 0.000089 Loss 8.778823, Accuracy 69.709%\n",
      "Epoch 10, Batch 23, LR 0.000089 Loss 8.791358, Accuracy 69.565%\n",
      "Epoch 10, Batch 24, LR 0.000089 Loss 8.800517, Accuracy 69.401%\n",
      "Epoch 10, Batch 25, LR 0.000089 Loss 8.808722, Accuracy 69.500%\n",
      "Epoch 10, Batch 26, LR 0.000089 Loss 8.809089, Accuracy 69.471%\n",
      "Epoch 10, Batch 27, LR 0.000089 Loss 8.818280, Accuracy 69.358%\n",
      "Epoch 10, Batch 28, LR 0.000089 Loss 8.815001, Accuracy 69.336%\n",
      "Epoch 10, Batch 29, LR 0.000089 Loss 8.808384, Accuracy 69.343%\n",
      "Epoch 10, Batch 30, LR 0.000089 Loss 8.803379, Accuracy 69.479%\n",
      "Epoch 10, Batch 31, LR 0.000089 Loss 8.790105, Accuracy 69.456%\n",
      "Epoch 10, Batch 32, LR 0.000089 Loss 8.796985, Accuracy 69.507%\n",
      "Epoch 10, Batch 33, LR 0.000089 Loss 8.800891, Accuracy 69.460%\n",
      "Epoch 10, Batch 34, LR 0.000089 Loss 8.787235, Accuracy 69.577%\n",
      "Epoch 10, Batch 35, LR 0.000089 Loss 8.795286, Accuracy 69.643%\n",
      "Epoch 10, Batch 36, LR 0.000089 Loss 8.811399, Accuracy 69.596%\n",
      "Epoch 10, Batch 37, LR 0.000089 Loss 8.819622, Accuracy 69.552%\n",
      "Epoch 10, Batch 38, LR 0.000089 Loss 8.829861, Accuracy 69.449%\n",
      "Epoch 10, Batch 39, LR 0.000089 Loss 8.833641, Accuracy 69.511%\n",
      "Epoch 10, Batch 40, LR 0.000089 Loss 8.838554, Accuracy 69.590%\n",
      "Epoch 10, Batch 41, LR 0.000089 Loss 8.849844, Accuracy 69.531%\n",
      "Epoch 10, Batch 42, LR 0.000089 Loss 8.840733, Accuracy 69.494%\n",
      "Epoch 10, Batch 43, LR 0.000089 Loss 8.848905, Accuracy 69.495%\n",
      "Epoch 10, Batch 44, LR 0.000089 Loss 8.847984, Accuracy 69.513%\n",
      "Epoch 10, Batch 45, LR 0.000089 Loss 8.847683, Accuracy 69.375%\n",
      "Epoch 10, Batch 46, LR 0.000089 Loss 8.860328, Accuracy 69.276%\n",
      "Epoch 10, Batch 47, LR 0.000089 Loss 8.850262, Accuracy 69.382%\n",
      "Epoch 10, Batch 48, LR 0.000089 Loss 8.839098, Accuracy 69.466%\n",
      "Epoch 10, Batch 49, LR 0.000089 Loss 8.817541, Accuracy 69.547%\n",
      "Epoch 10, Batch 50, LR 0.000089 Loss 8.809687, Accuracy 69.688%\n",
      "Epoch 10, Batch 51, LR 0.000089 Loss 8.821773, Accuracy 69.623%\n",
      "Epoch 10, Batch 52, LR 0.000089 Loss 8.833349, Accuracy 69.531%\n",
      "Epoch 10, Batch 53, LR 0.000089 Loss 8.821061, Accuracy 69.487%\n",
      "Epoch 10, Batch 54, LR 0.000089 Loss 8.827772, Accuracy 69.387%\n",
      "Epoch 10, Batch 55, LR 0.000089 Loss 8.838299, Accuracy 69.418%\n",
      "Epoch 10, Batch 56, LR 0.000089 Loss 8.838523, Accuracy 69.434%\n",
      "Epoch 10, Batch 57, LR 0.000089 Loss 8.862188, Accuracy 69.230%\n",
      "Epoch 10, Batch 58, LR 0.000089 Loss 8.862848, Accuracy 69.195%\n",
      "Epoch 10, Batch 59, LR 0.000089 Loss 8.869437, Accuracy 69.213%\n",
      "Epoch 10, Batch 60, LR 0.000089 Loss 8.868281, Accuracy 69.336%\n",
      "Epoch 10, Batch 61, LR 0.000089 Loss 8.866134, Accuracy 69.378%\n",
      "Epoch 10, Batch 62, LR 0.000089 Loss 8.874363, Accuracy 69.355%\n",
      "Epoch 10, Batch 63, LR 0.000089 Loss 8.885693, Accuracy 69.345%\n",
      "Epoch 10, Batch 64, LR 0.000089 Loss 8.886300, Accuracy 69.336%\n",
      "Epoch 10, Batch 65, LR 0.000089 Loss 8.894680, Accuracy 69.231%\n",
      "Epoch 10, Batch 66, LR 0.000089 Loss 8.906228, Accuracy 69.188%\n",
      "Epoch 10, Batch 67, LR 0.000089 Loss 8.880355, Accuracy 69.356%\n",
      "Epoch 10, Batch 68, LR 0.000089 Loss 8.891178, Accuracy 69.313%\n",
      "Epoch 10, Batch 69, LR 0.000089 Loss 8.895245, Accuracy 69.282%\n",
      "Epoch 10, Batch 70, LR 0.000089 Loss 8.905858, Accuracy 69.208%\n",
      "Epoch 10, Batch 71, LR 0.000089 Loss 8.904616, Accuracy 69.212%\n",
      "Epoch 10, Batch 72, LR 0.000089 Loss 8.913317, Accuracy 69.119%\n",
      "Epoch 10, Batch 73, LR 0.000089 Loss 8.910219, Accuracy 69.114%\n",
      "Epoch 10, Batch 74, LR 0.000089 Loss 8.916421, Accuracy 69.046%\n",
      "Epoch 10, Batch 75, LR 0.000089 Loss 8.922951, Accuracy 69.083%\n",
      "Epoch 10, Batch 76, LR 0.000089 Loss 8.916515, Accuracy 69.110%\n",
      "Epoch 10, Batch 77, LR 0.000089 Loss 8.922282, Accuracy 69.065%\n",
      "Epoch 10, Batch 78, LR 0.000089 Loss 8.917064, Accuracy 69.101%\n",
      "Epoch 10, Batch 79, LR 0.000089 Loss 8.910096, Accuracy 69.175%\n",
      "Epoch 10, Batch 80, LR 0.000089 Loss 8.913598, Accuracy 69.150%\n",
      "Epoch 10, Batch 81, LR 0.000089 Loss 8.905008, Accuracy 69.203%\n",
      "Epoch 10, Batch 82, LR 0.000089 Loss 8.907567, Accuracy 69.160%\n",
      "Epoch 10, Batch 83, LR 0.000089 Loss 8.913834, Accuracy 69.098%\n",
      "Epoch 10, Batch 84, LR 0.000089 Loss 8.906251, Accuracy 69.122%\n",
      "Epoch 10, Batch 85, LR 0.000089 Loss 8.908903, Accuracy 69.090%\n",
      "Epoch 10, Batch 86, LR 0.000089 Loss 8.912399, Accuracy 69.050%\n",
      "Epoch 10, Batch 87, LR 0.000089 Loss 8.916489, Accuracy 68.983%\n",
      "Epoch 10, Batch 88, LR 0.000089 Loss 8.917229, Accuracy 68.945%\n",
      "Epoch 10, Batch 89, LR 0.000089 Loss 8.923496, Accuracy 68.926%\n",
      "Epoch 10, Batch 90, LR 0.000089 Loss 8.933523, Accuracy 68.906%\n",
      "Epoch 10, Batch 91, LR 0.000089 Loss 8.942671, Accuracy 68.836%\n",
      "Epoch 10, Batch 92, LR 0.000089 Loss 8.939271, Accuracy 68.894%\n",
      "Epoch 10, Batch 93, LR 0.000089 Loss 8.938624, Accuracy 68.918%\n",
      "Epoch 10, Batch 94, LR 0.000089 Loss 8.937509, Accuracy 68.925%\n",
      "Epoch 10, Batch 95, LR 0.000089 Loss 8.936144, Accuracy 68.898%\n",
      "Epoch 10, Batch 96, LR 0.000089 Loss 8.930484, Accuracy 68.913%\n",
      "Epoch 10, Batch 97, LR 0.000089 Loss 8.927182, Accuracy 68.967%\n",
      "Epoch 10, Batch 98, LR 0.000089 Loss 8.914594, Accuracy 69.053%\n",
      "Epoch 10, Batch 99, LR 0.000089 Loss 8.914440, Accuracy 69.018%\n",
      "Epoch 10, Batch 100, LR 0.000089 Loss 8.918171, Accuracy 68.953%\n",
      "Epoch 10, Batch 101, LR 0.000089 Loss 8.913779, Accuracy 68.974%\n",
      "Epoch 10, Batch 102, LR 0.000089 Loss 8.918138, Accuracy 68.941%\n",
      "Epoch 10, Batch 103, LR 0.000089 Loss 8.918613, Accuracy 68.962%\n",
      "Epoch 10, Batch 104, LR 0.000089 Loss 8.913616, Accuracy 68.998%\n",
      "Epoch 10, Batch 105, LR 0.000089 Loss 8.917489, Accuracy 68.988%\n",
      "Epoch 10, Batch 106, LR 0.000089 Loss 8.914226, Accuracy 69.015%\n",
      "Epoch 10, Batch 107, LR 0.000089 Loss 8.913092, Accuracy 69.042%\n",
      "Epoch 10, Batch 108, LR 0.000089 Loss 8.905895, Accuracy 69.083%\n",
      "Epoch 10, Batch 109, LR 0.000089 Loss 8.910854, Accuracy 69.080%\n",
      "Epoch 10, Batch 110, LR 0.000089 Loss 8.909523, Accuracy 69.105%\n",
      "Epoch 10, Batch 111, LR 0.000089 Loss 8.910273, Accuracy 69.095%\n",
      "Epoch 10, Batch 112, LR 0.000089 Loss 8.912237, Accuracy 69.078%\n",
      "Epoch 10, Batch 113, LR 0.000089 Loss 8.906857, Accuracy 69.082%\n",
      "Epoch 10, Batch 114, LR 0.000089 Loss 8.903646, Accuracy 69.120%\n",
      "Epoch 10, Batch 115, LR 0.000089 Loss 8.904903, Accuracy 69.090%\n",
      "Epoch 10, Batch 116, LR 0.000089 Loss 8.914576, Accuracy 69.006%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 117, LR 0.000089 Loss 8.920202, Accuracy 68.984%\n",
      "Epoch 10, Batch 118, LR 0.000089 Loss 8.918994, Accuracy 68.988%\n",
      "Epoch 10, Batch 119, LR 0.000089 Loss 8.912778, Accuracy 69.006%\n",
      "Epoch 10, Batch 120, LR 0.000089 Loss 8.910857, Accuracy 69.036%\n",
      "Epoch 10, Batch 121, LR 0.000089 Loss 8.914980, Accuracy 69.002%\n",
      "Epoch 10, Batch 122, LR 0.000089 Loss 8.912552, Accuracy 69.045%\n",
      "Epoch 10, Batch 123, LR 0.000089 Loss 8.915665, Accuracy 69.023%\n",
      "Epoch 10, Batch 124, LR 0.000089 Loss 8.909271, Accuracy 69.040%\n",
      "Epoch 10, Batch 125, LR 0.000089 Loss 8.908731, Accuracy 69.031%\n",
      "Epoch 10, Batch 126, LR 0.000089 Loss 8.906471, Accuracy 69.066%\n",
      "Epoch 10, Batch 127, LR 0.000089 Loss 8.904442, Accuracy 69.064%\n",
      "Epoch 10, Batch 128, LR 0.000089 Loss 8.905232, Accuracy 69.073%\n",
      "Epoch 10, Batch 129, LR 0.000089 Loss 8.906962, Accuracy 69.071%\n",
      "Epoch 10, Batch 130, LR 0.000089 Loss 8.906341, Accuracy 69.075%\n",
      "Epoch 10, Batch 131, LR 0.000089 Loss 8.906835, Accuracy 69.042%\n",
      "Epoch 10, Batch 132, LR 0.000089 Loss 8.901830, Accuracy 69.046%\n",
      "Epoch 10, Batch 133, LR 0.000089 Loss 8.908169, Accuracy 68.973%\n",
      "Epoch 10, Batch 134, LR 0.000089 Loss 8.909237, Accuracy 68.977%\n",
      "Epoch 10, Batch 135, LR 0.000089 Loss 8.906093, Accuracy 69.039%\n",
      "Epoch 10, Batch 136, LR 0.000089 Loss 8.906327, Accuracy 69.060%\n",
      "Epoch 10, Batch 137, LR 0.000089 Loss 8.902179, Accuracy 69.052%\n",
      "Epoch 10, Batch 138, LR 0.000089 Loss 8.901547, Accuracy 69.067%\n",
      "Epoch 10, Batch 139, LR 0.000089 Loss 8.900754, Accuracy 69.065%\n",
      "Epoch 10, Batch 140, LR 0.000089 Loss 8.906650, Accuracy 68.984%\n",
      "Epoch 10, Batch 141, LR 0.000089 Loss 8.908892, Accuracy 68.988%\n",
      "Epoch 10, Batch 142, LR 0.000089 Loss 8.909064, Accuracy 68.992%\n",
      "Epoch 10, Batch 143, LR 0.000089 Loss 8.909343, Accuracy 69.012%\n",
      "Epoch 10, Batch 144, LR 0.000089 Loss 8.914425, Accuracy 68.962%\n",
      "Epoch 10, Batch 145, LR 0.000089 Loss 8.919595, Accuracy 68.944%\n",
      "Epoch 10, Batch 146, LR 0.000089 Loss 8.919682, Accuracy 68.964%\n",
      "Epoch 10, Batch 147, LR 0.000089 Loss 8.920481, Accuracy 68.936%\n",
      "Epoch 10, Batch 148, LR 0.000089 Loss 8.923764, Accuracy 68.919%\n",
      "Epoch 10, Batch 149, LR 0.000089 Loss 8.923654, Accuracy 68.886%\n",
      "Epoch 10, Batch 150, LR 0.000089 Loss 8.921255, Accuracy 68.932%\n",
      "Epoch 10, Batch 151, LR 0.000089 Loss 8.920536, Accuracy 68.947%\n",
      "Epoch 10, Batch 152, LR 0.000089 Loss 8.917739, Accuracy 68.976%\n",
      "Epoch 10, Batch 153, LR 0.000089 Loss 8.920608, Accuracy 68.959%\n",
      "Epoch 10, Batch 154, LR 0.000089 Loss 8.922325, Accuracy 68.917%\n",
      "Epoch 10, Batch 155, LR 0.000089 Loss 8.920377, Accuracy 68.947%\n",
      "Epoch 10, Batch 156, LR 0.000089 Loss 8.921218, Accuracy 68.975%\n",
      "Epoch 10, Batch 157, LR 0.000089 Loss 8.920303, Accuracy 69.009%\n",
      "Epoch 10, Batch 158, LR 0.000089 Loss 8.917849, Accuracy 69.022%\n",
      "Epoch 10, Batch 159, LR 0.000089 Loss 8.916334, Accuracy 69.045%\n",
      "Epoch 10, Batch 160, LR 0.000089 Loss 8.914344, Accuracy 69.092%\n",
      "Epoch 10, Batch 161, LR 0.000089 Loss 8.913722, Accuracy 69.109%\n",
      "Epoch 10, Batch 162, LR 0.000089 Loss 8.915004, Accuracy 69.107%\n",
      "Epoch 10, Batch 163, LR 0.000089 Loss 8.911111, Accuracy 69.143%\n",
      "Epoch 10, Batch 164, LR 0.000089 Loss 8.906836, Accuracy 69.174%\n",
      "Epoch 10, Batch 165, LR 0.000089 Loss 8.906363, Accuracy 69.195%\n",
      "Epoch 10, Batch 166, LR 0.000089 Loss 8.903214, Accuracy 69.202%\n",
      "Epoch 10, Batch 167, LR 0.000089 Loss 8.901785, Accuracy 69.180%\n",
      "Epoch 10, Batch 168, LR 0.000089 Loss 8.904245, Accuracy 69.169%\n",
      "Epoch 10, Batch 169, LR 0.000089 Loss 8.899274, Accuracy 69.194%\n",
      "Epoch 10, Batch 170, LR 0.000089 Loss 8.893555, Accuracy 69.233%\n",
      "Epoch 10, Batch 171, LR 0.000089 Loss 8.896128, Accuracy 69.184%\n",
      "Epoch 10, Batch 172, LR 0.000089 Loss 8.897529, Accuracy 69.159%\n",
      "Epoch 10, Batch 173, LR 0.000089 Loss 8.899360, Accuracy 69.125%\n",
      "Epoch 10, Batch 174, LR 0.000089 Loss 8.898213, Accuracy 69.109%\n",
      "Epoch 10, Batch 175, LR 0.000089 Loss 8.899257, Accuracy 69.116%\n",
      "Epoch 10, Batch 176, LR 0.000089 Loss 8.896979, Accuracy 69.118%\n",
      "Epoch 10, Batch 177, LR 0.000089 Loss 8.893651, Accuracy 69.125%\n",
      "Epoch 10, Batch 178, LR 0.000089 Loss 8.890457, Accuracy 69.158%\n",
      "Epoch 10, Batch 179, LR 0.000089 Loss 8.893277, Accuracy 69.138%\n",
      "Epoch 10, Batch 180, LR 0.000089 Loss 8.897274, Accuracy 69.102%\n",
      "Epoch 10, Batch 181, LR 0.000089 Loss 8.899443, Accuracy 69.108%\n",
      "Epoch 10, Batch 182, LR 0.000089 Loss 8.896714, Accuracy 69.123%\n",
      "Epoch 10, Batch 183, LR 0.000089 Loss 8.897388, Accuracy 69.100%\n",
      "Epoch 10, Batch 184, LR 0.000089 Loss 8.896174, Accuracy 69.077%\n",
      "Epoch 10, Batch 185, LR 0.000089 Loss 8.900332, Accuracy 69.037%\n",
      "Epoch 10, Batch 186, LR 0.000089 Loss 8.903889, Accuracy 69.002%\n",
      "Epoch 10, Batch 187, LR 0.000089 Loss 8.904117, Accuracy 69.001%\n",
      "Epoch 10, Batch 188, LR 0.000089 Loss 8.903465, Accuracy 69.008%\n",
      "Epoch 10, Batch 189, LR 0.000089 Loss 8.901509, Accuracy 69.031%\n",
      "Epoch 10, Batch 190, LR 0.000089 Loss 8.905322, Accuracy 69.030%\n",
      "Epoch 10, Batch 191, LR 0.000089 Loss 8.905051, Accuracy 69.045%\n",
      "Epoch 10, Batch 192, LR 0.000089 Loss 8.904551, Accuracy 69.035%\n",
      "Epoch 10, Batch 193, LR 0.000089 Loss 8.906187, Accuracy 69.045%\n",
      "Epoch 10, Batch 194, LR 0.000089 Loss 8.908250, Accuracy 69.016%\n",
      "Epoch 10, Batch 195, LR 0.000089 Loss 8.909152, Accuracy 69.042%\n",
      "Epoch 10, Batch 196, LR 0.000089 Loss 8.907980, Accuracy 69.057%\n",
      "Epoch 10, Batch 197, LR 0.000089 Loss 8.909303, Accuracy 69.059%\n",
      "Epoch 10, Batch 198, LR 0.000089 Loss 8.911367, Accuracy 69.081%\n",
      "Epoch 10, Batch 199, LR 0.000089 Loss 8.911978, Accuracy 69.080%\n",
      "Epoch 10, Batch 200, LR 0.000089 Loss 8.912360, Accuracy 69.078%\n",
      "Epoch 10, Batch 201, LR 0.000089 Loss 8.911240, Accuracy 69.092%\n",
      "Epoch 10, Batch 202, LR 0.000089 Loss 8.911281, Accuracy 69.114%\n",
      "Epoch 10, Batch 203, LR 0.000089 Loss 8.916968, Accuracy 69.058%\n",
      "Epoch 10, Batch 204, LR 0.000089 Loss 8.912890, Accuracy 69.076%\n",
      "Epoch 10, Batch 205, LR 0.000089 Loss 8.915986, Accuracy 69.047%\n",
      "Epoch 10, Batch 206, LR 0.000089 Loss 8.917610, Accuracy 69.034%\n",
      "Epoch 10, Batch 207, LR 0.000089 Loss 8.919193, Accuracy 69.026%\n",
      "Epoch 10, Batch 208, LR 0.000089 Loss 8.918679, Accuracy 69.035%\n",
      "Epoch 10, Batch 209, LR 0.000089 Loss 8.917665, Accuracy 69.030%\n",
      "Epoch 10, Batch 210, LR 0.000089 Loss 8.918238, Accuracy 69.036%\n",
      "Epoch 10, Batch 211, LR 0.000089 Loss 8.918594, Accuracy 69.035%\n",
      "Epoch 10, Batch 212, LR 0.000089 Loss 8.917215, Accuracy 69.045%\n",
      "Epoch 10, Batch 213, LR 0.000089 Loss 8.917588, Accuracy 69.036%\n",
      "Epoch 10, Batch 214, LR 0.000089 Loss 8.913793, Accuracy 69.075%\n",
      "Epoch 10, Batch 215, LR 0.000089 Loss 8.913566, Accuracy 69.044%\n",
      "Epoch 10, Batch 216, LR 0.000089 Loss 8.914706, Accuracy 69.054%\n",
      "Epoch 10, Batch 217, LR 0.000089 Loss 8.912912, Accuracy 69.063%\n",
      "Epoch 10, Batch 218, LR 0.000089 Loss 8.914246, Accuracy 69.055%\n",
      "Epoch 10, Batch 219, LR 0.000089 Loss 8.916205, Accuracy 69.018%\n",
      "Epoch 10, Batch 220, LR 0.000089 Loss 8.911997, Accuracy 69.045%\n",
      "Epoch 10, Batch 221, LR 0.000089 Loss 8.912932, Accuracy 69.022%\n",
      "Epoch 10, Batch 222, LR 0.000089 Loss 8.912148, Accuracy 69.024%\n",
      "Epoch 10, Batch 223, LR 0.000089 Loss 8.913604, Accuracy 69.023%\n",
      "Epoch 10, Batch 224, LR 0.000089 Loss 8.913427, Accuracy 69.012%\n",
      "Epoch 10, Batch 225, LR 0.000089 Loss 8.908861, Accuracy 69.042%\n",
      "Epoch 10, Batch 226, LR 0.000089 Loss 8.909659, Accuracy 69.051%\n",
      "Epoch 10, Batch 227, LR 0.000089 Loss 8.911601, Accuracy 69.036%\n",
      "Epoch 10, Batch 228, LR 0.000089 Loss 8.912335, Accuracy 69.024%\n",
      "Epoch 10, Batch 229, LR 0.000089 Loss 8.915209, Accuracy 69.002%\n",
      "Epoch 10, Batch 230, LR 0.000089 Loss 8.915783, Accuracy 69.012%\n",
      "Epoch 10, Batch 231, LR 0.000089 Loss 8.913537, Accuracy 69.014%\n",
      "Epoch 10, Batch 232, LR 0.000089 Loss 8.913308, Accuracy 69.006%\n",
      "Epoch 10, Batch 233, LR 0.000089 Loss 8.913597, Accuracy 69.008%\n",
      "Epoch 10, Batch 234, LR 0.000089 Loss 8.915340, Accuracy 68.987%\n",
      "Epoch 10, Batch 235, LR 0.000089 Loss 8.915839, Accuracy 68.976%\n",
      "Epoch 10, Batch 236, LR 0.000089 Loss 8.917098, Accuracy 68.955%\n",
      "Epoch 10, Batch 237, LR 0.000089 Loss 8.916772, Accuracy 68.958%\n",
      "Epoch 10, Batch 238, LR 0.000089 Loss 8.915991, Accuracy 68.960%\n",
      "Epoch 10, Batch 239, LR 0.000089 Loss 8.916723, Accuracy 68.966%\n",
      "Epoch 10, Batch 240, LR 0.000089 Loss 8.916374, Accuracy 68.971%\n",
      "Epoch 10, Batch 241, LR 0.000089 Loss 8.911241, Accuracy 69.009%\n",
      "Epoch 10, Batch 242, LR 0.000089 Loss 8.914267, Accuracy 68.970%\n",
      "Epoch 10, Batch 243, LR 0.000089 Loss 8.913462, Accuracy 68.956%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 244, LR 0.000089 Loss 8.917470, Accuracy 68.910%\n",
      "Epoch 10, Batch 245, LR 0.000089 Loss 8.917895, Accuracy 68.893%\n",
      "Epoch 10, Batch 246, LR 0.000089 Loss 8.919181, Accuracy 68.890%\n",
      "Epoch 10, Batch 247, LR 0.000089 Loss 8.919620, Accuracy 68.880%\n",
      "Epoch 10, Batch 248, LR 0.000089 Loss 8.922544, Accuracy 68.854%\n",
      "Epoch 10, Batch 249, LR 0.000089 Loss 8.920143, Accuracy 68.847%\n",
      "Epoch 10, Batch 250, LR 0.000089 Loss 8.922111, Accuracy 68.825%\n",
      "Epoch 10, Batch 251, LR 0.000089 Loss 8.921886, Accuracy 68.822%\n",
      "Epoch 10, Batch 252, LR 0.000089 Loss 8.919429, Accuracy 68.849%\n",
      "Epoch 10, Batch 253, LR 0.000089 Loss 8.918063, Accuracy 68.849%\n",
      "Epoch 10, Batch 254, LR 0.000089 Loss 8.918500, Accuracy 68.830%\n",
      "Epoch 10, Batch 255, LR 0.000089 Loss 8.919355, Accuracy 68.830%\n",
      "Epoch 10, Batch 256, LR 0.000089 Loss 8.921224, Accuracy 68.811%\n",
      "Epoch 10, Batch 257, LR 0.000089 Loss 8.920844, Accuracy 68.805%\n",
      "Epoch 10, Batch 258, LR 0.000089 Loss 8.923567, Accuracy 68.801%\n",
      "Epoch 10, Batch 259, LR 0.000089 Loss 8.927193, Accuracy 68.777%\n",
      "Epoch 10, Batch 260, LR 0.000089 Loss 8.926462, Accuracy 68.777%\n",
      "Epoch 10, Batch 261, LR 0.000089 Loss 8.927661, Accuracy 68.783%\n",
      "Epoch 10, Batch 262, LR 0.000089 Loss 8.928602, Accuracy 68.762%\n",
      "Epoch 10, Batch 263, LR 0.000089 Loss 8.927831, Accuracy 68.795%\n",
      "Epoch 10, Batch 264, LR 0.000089 Loss 8.926580, Accuracy 68.815%\n",
      "Epoch 10, Batch 265, LR 0.000089 Loss 8.928551, Accuracy 68.812%\n",
      "Epoch 10, Batch 266, LR 0.000089 Loss 8.926096, Accuracy 68.838%\n",
      "Epoch 10, Batch 267, LR 0.000089 Loss 8.925518, Accuracy 68.849%\n",
      "Epoch 10, Batch 268, LR 0.000089 Loss 8.922419, Accuracy 68.875%\n",
      "Epoch 10, Batch 269, LR 0.000089 Loss 8.921553, Accuracy 68.860%\n",
      "Epoch 10, Batch 270, LR 0.000089 Loss 8.918713, Accuracy 68.886%\n",
      "Epoch 10, Batch 271, LR 0.000089 Loss 8.919583, Accuracy 68.874%\n",
      "Epoch 10, Batch 272, LR 0.000089 Loss 8.918908, Accuracy 68.894%\n",
      "Epoch 10, Batch 273, LR 0.000089 Loss 8.919639, Accuracy 68.899%\n",
      "Epoch 10, Batch 274, LR 0.000089 Loss 8.921945, Accuracy 68.893%\n",
      "Epoch 10, Batch 275, LR 0.000089 Loss 8.923330, Accuracy 68.886%\n",
      "Epoch 10, Batch 276, LR 0.000089 Loss 8.924016, Accuracy 68.892%\n",
      "Epoch 10, Batch 277, LR 0.000089 Loss 8.926074, Accuracy 68.863%\n",
      "Epoch 10, Batch 278, LR 0.000089 Loss 8.928931, Accuracy 68.834%\n",
      "Epoch 10, Batch 279, LR 0.000089 Loss 8.930890, Accuracy 68.834%\n",
      "Epoch 10, Batch 280, LR 0.000089 Loss 8.931070, Accuracy 68.831%\n",
      "Epoch 10, Batch 281, LR 0.000089 Loss 8.930533, Accuracy 68.828%\n",
      "Epoch 10, Batch 282, LR 0.000089 Loss 8.929782, Accuracy 68.830%\n",
      "Epoch 10, Batch 283, LR 0.000089 Loss 8.928973, Accuracy 68.844%\n",
      "Epoch 10, Batch 284, LR 0.000089 Loss 8.928508, Accuracy 68.855%\n",
      "Epoch 10, Batch 285, LR 0.000089 Loss 8.929065, Accuracy 68.846%\n",
      "Epoch 10, Batch 286, LR 0.000089 Loss 8.927158, Accuracy 68.854%\n",
      "Epoch 10, Batch 287, LR 0.000089 Loss 8.929293, Accuracy 68.840%\n",
      "Epoch 10, Batch 288, LR 0.000089 Loss 8.932332, Accuracy 68.818%\n",
      "Epoch 10, Batch 289, LR 0.000089 Loss 8.933048, Accuracy 68.815%\n",
      "Epoch 10, Batch 290, LR 0.000089 Loss 8.932142, Accuracy 68.801%\n",
      "Epoch 10, Batch 291, LR 0.000089 Loss 8.932207, Accuracy 68.801%\n",
      "Epoch 10, Batch 292, LR 0.000089 Loss 8.934010, Accuracy 68.804%\n",
      "Epoch 10, Batch 293, LR 0.000089 Loss 8.935207, Accuracy 68.795%\n",
      "Epoch 10, Batch 294, LR 0.000089 Loss 8.932950, Accuracy 68.808%\n",
      "Epoch 10, Batch 295, LR 0.000089 Loss 8.934387, Accuracy 68.816%\n",
      "Epoch 10, Batch 296, LR 0.000089 Loss 8.935147, Accuracy 68.792%\n",
      "Epoch 10, Batch 297, LR 0.000089 Loss 8.932680, Accuracy 68.816%\n",
      "Epoch 10, Batch 298, LR 0.000089 Loss 8.933299, Accuracy 68.810%\n",
      "Epoch 10, Batch 299, LR 0.000089 Loss 8.932523, Accuracy 68.821%\n",
      "Epoch 10, Batch 300, LR 0.000089 Loss 8.934760, Accuracy 68.807%\n",
      "Epoch 10, Batch 301, LR 0.000089 Loss 8.935910, Accuracy 68.815%\n",
      "Epoch 10, Batch 302, LR 0.000089 Loss 8.935047, Accuracy 68.812%\n",
      "Epoch 10, Batch 303, LR 0.000089 Loss 8.932159, Accuracy 68.838%\n",
      "Epoch 10, Batch 304, LR 0.000089 Loss 8.931636, Accuracy 68.843%\n",
      "Epoch 10, Batch 305, LR 0.000089 Loss 8.931847, Accuracy 68.835%\n",
      "Epoch 10, Batch 306, LR 0.000089 Loss 8.930548, Accuracy 68.852%\n",
      "Epoch 10, Batch 307, LR 0.000089 Loss 8.928726, Accuracy 68.849%\n",
      "Epoch 10, Batch 308, LR 0.000089 Loss 8.926777, Accuracy 68.867%\n",
      "Epoch 10, Batch 309, LR 0.000089 Loss 8.925210, Accuracy 68.861%\n",
      "Epoch 10, Batch 310, LR 0.000089 Loss 8.924972, Accuracy 68.863%\n",
      "Epoch 10, Batch 311, LR 0.000089 Loss 8.922357, Accuracy 68.886%\n",
      "Epoch 10, Batch 312, LR 0.000089 Loss 8.923144, Accuracy 68.888%\n",
      "Epoch 10, Batch 313, LR 0.000089 Loss 8.923639, Accuracy 68.885%\n",
      "Epoch 10, Batch 314, LR 0.000089 Loss 8.924463, Accuracy 68.879%\n",
      "Epoch 10, Batch 315, LR 0.000089 Loss 8.925258, Accuracy 68.879%\n",
      "Epoch 10, Batch 316, LR 0.000089 Loss 8.926088, Accuracy 68.884%\n",
      "Epoch 10, Batch 317, LR 0.000089 Loss 8.924824, Accuracy 68.881%\n",
      "Epoch 10, Batch 318, LR 0.000089 Loss 8.925231, Accuracy 68.878%\n",
      "Epoch 10, Batch 319, LR 0.000089 Loss 8.925904, Accuracy 68.872%\n",
      "Epoch 10, Batch 320, LR 0.000089 Loss 8.927322, Accuracy 68.853%\n",
      "Epoch 10, Batch 321, LR 0.000089 Loss 8.924093, Accuracy 68.869%\n",
      "Epoch 10, Batch 322, LR 0.000089 Loss 8.924811, Accuracy 68.857%\n",
      "Epoch 10, Batch 323, LR 0.000089 Loss 8.923974, Accuracy 68.871%\n",
      "Epoch 10, Batch 324, LR 0.000089 Loss 8.926266, Accuracy 68.861%\n",
      "Epoch 10, Batch 325, LR 0.000089 Loss 8.927178, Accuracy 68.861%\n",
      "Epoch 10, Batch 326, LR 0.000089 Loss 8.927163, Accuracy 68.853%\n",
      "Epoch 10, Batch 327, LR 0.000089 Loss 8.924968, Accuracy 68.846%\n",
      "Epoch 10, Batch 328, LR 0.000089 Loss 8.924683, Accuracy 68.850%\n",
      "Epoch 10, Batch 329, LR 0.000089 Loss 8.925908, Accuracy 68.838%\n",
      "Epoch 10, Batch 330, LR 0.000089 Loss 8.926239, Accuracy 68.828%\n",
      "Epoch 10, Batch 331, LR 0.000089 Loss 8.926533, Accuracy 68.821%\n",
      "Epoch 10, Batch 332, LR 0.000089 Loss 8.927746, Accuracy 68.814%\n",
      "Epoch 10, Batch 333, LR 0.000089 Loss 8.925913, Accuracy 68.841%\n",
      "Epoch 10, Batch 334, LR 0.000089 Loss 8.926411, Accuracy 68.839%\n",
      "Epoch 10, Batch 335, LR 0.000089 Loss 8.927420, Accuracy 68.834%\n",
      "Epoch 10, Batch 336, LR 0.000089 Loss 8.925107, Accuracy 68.859%\n",
      "Epoch 10, Batch 337, LR 0.000089 Loss 8.925591, Accuracy 68.873%\n",
      "Epoch 10, Batch 338, LR 0.000089 Loss 8.925303, Accuracy 68.896%\n",
      "Epoch 10, Batch 339, LR 0.000089 Loss 8.925193, Accuracy 68.897%\n",
      "Epoch 10, Batch 340, LR 0.000089 Loss 8.924750, Accuracy 68.906%\n",
      "Epoch 10, Batch 341, LR 0.000089 Loss 8.924759, Accuracy 68.910%\n",
      "Epoch 10, Batch 342, LR 0.000089 Loss 8.924182, Accuracy 68.921%\n",
      "Epoch 10, Batch 343, LR 0.000089 Loss 8.925336, Accuracy 68.919%\n",
      "Epoch 10, Batch 344, LR 0.000089 Loss 8.926727, Accuracy 68.902%\n",
      "Epoch 10, Batch 345, LR 0.000089 Loss 8.924940, Accuracy 68.920%\n",
      "Epoch 10, Batch 346, LR 0.000089 Loss 8.924362, Accuracy 68.922%\n",
      "Epoch 10, Batch 347, LR 0.000089 Loss 8.923594, Accuracy 68.928%\n",
      "Epoch 10, Batch 348, LR 0.000089 Loss 8.921033, Accuracy 68.945%\n",
      "Epoch 10, Batch 349, LR 0.000089 Loss 8.921230, Accuracy 68.929%\n",
      "Epoch 10, Batch 350, LR 0.000089 Loss 8.921482, Accuracy 68.917%\n",
      "Epoch 10, Batch 351, LR 0.000089 Loss 8.922287, Accuracy 68.904%\n",
      "Epoch 10, Batch 352, LR 0.000089 Loss 8.922696, Accuracy 68.892%\n",
      "Epoch 10, Batch 353, LR 0.000089 Loss 8.921325, Accuracy 68.900%\n",
      "Epoch 10, Batch 354, LR 0.000089 Loss 8.921481, Accuracy 68.893%\n",
      "Epoch 10, Batch 355, LR 0.000089 Loss 8.919182, Accuracy 68.913%\n",
      "Epoch 10, Batch 356, LR 0.000089 Loss 8.919368, Accuracy 68.917%\n",
      "Epoch 10, Batch 357, LR 0.000089 Loss 8.919248, Accuracy 68.910%\n",
      "Epoch 10, Batch 358, LR 0.000089 Loss 8.918097, Accuracy 68.916%\n",
      "Epoch 10, Batch 359, LR 0.000089 Loss 8.918029, Accuracy 68.924%\n",
      "Epoch 10, Batch 360, LR 0.000089 Loss 8.918522, Accuracy 68.919%\n",
      "Epoch 10, Batch 361, LR 0.000089 Loss 8.919507, Accuracy 68.906%\n",
      "Epoch 10, Batch 362, LR 0.000089 Loss 8.919472, Accuracy 68.908%\n",
      "Epoch 10, Batch 363, LR 0.000089 Loss 8.918030, Accuracy 68.911%\n",
      "Epoch 10, Batch 364, LR 0.000089 Loss 8.918393, Accuracy 68.894%\n",
      "Epoch 10, Batch 365, LR 0.000089 Loss 8.920018, Accuracy 68.874%\n",
      "Epoch 10, Batch 366, LR 0.000089 Loss 8.918785, Accuracy 68.870%\n",
      "Epoch 10, Batch 367, LR 0.000089 Loss 8.919712, Accuracy 68.869%\n",
      "Epoch 10, Batch 368, LR 0.000089 Loss 8.918315, Accuracy 68.875%\n",
      "Epoch 10, Batch 369, LR 0.000089 Loss 8.916777, Accuracy 68.890%\n",
      "Epoch 10, Batch 370, LR 0.000089 Loss 8.915811, Accuracy 68.889%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 371, LR 0.000089 Loss 8.914795, Accuracy 68.897%\n",
      "Epoch 10, Batch 372, LR 0.000089 Loss 8.915213, Accuracy 68.914%\n",
      "Epoch 10, Batch 373, LR 0.000089 Loss 8.914518, Accuracy 68.920%\n",
      "Epoch 10, Batch 374, LR 0.000089 Loss 8.916139, Accuracy 68.915%\n",
      "Epoch 10, Batch 375, LR 0.000089 Loss 8.916046, Accuracy 68.910%\n",
      "Epoch 10, Batch 376, LR 0.000089 Loss 8.915441, Accuracy 68.906%\n",
      "Epoch 10, Batch 377, LR 0.000089 Loss 8.914371, Accuracy 68.914%\n",
      "Epoch 10, Batch 378, LR 0.000089 Loss 8.914030, Accuracy 68.903%\n",
      "Epoch 10, Batch 379, LR 0.000089 Loss 8.913267, Accuracy 68.917%\n",
      "Epoch 10, Batch 380, LR 0.000089 Loss 8.915020, Accuracy 68.894%\n",
      "Epoch 10, Batch 381, LR 0.000089 Loss 8.913262, Accuracy 68.896%\n",
      "Epoch 10, Batch 382, LR 0.000089 Loss 8.911961, Accuracy 68.912%\n",
      "Epoch 10, Batch 383, LR 0.000089 Loss 8.911972, Accuracy 68.901%\n",
      "Epoch 10, Batch 384, LR 0.000089 Loss 8.913297, Accuracy 68.888%\n",
      "Epoch 10, Batch 385, LR 0.000089 Loss 8.914672, Accuracy 68.880%\n",
      "Epoch 10, Batch 386, LR 0.000089 Loss 8.913430, Accuracy 68.900%\n",
      "Epoch 10, Batch 387, LR 0.000089 Loss 8.914166, Accuracy 68.891%\n",
      "Epoch 10, Batch 388, LR 0.000088 Loss 8.913385, Accuracy 68.903%\n",
      "Epoch 10, Batch 389, LR 0.000088 Loss 8.914343, Accuracy 68.897%\n",
      "Epoch 10, Batch 390, LR 0.000088 Loss 8.911700, Accuracy 68.910%\n",
      "Epoch 10, Batch 391, LR 0.000088 Loss 8.912092, Accuracy 68.906%\n",
      "Epoch 10, Batch 392, LR 0.000088 Loss 8.913244, Accuracy 68.913%\n",
      "Epoch 10, Batch 393, LR 0.000088 Loss 8.910247, Accuracy 68.927%\n",
      "Epoch 10, Batch 394, LR 0.000088 Loss 8.908381, Accuracy 68.944%\n",
      "Epoch 10, Batch 395, LR 0.000088 Loss 8.907794, Accuracy 68.950%\n",
      "Epoch 10, Batch 396, LR 0.000088 Loss 8.906184, Accuracy 68.961%\n",
      "Epoch 10, Batch 397, LR 0.000088 Loss 8.907448, Accuracy 68.959%\n",
      "Epoch 10, Batch 398, LR 0.000088 Loss 8.904576, Accuracy 68.986%\n",
      "Epoch 10, Batch 399, LR 0.000088 Loss 8.905705, Accuracy 68.987%\n",
      "Epoch 10, Batch 400, LR 0.000088 Loss 8.905768, Accuracy 68.990%\n",
      "Epoch 10, Batch 401, LR 0.000088 Loss 8.904820, Accuracy 69.005%\n",
      "Epoch 10, Batch 402, LR 0.000088 Loss 8.906409, Accuracy 68.987%\n",
      "Epoch 10, Batch 403, LR 0.000088 Loss 8.907053, Accuracy 68.977%\n",
      "Epoch 10, Batch 404, LR 0.000088 Loss 8.905498, Accuracy 68.984%\n",
      "Epoch 10, Batch 405, LR 0.000088 Loss 8.903013, Accuracy 68.989%\n",
      "Epoch 10, Batch 406, LR 0.000088 Loss 8.902703, Accuracy 68.994%\n",
      "Epoch 10, Batch 407, LR 0.000088 Loss 8.902972, Accuracy 68.994%\n",
      "Epoch 10, Batch 408, LR 0.000088 Loss 8.904074, Accuracy 68.989%\n",
      "Epoch 10, Batch 409, LR 0.000088 Loss 8.903949, Accuracy 68.993%\n",
      "Epoch 10, Batch 410, LR 0.000088 Loss 8.903357, Accuracy 69.011%\n",
      "Epoch 10, Batch 411, LR 0.000088 Loss 8.903159, Accuracy 68.999%\n",
      "Epoch 10, Batch 412, LR 0.000088 Loss 8.904218, Accuracy 69.004%\n",
      "Epoch 10, Batch 413, LR 0.000088 Loss 8.903287, Accuracy 69.005%\n",
      "Epoch 10, Batch 414, LR 0.000088 Loss 8.904398, Accuracy 68.984%\n",
      "Epoch 10, Batch 415, LR 0.000088 Loss 8.902286, Accuracy 68.989%\n",
      "Epoch 10, Batch 416, LR 0.000088 Loss 8.902082, Accuracy 68.989%\n",
      "Epoch 10, Batch 417, LR 0.000088 Loss 8.900425, Accuracy 69.007%\n",
      "Epoch 10, Batch 418, LR 0.000088 Loss 8.898831, Accuracy 69.019%\n",
      "Epoch 10, Batch 419, LR 0.000088 Loss 8.899140, Accuracy 69.022%\n",
      "Epoch 10, Batch 420, LR 0.000088 Loss 8.896429, Accuracy 69.035%\n",
      "Epoch 10, Batch 421, LR 0.000088 Loss 8.896082, Accuracy 69.052%\n",
      "Epoch 10, Batch 422, LR 0.000088 Loss 8.896109, Accuracy 69.046%\n",
      "Epoch 10, Batch 423, LR 0.000088 Loss 8.893642, Accuracy 69.068%\n",
      "Epoch 10, Batch 424, LR 0.000088 Loss 8.894938, Accuracy 69.058%\n",
      "Epoch 10, Batch 425, LR 0.000088 Loss 8.894742, Accuracy 69.062%\n",
      "Epoch 10, Batch 426, LR 0.000088 Loss 8.896375, Accuracy 69.053%\n",
      "Epoch 10, Batch 427, LR 0.000088 Loss 8.897010, Accuracy 69.050%\n",
      "Epoch 10, Batch 428, LR 0.000088 Loss 8.899829, Accuracy 69.022%\n",
      "Epoch 10, Batch 429, LR 0.000088 Loss 8.900166, Accuracy 69.018%\n",
      "Epoch 10, Batch 430, LR 0.000088 Loss 8.899456, Accuracy 69.021%\n",
      "Epoch 10, Batch 431, LR 0.000088 Loss 8.899444, Accuracy 69.018%\n",
      "Epoch 10, Batch 432, LR 0.000088 Loss 8.899780, Accuracy 69.023%\n",
      "Epoch 10, Batch 433, LR 0.000088 Loss 8.900058, Accuracy 69.030%\n",
      "Epoch 10, Batch 434, LR 0.000088 Loss 8.899442, Accuracy 69.034%\n",
      "Epoch 10, Batch 435, LR 0.000088 Loss 8.899584, Accuracy 69.030%\n",
      "Epoch 10, Batch 436, LR 0.000088 Loss 8.898488, Accuracy 69.035%\n",
      "Epoch 10, Batch 437, LR 0.000088 Loss 8.898843, Accuracy 69.034%\n",
      "Epoch 10, Batch 438, LR 0.000088 Loss 8.898097, Accuracy 69.046%\n",
      "Epoch 10, Batch 439, LR 0.000088 Loss 8.898324, Accuracy 69.047%\n",
      "Epoch 10, Batch 440, LR 0.000088 Loss 8.898062, Accuracy 69.041%\n",
      "Epoch 10, Batch 441, LR 0.000088 Loss 8.898643, Accuracy 69.046%\n",
      "Epoch 10, Batch 442, LR 0.000088 Loss 8.899229, Accuracy 69.038%\n",
      "Epoch 10, Batch 443, LR 0.000088 Loss 8.897700, Accuracy 69.050%\n",
      "Epoch 10, Batch 444, LR 0.000088 Loss 8.897826, Accuracy 69.047%\n",
      "Epoch 10, Batch 445, LR 0.000088 Loss 8.897898, Accuracy 69.047%\n",
      "Epoch 10, Batch 446, LR 0.000088 Loss 8.898538, Accuracy 69.034%\n",
      "Epoch 10, Batch 447, LR 0.000088 Loss 8.896502, Accuracy 69.038%\n",
      "Epoch 10, Batch 448, LR 0.000088 Loss 8.895251, Accuracy 69.039%\n",
      "Epoch 10, Batch 449, LR 0.000088 Loss 8.895330, Accuracy 69.035%\n",
      "Epoch 10, Batch 450, LR 0.000088 Loss 8.892704, Accuracy 69.045%\n",
      "Epoch 10, Batch 451, LR 0.000088 Loss 8.890142, Accuracy 69.053%\n",
      "Epoch 10, Batch 452, LR 0.000088 Loss 8.888795, Accuracy 69.063%\n",
      "Epoch 10, Batch 453, LR 0.000088 Loss 8.889282, Accuracy 69.055%\n",
      "Epoch 10, Batch 454, LR 0.000088 Loss 8.887595, Accuracy 69.061%\n",
      "Epoch 10, Batch 455, LR 0.000088 Loss 8.887348, Accuracy 69.057%\n",
      "Epoch 10, Batch 456, LR 0.000088 Loss 8.888784, Accuracy 69.058%\n",
      "Epoch 10, Batch 457, LR 0.000088 Loss 8.889236, Accuracy 69.046%\n",
      "Epoch 10, Batch 458, LR 0.000088 Loss 8.890114, Accuracy 69.049%\n",
      "Epoch 10, Batch 459, LR 0.000088 Loss 8.887544, Accuracy 69.061%\n",
      "Epoch 10, Batch 460, LR 0.000088 Loss 8.886823, Accuracy 69.071%\n",
      "Epoch 10, Batch 461, LR 0.000088 Loss 8.887495, Accuracy 69.064%\n",
      "Epoch 10, Batch 462, LR 0.000088 Loss 8.884728, Accuracy 69.080%\n",
      "Epoch 10, Batch 463, LR 0.000088 Loss 8.882664, Accuracy 69.081%\n",
      "Epoch 10, Batch 464, LR 0.000088 Loss 8.882907, Accuracy 69.087%\n",
      "Epoch 10, Batch 465, LR 0.000088 Loss 8.883700, Accuracy 69.084%\n",
      "Epoch 10, Batch 466, LR 0.000088 Loss 8.882455, Accuracy 69.095%\n",
      "Epoch 10, Batch 467, LR 0.000088 Loss 8.881030, Accuracy 69.108%\n",
      "Epoch 10, Batch 468, LR 0.000088 Loss 8.880881, Accuracy 69.102%\n",
      "Epoch 10, Batch 469, LR 0.000088 Loss 8.880762, Accuracy 69.110%\n",
      "Epoch 10, Batch 470, LR 0.000088 Loss 8.879097, Accuracy 69.119%\n",
      "Epoch 10, Batch 471, LR 0.000088 Loss 8.877762, Accuracy 69.125%\n",
      "Epoch 10, Batch 472, LR 0.000088 Loss 8.875807, Accuracy 69.146%\n",
      "Epoch 10, Batch 473, LR 0.000088 Loss 8.874145, Accuracy 69.155%\n",
      "Epoch 10, Batch 474, LR 0.000088 Loss 8.875639, Accuracy 69.144%\n",
      "Epoch 10, Batch 475, LR 0.000088 Loss 8.874695, Accuracy 69.148%\n",
      "Epoch 10, Batch 476, LR 0.000088 Loss 8.874156, Accuracy 69.157%\n",
      "Epoch 10, Batch 477, LR 0.000088 Loss 8.873983, Accuracy 69.151%\n",
      "Epoch 10, Batch 478, LR 0.000088 Loss 8.872561, Accuracy 69.152%\n",
      "Epoch 10, Batch 479, LR 0.000088 Loss 8.872069, Accuracy 69.156%\n",
      "Epoch 10, Batch 480, LR 0.000088 Loss 8.872478, Accuracy 69.159%\n",
      "Epoch 10, Batch 481, LR 0.000088 Loss 8.872857, Accuracy 69.153%\n",
      "Epoch 10, Batch 482, LR 0.000088 Loss 8.871998, Accuracy 69.160%\n",
      "Epoch 10, Batch 483, LR 0.000088 Loss 8.872055, Accuracy 69.156%\n",
      "Epoch 10, Batch 484, LR 0.000088 Loss 8.871422, Accuracy 69.155%\n",
      "Epoch 10, Batch 485, LR 0.000088 Loss 8.871635, Accuracy 69.158%\n",
      "Epoch 10, Batch 486, LR 0.000088 Loss 8.871044, Accuracy 69.163%\n",
      "Epoch 10, Batch 487, LR 0.000088 Loss 8.870025, Accuracy 69.169%\n",
      "Epoch 10, Batch 488, LR 0.000088 Loss 8.871689, Accuracy 69.161%\n",
      "Epoch 10, Batch 489, LR 0.000088 Loss 8.869926, Accuracy 69.170%\n",
      "Epoch 10, Batch 490, LR 0.000088 Loss 8.869889, Accuracy 69.180%\n",
      "Epoch 10, Batch 491, LR 0.000088 Loss 8.870831, Accuracy 69.175%\n",
      "Epoch 10, Batch 492, LR 0.000088 Loss 8.873357, Accuracy 69.163%\n",
      "Epoch 10, Batch 493, LR 0.000088 Loss 8.872644, Accuracy 69.165%\n",
      "Epoch 10, Batch 494, LR 0.000088 Loss 8.871384, Accuracy 69.175%\n",
      "Epoch 10, Batch 495, LR 0.000088 Loss 8.873258, Accuracy 69.167%\n",
      "Epoch 10, Batch 496, LR 0.000088 Loss 8.873436, Accuracy 69.160%\n",
      "Epoch 10, Batch 497, LR 0.000088 Loss 8.874437, Accuracy 69.152%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 498, LR 0.000088 Loss 8.872909, Accuracy 69.158%\n",
      "Epoch 10, Batch 499, LR 0.000088 Loss 8.873960, Accuracy 69.145%\n",
      "Epoch 10, Batch 500, LR 0.000088 Loss 8.875492, Accuracy 69.133%\n",
      "Epoch 10, Batch 501, LR 0.000088 Loss 8.876207, Accuracy 69.130%\n",
      "Epoch 10, Batch 502, LR 0.000088 Loss 8.876448, Accuracy 69.128%\n",
      "Epoch 10, Batch 503, LR 0.000088 Loss 8.876815, Accuracy 69.134%\n",
      "Epoch 10, Batch 504, LR 0.000088 Loss 8.875268, Accuracy 69.148%\n",
      "Epoch 10, Batch 505, LR 0.000088 Loss 8.873516, Accuracy 69.165%\n",
      "Epoch 10, Batch 506, LR 0.000088 Loss 8.873891, Accuracy 69.162%\n",
      "Epoch 10, Batch 507, LR 0.000088 Loss 8.872993, Accuracy 69.151%\n",
      "Epoch 10, Batch 508, LR 0.000088 Loss 8.874315, Accuracy 69.141%\n",
      "Epoch 10, Batch 509, LR 0.000088 Loss 8.874254, Accuracy 69.138%\n",
      "Epoch 10, Batch 510, LR 0.000088 Loss 8.873289, Accuracy 69.138%\n",
      "Epoch 10, Batch 511, LR 0.000088 Loss 8.874132, Accuracy 69.131%\n",
      "Epoch 10, Batch 512, LR 0.000088 Loss 8.872771, Accuracy 69.138%\n",
      "Epoch 10, Batch 513, LR 0.000088 Loss 8.872600, Accuracy 69.140%\n",
      "Epoch 10, Batch 514, LR 0.000088 Loss 8.871193, Accuracy 69.151%\n",
      "Epoch 10, Batch 515, LR 0.000088 Loss 8.870465, Accuracy 69.157%\n",
      "Epoch 10, Batch 516, LR 0.000088 Loss 8.872851, Accuracy 69.147%\n",
      "Epoch 10, Batch 517, LR 0.000088 Loss 8.872398, Accuracy 69.149%\n",
      "Epoch 10, Batch 518, LR 0.000088 Loss 8.873194, Accuracy 69.141%\n",
      "Epoch 10, Batch 519, LR 0.000088 Loss 8.872260, Accuracy 69.146%\n",
      "Epoch 10, Batch 520, LR 0.000088 Loss 8.870886, Accuracy 69.153%\n",
      "Epoch 10, Batch 521, LR 0.000088 Loss 8.870674, Accuracy 69.155%\n",
      "Epoch 10, Batch 522, LR 0.000088 Loss 8.871515, Accuracy 69.142%\n",
      "Epoch 10, Batch 523, LR 0.000088 Loss 8.870889, Accuracy 69.152%\n",
      "Epoch 10, Batch 524, LR 0.000088 Loss 8.871962, Accuracy 69.153%\n",
      "Epoch 10, Batch 525, LR 0.000088 Loss 8.870741, Accuracy 69.156%\n",
      "Epoch 10, Batch 526, LR 0.000088 Loss 8.869775, Accuracy 69.155%\n",
      "Epoch 10, Batch 527, LR 0.000088 Loss 8.868641, Accuracy 69.168%\n",
      "Epoch 10, Batch 528, LR 0.000088 Loss 8.867700, Accuracy 69.173%\n",
      "Epoch 10, Batch 529, LR 0.000088 Loss 8.867237, Accuracy 69.180%\n",
      "Epoch 10, Batch 530, LR 0.000088 Loss 8.867620, Accuracy 69.188%\n",
      "Epoch 10, Batch 531, LR 0.000088 Loss 8.866282, Accuracy 69.202%\n",
      "Epoch 10, Batch 532, LR 0.000088 Loss 8.866726, Accuracy 69.201%\n",
      "Epoch 10, Batch 533, LR 0.000088 Loss 8.868608, Accuracy 69.191%\n",
      "Epoch 10, Batch 534, LR 0.000088 Loss 8.868751, Accuracy 69.183%\n",
      "Epoch 10, Batch 535, LR 0.000088 Loss 8.868459, Accuracy 69.184%\n",
      "Epoch 10, Batch 536, LR 0.000088 Loss 8.868751, Accuracy 69.190%\n",
      "Epoch 10, Batch 537, LR 0.000088 Loss 8.868787, Accuracy 69.189%\n",
      "Epoch 10, Batch 538, LR 0.000088 Loss 8.868361, Accuracy 69.186%\n",
      "Epoch 10, Batch 539, LR 0.000088 Loss 8.867766, Accuracy 69.189%\n",
      "Epoch 10, Batch 540, LR 0.000088 Loss 8.867511, Accuracy 69.193%\n",
      "Epoch 10, Batch 541, LR 0.000088 Loss 8.866393, Accuracy 69.192%\n",
      "Epoch 10, Batch 542, LR 0.000088 Loss 8.865111, Accuracy 69.197%\n",
      "Epoch 10, Batch 543, LR 0.000088 Loss 8.865679, Accuracy 69.190%\n",
      "Epoch 10, Batch 544, LR 0.000088 Loss 8.864914, Accuracy 69.189%\n",
      "Epoch 10, Batch 545, LR 0.000088 Loss 8.864002, Accuracy 69.194%\n",
      "Epoch 10, Batch 546, LR 0.000088 Loss 8.863338, Accuracy 69.199%\n",
      "Epoch 10, Batch 547, LR 0.000088 Loss 8.863530, Accuracy 69.208%\n",
      "Epoch 10, Batch 548, LR 0.000088 Loss 8.864361, Accuracy 69.202%\n",
      "Epoch 10, Batch 549, LR 0.000088 Loss 8.863425, Accuracy 69.208%\n",
      "Epoch 10, Batch 550, LR 0.000088 Loss 8.862543, Accuracy 69.214%\n",
      "Epoch 10, Batch 551, LR 0.000088 Loss 8.861726, Accuracy 69.211%\n",
      "Epoch 10, Batch 552, LR 0.000088 Loss 8.861984, Accuracy 69.207%\n",
      "Epoch 10, Batch 553, LR 0.000088 Loss 8.861932, Accuracy 69.206%\n",
      "Epoch 10, Batch 554, LR 0.000088 Loss 8.861054, Accuracy 69.214%\n",
      "Epoch 10, Batch 555, LR 0.000088 Loss 8.860717, Accuracy 69.216%\n",
      "Epoch 10, Batch 556, LR 0.000088 Loss 8.861630, Accuracy 69.214%\n",
      "Epoch 10, Batch 557, LR 0.000088 Loss 8.861606, Accuracy 69.211%\n",
      "Epoch 10, Batch 558, LR 0.000088 Loss 8.862944, Accuracy 69.199%\n",
      "Epoch 10, Batch 559, LR 0.000088 Loss 8.862957, Accuracy 69.200%\n",
      "Epoch 10, Batch 560, LR 0.000088 Loss 8.862395, Accuracy 69.206%\n",
      "Epoch 10, Batch 561, LR 0.000088 Loss 8.861014, Accuracy 69.215%\n",
      "Epoch 10, Batch 562, LR 0.000088 Loss 8.860016, Accuracy 69.224%\n",
      "Epoch 10, Batch 563, LR 0.000088 Loss 8.859748, Accuracy 69.227%\n",
      "Epoch 10, Batch 564, LR 0.000088 Loss 8.859084, Accuracy 69.239%\n",
      "Epoch 10, Batch 565, LR 0.000088 Loss 8.859711, Accuracy 69.226%\n",
      "Epoch 10, Batch 566, LR 0.000088 Loss 8.859339, Accuracy 69.236%\n",
      "Epoch 10, Batch 567, LR 0.000088 Loss 8.857300, Accuracy 69.249%\n",
      "Epoch 10, Batch 568, LR 0.000088 Loss 8.856621, Accuracy 69.255%\n",
      "Epoch 10, Batch 569, LR 0.000088 Loss 8.854827, Accuracy 69.268%\n",
      "Epoch 10, Batch 570, LR 0.000088 Loss 8.854809, Accuracy 69.264%\n",
      "Epoch 10, Batch 571, LR 0.000088 Loss 8.855269, Accuracy 69.262%\n",
      "Epoch 10, Batch 572, LR 0.000088 Loss 8.855315, Accuracy 69.258%\n",
      "Epoch 10, Batch 573, LR 0.000088 Loss 8.855529, Accuracy 69.260%\n",
      "Epoch 10, Batch 574, LR 0.000088 Loss 8.855279, Accuracy 69.262%\n",
      "Epoch 10, Batch 575, LR 0.000088 Loss 8.854854, Accuracy 69.266%\n",
      "Epoch 10, Batch 576, LR 0.000088 Loss 8.856327, Accuracy 69.257%\n",
      "Epoch 10, Batch 577, LR 0.000088 Loss 8.855544, Accuracy 69.269%\n",
      "Epoch 10, Batch 578, LR 0.000088 Loss 8.855676, Accuracy 69.266%\n",
      "Epoch 10, Batch 579, LR 0.000088 Loss 8.856216, Accuracy 69.259%\n",
      "Epoch 10, Batch 580, LR 0.000088 Loss 8.855443, Accuracy 69.267%\n",
      "Epoch 10, Batch 581, LR 0.000088 Loss 8.854940, Accuracy 69.273%\n",
      "Epoch 10, Batch 582, LR 0.000088 Loss 8.854885, Accuracy 69.275%\n",
      "Epoch 10, Batch 583, LR 0.000088 Loss 8.855848, Accuracy 69.273%\n",
      "Epoch 10, Batch 584, LR 0.000088 Loss 8.856850, Accuracy 69.258%\n",
      "Epoch 10, Batch 585, LR 0.000088 Loss 8.857893, Accuracy 69.248%\n",
      "Epoch 10, Batch 586, LR 0.000088 Loss 8.859341, Accuracy 69.243%\n",
      "Epoch 10, Batch 587, LR 0.000088 Loss 8.859250, Accuracy 69.245%\n",
      "Epoch 10, Batch 588, LR 0.000088 Loss 8.859426, Accuracy 69.240%\n",
      "Epoch 10, Batch 589, LR 0.000088 Loss 8.858876, Accuracy 69.242%\n",
      "Epoch 10, Batch 590, LR 0.000088 Loss 8.859404, Accuracy 69.233%\n",
      "Epoch 10, Batch 591, LR 0.000088 Loss 8.858464, Accuracy 69.235%\n",
      "Epoch 10, Batch 592, LR 0.000088 Loss 8.857408, Accuracy 69.244%\n",
      "Epoch 10, Batch 593, LR 0.000088 Loss 8.857807, Accuracy 69.243%\n",
      "Epoch 10, Batch 594, LR 0.000088 Loss 8.857028, Accuracy 69.242%\n",
      "Epoch 10, Batch 595, LR 0.000088 Loss 8.857307, Accuracy 69.235%\n",
      "Epoch 10, Batch 596, LR 0.000088 Loss 8.857282, Accuracy 69.232%\n",
      "Epoch 10, Batch 597, LR 0.000088 Loss 8.857170, Accuracy 69.234%\n",
      "Epoch 10, Batch 598, LR 0.000088 Loss 8.856396, Accuracy 69.240%\n",
      "Epoch 10, Batch 599, LR 0.000088 Loss 8.856817, Accuracy 69.235%\n",
      "Epoch 10, Batch 600, LR 0.000088 Loss 8.857799, Accuracy 69.233%\n",
      "Epoch 10, Batch 601, LR 0.000088 Loss 8.858580, Accuracy 69.226%\n",
      "Epoch 10, Batch 602, LR 0.000088 Loss 8.858974, Accuracy 69.225%\n",
      "Epoch 10, Batch 603, LR 0.000088 Loss 8.858458, Accuracy 69.227%\n",
      "Epoch 10, Batch 604, LR 0.000088 Loss 8.857537, Accuracy 69.234%\n",
      "Epoch 10, Batch 605, LR 0.000088 Loss 8.854743, Accuracy 69.256%\n",
      "Epoch 10, Batch 606, LR 0.000088 Loss 8.854607, Accuracy 69.254%\n",
      "Epoch 10, Batch 607, LR 0.000088 Loss 8.854138, Accuracy 69.262%\n",
      "Epoch 10, Batch 608, LR 0.000088 Loss 8.855006, Accuracy 69.259%\n",
      "Epoch 10, Batch 609, LR 0.000088 Loss 8.854999, Accuracy 69.254%\n",
      "Epoch 10, Batch 610, LR 0.000088 Loss 8.855831, Accuracy 69.244%\n",
      "Epoch 10, Batch 611, LR 0.000088 Loss 8.856198, Accuracy 69.242%\n",
      "Epoch 10, Batch 612, LR 0.000088 Loss 8.855702, Accuracy 69.244%\n",
      "Epoch 10, Batch 613, LR 0.000088 Loss 8.855411, Accuracy 69.241%\n",
      "Epoch 10, Batch 614, LR 0.000088 Loss 8.855286, Accuracy 69.245%\n",
      "Epoch 10, Batch 615, LR 0.000088 Loss 8.855724, Accuracy 69.237%\n",
      "Epoch 10, Batch 616, LR 0.000088 Loss 8.854681, Accuracy 69.238%\n",
      "Epoch 10, Batch 617, LR 0.000088 Loss 8.853475, Accuracy 69.240%\n",
      "Epoch 10, Batch 618, LR 0.000088 Loss 8.853656, Accuracy 69.240%\n",
      "Epoch 10, Batch 619, LR 0.000088 Loss 8.853200, Accuracy 69.242%\n",
      "Epoch 10, Batch 620, LR 0.000088 Loss 8.852247, Accuracy 69.245%\n",
      "Epoch 10, Batch 621, LR 0.000088 Loss 8.852292, Accuracy 69.247%\n",
      "Epoch 10, Batch 622, LR 0.000088 Loss 8.850943, Accuracy 69.255%\n",
      "Epoch 10, Batch 623, LR 0.000088 Loss 8.851450, Accuracy 69.245%\n",
      "Epoch 10, Batch 624, LR 0.000088 Loss 8.850746, Accuracy 69.240%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 625, LR 0.000088 Loss 8.851688, Accuracy 69.234%\n",
      "Epoch 10, Batch 626, LR 0.000088 Loss 8.851303, Accuracy 69.234%\n",
      "Epoch 10, Batch 627, LR 0.000088 Loss 8.851249, Accuracy 69.237%\n",
      "Epoch 10, Batch 628, LR 0.000088 Loss 8.850597, Accuracy 69.240%\n",
      "Epoch 10, Batch 629, LR 0.000088 Loss 8.851324, Accuracy 69.234%\n",
      "Epoch 10, Batch 630, LR 0.000088 Loss 8.850472, Accuracy 69.249%\n",
      "Epoch 10, Batch 631, LR 0.000088 Loss 8.850180, Accuracy 69.245%\n",
      "Epoch 10, Batch 632, LR 0.000088 Loss 8.849685, Accuracy 69.242%\n",
      "Epoch 10, Batch 633, LR 0.000088 Loss 8.849785, Accuracy 69.242%\n",
      "Epoch 10, Batch 634, LR 0.000088 Loss 8.849234, Accuracy 69.253%\n",
      "Epoch 10, Batch 635, LR 0.000088 Loss 8.848928, Accuracy 69.258%\n",
      "Epoch 10, Batch 636, LR 0.000088 Loss 8.848109, Accuracy 69.255%\n",
      "Epoch 10, Batch 637, LR 0.000088 Loss 8.848319, Accuracy 69.253%\n",
      "Epoch 10, Batch 638, LR 0.000088 Loss 8.849046, Accuracy 69.251%\n",
      "Epoch 10, Batch 639, LR 0.000088 Loss 8.848972, Accuracy 69.252%\n",
      "Epoch 10, Batch 640, LR 0.000088 Loss 8.848993, Accuracy 69.248%\n",
      "Epoch 10, Batch 641, LR 0.000088 Loss 8.849035, Accuracy 69.240%\n",
      "Epoch 10, Batch 642, LR 0.000088 Loss 8.848192, Accuracy 69.248%\n",
      "Epoch 10, Batch 643, LR 0.000088 Loss 8.848095, Accuracy 69.247%\n",
      "Epoch 10, Batch 644, LR 0.000088 Loss 8.848160, Accuracy 69.249%\n",
      "Epoch 10, Batch 645, LR 0.000088 Loss 8.849884, Accuracy 69.239%\n",
      "Epoch 10, Batch 646, LR 0.000088 Loss 8.848572, Accuracy 69.249%\n",
      "Epoch 10, Batch 647, LR 0.000088 Loss 8.847626, Accuracy 69.258%\n",
      "Epoch 10, Batch 648, LR 0.000088 Loss 8.847268, Accuracy 69.255%\n",
      "Epoch 10, Batch 649, LR 0.000088 Loss 8.846930, Accuracy 69.259%\n",
      "Epoch 10, Batch 650, LR 0.000088 Loss 8.848696, Accuracy 69.236%\n",
      "Epoch 10, Batch 651, LR 0.000088 Loss 8.849700, Accuracy 69.238%\n",
      "Epoch 10, Batch 652, LR 0.000088 Loss 8.849345, Accuracy 69.240%\n",
      "Epoch 10, Batch 653, LR 0.000088 Loss 8.850357, Accuracy 69.231%\n",
      "Epoch 10, Batch 654, LR 0.000088 Loss 8.850141, Accuracy 69.233%\n",
      "Epoch 10, Batch 655, LR 0.000088 Loss 8.850081, Accuracy 69.233%\n",
      "Epoch 10, Batch 656, LR 0.000088 Loss 8.851095, Accuracy 69.224%\n",
      "Epoch 10, Batch 657, LR 0.000088 Loss 8.851430, Accuracy 69.221%\n",
      "Epoch 10, Batch 658, LR 0.000088 Loss 8.851735, Accuracy 69.220%\n",
      "Epoch 10, Batch 659, LR 0.000088 Loss 8.851481, Accuracy 69.221%\n",
      "Epoch 10, Batch 660, LR 0.000088 Loss 8.851067, Accuracy 69.226%\n",
      "Epoch 10, Batch 661, LR 0.000088 Loss 8.850452, Accuracy 69.232%\n",
      "Epoch 10, Batch 662, LR 0.000088 Loss 8.851393, Accuracy 69.226%\n",
      "Epoch 10, Batch 663, LR 0.000088 Loss 8.851034, Accuracy 69.225%\n",
      "Epoch 10, Batch 664, LR 0.000088 Loss 8.851414, Accuracy 69.221%\n",
      "Epoch 10, Batch 665, LR 0.000088 Loss 8.852164, Accuracy 69.215%\n",
      "Epoch 10, Batch 666, LR 0.000088 Loss 8.852671, Accuracy 69.211%\n",
      "Epoch 10, Batch 667, LR 0.000088 Loss 8.851945, Accuracy 69.215%\n",
      "Epoch 10, Batch 668, LR 0.000088 Loss 8.852801, Accuracy 69.204%\n",
      "Epoch 10, Batch 669, LR 0.000088 Loss 8.852786, Accuracy 69.203%\n",
      "Epoch 10, Batch 670, LR 0.000088 Loss 8.853202, Accuracy 69.200%\n",
      "Epoch 10, Batch 671, LR 0.000088 Loss 8.853756, Accuracy 69.204%\n",
      "Epoch 10, Batch 672, LR 0.000088 Loss 8.853927, Accuracy 69.198%\n",
      "Epoch 10, Batch 673, LR 0.000088 Loss 8.853008, Accuracy 69.199%\n",
      "Epoch 10, Batch 674, LR 0.000088 Loss 8.853267, Accuracy 69.194%\n",
      "Epoch 10, Batch 675, LR 0.000088 Loss 8.853709, Accuracy 69.196%\n",
      "Epoch 10, Batch 676, LR 0.000088 Loss 8.855166, Accuracy 69.183%\n",
      "Epoch 10, Batch 677, LR 0.000088 Loss 8.854295, Accuracy 69.185%\n",
      "Epoch 10, Batch 678, LR 0.000088 Loss 8.854104, Accuracy 69.182%\n",
      "Epoch 10, Batch 679, LR 0.000088 Loss 8.854052, Accuracy 69.181%\n",
      "Epoch 10, Batch 680, LR 0.000088 Loss 8.854355, Accuracy 69.188%\n",
      "Epoch 10, Batch 681, LR 0.000088 Loss 8.853860, Accuracy 69.192%\n",
      "Epoch 10, Batch 682, LR 0.000088 Loss 8.853676, Accuracy 69.189%\n",
      "Epoch 10, Batch 683, LR 0.000088 Loss 8.854411, Accuracy 69.184%\n",
      "Epoch 10, Batch 684, LR 0.000088 Loss 8.853063, Accuracy 69.189%\n",
      "Epoch 10, Batch 685, LR 0.000088 Loss 8.852767, Accuracy 69.190%\n",
      "Epoch 10, Batch 686, LR 0.000088 Loss 8.852500, Accuracy 69.193%\n",
      "Epoch 10, Batch 687, LR 0.000088 Loss 8.851135, Accuracy 69.199%\n",
      "Epoch 10, Batch 688, LR 0.000088 Loss 8.852200, Accuracy 69.193%\n",
      "Epoch 10, Batch 689, LR 0.000088 Loss 8.851265, Accuracy 69.198%\n",
      "Epoch 10, Batch 690, LR 0.000088 Loss 8.850607, Accuracy 69.204%\n",
      "Epoch 10, Batch 691, LR 0.000088 Loss 8.850747, Accuracy 69.203%\n",
      "Epoch 10, Batch 692, LR 0.000088 Loss 8.852438, Accuracy 69.196%\n",
      "Epoch 10, Batch 693, LR 0.000088 Loss 8.852113, Accuracy 69.198%\n",
      "Epoch 10, Batch 694, LR 0.000088 Loss 8.851622, Accuracy 69.198%\n",
      "Epoch 10, Batch 695, LR 0.000088 Loss 8.853395, Accuracy 69.194%\n",
      "Epoch 10, Batch 696, LR 0.000088 Loss 8.851735, Accuracy 69.197%\n",
      "Epoch 10, Batch 697, LR 0.000088 Loss 8.851430, Accuracy 69.199%\n",
      "Epoch 10, Batch 698, LR 0.000088 Loss 8.852641, Accuracy 69.195%\n",
      "Epoch 10, Batch 699, LR 0.000088 Loss 8.852516, Accuracy 69.199%\n",
      "Epoch 10, Batch 700, LR 0.000088 Loss 8.851940, Accuracy 69.201%\n",
      "Epoch 10, Batch 701, LR 0.000088 Loss 8.852132, Accuracy 69.205%\n",
      "Epoch 10, Batch 702, LR 0.000088 Loss 8.851501, Accuracy 69.209%\n",
      "Epoch 10, Batch 703, LR 0.000088 Loss 8.852761, Accuracy 69.202%\n",
      "Epoch 10, Batch 704, LR 0.000088 Loss 8.852023, Accuracy 69.211%\n",
      "Epoch 10, Batch 705, LR 0.000088 Loss 8.852710, Accuracy 69.208%\n",
      "Epoch 10, Batch 706, LR 0.000088 Loss 8.851128, Accuracy 69.217%\n",
      "Epoch 10, Batch 707, LR 0.000088 Loss 8.850459, Accuracy 69.222%\n",
      "Epoch 10, Batch 708, LR 0.000088 Loss 8.851370, Accuracy 69.216%\n",
      "Epoch 10, Batch 709, LR 0.000088 Loss 8.852291, Accuracy 69.216%\n",
      "Epoch 10, Batch 710, LR 0.000088 Loss 8.851150, Accuracy 69.219%\n",
      "Epoch 10, Batch 711, LR 0.000088 Loss 8.850669, Accuracy 69.225%\n",
      "Epoch 10, Batch 712, LR 0.000088 Loss 8.849924, Accuracy 69.227%\n",
      "Epoch 10, Batch 713, LR 0.000088 Loss 8.850719, Accuracy 69.219%\n",
      "Epoch 10, Batch 714, LR 0.000088 Loss 8.851230, Accuracy 69.211%\n",
      "Epoch 10, Batch 715, LR 0.000088 Loss 8.850127, Accuracy 69.215%\n",
      "Epoch 10, Batch 716, LR 0.000088 Loss 8.850662, Accuracy 69.214%\n",
      "Epoch 10, Batch 717, LR 0.000088 Loss 8.850613, Accuracy 69.220%\n",
      "Epoch 10, Batch 718, LR 0.000088 Loss 8.851240, Accuracy 69.216%\n",
      "Epoch 10, Batch 719, LR 0.000088 Loss 8.851958, Accuracy 69.209%\n",
      "Epoch 10, Batch 720, LR 0.000088 Loss 8.850609, Accuracy 69.221%\n",
      "Epoch 10, Batch 721, LR 0.000088 Loss 8.849720, Accuracy 69.228%\n",
      "Epoch 10, Batch 722, LR 0.000088 Loss 8.849262, Accuracy 69.236%\n",
      "Epoch 10, Batch 723, LR 0.000088 Loss 8.848773, Accuracy 69.239%\n",
      "Epoch 10, Batch 724, LR 0.000088 Loss 8.849046, Accuracy 69.240%\n",
      "Epoch 10, Batch 725, LR 0.000088 Loss 8.848793, Accuracy 69.241%\n",
      "Epoch 10, Batch 726, LR 0.000088 Loss 8.848909, Accuracy 69.246%\n",
      "Epoch 10, Batch 727, LR 0.000088 Loss 8.849107, Accuracy 69.246%\n",
      "Epoch 10, Batch 728, LR 0.000088 Loss 8.848138, Accuracy 69.260%\n",
      "Epoch 10, Batch 729, LR 0.000088 Loss 8.847944, Accuracy 69.265%\n",
      "Epoch 10, Batch 730, LR 0.000088 Loss 8.847883, Accuracy 69.263%\n",
      "Epoch 10, Batch 731, LR 0.000088 Loss 8.848771, Accuracy 69.248%\n",
      "Epoch 10, Batch 732, LR 0.000088 Loss 8.849274, Accuracy 69.243%\n",
      "Epoch 10, Batch 733, LR 0.000088 Loss 8.848193, Accuracy 69.248%\n",
      "Epoch 10, Batch 734, LR 0.000088 Loss 8.848026, Accuracy 69.246%\n",
      "Epoch 10, Batch 735, LR 0.000088 Loss 8.848633, Accuracy 69.244%\n",
      "Epoch 10, Batch 736, LR 0.000088 Loss 8.847910, Accuracy 69.245%\n",
      "Epoch 10, Batch 737, LR 0.000088 Loss 8.847974, Accuracy 69.242%\n",
      "Epoch 10, Batch 738, LR 0.000088 Loss 8.848620, Accuracy 69.243%\n",
      "Epoch 10, Batch 739, LR 0.000088 Loss 8.849095, Accuracy 69.243%\n",
      "Epoch 10, Batch 740, LR 0.000088 Loss 8.849779, Accuracy 69.236%\n",
      "Epoch 10, Batch 741, LR 0.000088 Loss 8.849750, Accuracy 69.241%\n",
      "Epoch 10, Batch 742, LR 0.000088 Loss 8.850646, Accuracy 69.236%\n",
      "Epoch 10, Batch 743, LR 0.000088 Loss 8.850272, Accuracy 69.234%\n",
      "Epoch 10, Batch 744, LR 0.000088 Loss 8.850693, Accuracy 69.229%\n",
      "Epoch 10, Batch 745, LR 0.000088 Loss 8.850633, Accuracy 69.226%\n",
      "Epoch 10, Batch 746, LR 0.000088 Loss 8.850085, Accuracy 69.231%\n",
      "Epoch 10, Batch 747, LR 0.000088 Loss 8.849736, Accuracy 69.230%\n",
      "Epoch 10, Batch 748, LR 0.000088 Loss 8.850388, Accuracy 69.226%\n",
      "Epoch 10, Batch 749, LR 0.000088 Loss 8.850587, Accuracy 69.225%\n",
      "Epoch 10, Batch 750, LR 0.000088 Loss 8.849455, Accuracy 69.226%\n",
      "Epoch 10, Batch 751, LR 0.000088 Loss 8.849497, Accuracy 69.227%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 752, LR 0.000088 Loss 8.850219, Accuracy 69.219%\n",
      "Epoch 10, Batch 753, LR 0.000088 Loss 8.850719, Accuracy 69.216%\n",
      "Epoch 10, Batch 754, LR 0.000088 Loss 8.850244, Accuracy 69.220%\n",
      "Epoch 10, Batch 755, LR 0.000088 Loss 8.849290, Accuracy 69.228%\n",
      "Epoch 10, Batch 756, LR 0.000088 Loss 8.849547, Accuracy 69.229%\n",
      "Epoch 10, Batch 757, LR 0.000088 Loss 8.850566, Accuracy 69.231%\n",
      "Epoch 10, Batch 758, LR 0.000088 Loss 8.850996, Accuracy 69.227%\n",
      "Epoch 10, Batch 759, LR 0.000088 Loss 8.851675, Accuracy 69.216%\n",
      "Epoch 10, Batch 760, LR 0.000088 Loss 8.850456, Accuracy 69.229%\n",
      "Epoch 10, Batch 761, LR 0.000088 Loss 8.850398, Accuracy 69.231%\n",
      "Epoch 10, Batch 762, LR 0.000088 Loss 8.851096, Accuracy 69.228%\n",
      "Epoch 10, Batch 763, LR 0.000088 Loss 8.850028, Accuracy 69.231%\n",
      "Epoch 10, Batch 764, LR 0.000088 Loss 8.849918, Accuracy 69.235%\n",
      "Epoch 10, Batch 765, LR 0.000088 Loss 8.848873, Accuracy 69.235%\n",
      "Epoch 10, Batch 766, LR 0.000088 Loss 8.848861, Accuracy 69.234%\n",
      "Epoch 10, Batch 767, LR 0.000088 Loss 8.849836, Accuracy 69.231%\n",
      "Epoch 10, Batch 768, LR 0.000088 Loss 8.849913, Accuracy 69.223%\n",
      "Epoch 10, Batch 769, LR 0.000088 Loss 8.849153, Accuracy 69.227%\n",
      "Epoch 10, Batch 770, LR 0.000088 Loss 8.848673, Accuracy 69.226%\n",
      "Epoch 10, Batch 771, LR 0.000088 Loss 8.848019, Accuracy 69.236%\n",
      "Epoch 10, Batch 772, LR 0.000088 Loss 8.847413, Accuracy 69.240%\n",
      "Epoch 10, Batch 773, LR 0.000088 Loss 8.847441, Accuracy 69.239%\n",
      "Epoch 10, Batch 774, LR 0.000088 Loss 8.846359, Accuracy 69.247%\n",
      "Epoch 10, Batch 775, LR 0.000088 Loss 8.846414, Accuracy 69.244%\n",
      "Epoch 10, Batch 776, LR 0.000088 Loss 8.845835, Accuracy 69.242%\n",
      "Epoch 10, Batch 777, LR 0.000087 Loss 8.845564, Accuracy 69.242%\n",
      "Epoch 10, Batch 778, LR 0.000087 Loss 8.845878, Accuracy 69.235%\n",
      "Epoch 10, Batch 779, LR 0.000087 Loss 8.844866, Accuracy 69.242%\n",
      "Epoch 10, Batch 780, LR 0.000087 Loss 8.844496, Accuracy 69.239%\n",
      "Epoch 10, Batch 781, LR 0.000087 Loss 8.844375, Accuracy 69.236%\n",
      "Epoch 10, Batch 782, LR 0.000087 Loss 8.843632, Accuracy 69.241%\n",
      "Epoch 10, Batch 783, LR 0.000087 Loss 8.843082, Accuracy 69.246%\n",
      "Epoch 10, Batch 784, LR 0.000087 Loss 8.842715, Accuracy 69.246%\n",
      "Epoch 10, Batch 785, LR 0.000087 Loss 8.842342, Accuracy 69.246%\n",
      "Epoch 10, Batch 786, LR 0.000087 Loss 8.843237, Accuracy 69.236%\n",
      "Epoch 10, Batch 787, LR 0.000087 Loss 8.843041, Accuracy 69.238%\n",
      "Epoch 10, Batch 788, LR 0.000087 Loss 8.843179, Accuracy 69.242%\n",
      "Epoch 10, Batch 789, LR 0.000087 Loss 8.842277, Accuracy 69.242%\n",
      "Epoch 10, Batch 790, LR 0.000087 Loss 8.842185, Accuracy 69.245%\n",
      "Epoch 10, Batch 791, LR 0.000087 Loss 8.841716, Accuracy 69.245%\n",
      "Epoch 10, Batch 792, LR 0.000087 Loss 8.843005, Accuracy 69.232%\n",
      "Epoch 10, Batch 793, LR 0.000087 Loss 8.842490, Accuracy 69.236%\n",
      "Epoch 10, Batch 794, LR 0.000087 Loss 8.841852, Accuracy 69.237%\n",
      "Epoch 10, Batch 795, LR 0.000087 Loss 8.841725, Accuracy 69.235%\n",
      "Epoch 10, Batch 796, LR 0.000087 Loss 8.841609, Accuracy 69.232%\n",
      "Epoch 10, Batch 797, LR 0.000087 Loss 8.841568, Accuracy 69.227%\n",
      "Epoch 10, Batch 798, LR 0.000087 Loss 8.842119, Accuracy 69.219%\n",
      "Epoch 10, Batch 799, LR 0.000087 Loss 8.843915, Accuracy 69.207%\n",
      "Epoch 10, Batch 800, LR 0.000087 Loss 8.844901, Accuracy 69.198%\n",
      "Epoch 10, Batch 801, LR 0.000087 Loss 8.844844, Accuracy 69.193%\n",
      "Epoch 10, Batch 802, LR 0.000087 Loss 8.846015, Accuracy 69.187%\n",
      "Epoch 10, Batch 803, LR 0.000087 Loss 8.845507, Accuracy 69.193%\n",
      "Epoch 10, Batch 804, LR 0.000087 Loss 8.844447, Accuracy 69.198%\n",
      "Epoch 10, Batch 805, LR 0.000087 Loss 8.844094, Accuracy 69.198%\n",
      "Epoch 10, Batch 806, LR 0.000087 Loss 8.844069, Accuracy 69.193%\n",
      "Epoch 10, Batch 807, LR 0.000087 Loss 8.844372, Accuracy 69.194%\n",
      "Epoch 10, Batch 808, LR 0.000087 Loss 8.843500, Accuracy 69.199%\n",
      "Epoch 10, Batch 809, LR 0.000087 Loss 8.843900, Accuracy 69.193%\n",
      "Epoch 10, Batch 810, LR 0.000087 Loss 8.844194, Accuracy 69.193%\n",
      "Epoch 10, Batch 811, LR 0.000087 Loss 8.844525, Accuracy 69.193%\n",
      "Epoch 10, Batch 812, LR 0.000087 Loss 8.844313, Accuracy 69.195%\n",
      "Epoch 10, Batch 813, LR 0.000087 Loss 8.843298, Accuracy 69.205%\n",
      "Epoch 10, Batch 814, LR 0.000087 Loss 8.844268, Accuracy 69.202%\n",
      "Epoch 10, Batch 815, LR 0.000087 Loss 8.844752, Accuracy 69.199%\n",
      "Epoch 10, Batch 816, LR 0.000087 Loss 8.844582, Accuracy 69.196%\n",
      "Epoch 10, Batch 817, LR 0.000087 Loss 8.843858, Accuracy 69.196%\n",
      "Epoch 10, Batch 818, LR 0.000087 Loss 8.843939, Accuracy 69.197%\n",
      "Epoch 10, Batch 819, LR 0.000087 Loss 8.842800, Accuracy 69.211%\n",
      "Epoch 10, Batch 820, LR 0.000087 Loss 8.843103, Accuracy 69.207%\n",
      "Epoch 10, Batch 821, LR 0.000087 Loss 8.842206, Accuracy 69.219%\n",
      "Epoch 10, Batch 822, LR 0.000087 Loss 8.841709, Accuracy 69.222%\n",
      "Epoch 10, Batch 823, LR 0.000087 Loss 8.842297, Accuracy 69.223%\n",
      "Epoch 10, Batch 824, LR 0.000087 Loss 8.841001, Accuracy 69.234%\n",
      "Epoch 10, Batch 825, LR 0.000087 Loss 8.841350, Accuracy 69.236%\n",
      "Epoch 10, Batch 826, LR 0.000087 Loss 8.841325, Accuracy 69.239%\n",
      "Epoch 10, Batch 827, LR 0.000087 Loss 8.841628, Accuracy 69.235%\n",
      "Epoch 10, Batch 828, LR 0.000087 Loss 8.841334, Accuracy 69.238%\n",
      "Epoch 10, Batch 829, LR 0.000087 Loss 8.841848, Accuracy 69.230%\n",
      "Epoch 10, Batch 830, LR 0.000087 Loss 8.840990, Accuracy 69.239%\n",
      "Epoch 10, Batch 831, LR 0.000087 Loss 8.841355, Accuracy 69.235%\n",
      "Epoch 10, Batch 832, LR 0.000087 Loss 8.840567, Accuracy 69.237%\n",
      "Epoch 10, Batch 833, LR 0.000087 Loss 8.839755, Accuracy 69.240%\n",
      "Epoch 10, Batch 834, LR 0.000087 Loss 8.840080, Accuracy 69.241%\n",
      "Epoch 10, Batch 835, LR 0.000087 Loss 8.839797, Accuracy 69.241%\n",
      "Epoch 10, Batch 836, LR 0.000087 Loss 8.840878, Accuracy 69.237%\n",
      "Epoch 10, Batch 837, LR 0.000087 Loss 8.840709, Accuracy 69.233%\n",
      "Epoch 10, Batch 838, LR 0.000087 Loss 8.839882, Accuracy 69.237%\n",
      "Epoch 10, Batch 839, LR 0.000087 Loss 8.839666, Accuracy 69.239%\n",
      "Epoch 10, Batch 840, LR 0.000087 Loss 8.839846, Accuracy 69.237%\n",
      "Epoch 10, Batch 841, LR 0.000087 Loss 8.839186, Accuracy 69.240%\n",
      "Epoch 10, Batch 842, LR 0.000087 Loss 8.839334, Accuracy 69.243%\n",
      "Epoch 10, Batch 843, LR 0.000087 Loss 8.838601, Accuracy 69.249%\n",
      "Epoch 10, Batch 844, LR 0.000087 Loss 8.837540, Accuracy 69.257%\n",
      "Epoch 10, Batch 845, LR 0.000087 Loss 8.836887, Accuracy 69.264%\n",
      "Epoch 10, Batch 846, LR 0.000087 Loss 8.836868, Accuracy 69.263%\n",
      "Epoch 10, Batch 847, LR 0.000087 Loss 8.836578, Accuracy 69.270%\n",
      "Epoch 10, Batch 848, LR 0.000087 Loss 8.836731, Accuracy 69.266%\n",
      "Epoch 10, Batch 849, LR 0.000087 Loss 8.836185, Accuracy 69.267%\n",
      "Epoch 10, Batch 850, LR 0.000087 Loss 8.836568, Accuracy 69.260%\n",
      "Epoch 10, Batch 851, LR 0.000087 Loss 8.836669, Accuracy 69.257%\n",
      "Epoch 10, Batch 852, LR 0.000087 Loss 8.837102, Accuracy 69.249%\n",
      "Epoch 10, Batch 853, LR 0.000087 Loss 8.837815, Accuracy 69.247%\n",
      "Epoch 10, Batch 854, LR 0.000087 Loss 8.837049, Accuracy 69.260%\n",
      "Epoch 10, Batch 855, LR 0.000087 Loss 8.837243, Accuracy 69.259%\n",
      "Epoch 10, Batch 856, LR 0.000087 Loss 8.837785, Accuracy 69.257%\n",
      "Epoch 10, Batch 857, LR 0.000087 Loss 8.837842, Accuracy 69.251%\n",
      "Epoch 10, Batch 858, LR 0.000087 Loss 8.837431, Accuracy 69.250%\n",
      "Epoch 10, Batch 859, LR 0.000087 Loss 8.837407, Accuracy 69.255%\n",
      "Epoch 10, Batch 860, LR 0.000087 Loss 8.837369, Accuracy 69.256%\n",
      "Epoch 10, Batch 861, LR 0.000087 Loss 8.837330, Accuracy 69.255%\n",
      "Epoch 10, Batch 862, LR 0.000087 Loss 8.837497, Accuracy 69.253%\n",
      "Epoch 10, Batch 863, LR 0.000087 Loss 8.836417, Accuracy 69.261%\n",
      "Epoch 10, Batch 864, LR 0.000087 Loss 8.836297, Accuracy 69.265%\n",
      "Epoch 10, Batch 865, LR 0.000087 Loss 8.836906, Accuracy 69.261%\n",
      "Epoch 10, Batch 866, LR 0.000087 Loss 8.835922, Accuracy 69.273%\n",
      "Epoch 10, Batch 867, LR 0.000087 Loss 8.835076, Accuracy 69.276%\n",
      "Epoch 10, Batch 868, LR 0.000087 Loss 8.835065, Accuracy 69.278%\n",
      "Epoch 10, Batch 869, LR 0.000087 Loss 8.835189, Accuracy 69.270%\n",
      "Epoch 10, Batch 870, LR 0.000087 Loss 8.834417, Accuracy 69.274%\n",
      "Epoch 10, Batch 871, LR 0.000087 Loss 8.834756, Accuracy 69.271%\n",
      "Epoch 10, Batch 872, LR 0.000087 Loss 8.834033, Accuracy 69.273%\n",
      "Epoch 10, Batch 873, LR 0.000087 Loss 8.833957, Accuracy 69.277%\n",
      "Epoch 10, Batch 874, LR 0.000087 Loss 8.833054, Accuracy 69.283%\n",
      "Epoch 10, Batch 875, LR 0.000087 Loss 8.833517, Accuracy 69.279%\n",
      "Epoch 10, Batch 876, LR 0.000087 Loss 8.833502, Accuracy 69.283%\n",
      "Epoch 10, Batch 877, LR 0.000087 Loss 8.833736, Accuracy 69.281%\n",
      "Epoch 10, Batch 878, LR 0.000087 Loss 8.833545, Accuracy 69.286%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 879, LR 0.000087 Loss 8.832648, Accuracy 69.294%\n",
      "Epoch 10, Batch 880, LR 0.000087 Loss 8.832030, Accuracy 69.295%\n",
      "Epoch 10, Batch 881, LR 0.000087 Loss 8.832055, Accuracy 69.297%\n",
      "Epoch 10, Batch 882, LR 0.000087 Loss 8.831405, Accuracy 69.299%\n",
      "Epoch 10, Batch 883, LR 0.000087 Loss 8.831676, Accuracy 69.296%\n",
      "Epoch 10, Batch 884, LR 0.000087 Loss 8.831390, Accuracy 69.302%\n",
      "Epoch 10, Batch 885, LR 0.000087 Loss 8.831345, Accuracy 69.299%\n",
      "Epoch 10, Batch 886, LR 0.000087 Loss 8.830753, Accuracy 69.305%\n",
      "Epoch 10, Batch 887, LR 0.000087 Loss 8.830822, Accuracy 69.304%\n",
      "Epoch 10, Batch 888, LR 0.000087 Loss 8.829631, Accuracy 69.307%\n",
      "Epoch 10, Batch 889, LR 0.000087 Loss 8.829419, Accuracy 69.310%\n",
      "Epoch 10, Batch 890, LR 0.000087 Loss 8.829514, Accuracy 69.314%\n",
      "Epoch 10, Batch 891, LR 0.000087 Loss 8.828469, Accuracy 69.323%\n",
      "Epoch 10, Batch 892, LR 0.000087 Loss 8.828810, Accuracy 69.319%\n",
      "Epoch 10, Batch 893, LR 0.000087 Loss 8.829284, Accuracy 69.314%\n",
      "Epoch 10, Batch 894, LR 0.000087 Loss 8.830371, Accuracy 69.313%\n",
      "Epoch 10, Batch 895, LR 0.000087 Loss 8.829628, Accuracy 69.316%\n",
      "Epoch 10, Batch 896, LR 0.000087 Loss 8.828422, Accuracy 69.323%\n",
      "Epoch 10, Batch 897, LR 0.000087 Loss 8.827580, Accuracy 69.326%\n",
      "Epoch 10, Batch 898, LR 0.000087 Loss 8.827446, Accuracy 69.327%\n",
      "Epoch 10, Batch 899, LR 0.000087 Loss 8.827648, Accuracy 69.325%\n",
      "Epoch 10, Batch 900, LR 0.000087 Loss 8.826795, Accuracy 69.331%\n",
      "Epoch 10, Batch 901, LR 0.000087 Loss 8.826798, Accuracy 69.333%\n",
      "Epoch 10, Batch 902, LR 0.000087 Loss 8.825503, Accuracy 69.340%\n",
      "Epoch 10, Batch 903, LR 0.000087 Loss 8.825249, Accuracy 69.344%\n",
      "Epoch 10, Batch 904, LR 0.000087 Loss 8.825045, Accuracy 69.346%\n",
      "Epoch 10, Batch 905, LR 0.000087 Loss 8.824258, Accuracy 69.347%\n",
      "Epoch 10, Batch 906, LR 0.000087 Loss 8.824388, Accuracy 69.344%\n",
      "Epoch 10, Batch 907, LR 0.000087 Loss 8.823841, Accuracy 69.344%\n",
      "Epoch 10, Batch 908, LR 0.000087 Loss 8.823423, Accuracy 69.340%\n",
      "Epoch 10, Batch 909, LR 0.000087 Loss 8.822728, Accuracy 69.345%\n",
      "Epoch 10, Batch 910, LR 0.000087 Loss 8.822136, Accuracy 69.348%\n",
      "Epoch 10, Batch 911, LR 0.000087 Loss 8.821302, Accuracy 69.349%\n",
      "Epoch 10, Batch 912, LR 0.000087 Loss 8.820627, Accuracy 69.350%\n",
      "Epoch 10, Batch 913, LR 0.000087 Loss 8.820319, Accuracy 69.355%\n",
      "Epoch 10, Batch 914, LR 0.000087 Loss 8.820712, Accuracy 69.352%\n",
      "Epoch 10, Batch 915, LR 0.000087 Loss 8.820760, Accuracy 69.354%\n",
      "Epoch 10, Batch 916, LR 0.000087 Loss 8.820264, Accuracy 69.356%\n",
      "Epoch 10, Batch 917, LR 0.000087 Loss 8.820319, Accuracy 69.351%\n",
      "Epoch 10, Batch 918, LR 0.000087 Loss 8.819277, Accuracy 69.354%\n",
      "Epoch 10, Batch 919, LR 0.000087 Loss 8.818610, Accuracy 69.357%\n",
      "Epoch 10, Batch 920, LR 0.000087 Loss 8.819043, Accuracy 69.354%\n",
      "Epoch 10, Batch 921, LR 0.000087 Loss 8.819744, Accuracy 69.351%\n",
      "Epoch 10, Batch 922, LR 0.000087 Loss 8.820092, Accuracy 69.351%\n",
      "Epoch 10, Batch 923, LR 0.000087 Loss 8.820161, Accuracy 69.351%\n",
      "Epoch 10, Batch 924, LR 0.000087 Loss 8.820087, Accuracy 69.349%\n",
      "Epoch 10, Batch 925, LR 0.000087 Loss 8.819227, Accuracy 69.348%\n",
      "Epoch 10, Batch 926, LR 0.000087 Loss 8.818517, Accuracy 69.353%\n",
      "Epoch 10, Batch 927, LR 0.000087 Loss 8.818735, Accuracy 69.352%\n",
      "Epoch 10, Batch 928, LR 0.000087 Loss 8.818306, Accuracy 69.356%\n",
      "Epoch 10, Batch 929, LR 0.000087 Loss 8.817242, Accuracy 69.366%\n",
      "Epoch 10, Batch 930, LR 0.000087 Loss 8.817779, Accuracy 69.367%\n",
      "Epoch 10, Batch 931, LR 0.000087 Loss 8.817578, Accuracy 69.368%\n",
      "Epoch 10, Batch 932, LR 0.000087 Loss 8.817740, Accuracy 69.365%\n",
      "Epoch 10, Batch 933, LR 0.000087 Loss 8.817590, Accuracy 69.370%\n",
      "Epoch 10, Batch 934, LR 0.000087 Loss 8.817258, Accuracy 69.373%\n",
      "Epoch 10, Batch 935, LR 0.000087 Loss 8.816720, Accuracy 69.373%\n",
      "Epoch 10, Batch 936, LR 0.000087 Loss 8.815289, Accuracy 69.376%\n",
      "Epoch 10, Batch 937, LR 0.000087 Loss 8.815220, Accuracy 69.380%\n",
      "Epoch 10, Batch 938, LR 0.000087 Loss 8.814276, Accuracy 69.390%\n",
      "Epoch 10, Batch 939, LR 0.000087 Loss 8.813961, Accuracy 69.393%\n",
      "Epoch 10, Batch 940, LR 0.000087 Loss 8.813763, Accuracy 69.391%\n",
      "Epoch 10, Batch 941, LR 0.000087 Loss 8.812923, Accuracy 69.395%\n",
      "Epoch 10, Batch 942, LR 0.000087 Loss 8.813463, Accuracy 69.394%\n",
      "Epoch 10, Batch 943, LR 0.000087 Loss 8.813152, Accuracy 69.395%\n",
      "Epoch 10, Batch 944, LR 0.000087 Loss 8.812846, Accuracy 69.400%\n",
      "Epoch 10, Batch 945, LR 0.000087 Loss 8.812152, Accuracy 69.414%\n",
      "Epoch 10, Batch 946, LR 0.000087 Loss 8.811484, Accuracy 69.415%\n",
      "Epoch 10, Batch 947, LR 0.000087 Loss 8.810994, Accuracy 69.419%\n",
      "Epoch 10, Batch 948, LR 0.000087 Loss 8.810497, Accuracy 69.422%\n",
      "Epoch 10, Batch 949, LR 0.000087 Loss 8.810173, Accuracy 69.421%\n",
      "Epoch 10, Batch 950, LR 0.000087 Loss 8.809851, Accuracy 69.421%\n",
      "Epoch 10, Batch 951, LR 0.000087 Loss 8.809166, Accuracy 69.420%\n",
      "Epoch 10, Batch 952, LR 0.000087 Loss 8.809483, Accuracy 69.420%\n",
      "Epoch 10, Batch 953, LR 0.000087 Loss 8.809115, Accuracy 69.423%\n",
      "Epoch 10, Batch 954, LR 0.000087 Loss 8.809265, Accuracy 69.418%\n",
      "Epoch 10, Batch 955, LR 0.000087 Loss 8.808545, Accuracy 69.423%\n",
      "Epoch 10, Batch 956, LR 0.000087 Loss 8.808836, Accuracy 69.421%\n",
      "Epoch 10, Batch 957, LR 0.000087 Loss 8.809046, Accuracy 69.417%\n",
      "Epoch 10, Batch 958, LR 0.000087 Loss 8.809787, Accuracy 69.411%\n",
      "Epoch 10, Batch 959, LR 0.000087 Loss 8.809419, Accuracy 69.414%\n",
      "Epoch 10, Batch 960, LR 0.000087 Loss 8.809962, Accuracy 69.406%\n",
      "Epoch 10, Batch 961, LR 0.000087 Loss 8.810212, Accuracy 69.400%\n",
      "Epoch 10, Batch 962, LR 0.000087 Loss 8.810347, Accuracy 69.396%\n",
      "Epoch 10, Batch 963, LR 0.000087 Loss 8.811019, Accuracy 69.396%\n",
      "Epoch 10, Batch 964, LR 0.000087 Loss 8.810714, Accuracy 69.396%\n",
      "Epoch 10, Batch 965, LR 0.000087 Loss 8.809891, Accuracy 69.403%\n",
      "Epoch 10, Batch 966, LR 0.000087 Loss 8.809770, Accuracy 69.411%\n",
      "Epoch 10, Batch 967, LR 0.000087 Loss 8.808897, Accuracy 69.414%\n",
      "Epoch 10, Batch 968, LR 0.000087 Loss 8.808697, Accuracy 69.412%\n",
      "Epoch 10, Batch 969, LR 0.000087 Loss 8.809005, Accuracy 69.410%\n",
      "Epoch 10, Batch 970, LR 0.000087 Loss 8.810208, Accuracy 69.398%\n",
      "Epoch 10, Batch 971, LR 0.000087 Loss 8.809253, Accuracy 69.401%\n",
      "Epoch 10, Batch 972, LR 0.000087 Loss 8.808257, Accuracy 69.401%\n",
      "Epoch 10, Batch 973, LR 0.000087 Loss 8.807702, Accuracy 69.408%\n",
      "Epoch 10, Batch 974, LR 0.000087 Loss 8.807915, Accuracy 69.406%\n",
      "Epoch 10, Batch 975, LR 0.000087 Loss 8.807573, Accuracy 69.409%\n",
      "Epoch 10, Batch 976, LR 0.000087 Loss 8.807620, Accuracy 69.409%\n",
      "Epoch 10, Batch 977, LR 0.000087 Loss 8.807196, Accuracy 69.415%\n",
      "Epoch 10, Batch 978, LR 0.000087 Loss 8.806050, Accuracy 69.429%\n",
      "Epoch 10, Batch 979, LR 0.000087 Loss 8.805896, Accuracy 69.431%\n",
      "Epoch 10, Batch 980, LR 0.000087 Loss 8.805579, Accuracy 69.432%\n",
      "Epoch 10, Batch 981, LR 0.000087 Loss 8.806094, Accuracy 69.427%\n",
      "Epoch 10, Batch 982, LR 0.000087 Loss 8.805525, Accuracy 69.430%\n",
      "Epoch 10, Batch 983, LR 0.000087 Loss 8.805507, Accuracy 69.427%\n",
      "Epoch 10, Batch 984, LR 0.000087 Loss 8.805644, Accuracy 69.426%\n",
      "Epoch 10, Batch 985, LR 0.000087 Loss 8.805315, Accuracy 69.428%\n",
      "Epoch 10, Batch 986, LR 0.000087 Loss 8.804106, Accuracy 69.431%\n",
      "Epoch 10, Batch 987, LR 0.000087 Loss 8.804672, Accuracy 69.431%\n",
      "Epoch 10, Batch 988, LR 0.000087 Loss 8.803867, Accuracy 69.438%\n",
      "Epoch 10, Batch 989, LR 0.000087 Loss 8.804644, Accuracy 69.440%\n",
      "Epoch 10, Batch 990, LR 0.000087 Loss 8.805269, Accuracy 69.436%\n",
      "Epoch 10, Batch 991, LR 0.000087 Loss 8.805853, Accuracy 69.434%\n",
      "Epoch 10, Batch 992, LR 0.000087 Loss 8.804764, Accuracy 69.441%\n",
      "Epoch 10, Batch 993, LR 0.000087 Loss 8.806123, Accuracy 69.427%\n",
      "Epoch 10, Batch 994, LR 0.000087 Loss 8.804854, Accuracy 69.429%\n",
      "Epoch 10, Batch 995, LR 0.000087 Loss 8.804555, Accuracy 69.428%\n",
      "Epoch 10, Batch 996, LR 0.000087 Loss 8.804526, Accuracy 69.432%\n",
      "Epoch 10, Batch 997, LR 0.000087 Loss 8.804332, Accuracy 69.431%\n",
      "Epoch 10, Batch 998, LR 0.000087 Loss 8.804409, Accuracy 69.429%\n",
      "Epoch 10, Batch 999, LR 0.000087 Loss 8.803939, Accuracy 69.433%\n",
      "Epoch 10, Batch 1000, LR 0.000087 Loss 8.803544, Accuracy 69.442%\n",
      "Epoch 10, Batch 1001, LR 0.000087 Loss 8.803306, Accuracy 69.445%\n",
      "Epoch 10, Batch 1002, LR 0.000087 Loss 8.802985, Accuracy 69.446%\n",
      "Epoch 10, Batch 1003, LR 0.000087 Loss 8.803181, Accuracy 69.446%\n",
      "Epoch 10, Batch 1004, LR 0.000087 Loss 8.802882, Accuracy 69.446%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 1005, LR 0.000087 Loss 8.802933, Accuracy 69.444%\n",
      "Epoch 10, Batch 1006, LR 0.000087 Loss 8.802191, Accuracy 69.445%\n",
      "Epoch 10, Batch 1007, LR 0.000087 Loss 8.801141, Accuracy 69.452%\n",
      "Epoch 10, Batch 1008, LR 0.000087 Loss 8.800960, Accuracy 69.455%\n",
      "Epoch 10, Batch 1009, LR 0.000087 Loss 8.801400, Accuracy 69.447%\n",
      "Epoch 10, Batch 1010, LR 0.000087 Loss 8.800917, Accuracy 69.449%\n",
      "Epoch 10, Batch 1011, LR 0.000087 Loss 8.800525, Accuracy 69.449%\n",
      "Epoch 10, Batch 1012, LR 0.000087 Loss 8.800617, Accuracy 69.449%\n",
      "Epoch 10, Batch 1013, LR 0.000087 Loss 8.801288, Accuracy 69.445%\n",
      "Epoch 10, Batch 1014, LR 0.000087 Loss 8.801200, Accuracy 69.444%\n",
      "Epoch 10, Batch 1015, LR 0.000087 Loss 8.800354, Accuracy 69.449%\n",
      "Epoch 10, Batch 1016, LR 0.000087 Loss 8.800029, Accuracy 69.451%\n",
      "Epoch 10, Batch 1017, LR 0.000087 Loss 8.800189, Accuracy 69.449%\n",
      "Epoch 10, Batch 1018, LR 0.000087 Loss 8.800311, Accuracy 69.445%\n",
      "Epoch 10, Batch 1019, LR 0.000087 Loss 8.800675, Accuracy 69.442%\n",
      "Epoch 10, Batch 1020, LR 0.000087 Loss 8.800275, Accuracy 69.439%\n",
      "Epoch 10, Batch 1021, LR 0.000087 Loss 8.799857, Accuracy 69.443%\n",
      "Epoch 10, Batch 1022, LR 0.000087 Loss 8.799677, Accuracy 69.446%\n",
      "Epoch 10, Batch 1023, LR 0.000087 Loss 8.799666, Accuracy 69.450%\n",
      "Epoch 10, Batch 1024, LR 0.000087 Loss 8.799693, Accuracy 69.447%\n",
      "Epoch 10, Batch 1025, LR 0.000087 Loss 8.799032, Accuracy 69.449%\n",
      "Epoch 10, Batch 1026, LR 0.000087 Loss 8.798927, Accuracy 69.451%\n",
      "Epoch 10, Batch 1027, LR 0.000087 Loss 8.798847, Accuracy 69.454%\n",
      "Epoch 10, Batch 1028, LR 0.000087 Loss 8.798602, Accuracy 69.451%\n",
      "Epoch 10, Batch 1029, LR 0.000087 Loss 8.798379, Accuracy 69.454%\n",
      "Epoch 10, Batch 1030, LR 0.000087 Loss 8.798804, Accuracy 69.452%\n",
      "Epoch 10, Batch 1031, LR 0.000087 Loss 8.798645, Accuracy 69.450%\n",
      "Epoch 10, Batch 1032, LR 0.000087 Loss 8.798307, Accuracy 69.455%\n",
      "Epoch 10, Batch 1033, LR 0.000087 Loss 8.797588, Accuracy 69.458%\n",
      "Epoch 10, Batch 1034, LR 0.000087 Loss 8.796527, Accuracy 69.466%\n",
      "Epoch 10, Batch 1035, LR 0.000087 Loss 8.796456, Accuracy 69.467%\n",
      "Epoch 10, Batch 1036, LR 0.000087 Loss 8.796777, Accuracy 69.466%\n",
      "Epoch 10, Batch 1037, LR 0.000087 Loss 8.796567, Accuracy 69.468%\n",
      "Epoch 10, Batch 1038, LR 0.000087 Loss 8.796688, Accuracy 69.465%\n",
      "Epoch 10, Batch 1039, LR 0.000087 Loss 8.797261, Accuracy 69.461%\n",
      "Epoch 10, Batch 1040, LR 0.000087 Loss 8.797396, Accuracy 69.464%\n",
      "Epoch 10, Batch 1041, LR 0.000087 Loss 8.797422, Accuracy 69.461%\n",
      "Epoch 10, Batch 1042, LR 0.000087 Loss 8.797202, Accuracy 69.465%\n",
      "Epoch 10, Batch 1043, LR 0.000087 Loss 8.796627, Accuracy 69.470%\n",
      "Epoch 10, Batch 1044, LR 0.000087 Loss 8.796648, Accuracy 69.470%\n",
      "Epoch 10, Batch 1045, LR 0.000087 Loss 8.796692, Accuracy 69.467%\n",
      "Epoch 10, Batch 1046, LR 0.000087 Loss 8.796274, Accuracy 69.470%\n",
      "Epoch 10, Batch 1047, LR 0.000087 Loss 8.796975, Accuracy 69.465%\n",
      "Epoch 10, Loss (train set) 8.796975, Accuracy (train set) 69.465%\n",
      "Epoch 11, Batch 1, LR 0.000087 Loss 8.075241, Accuracy 74.219%\n",
      "Epoch 11, Batch 2, LR 0.000087 Loss 8.375689, Accuracy 71.094%\n",
      "Epoch 11, Batch 3, LR 0.000087 Loss 8.483779, Accuracy 71.354%\n",
      "Epoch 11, Batch 4, LR 0.000087 Loss 8.578014, Accuracy 70.703%\n",
      "Epoch 11, Batch 5, LR 0.000087 Loss 8.441160, Accuracy 71.719%\n",
      "Epoch 11, Batch 6, LR 0.000087 Loss 8.430077, Accuracy 72.005%\n",
      "Epoch 11, Batch 7, LR 0.000087 Loss 8.304746, Accuracy 72.991%\n",
      "Epoch 11, Batch 8, LR 0.000087 Loss 8.300234, Accuracy 72.852%\n",
      "Epoch 11, Batch 9, LR 0.000087 Loss 8.241265, Accuracy 73.438%\n",
      "Epoch 11, Batch 10, LR 0.000087 Loss 8.260971, Accuracy 73.281%\n",
      "Epoch 11, Batch 11, LR 0.000087 Loss 8.275414, Accuracy 73.224%\n",
      "Epoch 11, Batch 12, LR 0.000087 Loss 8.260138, Accuracy 73.177%\n",
      "Epoch 11, Batch 13, LR 0.000087 Loss 8.287185, Accuracy 72.897%\n",
      "Epoch 11, Batch 14, LR 0.000087 Loss 8.321932, Accuracy 72.600%\n",
      "Epoch 11, Batch 15, LR 0.000087 Loss 8.301727, Accuracy 72.812%\n",
      "Epoch 11, Batch 16, LR 0.000087 Loss 8.333676, Accuracy 72.461%\n",
      "Epoch 11, Batch 17, LR 0.000087 Loss 8.317208, Accuracy 72.426%\n",
      "Epoch 11, Batch 18, LR 0.000087 Loss 8.355384, Accuracy 72.222%\n",
      "Epoch 11, Batch 19, LR 0.000087 Loss 8.370126, Accuracy 72.163%\n",
      "Epoch 11, Batch 20, LR 0.000087 Loss 8.391766, Accuracy 71.914%\n",
      "Epoch 11, Batch 21, LR 0.000087 Loss 8.356242, Accuracy 72.061%\n",
      "Epoch 11, Batch 22, LR 0.000087 Loss 8.342842, Accuracy 72.017%\n",
      "Epoch 11, Batch 23, LR 0.000087 Loss 8.381032, Accuracy 71.841%\n",
      "Epoch 11, Batch 24, LR 0.000087 Loss 8.385973, Accuracy 71.810%\n",
      "Epoch 11, Batch 25, LR 0.000087 Loss 8.390484, Accuracy 71.844%\n",
      "Epoch 11, Batch 26, LR 0.000087 Loss 8.423261, Accuracy 71.665%\n",
      "Epoch 11, Batch 27, LR 0.000087 Loss 8.422904, Accuracy 71.788%\n",
      "Epoch 11, Batch 28, LR 0.000087 Loss 8.431445, Accuracy 71.708%\n",
      "Epoch 11, Batch 29, LR 0.000087 Loss 8.463179, Accuracy 71.471%\n",
      "Epoch 11, Batch 30, LR 0.000087 Loss 8.433571, Accuracy 71.589%\n",
      "Epoch 11, Batch 31, LR 0.000087 Loss 8.451559, Accuracy 71.547%\n",
      "Epoch 11, Batch 32, LR 0.000087 Loss 8.471038, Accuracy 71.387%\n",
      "Epoch 11, Batch 33, LR 0.000087 Loss 8.464471, Accuracy 71.425%\n",
      "Epoch 11, Batch 34, LR 0.000087 Loss 8.478900, Accuracy 71.369%\n",
      "Epoch 11, Batch 35, LR 0.000087 Loss 8.485085, Accuracy 71.272%\n",
      "Epoch 11, Batch 36, LR 0.000087 Loss 8.473177, Accuracy 71.376%\n",
      "Epoch 11, Batch 37, LR 0.000087 Loss 8.468198, Accuracy 71.410%\n",
      "Epoch 11, Batch 38, LR 0.000087 Loss 8.472054, Accuracy 71.464%\n",
      "Epoch 11, Batch 39, LR 0.000087 Loss 8.485111, Accuracy 71.494%\n",
      "Epoch 11, Batch 40, LR 0.000087 Loss 8.482448, Accuracy 71.602%\n",
      "Epoch 11, Batch 41, LR 0.000087 Loss 8.500555, Accuracy 71.475%\n",
      "Epoch 11, Batch 42, LR 0.000087 Loss 8.505644, Accuracy 71.484%\n",
      "Epoch 11, Batch 43, LR 0.000087 Loss 8.504517, Accuracy 71.584%\n",
      "Epoch 11, Batch 44, LR 0.000087 Loss 8.497377, Accuracy 71.502%\n",
      "Epoch 11, Batch 45, LR 0.000087 Loss 8.485774, Accuracy 71.649%\n",
      "Epoch 11, Batch 46, LR 0.000087 Loss 8.477203, Accuracy 71.671%\n",
      "Epoch 11, Batch 47, LR 0.000087 Loss 8.470938, Accuracy 71.742%\n",
      "Epoch 11, Batch 48, LR 0.000087 Loss 8.461927, Accuracy 71.842%\n",
      "Epoch 11, Batch 49, LR 0.000087 Loss 8.459964, Accuracy 71.859%\n",
      "Epoch 11, Batch 50, LR 0.000087 Loss 8.447448, Accuracy 71.906%\n",
      "Epoch 11, Batch 51, LR 0.000087 Loss 8.439675, Accuracy 71.906%\n",
      "Epoch 11, Batch 52, LR 0.000087 Loss 8.457740, Accuracy 71.890%\n",
      "Epoch 11, Batch 53, LR 0.000087 Loss 8.447089, Accuracy 71.934%\n",
      "Epoch 11, Batch 54, LR 0.000087 Loss 8.443345, Accuracy 72.005%\n",
      "Epoch 11, Batch 55, LR 0.000087 Loss 8.438025, Accuracy 72.003%\n",
      "Epoch 11, Batch 56, LR 0.000087 Loss 8.437366, Accuracy 72.015%\n",
      "Epoch 11, Batch 57, LR 0.000087 Loss 8.429959, Accuracy 71.971%\n",
      "Epoch 11, Batch 58, LR 0.000087 Loss 8.430780, Accuracy 71.983%\n",
      "Epoch 11, Batch 59, LR 0.000087 Loss 8.438760, Accuracy 72.007%\n",
      "Epoch 11, Batch 60, LR 0.000087 Loss 8.447840, Accuracy 71.901%\n",
      "Epoch 11, Batch 61, LR 0.000087 Loss 8.447849, Accuracy 71.901%\n",
      "Epoch 11, Batch 62, LR 0.000087 Loss 8.442005, Accuracy 71.837%\n",
      "Epoch 11, Batch 63, LR 0.000087 Loss 8.437995, Accuracy 71.900%\n",
      "Epoch 11, Batch 64, LR 0.000087 Loss 8.427177, Accuracy 71.899%\n",
      "Epoch 11, Batch 65, LR 0.000087 Loss 8.432518, Accuracy 71.887%\n",
      "Epoch 11, Batch 66, LR 0.000087 Loss 8.434495, Accuracy 71.899%\n",
      "Epoch 11, Batch 67, LR 0.000087 Loss 8.445549, Accuracy 71.852%\n",
      "Epoch 11, Batch 68, LR 0.000087 Loss 8.447356, Accuracy 71.818%\n",
      "Epoch 11, Batch 69, LR 0.000087 Loss 8.446261, Accuracy 71.852%\n",
      "Epoch 11, Batch 70, LR 0.000087 Loss 8.450172, Accuracy 71.797%\n",
      "Epoch 11, Batch 71, LR 0.000087 Loss 8.446749, Accuracy 71.886%\n",
      "Epoch 11, Batch 72, LR 0.000087 Loss 8.442244, Accuracy 71.962%\n",
      "Epoch 11, Batch 73, LR 0.000087 Loss 8.436327, Accuracy 71.929%\n",
      "Epoch 11, Batch 74, LR 0.000087 Loss 8.444177, Accuracy 71.833%\n",
      "Epoch 11, Batch 75, LR 0.000087 Loss 8.451477, Accuracy 71.812%\n",
      "Epoch 11, Batch 76, LR 0.000087 Loss 8.458162, Accuracy 71.762%\n",
      "Epoch 11, Batch 77, LR 0.000087 Loss 8.461239, Accuracy 71.713%\n",
      "Epoch 11, Batch 78, LR 0.000087 Loss 8.464075, Accuracy 71.665%\n",
      "Epoch 11, Batch 79, LR 0.000087 Loss 8.455757, Accuracy 71.727%\n",
      "Epoch 11, Batch 80, LR 0.000087 Loss 8.454152, Accuracy 71.680%\n",
      "Epoch 11, Batch 81, LR 0.000087 Loss 8.452189, Accuracy 71.692%\n",
      "Epoch 11, Batch 82, LR 0.000087 Loss 8.451884, Accuracy 71.694%\n",
      "Epoch 11, Batch 83, LR 0.000087 Loss 8.442919, Accuracy 71.743%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 84, LR 0.000087 Loss 8.441963, Accuracy 71.698%\n",
      "Epoch 11, Batch 85, LR 0.000087 Loss 8.449973, Accuracy 71.664%\n",
      "Epoch 11, Batch 86, LR 0.000087 Loss 8.450611, Accuracy 71.630%\n",
      "Epoch 11, Batch 87, LR 0.000087 Loss 8.448008, Accuracy 71.651%\n",
      "Epoch 11, Batch 88, LR 0.000087 Loss 8.452392, Accuracy 71.573%\n",
      "Epoch 11, Batch 89, LR 0.000087 Loss 8.462484, Accuracy 71.533%\n",
      "Epoch 11, Batch 90, LR 0.000087 Loss 8.456561, Accuracy 71.554%\n",
      "Epoch 11, Batch 91, LR 0.000087 Loss 8.451785, Accuracy 71.592%\n",
      "Epoch 11, Batch 92, LR 0.000087 Loss 8.453373, Accuracy 71.586%\n",
      "Epoch 11, Batch 93, LR 0.000087 Loss 8.450555, Accuracy 71.606%\n",
      "Epoch 11, Batch 94, LR 0.000087 Loss 8.454062, Accuracy 71.576%\n",
      "Epoch 11, Batch 95, LR 0.000087 Loss 8.463000, Accuracy 71.521%\n",
      "Epoch 11, Batch 96, LR 0.000087 Loss 8.448277, Accuracy 71.615%\n",
      "Epoch 11, Batch 97, LR 0.000087 Loss 8.454520, Accuracy 71.601%\n",
      "Epoch 11, Batch 98, LR 0.000087 Loss 8.460706, Accuracy 71.588%\n",
      "Epoch 11, Batch 99, LR 0.000087 Loss 8.463116, Accuracy 71.536%\n",
      "Epoch 11, Batch 100, LR 0.000087 Loss 8.458508, Accuracy 71.594%\n",
      "Epoch 11, Batch 101, LR 0.000087 Loss 8.462955, Accuracy 71.542%\n",
      "Epoch 11, Batch 102, LR 0.000087 Loss 8.460360, Accuracy 71.523%\n",
      "Epoch 11, Batch 103, LR 0.000087 Loss 8.466725, Accuracy 71.481%\n",
      "Epoch 11, Batch 104, LR 0.000087 Loss 8.466047, Accuracy 71.484%\n",
      "Epoch 11, Batch 105, LR 0.000087 Loss 8.463201, Accuracy 71.473%\n",
      "Epoch 11, Batch 106, LR 0.000087 Loss 8.464530, Accuracy 71.433%\n",
      "Epoch 11, Batch 107, LR 0.000086 Loss 8.472085, Accuracy 71.393%\n",
      "Epoch 11, Batch 108, LR 0.000086 Loss 8.472704, Accuracy 71.398%\n",
      "Epoch 11, Batch 109, LR 0.000086 Loss 8.474972, Accuracy 71.409%\n",
      "Epoch 11, Batch 110, LR 0.000086 Loss 8.475188, Accuracy 71.413%\n",
      "Epoch 11, Batch 111, LR 0.000086 Loss 8.477913, Accuracy 71.361%\n",
      "Epoch 11, Batch 112, LR 0.000086 Loss 8.478495, Accuracy 71.359%\n",
      "Epoch 11, Batch 113, LR 0.000086 Loss 8.480119, Accuracy 71.398%\n",
      "Epoch 11, Batch 114, LR 0.000086 Loss 8.476749, Accuracy 71.430%\n",
      "Epoch 11, Batch 115, LR 0.000086 Loss 8.474970, Accuracy 71.413%\n",
      "Epoch 11, Batch 116, LR 0.000086 Loss 8.482625, Accuracy 71.316%\n",
      "Epoch 11, Batch 117, LR 0.000086 Loss 8.483382, Accuracy 71.301%\n",
      "Epoch 11, Batch 118, LR 0.000086 Loss 8.480486, Accuracy 71.286%\n",
      "Epoch 11, Batch 119, LR 0.000086 Loss 8.477789, Accuracy 71.278%\n",
      "Epoch 11, Batch 120, LR 0.000086 Loss 8.478511, Accuracy 71.283%\n",
      "Epoch 11, Batch 121, LR 0.000086 Loss 8.478823, Accuracy 71.242%\n",
      "Epoch 11, Batch 122, LR 0.000086 Loss 8.474281, Accuracy 71.260%\n",
      "Epoch 11, Batch 123, LR 0.000086 Loss 8.478987, Accuracy 71.221%\n",
      "Epoch 11, Batch 124, LR 0.000086 Loss 8.481313, Accuracy 71.188%\n",
      "Epoch 11, Batch 125, LR 0.000086 Loss 8.483384, Accuracy 71.213%\n",
      "Epoch 11, Batch 126, LR 0.000086 Loss 8.485795, Accuracy 71.205%\n",
      "Epoch 11, Batch 127, LR 0.000086 Loss 8.487130, Accuracy 71.229%\n",
      "Epoch 11, Batch 128, LR 0.000086 Loss 8.487999, Accuracy 71.246%\n",
      "Epoch 11, Batch 129, LR 0.000086 Loss 8.486074, Accuracy 71.288%\n",
      "Epoch 11, Batch 130, LR 0.000086 Loss 8.489479, Accuracy 71.286%\n",
      "Epoch 11, Batch 131, LR 0.000086 Loss 8.489676, Accuracy 71.332%\n",
      "Epoch 11, Batch 132, LR 0.000086 Loss 8.485604, Accuracy 71.325%\n",
      "Epoch 11, Batch 133, LR 0.000086 Loss 8.483242, Accuracy 71.329%\n",
      "Epoch 11, Batch 134, LR 0.000086 Loss 8.490997, Accuracy 71.263%\n",
      "Epoch 11, Batch 135, LR 0.000086 Loss 8.486258, Accuracy 71.302%\n",
      "Epoch 11, Batch 136, LR 0.000086 Loss 8.485516, Accuracy 71.335%\n",
      "Epoch 11, Batch 137, LR 0.000086 Loss 8.486535, Accuracy 71.339%\n",
      "Epoch 11, Batch 138, LR 0.000086 Loss 8.487074, Accuracy 71.343%\n",
      "Epoch 11, Batch 139, LR 0.000086 Loss 8.491949, Accuracy 71.296%\n",
      "Epoch 11, Batch 140, LR 0.000086 Loss 8.490109, Accuracy 71.289%\n",
      "Epoch 11, Batch 141, LR 0.000086 Loss 8.492769, Accuracy 71.277%\n",
      "Epoch 11, Batch 142, LR 0.000086 Loss 8.491672, Accuracy 71.281%\n",
      "Epoch 11, Batch 143, LR 0.000086 Loss 8.490556, Accuracy 71.280%\n",
      "Epoch 11, Batch 144, LR 0.000086 Loss 8.489340, Accuracy 71.262%\n",
      "Epoch 11, Batch 145, LR 0.000086 Loss 8.491826, Accuracy 71.245%\n",
      "Epoch 11, Batch 146, LR 0.000086 Loss 8.492114, Accuracy 71.286%\n",
      "Epoch 11, Batch 147, LR 0.000086 Loss 8.495937, Accuracy 71.269%\n",
      "Epoch 11, Batch 148, LR 0.000086 Loss 8.499055, Accuracy 71.257%\n",
      "Epoch 11, Batch 149, LR 0.000086 Loss 8.493242, Accuracy 71.335%\n",
      "Epoch 11, Batch 150, LR 0.000086 Loss 8.486909, Accuracy 71.385%\n",
      "Epoch 11, Batch 151, LR 0.000086 Loss 8.488279, Accuracy 71.373%\n",
      "Epoch 11, Batch 152, LR 0.000086 Loss 8.492809, Accuracy 71.340%\n",
      "Epoch 11, Batch 153, LR 0.000086 Loss 8.493992, Accuracy 71.339%\n",
      "Epoch 11, Batch 154, LR 0.000086 Loss 8.493367, Accuracy 71.332%\n",
      "Epoch 11, Batch 155, LR 0.000086 Loss 8.494065, Accuracy 71.331%\n",
      "Epoch 11, Batch 156, LR 0.000086 Loss 8.499750, Accuracy 71.254%\n",
      "Epoch 11, Batch 157, LR 0.000086 Loss 8.497407, Accuracy 71.308%\n",
      "Epoch 11, Batch 158, LR 0.000086 Loss 8.497868, Accuracy 71.326%\n",
      "Epoch 11, Batch 159, LR 0.000086 Loss 8.498579, Accuracy 71.339%\n",
      "Epoch 11, Batch 160, LR 0.000086 Loss 8.501781, Accuracy 71.328%\n",
      "Epoch 11, Batch 161, LR 0.000086 Loss 8.499016, Accuracy 71.351%\n",
      "Epoch 11, Batch 162, LR 0.000086 Loss 8.497549, Accuracy 71.373%\n",
      "Epoch 11, Batch 163, LR 0.000086 Loss 8.501413, Accuracy 71.367%\n",
      "Epoch 11, Batch 164, LR 0.000086 Loss 8.497883, Accuracy 71.422%\n",
      "Epoch 11, Batch 165, LR 0.000086 Loss 8.498171, Accuracy 71.425%\n",
      "Epoch 11, Batch 166, LR 0.000086 Loss 8.499633, Accuracy 71.428%\n",
      "Epoch 11, Batch 167, LR 0.000086 Loss 8.499119, Accuracy 71.421%\n",
      "Epoch 11, Batch 168, LR 0.000086 Loss 8.496430, Accuracy 71.438%\n",
      "Epoch 11, Batch 169, LR 0.000086 Loss 8.498240, Accuracy 71.427%\n",
      "Epoch 11, Batch 170, LR 0.000086 Loss 8.500322, Accuracy 71.397%\n",
      "Epoch 11, Batch 171, LR 0.000086 Loss 8.500746, Accuracy 71.414%\n",
      "Epoch 11, Batch 172, LR 0.000086 Loss 8.496495, Accuracy 71.412%\n",
      "Epoch 11, Batch 173, LR 0.000086 Loss 8.493715, Accuracy 71.428%\n",
      "Epoch 11, Batch 174, LR 0.000086 Loss 8.493061, Accuracy 71.430%\n",
      "Epoch 11, Batch 175, LR 0.000086 Loss 8.496012, Accuracy 71.424%\n",
      "Epoch 11, Batch 176, LR 0.000086 Loss 8.494526, Accuracy 71.418%\n",
      "Epoch 11, Batch 177, LR 0.000086 Loss 8.497002, Accuracy 71.394%\n",
      "Epoch 11, Batch 178, LR 0.000086 Loss 8.491280, Accuracy 71.410%\n",
      "Epoch 11, Batch 179, LR 0.000086 Loss 8.489322, Accuracy 71.452%\n",
      "Epoch 11, Batch 180, LR 0.000086 Loss 8.488897, Accuracy 71.471%\n",
      "Epoch 11, Batch 181, LR 0.000086 Loss 8.491480, Accuracy 71.465%\n",
      "Epoch 11, Batch 182, LR 0.000086 Loss 8.487287, Accuracy 71.493%\n",
      "Epoch 11, Batch 183, LR 0.000086 Loss 8.489211, Accuracy 71.512%\n",
      "Epoch 11, Batch 184, LR 0.000086 Loss 8.494645, Accuracy 71.455%\n",
      "Epoch 11, Batch 185, LR 0.000086 Loss 8.491674, Accuracy 71.470%\n",
      "Epoch 11, Batch 186, LR 0.000086 Loss 8.490622, Accuracy 71.476%\n",
      "Epoch 11, Batch 187, LR 0.000086 Loss 8.490693, Accuracy 71.486%\n",
      "Epoch 11, Batch 188, LR 0.000086 Loss 8.493197, Accuracy 71.472%\n",
      "Epoch 11, Batch 189, LR 0.000086 Loss 8.494626, Accuracy 71.453%\n",
      "Epoch 11, Batch 190, LR 0.000086 Loss 8.493597, Accuracy 71.468%\n",
      "Epoch 11, Batch 191, LR 0.000086 Loss 8.497637, Accuracy 71.454%\n",
      "Epoch 11, Batch 192, LR 0.000086 Loss 8.497188, Accuracy 71.440%\n",
      "Epoch 11, Batch 193, LR 0.000086 Loss 8.496626, Accuracy 71.438%\n",
      "Epoch 11, Batch 194, LR 0.000086 Loss 8.496643, Accuracy 71.440%\n",
      "Epoch 11, Batch 195, LR 0.000086 Loss 8.491154, Accuracy 71.462%\n",
      "Epoch 11, Batch 196, LR 0.000086 Loss 8.489070, Accuracy 71.472%\n",
      "Epoch 11, Batch 197, LR 0.000086 Loss 8.488159, Accuracy 71.470%\n",
      "Epoch 11, Batch 198, LR 0.000086 Loss 8.486663, Accuracy 71.480%\n",
      "Epoch 11, Batch 199, LR 0.000086 Loss 8.487444, Accuracy 71.506%\n",
      "Epoch 11, Batch 200, LR 0.000086 Loss 8.486204, Accuracy 71.516%\n",
      "Epoch 11, Batch 201, LR 0.000086 Loss 8.487846, Accuracy 71.490%\n",
      "Epoch 11, Batch 202, LR 0.000086 Loss 8.486022, Accuracy 71.504%\n",
      "Epoch 11, Batch 203, LR 0.000086 Loss 8.488871, Accuracy 71.490%\n",
      "Epoch 11, Batch 204, LR 0.000086 Loss 8.487171, Accuracy 71.500%\n",
      "Epoch 11, Batch 205, LR 0.000086 Loss 8.488877, Accuracy 71.479%\n",
      "Epoch 11, Batch 206, LR 0.000086 Loss 8.491791, Accuracy 71.450%\n",
      "Epoch 11, Batch 207, LR 0.000086 Loss 8.490961, Accuracy 71.418%\n",
      "Epoch 11, Batch 208, LR 0.000086 Loss 8.490986, Accuracy 71.402%\n",
      "Epoch 11, Batch 209, LR 0.000086 Loss 8.489250, Accuracy 71.411%\n",
      "Epoch 11, Batch 210, LR 0.000086 Loss 8.492542, Accuracy 71.417%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 211, LR 0.000086 Loss 8.492002, Accuracy 71.434%\n",
      "Epoch 11, Batch 212, LR 0.000086 Loss 8.492342, Accuracy 71.444%\n",
      "Epoch 11, Batch 213, LR 0.000086 Loss 8.491803, Accuracy 71.450%\n",
      "Epoch 11, Batch 214, LR 0.000086 Loss 8.494076, Accuracy 71.437%\n",
      "Epoch 11, Batch 215, LR 0.000086 Loss 8.493601, Accuracy 71.453%\n",
      "Epoch 11, Batch 216, LR 0.000086 Loss 8.496548, Accuracy 71.423%\n",
      "Epoch 11, Batch 217, LR 0.000086 Loss 8.498442, Accuracy 71.403%\n",
      "Epoch 11, Batch 218, LR 0.000086 Loss 8.495100, Accuracy 71.416%\n",
      "Epoch 11, Batch 219, LR 0.000086 Loss 8.492439, Accuracy 71.408%\n",
      "Epoch 11, Batch 220, LR 0.000086 Loss 8.496081, Accuracy 71.367%\n",
      "Epoch 11, Batch 221, LR 0.000086 Loss 8.494645, Accuracy 71.380%\n",
      "Epoch 11, Batch 222, LR 0.000086 Loss 8.493707, Accuracy 71.386%\n",
      "Epoch 11, Batch 223, LR 0.000086 Loss 8.496304, Accuracy 71.367%\n",
      "Epoch 11, Batch 224, LR 0.000086 Loss 8.495224, Accuracy 71.352%\n",
      "Epoch 11, Batch 225, LR 0.000086 Loss 8.493789, Accuracy 71.365%\n",
      "Epoch 11, Batch 226, LR 0.000086 Loss 8.493957, Accuracy 71.367%\n",
      "Epoch 11, Batch 227, LR 0.000086 Loss 8.493879, Accuracy 71.342%\n",
      "Epoch 11, Batch 228, LR 0.000086 Loss 8.490276, Accuracy 71.364%\n",
      "Epoch 11, Batch 229, LR 0.000086 Loss 8.487695, Accuracy 71.384%\n",
      "Epoch 11, Batch 230, LR 0.000086 Loss 8.488164, Accuracy 71.382%\n",
      "Epoch 11, Batch 231, LR 0.000086 Loss 8.485348, Accuracy 71.402%\n",
      "Epoch 11, Batch 232, LR 0.000086 Loss 8.483441, Accuracy 71.400%\n",
      "Epoch 11, Batch 233, LR 0.000086 Loss 8.485512, Accuracy 71.382%\n",
      "Epoch 11, Batch 234, LR 0.000086 Loss 8.485819, Accuracy 71.374%\n",
      "Epoch 11, Batch 235, LR 0.000086 Loss 8.482880, Accuracy 71.396%\n",
      "Epoch 11, Batch 236, LR 0.000086 Loss 8.482404, Accuracy 71.415%\n",
      "Epoch 11, Batch 237, LR 0.000086 Loss 8.481186, Accuracy 71.443%\n",
      "Epoch 11, Batch 238, LR 0.000086 Loss 8.478412, Accuracy 71.461%\n",
      "Epoch 11, Batch 239, LR 0.000086 Loss 8.475896, Accuracy 71.466%\n",
      "Epoch 11, Batch 240, LR 0.000086 Loss 8.470738, Accuracy 71.481%\n",
      "Epoch 11, Batch 241, LR 0.000086 Loss 8.475704, Accuracy 71.431%\n",
      "Epoch 11, Batch 242, LR 0.000086 Loss 8.479203, Accuracy 71.423%\n",
      "Epoch 11, Batch 243, LR 0.000086 Loss 8.479332, Accuracy 71.393%\n",
      "Epoch 11, Batch 244, LR 0.000086 Loss 8.480026, Accuracy 71.388%\n",
      "Epoch 11, Batch 245, LR 0.000086 Loss 8.478142, Accuracy 71.384%\n",
      "Epoch 11, Batch 246, LR 0.000086 Loss 8.480571, Accuracy 71.367%\n",
      "Epoch 11, Batch 247, LR 0.000086 Loss 8.482625, Accuracy 71.350%\n",
      "Epoch 11, Batch 248, LR 0.000086 Loss 8.482771, Accuracy 71.349%\n",
      "Epoch 11, Batch 249, LR 0.000086 Loss 8.483752, Accuracy 71.335%\n",
      "Epoch 11, Batch 250, LR 0.000086 Loss 8.483952, Accuracy 71.344%\n",
      "Epoch 11, Batch 251, LR 0.000086 Loss 8.483142, Accuracy 71.321%\n",
      "Epoch 11, Batch 252, LR 0.000086 Loss 8.480617, Accuracy 71.351%\n",
      "Epoch 11, Batch 253, LR 0.000086 Loss 8.478826, Accuracy 71.372%\n",
      "Epoch 11, Batch 254, LR 0.000086 Loss 8.480018, Accuracy 71.349%\n",
      "Epoch 11, Batch 255, LR 0.000086 Loss 8.481362, Accuracy 71.330%\n",
      "Epoch 11, Batch 256, LR 0.000086 Loss 8.479374, Accuracy 71.323%\n",
      "Epoch 11, Batch 257, LR 0.000086 Loss 8.477344, Accuracy 71.337%\n",
      "Epoch 11, Batch 258, LR 0.000086 Loss 8.478351, Accuracy 71.312%\n",
      "Epoch 11, Batch 259, LR 0.000086 Loss 8.478454, Accuracy 71.329%\n",
      "Epoch 11, Batch 260, LR 0.000086 Loss 8.479885, Accuracy 71.313%\n",
      "Epoch 11, Batch 261, LR 0.000086 Loss 8.481906, Accuracy 71.306%\n",
      "Epoch 11, Batch 262, LR 0.000086 Loss 8.482318, Accuracy 71.302%\n",
      "Epoch 11, Batch 263, LR 0.000086 Loss 8.482371, Accuracy 71.293%\n",
      "Epoch 11, Batch 264, LR 0.000086 Loss 8.484783, Accuracy 71.274%\n",
      "Epoch 11, Batch 265, LR 0.000086 Loss 8.486673, Accuracy 71.268%\n",
      "Epoch 11, Batch 266, LR 0.000086 Loss 8.484721, Accuracy 71.285%\n",
      "Epoch 11, Batch 267, LR 0.000086 Loss 8.484778, Accuracy 71.284%\n",
      "Epoch 11, Batch 268, LR 0.000086 Loss 8.484619, Accuracy 71.272%\n",
      "Epoch 11, Batch 269, LR 0.000086 Loss 8.484045, Accuracy 71.277%\n",
      "Epoch 11, Batch 270, LR 0.000086 Loss 8.481928, Accuracy 71.279%\n",
      "Epoch 11, Batch 271, LR 0.000086 Loss 8.481083, Accuracy 71.281%\n",
      "Epoch 11, Batch 272, LR 0.000086 Loss 8.479665, Accuracy 71.286%\n",
      "Epoch 11, Batch 273, LR 0.000086 Loss 8.479429, Accuracy 71.283%\n",
      "Epoch 11, Batch 274, LR 0.000086 Loss 8.479103, Accuracy 71.273%\n",
      "Epoch 11, Batch 275, LR 0.000086 Loss 8.478738, Accuracy 71.284%\n",
      "Epoch 11, Batch 276, LR 0.000086 Loss 8.476652, Accuracy 71.300%\n",
      "Epoch 11, Batch 277, LR 0.000086 Loss 8.476208, Accuracy 71.297%\n",
      "Epoch 11, Batch 278, LR 0.000086 Loss 8.475849, Accuracy 71.290%\n",
      "Epoch 11, Batch 279, LR 0.000086 Loss 8.474077, Accuracy 71.298%\n",
      "Epoch 11, Batch 280, LR 0.000086 Loss 8.474855, Accuracy 71.295%\n",
      "Epoch 11, Batch 281, LR 0.000086 Loss 8.474158, Accuracy 71.305%\n",
      "Epoch 11, Batch 282, LR 0.000086 Loss 8.473075, Accuracy 71.310%\n",
      "Epoch 11, Batch 283, LR 0.000086 Loss 8.472450, Accuracy 71.320%\n",
      "Epoch 11, Batch 284, LR 0.000086 Loss 8.474278, Accuracy 71.297%\n",
      "Epoch 11, Batch 285, LR 0.000086 Loss 8.472569, Accuracy 71.319%\n",
      "Epoch 11, Batch 286, LR 0.000086 Loss 8.472736, Accuracy 71.329%\n",
      "Epoch 11, Batch 287, LR 0.000086 Loss 8.470972, Accuracy 71.347%\n",
      "Epoch 11, Batch 288, LR 0.000086 Loss 8.468864, Accuracy 71.341%\n",
      "Epoch 11, Batch 289, LR 0.000086 Loss 8.467641, Accuracy 71.348%\n",
      "Epoch 11, Batch 290, LR 0.000086 Loss 8.468072, Accuracy 71.334%\n",
      "Epoch 11, Batch 291, LR 0.000086 Loss 8.467521, Accuracy 71.335%\n",
      "Epoch 11, Batch 292, LR 0.000086 Loss 8.466451, Accuracy 71.351%\n",
      "Epoch 11, Batch 293, LR 0.000086 Loss 8.465586, Accuracy 71.347%\n",
      "Epoch 11, Batch 294, LR 0.000086 Loss 8.467251, Accuracy 71.330%\n",
      "Epoch 11, Batch 295, LR 0.000086 Loss 8.467665, Accuracy 71.327%\n",
      "Epoch 11, Batch 296, LR 0.000086 Loss 8.468661, Accuracy 71.329%\n",
      "Epoch 11, Batch 297, LR 0.000086 Loss 8.469462, Accuracy 71.346%\n",
      "Epoch 11, Batch 298, LR 0.000086 Loss 8.469514, Accuracy 71.353%\n",
      "Epoch 11, Batch 299, LR 0.000086 Loss 8.467321, Accuracy 71.350%\n",
      "Epoch 11, Batch 300, LR 0.000086 Loss 8.468410, Accuracy 71.339%\n",
      "Epoch 11, Batch 301, LR 0.000086 Loss 8.469117, Accuracy 71.322%\n",
      "Epoch 11, Batch 302, LR 0.000086 Loss 8.469881, Accuracy 71.311%\n",
      "Epoch 11, Batch 303, LR 0.000086 Loss 8.467251, Accuracy 71.334%\n",
      "Epoch 11, Batch 304, LR 0.000086 Loss 8.467780, Accuracy 71.325%\n",
      "Epoch 11, Batch 305, LR 0.000086 Loss 8.464439, Accuracy 71.340%\n",
      "Epoch 11, Batch 306, LR 0.000086 Loss 8.463649, Accuracy 71.341%\n",
      "Epoch 11, Batch 307, LR 0.000086 Loss 8.463545, Accuracy 71.338%\n",
      "Epoch 11, Batch 308, LR 0.000086 Loss 8.463706, Accuracy 71.337%\n",
      "Epoch 11, Batch 309, LR 0.000086 Loss 8.462492, Accuracy 71.344%\n",
      "Epoch 11, Batch 310, LR 0.000086 Loss 8.463449, Accuracy 71.336%\n",
      "Epoch 11, Batch 311, LR 0.000086 Loss 8.463573, Accuracy 71.337%\n",
      "Epoch 11, Batch 312, LR 0.000086 Loss 8.463214, Accuracy 71.349%\n",
      "Epoch 11, Batch 313, LR 0.000086 Loss 8.462905, Accuracy 71.356%\n",
      "Epoch 11, Batch 314, LR 0.000086 Loss 8.460438, Accuracy 71.355%\n",
      "Epoch 11, Batch 315, LR 0.000086 Loss 8.460598, Accuracy 71.367%\n",
      "Epoch 11, Batch 316, LR 0.000086 Loss 8.458002, Accuracy 71.395%\n",
      "Epoch 11, Batch 317, LR 0.000086 Loss 8.456409, Accuracy 71.392%\n",
      "Epoch 11, Batch 318, LR 0.000086 Loss 8.457133, Accuracy 71.381%\n",
      "Epoch 11, Batch 319, LR 0.000086 Loss 8.454516, Accuracy 71.385%\n",
      "Epoch 11, Batch 320, LR 0.000086 Loss 8.454187, Accuracy 71.379%\n",
      "Epoch 11, Batch 321, LR 0.000086 Loss 8.455725, Accuracy 71.361%\n",
      "Epoch 11, Batch 322, LR 0.000086 Loss 8.455715, Accuracy 71.361%\n",
      "Epoch 11, Batch 323, LR 0.000086 Loss 8.458808, Accuracy 71.333%\n",
      "Epoch 11, Batch 324, LR 0.000086 Loss 8.458074, Accuracy 71.330%\n",
      "Epoch 11, Batch 325, LR 0.000086 Loss 8.456097, Accuracy 71.349%\n",
      "Epoch 11, Batch 326, LR 0.000086 Loss 8.453736, Accuracy 71.365%\n",
      "Epoch 11, Batch 327, LR 0.000086 Loss 8.453705, Accuracy 71.357%\n",
      "Epoch 11, Batch 328, LR 0.000086 Loss 8.454013, Accuracy 71.346%\n",
      "Epoch 11, Batch 329, LR 0.000086 Loss 8.451928, Accuracy 71.353%\n",
      "Epoch 11, Batch 330, LR 0.000086 Loss 8.450252, Accuracy 71.364%\n",
      "Epoch 11, Batch 331, LR 0.000086 Loss 8.450422, Accuracy 71.351%\n",
      "Epoch 11, Batch 332, LR 0.000086 Loss 8.447987, Accuracy 71.362%\n",
      "Epoch 11, Batch 333, LR 0.000086 Loss 8.446010, Accuracy 71.373%\n",
      "Epoch 11, Batch 334, LR 0.000086 Loss 8.446199, Accuracy 71.367%\n",
      "Epoch 11, Batch 335, LR 0.000086 Loss 8.446597, Accuracy 71.378%\n",
      "Epoch 11, Batch 336, LR 0.000086 Loss 8.448550, Accuracy 71.370%\n",
      "Epoch 11, Batch 337, LR 0.000086 Loss 8.450161, Accuracy 71.372%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 338, LR 0.000086 Loss 8.450817, Accuracy 71.364%\n",
      "Epoch 11, Batch 339, LR 0.000086 Loss 8.450293, Accuracy 71.380%\n",
      "Epoch 11, Batch 340, LR 0.000086 Loss 8.450070, Accuracy 71.372%\n",
      "Epoch 11, Batch 341, LR 0.000086 Loss 8.452371, Accuracy 71.360%\n",
      "Epoch 11, Batch 342, LR 0.000086 Loss 8.451595, Accuracy 71.361%\n",
      "Epoch 11, Batch 343, LR 0.000086 Loss 8.452734, Accuracy 71.353%\n",
      "Epoch 11, Batch 344, LR 0.000086 Loss 8.453834, Accuracy 71.348%\n",
      "Epoch 11, Batch 345, LR 0.000086 Loss 8.454413, Accuracy 71.338%\n",
      "Epoch 11, Batch 346, LR 0.000086 Loss 8.453578, Accuracy 71.356%\n",
      "Epoch 11, Batch 347, LR 0.000086 Loss 8.454882, Accuracy 71.348%\n",
      "Epoch 11, Batch 348, LR 0.000086 Loss 8.456418, Accuracy 71.336%\n",
      "Epoch 11, Batch 349, LR 0.000086 Loss 8.456812, Accuracy 71.338%\n",
      "Epoch 11, Batch 350, LR 0.000086 Loss 8.455317, Accuracy 71.346%\n",
      "Epoch 11, Batch 351, LR 0.000086 Loss 8.454271, Accuracy 71.347%\n",
      "Epoch 11, Batch 352, LR 0.000086 Loss 8.455553, Accuracy 71.329%\n",
      "Epoch 11, Batch 353, LR 0.000086 Loss 8.455467, Accuracy 71.331%\n",
      "Epoch 11, Batch 354, LR 0.000086 Loss 8.453554, Accuracy 71.339%\n",
      "Epoch 11, Batch 355, LR 0.000086 Loss 8.453498, Accuracy 71.342%\n",
      "Epoch 11, Batch 356, LR 0.000086 Loss 8.454451, Accuracy 71.324%\n",
      "Epoch 11, Batch 357, LR 0.000086 Loss 8.452784, Accuracy 71.339%\n",
      "Epoch 11, Batch 358, LR 0.000086 Loss 8.452855, Accuracy 71.332%\n",
      "Epoch 11, Batch 359, LR 0.000086 Loss 8.452410, Accuracy 71.331%\n",
      "Epoch 11, Batch 360, LR 0.000086 Loss 8.452835, Accuracy 71.324%\n",
      "Epoch 11, Batch 361, LR 0.000086 Loss 8.451106, Accuracy 71.332%\n",
      "Epoch 11, Batch 362, LR 0.000086 Loss 8.450180, Accuracy 71.338%\n",
      "Epoch 11, Batch 363, LR 0.000086 Loss 8.449696, Accuracy 71.341%\n",
      "Epoch 11, Batch 364, LR 0.000086 Loss 8.449722, Accuracy 71.345%\n",
      "Epoch 11, Batch 365, LR 0.000086 Loss 8.450361, Accuracy 71.338%\n",
      "Epoch 11, Batch 366, LR 0.000086 Loss 8.447825, Accuracy 71.354%\n",
      "Epoch 11, Batch 367, LR 0.000086 Loss 8.447192, Accuracy 71.360%\n",
      "Epoch 11, Batch 368, LR 0.000086 Loss 8.445685, Accuracy 71.372%\n",
      "Epoch 11, Batch 369, LR 0.000086 Loss 8.443058, Accuracy 71.384%\n",
      "Epoch 11, Batch 370, LR 0.000086 Loss 8.443627, Accuracy 71.366%\n",
      "Epoch 11, Batch 371, LR 0.000086 Loss 8.440905, Accuracy 71.395%\n",
      "Epoch 11, Batch 372, LR 0.000086 Loss 8.440717, Accuracy 71.409%\n",
      "Epoch 11, Batch 373, LR 0.000086 Loss 8.441442, Accuracy 71.414%\n",
      "Epoch 11, Batch 374, LR 0.000086 Loss 8.440197, Accuracy 71.422%\n",
      "Epoch 11, Batch 375, LR 0.000086 Loss 8.441107, Accuracy 71.421%\n",
      "Epoch 11, Batch 376, LR 0.000086 Loss 8.439909, Accuracy 71.428%\n",
      "Epoch 11, Batch 377, LR 0.000086 Loss 8.438156, Accuracy 71.444%\n",
      "Epoch 11, Batch 378, LR 0.000086 Loss 8.438092, Accuracy 71.447%\n",
      "Epoch 11, Batch 379, LR 0.000086 Loss 8.437474, Accuracy 71.459%\n",
      "Epoch 11, Batch 380, LR 0.000086 Loss 8.435554, Accuracy 71.476%\n",
      "Epoch 11, Batch 381, LR 0.000086 Loss 8.434908, Accuracy 71.496%\n",
      "Epoch 11, Batch 382, LR 0.000086 Loss 8.435576, Accuracy 71.478%\n",
      "Epoch 11, Batch 383, LR 0.000086 Loss 8.436176, Accuracy 71.483%\n",
      "Epoch 11, Batch 384, LR 0.000086 Loss 8.437488, Accuracy 71.474%\n",
      "Epoch 11, Batch 385, LR 0.000086 Loss 8.437916, Accuracy 71.479%\n",
      "Epoch 11, Batch 386, LR 0.000086 Loss 8.436429, Accuracy 71.492%\n",
      "Epoch 11, Batch 387, LR 0.000086 Loss 8.434302, Accuracy 71.504%\n",
      "Epoch 11, Batch 388, LR 0.000086 Loss 8.435635, Accuracy 71.494%\n",
      "Epoch 11, Batch 389, LR 0.000086 Loss 8.437301, Accuracy 71.485%\n",
      "Epoch 11, Batch 390, LR 0.000086 Loss 8.437458, Accuracy 71.474%\n",
      "Epoch 11, Batch 391, LR 0.000086 Loss 8.437257, Accuracy 71.463%\n",
      "Epoch 11, Batch 392, LR 0.000086 Loss 8.438135, Accuracy 71.468%\n",
      "Epoch 11, Batch 393, LR 0.000086 Loss 8.439579, Accuracy 71.450%\n",
      "Epoch 11, Batch 394, LR 0.000086 Loss 8.437655, Accuracy 71.465%\n",
      "Epoch 11, Batch 395, LR 0.000086 Loss 8.437354, Accuracy 71.460%\n",
      "Epoch 11, Batch 396, LR 0.000086 Loss 8.439177, Accuracy 71.447%\n",
      "Epoch 11, Batch 397, LR 0.000086 Loss 8.438748, Accuracy 71.462%\n",
      "Epoch 11, Batch 398, LR 0.000086 Loss 8.437276, Accuracy 71.463%\n",
      "Epoch 11, Batch 399, LR 0.000086 Loss 8.436952, Accuracy 71.458%\n",
      "Epoch 11, Batch 400, LR 0.000086 Loss 8.435895, Accuracy 71.459%\n",
      "Epoch 11, Batch 401, LR 0.000086 Loss 8.435848, Accuracy 71.462%\n",
      "Epoch 11, Batch 402, LR 0.000086 Loss 8.436142, Accuracy 71.457%\n",
      "Epoch 11, Batch 403, LR 0.000086 Loss 8.436938, Accuracy 71.456%\n",
      "Epoch 11, Batch 404, LR 0.000086 Loss 8.438421, Accuracy 71.442%\n",
      "Epoch 11, Batch 405, LR 0.000086 Loss 8.436899, Accuracy 71.458%\n",
      "Epoch 11, Batch 406, LR 0.000086 Loss 8.439586, Accuracy 71.430%\n",
      "Epoch 11, Batch 407, LR 0.000086 Loss 8.439761, Accuracy 71.420%\n",
      "Epoch 11, Batch 408, LR 0.000086 Loss 8.440760, Accuracy 71.408%\n",
      "Epoch 11, Batch 409, LR 0.000086 Loss 8.438784, Accuracy 71.430%\n",
      "Epoch 11, Batch 410, LR 0.000086 Loss 8.438075, Accuracy 71.441%\n",
      "Epoch 11, Batch 411, LR 0.000086 Loss 8.439991, Accuracy 71.426%\n",
      "Epoch 11, Batch 412, LR 0.000086 Loss 8.440559, Accuracy 71.416%\n",
      "Epoch 11, Batch 413, LR 0.000086 Loss 8.441094, Accuracy 71.406%\n",
      "Epoch 11, Batch 414, LR 0.000086 Loss 8.441060, Accuracy 71.409%\n",
      "Epoch 11, Batch 415, LR 0.000086 Loss 8.441541, Accuracy 71.406%\n",
      "Epoch 11, Batch 416, LR 0.000086 Loss 8.438961, Accuracy 71.422%\n",
      "Epoch 11, Batch 417, LR 0.000086 Loss 8.438613, Accuracy 71.420%\n",
      "Epoch 11, Batch 418, LR 0.000086 Loss 8.438841, Accuracy 71.415%\n",
      "Epoch 11, Batch 419, LR 0.000086 Loss 8.438825, Accuracy 71.428%\n",
      "Epoch 11, Batch 420, LR 0.000086 Loss 8.440291, Accuracy 71.416%\n",
      "Epoch 11, Batch 421, LR 0.000086 Loss 8.441378, Accuracy 71.406%\n",
      "Epoch 11, Batch 422, LR 0.000086 Loss 8.441241, Accuracy 71.408%\n",
      "Epoch 11, Batch 423, LR 0.000086 Loss 8.440764, Accuracy 71.413%\n",
      "Epoch 11, Batch 424, LR 0.000086 Loss 8.440908, Accuracy 71.403%\n",
      "Epoch 11, Batch 425, LR 0.000086 Loss 8.439249, Accuracy 71.419%\n",
      "Epoch 11, Batch 426, LR 0.000086 Loss 8.441351, Accuracy 71.402%\n",
      "Epoch 11, Batch 427, LR 0.000086 Loss 8.441271, Accuracy 71.397%\n",
      "Epoch 11, Batch 428, LR 0.000086 Loss 8.440599, Accuracy 71.400%\n",
      "Epoch 11, Batch 429, LR 0.000086 Loss 8.440789, Accuracy 71.407%\n",
      "Epoch 11, Batch 430, LR 0.000086 Loss 8.441771, Accuracy 71.404%\n",
      "Epoch 11, Batch 431, LR 0.000086 Loss 8.442231, Accuracy 71.396%\n",
      "Epoch 11, Batch 432, LR 0.000086 Loss 8.443185, Accuracy 71.392%\n",
      "Epoch 11, Batch 433, LR 0.000086 Loss 8.442450, Accuracy 71.402%\n",
      "Epoch 11, Batch 434, LR 0.000086 Loss 8.442217, Accuracy 71.398%\n",
      "Epoch 11, Batch 435, LR 0.000086 Loss 8.441095, Accuracy 71.419%\n",
      "Epoch 11, Batch 436, LR 0.000086 Loss 8.441013, Accuracy 71.411%\n",
      "Epoch 11, Batch 437, LR 0.000086 Loss 8.439519, Accuracy 71.428%\n",
      "Epoch 11, Batch 438, LR 0.000086 Loss 8.437754, Accuracy 71.452%\n",
      "Epoch 11, Batch 439, LR 0.000086 Loss 8.436056, Accuracy 71.457%\n",
      "Epoch 11, Batch 440, LR 0.000086 Loss 8.436963, Accuracy 71.452%\n",
      "Epoch 11, Batch 441, LR 0.000086 Loss 8.435484, Accuracy 71.452%\n",
      "Epoch 11, Batch 442, LR 0.000086 Loss 8.435991, Accuracy 71.454%\n",
      "Epoch 11, Batch 443, LR 0.000086 Loss 8.436155, Accuracy 71.459%\n",
      "Epoch 11, Batch 444, LR 0.000086 Loss 8.438182, Accuracy 71.444%\n",
      "Epoch 11, Batch 445, LR 0.000086 Loss 8.438409, Accuracy 71.447%\n",
      "Epoch 11, Batch 446, LR 0.000086 Loss 8.438656, Accuracy 71.439%\n",
      "Epoch 11, Batch 447, LR 0.000086 Loss 8.439765, Accuracy 71.433%\n",
      "Epoch 11, Batch 448, LR 0.000086 Loss 8.438520, Accuracy 71.446%\n",
      "Epoch 11, Batch 449, LR 0.000086 Loss 8.437969, Accuracy 71.450%\n",
      "Epoch 11, Batch 450, LR 0.000086 Loss 8.439988, Accuracy 71.434%\n",
      "Epoch 11, Batch 451, LR 0.000086 Loss 8.438419, Accuracy 71.440%\n",
      "Epoch 11, Batch 452, LR 0.000086 Loss 8.437982, Accuracy 71.445%\n",
      "Epoch 11, Batch 453, LR 0.000086 Loss 8.436710, Accuracy 71.454%\n",
      "Epoch 11, Batch 454, LR 0.000086 Loss 8.434696, Accuracy 71.465%\n",
      "Epoch 11, Batch 455, LR 0.000086 Loss 8.433260, Accuracy 71.487%\n",
      "Epoch 11, Batch 456, LR 0.000086 Loss 8.431831, Accuracy 71.498%\n",
      "Epoch 11, Batch 457, LR 0.000086 Loss 8.432172, Accuracy 71.501%\n",
      "Epoch 11, Batch 458, LR 0.000086 Loss 8.432934, Accuracy 71.500%\n",
      "Epoch 11, Batch 459, LR 0.000086 Loss 8.433487, Accuracy 71.495%\n",
      "Epoch 11, Batch 460, LR 0.000086 Loss 8.433184, Accuracy 71.495%\n",
      "Epoch 11, Batch 461, LR 0.000086 Loss 8.432019, Accuracy 71.490%\n",
      "Epoch 11, Batch 462, LR 0.000086 Loss 8.432870, Accuracy 71.488%\n",
      "Epoch 11, Batch 463, LR 0.000086 Loss 8.432930, Accuracy 71.482%\n",
      "Epoch 11, Batch 464, LR 0.000086 Loss 8.430329, Accuracy 71.498%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 465, LR 0.000086 Loss 8.429403, Accuracy 71.507%\n",
      "Epoch 11, Batch 466, LR 0.000086 Loss 8.430998, Accuracy 71.496%\n",
      "Epoch 11, Batch 467, LR 0.000086 Loss 8.430550, Accuracy 71.507%\n",
      "Epoch 11, Batch 468, LR 0.000086 Loss 8.430030, Accuracy 71.509%\n",
      "Epoch 11, Batch 469, LR 0.000086 Loss 8.429386, Accuracy 71.505%\n",
      "Epoch 11, Batch 470, LR 0.000086 Loss 8.429935, Accuracy 71.514%\n",
      "Epoch 11, Batch 471, LR 0.000086 Loss 8.430471, Accuracy 71.518%\n",
      "Epoch 11, Batch 472, LR 0.000085 Loss 8.432090, Accuracy 71.517%\n",
      "Epoch 11, Batch 473, LR 0.000085 Loss 8.431970, Accuracy 71.518%\n",
      "Epoch 11, Batch 474, LR 0.000085 Loss 8.429100, Accuracy 71.534%\n",
      "Epoch 11, Batch 475, LR 0.000085 Loss 8.427368, Accuracy 71.548%\n",
      "Epoch 11, Batch 476, LR 0.000085 Loss 8.427470, Accuracy 71.548%\n",
      "Epoch 11, Batch 477, LR 0.000085 Loss 8.426564, Accuracy 71.549%\n",
      "Epoch 11, Batch 478, LR 0.000085 Loss 8.425369, Accuracy 71.555%\n",
      "Epoch 11, Batch 479, LR 0.000085 Loss 8.423089, Accuracy 71.568%\n",
      "Epoch 11, Batch 480, LR 0.000085 Loss 8.420713, Accuracy 71.582%\n",
      "Epoch 11, Batch 481, LR 0.000085 Loss 8.420258, Accuracy 71.591%\n",
      "Epoch 11, Batch 482, LR 0.000085 Loss 8.420419, Accuracy 71.596%\n",
      "Epoch 11, Batch 483, LR 0.000085 Loss 8.419482, Accuracy 71.600%\n",
      "Epoch 11, Batch 484, LR 0.000085 Loss 8.420150, Accuracy 71.594%\n",
      "Epoch 11, Batch 485, LR 0.000085 Loss 8.421402, Accuracy 71.591%\n",
      "Epoch 11, Batch 486, LR 0.000085 Loss 8.421818, Accuracy 71.590%\n",
      "Epoch 11, Batch 487, LR 0.000085 Loss 8.422773, Accuracy 71.580%\n",
      "Epoch 11, Batch 488, LR 0.000085 Loss 8.423194, Accuracy 71.572%\n",
      "Epoch 11, Batch 489, LR 0.000085 Loss 8.422119, Accuracy 71.575%\n",
      "Epoch 11, Batch 490, LR 0.000085 Loss 8.421702, Accuracy 71.578%\n",
      "Epoch 11, Batch 491, LR 0.000085 Loss 8.421429, Accuracy 71.585%\n",
      "Epoch 11, Batch 492, LR 0.000085 Loss 8.422573, Accuracy 71.580%\n",
      "Epoch 11, Batch 493, LR 0.000085 Loss 8.423712, Accuracy 71.574%\n",
      "Epoch 11, Batch 494, LR 0.000085 Loss 8.424529, Accuracy 71.560%\n",
      "Epoch 11, Batch 495, LR 0.000085 Loss 8.423659, Accuracy 71.566%\n",
      "Epoch 11, Batch 496, LR 0.000085 Loss 8.423758, Accuracy 71.573%\n",
      "Epoch 11, Batch 497, LR 0.000085 Loss 8.423287, Accuracy 71.578%\n",
      "Epoch 11, Batch 498, LR 0.000085 Loss 8.424256, Accuracy 71.572%\n",
      "Epoch 11, Batch 499, LR 0.000085 Loss 8.424081, Accuracy 71.581%\n",
      "Epoch 11, Batch 500, LR 0.000085 Loss 8.423713, Accuracy 71.586%\n",
      "Epoch 11, Batch 501, LR 0.000085 Loss 8.424671, Accuracy 71.572%\n",
      "Epoch 11, Batch 502, LR 0.000085 Loss 8.422924, Accuracy 71.587%\n",
      "Epoch 11, Batch 503, LR 0.000085 Loss 8.424318, Accuracy 71.578%\n",
      "Epoch 11, Batch 504, LR 0.000085 Loss 8.422412, Accuracy 71.591%\n",
      "Epoch 11, Batch 505, LR 0.000085 Loss 8.422410, Accuracy 71.597%\n",
      "Epoch 11, Batch 506, LR 0.000085 Loss 8.421958, Accuracy 71.605%\n",
      "Epoch 11, Batch 507, LR 0.000085 Loss 8.422588, Accuracy 71.599%\n",
      "Epoch 11, Batch 508, LR 0.000085 Loss 8.422543, Accuracy 71.598%\n",
      "Epoch 11, Batch 509, LR 0.000085 Loss 8.423434, Accuracy 71.591%\n",
      "Epoch 11, Batch 510, LR 0.000085 Loss 8.424120, Accuracy 71.579%\n",
      "Epoch 11, Batch 511, LR 0.000085 Loss 8.423919, Accuracy 71.585%\n",
      "Epoch 11, Batch 512, LR 0.000085 Loss 8.424012, Accuracy 71.591%\n",
      "Epoch 11, Batch 513, LR 0.000085 Loss 8.423269, Accuracy 71.599%\n",
      "Epoch 11, Batch 514, LR 0.000085 Loss 8.423819, Accuracy 71.595%\n",
      "Epoch 11, Batch 515, LR 0.000085 Loss 8.423023, Accuracy 71.599%\n",
      "Epoch 11, Batch 516, LR 0.000085 Loss 8.422639, Accuracy 71.609%\n",
      "Epoch 11, Batch 517, LR 0.000085 Loss 8.424569, Accuracy 71.592%\n",
      "Epoch 11, Batch 518, LR 0.000085 Loss 8.426137, Accuracy 71.576%\n",
      "Epoch 11, Batch 519, LR 0.000085 Loss 8.425848, Accuracy 71.575%\n",
      "Epoch 11, Batch 520, LR 0.000085 Loss 8.424238, Accuracy 71.581%\n",
      "Epoch 11, Batch 521, LR 0.000085 Loss 8.424740, Accuracy 71.577%\n",
      "Epoch 11, Batch 522, LR 0.000085 Loss 8.424197, Accuracy 71.579%\n",
      "Epoch 11, Batch 523, LR 0.000085 Loss 8.425122, Accuracy 71.579%\n",
      "Epoch 11, Batch 524, LR 0.000085 Loss 8.425386, Accuracy 71.575%\n",
      "Epoch 11, Batch 525, LR 0.000085 Loss 8.424461, Accuracy 71.580%\n",
      "Epoch 11, Batch 526, LR 0.000085 Loss 8.423733, Accuracy 71.590%\n",
      "Epoch 11, Batch 527, LR 0.000085 Loss 8.424861, Accuracy 71.579%\n",
      "Epoch 11, Batch 528, LR 0.000085 Loss 8.425118, Accuracy 71.575%\n",
      "Epoch 11, Batch 529, LR 0.000085 Loss 8.424186, Accuracy 71.581%\n",
      "Epoch 11, Batch 530, LR 0.000085 Loss 8.423058, Accuracy 71.589%\n",
      "Epoch 11, Batch 531, LR 0.000085 Loss 8.421633, Accuracy 71.595%\n",
      "Epoch 11, Batch 532, LR 0.000085 Loss 8.421461, Accuracy 71.597%\n",
      "Epoch 11, Batch 533, LR 0.000085 Loss 8.421529, Accuracy 71.602%\n",
      "Epoch 11, Batch 534, LR 0.000085 Loss 8.421016, Accuracy 71.612%\n",
      "Epoch 11, Batch 535, LR 0.000085 Loss 8.419307, Accuracy 71.617%\n",
      "Epoch 11, Batch 536, LR 0.000085 Loss 8.418083, Accuracy 71.618%\n",
      "Epoch 11, Batch 537, LR 0.000085 Loss 8.418477, Accuracy 71.616%\n",
      "Epoch 11, Batch 538, LR 0.000085 Loss 8.418620, Accuracy 71.614%\n",
      "Epoch 11, Batch 539, LR 0.000085 Loss 8.420260, Accuracy 71.605%\n",
      "Epoch 11, Batch 540, LR 0.000085 Loss 8.420263, Accuracy 71.607%\n",
      "Epoch 11, Batch 541, LR 0.000085 Loss 8.420018, Accuracy 71.611%\n",
      "Epoch 11, Batch 542, LR 0.000085 Loss 8.420068, Accuracy 71.620%\n",
      "Epoch 11, Batch 543, LR 0.000085 Loss 8.420075, Accuracy 71.625%\n",
      "Epoch 11, Batch 544, LR 0.000085 Loss 8.420680, Accuracy 71.622%\n",
      "Epoch 11, Batch 545, LR 0.000085 Loss 8.421599, Accuracy 71.628%\n",
      "Epoch 11, Batch 546, LR 0.000085 Loss 8.421104, Accuracy 71.632%\n",
      "Epoch 11, Batch 547, LR 0.000085 Loss 8.421498, Accuracy 71.629%\n",
      "Epoch 11, Batch 548, LR 0.000085 Loss 8.421040, Accuracy 71.633%\n",
      "Epoch 11, Batch 549, LR 0.000085 Loss 8.420379, Accuracy 71.633%\n",
      "Epoch 11, Batch 550, LR 0.000085 Loss 8.419878, Accuracy 71.634%\n",
      "Epoch 11, Batch 551, LR 0.000085 Loss 8.420892, Accuracy 71.624%\n",
      "Epoch 11, Batch 552, LR 0.000085 Loss 8.421835, Accuracy 71.619%\n",
      "Epoch 11, Batch 553, LR 0.000085 Loss 8.422483, Accuracy 71.605%\n",
      "Epoch 11, Batch 554, LR 0.000085 Loss 8.422045, Accuracy 71.599%\n",
      "Epoch 11, Batch 555, LR 0.000085 Loss 8.421687, Accuracy 71.593%\n",
      "Epoch 11, Batch 556, LR 0.000085 Loss 8.419858, Accuracy 71.604%\n",
      "Epoch 11, Batch 557, LR 0.000085 Loss 8.420175, Accuracy 71.606%\n",
      "Epoch 11, Batch 558, LR 0.000085 Loss 8.419784, Accuracy 71.608%\n",
      "Epoch 11, Batch 559, LR 0.000085 Loss 8.420584, Accuracy 71.614%\n",
      "Epoch 11, Batch 560, LR 0.000085 Loss 8.420128, Accuracy 71.618%\n",
      "Epoch 11, Batch 561, LR 0.000085 Loss 8.420696, Accuracy 71.619%\n",
      "Epoch 11, Batch 562, LR 0.000085 Loss 8.422158, Accuracy 71.616%\n",
      "Epoch 11, Batch 563, LR 0.000085 Loss 8.421696, Accuracy 71.622%\n",
      "Epoch 11, Batch 564, LR 0.000085 Loss 8.422839, Accuracy 71.609%\n",
      "Epoch 11, Batch 565, LR 0.000085 Loss 8.423517, Accuracy 71.604%\n",
      "Epoch 11, Batch 566, LR 0.000085 Loss 8.422273, Accuracy 71.616%\n",
      "Epoch 11, Batch 567, LR 0.000085 Loss 8.423181, Accuracy 71.609%\n",
      "Epoch 11, Batch 568, LR 0.000085 Loss 8.423091, Accuracy 71.611%\n",
      "Epoch 11, Batch 569, LR 0.000085 Loss 8.422464, Accuracy 71.611%\n",
      "Epoch 11, Batch 570, LR 0.000085 Loss 8.422147, Accuracy 71.609%\n",
      "Epoch 11, Batch 571, LR 0.000085 Loss 8.423598, Accuracy 71.603%\n",
      "Epoch 11, Batch 572, LR 0.000085 Loss 8.423353, Accuracy 71.599%\n",
      "Epoch 11, Batch 573, LR 0.000085 Loss 8.421974, Accuracy 71.608%\n",
      "Epoch 11, Batch 574, LR 0.000085 Loss 8.422890, Accuracy 71.597%\n",
      "Epoch 11, Batch 575, LR 0.000085 Loss 8.424799, Accuracy 71.584%\n",
      "Epoch 11, Batch 576, LR 0.000085 Loss 8.424101, Accuracy 71.587%\n",
      "Epoch 11, Batch 577, LR 0.000085 Loss 8.422526, Accuracy 71.601%\n",
      "Epoch 11, Batch 578, LR 0.000085 Loss 8.421334, Accuracy 71.615%\n",
      "Epoch 11, Batch 579, LR 0.000085 Loss 8.420156, Accuracy 71.625%\n",
      "Epoch 11, Batch 580, LR 0.000085 Loss 8.420507, Accuracy 71.623%\n",
      "Epoch 11, Batch 581, LR 0.000085 Loss 8.420366, Accuracy 71.625%\n",
      "Epoch 11, Batch 582, LR 0.000085 Loss 8.421357, Accuracy 71.627%\n",
      "Epoch 11, Batch 583, LR 0.000085 Loss 8.421469, Accuracy 71.624%\n",
      "Epoch 11, Batch 584, LR 0.000085 Loss 8.420963, Accuracy 71.619%\n",
      "Epoch 11, Batch 585, LR 0.000085 Loss 8.421256, Accuracy 71.620%\n",
      "Epoch 11, Batch 586, LR 0.000085 Loss 8.421857, Accuracy 71.616%\n",
      "Epoch 11, Batch 587, LR 0.000085 Loss 8.422521, Accuracy 71.610%\n",
      "Epoch 11, Batch 588, LR 0.000085 Loss 8.422522, Accuracy 71.615%\n",
      "Epoch 11, Batch 589, LR 0.000085 Loss 8.423436, Accuracy 71.614%\n",
      "Epoch 11, Batch 590, LR 0.000085 Loss 8.422897, Accuracy 71.609%\n",
      "Epoch 11, Batch 591, LR 0.000085 Loss 8.422847, Accuracy 71.608%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 592, LR 0.000085 Loss 8.422486, Accuracy 71.606%\n",
      "Epoch 11, Batch 593, LR 0.000085 Loss 8.422037, Accuracy 71.606%\n",
      "Epoch 11, Batch 594, LR 0.000085 Loss 8.421732, Accuracy 71.611%\n",
      "Epoch 11, Batch 595, LR 0.000085 Loss 8.421158, Accuracy 71.619%\n",
      "Epoch 11, Batch 596, LR 0.000085 Loss 8.420551, Accuracy 71.623%\n",
      "Epoch 11, Batch 597, LR 0.000085 Loss 8.419453, Accuracy 71.639%\n",
      "Epoch 11, Batch 598, LR 0.000085 Loss 8.418356, Accuracy 71.650%\n",
      "Epoch 11, Batch 599, LR 0.000085 Loss 8.419398, Accuracy 71.643%\n",
      "Epoch 11, Batch 600, LR 0.000085 Loss 8.419812, Accuracy 71.634%\n",
      "Epoch 11, Batch 601, LR 0.000085 Loss 8.420631, Accuracy 71.633%\n",
      "Epoch 11, Batch 602, LR 0.000085 Loss 8.419774, Accuracy 71.634%\n",
      "Epoch 11, Batch 603, LR 0.000085 Loss 8.419727, Accuracy 71.625%\n",
      "Epoch 11, Batch 604, LR 0.000085 Loss 8.418689, Accuracy 71.632%\n",
      "Epoch 11, Batch 605, LR 0.000085 Loss 8.418710, Accuracy 71.636%\n",
      "Epoch 11, Batch 606, LR 0.000085 Loss 8.418244, Accuracy 71.643%\n",
      "Epoch 11, Batch 607, LR 0.000085 Loss 8.417370, Accuracy 71.646%\n",
      "Epoch 11, Batch 608, LR 0.000085 Loss 8.416143, Accuracy 71.655%\n",
      "Epoch 11, Batch 609, LR 0.000085 Loss 8.415737, Accuracy 71.656%\n",
      "Epoch 11, Batch 610, LR 0.000085 Loss 8.415677, Accuracy 71.652%\n",
      "Epoch 11, Batch 611, LR 0.000085 Loss 8.415986, Accuracy 71.647%\n",
      "Epoch 11, Batch 612, LR 0.000085 Loss 8.417695, Accuracy 71.639%\n",
      "Epoch 11, Batch 613, LR 0.000085 Loss 8.416220, Accuracy 71.648%\n",
      "Epoch 11, Batch 614, LR 0.000085 Loss 8.415808, Accuracy 71.655%\n",
      "Epoch 11, Batch 615, LR 0.000085 Loss 8.416173, Accuracy 71.657%\n",
      "Epoch 11, Batch 616, LR 0.000085 Loss 8.416621, Accuracy 71.653%\n",
      "Epoch 11, Batch 617, LR 0.000085 Loss 8.415632, Accuracy 71.661%\n",
      "Epoch 11, Batch 618, LR 0.000085 Loss 8.416018, Accuracy 71.654%\n",
      "Epoch 11, Batch 619, LR 0.000085 Loss 8.416379, Accuracy 71.652%\n",
      "Epoch 11, Batch 620, LR 0.000085 Loss 8.416062, Accuracy 71.656%\n",
      "Epoch 11, Batch 621, LR 0.000085 Loss 8.417650, Accuracy 71.644%\n",
      "Epoch 11, Batch 622, LR 0.000085 Loss 8.417399, Accuracy 71.645%\n",
      "Epoch 11, Batch 623, LR 0.000085 Loss 8.417222, Accuracy 71.642%\n",
      "Epoch 11, Batch 624, LR 0.000085 Loss 8.417156, Accuracy 71.641%\n",
      "Epoch 11, Batch 625, LR 0.000085 Loss 8.417064, Accuracy 71.645%\n",
      "Epoch 11, Batch 626, LR 0.000085 Loss 8.417174, Accuracy 71.648%\n",
      "Epoch 11, Batch 627, LR 0.000085 Loss 8.418347, Accuracy 71.637%\n",
      "Epoch 11, Batch 628, LR 0.000085 Loss 8.418028, Accuracy 71.644%\n",
      "Epoch 11, Batch 629, LR 0.000085 Loss 8.417704, Accuracy 71.639%\n",
      "Epoch 11, Batch 630, LR 0.000085 Loss 8.415828, Accuracy 71.649%\n",
      "Epoch 11, Batch 631, LR 0.000085 Loss 8.416956, Accuracy 71.641%\n",
      "Epoch 11, Batch 632, LR 0.000085 Loss 8.418251, Accuracy 71.629%\n",
      "Epoch 11, Batch 633, LR 0.000085 Loss 8.418958, Accuracy 71.623%\n",
      "Epoch 11, Batch 634, LR 0.000085 Loss 8.419900, Accuracy 71.613%\n",
      "Epoch 11, Batch 635, LR 0.000085 Loss 8.418811, Accuracy 71.618%\n",
      "Epoch 11, Batch 636, LR 0.000085 Loss 8.419196, Accuracy 71.617%\n",
      "Epoch 11, Batch 637, LR 0.000085 Loss 8.418963, Accuracy 71.624%\n",
      "Epoch 11, Batch 638, LR 0.000085 Loss 8.417974, Accuracy 71.623%\n",
      "Epoch 11, Batch 639, LR 0.000085 Loss 8.417692, Accuracy 71.635%\n",
      "Epoch 11, Batch 640, LR 0.000085 Loss 8.416359, Accuracy 71.642%\n",
      "Epoch 11, Batch 641, LR 0.000085 Loss 8.415663, Accuracy 71.648%\n",
      "Epoch 11, Batch 642, LR 0.000085 Loss 8.416310, Accuracy 71.646%\n",
      "Epoch 11, Batch 643, LR 0.000085 Loss 8.416646, Accuracy 71.638%\n",
      "Epoch 11, Batch 644, LR 0.000085 Loss 8.416927, Accuracy 71.641%\n",
      "Epoch 11, Batch 645, LR 0.000085 Loss 8.416539, Accuracy 71.639%\n",
      "Epoch 11, Batch 646, LR 0.000085 Loss 8.416175, Accuracy 71.637%\n",
      "Epoch 11, Batch 647, LR 0.000085 Loss 8.416661, Accuracy 71.634%\n",
      "Epoch 11, Batch 648, LR 0.000085 Loss 8.417221, Accuracy 71.633%\n",
      "Epoch 11, Batch 649, LR 0.000085 Loss 8.417439, Accuracy 71.639%\n",
      "Epoch 11, Batch 650, LR 0.000085 Loss 8.417888, Accuracy 71.636%\n",
      "Epoch 11, Batch 651, LR 0.000085 Loss 8.416374, Accuracy 71.641%\n",
      "Epoch 11, Batch 652, LR 0.000085 Loss 8.416060, Accuracy 71.646%\n",
      "Epoch 11, Batch 653, LR 0.000085 Loss 8.417433, Accuracy 71.638%\n",
      "Epoch 11, Batch 654, LR 0.000085 Loss 8.418057, Accuracy 71.630%\n",
      "Epoch 11, Batch 655, LR 0.000085 Loss 8.416596, Accuracy 71.647%\n",
      "Epoch 11, Batch 656, LR 0.000085 Loss 8.415343, Accuracy 71.653%\n",
      "Epoch 11, Batch 657, LR 0.000085 Loss 8.416145, Accuracy 71.654%\n",
      "Epoch 11, Batch 658, LR 0.000085 Loss 8.413897, Accuracy 71.661%\n",
      "Epoch 11, Batch 659, LR 0.000085 Loss 8.413839, Accuracy 71.660%\n",
      "Epoch 11, Batch 660, LR 0.000085 Loss 8.414238, Accuracy 71.651%\n",
      "Epoch 11, Batch 661, LR 0.000085 Loss 8.415510, Accuracy 71.643%\n",
      "Epoch 11, Batch 662, LR 0.000085 Loss 8.413527, Accuracy 71.655%\n",
      "Epoch 11, Batch 663, LR 0.000085 Loss 8.413046, Accuracy 71.652%\n",
      "Epoch 11, Batch 664, LR 0.000085 Loss 8.412611, Accuracy 71.650%\n",
      "Epoch 11, Batch 665, LR 0.000085 Loss 8.411196, Accuracy 71.654%\n",
      "Epoch 11, Batch 666, LR 0.000085 Loss 8.410593, Accuracy 71.656%\n",
      "Epoch 11, Batch 667, LR 0.000085 Loss 8.410846, Accuracy 71.661%\n",
      "Epoch 11, Batch 668, LR 0.000085 Loss 8.410307, Accuracy 71.663%\n",
      "Epoch 11, Batch 669, LR 0.000085 Loss 8.410590, Accuracy 71.658%\n",
      "Epoch 11, Batch 670, LR 0.000085 Loss 8.410524, Accuracy 71.653%\n",
      "Epoch 11, Batch 671, LR 0.000085 Loss 8.409945, Accuracy 71.657%\n",
      "Epoch 11, Batch 672, LR 0.000085 Loss 8.409062, Accuracy 71.663%\n",
      "Epoch 11, Batch 673, LR 0.000085 Loss 8.409198, Accuracy 71.663%\n",
      "Epoch 11, Batch 674, LR 0.000085 Loss 8.406724, Accuracy 71.674%\n",
      "Epoch 11, Batch 675, LR 0.000085 Loss 8.406724, Accuracy 71.675%\n",
      "Epoch 11, Batch 676, LR 0.000085 Loss 8.406451, Accuracy 71.672%\n",
      "Epoch 11, Batch 677, LR 0.000085 Loss 8.405719, Accuracy 71.680%\n",
      "Epoch 11, Batch 678, LR 0.000085 Loss 8.405191, Accuracy 71.681%\n",
      "Epoch 11, Batch 679, LR 0.000085 Loss 8.405285, Accuracy 71.681%\n",
      "Epoch 11, Batch 680, LR 0.000085 Loss 8.405084, Accuracy 71.684%\n",
      "Epoch 11, Batch 681, LR 0.000085 Loss 8.405072, Accuracy 71.694%\n",
      "Epoch 11, Batch 682, LR 0.000085 Loss 8.404521, Accuracy 71.699%\n",
      "Epoch 11, Batch 683, LR 0.000085 Loss 8.403589, Accuracy 71.706%\n",
      "Epoch 11, Batch 684, LR 0.000085 Loss 8.402422, Accuracy 71.705%\n",
      "Epoch 11, Batch 685, LR 0.000085 Loss 8.402398, Accuracy 71.701%\n",
      "Epoch 11, Batch 686, LR 0.000085 Loss 8.402981, Accuracy 71.701%\n",
      "Epoch 11, Batch 687, LR 0.000085 Loss 8.403598, Accuracy 71.699%\n",
      "Epoch 11, Batch 688, LR 0.000085 Loss 8.403755, Accuracy 71.699%\n",
      "Epoch 11, Batch 689, LR 0.000085 Loss 8.403958, Accuracy 71.700%\n",
      "Epoch 11, Batch 690, LR 0.000085 Loss 8.404242, Accuracy 71.700%\n",
      "Epoch 11, Batch 691, LR 0.000085 Loss 8.404747, Accuracy 71.694%\n",
      "Epoch 11, Batch 692, LR 0.000085 Loss 8.403907, Accuracy 71.700%\n",
      "Epoch 11, Batch 693, LR 0.000085 Loss 8.404572, Accuracy 71.701%\n",
      "Epoch 11, Batch 694, LR 0.000085 Loss 8.404953, Accuracy 71.703%\n",
      "Epoch 11, Batch 695, LR 0.000085 Loss 8.404746, Accuracy 71.700%\n",
      "Epoch 11, Batch 696, LR 0.000085 Loss 8.404999, Accuracy 71.692%\n",
      "Epoch 11, Batch 697, LR 0.000085 Loss 8.405470, Accuracy 71.692%\n",
      "Epoch 11, Batch 698, LR 0.000085 Loss 8.404838, Accuracy 71.699%\n",
      "Epoch 11, Batch 699, LR 0.000085 Loss 8.404910, Accuracy 71.702%\n",
      "Epoch 11, Batch 700, LR 0.000085 Loss 8.404421, Accuracy 71.704%\n",
      "Epoch 11, Batch 701, LR 0.000085 Loss 8.404849, Accuracy 71.701%\n",
      "Epoch 11, Batch 702, LR 0.000085 Loss 8.403799, Accuracy 71.705%\n",
      "Epoch 11, Batch 703, LR 0.000085 Loss 8.403676, Accuracy 71.703%\n",
      "Epoch 11, Batch 704, LR 0.000085 Loss 8.402889, Accuracy 71.711%\n",
      "Epoch 11, Batch 705, LR 0.000085 Loss 8.403445, Accuracy 71.704%\n",
      "Epoch 11, Batch 706, LR 0.000085 Loss 8.403409, Accuracy 71.708%\n",
      "Epoch 11, Batch 707, LR 0.000085 Loss 8.404019, Accuracy 71.706%\n",
      "Epoch 11, Batch 708, LR 0.000085 Loss 8.404396, Accuracy 71.697%\n",
      "Epoch 11, Batch 709, LR 0.000085 Loss 8.403989, Accuracy 71.700%\n",
      "Epoch 11, Batch 710, LR 0.000085 Loss 8.404141, Accuracy 71.699%\n",
      "Epoch 11, Batch 711, LR 0.000085 Loss 8.404303, Accuracy 71.695%\n",
      "Epoch 11, Batch 712, LR 0.000085 Loss 8.403757, Accuracy 71.696%\n",
      "Epoch 11, Batch 713, LR 0.000085 Loss 8.403950, Accuracy 71.691%\n",
      "Epoch 11, Batch 714, LR 0.000085 Loss 8.404293, Accuracy 71.691%\n",
      "Epoch 11, Batch 715, LR 0.000085 Loss 8.403358, Accuracy 71.695%\n",
      "Epoch 11, Batch 716, LR 0.000085 Loss 8.402794, Accuracy 71.703%\n",
      "Epoch 11, Batch 717, LR 0.000085 Loss 8.403043, Accuracy 71.706%\n",
      "Epoch 11, Batch 718, LR 0.000085 Loss 8.403099, Accuracy 71.703%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 719, LR 0.000085 Loss 8.402990, Accuracy 71.699%\n",
      "Epoch 11, Batch 720, LR 0.000085 Loss 8.403074, Accuracy 71.705%\n",
      "Epoch 11, Batch 721, LR 0.000085 Loss 8.402415, Accuracy 71.711%\n",
      "Epoch 11, Batch 722, LR 0.000085 Loss 8.401947, Accuracy 71.716%\n",
      "Epoch 11, Batch 723, LR 0.000085 Loss 8.402715, Accuracy 71.704%\n",
      "Epoch 11, Batch 724, LR 0.000085 Loss 8.402436, Accuracy 71.703%\n",
      "Epoch 11, Batch 725, LR 0.000085 Loss 8.403355, Accuracy 71.690%\n",
      "Epoch 11, Batch 726, LR 0.000085 Loss 8.403291, Accuracy 71.689%\n",
      "Epoch 11, Batch 727, LR 0.000085 Loss 8.404277, Accuracy 71.684%\n",
      "Epoch 11, Batch 728, LR 0.000085 Loss 8.404031, Accuracy 71.691%\n",
      "Epoch 11, Batch 729, LR 0.000085 Loss 8.404075, Accuracy 71.682%\n",
      "Epoch 11, Batch 730, LR 0.000085 Loss 8.404235, Accuracy 71.680%\n",
      "Epoch 11, Batch 731, LR 0.000085 Loss 8.404718, Accuracy 71.678%\n",
      "Epoch 11, Batch 732, LR 0.000085 Loss 8.405146, Accuracy 71.679%\n",
      "Epoch 11, Batch 733, LR 0.000085 Loss 8.404311, Accuracy 71.684%\n",
      "Epoch 11, Batch 734, LR 0.000085 Loss 8.404356, Accuracy 71.683%\n",
      "Epoch 11, Batch 735, LR 0.000085 Loss 8.403618, Accuracy 71.688%\n",
      "Epoch 11, Batch 736, LR 0.000085 Loss 8.403934, Accuracy 71.684%\n",
      "Epoch 11, Batch 737, LR 0.000085 Loss 8.403834, Accuracy 71.689%\n",
      "Epoch 11, Batch 738, LR 0.000085 Loss 8.405152, Accuracy 71.680%\n",
      "Epoch 11, Batch 739, LR 0.000085 Loss 8.404610, Accuracy 71.682%\n",
      "Epoch 11, Batch 740, LR 0.000085 Loss 8.403722, Accuracy 71.686%\n",
      "Epoch 11, Batch 741, LR 0.000085 Loss 8.403851, Accuracy 71.683%\n",
      "Epoch 11, Batch 742, LR 0.000085 Loss 8.404497, Accuracy 71.673%\n",
      "Epoch 11, Batch 743, LR 0.000085 Loss 8.403455, Accuracy 71.684%\n",
      "Epoch 11, Batch 744, LR 0.000085 Loss 8.403319, Accuracy 71.681%\n",
      "Epoch 11, Batch 745, LR 0.000085 Loss 8.403657, Accuracy 71.684%\n",
      "Epoch 11, Batch 746, LR 0.000085 Loss 8.404304, Accuracy 71.674%\n",
      "Epoch 11, Batch 747, LR 0.000085 Loss 8.404246, Accuracy 71.675%\n",
      "Epoch 11, Batch 748, LR 0.000085 Loss 8.403586, Accuracy 71.679%\n",
      "Epoch 11, Batch 749, LR 0.000085 Loss 8.403496, Accuracy 71.686%\n",
      "Epoch 11, Batch 750, LR 0.000085 Loss 8.403141, Accuracy 71.699%\n",
      "Epoch 11, Batch 751, LR 0.000085 Loss 8.403863, Accuracy 71.698%\n",
      "Epoch 11, Batch 752, LR 0.000085 Loss 8.402990, Accuracy 71.706%\n",
      "Epoch 11, Batch 753, LR 0.000085 Loss 8.402077, Accuracy 71.711%\n",
      "Epoch 11, Batch 754, LR 0.000085 Loss 8.401536, Accuracy 71.721%\n",
      "Epoch 11, Batch 755, LR 0.000085 Loss 8.400221, Accuracy 71.729%\n",
      "Epoch 11, Batch 756, LR 0.000085 Loss 8.400728, Accuracy 71.726%\n",
      "Epoch 11, Batch 757, LR 0.000085 Loss 8.400452, Accuracy 71.728%\n",
      "Epoch 11, Batch 758, LR 0.000085 Loss 8.400287, Accuracy 71.735%\n",
      "Epoch 11, Batch 759, LR 0.000085 Loss 8.400598, Accuracy 71.734%\n",
      "Epoch 11, Batch 760, LR 0.000085 Loss 8.399460, Accuracy 71.739%\n",
      "Epoch 11, Batch 761, LR 0.000085 Loss 8.399641, Accuracy 71.739%\n",
      "Epoch 11, Batch 762, LR 0.000085 Loss 8.399405, Accuracy 71.740%\n",
      "Epoch 11, Batch 763, LR 0.000085 Loss 8.398901, Accuracy 71.744%\n",
      "Epoch 11, Batch 764, LR 0.000085 Loss 8.399514, Accuracy 71.741%\n",
      "Epoch 11, Batch 765, LR 0.000085 Loss 8.399490, Accuracy 71.741%\n",
      "Epoch 11, Batch 766, LR 0.000085 Loss 8.399925, Accuracy 71.735%\n",
      "Epoch 11, Batch 767, LR 0.000085 Loss 8.398997, Accuracy 71.737%\n",
      "Epoch 11, Batch 768, LR 0.000085 Loss 8.399033, Accuracy 71.739%\n",
      "Epoch 11, Batch 769, LR 0.000085 Loss 8.398682, Accuracy 71.745%\n",
      "Epoch 11, Batch 770, LR 0.000085 Loss 8.398189, Accuracy 71.747%\n",
      "Epoch 11, Batch 771, LR 0.000085 Loss 8.398778, Accuracy 71.738%\n",
      "Epoch 11, Batch 772, LR 0.000085 Loss 8.398621, Accuracy 71.736%\n",
      "Epoch 11, Batch 773, LR 0.000085 Loss 8.398054, Accuracy 71.744%\n",
      "Epoch 11, Batch 774, LR 0.000085 Loss 8.398034, Accuracy 71.739%\n",
      "Epoch 11, Batch 775, LR 0.000085 Loss 8.397844, Accuracy 71.741%\n",
      "Epoch 11, Batch 776, LR 0.000085 Loss 8.398388, Accuracy 71.736%\n",
      "Epoch 11, Batch 777, LR 0.000085 Loss 8.397916, Accuracy 71.734%\n",
      "Epoch 11, Batch 778, LR 0.000085 Loss 8.397776, Accuracy 71.736%\n",
      "Epoch 11, Batch 779, LR 0.000085 Loss 8.396842, Accuracy 71.736%\n",
      "Epoch 11, Batch 780, LR 0.000085 Loss 8.397002, Accuracy 71.732%\n",
      "Epoch 11, Batch 781, LR 0.000085 Loss 8.397947, Accuracy 71.728%\n",
      "Epoch 11, Batch 782, LR 0.000085 Loss 8.397079, Accuracy 71.735%\n",
      "Epoch 11, Batch 783, LR 0.000085 Loss 8.397388, Accuracy 71.732%\n",
      "Epoch 11, Batch 784, LR 0.000085 Loss 8.397323, Accuracy 71.732%\n",
      "Epoch 11, Batch 785, LR 0.000085 Loss 8.396505, Accuracy 71.728%\n",
      "Epoch 11, Batch 786, LR 0.000085 Loss 8.397486, Accuracy 71.721%\n",
      "Epoch 11, Batch 787, LR 0.000085 Loss 8.397977, Accuracy 71.713%\n",
      "Epoch 11, Batch 788, LR 0.000085 Loss 8.397967, Accuracy 71.715%\n",
      "Epoch 11, Batch 789, LR 0.000085 Loss 8.396780, Accuracy 71.723%\n",
      "Epoch 11, Batch 790, LR 0.000085 Loss 8.396279, Accuracy 71.730%\n",
      "Epoch 11, Batch 791, LR 0.000085 Loss 8.395276, Accuracy 71.731%\n",
      "Epoch 11, Batch 792, LR 0.000085 Loss 8.394367, Accuracy 71.731%\n",
      "Epoch 11, Batch 793, LR 0.000085 Loss 8.394374, Accuracy 71.726%\n",
      "Epoch 11, Batch 794, LR 0.000085 Loss 8.394449, Accuracy 71.725%\n",
      "Epoch 11, Batch 795, LR 0.000085 Loss 8.395063, Accuracy 71.720%\n",
      "Epoch 11, Batch 796, LR 0.000085 Loss 8.395516, Accuracy 71.716%\n",
      "Epoch 11, Batch 797, LR 0.000085 Loss 8.394785, Accuracy 71.720%\n",
      "Epoch 11, Batch 798, LR 0.000085 Loss 8.395083, Accuracy 71.720%\n",
      "Epoch 11, Batch 799, LR 0.000085 Loss 8.395400, Accuracy 71.723%\n",
      "Epoch 11, Batch 800, LR 0.000085 Loss 8.395498, Accuracy 71.722%\n",
      "Epoch 11, Batch 801, LR 0.000085 Loss 8.395728, Accuracy 71.719%\n",
      "Epoch 11, Batch 802, LR 0.000085 Loss 8.395992, Accuracy 71.719%\n",
      "Epoch 11, Batch 803, LR 0.000085 Loss 8.395186, Accuracy 71.724%\n",
      "Epoch 11, Batch 804, LR 0.000085 Loss 8.394266, Accuracy 71.725%\n",
      "Epoch 11, Batch 805, LR 0.000085 Loss 8.393204, Accuracy 71.739%\n",
      "Epoch 11, Batch 806, LR 0.000085 Loss 8.393818, Accuracy 71.734%\n",
      "Epoch 11, Batch 807, LR 0.000085 Loss 8.393796, Accuracy 71.738%\n",
      "Epoch 11, Batch 808, LR 0.000085 Loss 8.393621, Accuracy 71.742%\n",
      "Epoch 11, Batch 809, LR 0.000085 Loss 8.393650, Accuracy 71.748%\n",
      "Epoch 11, Batch 810, LR 0.000085 Loss 8.393551, Accuracy 71.745%\n",
      "Epoch 11, Batch 811, LR 0.000085 Loss 8.394638, Accuracy 71.734%\n",
      "Epoch 11, Batch 812, LR 0.000085 Loss 8.394229, Accuracy 71.734%\n",
      "Epoch 11, Batch 813, LR 0.000085 Loss 8.393823, Accuracy 71.737%\n",
      "Epoch 11, Batch 814, LR 0.000085 Loss 8.393211, Accuracy 71.738%\n",
      "Epoch 11, Batch 815, LR 0.000085 Loss 8.391961, Accuracy 71.748%\n",
      "Epoch 11, Batch 816, LR 0.000085 Loss 8.391304, Accuracy 71.756%\n",
      "Epoch 11, Batch 817, LR 0.000085 Loss 8.390520, Accuracy 71.759%\n",
      "Epoch 11, Batch 818, LR 0.000085 Loss 8.391630, Accuracy 71.751%\n",
      "Epoch 11, Batch 819, LR 0.000085 Loss 8.391149, Accuracy 71.751%\n",
      "Epoch 11, Batch 820, LR 0.000085 Loss 8.391742, Accuracy 71.744%\n",
      "Epoch 11, Batch 821, LR 0.000085 Loss 8.391210, Accuracy 71.747%\n",
      "Epoch 11, Batch 822, LR 0.000085 Loss 8.391814, Accuracy 71.747%\n",
      "Epoch 11, Batch 823, LR 0.000085 Loss 8.392711, Accuracy 71.739%\n",
      "Epoch 11, Batch 824, LR 0.000085 Loss 8.391954, Accuracy 71.740%\n",
      "Epoch 11, Batch 825, LR 0.000085 Loss 8.392118, Accuracy 71.741%\n",
      "Epoch 11, Batch 826, LR 0.000085 Loss 8.392980, Accuracy 71.733%\n",
      "Epoch 11, Batch 827, LR 0.000084 Loss 8.393227, Accuracy 71.730%\n",
      "Epoch 11, Batch 828, LR 0.000084 Loss 8.392620, Accuracy 71.735%\n",
      "Epoch 11, Batch 829, LR 0.000084 Loss 8.391456, Accuracy 71.742%\n",
      "Epoch 11, Batch 830, LR 0.000084 Loss 8.391126, Accuracy 71.745%\n",
      "Epoch 11, Batch 831, LR 0.000084 Loss 8.390365, Accuracy 71.749%\n",
      "Epoch 11, Batch 832, LR 0.000084 Loss 8.391358, Accuracy 71.740%\n",
      "Epoch 11, Batch 833, LR 0.000084 Loss 8.391700, Accuracy 71.744%\n",
      "Epoch 11, Batch 834, LR 0.000084 Loss 8.392275, Accuracy 71.743%\n",
      "Epoch 11, Batch 835, LR 0.000084 Loss 8.392132, Accuracy 71.737%\n",
      "Epoch 11, Batch 836, LR 0.000084 Loss 8.391420, Accuracy 71.740%\n",
      "Epoch 11, Batch 837, LR 0.000084 Loss 8.390861, Accuracy 71.743%\n",
      "Epoch 11, Batch 838, LR 0.000084 Loss 8.390447, Accuracy 71.740%\n",
      "Epoch 11, Batch 839, LR 0.000084 Loss 8.391146, Accuracy 71.735%\n",
      "Epoch 11, Batch 840, LR 0.000084 Loss 8.391261, Accuracy 71.735%\n",
      "Epoch 11, Batch 841, LR 0.000084 Loss 8.391500, Accuracy 71.730%\n",
      "Epoch 11, Batch 842, LR 0.000084 Loss 8.391525, Accuracy 71.735%\n",
      "Epoch 11, Batch 843, LR 0.000084 Loss 8.391480, Accuracy 71.732%\n",
      "Epoch 11, Batch 844, LR 0.000084 Loss 8.391710, Accuracy 71.736%\n",
      "Epoch 11, Batch 845, LR 0.000084 Loss 8.390678, Accuracy 71.743%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 846, LR 0.000084 Loss 8.389987, Accuracy 71.743%\n",
      "Epoch 11, Batch 847, LR 0.000084 Loss 8.389480, Accuracy 71.745%\n",
      "Epoch 11, Batch 848, LR 0.000084 Loss 8.388301, Accuracy 71.752%\n",
      "Epoch 11, Batch 849, LR 0.000084 Loss 8.387521, Accuracy 71.760%\n",
      "Epoch 11, Batch 850, LR 0.000084 Loss 8.387306, Accuracy 71.763%\n",
      "Epoch 11, Batch 851, LR 0.000084 Loss 8.386991, Accuracy 71.759%\n",
      "Epoch 11, Batch 852, LR 0.000084 Loss 8.386593, Accuracy 71.765%\n",
      "Epoch 11, Batch 853, LR 0.000084 Loss 8.385623, Accuracy 71.772%\n",
      "Epoch 11, Batch 854, LR 0.000084 Loss 8.385212, Accuracy 71.776%\n",
      "Epoch 11, Batch 855, LR 0.000084 Loss 8.385424, Accuracy 71.778%\n",
      "Epoch 11, Batch 856, LR 0.000084 Loss 8.384910, Accuracy 71.777%\n",
      "Epoch 11, Batch 857, LR 0.000084 Loss 8.385093, Accuracy 71.776%\n",
      "Epoch 11, Batch 858, LR 0.000084 Loss 8.384855, Accuracy 71.770%\n",
      "Epoch 11, Batch 859, LR 0.000084 Loss 8.384624, Accuracy 71.766%\n",
      "Epoch 11, Batch 860, LR 0.000084 Loss 8.384091, Accuracy 71.765%\n",
      "Epoch 11, Batch 861, LR 0.000084 Loss 8.383116, Accuracy 71.770%\n",
      "Epoch 11, Batch 862, LR 0.000084 Loss 8.383415, Accuracy 71.766%\n",
      "Epoch 11, Batch 863, LR 0.000084 Loss 8.382914, Accuracy 71.772%\n",
      "Epoch 11, Batch 864, LR 0.000084 Loss 8.382693, Accuracy 71.774%\n",
      "Epoch 11, Batch 865, LR 0.000084 Loss 8.381525, Accuracy 71.783%\n",
      "Epoch 11, Batch 866, LR 0.000084 Loss 8.381618, Accuracy 71.782%\n",
      "Epoch 11, Batch 867, LR 0.000084 Loss 8.381334, Accuracy 71.783%\n",
      "Epoch 11, Batch 868, LR 0.000084 Loss 8.381427, Accuracy 71.783%\n",
      "Epoch 11, Batch 869, LR 0.000084 Loss 8.381886, Accuracy 71.780%\n",
      "Epoch 11, Batch 870, LR 0.000084 Loss 8.381107, Accuracy 71.784%\n",
      "Epoch 11, Batch 871, LR 0.000084 Loss 8.380301, Accuracy 71.785%\n",
      "Epoch 11, Batch 872, LR 0.000084 Loss 8.380358, Accuracy 71.789%\n",
      "Epoch 11, Batch 873, LR 0.000084 Loss 8.378967, Accuracy 71.799%\n",
      "Epoch 11, Batch 874, LR 0.000084 Loss 8.377481, Accuracy 71.803%\n",
      "Epoch 11, Batch 875, LR 0.000084 Loss 8.377185, Accuracy 71.802%\n",
      "Epoch 11, Batch 876, LR 0.000084 Loss 8.377524, Accuracy 71.799%\n",
      "Epoch 11, Batch 877, LR 0.000084 Loss 8.376966, Accuracy 71.806%\n",
      "Epoch 11, Batch 878, LR 0.000084 Loss 8.376852, Accuracy 71.807%\n",
      "Epoch 11, Batch 879, LR 0.000084 Loss 8.376782, Accuracy 71.807%\n",
      "Epoch 11, Batch 880, LR 0.000084 Loss 8.375947, Accuracy 71.812%\n",
      "Epoch 11, Batch 881, LR 0.000084 Loss 8.375377, Accuracy 71.819%\n",
      "Epoch 11, Batch 882, LR 0.000084 Loss 8.375458, Accuracy 71.818%\n",
      "Epoch 11, Batch 883, LR 0.000084 Loss 8.374770, Accuracy 71.823%\n",
      "Epoch 11, Batch 884, LR 0.000084 Loss 8.374164, Accuracy 71.828%\n",
      "Epoch 11, Batch 885, LR 0.000084 Loss 8.374395, Accuracy 71.827%\n",
      "Epoch 11, Batch 886, LR 0.000084 Loss 8.373792, Accuracy 71.836%\n",
      "Epoch 11, Batch 887, LR 0.000084 Loss 8.373903, Accuracy 71.833%\n",
      "Epoch 11, Batch 888, LR 0.000084 Loss 8.373406, Accuracy 71.832%\n",
      "Epoch 11, Batch 889, LR 0.000084 Loss 8.373828, Accuracy 71.833%\n",
      "Epoch 11, Batch 890, LR 0.000084 Loss 8.373303, Accuracy 71.832%\n",
      "Epoch 11, Batch 891, LR 0.000084 Loss 8.372234, Accuracy 71.840%\n",
      "Epoch 11, Batch 892, LR 0.000084 Loss 8.372896, Accuracy 71.835%\n",
      "Epoch 11, Batch 893, LR 0.000084 Loss 8.372624, Accuracy 71.837%\n",
      "Epoch 11, Batch 894, LR 0.000084 Loss 8.371917, Accuracy 71.844%\n",
      "Epoch 11, Batch 895, LR 0.000084 Loss 8.371611, Accuracy 71.846%\n",
      "Epoch 11, Batch 896, LR 0.000084 Loss 8.371898, Accuracy 71.848%\n",
      "Epoch 11, Batch 897, LR 0.000084 Loss 8.371877, Accuracy 71.850%\n",
      "Epoch 11, Batch 898, LR 0.000084 Loss 8.371116, Accuracy 71.855%\n",
      "Epoch 11, Batch 899, LR 0.000084 Loss 8.369928, Accuracy 71.858%\n",
      "Epoch 11, Batch 900, LR 0.000084 Loss 8.370336, Accuracy 71.855%\n",
      "Epoch 11, Batch 901, LR 0.000084 Loss 8.369188, Accuracy 71.863%\n",
      "Epoch 11, Batch 902, LR 0.000084 Loss 8.369105, Accuracy 71.861%\n",
      "Epoch 11, Batch 903, LR 0.000084 Loss 8.370166, Accuracy 71.856%\n",
      "Epoch 11, Batch 904, LR 0.000084 Loss 8.369885, Accuracy 71.859%\n",
      "Epoch 11, Batch 905, LR 0.000084 Loss 8.369705, Accuracy 71.858%\n",
      "Epoch 11, Batch 906, LR 0.000084 Loss 8.370302, Accuracy 71.859%\n",
      "Epoch 11, Batch 907, LR 0.000084 Loss 8.370342, Accuracy 71.863%\n",
      "Epoch 11, Batch 908, LR 0.000084 Loss 8.369981, Accuracy 71.866%\n",
      "Epoch 11, Batch 909, LR 0.000084 Loss 8.369670, Accuracy 71.869%\n",
      "Epoch 11, Batch 910, LR 0.000084 Loss 8.369072, Accuracy 71.875%\n",
      "Epoch 11, Batch 911, LR 0.000084 Loss 8.367327, Accuracy 71.884%\n",
      "Epoch 11, Batch 912, LR 0.000084 Loss 8.367595, Accuracy 71.884%\n",
      "Epoch 11, Batch 913, LR 0.000084 Loss 8.366764, Accuracy 71.885%\n",
      "Epoch 11, Batch 914, LR 0.000084 Loss 8.366993, Accuracy 71.885%\n",
      "Epoch 11, Batch 915, LR 0.000084 Loss 8.367430, Accuracy 71.887%\n",
      "Epoch 11, Batch 916, LR 0.000084 Loss 8.367869, Accuracy 71.879%\n",
      "Epoch 11, Batch 917, LR 0.000084 Loss 8.368149, Accuracy 71.878%\n",
      "Epoch 11, Batch 918, LR 0.000084 Loss 8.367762, Accuracy 71.880%\n",
      "Epoch 11, Batch 919, LR 0.000084 Loss 8.367667, Accuracy 71.877%\n",
      "Epoch 11, Batch 920, LR 0.000084 Loss 8.367724, Accuracy 71.878%\n",
      "Epoch 11, Batch 921, LR 0.000084 Loss 8.367653, Accuracy 71.874%\n",
      "Epoch 11, Batch 922, LR 0.000084 Loss 8.367423, Accuracy 71.875%\n",
      "Epoch 11, Batch 923, LR 0.000084 Loss 8.367145, Accuracy 71.872%\n",
      "Epoch 11, Batch 924, LR 0.000084 Loss 8.367488, Accuracy 71.867%\n",
      "Epoch 11, Batch 925, LR 0.000084 Loss 8.367562, Accuracy 71.863%\n",
      "Epoch 11, Batch 926, LR 0.000084 Loss 8.367233, Accuracy 71.862%\n",
      "Epoch 11, Batch 927, LR 0.000084 Loss 8.366415, Accuracy 71.867%\n",
      "Epoch 11, Batch 928, LR 0.000084 Loss 8.365907, Accuracy 71.872%\n",
      "Epoch 11, Batch 929, LR 0.000084 Loss 8.365608, Accuracy 71.874%\n",
      "Epoch 11, Batch 930, LR 0.000084 Loss 8.366085, Accuracy 71.873%\n",
      "Epoch 11, Batch 931, LR 0.000084 Loss 8.366124, Accuracy 71.873%\n",
      "Epoch 11, Batch 932, LR 0.000084 Loss 8.366507, Accuracy 71.872%\n",
      "Epoch 11, Batch 933, LR 0.000084 Loss 8.366838, Accuracy 71.875%\n",
      "Epoch 11, Batch 934, LR 0.000084 Loss 8.366299, Accuracy 71.874%\n",
      "Epoch 11, Batch 935, LR 0.000084 Loss 8.365937, Accuracy 71.872%\n",
      "Epoch 11, Batch 936, LR 0.000084 Loss 8.365588, Accuracy 71.874%\n",
      "Epoch 11, Batch 937, LR 0.000084 Loss 8.364994, Accuracy 71.882%\n",
      "Epoch 11, Batch 938, LR 0.000084 Loss 8.365138, Accuracy 71.879%\n",
      "Epoch 11, Batch 939, LR 0.000084 Loss 8.364393, Accuracy 71.881%\n",
      "Epoch 11, Batch 940, LR 0.000084 Loss 8.364291, Accuracy 71.881%\n",
      "Epoch 11, Batch 941, LR 0.000084 Loss 8.364155, Accuracy 71.880%\n",
      "Epoch 11, Batch 942, LR 0.000084 Loss 8.363404, Accuracy 71.888%\n",
      "Epoch 11, Batch 943, LR 0.000084 Loss 8.362984, Accuracy 71.892%\n",
      "Epoch 11, Batch 944, LR 0.000084 Loss 8.362511, Accuracy 71.886%\n",
      "Epoch 11, Batch 945, LR 0.000084 Loss 8.361975, Accuracy 71.892%\n",
      "Epoch 11, Batch 946, LR 0.000084 Loss 8.362151, Accuracy 71.889%\n",
      "Epoch 11, Batch 947, LR 0.000084 Loss 8.361830, Accuracy 71.887%\n",
      "Epoch 11, Batch 948, LR 0.000084 Loss 8.362049, Accuracy 71.886%\n",
      "Epoch 11, Batch 949, LR 0.000084 Loss 8.361085, Accuracy 71.895%\n",
      "Epoch 11, Batch 950, LR 0.000084 Loss 8.360234, Accuracy 71.901%\n",
      "Epoch 11, Batch 951, LR 0.000084 Loss 8.359743, Accuracy 71.905%\n",
      "Epoch 11, Batch 952, LR 0.000084 Loss 8.359145, Accuracy 71.909%\n",
      "Epoch 11, Batch 953, LR 0.000084 Loss 8.357931, Accuracy 71.918%\n",
      "Epoch 11, Batch 954, LR 0.000084 Loss 8.357117, Accuracy 71.924%\n",
      "Epoch 11, Batch 955, LR 0.000084 Loss 8.356283, Accuracy 71.925%\n",
      "Epoch 11, Batch 956, LR 0.000084 Loss 8.356421, Accuracy 71.923%\n",
      "Epoch 11, Batch 957, LR 0.000084 Loss 8.356364, Accuracy 71.925%\n",
      "Epoch 11, Batch 958, LR 0.000084 Loss 8.356530, Accuracy 71.926%\n",
      "Epoch 11, Batch 959, LR 0.000084 Loss 8.356468, Accuracy 71.925%\n",
      "Epoch 11, Batch 960, LR 0.000084 Loss 8.357451, Accuracy 71.915%\n",
      "Epoch 11, Batch 961, LR 0.000084 Loss 8.357139, Accuracy 71.920%\n",
      "Epoch 11, Batch 962, LR 0.000084 Loss 8.357622, Accuracy 71.916%\n",
      "Epoch 11, Batch 963, LR 0.000084 Loss 8.357661, Accuracy 71.914%\n",
      "Epoch 11, Batch 964, LR 0.000084 Loss 8.358301, Accuracy 71.907%\n",
      "Epoch 11, Batch 965, LR 0.000084 Loss 8.357768, Accuracy 71.906%\n",
      "Epoch 11, Batch 966, LR 0.000084 Loss 8.357419, Accuracy 71.909%\n",
      "Epoch 11, Batch 967, LR 0.000084 Loss 8.357438, Accuracy 71.911%\n",
      "Epoch 11, Batch 968, LR 0.000084 Loss 8.357337, Accuracy 71.907%\n",
      "Epoch 11, Batch 969, LR 0.000084 Loss 8.357229, Accuracy 71.910%\n",
      "Epoch 11, Batch 970, LR 0.000084 Loss 8.356343, Accuracy 71.916%\n",
      "Epoch 11, Batch 971, LR 0.000084 Loss 8.356226, Accuracy 71.913%\n",
      "Epoch 11, Batch 972, LR 0.000084 Loss 8.355905, Accuracy 71.910%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 973, LR 0.000084 Loss 8.356276, Accuracy 71.909%\n",
      "Epoch 11, Batch 974, LR 0.000084 Loss 8.355586, Accuracy 71.911%\n",
      "Epoch 11, Batch 975, LR 0.000084 Loss 8.355192, Accuracy 71.915%\n",
      "Epoch 11, Batch 976, LR 0.000084 Loss 8.355606, Accuracy 71.910%\n",
      "Epoch 11, Batch 977, LR 0.000084 Loss 8.355344, Accuracy 71.909%\n",
      "Epoch 11, Batch 978, LR 0.000084 Loss 8.354984, Accuracy 71.910%\n",
      "Epoch 11, Batch 979, LR 0.000084 Loss 8.354645, Accuracy 71.912%\n",
      "Epoch 11, Batch 980, LR 0.000084 Loss 8.354082, Accuracy 71.914%\n",
      "Epoch 11, Batch 981, LR 0.000084 Loss 8.353971, Accuracy 71.915%\n",
      "Epoch 11, Batch 982, LR 0.000084 Loss 8.352858, Accuracy 71.916%\n",
      "Epoch 11, Batch 983, LR 0.000084 Loss 8.352815, Accuracy 71.920%\n",
      "Epoch 11, Batch 984, LR 0.000084 Loss 8.352504, Accuracy 71.920%\n",
      "Epoch 11, Batch 985, LR 0.000084 Loss 8.352543, Accuracy 71.923%\n",
      "Epoch 11, Batch 986, LR 0.000084 Loss 8.352783, Accuracy 71.919%\n",
      "Epoch 11, Batch 987, LR 0.000084 Loss 8.353051, Accuracy 71.919%\n",
      "Epoch 11, Batch 988, LR 0.000084 Loss 8.352351, Accuracy 71.926%\n",
      "Epoch 11, Batch 989, LR 0.000084 Loss 8.351995, Accuracy 71.925%\n",
      "Epoch 11, Batch 990, LR 0.000084 Loss 8.352243, Accuracy 71.922%\n",
      "Epoch 11, Batch 991, LR 0.000084 Loss 8.351761, Accuracy 71.924%\n",
      "Epoch 11, Batch 992, LR 0.000084 Loss 8.351811, Accuracy 71.924%\n",
      "Epoch 11, Batch 993, LR 0.000084 Loss 8.351506, Accuracy 71.927%\n",
      "Epoch 11, Batch 994, LR 0.000084 Loss 8.352208, Accuracy 71.922%\n",
      "Epoch 11, Batch 995, LR 0.000084 Loss 8.352168, Accuracy 71.921%\n",
      "Epoch 11, Batch 996, LR 0.000084 Loss 8.351786, Accuracy 71.924%\n",
      "Epoch 11, Batch 997, LR 0.000084 Loss 8.351337, Accuracy 71.931%\n",
      "Epoch 11, Batch 998, LR 0.000084 Loss 8.351145, Accuracy 71.933%\n",
      "Epoch 11, Batch 999, LR 0.000084 Loss 8.351507, Accuracy 71.926%\n",
      "Epoch 11, Batch 1000, LR 0.000084 Loss 8.351239, Accuracy 71.928%\n",
      "Epoch 11, Batch 1001, LR 0.000084 Loss 8.351236, Accuracy 71.930%\n",
      "Epoch 11, Batch 1002, LR 0.000084 Loss 8.350829, Accuracy 71.937%\n",
      "Epoch 11, Batch 1003, LR 0.000084 Loss 8.350963, Accuracy 71.937%\n",
      "Epoch 11, Batch 1004, LR 0.000084 Loss 8.351328, Accuracy 71.933%\n",
      "Epoch 11, Batch 1005, LR 0.000084 Loss 8.350063, Accuracy 71.942%\n",
      "Epoch 11, Batch 1006, LR 0.000084 Loss 8.349253, Accuracy 71.946%\n",
      "Epoch 11, Batch 1007, LR 0.000084 Loss 8.348414, Accuracy 71.946%\n",
      "Epoch 11, Batch 1008, LR 0.000084 Loss 8.348708, Accuracy 71.946%\n",
      "Epoch 11, Batch 1009, LR 0.000084 Loss 8.348636, Accuracy 71.946%\n",
      "Epoch 11, Batch 1010, LR 0.000084 Loss 8.348793, Accuracy 71.942%\n",
      "Epoch 11, Batch 1011, LR 0.000084 Loss 8.348102, Accuracy 71.944%\n",
      "Epoch 11, Batch 1012, LR 0.000084 Loss 8.348013, Accuracy 71.944%\n",
      "Epoch 11, Batch 1013, LR 0.000084 Loss 8.347960, Accuracy 71.944%\n",
      "Epoch 11, Batch 1014, LR 0.000084 Loss 8.347173, Accuracy 71.953%\n",
      "Epoch 11, Batch 1015, LR 0.000084 Loss 8.346483, Accuracy 71.956%\n",
      "Epoch 11, Batch 1016, LR 0.000084 Loss 8.347225, Accuracy 71.956%\n",
      "Epoch 11, Batch 1017, LR 0.000084 Loss 8.347260, Accuracy 71.955%\n",
      "Epoch 11, Batch 1018, LR 0.000084 Loss 8.347034, Accuracy 71.958%\n",
      "Epoch 11, Batch 1019, LR 0.000084 Loss 8.346720, Accuracy 71.960%\n",
      "Epoch 11, Batch 1020, LR 0.000084 Loss 8.347455, Accuracy 71.951%\n",
      "Epoch 11, Batch 1021, LR 0.000084 Loss 8.347373, Accuracy 71.952%\n",
      "Epoch 11, Batch 1022, LR 0.000084 Loss 8.347746, Accuracy 71.950%\n",
      "Epoch 11, Batch 1023, LR 0.000084 Loss 8.346583, Accuracy 71.955%\n",
      "Epoch 11, Batch 1024, LR 0.000084 Loss 8.346288, Accuracy 71.956%\n",
      "Epoch 11, Batch 1025, LR 0.000084 Loss 8.346150, Accuracy 71.956%\n",
      "Epoch 11, Batch 1026, LR 0.000084 Loss 8.344995, Accuracy 71.960%\n",
      "Epoch 11, Batch 1027, LR 0.000084 Loss 8.344658, Accuracy 71.965%\n",
      "Epoch 11, Batch 1028, LR 0.000084 Loss 8.344260, Accuracy 71.968%\n",
      "Epoch 11, Batch 1029, LR 0.000084 Loss 8.343473, Accuracy 71.971%\n",
      "Epoch 11, Batch 1030, LR 0.000084 Loss 8.343186, Accuracy 71.978%\n",
      "Epoch 11, Batch 1031, LR 0.000084 Loss 8.343092, Accuracy 71.982%\n",
      "Epoch 11, Batch 1032, LR 0.000084 Loss 8.343784, Accuracy 71.976%\n",
      "Epoch 11, Batch 1033, LR 0.000084 Loss 8.342734, Accuracy 71.981%\n",
      "Epoch 11, Batch 1034, LR 0.000084 Loss 8.342954, Accuracy 71.979%\n",
      "Epoch 11, Batch 1035, LR 0.000084 Loss 8.342425, Accuracy 71.981%\n",
      "Epoch 11, Batch 1036, LR 0.000084 Loss 8.341834, Accuracy 71.984%\n",
      "Epoch 11, Batch 1037, LR 0.000084 Loss 8.342079, Accuracy 71.980%\n",
      "Epoch 11, Batch 1038, LR 0.000084 Loss 8.341841, Accuracy 71.980%\n",
      "Epoch 11, Batch 1039, LR 0.000084 Loss 8.341193, Accuracy 71.981%\n",
      "Epoch 11, Batch 1040, LR 0.000084 Loss 8.340465, Accuracy 71.985%\n",
      "Epoch 11, Batch 1041, LR 0.000084 Loss 8.339838, Accuracy 71.989%\n",
      "Epoch 11, Batch 1042, LR 0.000084 Loss 8.339993, Accuracy 71.987%\n",
      "Epoch 11, Batch 1043, LR 0.000084 Loss 8.339198, Accuracy 71.990%\n",
      "Epoch 11, Batch 1044, LR 0.000084 Loss 8.339217, Accuracy 71.990%\n",
      "Epoch 11, Batch 1045, LR 0.000084 Loss 8.339913, Accuracy 71.984%\n",
      "Epoch 11, Batch 1046, LR 0.000084 Loss 8.339607, Accuracy 71.986%\n",
      "Epoch 11, Batch 1047, LR 0.000084 Loss 8.339299, Accuracy 71.989%\n",
      "Epoch 11, Loss (train set) 8.339299, Accuracy (train set) 71.989%\n",
      "Epoch 12, Batch 1, LR 0.000084 Loss 8.810670, Accuracy 65.625%\n",
      "Epoch 12, Batch 2, LR 0.000084 Loss 8.367055, Accuracy 69.141%\n",
      "Epoch 12, Batch 3, LR 0.000084 Loss 8.468987, Accuracy 67.708%\n",
      "Epoch 12, Batch 4, LR 0.000084 Loss 8.471954, Accuracy 67.188%\n",
      "Epoch 12, Batch 5, LR 0.000084 Loss 8.532485, Accuracy 67.656%\n",
      "Epoch 12, Batch 6, LR 0.000084 Loss 8.466502, Accuracy 68.620%\n",
      "Epoch 12, Batch 7, LR 0.000084 Loss 8.434878, Accuracy 68.638%\n",
      "Epoch 12, Batch 8, LR 0.000084 Loss 8.407446, Accuracy 69.238%\n",
      "Epoch 12, Batch 9, LR 0.000084 Loss 8.316927, Accuracy 70.052%\n",
      "Epoch 12, Batch 10, LR 0.000084 Loss 8.363971, Accuracy 70.000%\n",
      "Epoch 12, Batch 11, LR 0.000084 Loss 8.327290, Accuracy 70.526%\n",
      "Epoch 12, Batch 12, LR 0.000084 Loss 8.306164, Accuracy 71.094%\n",
      "Epoch 12, Batch 13, LR 0.000084 Loss 8.266773, Accuracy 71.755%\n",
      "Epoch 12, Batch 14, LR 0.000084 Loss 8.212731, Accuracy 72.042%\n",
      "Epoch 12, Batch 15, LR 0.000084 Loss 8.229713, Accuracy 72.188%\n",
      "Epoch 12, Batch 16, LR 0.000084 Loss 8.199363, Accuracy 72.266%\n",
      "Epoch 12, Batch 17, LR 0.000084 Loss 8.192615, Accuracy 72.335%\n",
      "Epoch 12, Batch 18, LR 0.000084 Loss 8.182165, Accuracy 72.483%\n",
      "Epoch 12, Batch 19, LR 0.000084 Loss 8.155792, Accuracy 72.656%\n",
      "Epoch 12, Batch 20, LR 0.000084 Loss 8.131862, Accuracy 72.812%\n",
      "Epoch 12, Batch 21, LR 0.000084 Loss 8.089225, Accuracy 73.065%\n",
      "Epoch 12, Batch 22, LR 0.000084 Loss 8.112247, Accuracy 72.798%\n",
      "Epoch 12, Batch 23, LR 0.000084 Loss 8.102817, Accuracy 72.860%\n",
      "Epoch 12, Batch 24, LR 0.000084 Loss 8.120250, Accuracy 72.721%\n",
      "Epoch 12, Batch 25, LR 0.000084 Loss 8.109268, Accuracy 72.719%\n",
      "Epoch 12, Batch 26, LR 0.000084 Loss 8.110747, Accuracy 72.867%\n",
      "Epoch 12, Batch 27, LR 0.000084 Loss 8.084199, Accuracy 73.061%\n",
      "Epoch 12, Batch 28, LR 0.000084 Loss 8.091753, Accuracy 72.768%\n",
      "Epoch 12, Batch 29, LR 0.000084 Loss 8.097747, Accuracy 72.791%\n",
      "Epoch 12, Batch 30, LR 0.000084 Loss 8.097115, Accuracy 72.943%\n",
      "Epoch 12, Batch 31, LR 0.000084 Loss 8.083297, Accuracy 73.034%\n",
      "Epoch 12, Batch 32, LR 0.000084 Loss 8.104564, Accuracy 72.705%\n",
      "Epoch 12, Batch 33, LR 0.000084 Loss 8.092820, Accuracy 72.775%\n",
      "Epoch 12, Batch 34, LR 0.000084 Loss 8.086087, Accuracy 72.863%\n",
      "Epoch 12, Batch 35, LR 0.000084 Loss 8.094485, Accuracy 72.902%\n",
      "Epoch 12, Batch 36, LR 0.000084 Loss 8.097643, Accuracy 73.003%\n",
      "Epoch 12, Batch 37, LR 0.000084 Loss 8.097188, Accuracy 73.079%\n",
      "Epoch 12, Batch 38, LR 0.000084 Loss 8.103754, Accuracy 73.211%\n",
      "Epoch 12, Batch 39, LR 0.000084 Loss 8.094568, Accuracy 73.257%\n",
      "Epoch 12, Batch 40, LR 0.000084 Loss 8.102795, Accuracy 73.359%\n",
      "Epoch 12, Batch 41, LR 0.000084 Loss 8.096135, Accuracy 73.399%\n",
      "Epoch 12, Batch 42, LR 0.000084 Loss 8.084152, Accuracy 73.438%\n",
      "Epoch 12, Batch 43, LR 0.000084 Loss 8.077756, Accuracy 73.456%\n",
      "Epoch 12, Batch 44, LR 0.000084 Loss 8.064401, Accuracy 73.509%\n",
      "Epoch 12, Batch 45, LR 0.000084 Loss 8.052563, Accuracy 73.507%\n",
      "Epoch 12, Batch 46, LR 0.000084 Loss 8.052221, Accuracy 73.556%\n",
      "Epoch 12, Batch 47, LR 0.000084 Loss 8.055543, Accuracy 73.604%\n",
      "Epoch 12, Batch 48, LR 0.000084 Loss 8.065938, Accuracy 73.454%\n",
      "Epoch 12, Batch 49, LR 0.000084 Loss 8.066773, Accuracy 73.485%\n",
      "Epoch 12, Batch 50, LR 0.000084 Loss 8.065270, Accuracy 73.516%\n",
      "Epoch 12, Batch 51, LR 0.000084 Loss 8.089215, Accuracy 73.376%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 52, LR 0.000084 Loss 8.077304, Accuracy 73.483%\n",
      "Epoch 12, Batch 53, LR 0.000084 Loss 8.080553, Accuracy 73.496%\n",
      "Epoch 12, Batch 54, LR 0.000084 Loss 8.070697, Accuracy 73.568%\n",
      "Epoch 12, Batch 55, LR 0.000084 Loss 8.062652, Accuracy 73.679%\n",
      "Epoch 12, Batch 56, LR 0.000084 Loss 8.063931, Accuracy 73.549%\n",
      "Epoch 12, Batch 57, LR 0.000084 Loss 8.056355, Accuracy 73.588%\n",
      "Epoch 12, Batch 58, LR 0.000084 Loss 8.062304, Accuracy 73.545%\n",
      "Epoch 12, Batch 59, LR 0.000084 Loss 8.070806, Accuracy 73.424%\n",
      "Epoch 12, Batch 60, LR 0.000084 Loss 8.069213, Accuracy 73.438%\n",
      "Epoch 12, Batch 61, LR 0.000084 Loss 8.063493, Accuracy 73.527%\n",
      "Epoch 12, Batch 62, LR 0.000084 Loss 8.078626, Accuracy 73.425%\n",
      "Epoch 12, Batch 63, LR 0.000084 Loss 8.073457, Accuracy 73.487%\n",
      "Epoch 12, Batch 64, LR 0.000084 Loss 8.058678, Accuracy 73.523%\n",
      "Epoch 12, Batch 65, LR 0.000084 Loss 8.055940, Accuracy 73.546%\n",
      "Epoch 12, Batch 66, LR 0.000084 Loss 8.041009, Accuracy 73.568%\n",
      "Epoch 12, Batch 67, LR 0.000084 Loss 8.030697, Accuracy 73.554%\n",
      "Epoch 12, Batch 68, LR 0.000084 Loss 8.036587, Accuracy 73.506%\n",
      "Epoch 12, Batch 69, LR 0.000084 Loss 8.038520, Accuracy 73.426%\n",
      "Epoch 12, Batch 70, LR 0.000084 Loss 8.039567, Accuracy 73.438%\n",
      "Epoch 12, Batch 71, LR 0.000084 Loss 8.032717, Accuracy 73.504%\n",
      "Epoch 12, Batch 72, LR 0.000084 Loss 8.041347, Accuracy 73.427%\n",
      "Epoch 12, Batch 73, LR 0.000084 Loss 8.035653, Accuracy 73.448%\n",
      "Epoch 12, Batch 74, LR 0.000084 Loss 8.031135, Accuracy 73.448%\n",
      "Epoch 12, Batch 75, LR 0.000084 Loss 8.039210, Accuracy 73.500%\n",
      "Epoch 12, Batch 76, LR 0.000084 Loss 8.032998, Accuracy 73.489%\n",
      "Epoch 12, Batch 77, LR 0.000084 Loss 8.040872, Accuracy 73.438%\n",
      "Epoch 12, Batch 78, LR 0.000084 Loss 8.047633, Accuracy 73.508%\n",
      "Epoch 12, Batch 79, LR 0.000084 Loss 8.046208, Accuracy 73.527%\n",
      "Epoch 12, Batch 80, LR 0.000084 Loss 8.047450, Accuracy 73.574%\n",
      "Epoch 12, Batch 81, LR 0.000084 Loss 8.051282, Accuracy 73.582%\n",
      "Epoch 12, Batch 82, LR 0.000084 Loss 8.045756, Accuracy 73.619%\n",
      "Epoch 12, Batch 83, LR 0.000084 Loss 8.043690, Accuracy 73.598%\n",
      "Epoch 12, Batch 84, LR 0.000084 Loss 8.035859, Accuracy 73.605%\n",
      "Epoch 12, Batch 85, LR 0.000084 Loss 8.047458, Accuracy 73.529%\n",
      "Epoch 12, Batch 86, LR 0.000084 Loss 8.038079, Accuracy 73.547%\n",
      "Epoch 12, Batch 87, LR 0.000084 Loss 8.030177, Accuracy 73.563%\n",
      "Epoch 12, Batch 88, LR 0.000084 Loss 8.038167, Accuracy 73.482%\n",
      "Epoch 12, Batch 89, LR 0.000084 Loss 8.035769, Accuracy 73.552%\n",
      "Epoch 12, Batch 90, LR 0.000084 Loss 8.035527, Accuracy 73.516%\n",
      "Epoch 12, Batch 91, LR 0.000084 Loss 8.039820, Accuracy 73.498%\n",
      "Epoch 12, Batch 92, LR 0.000084 Loss 8.043782, Accuracy 73.497%\n",
      "Epoch 12, Batch 93, LR 0.000084 Loss 8.040060, Accuracy 73.522%\n",
      "Epoch 12, Batch 94, LR 0.000084 Loss 8.035350, Accuracy 73.546%\n",
      "Epoch 12, Batch 95, LR 0.000084 Loss 8.037773, Accuracy 73.520%\n",
      "Epoch 12, Batch 96, LR 0.000084 Loss 8.037743, Accuracy 73.519%\n",
      "Epoch 12, Batch 97, LR 0.000084 Loss 8.043641, Accuracy 73.438%\n",
      "Epoch 12, Batch 98, LR 0.000084 Loss 8.043833, Accuracy 73.477%\n",
      "Epoch 12, Batch 99, LR 0.000084 Loss 8.038980, Accuracy 73.485%\n",
      "Epoch 12, Batch 100, LR 0.000084 Loss 8.039788, Accuracy 73.531%\n",
      "Epoch 12, Batch 101, LR 0.000084 Loss 8.034426, Accuracy 73.584%\n",
      "Epoch 12, Batch 102, LR 0.000084 Loss 8.032468, Accuracy 73.583%\n",
      "Epoch 12, Batch 103, LR 0.000084 Loss 8.029542, Accuracy 73.566%\n",
      "Epoch 12, Batch 104, LR 0.000084 Loss 8.030926, Accuracy 73.535%\n",
      "Epoch 12, Batch 105, LR 0.000084 Loss 8.028486, Accuracy 73.579%\n",
      "Epoch 12, Batch 106, LR 0.000084 Loss 8.032376, Accuracy 73.570%\n",
      "Epoch 12, Batch 107, LR 0.000084 Loss 8.041118, Accuracy 73.554%\n",
      "Epoch 12, Batch 108, LR 0.000084 Loss 8.047986, Accuracy 73.510%\n",
      "Epoch 12, Batch 109, LR 0.000084 Loss 8.050403, Accuracy 73.509%\n",
      "Epoch 12, Batch 110, LR 0.000084 Loss 8.051167, Accuracy 73.537%\n",
      "Epoch 12, Batch 111, LR 0.000084 Loss 8.041627, Accuracy 73.606%\n",
      "Epoch 12, Batch 112, LR 0.000084 Loss 8.051342, Accuracy 73.514%\n",
      "Epoch 12, Batch 113, LR 0.000084 Loss 8.048471, Accuracy 73.507%\n",
      "Epoch 12, Batch 114, LR 0.000084 Loss 8.047393, Accuracy 73.479%\n",
      "Epoch 12, Batch 115, LR 0.000084 Loss 8.053320, Accuracy 73.424%\n",
      "Epoch 12, Batch 116, LR 0.000084 Loss 8.042645, Accuracy 73.438%\n",
      "Epoch 12, Batch 117, LR 0.000084 Loss 8.037269, Accuracy 73.464%\n",
      "Epoch 12, Batch 118, LR 0.000084 Loss 8.036753, Accuracy 73.457%\n",
      "Epoch 12, Batch 119, LR 0.000084 Loss 8.044036, Accuracy 73.431%\n",
      "Epoch 12, Batch 120, LR 0.000084 Loss 8.047061, Accuracy 73.405%\n",
      "Epoch 12, Batch 121, LR 0.000084 Loss 8.051126, Accuracy 73.392%\n",
      "Epoch 12, Batch 122, LR 0.000084 Loss 8.052485, Accuracy 73.386%\n",
      "Epoch 12, Batch 123, LR 0.000084 Loss 8.051838, Accuracy 73.374%\n",
      "Epoch 12, Batch 124, LR 0.000084 Loss 8.056301, Accuracy 73.311%\n",
      "Epoch 12, Batch 125, LR 0.000083 Loss 8.055088, Accuracy 73.306%\n",
      "Epoch 12, Batch 126, LR 0.000083 Loss 8.055373, Accuracy 73.320%\n",
      "Epoch 12, Batch 127, LR 0.000083 Loss 8.055338, Accuracy 73.333%\n",
      "Epoch 12, Batch 128, LR 0.000083 Loss 8.046411, Accuracy 73.395%\n",
      "Epoch 12, Batch 129, LR 0.000083 Loss 8.044339, Accuracy 73.395%\n",
      "Epoch 12, Batch 130, LR 0.000083 Loss 8.050700, Accuracy 73.371%\n",
      "Epoch 12, Batch 131, LR 0.000083 Loss 8.050583, Accuracy 73.348%\n",
      "Epoch 12, Batch 132, LR 0.000083 Loss 8.051248, Accuracy 73.361%\n",
      "Epoch 12, Batch 133, LR 0.000083 Loss 8.048518, Accuracy 73.396%\n",
      "Epoch 12, Batch 134, LR 0.000083 Loss 8.050401, Accuracy 73.385%\n",
      "Epoch 12, Batch 135, LR 0.000083 Loss 8.051848, Accuracy 73.374%\n",
      "Epoch 12, Batch 136, LR 0.000083 Loss 8.051903, Accuracy 73.363%\n",
      "Epoch 12, Batch 137, LR 0.000083 Loss 8.053807, Accuracy 73.380%\n",
      "Epoch 12, Batch 138, LR 0.000083 Loss 8.056920, Accuracy 73.353%\n",
      "Epoch 12, Batch 139, LR 0.000083 Loss 8.055605, Accuracy 73.319%\n",
      "Epoch 12, Batch 140, LR 0.000083 Loss 8.056589, Accuracy 73.292%\n",
      "Epoch 12, Batch 141, LR 0.000083 Loss 8.053644, Accuracy 73.293%\n",
      "Epoch 12, Batch 142, LR 0.000083 Loss 8.047763, Accuracy 73.305%\n",
      "Epoch 12, Batch 143, LR 0.000083 Loss 8.050488, Accuracy 73.268%\n",
      "Epoch 12, Batch 144, LR 0.000083 Loss 8.054296, Accuracy 73.215%\n",
      "Epoch 12, Batch 145, LR 0.000083 Loss 8.058457, Accuracy 73.179%\n",
      "Epoch 12, Batch 146, LR 0.000083 Loss 8.062162, Accuracy 73.143%\n",
      "Epoch 12, Batch 147, LR 0.000083 Loss 8.066424, Accuracy 73.140%\n",
      "Epoch 12, Batch 148, LR 0.000083 Loss 8.061177, Accuracy 73.200%\n",
      "Epoch 12, Batch 149, LR 0.000083 Loss 8.059205, Accuracy 73.212%\n",
      "Epoch 12, Batch 150, LR 0.000083 Loss 8.064717, Accuracy 73.188%\n",
      "Epoch 12, Batch 151, LR 0.000083 Loss 8.060392, Accuracy 73.220%\n",
      "Epoch 12, Batch 152, LR 0.000083 Loss 8.056119, Accuracy 73.258%\n",
      "Epoch 12, Batch 153, LR 0.000083 Loss 8.053581, Accuracy 73.295%\n",
      "Epoch 12, Batch 154, LR 0.000083 Loss 8.051458, Accuracy 73.290%\n",
      "Epoch 12, Batch 155, LR 0.000083 Loss 8.044855, Accuracy 73.306%\n",
      "Epoch 12, Batch 156, LR 0.000083 Loss 8.047962, Accuracy 73.292%\n",
      "Epoch 12, Batch 157, LR 0.000083 Loss 8.049204, Accuracy 73.273%\n",
      "Epoch 12, Batch 158, LR 0.000083 Loss 8.048666, Accuracy 73.289%\n",
      "Epoch 12, Batch 159, LR 0.000083 Loss 8.049359, Accuracy 73.295%\n",
      "Epoch 12, Batch 160, LR 0.000083 Loss 8.053667, Accuracy 73.252%\n",
      "Epoch 12, Batch 161, LR 0.000083 Loss 8.051519, Accuracy 73.277%\n",
      "Epoch 12, Batch 162, LR 0.000083 Loss 8.047986, Accuracy 73.317%\n",
      "Epoch 12, Batch 163, LR 0.000083 Loss 8.046834, Accuracy 73.303%\n",
      "Epoch 12, Batch 164, LR 0.000083 Loss 8.047610, Accuracy 73.290%\n",
      "Epoch 12, Batch 165, LR 0.000083 Loss 8.045442, Accuracy 73.319%\n",
      "Epoch 12, Batch 166, LR 0.000083 Loss 8.046369, Accuracy 73.310%\n",
      "Epoch 12, Batch 167, LR 0.000083 Loss 8.051653, Accuracy 73.274%\n",
      "Epoch 12, Batch 168, LR 0.000083 Loss 8.049481, Accuracy 73.284%\n",
      "Epoch 12, Batch 169, LR 0.000083 Loss 8.046611, Accuracy 73.317%\n",
      "Epoch 12, Batch 170, LR 0.000083 Loss 8.049081, Accuracy 73.309%\n",
      "Epoch 12, Batch 171, LR 0.000083 Loss 8.048862, Accuracy 73.319%\n",
      "Epoch 12, Batch 172, LR 0.000083 Loss 8.045417, Accuracy 73.347%\n",
      "Epoch 12, Batch 173, LR 0.000083 Loss 8.047081, Accuracy 73.338%\n",
      "Epoch 12, Batch 174, LR 0.000083 Loss 8.049401, Accuracy 73.321%\n",
      "Epoch 12, Batch 175, LR 0.000083 Loss 8.051609, Accuracy 73.295%\n",
      "Epoch 12, Batch 176, LR 0.000083 Loss 8.052286, Accuracy 73.282%\n",
      "Epoch 12, Batch 177, LR 0.000083 Loss 8.054735, Accuracy 73.257%\n",
      "Epoch 12, Batch 178, LR 0.000083 Loss 8.053913, Accuracy 73.236%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 179, LR 0.000083 Loss 8.054096, Accuracy 73.224%\n",
      "Epoch 12, Batch 180, LR 0.000083 Loss 8.053293, Accuracy 73.234%\n",
      "Epoch 12, Batch 181, LR 0.000083 Loss 8.050682, Accuracy 73.235%\n",
      "Epoch 12, Batch 182, LR 0.000083 Loss 8.050859, Accuracy 73.244%\n",
      "Epoch 12, Batch 183, LR 0.000083 Loss 8.046142, Accuracy 73.284%\n",
      "Epoch 12, Batch 184, LR 0.000083 Loss 8.044487, Accuracy 73.293%\n",
      "Epoch 12, Batch 185, LR 0.000083 Loss 8.042432, Accuracy 73.315%\n",
      "Epoch 12, Batch 186, LR 0.000083 Loss 8.036468, Accuracy 73.337%\n",
      "Epoch 12, Batch 187, LR 0.000083 Loss 8.031781, Accuracy 73.366%\n",
      "Epoch 12, Batch 188, LR 0.000083 Loss 8.030116, Accuracy 73.379%\n",
      "Epoch 12, Batch 189, LR 0.000083 Loss 8.029769, Accuracy 73.388%\n",
      "Epoch 12, Batch 190, LR 0.000083 Loss 8.029371, Accuracy 73.380%\n",
      "Epoch 12, Batch 191, LR 0.000083 Loss 8.028586, Accuracy 73.376%\n",
      "Epoch 12, Batch 192, LR 0.000083 Loss 8.028376, Accuracy 73.372%\n",
      "Epoch 12, Batch 193, LR 0.000083 Loss 8.026023, Accuracy 73.385%\n",
      "Epoch 12, Batch 194, LR 0.000083 Loss 8.029017, Accuracy 73.361%\n",
      "Epoch 12, Batch 195, LR 0.000083 Loss 8.030499, Accuracy 73.345%\n",
      "Epoch 12, Batch 196, LR 0.000083 Loss 8.031576, Accuracy 73.338%\n",
      "Epoch 12, Batch 197, LR 0.000083 Loss 8.030850, Accuracy 73.354%\n",
      "Epoch 12, Batch 198, LR 0.000083 Loss 8.031521, Accuracy 73.335%\n",
      "Epoch 12, Batch 199, LR 0.000083 Loss 8.031244, Accuracy 73.324%\n",
      "Epoch 12, Batch 200, LR 0.000083 Loss 8.030613, Accuracy 73.316%\n",
      "Epoch 12, Batch 201, LR 0.000083 Loss 8.029202, Accuracy 73.301%\n",
      "Epoch 12, Batch 202, LR 0.000083 Loss 8.026449, Accuracy 73.329%\n",
      "Epoch 12, Batch 203, LR 0.000083 Loss 8.027784, Accuracy 73.345%\n",
      "Epoch 12, Batch 204, LR 0.000083 Loss 8.028471, Accuracy 73.349%\n",
      "Epoch 12, Batch 205, LR 0.000083 Loss 8.028396, Accuracy 73.369%\n",
      "Epoch 12, Batch 206, LR 0.000083 Loss 8.029835, Accuracy 73.373%\n",
      "Epoch 12, Batch 207, LR 0.000083 Loss 8.028331, Accuracy 73.385%\n",
      "Epoch 12, Batch 208, LR 0.000083 Loss 8.030141, Accuracy 73.359%\n",
      "Epoch 12, Batch 209, LR 0.000083 Loss 8.032715, Accuracy 73.337%\n",
      "Epoch 12, Batch 210, LR 0.000083 Loss 8.035564, Accuracy 73.318%\n",
      "Epoch 12, Batch 211, LR 0.000083 Loss 8.036709, Accuracy 73.323%\n",
      "Epoch 12, Batch 212, LR 0.000083 Loss 8.035960, Accuracy 73.345%\n",
      "Epoch 12, Batch 213, LR 0.000083 Loss 8.035269, Accuracy 73.364%\n",
      "Epoch 12, Batch 214, LR 0.000083 Loss 8.032406, Accuracy 73.386%\n",
      "Epoch 12, Batch 215, LR 0.000083 Loss 8.034055, Accuracy 73.390%\n",
      "Epoch 12, Batch 216, LR 0.000083 Loss 8.034811, Accuracy 73.394%\n",
      "Epoch 12, Batch 217, LR 0.000083 Loss 8.032559, Accuracy 73.419%\n",
      "Epoch 12, Batch 218, LR 0.000083 Loss 8.031930, Accuracy 73.441%\n",
      "Epoch 12, Batch 219, LR 0.000083 Loss 8.030324, Accuracy 73.452%\n",
      "Epoch 12, Batch 220, LR 0.000083 Loss 8.032831, Accuracy 73.441%\n",
      "Epoch 12, Batch 221, LR 0.000083 Loss 8.035382, Accuracy 73.438%\n",
      "Epoch 12, Batch 222, LR 0.000083 Loss 8.035551, Accuracy 73.441%\n",
      "Epoch 12, Batch 223, LR 0.000083 Loss 8.032286, Accuracy 73.480%\n",
      "Epoch 12, Batch 224, LR 0.000083 Loss 8.034247, Accuracy 73.462%\n",
      "Epoch 12, Batch 225, LR 0.000083 Loss 8.033164, Accuracy 73.462%\n",
      "Epoch 12, Batch 226, LR 0.000083 Loss 8.031129, Accuracy 73.472%\n",
      "Epoch 12, Batch 227, LR 0.000083 Loss 8.033453, Accuracy 73.451%\n",
      "Epoch 12, Batch 228, LR 0.000083 Loss 8.035509, Accuracy 73.424%\n",
      "Epoch 12, Batch 229, LR 0.000083 Loss 8.035218, Accuracy 73.420%\n",
      "Epoch 12, Batch 230, LR 0.000083 Loss 8.032350, Accuracy 73.454%\n",
      "Epoch 12, Batch 231, LR 0.000083 Loss 8.029917, Accuracy 73.471%\n",
      "Epoch 12, Batch 232, LR 0.000083 Loss 8.029096, Accuracy 73.464%\n",
      "Epoch 12, Batch 233, LR 0.000083 Loss 8.029730, Accuracy 73.448%\n",
      "Epoch 12, Batch 234, LR 0.000083 Loss 8.030007, Accuracy 73.414%\n",
      "Epoch 12, Batch 235, LR 0.000083 Loss 8.029976, Accuracy 73.408%\n",
      "Epoch 12, Batch 236, LR 0.000083 Loss 8.029624, Accuracy 73.414%\n",
      "Epoch 12, Batch 237, LR 0.000083 Loss 8.024823, Accuracy 73.434%\n",
      "Epoch 12, Batch 238, LR 0.000083 Loss 8.025451, Accuracy 73.424%\n",
      "Epoch 12, Batch 239, LR 0.000083 Loss 8.028870, Accuracy 73.405%\n",
      "Epoch 12, Batch 240, LR 0.000083 Loss 8.030846, Accuracy 73.395%\n",
      "Epoch 12, Batch 241, LR 0.000083 Loss 8.028725, Accuracy 73.415%\n",
      "Epoch 12, Batch 242, LR 0.000083 Loss 8.026435, Accuracy 73.434%\n",
      "Epoch 12, Batch 243, LR 0.000083 Loss 8.027983, Accuracy 73.428%\n",
      "Epoch 12, Batch 244, LR 0.000083 Loss 8.028406, Accuracy 73.441%\n",
      "Epoch 12, Batch 245, LR 0.000083 Loss 8.029613, Accuracy 73.418%\n",
      "Epoch 12, Batch 246, LR 0.000083 Loss 8.027973, Accuracy 73.412%\n",
      "Epoch 12, Batch 247, LR 0.000083 Loss 8.032321, Accuracy 73.374%\n",
      "Epoch 12, Batch 248, LR 0.000083 Loss 8.031872, Accuracy 73.378%\n",
      "Epoch 12, Batch 249, LR 0.000083 Loss 8.027405, Accuracy 73.390%\n",
      "Epoch 12, Batch 250, LR 0.000083 Loss 8.026830, Accuracy 73.412%\n",
      "Epoch 12, Batch 251, LR 0.000083 Loss 8.023383, Accuracy 73.431%\n",
      "Epoch 12, Batch 252, LR 0.000083 Loss 8.023848, Accuracy 73.416%\n",
      "Epoch 12, Batch 253, LR 0.000083 Loss 8.024032, Accuracy 73.434%\n",
      "Epoch 12, Batch 254, LR 0.000083 Loss 8.024500, Accuracy 73.465%\n",
      "Epoch 12, Batch 255, LR 0.000083 Loss 8.021406, Accuracy 73.477%\n",
      "Epoch 12, Batch 256, LR 0.000083 Loss 8.019771, Accuracy 73.495%\n",
      "Epoch 12, Batch 257, LR 0.000083 Loss 8.015415, Accuracy 73.517%\n",
      "Epoch 12, Batch 258, LR 0.000083 Loss 8.015773, Accuracy 73.519%\n",
      "Epoch 12, Batch 259, LR 0.000083 Loss 8.015063, Accuracy 73.528%\n",
      "Epoch 12, Batch 260, LR 0.000083 Loss 8.014199, Accuracy 73.546%\n",
      "Epoch 12, Batch 261, LR 0.000083 Loss 8.011794, Accuracy 73.560%\n",
      "Epoch 12, Batch 262, LR 0.000083 Loss 8.016319, Accuracy 73.530%\n",
      "Epoch 12, Batch 263, LR 0.000083 Loss 8.013178, Accuracy 73.550%\n",
      "Epoch 12, Batch 264, LR 0.000083 Loss 8.010499, Accuracy 73.568%\n",
      "Epoch 12, Batch 265, LR 0.000083 Loss 8.006486, Accuracy 73.582%\n",
      "Epoch 12, Batch 266, LR 0.000083 Loss 8.008627, Accuracy 73.567%\n",
      "Epoch 12, Batch 267, LR 0.000083 Loss 8.010737, Accuracy 73.552%\n",
      "Epoch 12, Batch 268, LR 0.000083 Loss 8.013508, Accuracy 73.534%\n",
      "Epoch 12, Batch 269, LR 0.000083 Loss 8.014311, Accuracy 73.522%\n",
      "Epoch 12, Batch 270, LR 0.000083 Loss 8.016805, Accuracy 73.510%\n",
      "Epoch 12, Batch 271, LR 0.000083 Loss 8.014434, Accuracy 73.527%\n",
      "Epoch 12, Batch 272, LR 0.000083 Loss 8.015942, Accuracy 73.529%\n",
      "Epoch 12, Batch 273, LR 0.000083 Loss 8.020836, Accuracy 73.518%\n",
      "Epoch 12, Batch 274, LR 0.000083 Loss 8.019048, Accuracy 73.549%\n",
      "Epoch 12, Batch 275, LR 0.000083 Loss 8.018828, Accuracy 73.534%\n",
      "Epoch 12, Batch 276, LR 0.000083 Loss 8.020028, Accuracy 73.522%\n",
      "Epoch 12, Batch 277, LR 0.000083 Loss 8.021197, Accuracy 73.511%\n",
      "Epoch 12, Batch 278, LR 0.000083 Loss 8.019363, Accuracy 73.516%\n",
      "Epoch 12, Batch 279, LR 0.000083 Loss 8.016028, Accuracy 73.547%\n",
      "Epoch 12, Batch 280, LR 0.000083 Loss 8.017696, Accuracy 73.535%\n",
      "Epoch 12, Batch 281, LR 0.000083 Loss 8.016872, Accuracy 73.538%\n",
      "Epoch 12, Batch 282, LR 0.000083 Loss 8.015048, Accuracy 73.557%\n",
      "Epoch 12, Batch 283, LR 0.000083 Loss 8.020126, Accuracy 73.542%\n",
      "Epoch 12, Batch 284, LR 0.000083 Loss 8.020094, Accuracy 73.534%\n",
      "Epoch 12, Batch 285, LR 0.000083 Loss 8.020985, Accuracy 73.522%\n",
      "Epoch 12, Batch 286, LR 0.000083 Loss 8.020573, Accuracy 73.528%\n",
      "Epoch 12, Batch 287, LR 0.000083 Loss 8.023133, Accuracy 73.525%\n",
      "Epoch 12, Batch 288, LR 0.000083 Loss 8.023229, Accuracy 73.532%\n",
      "Epoch 12, Batch 289, LR 0.000083 Loss 8.024182, Accuracy 73.535%\n",
      "Epoch 12, Batch 290, LR 0.000083 Loss 8.023355, Accuracy 73.545%\n",
      "Epoch 12, Batch 291, LR 0.000083 Loss 8.022767, Accuracy 73.553%\n",
      "Epoch 12, Batch 292, LR 0.000083 Loss 8.021628, Accuracy 73.566%\n",
      "Epoch 12, Batch 293, LR 0.000083 Loss 8.018875, Accuracy 73.581%\n",
      "Epoch 12, Batch 294, LR 0.000083 Loss 8.019840, Accuracy 73.584%\n",
      "Epoch 12, Batch 295, LR 0.000083 Loss 8.018829, Accuracy 73.586%\n",
      "Epoch 12, Batch 296, LR 0.000083 Loss 8.017319, Accuracy 73.606%\n",
      "Epoch 12, Batch 297, LR 0.000083 Loss 8.017865, Accuracy 73.601%\n",
      "Epoch 12, Batch 298, LR 0.000083 Loss 8.019656, Accuracy 73.590%\n",
      "Epoch 12, Batch 299, LR 0.000083 Loss 8.019617, Accuracy 73.594%\n",
      "Epoch 12, Batch 300, LR 0.000083 Loss 8.019960, Accuracy 73.602%\n",
      "Epoch 12, Batch 301, LR 0.000083 Loss 8.023149, Accuracy 73.593%\n",
      "Epoch 12, Batch 302, LR 0.000083 Loss 8.020913, Accuracy 73.619%\n",
      "Epoch 12, Batch 303, LR 0.000083 Loss 8.020839, Accuracy 73.621%\n",
      "Epoch 12, Batch 304, LR 0.000083 Loss 8.020691, Accuracy 73.623%\n",
      "Epoch 12, Batch 305, LR 0.000083 Loss 8.017265, Accuracy 73.642%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 306, LR 0.000083 Loss 8.017402, Accuracy 73.637%\n",
      "Epoch 12, Batch 307, LR 0.000083 Loss 8.017000, Accuracy 73.651%\n",
      "Epoch 12, Batch 308, LR 0.000083 Loss 8.017262, Accuracy 73.635%\n",
      "Epoch 12, Batch 309, LR 0.000083 Loss 8.014482, Accuracy 73.657%\n",
      "Epoch 12, Batch 310, LR 0.000083 Loss 8.012481, Accuracy 73.664%\n",
      "Epoch 12, Batch 311, LR 0.000083 Loss 8.011781, Accuracy 73.666%\n",
      "Epoch 12, Batch 312, LR 0.000083 Loss 8.011405, Accuracy 73.678%\n",
      "Epoch 12, Batch 313, LR 0.000083 Loss 8.009656, Accuracy 73.700%\n",
      "Epoch 12, Batch 314, LR 0.000083 Loss 8.010167, Accuracy 73.689%\n",
      "Epoch 12, Batch 315, LR 0.000083 Loss 8.010659, Accuracy 73.693%\n",
      "Epoch 12, Batch 316, LR 0.000083 Loss 8.008746, Accuracy 73.705%\n",
      "Epoch 12, Batch 317, LR 0.000083 Loss 8.008578, Accuracy 73.709%\n",
      "Epoch 12, Batch 318, LR 0.000083 Loss 8.009764, Accuracy 73.715%\n",
      "Epoch 12, Batch 319, LR 0.000083 Loss 8.010548, Accuracy 73.729%\n",
      "Epoch 12, Batch 320, LR 0.000083 Loss 8.011977, Accuracy 73.723%\n",
      "Epoch 12, Batch 321, LR 0.000083 Loss 8.014949, Accuracy 73.710%\n",
      "Epoch 12, Batch 322, LR 0.000083 Loss 8.016970, Accuracy 73.697%\n",
      "Epoch 12, Batch 323, LR 0.000083 Loss 8.014732, Accuracy 73.706%\n",
      "Epoch 12, Batch 324, LR 0.000083 Loss 8.013787, Accuracy 73.717%\n",
      "Epoch 12, Batch 325, LR 0.000083 Loss 8.013892, Accuracy 73.709%\n",
      "Epoch 12, Batch 326, LR 0.000083 Loss 8.015159, Accuracy 73.696%\n",
      "Epoch 12, Batch 327, LR 0.000083 Loss 8.016090, Accuracy 73.698%\n",
      "Epoch 12, Batch 328, LR 0.000083 Loss 8.018845, Accuracy 73.669%\n",
      "Epoch 12, Batch 329, LR 0.000083 Loss 8.019633, Accuracy 73.668%\n",
      "Epoch 12, Batch 330, LR 0.000083 Loss 8.020292, Accuracy 73.660%\n",
      "Epoch 12, Batch 331, LR 0.000083 Loss 8.017855, Accuracy 73.671%\n",
      "Epoch 12, Batch 332, LR 0.000083 Loss 8.017194, Accuracy 73.661%\n",
      "Epoch 12, Batch 333, LR 0.000083 Loss 8.014831, Accuracy 73.679%\n",
      "Epoch 12, Batch 334, LR 0.000083 Loss 8.014478, Accuracy 73.678%\n",
      "Epoch 12, Batch 335, LR 0.000083 Loss 8.015646, Accuracy 73.675%\n",
      "Epoch 12, Batch 336, LR 0.000083 Loss 8.016642, Accuracy 73.675%\n",
      "Epoch 12, Batch 337, LR 0.000083 Loss 8.017291, Accuracy 73.662%\n",
      "Epoch 12, Batch 338, LR 0.000083 Loss 8.016308, Accuracy 73.676%\n",
      "Epoch 12, Batch 339, LR 0.000083 Loss 8.016387, Accuracy 73.666%\n",
      "Epoch 12, Batch 340, LR 0.000083 Loss 8.016068, Accuracy 73.658%\n",
      "Epoch 12, Batch 341, LR 0.000083 Loss 8.019108, Accuracy 73.641%\n",
      "Epoch 12, Batch 342, LR 0.000083 Loss 8.018883, Accuracy 73.643%\n",
      "Epoch 12, Batch 343, LR 0.000083 Loss 8.018271, Accuracy 73.633%\n",
      "Epoch 12, Batch 344, LR 0.000083 Loss 8.018359, Accuracy 73.644%\n",
      "Epoch 12, Batch 345, LR 0.000083 Loss 8.018401, Accuracy 73.637%\n",
      "Epoch 12, Batch 346, LR 0.000083 Loss 8.020986, Accuracy 73.625%\n",
      "Epoch 12, Batch 347, LR 0.000083 Loss 8.020880, Accuracy 73.615%\n",
      "Epoch 12, Batch 348, LR 0.000083 Loss 8.020071, Accuracy 73.626%\n",
      "Epoch 12, Batch 349, LR 0.000083 Loss 8.019964, Accuracy 73.612%\n",
      "Epoch 12, Batch 350, LR 0.000083 Loss 8.021546, Accuracy 73.605%\n",
      "Epoch 12, Batch 351, LR 0.000083 Loss 8.022917, Accuracy 73.600%\n",
      "Epoch 12, Batch 352, LR 0.000083 Loss 8.024469, Accuracy 73.595%\n",
      "Epoch 12, Batch 353, LR 0.000083 Loss 8.024024, Accuracy 73.595%\n",
      "Epoch 12, Batch 354, LR 0.000083 Loss 8.024024, Accuracy 73.592%\n",
      "Epoch 12, Batch 355, LR 0.000083 Loss 8.023280, Accuracy 73.600%\n",
      "Epoch 12, Batch 356, LR 0.000083 Loss 8.022124, Accuracy 73.611%\n",
      "Epoch 12, Batch 357, LR 0.000083 Loss 8.021146, Accuracy 73.613%\n",
      "Epoch 12, Batch 358, LR 0.000083 Loss 8.019994, Accuracy 73.636%\n",
      "Epoch 12, Batch 359, LR 0.000083 Loss 8.020838, Accuracy 73.620%\n",
      "Epoch 12, Batch 360, LR 0.000083 Loss 8.020730, Accuracy 73.613%\n",
      "Epoch 12, Batch 361, LR 0.000083 Loss 8.022375, Accuracy 73.606%\n",
      "Epoch 12, Batch 362, LR 0.000083 Loss 8.023504, Accuracy 73.610%\n",
      "Epoch 12, Batch 363, LR 0.000083 Loss 8.023265, Accuracy 73.612%\n",
      "Epoch 12, Batch 364, LR 0.000083 Loss 8.023122, Accuracy 73.613%\n",
      "Epoch 12, Batch 365, LR 0.000083 Loss 8.021200, Accuracy 73.624%\n",
      "Epoch 12, Batch 366, LR 0.000083 Loss 8.019486, Accuracy 73.636%\n",
      "Epoch 12, Batch 367, LR 0.000083 Loss 8.019534, Accuracy 73.640%\n",
      "Epoch 12, Batch 368, LR 0.000083 Loss 8.018455, Accuracy 73.652%\n",
      "Epoch 12, Batch 369, LR 0.000083 Loss 8.018059, Accuracy 73.664%\n",
      "Epoch 12, Batch 370, LR 0.000083 Loss 8.018660, Accuracy 73.659%\n",
      "Epoch 12, Batch 371, LR 0.000083 Loss 8.019152, Accuracy 73.646%\n",
      "Epoch 12, Batch 372, LR 0.000083 Loss 8.020523, Accuracy 73.639%\n",
      "Epoch 12, Batch 373, LR 0.000083 Loss 8.018380, Accuracy 73.641%\n",
      "Epoch 12, Batch 374, LR 0.000083 Loss 8.019719, Accuracy 73.630%\n",
      "Epoch 12, Batch 375, LR 0.000083 Loss 8.019559, Accuracy 73.633%\n",
      "Epoch 12, Batch 376, LR 0.000083 Loss 8.020602, Accuracy 73.639%\n",
      "Epoch 12, Batch 377, LR 0.000083 Loss 8.020890, Accuracy 73.641%\n",
      "Epoch 12, Batch 378, LR 0.000083 Loss 8.019355, Accuracy 73.638%\n",
      "Epoch 12, Batch 379, LR 0.000083 Loss 8.019778, Accuracy 73.644%\n",
      "Epoch 12, Batch 380, LR 0.000083 Loss 8.018779, Accuracy 73.647%\n",
      "Epoch 12, Batch 381, LR 0.000083 Loss 8.018386, Accuracy 73.647%\n",
      "Epoch 12, Batch 382, LR 0.000083 Loss 8.019491, Accuracy 73.646%\n",
      "Epoch 12, Batch 383, LR 0.000083 Loss 8.020123, Accuracy 73.644%\n",
      "Epoch 12, Batch 384, LR 0.000083 Loss 8.021715, Accuracy 73.639%\n",
      "Epoch 12, Batch 385, LR 0.000083 Loss 8.022519, Accuracy 73.626%\n",
      "Epoch 12, Batch 386, LR 0.000083 Loss 8.023327, Accuracy 73.614%\n",
      "Epoch 12, Batch 387, LR 0.000083 Loss 8.026248, Accuracy 73.607%\n",
      "Epoch 12, Batch 388, LR 0.000083 Loss 8.025651, Accuracy 73.613%\n",
      "Epoch 12, Batch 389, LR 0.000083 Loss 8.024153, Accuracy 73.624%\n",
      "Epoch 12, Batch 390, LR 0.000083 Loss 8.026533, Accuracy 73.612%\n",
      "Epoch 12, Batch 391, LR 0.000083 Loss 8.028025, Accuracy 73.601%\n",
      "Epoch 12, Batch 392, LR 0.000083 Loss 8.025718, Accuracy 73.609%\n",
      "Epoch 12, Batch 393, LR 0.000083 Loss 8.024938, Accuracy 73.616%\n",
      "Epoch 12, Batch 394, LR 0.000083 Loss 8.026002, Accuracy 73.606%\n",
      "Epoch 12, Batch 395, LR 0.000083 Loss 8.024604, Accuracy 73.606%\n",
      "Epoch 12, Batch 396, LR 0.000083 Loss 8.024285, Accuracy 73.601%\n",
      "Epoch 12, Batch 397, LR 0.000083 Loss 8.021089, Accuracy 73.628%\n",
      "Epoch 12, Batch 398, LR 0.000083 Loss 8.022770, Accuracy 73.620%\n",
      "Epoch 12, Batch 399, LR 0.000083 Loss 8.024262, Accuracy 73.610%\n",
      "Epoch 12, Batch 400, LR 0.000083 Loss 8.022399, Accuracy 73.623%\n",
      "Epoch 12, Batch 401, LR 0.000083 Loss 8.022835, Accuracy 73.621%\n",
      "Epoch 12, Batch 402, LR 0.000083 Loss 8.023776, Accuracy 73.616%\n",
      "Epoch 12, Batch 403, LR 0.000083 Loss 8.023675, Accuracy 73.620%\n",
      "Epoch 12, Batch 404, LR 0.000083 Loss 8.024121, Accuracy 73.621%\n",
      "Epoch 12, Batch 405, LR 0.000083 Loss 8.022024, Accuracy 73.632%\n",
      "Epoch 12, Batch 406, LR 0.000083 Loss 8.022434, Accuracy 73.632%\n",
      "Epoch 12, Batch 407, LR 0.000083 Loss 8.021935, Accuracy 73.631%\n",
      "Epoch 12, Batch 408, LR 0.000083 Loss 8.021185, Accuracy 73.637%\n",
      "Epoch 12, Batch 409, LR 0.000083 Loss 8.020897, Accuracy 73.640%\n",
      "Epoch 12, Batch 410, LR 0.000083 Loss 8.019822, Accuracy 73.638%\n",
      "Epoch 12, Batch 411, LR 0.000083 Loss 8.021477, Accuracy 73.635%\n",
      "Epoch 12, Batch 412, LR 0.000083 Loss 8.020759, Accuracy 73.639%\n",
      "Epoch 12, Batch 413, LR 0.000083 Loss 8.020140, Accuracy 73.632%\n",
      "Epoch 12, Batch 414, LR 0.000083 Loss 8.020886, Accuracy 73.632%\n",
      "Epoch 12, Batch 415, LR 0.000083 Loss 8.021708, Accuracy 73.626%\n",
      "Epoch 12, Batch 416, LR 0.000083 Loss 8.021403, Accuracy 73.633%\n",
      "Epoch 12, Batch 417, LR 0.000083 Loss 8.020700, Accuracy 73.647%\n",
      "Epoch 12, Batch 418, LR 0.000083 Loss 8.021127, Accuracy 73.647%\n",
      "Epoch 12, Batch 419, LR 0.000083 Loss 8.020639, Accuracy 73.648%\n",
      "Epoch 12, Batch 420, LR 0.000083 Loss 8.021949, Accuracy 73.640%\n",
      "Epoch 12, Batch 421, LR 0.000083 Loss 8.024330, Accuracy 73.621%\n",
      "Epoch 12, Batch 422, LR 0.000083 Loss 8.023464, Accuracy 73.612%\n",
      "Epoch 12, Batch 423, LR 0.000083 Loss 8.022412, Accuracy 73.622%\n",
      "Epoch 12, Batch 424, LR 0.000083 Loss 8.023224, Accuracy 73.614%\n",
      "Epoch 12, Batch 425, LR 0.000083 Loss 8.023686, Accuracy 73.625%\n",
      "Epoch 12, Batch 426, LR 0.000083 Loss 8.024430, Accuracy 73.619%\n",
      "Epoch 12, Batch 427, LR 0.000083 Loss 8.022938, Accuracy 73.639%\n",
      "Epoch 12, Batch 428, LR 0.000083 Loss 8.022581, Accuracy 73.629%\n",
      "Epoch 12, Batch 429, LR 0.000083 Loss 8.021450, Accuracy 73.638%\n",
      "Epoch 12, Batch 430, LR 0.000083 Loss 8.020600, Accuracy 73.648%\n",
      "Epoch 12, Batch 431, LR 0.000083 Loss 8.020280, Accuracy 73.657%\n",
      "Epoch 12, Batch 432, LR 0.000083 Loss 8.019508, Accuracy 73.662%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 433, LR 0.000083 Loss 8.019559, Accuracy 73.670%\n",
      "Epoch 12, Batch 434, LR 0.000083 Loss 8.019518, Accuracy 73.673%\n",
      "Epoch 12, Batch 435, LR 0.000083 Loss 8.021395, Accuracy 73.658%\n",
      "Epoch 12, Batch 436, LR 0.000083 Loss 8.020629, Accuracy 73.653%\n",
      "Epoch 12, Batch 437, LR 0.000083 Loss 8.020564, Accuracy 73.648%\n",
      "Epoch 12, Batch 438, LR 0.000083 Loss 8.019933, Accuracy 73.652%\n",
      "Epoch 12, Batch 439, LR 0.000083 Loss 8.019636, Accuracy 73.647%\n",
      "Epoch 12, Batch 440, LR 0.000083 Loss 8.020897, Accuracy 73.645%\n",
      "Epoch 12, Batch 441, LR 0.000083 Loss 8.018837, Accuracy 73.666%\n",
      "Epoch 12, Batch 442, LR 0.000083 Loss 8.018284, Accuracy 73.676%\n",
      "Epoch 12, Batch 443, LR 0.000083 Loss 8.020034, Accuracy 73.669%\n",
      "Epoch 12, Batch 444, LR 0.000083 Loss 8.021229, Accuracy 73.666%\n",
      "Epoch 12, Batch 445, LR 0.000083 Loss 8.022180, Accuracy 73.660%\n",
      "Epoch 12, Batch 446, LR 0.000083 Loss 8.022674, Accuracy 73.644%\n",
      "Epoch 12, Batch 447, LR 0.000083 Loss 8.023256, Accuracy 73.651%\n",
      "Epoch 12, Batch 448, LR 0.000083 Loss 8.022758, Accuracy 73.655%\n",
      "Epoch 12, Batch 449, LR 0.000083 Loss 8.022012, Accuracy 73.650%\n",
      "Epoch 12, Batch 450, LR 0.000083 Loss 8.021131, Accuracy 73.648%\n",
      "Epoch 12, Batch 451, LR 0.000083 Loss 8.022745, Accuracy 73.642%\n",
      "Epoch 12, Batch 452, LR 0.000083 Loss 8.022571, Accuracy 73.648%\n",
      "Epoch 12, Batch 453, LR 0.000083 Loss 8.022294, Accuracy 73.639%\n",
      "Epoch 12, Batch 454, LR 0.000083 Loss 8.020445, Accuracy 73.659%\n",
      "Epoch 12, Batch 455, LR 0.000083 Loss 8.019494, Accuracy 73.666%\n",
      "Epoch 12, Batch 456, LR 0.000083 Loss 8.018034, Accuracy 73.677%\n",
      "Epoch 12, Batch 457, LR 0.000083 Loss 8.018190, Accuracy 73.691%\n",
      "Epoch 12, Batch 458, LR 0.000083 Loss 8.018486, Accuracy 73.685%\n",
      "Epoch 12, Batch 459, LR 0.000083 Loss 8.017695, Accuracy 73.698%\n",
      "Epoch 12, Batch 460, LR 0.000083 Loss 8.016803, Accuracy 73.702%\n",
      "Epoch 12, Batch 461, LR 0.000083 Loss 8.017016, Accuracy 73.697%\n",
      "Epoch 12, Batch 462, LR 0.000083 Loss 8.017303, Accuracy 73.693%\n",
      "Epoch 12, Batch 463, LR 0.000082 Loss 8.018116, Accuracy 73.682%\n",
      "Epoch 12, Batch 464, LR 0.000082 Loss 8.018306, Accuracy 73.677%\n",
      "Epoch 12, Batch 465, LR 0.000082 Loss 8.019145, Accuracy 73.674%\n",
      "Epoch 12, Batch 466, LR 0.000082 Loss 8.019216, Accuracy 73.676%\n",
      "Epoch 12, Batch 467, LR 0.000082 Loss 8.017541, Accuracy 73.683%\n",
      "Epoch 12, Batch 468, LR 0.000082 Loss 8.017556, Accuracy 73.681%\n",
      "Epoch 12, Batch 469, LR 0.000082 Loss 8.017741, Accuracy 73.677%\n",
      "Epoch 12, Batch 470, LR 0.000082 Loss 8.017183, Accuracy 73.674%\n",
      "Epoch 12, Batch 471, LR 0.000082 Loss 8.015116, Accuracy 73.680%\n",
      "Epoch 12, Batch 472, LR 0.000082 Loss 8.014287, Accuracy 73.682%\n",
      "Epoch 12, Batch 473, LR 0.000082 Loss 8.013758, Accuracy 73.702%\n",
      "Epoch 12, Batch 474, LR 0.000082 Loss 8.015013, Accuracy 73.709%\n",
      "Epoch 12, Batch 475, LR 0.000082 Loss 8.015756, Accuracy 73.709%\n",
      "Epoch 12, Batch 476, LR 0.000082 Loss 8.015403, Accuracy 73.707%\n",
      "Epoch 12, Batch 477, LR 0.000082 Loss 8.016911, Accuracy 73.700%\n",
      "Epoch 12, Batch 478, LR 0.000082 Loss 8.017537, Accuracy 73.701%\n",
      "Epoch 12, Batch 479, LR 0.000082 Loss 8.017562, Accuracy 73.712%\n",
      "Epoch 12, Batch 480, LR 0.000082 Loss 8.017449, Accuracy 73.716%\n",
      "Epoch 12, Batch 481, LR 0.000082 Loss 8.017466, Accuracy 73.714%\n",
      "Epoch 12, Batch 482, LR 0.000082 Loss 8.015980, Accuracy 73.720%\n",
      "Epoch 12, Batch 483, LR 0.000082 Loss 8.015727, Accuracy 73.721%\n",
      "Epoch 12, Batch 484, LR 0.000082 Loss 8.014468, Accuracy 73.717%\n",
      "Epoch 12, Batch 485, LR 0.000082 Loss 8.012681, Accuracy 73.734%\n",
      "Epoch 12, Batch 486, LR 0.000082 Loss 8.012916, Accuracy 73.736%\n",
      "Epoch 12, Batch 487, LR 0.000082 Loss 8.013567, Accuracy 73.731%\n",
      "Epoch 12, Batch 488, LR 0.000082 Loss 8.013439, Accuracy 73.727%\n",
      "Epoch 12, Batch 489, LR 0.000082 Loss 8.013053, Accuracy 73.723%\n",
      "Epoch 12, Batch 490, LR 0.000082 Loss 8.013412, Accuracy 73.721%\n",
      "Epoch 12, Batch 491, LR 0.000082 Loss 8.013649, Accuracy 73.718%\n",
      "Epoch 12, Batch 492, LR 0.000082 Loss 8.013143, Accuracy 73.719%\n",
      "Epoch 12, Batch 493, LR 0.000082 Loss 8.013906, Accuracy 73.712%\n",
      "Epoch 12, Batch 494, LR 0.000082 Loss 8.013810, Accuracy 73.710%\n",
      "Epoch 12, Batch 495, LR 0.000082 Loss 8.013810, Accuracy 73.707%\n",
      "Epoch 12, Batch 496, LR 0.000082 Loss 8.013751, Accuracy 73.712%\n",
      "Epoch 12, Batch 497, LR 0.000082 Loss 8.013750, Accuracy 73.700%\n",
      "Epoch 12, Batch 498, LR 0.000082 Loss 8.012561, Accuracy 73.707%\n",
      "Epoch 12, Batch 499, LR 0.000082 Loss 8.014326, Accuracy 73.704%\n",
      "Epoch 12, Batch 500, LR 0.000082 Loss 8.013037, Accuracy 73.717%\n",
      "Epoch 12, Batch 501, LR 0.000082 Loss 8.012700, Accuracy 73.715%\n",
      "Epoch 12, Batch 502, LR 0.000082 Loss 8.011142, Accuracy 73.730%\n",
      "Epoch 12, Batch 503, LR 0.000082 Loss 8.010871, Accuracy 73.736%\n",
      "Epoch 12, Batch 504, LR 0.000082 Loss 8.009420, Accuracy 73.743%\n",
      "Epoch 12, Batch 505, LR 0.000082 Loss 8.009680, Accuracy 73.744%\n",
      "Epoch 12, Batch 506, LR 0.000082 Loss 8.009015, Accuracy 73.757%\n",
      "Epoch 12, Batch 507, LR 0.000082 Loss 8.010351, Accuracy 73.746%\n",
      "Epoch 12, Batch 508, LR 0.000082 Loss 8.010715, Accuracy 73.748%\n",
      "Epoch 12, Batch 509, LR 0.000082 Loss 8.010752, Accuracy 73.748%\n",
      "Epoch 12, Batch 510, LR 0.000082 Loss 8.011522, Accuracy 73.747%\n",
      "Epoch 12, Batch 511, LR 0.000082 Loss 8.011400, Accuracy 73.748%\n",
      "Epoch 12, Batch 512, LR 0.000082 Loss 8.010732, Accuracy 73.753%\n",
      "Epoch 12, Batch 513, LR 0.000082 Loss 8.012172, Accuracy 73.747%\n",
      "Epoch 12, Batch 514, LR 0.000082 Loss 8.013163, Accuracy 73.740%\n",
      "Epoch 12, Batch 515, LR 0.000082 Loss 8.013963, Accuracy 73.732%\n",
      "Epoch 12, Batch 516, LR 0.000082 Loss 8.014015, Accuracy 73.725%\n",
      "Epoch 12, Batch 517, LR 0.000082 Loss 8.015141, Accuracy 73.726%\n",
      "Epoch 12, Batch 518, LR 0.000082 Loss 8.014305, Accuracy 73.732%\n",
      "Epoch 12, Batch 519, LR 0.000082 Loss 8.015735, Accuracy 73.720%\n",
      "Epoch 12, Batch 520, LR 0.000082 Loss 8.014958, Accuracy 73.723%\n",
      "Epoch 12, Batch 521, LR 0.000082 Loss 8.014864, Accuracy 73.718%\n",
      "Epoch 12, Batch 522, LR 0.000082 Loss 8.015606, Accuracy 73.713%\n",
      "Epoch 12, Batch 523, LR 0.000082 Loss 8.015151, Accuracy 73.708%\n",
      "Epoch 12, Batch 524, LR 0.000082 Loss 8.013981, Accuracy 73.715%\n",
      "Epoch 12, Batch 525, LR 0.000082 Loss 8.013733, Accuracy 73.717%\n",
      "Epoch 12, Batch 526, LR 0.000082 Loss 8.013270, Accuracy 73.723%\n",
      "Epoch 12, Batch 527, LR 0.000082 Loss 8.012632, Accuracy 73.730%\n",
      "Epoch 12, Batch 528, LR 0.000082 Loss 8.013224, Accuracy 73.726%\n",
      "Epoch 12, Batch 529, LR 0.000082 Loss 8.013348, Accuracy 73.725%\n",
      "Epoch 12, Batch 530, LR 0.000082 Loss 8.012955, Accuracy 73.738%\n",
      "Epoch 12, Batch 531, LR 0.000082 Loss 8.011407, Accuracy 73.744%\n",
      "Epoch 12, Batch 532, LR 0.000082 Loss 8.010931, Accuracy 73.741%\n",
      "Epoch 12, Batch 533, LR 0.000082 Loss 8.011070, Accuracy 73.739%\n",
      "Epoch 12, Batch 534, LR 0.000082 Loss 8.010016, Accuracy 73.754%\n",
      "Epoch 12, Batch 535, LR 0.000082 Loss 8.008163, Accuracy 73.765%\n",
      "Epoch 12, Batch 536, LR 0.000082 Loss 8.007524, Accuracy 73.765%\n",
      "Epoch 12, Batch 537, LR 0.000082 Loss 8.009072, Accuracy 73.749%\n",
      "Epoch 12, Batch 538, LR 0.000082 Loss 8.008886, Accuracy 73.753%\n",
      "Epoch 12, Batch 539, LR 0.000082 Loss 8.009534, Accuracy 73.745%\n",
      "Epoch 12, Batch 540, LR 0.000082 Loss 8.010038, Accuracy 73.744%\n",
      "Epoch 12, Batch 541, LR 0.000082 Loss 8.009456, Accuracy 73.744%\n",
      "Epoch 12, Batch 542, LR 0.000082 Loss 8.009581, Accuracy 73.750%\n",
      "Epoch 12, Batch 543, LR 0.000082 Loss 8.008068, Accuracy 73.750%\n",
      "Epoch 12, Batch 544, LR 0.000082 Loss 8.008972, Accuracy 73.746%\n",
      "Epoch 12, Batch 545, LR 0.000082 Loss 8.008803, Accuracy 73.749%\n",
      "Epoch 12, Batch 546, LR 0.000082 Loss 8.011795, Accuracy 73.731%\n",
      "Epoch 12, Batch 547, LR 0.000082 Loss 8.012066, Accuracy 73.727%\n",
      "Epoch 12, Batch 548, LR 0.000082 Loss 8.011052, Accuracy 73.730%\n",
      "Epoch 12, Batch 549, LR 0.000082 Loss 8.011957, Accuracy 73.718%\n",
      "Epoch 12, Batch 550, LR 0.000082 Loss 8.012917, Accuracy 73.714%\n",
      "Epoch 12, Batch 551, LR 0.000082 Loss 8.012342, Accuracy 73.718%\n",
      "Epoch 12, Batch 552, LR 0.000082 Loss 8.013697, Accuracy 73.721%\n",
      "Epoch 12, Batch 553, LR 0.000082 Loss 8.012648, Accuracy 73.727%\n",
      "Epoch 12, Batch 554, LR 0.000082 Loss 8.013449, Accuracy 73.722%\n",
      "Epoch 12, Batch 555, LR 0.000082 Loss 8.012069, Accuracy 73.733%\n",
      "Epoch 12, Batch 556, LR 0.000082 Loss 8.011786, Accuracy 73.737%\n",
      "Epoch 12, Batch 557, LR 0.000082 Loss 8.011245, Accuracy 73.739%\n",
      "Epoch 12, Batch 558, LR 0.000082 Loss 8.008646, Accuracy 73.751%\n",
      "Epoch 12, Batch 559, LR 0.000082 Loss 8.010069, Accuracy 73.742%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 560, LR 0.000082 Loss 8.008642, Accuracy 73.753%\n",
      "Epoch 12, Batch 561, LR 0.000082 Loss 8.009867, Accuracy 73.741%\n",
      "Epoch 12, Batch 562, LR 0.000082 Loss 8.010415, Accuracy 73.743%\n",
      "Epoch 12, Batch 563, LR 0.000082 Loss 8.010668, Accuracy 73.736%\n",
      "Epoch 12, Batch 564, LR 0.000082 Loss 8.010508, Accuracy 73.737%\n",
      "Epoch 12, Batch 565, LR 0.000082 Loss 8.010609, Accuracy 73.728%\n",
      "Epoch 12, Batch 566, LR 0.000082 Loss 8.009581, Accuracy 73.736%\n",
      "Epoch 12, Batch 567, LR 0.000082 Loss 8.009748, Accuracy 73.732%\n",
      "Epoch 12, Batch 568, LR 0.000082 Loss 8.007640, Accuracy 73.743%\n",
      "Epoch 12, Batch 569, LR 0.000082 Loss 8.005314, Accuracy 73.752%\n",
      "Epoch 12, Batch 570, LR 0.000082 Loss 8.004073, Accuracy 73.760%\n",
      "Epoch 12, Batch 571, LR 0.000082 Loss 8.002654, Accuracy 73.763%\n",
      "Epoch 12, Batch 572, LR 0.000082 Loss 8.000922, Accuracy 73.771%\n",
      "Epoch 12, Batch 573, LR 0.000082 Loss 8.002446, Accuracy 73.759%\n",
      "Epoch 12, Batch 574, LR 0.000082 Loss 8.002568, Accuracy 73.756%\n",
      "Epoch 12, Batch 575, LR 0.000082 Loss 8.001948, Accuracy 73.769%\n",
      "Epoch 12, Batch 576, LR 0.000082 Loss 8.002404, Accuracy 73.762%\n",
      "Epoch 12, Batch 577, LR 0.000082 Loss 8.001685, Accuracy 73.765%\n",
      "Epoch 12, Batch 578, LR 0.000082 Loss 8.000494, Accuracy 73.778%\n",
      "Epoch 12, Batch 579, LR 0.000082 Loss 8.002444, Accuracy 73.765%\n",
      "Epoch 12, Batch 580, LR 0.000082 Loss 8.002779, Accuracy 73.765%\n",
      "Epoch 12, Batch 581, LR 0.000082 Loss 8.002673, Accuracy 73.753%\n",
      "Epoch 12, Batch 582, LR 0.000082 Loss 8.001620, Accuracy 73.758%\n",
      "Epoch 12, Batch 583, LR 0.000082 Loss 8.001485, Accuracy 73.759%\n",
      "Epoch 12, Batch 584, LR 0.000082 Loss 8.000514, Accuracy 73.768%\n",
      "Epoch 12, Batch 585, LR 0.000082 Loss 8.001620, Accuracy 73.766%\n",
      "Epoch 12, Batch 586, LR 0.000082 Loss 8.001974, Accuracy 73.768%\n",
      "Epoch 12, Batch 587, LR 0.000082 Loss 8.001289, Accuracy 73.772%\n",
      "Epoch 12, Batch 588, LR 0.000082 Loss 8.001462, Accuracy 73.768%\n",
      "Epoch 12, Batch 589, LR 0.000082 Loss 8.002699, Accuracy 73.765%\n",
      "Epoch 12, Batch 590, LR 0.000082 Loss 8.003318, Accuracy 73.761%\n",
      "Epoch 12, Batch 591, LR 0.000082 Loss 8.003190, Accuracy 73.768%\n",
      "Epoch 12, Batch 592, LR 0.000082 Loss 8.003644, Accuracy 73.769%\n",
      "Epoch 12, Batch 593, LR 0.000082 Loss 8.003653, Accuracy 73.769%\n",
      "Epoch 12, Batch 594, LR 0.000082 Loss 8.005516, Accuracy 73.760%\n",
      "Epoch 12, Batch 595, LR 0.000082 Loss 8.004653, Accuracy 73.766%\n",
      "Epoch 12, Batch 596, LR 0.000082 Loss 8.004317, Accuracy 73.760%\n",
      "Epoch 12, Batch 597, LR 0.000082 Loss 8.005349, Accuracy 73.759%\n",
      "Epoch 12, Batch 598, LR 0.000082 Loss 8.005872, Accuracy 73.759%\n",
      "Epoch 12, Batch 599, LR 0.000082 Loss 8.004608, Accuracy 73.762%\n",
      "Epoch 12, Batch 600, LR 0.000082 Loss 8.003442, Accuracy 73.764%\n",
      "Epoch 12, Batch 601, LR 0.000082 Loss 8.004395, Accuracy 73.755%\n",
      "Epoch 12, Batch 602, LR 0.000082 Loss 8.005100, Accuracy 73.754%\n",
      "Epoch 12, Batch 603, LR 0.000082 Loss 8.004731, Accuracy 73.755%\n",
      "Epoch 12, Batch 604, LR 0.000082 Loss 8.004544, Accuracy 73.760%\n",
      "Epoch 12, Batch 605, LR 0.000082 Loss 8.004189, Accuracy 73.756%\n",
      "Epoch 12, Batch 606, LR 0.000082 Loss 8.004357, Accuracy 73.761%\n",
      "Epoch 12, Batch 607, LR 0.000082 Loss 8.004066, Accuracy 73.768%\n",
      "Epoch 12, Batch 608, LR 0.000082 Loss 8.005115, Accuracy 73.761%\n",
      "Epoch 12, Batch 609, LR 0.000082 Loss 8.005776, Accuracy 73.756%\n",
      "Epoch 12, Batch 610, LR 0.000082 Loss 8.005938, Accuracy 73.753%\n",
      "Epoch 12, Batch 611, LR 0.000082 Loss 8.005643, Accuracy 73.760%\n",
      "Epoch 12, Batch 612, LR 0.000082 Loss 8.005543, Accuracy 73.762%\n",
      "Epoch 12, Batch 613, LR 0.000082 Loss 8.005929, Accuracy 73.761%\n",
      "Epoch 12, Batch 614, LR 0.000082 Loss 8.006316, Accuracy 73.763%\n",
      "Epoch 12, Batch 615, LR 0.000082 Loss 8.006232, Accuracy 73.774%\n",
      "Epoch 12, Batch 616, LR 0.000082 Loss 8.006206, Accuracy 73.776%\n",
      "Epoch 12, Batch 617, LR 0.000082 Loss 8.006940, Accuracy 73.772%\n",
      "Epoch 12, Batch 618, LR 0.000082 Loss 8.006912, Accuracy 73.773%\n",
      "Epoch 12, Batch 619, LR 0.000082 Loss 8.007101, Accuracy 73.771%\n",
      "Epoch 12, Batch 620, LR 0.000082 Loss 8.007133, Accuracy 73.771%\n",
      "Epoch 12, Batch 621, LR 0.000082 Loss 8.007203, Accuracy 73.768%\n",
      "Epoch 12, Batch 622, LR 0.000082 Loss 8.006545, Accuracy 73.774%\n",
      "Epoch 12, Batch 623, LR 0.000082 Loss 8.005494, Accuracy 73.782%\n",
      "Epoch 12, Batch 624, LR 0.000082 Loss 8.004596, Accuracy 73.792%\n",
      "Epoch 12, Batch 625, LR 0.000082 Loss 8.003840, Accuracy 73.801%\n",
      "Epoch 12, Batch 626, LR 0.000082 Loss 8.003948, Accuracy 73.801%\n",
      "Epoch 12, Batch 627, LR 0.000082 Loss 8.003126, Accuracy 73.808%\n",
      "Epoch 12, Batch 628, LR 0.000082 Loss 8.003793, Accuracy 73.808%\n",
      "Epoch 12, Batch 629, LR 0.000082 Loss 8.002997, Accuracy 73.806%\n",
      "Epoch 12, Batch 630, LR 0.000082 Loss 8.001671, Accuracy 73.814%\n",
      "Epoch 12, Batch 631, LR 0.000082 Loss 8.000848, Accuracy 73.816%\n",
      "Epoch 12, Batch 632, LR 0.000082 Loss 8.000904, Accuracy 73.821%\n",
      "Epoch 12, Batch 633, LR 0.000082 Loss 8.000534, Accuracy 73.815%\n",
      "Epoch 12, Batch 634, LR 0.000082 Loss 8.000911, Accuracy 73.813%\n",
      "Epoch 12, Batch 635, LR 0.000082 Loss 8.000135, Accuracy 73.823%\n",
      "Epoch 12, Batch 636, LR 0.000082 Loss 8.000355, Accuracy 73.826%\n",
      "Epoch 12, Batch 637, LR 0.000082 Loss 8.000171, Accuracy 73.816%\n",
      "Epoch 12, Batch 638, LR 0.000082 Loss 7.999561, Accuracy 73.829%\n",
      "Epoch 12, Batch 639, LR 0.000082 Loss 8.000107, Accuracy 73.819%\n",
      "Epoch 12, Batch 640, LR 0.000082 Loss 8.001457, Accuracy 73.812%\n",
      "Epoch 12, Batch 641, LR 0.000082 Loss 8.001752, Accuracy 73.813%\n",
      "Epoch 12, Batch 642, LR 0.000082 Loss 8.002531, Accuracy 73.809%\n",
      "Epoch 12, Batch 643, LR 0.000082 Loss 8.003163, Accuracy 73.806%\n",
      "Epoch 12, Batch 644, LR 0.000082 Loss 8.003841, Accuracy 73.803%\n",
      "Epoch 12, Batch 645, LR 0.000082 Loss 8.002952, Accuracy 73.812%\n",
      "Epoch 12, Batch 646, LR 0.000082 Loss 8.002643, Accuracy 73.814%\n",
      "Epoch 12, Batch 647, LR 0.000082 Loss 8.003796, Accuracy 73.808%\n",
      "Epoch 12, Batch 648, LR 0.000082 Loss 8.003612, Accuracy 73.808%\n",
      "Epoch 12, Batch 649, LR 0.000082 Loss 8.004201, Accuracy 73.796%\n",
      "Epoch 12, Batch 650, LR 0.000082 Loss 8.002928, Accuracy 73.803%\n",
      "Epoch 12, Batch 651, LR 0.000082 Loss 8.002526, Accuracy 73.811%\n",
      "Epoch 12, Batch 652, LR 0.000082 Loss 8.001247, Accuracy 73.819%\n",
      "Epoch 12, Batch 653, LR 0.000082 Loss 8.000564, Accuracy 73.825%\n",
      "Epoch 12, Batch 654, LR 0.000082 Loss 8.000287, Accuracy 73.828%\n",
      "Epoch 12, Batch 655, LR 0.000082 Loss 7.999647, Accuracy 73.828%\n",
      "Epoch 12, Batch 656, LR 0.000082 Loss 7.998857, Accuracy 73.835%\n",
      "Epoch 12, Batch 657, LR 0.000082 Loss 7.997740, Accuracy 73.839%\n",
      "Epoch 12, Batch 658, LR 0.000082 Loss 7.998604, Accuracy 73.835%\n",
      "Epoch 12, Batch 659, LR 0.000082 Loss 7.999019, Accuracy 73.831%\n",
      "Epoch 12, Batch 660, LR 0.000082 Loss 7.999377, Accuracy 73.827%\n",
      "Epoch 12, Batch 661, LR 0.000082 Loss 8.000201, Accuracy 73.828%\n",
      "Epoch 12, Batch 662, LR 0.000082 Loss 8.001448, Accuracy 73.819%\n",
      "Epoch 12, Batch 663, LR 0.000082 Loss 8.000873, Accuracy 73.822%\n",
      "Epoch 12, Batch 664, LR 0.000082 Loss 8.001368, Accuracy 73.818%\n",
      "Epoch 12, Batch 665, LR 0.000082 Loss 8.001650, Accuracy 73.813%\n",
      "Epoch 12, Batch 666, LR 0.000082 Loss 8.002521, Accuracy 73.811%\n",
      "Epoch 12, Batch 667, LR 0.000082 Loss 8.002279, Accuracy 73.815%\n",
      "Epoch 12, Batch 668, LR 0.000082 Loss 8.001743, Accuracy 73.819%\n",
      "Epoch 12, Batch 669, LR 0.000082 Loss 8.001880, Accuracy 73.822%\n",
      "Epoch 12, Batch 670, LR 0.000082 Loss 8.001717, Accuracy 73.829%\n",
      "Epoch 12, Batch 671, LR 0.000082 Loss 8.001973, Accuracy 73.832%\n",
      "Epoch 12, Batch 672, LR 0.000082 Loss 8.002119, Accuracy 73.835%\n",
      "Epoch 12, Batch 673, LR 0.000082 Loss 8.000796, Accuracy 73.839%\n",
      "Epoch 12, Batch 674, LR 0.000082 Loss 8.000616, Accuracy 73.840%\n",
      "Epoch 12, Batch 675, LR 0.000082 Loss 8.000907, Accuracy 73.841%\n",
      "Epoch 12, Batch 676, LR 0.000082 Loss 8.001746, Accuracy 73.835%\n",
      "Epoch 12, Batch 677, LR 0.000082 Loss 8.001105, Accuracy 73.839%\n",
      "Epoch 12, Batch 678, LR 0.000082 Loss 8.001413, Accuracy 73.834%\n",
      "Epoch 12, Batch 679, LR 0.000082 Loss 8.000075, Accuracy 73.840%\n",
      "Epoch 12, Batch 680, LR 0.000082 Loss 7.999499, Accuracy 73.843%\n",
      "Epoch 12, Batch 681, LR 0.000082 Loss 8.000784, Accuracy 73.840%\n",
      "Epoch 12, Batch 682, LR 0.000082 Loss 8.000207, Accuracy 73.835%\n",
      "Epoch 12, Batch 683, LR 0.000082 Loss 8.000782, Accuracy 73.832%\n",
      "Epoch 12, Batch 684, LR 0.000082 Loss 8.000410, Accuracy 73.832%\n",
      "Epoch 12, Batch 685, LR 0.000082 Loss 7.999909, Accuracy 73.831%\n",
      "Epoch 12, Batch 686, LR 0.000082 Loss 7.998867, Accuracy 73.836%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 687, LR 0.000082 Loss 7.998595, Accuracy 73.839%\n",
      "Epoch 12, Batch 688, LR 0.000082 Loss 7.997328, Accuracy 73.849%\n",
      "Epoch 12, Batch 689, LR 0.000082 Loss 7.997809, Accuracy 73.848%\n",
      "Epoch 12, Batch 690, LR 0.000082 Loss 7.996182, Accuracy 73.859%\n",
      "Epoch 12, Batch 691, LR 0.000082 Loss 7.995757, Accuracy 73.861%\n",
      "Epoch 12, Batch 692, LR 0.000082 Loss 7.994914, Accuracy 73.868%\n",
      "Epoch 12, Batch 693, LR 0.000082 Loss 7.993744, Accuracy 73.869%\n",
      "Epoch 12, Batch 694, LR 0.000082 Loss 7.994436, Accuracy 73.862%\n",
      "Epoch 12, Batch 695, LR 0.000082 Loss 7.995295, Accuracy 73.855%\n",
      "Epoch 12, Batch 696, LR 0.000082 Loss 7.995585, Accuracy 73.846%\n",
      "Epoch 12, Batch 697, LR 0.000082 Loss 7.994777, Accuracy 73.848%\n",
      "Epoch 12, Batch 698, LR 0.000082 Loss 7.994018, Accuracy 73.852%\n",
      "Epoch 12, Batch 699, LR 0.000082 Loss 7.993244, Accuracy 73.859%\n",
      "Epoch 12, Batch 700, LR 0.000082 Loss 7.992884, Accuracy 73.864%\n",
      "Epoch 12, Batch 701, LR 0.000082 Loss 7.992188, Accuracy 73.868%\n",
      "Epoch 12, Batch 702, LR 0.000082 Loss 7.992451, Accuracy 73.860%\n",
      "Epoch 12, Batch 703, LR 0.000082 Loss 7.993061, Accuracy 73.860%\n",
      "Epoch 12, Batch 704, LR 0.000082 Loss 7.992707, Accuracy 73.863%\n",
      "Epoch 12, Batch 705, LR 0.000082 Loss 7.991589, Accuracy 73.873%\n",
      "Epoch 12, Batch 706, LR 0.000082 Loss 7.990676, Accuracy 73.873%\n",
      "Epoch 12, Batch 707, LR 0.000082 Loss 7.991002, Accuracy 73.873%\n",
      "Epoch 12, Batch 708, LR 0.000082 Loss 7.990259, Accuracy 73.880%\n",
      "Epoch 12, Batch 709, LR 0.000082 Loss 7.989459, Accuracy 73.882%\n",
      "Epoch 12, Batch 710, LR 0.000082 Loss 7.989202, Accuracy 73.884%\n",
      "Epoch 12, Batch 711, LR 0.000082 Loss 7.988560, Accuracy 73.891%\n",
      "Epoch 12, Batch 712, LR 0.000082 Loss 7.988121, Accuracy 73.890%\n",
      "Epoch 12, Batch 713, LR 0.000082 Loss 7.989030, Accuracy 73.883%\n",
      "Epoch 12, Batch 714, LR 0.000082 Loss 7.988743, Accuracy 73.881%\n",
      "Epoch 12, Batch 715, LR 0.000082 Loss 7.987428, Accuracy 73.881%\n",
      "Epoch 12, Batch 716, LR 0.000082 Loss 7.985858, Accuracy 73.885%\n",
      "Epoch 12, Batch 717, LR 0.000082 Loss 7.985068, Accuracy 73.889%\n",
      "Epoch 12, Batch 718, LR 0.000082 Loss 7.985659, Accuracy 73.884%\n",
      "Epoch 12, Batch 719, LR 0.000082 Loss 7.985420, Accuracy 73.890%\n",
      "Epoch 12, Batch 720, LR 0.000082 Loss 7.985836, Accuracy 73.883%\n",
      "Epoch 12, Batch 721, LR 0.000082 Loss 7.985706, Accuracy 73.874%\n",
      "Epoch 12, Batch 722, LR 0.000082 Loss 7.984419, Accuracy 73.887%\n",
      "Epoch 12, Batch 723, LR 0.000082 Loss 7.983254, Accuracy 73.891%\n",
      "Epoch 12, Batch 724, LR 0.000082 Loss 7.983974, Accuracy 73.889%\n",
      "Epoch 12, Batch 725, LR 0.000082 Loss 7.982524, Accuracy 73.898%\n",
      "Epoch 12, Batch 726, LR 0.000082 Loss 7.981582, Accuracy 73.901%\n",
      "Epoch 12, Batch 727, LR 0.000082 Loss 7.981603, Accuracy 73.899%\n",
      "Epoch 12, Batch 728, LR 0.000082 Loss 7.980688, Accuracy 73.909%\n",
      "Epoch 12, Batch 729, LR 0.000082 Loss 7.980712, Accuracy 73.906%\n",
      "Epoch 12, Batch 730, LR 0.000082 Loss 7.980964, Accuracy 73.900%\n",
      "Epoch 12, Batch 731, LR 0.000082 Loss 7.981242, Accuracy 73.898%\n",
      "Epoch 12, Batch 732, LR 0.000082 Loss 7.980414, Accuracy 73.902%\n",
      "Epoch 12, Batch 733, LR 0.000082 Loss 7.981139, Accuracy 73.900%\n",
      "Epoch 12, Batch 734, LR 0.000082 Loss 7.982090, Accuracy 73.890%\n",
      "Epoch 12, Batch 735, LR 0.000082 Loss 7.982000, Accuracy 73.887%\n",
      "Epoch 12, Batch 736, LR 0.000082 Loss 7.982789, Accuracy 73.881%\n",
      "Epoch 12, Batch 737, LR 0.000082 Loss 7.982494, Accuracy 73.878%\n",
      "Epoch 12, Batch 738, LR 0.000082 Loss 7.981211, Accuracy 73.882%\n",
      "Epoch 12, Batch 739, LR 0.000082 Loss 7.980989, Accuracy 73.877%\n",
      "Epoch 12, Batch 740, LR 0.000082 Loss 7.980320, Accuracy 73.883%\n",
      "Epoch 12, Batch 741, LR 0.000082 Loss 7.980748, Accuracy 73.879%\n",
      "Epoch 12, Batch 742, LR 0.000082 Loss 7.979890, Accuracy 73.886%\n",
      "Epoch 12, Batch 743, LR 0.000082 Loss 7.980548, Accuracy 73.878%\n",
      "Epoch 12, Batch 744, LR 0.000082 Loss 7.980167, Accuracy 73.876%\n",
      "Epoch 12, Batch 745, LR 0.000082 Loss 7.980505, Accuracy 73.872%\n",
      "Epoch 12, Batch 746, LR 0.000082 Loss 7.979717, Accuracy 73.870%\n",
      "Epoch 12, Batch 747, LR 0.000082 Loss 7.980779, Accuracy 73.869%\n",
      "Epoch 12, Batch 748, LR 0.000082 Loss 7.981122, Accuracy 73.868%\n",
      "Epoch 12, Batch 749, LR 0.000082 Loss 7.980838, Accuracy 73.868%\n",
      "Epoch 12, Batch 750, LR 0.000082 Loss 7.981010, Accuracy 73.869%\n",
      "Epoch 12, Batch 751, LR 0.000082 Loss 7.981187, Accuracy 73.872%\n",
      "Epoch 12, Batch 752, LR 0.000082 Loss 7.981771, Accuracy 73.872%\n",
      "Epoch 12, Batch 753, LR 0.000082 Loss 7.982109, Accuracy 73.868%\n",
      "Epoch 12, Batch 754, LR 0.000082 Loss 7.980873, Accuracy 73.876%\n",
      "Epoch 12, Batch 755, LR 0.000082 Loss 7.980759, Accuracy 73.874%\n",
      "Epoch 12, Batch 756, LR 0.000082 Loss 7.981079, Accuracy 73.875%\n",
      "Epoch 12, Batch 757, LR 0.000082 Loss 7.980048, Accuracy 73.878%\n",
      "Epoch 12, Batch 758, LR 0.000082 Loss 7.979281, Accuracy 73.886%\n",
      "Epoch 12, Batch 759, LR 0.000082 Loss 7.980094, Accuracy 73.881%\n",
      "Epoch 12, Batch 760, LR 0.000082 Loss 7.979857, Accuracy 73.875%\n",
      "Epoch 12, Batch 761, LR 0.000082 Loss 7.979153, Accuracy 73.877%\n",
      "Epoch 12, Batch 762, LR 0.000082 Loss 7.978813, Accuracy 73.883%\n",
      "Epoch 12, Batch 763, LR 0.000082 Loss 7.978479, Accuracy 73.882%\n",
      "Epoch 12, Batch 764, LR 0.000082 Loss 7.978118, Accuracy 73.886%\n",
      "Epoch 12, Batch 765, LR 0.000082 Loss 7.978389, Accuracy 73.887%\n",
      "Epoch 12, Batch 766, LR 0.000082 Loss 7.977525, Accuracy 73.893%\n",
      "Epoch 12, Batch 767, LR 0.000082 Loss 7.978032, Accuracy 73.890%\n",
      "Epoch 12, Batch 768, LR 0.000082 Loss 7.978248, Accuracy 73.888%\n",
      "Epoch 12, Batch 769, LR 0.000082 Loss 7.978326, Accuracy 73.893%\n",
      "Epoch 12, Batch 770, LR 0.000082 Loss 7.978465, Accuracy 73.895%\n",
      "Epoch 12, Batch 771, LR 0.000082 Loss 7.979524, Accuracy 73.888%\n",
      "Epoch 12, Batch 772, LR 0.000082 Loss 7.979102, Accuracy 73.898%\n",
      "Epoch 12, Batch 773, LR 0.000082 Loss 7.978104, Accuracy 73.908%\n",
      "Epoch 12, Batch 774, LR 0.000082 Loss 7.978044, Accuracy 73.908%\n",
      "Epoch 12, Batch 775, LR 0.000082 Loss 7.977998, Accuracy 73.909%\n",
      "Epoch 12, Batch 776, LR 0.000082 Loss 7.978142, Accuracy 73.905%\n",
      "Epoch 12, Batch 777, LR 0.000082 Loss 7.978500, Accuracy 73.901%\n",
      "Epoch 12, Batch 778, LR 0.000082 Loss 7.978595, Accuracy 73.905%\n",
      "Epoch 12, Batch 779, LR 0.000082 Loss 7.978425, Accuracy 73.911%\n",
      "Epoch 12, Batch 780, LR 0.000082 Loss 7.977963, Accuracy 73.915%\n",
      "Epoch 12, Batch 781, LR 0.000082 Loss 7.979008, Accuracy 73.916%\n",
      "Epoch 12, Batch 782, LR 0.000082 Loss 7.978561, Accuracy 73.914%\n",
      "Epoch 12, Batch 783, LR 0.000082 Loss 7.977884, Accuracy 73.923%\n",
      "Epoch 12, Batch 784, LR 0.000082 Loss 7.978389, Accuracy 73.919%\n",
      "Epoch 12, Batch 785, LR 0.000082 Loss 7.977709, Accuracy 73.919%\n",
      "Epoch 12, Batch 786, LR 0.000082 Loss 7.976608, Accuracy 73.921%\n",
      "Epoch 12, Batch 787, LR 0.000082 Loss 7.976194, Accuracy 73.918%\n",
      "Epoch 12, Batch 788, LR 0.000082 Loss 7.977272, Accuracy 73.907%\n",
      "Epoch 12, Batch 789, LR 0.000082 Loss 7.976756, Accuracy 73.910%\n",
      "Epoch 12, Batch 790, LR 0.000082 Loss 7.977197, Accuracy 73.902%\n",
      "Epoch 12, Batch 791, LR 0.000082 Loss 7.975819, Accuracy 73.912%\n",
      "Epoch 12, Batch 792, LR 0.000081 Loss 7.976856, Accuracy 73.904%\n",
      "Epoch 12, Batch 793, LR 0.000081 Loss 7.976359, Accuracy 73.899%\n",
      "Epoch 12, Batch 794, LR 0.000081 Loss 7.975811, Accuracy 73.901%\n",
      "Epoch 12, Batch 795, LR 0.000081 Loss 7.976449, Accuracy 73.896%\n",
      "Epoch 12, Batch 796, LR 0.000081 Loss 7.976079, Accuracy 73.901%\n",
      "Epoch 12, Batch 797, LR 0.000081 Loss 7.975171, Accuracy 73.907%\n",
      "Epoch 12, Batch 798, LR 0.000081 Loss 7.975138, Accuracy 73.907%\n",
      "Epoch 12, Batch 799, LR 0.000081 Loss 7.975121, Accuracy 73.904%\n",
      "Epoch 12, Batch 800, LR 0.000081 Loss 7.975568, Accuracy 73.898%\n",
      "Epoch 12, Batch 801, LR 0.000081 Loss 7.974437, Accuracy 73.897%\n",
      "Epoch 12, Batch 802, LR 0.000081 Loss 7.973684, Accuracy 73.900%\n",
      "Epoch 12, Batch 803, LR 0.000081 Loss 7.974389, Accuracy 73.893%\n",
      "Epoch 12, Batch 804, LR 0.000081 Loss 7.974329, Accuracy 73.892%\n",
      "Epoch 12, Batch 805, LR 0.000081 Loss 7.974531, Accuracy 73.885%\n",
      "Epoch 12, Batch 806, LR 0.000081 Loss 7.975039, Accuracy 73.883%\n",
      "Epoch 12, Batch 807, LR 0.000081 Loss 7.974699, Accuracy 73.885%\n",
      "Epoch 12, Batch 808, LR 0.000081 Loss 7.975122, Accuracy 73.883%\n",
      "Epoch 12, Batch 809, LR 0.000081 Loss 7.975204, Accuracy 73.886%\n",
      "Epoch 12, Batch 810, LR 0.000081 Loss 7.975386, Accuracy 73.882%\n",
      "Epoch 12, Batch 811, LR 0.000081 Loss 7.975863, Accuracy 73.880%\n",
      "Epoch 12, Batch 812, LR 0.000081 Loss 7.975182, Accuracy 73.885%\n",
      "Epoch 12, Batch 813, LR 0.000081 Loss 7.973895, Accuracy 73.892%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 814, LR 0.000081 Loss 7.973952, Accuracy 73.892%\n",
      "Epoch 12, Batch 815, LR 0.000081 Loss 7.973541, Accuracy 73.899%\n",
      "Epoch 12, Batch 816, LR 0.000081 Loss 7.974466, Accuracy 73.893%\n",
      "Epoch 12, Batch 817, LR 0.000081 Loss 7.974322, Accuracy 73.897%\n",
      "Epoch 12, Batch 818, LR 0.000081 Loss 7.974067, Accuracy 73.893%\n",
      "Epoch 12, Batch 819, LR 0.000081 Loss 7.975367, Accuracy 73.884%\n",
      "Epoch 12, Batch 820, LR 0.000081 Loss 7.975343, Accuracy 73.884%\n",
      "Epoch 12, Batch 821, LR 0.000081 Loss 7.975562, Accuracy 73.881%\n",
      "Epoch 12, Batch 822, LR 0.000081 Loss 7.974987, Accuracy 73.883%\n",
      "Epoch 12, Batch 823, LR 0.000081 Loss 7.975132, Accuracy 73.880%\n",
      "Epoch 12, Batch 824, LR 0.000081 Loss 7.974475, Accuracy 73.883%\n",
      "Epoch 12, Batch 825, LR 0.000081 Loss 7.974396, Accuracy 73.888%\n",
      "Epoch 12, Batch 826, LR 0.000081 Loss 7.973989, Accuracy 73.892%\n",
      "Epoch 12, Batch 827, LR 0.000081 Loss 7.973324, Accuracy 73.892%\n",
      "Epoch 12, Batch 828, LR 0.000081 Loss 7.973321, Accuracy 73.896%\n",
      "Epoch 12, Batch 829, LR 0.000081 Loss 7.973445, Accuracy 73.897%\n",
      "Epoch 12, Batch 830, LR 0.000081 Loss 7.972816, Accuracy 73.902%\n",
      "Epoch 12, Batch 831, LR 0.000081 Loss 7.971842, Accuracy 73.909%\n",
      "Epoch 12, Batch 832, LR 0.000081 Loss 7.972329, Accuracy 73.902%\n",
      "Epoch 12, Batch 833, LR 0.000081 Loss 7.972128, Accuracy 73.901%\n",
      "Epoch 12, Batch 834, LR 0.000081 Loss 7.972523, Accuracy 73.897%\n",
      "Epoch 12, Batch 835, LR 0.000081 Loss 7.972582, Accuracy 73.897%\n",
      "Epoch 12, Batch 836, LR 0.000081 Loss 7.972400, Accuracy 73.897%\n",
      "Epoch 12, Batch 837, LR 0.000081 Loss 7.973155, Accuracy 73.900%\n",
      "Epoch 12, Batch 838, LR 0.000081 Loss 7.972255, Accuracy 73.902%\n",
      "Epoch 12, Batch 839, LR 0.000081 Loss 7.972607, Accuracy 73.900%\n",
      "Epoch 12, Batch 840, LR 0.000081 Loss 7.971774, Accuracy 73.904%\n",
      "Epoch 12, Batch 841, LR 0.000081 Loss 7.970849, Accuracy 73.909%\n",
      "Epoch 12, Batch 842, LR 0.000081 Loss 7.971181, Accuracy 73.908%\n",
      "Epoch 12, Batch 843, LR 0.000081 Loss 7.970575, Accuracy 73.912%\n",
      "Epoch 12, Batch 844, LR 0.000081 Loss 7.969793, Accuracy 73.915%\n",
      "Epoch 12, Batch 845, LR 0.000081 Loss 7.969984, Accuracy 73.912%\n",
      "Epoch 12, Batch 846, LR 0.000081 Loss 7.969864, Accuracy 73.914%\n",
      "Epoch 12, Batch 847, LR 0.000081 Loss 7.970524, Accuracy 73.912%\n",
      "Epoch 12, Batch 848, LR 0.000081 Loss 7.969996, Accuracy 73.907%\n",
      "Epoch 12, Batch 849, LR 0.000081 Loss 7.970496, Accuracy 73.908%\n",
      "Epoch 12, Batch 850, LR 0.000081 Loss 7.969737, Accuracy 73.914%\n",
      "Epoch 12, Batch 851, LR 0.000081 Loss 7.969692, Accuracy 73.919%\n",
      "Epoch 12, Batch 852, LR 0.000081 Loss 7.969313, Accuracy 73.923%\n",
      "Epoch 12, Batch 853, LR 0.000081 Loss 7.969592, Accuracy 73.917%\n",
      "Epoch 12, Batch 854, LR 0.000081 Loss 7.970248, Accuracy 73.914%\n",
      "Epoch 12, Batch 855, LR 0.000081 Loss 7.970211, Accuracy 73.914%\n",
      "Epoch 12, Batch 856, LR 0.000081 Loss 7.969927, Accuracy 73.916%\n",
      "Epoch 12, Batch 857, LR 0.000081 Loss 7.970667, Accuracy 73.915%\n",
      "Epoch 12, Batch 858, LR 0.000081 Loss 7.969606, Accuracy 73.918%\n",
      "Epoch 12, Batch 859, LR 0.000081 Loss 7.969120, Accuracy 73.918%\n",
      "Epoch 12, Batch 860, LR 0.000081 Loss 7.968458, Accuracy 73.921%\n",
      "Epoch 12, Batch 861, LR 0.000081 Loss 7.967977, Accuracy 73.926%\n",
      "Epoch 12, Batch 862, LR 0.000081 Loss 7.967861, Accuracy 73.925%\n",
      "Epoch 12, Batch 863, LR 0.000081 Loss 7.967730, Accuracy 73.925%\n",
      "Epoch 12, Batch 864, LR 0.000081 Loss 7.968318, Accuracy 73.924%\n",
      "Epoch 12, Batch 865, LR 0.000081 Loss 7.969214, Accuracy 73.916%\n",
      "Epoch 12, Batch 866, LR 0.000081 Loss 7.968926, Accuracy 73.921%\n",
      "Epoch 12, Batch 867, LR 0.000081 Loss 7.969030, Accuracy 73.919%\n",
      "Epoch 12, Batch 868, LR 0.000081 Loss 7.969143, Accuracy 73.919%\n",
      "Epoch 12, Batch 869, LR 0.000081 Loss 7.968843, Accuracy 73.924%\n",
      "Epoch 12, Batch 870, LR 0.000081 Loss 7.968431, Accuracy 73.920%\n",
      "Epoch 12, Batch 871, LR 0.000081 Loss 7.968158, Accuracy 73.920%\n",
      "Epoch 12, Batch 872, LR 0.000081 Loss 7.967991, Accuracy 73.920%\n",
      "Epoch 12, Batch 873, LR 0.000081 Loss 7.967705, Accuracy 73.924%\n",
      "Epoch 12, Batch 874, LR 0.000081 Loss 7.967955, Accuracy 73.921%\n",
      "Epoch 12, Batch 875, LR 0.000081 Loss 7.967103, Accuracy 73.923%\n",
      "Epoch 12, Batch 876, LR 0.000081 Loss 7.966450, Accuracy 73.928%\n",
      "Epoch 12, Batch 877, LR 0.000081 Loss 7.967655, Accuracy 73.922%\n",
      "Epoch 12, Batch 878, LR 0.000081 Loss 7.966839, Accuracy 73.924%\n",
      "Epoch 12, Batch 879, LR 0.000081 Loss 7.967241, Accuracy 73.927%\n",
      "Epoch 12, Batch 880, LR 0.000081 Loss 7.966733, Accuracy 73.928%\n",
      "Epoch 12, Batch 881, LR 0.000081 Loss 7.966453, Accuracy 73.933%\n",
      "Epoch 12, Batch 882, LR 0.000081 Loss 7.965979, Accuracy 73.935%\n",
      "Epoch 12, Batch 883, LR 0.000081 Loss 7.965960, Accuracy 73.933%\n",
      "Epoch 12, Batch 884, LR 0.000081 Loss 7.966695, Accuracy 73.929%\n",
      "Epoch 12, Batch 885, LR 0.000081 Loss 7.966733, Accuracy 73.928%\n",
      "Epoch 12, Batch 886, LR 0.000081 Loss 7.966170, Accuracy 73.930%\n",
      "Epoch 12, Batch 887, LR 0.000081 Loss 7.965249, Accuracy 73.936%\n",
      "Epoch 12, Batch 888, LR 0.000081 Loss 7.964440, Accuracy 73.940%\n",
      "Epoch 12, Batch 889, LR 0.000081 Loss 7.964347, Accuracy 73.940%\n",
      "Epoch 12, Batch 890, LR 0.000081 Loss 7.964678, Accuracy 73.939%\n",
      "Epoch 12, Batch 891, LR 0.000081 Loss 7.965062, Accuracy 73.940%\n",
      "Epoch 12, Batch 892, LR 0.000081 Loss 7.964788, Accuracy 73.944%\n",
      "Epoch 12, Batch 893, LR 0.000081 Loss 7.964288, Accuracy 73.949%\n",
      "Epoch 12, Batch 894, LR 0.000081 Loss 7.964582, Accuracy 73.950%\n",
      "Epoch 12, Batch 895, LR 0.000081 Loss 7.963139, Accuracy 73.957%\n",
      "Epoch 12, Batch 896, LR 0.000081 Loss 7.962530, Accuracy 73.956%\n",
      "Epoch 12, Batch 897, LR 0.000081 Loss 7.962823, Accuracy 73.952%\n",
      "Epoch 12, Batch 898, LR 0.000081 Loss 7.962854, Accuracy 73.956%\n",
      "Epoch 12, Batch 899, LR 0.000081 Loss 7.962142, Accuracy 73.960%\n",
      "Epoch 12, Batch 900, LR 0.000081 Loss 7.961138, Accuracy 73.966%\n",
      "Epoch 12, Batch 901, LR 0.000081 Loss 7.961468, Accuracy 73.966%\n",
      "Epoch 12, Batch 902, LR 0.000081 Loss 7.961925, Accuracy 73.962%\n",
      "Epoch 12, Batch 903, LR 0.000081 Loss 7.961569, Accuracy 73.964%\n",
      "Epoch 12, Batch 904, LR 0.000081 Loss 7.961237, Accuracy 73.968%\n",
      "Epoch 12, Batch 905, LR 0.000081 Loss 7.962122, Accuracy 73.961%\n",
      "Epoch 12, Batch 906, LR 0.000081 Loss 7.962912, Accuracy 73.954%\n",
      "Epoch 12, Batch 907, LR 0.000081 Loss 7.962407, Accuracy 73.950%\n",
      "Epoch 12, Batch 908, LR 0.000081 Loss 7.961949, Accuracy 73.960%\n",
      "Epoch 12, Batch 909, LR 0.000081 Loss 7.962278, Accuracy 73.953%\n",
      "Epoch 12, Batch 910, LR 0.000081 Loss 7.962612, Accuracy 73.953%\n",
      "Epoch 12, Batch 911, LR 0.000081 Loss 7.961168, Accuracy 73.962%\n",
      "Epoch 12, Batch 912, LR 0.000081 Loss 7.961154, Accuracy 73.965%\n",
      "Epoch 12, Batch 913, LR 0.000081 Loss 7.961173, Accuracy 73.967%\n",
      "Epoch 12, Batch 914, LR 0.000081 Loss 7.961141, Accuracy 73.967%\n",
      "Epoch 12, Batch 915, LR 0.000081 Loss 7.960861, Accuracy 73.969%\n",
      "Epoch 12, Batch 916, LR 0.000081 Loss 7.961039, Accuracy 73.970%\n",
      "Epoch 12, Batch 917, LR 0.000081 Loss 7.960942, Accuracy 73.966%\n",
      "Epoch 12, Batch 918, LR 0.000081 Loss 7.960186, Accuracy 73.975%\n",
      "Epoch 12, Batch 919, LR 0.000081 Loss 7.960984, Accuracy 73.970%\n",
      "Epoch 12, Batch 920, LR 0.000081 Loss 7.960877, Accuracy 73.972%\n",
      "Epoch 12, Batch 921, LR 0.000081 Loss 7.960835, Accuracy 73.973%\n",
      "Epoch 12, Batch 922, LR 0.000081 Loss 7.960939, Accuracy 73.975%\n",
      "Epoch 12, Batch 923, LR 0.000081 Loss 7.961499, Accuracy 73.971%\n",
      "Epoch 12, Batch 924, LR 0.000081 Loss 7.960728, Accuracy 73.974%\n",
      "Epoch 12, Batch 925, LR 0.000081 Loss 7.961084, Accuracy 73.966%\n",
      "Epoch 12, Batch 926, LR 0.000081 Loss 7.960825, Accuracy 73.966%\n",
      "Epoch 12, Batch 927, LR 0.000081 Loss 7.960865, Accuracy 73.970%\n",
      "Epoch 12, Batch 928, LR 0.000081 Loss 7.960968, Accuracy 73.965%\n",
      "Epoch 12, Batch 929, LR 0.000081 Loss 7.961510, Accuracy 73.962%\n",
      "Epoch 12, Batch 930, LR 0.000081 Loss 7.961085, Accuracy 73.963%\n",
      "Epoch 12, Batch 931, LR 0.000081 Loss 7.960770, Accuracy 73.963%\n",
      "Epoch 12, Batch 932, LR 0.000081 Loss 7.960841, Accuracy 73.959%\n",
      "Epoch 12, Batch 933, LR 0.000081 Loss 7.960331, Accuracy 73.960%\n",
      "Epoch 12, Batch 934, LR 0.000081 Loss 7.961222, Accuracy 73.948%\n",
      "Epoch 12, Batch 935, LR 0.000081 Loss 7.960693, Accuracy 73.951%\n",
      "Epoch 12, Batch 936, LR 0.000081 Loss 7.959722, Accuracy 73.959%\n",
      "Epoch 12, Batch 937, LR 0.000081 Loss 7.960149, Accuracy 73.959%\n",
      "Epoch 12, Batch 938, LR 0.000081 Loss 7.959709, Accuracy 73.964%\n",
      "Epoch 12, Batch 939, LR 0.000081 Loss 7.959275, Accuracy 73.964%\n",
      "Epoch 12, Batch 940, LR 0.000081 Loss 7.958661, Accuracy 73.968%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 941, LR 0.000081 Loss 7.959209, Accuracy 73.968%\n",
      "Epoch 12, Batch 942, LR 0.000081 Loss 7.958365, Accuracy 73.975%\n",
      "Epoch 12, Batch 943, LR 0.000081 Loss 7.959199, Accuracy 73.969%\n",
      "Epoch 12, Batch 944, LR 0.000081 Loss 7.958840, Accuracy 73.972%\n",
      "Epoch 12, Batch 945, LR 0.000081 Loss 7.958911, Accuracy 73.971%\n",
      "Epoch 12, Batch 946, LR 0.000081 Loss 7.958509, Accuracy 73.973%\n",
      "Epoch 12, Batch 947, LR 0.000081 Loss 7.958860, Accuracy 73.972%\n",
      "Epoch 12, Batch 948, LR 0.000081 Loss 7.958967, Accuracy 73.971%\n",
      "Epoch 12, Batch 949, LR 0.000081 Loss 7.959196, Accuracy 73.973%\n",
      "Epoch 12, Batch 950, LR 0.000081 Loss 7.959044, Accuracy 73.975%\n",
      "Epoch 12, Batch 951, LR 0.000081 Loss 7.959158, Accuracy 73.978%\n",
      "Epoch 12, Batch 952, LR 0.000081 Loss 7.959164, Accuracy 73.980%\n",
      "Epoch 12, Batch 953, LR 0.000081 Loss 7.958974, Accuracy 73.983%\n",
      "Epoch 12, Batch 954, LR 0.000081 Loss 7.959656, Accuracy 73.973%\n",
      "Epoch 12, Batch 955, LR 0.000081 Loss 7.960334, Accuracy 73.969%\n",
      "Epoch 12, Batch 956, LR 0.000081 Loss 7.960158, Accuracy 73.973%\n",
      "Epoch 12, Batch 957, LR 0.000081 Loss 7.959859, Accuracy 73.976%\n",
      "Epoch 12, Batch 958, LR 0.000081 Loss 7.959799, Accuracy 73.976%\n",
      "Epoch 12, Batch 959, LR 0.000081 Loss 7.958891, Accuracy 73.977%\n",
      "Epoch 12, Batch 960, LR 0.000081 Loss 7.959159, Accuracy 73.971%\n",
      "Epoch 12, Batch 961, LR 0.000081 Loss 7.958721, Accuracy 73.973%\n",
      "Epoch 12, Batch 962, LR 0.000081 Loss 7.958201, Accuracy 73.973%\n",
      "Epoch 12, Batch 963, LR 0.000081 Loss 7.957802, Accuracy 73.972%\n",
      "Epoch 12, Batch 964, LR 0.000081 Loss 7.957585, Accuracy 73.973%\n",
      "Epoch 12, Batch 965, LR 0.000081 Loss 7.957906, Accuracy 73.969%\n",
      "Epoch 12, Batch 966, LR 0.000081 Loss 7.957656, Accuracy 73.970%\n",
      "Epoch 12, Batch 967, LR 0.000081 Loss 7.957719, Accuracy 73.972%\n",
      "Epoch 12, Batch 968, LR 0.000081 Loss 7.957438, Accuracy 73.974%\n",
      "Epoch 12, Batch 969, LR 0.000081 Loss 7.957643, Accuracy 73.978%\n",
      "Epoch 12, Batch 970, LR 0.000081 Loss 7.957809, Accuracy 73.976%\n",
      "Epoch 12, Batch 971, LR 0.000081 Loss 7.957899, Accuracy 73.973%\n",
      "Epoch 12, Batch 972, LR 0.000081 Loss 7.958293, Accuracy 73.969%\n",
      "Epoch 12, Batch 973, LR 0.000081 Loss 7.957759, Accuracy 73.971%\n",
      "Epoch 12, Batch 974, LR 0.000081 Loss 7.957822, Accuracy 73.971%\n",
      "Epoch 12, Batch 975, LR 0.000081 Loss 7.957505, Accuracy 73.969%\n",
      "Epoch 12, Batch 976, LR 0.000081 Loss 7.957522, Accuracy 73.967%\n",
      "Epoch 12, Batch 977, LR 0.000081 Loss 7.957044, Accuracy 73.968%\n",
      "Epoch 12, Batch 978, LR 0.000081 Loss 7.956676, Accuracy 73.970%\n",
      "Epoch 12, Batch 979, LR 0.000081 Loss 7.956682, Accuracy 73.971%\n",
      "Epoch 12, Batch 980, LR 0.000081 Loss 7.956876, Accuracy 73.972%\n",
      "Epoch 12, Batch 981, LR 0.000081 Loss 7.957056, Accuracy 73.969%\n",
      "Epoch 12, Batch 982, LR 0.000081 Loss 7.956774, Accuracy 73.965%\n",
      "Epoch 12, Batch 983, LR 0.000081 Loss 7.957222, Accuracy 73.965%\n",
      "Epoch 12, Batch 984, LR 0.000081 Loss 7.957489, Accuracy 73.961%\n",
      "Epoch 12, Batch 985, LR 0.000081 Loss 7.957661, Accuracy 73.958%\n",
      "Epoch 12, Batch 986, LR 0.000081 Loss 7.958149, Accuracy 73.958%\n",
      "Epoch 12, Batch 987, LR 0.000081 Loss 7.958175, Accuracy 73.962%\n",
      "Epoch 12, Batch 988, LR 0.000081 Loss 7.957774, Accuracy 73.964%\n",
      "Epoch 12, Batch 989, LR 0.000081 Loss 7.958523, Accuracy 73.964%\n",
      "Epoch 12, Batch 990, LR 0.000081 Loss 7.958556, Accuracy 73.965%\n",
      "Epoch 12, Batch 991, LR 0.000081 Loss 7.958542, Accuracy 73.966%\n",
      "Epoch 12, Batch 992, LR 0.000081 Loss 7.958383, Accuracy 73.968%\n",
      "Epoch 12, Batch 993, LR 0.000081 Loss 7.958257, Accuracy 73.971%\n",
      "Epoch 12, Batch 994, LR 0.000081 Loss 7.957475, Accuracy 73.977%\n",
      "Epoch 12, Batch 995, LR 0.000081 Loss 7.956138, Accuracy 73.986%\n",
      "Epoch 12, Batch 996, LR 0.000081 Loss 7.955698, Accuracy 73.992%\n",
      "Epoch 12, Batch 997, LR 0.000081 Loss 7.955401, Accuracy 73.997%\n",
      "Epoch 12, Batch 998, LR 0.000081 Loss 7.954398, Accuracy 74.003%\n",
      "Epoch 12, Batch 999, LR 0.000081 Loss 7.953609, Accuracy 74.012%\n",
      "Epoch 12, Batch 1000, LR 0.000081 Loss 7.953513, Accuracy 74.014%\n",
      "Epoch 12, Batch 1001, LR 0.000081 Loss 7.953223, Accuracy 74.012%\n",
      "Epoch 12, Batch 1002, LR 0.000081 Loss 7.953022, Accuracy 74.011%\n",
      "Epoch 12, Batch 1003, LR 0.000081 Loss 7.952101, Accuracy 74.013%\n",
      "Epoch 12, Batch 1004, LR 0.000081 Loss 7.951535, Accuracy 74.018%\n",
      "Epoch 12, Batch 1005, LR 0.000081 Loss 7.951813, Accuracy 74.010%\n",
      "Epoch 12, Batch 1006, LR 0.000081 Loss 7.951982, Accuracy 74.008%\n",
      "Epoch 12, Batch 1007, LR 0.000081 Loss 7.952050, Accuracy 74.009%\n",
      "Epoch 12, Batch 1008, LR 0.000081 Loss 7.952203, Accuracy 74.009%\n",
      "Epoch 12, Batch 1009, LR 0.000081 Loss 7.952061, Accuracy 74.013%\n",
      "Epoch 12, Batch 1010, LR 0.000081 Loss 7.951726, Accuracy 74.012%\n",
      "Epoch 12, Batch 1011, LR 0.000081 Loss 7.952323, Accuracy 74.009%\n",
      "Epoch 12, Batch 1012, LR 0.000081 Loss 7.952998, Accuracy 74.005%\n",
      "Epoch 12, Batch 1013, LR 0.000081 Loss 7.952800, Accuracy 74.007%\n",
      "Epoch 12, Batch 1014, LR 0.000081 Loss 7.952086, Accuracy 74.012%\n",
      "Epoch 12, Batch 1015, LR 0.000081 Loss 7.951656, Accuracy 74.012%\n",
      "Epoch 12, Batch 1016, LR 0.000081 Loss 7.950870, Accuracy 74.020%\n",
      "Epoch 12, Batch 1017, LR 0.000081 Loss 7.951082, Accuracy 74.021%\n",
      "Epoch 12, Batch 1018, LR 0.000081 Loss 7.951088, Accuracy 74.023%\n",
      "Epoch 12, Batch 1019, LR 0.000081 Loss 7.951389, Accuracy 74.024%\n",
      "Epoch 12, Batch 1020, LR 0.000081 Loss 7.950778, Accuracy 74.023%\n",
      "Epoch 12, Batch 1021, LR 0.000081 Loss 7.951454, Accuracy 74.021%\n",
      "Epoch 12, Batch 1022, LR 0.000081 Loss 7.951648, Accuracy 74.020%\n",
      "Epoch 12, Batch 1023, LR 0.000081 Loss 7.950713, Accuracy 74.027%\n",
      "Epoch 12, Batch 1024, LR 0.000081 Loss 7.951521, Accuracy 74.023%\n",
      "Epoch 12, Batch 1025, LR 0.000081 Loss 7.952238, Accuracy 74.018%\n",
      "Epoch 12, Batch 1026, LR 0.000081 Loss 7.951313, Accuracy 74.022%\n",
      "Epoch 12, Batch 1027, LR 0.000081 Loss 7.951430, Accuracy 74.019%\n",
      "Epoch 12, Batch 1028, LR 0.000081 Loss 7.952292, Accuracy 74.011%\n",
      "Epoch 12, Batch 1029, LR 0.000081 Loss 7.952531, Accuracy 74.011%\n",
      "Epoch 12, Batch 1030, LR 0.000081 Loss 7.952095, Accuracy 74.018%\n",
      "Epoch 12, Batch 1031, LR 0.000081 Loss 7.952270, Accuracy 74.016%\n",
      "Epoch 12, Batch 1032, LR 0.000081 Loss 7.951561, Accuracy 74.023%\n",
      "Epoch 12, Batch 1033, LR 0.000081 Loss 7.950655, Accuracy 74.030%\n",
      "Epoch 12, Batch 1034, LR 0.000081 Loss 7.950224, Accuracy 74.030%\n",
      "Epoch 12, Batch 1035, LR 0.000081 Loss 7.950062, Accuracy 74.029%\n",
      "Epoch 12, Batch 1036, LR 0.000081 Loss 7.948776, Accuracy 74.035%\n",
      "Epoch 12, Batch 1037, LR 0.000081 Loss 7.949123, Accuracy 74.034%\n",
      "Epoch 12, Batch 1038, LR 0.000081 Loss 7.948899, Accuracy 74.037%\n",
      "Epoch 12, Batch 1039, LR 0.000081 Loss 7.948787, Accuracy 74.034%\n",
      "Epoch 12, Batch 1040, LR 0.000081 Loss 7.948970, Accuracy 74.034%\n",
      "Epoch 12, Batch 1041, LR 0.000081 Loss 7.949146, Accuracy 74.037%\n",
      "Epoch 12, Batch 1042, LR 0.000081 Loss 7.948288, Accuracy 74.040%\n",
      "Epoch 12, Batch 1043, LR 0.000081 Loss 7.947803, Accuracy 74.042%\n",
      "Epoch 12, Batch 1044, LR 0.000081 Loss 7.947477, Accuracy 74.042%\n",
      "Epoch 12, Batch 1045, LR 0.000081 Loss 7.947477, Accuracy 74.044%\n",
      "Epoch 12, Batch 1046, LR 0.000081 Loss 7.947628, Accuracy 74.043%\n",
      "Epoch 12, Batch 1047, LR 0.000081 Loss 7.946697, Accuracy 74.049%\n",
      "Epoch 12, Loss (train set) 7.946697, Accuracy (train set) 74.049%\n",
      "Epoch 13, Batch 1, LR 0.000081 Loss 8.212623, Accuracy 71.094%\n",
      "Epoch 13, Batch 2, LR 0.000081 Loss 8.100458, Accuracy 72.656%\n",
      "Epoch 13, Batch 3, LR 0.000081 Loss 7.516339, Accuracy 77.083%\n",
      "Epoch 13, Batch 4, LR 0.000081 Loss 7.610575, Accuracy 76.953%\n",
      "Epoch 13, Batch 5, LR 0.000081 Loss 7.725939, Accuracy 76.406%\n",
      "Epoch 13, Batch 6, LR 0.000081 Loss 7.753600, Accuracy 75.260%\n",
      "Epoch 13, Batch 7, LR 0.000081 Loss 7.757386, Accuracy 75.558%\n",
      "Epoch 13, Batch 8, LR 0.000081 Loss 7.707896, Accuracy 75.684%\n",
      "Epoch 13, Batch 9, LR 0.000081 Loss 7.697757, Accuracy 75.781%\n",
      "Epoch 13, Batch 10, LR 0.000081 Loss 7.741280, Accuracy 75.234%\n",
      "Epoch 13, Batch 11, LR 0.000081 Loss 7.721614, Accuracy 75.426%\n",
      "Epoch 13, Batch 12, LR 0.000081 Loss 7.634118, Accuracy 76.237%\n",
      "Epoch 13, Batch 13, LR 0.000081 Loss 7.609620, Accuracy 76.562%\n",
      "Epoch 13, Batch 14, LR 0.000081 Loss 7.662336, Accuracy 76.060%\n",
      "Epoch 13, Batch 15, LR 0.000081 Loss 7.688272, Accuracy 75.938%\n",
      "Epoch 13, Batch 16, LR 0.000081 Loss 7.727627, Accuracy 75.342%\n",
      "Epoch 13, Batch 17, LR 0.000081 Loss 7.704136, Accuracy 75.460%\n",
      "Epoch 13, Batch 18, LR 0.000081 Loss 7.732778, Accuracy 75.564%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 19, LR 0.000081 Loss 7.764302, Accuracy 75.288%\n",
      "Epoch 13, Batch 20, LR 0.000081 Loss 7.773480, Accuracy 75.000%\n",
      "Epoch 13, Batch 21, LR 0.000081 Loss 7.771201, Accuracy 75.037%\n",
      "Epoch 13, Batch 22, LR 0.000081 Loss 7.746878, Accuracy 74.964%\n",
      "Epoch 13, Batch 23, LR 0.000081 Loss 7.762479, Accuracy 74.966%\n",
      "Epoch 13, Batch 24, LR 0.000081 Loss 7.744285, Accuracy 74.870%\n",
      "Epoch 13, Batch 25, LR 0.000081 Loss 7.727228, Accuracy 75.094%\n",
      "Epoch 13, Batch 26, LR 0.000081 Loss 7.706920, Accuracy 75.210%\n",
      "Epoch 13, Batch 27, LR 0.000081 Loss 7.700532, Accuracy 75.289%\n",
      "Epoch 13, Batch 28, LR 0.000081 Loss 7.703061, Accuracy 75.419%\n",
      "Epoch 13, Batch 29, LR 0.000081 Loss 7.710339, Accuracy 75.350%\n",
      "Epoch 13, Batch 30, LR 0.000081 Loss 7.722904, Accuracy 75.286%\n",
      "Epoch 13, Batch 31, LR 0.000081 Loss 7.720046, Accuracy 75.277%\n",
      "Epoch 13, Batch 32, LR 0.000081 Loss 7.706379, Accuracy 75.415%\n",
      "Epoch 13, Batch 33, LR 0.000081 Loss 7.715363, Accuracy 75.497%\n",
      "Epoch 13, Batch 34, LR 0.000081 Loss 7.713954, Accuracy 75.345%\n",
      "Epoch 13, Batch 35, LR 0.000081 Loss 7.697518, Accuracy 75.491%\n",
      "Epoch 13, Batch 36, LR 0.000081 Loss 7.707304, Accuracy 75.434%\n",
      "Epoch 13, Batch 37, LR 0.000081 Loss 7.689033, Accuracy 75.465%\n",
      "Epoch 13, Batch 38, LR 0.000081 Loss 7.702267, Accuracy 75.391%\n",
      "Epoch 13, Batch 39, LR 0.000081 Loss 7.699855, Accuracy 75.321%\n",
      "Epoch 13, Batch 40, LR 0.000081 Loss 7.667889, Accuracy 75.449%\n",
      "Epoch 13, Batch 41, LR 0.000081 Loss 7.653644, Accuracy 75.457%\n",
      "Epoch 13, Batch 42, LR 0.000081 Loss 7.650664, Accuracy 75.372%\n",
      "Epoch 13, Batch 43, LR 0.000081 Loss 7.639901, Accuracy 75.527%\n",
      "Epoch 13, Batch 44, LR 0.000081 Loss 7.641997, Accuracy 75.337%\n",
      "Epoch 13, Batch 45, LR 0.000081 Loss 7.633988, Accuracy 75.434%\n",
      "Epoch 13, Batch 46, LR 0.000081 Loss 7.635909, Accuracy 75.442%\n",
      "Epoch 13, Batch 47, LR 0.000081 Loss 7.639269, Accuracy 75.449%\n",
      "Epoch 13, Batch 48, LR 0.000081 Loss 7.619572, Accuracy 75.553%\n",
      "Epoch 13, Batch 49, LR 0.000081 Loss 7.623004, Accuracy 75.574%\n",
      "Epoch 13, Batch 50, LR 0.000081 Loss 7.630551, Accuracy 75.469%\n",
      "Epoch 13, Batch 51, LR 0.000081 Loss 7.641693, Accuracy 75.398%\n",
      "Epoch 13, Batch 52, LR 0.000081 Loss 7.654760, Accuracy 75.331%\n",
      "Epoch 13, Batch 53, LR 0.000081 Loss 7.652937, Accuracy 75.369%\n",
      "Epoch 13, Batch 54, LR 0.000081 Loss 7.661517, Accuracy 75.362%\n",
      "Epoch 13, Batch 55, LR 0.000081 Loss 7.654281, Accuracy 75.483%\n",
      "Epoch 13, Batch 56, LR 0.000081 Loss 7.653378, Accuracy 75.474%\n",
      "Epoch 13, Batch 57, LR 0.000081 Loss 7.662107, Accuracy 75.411%\n",
      "Epoch 13, Batch 58, LR 0.000081 Loss 7.673655, Accuracy 75.350%\n",
      "Epoch 13, Batch 59, LR 0.000081 Loss 7.673317, Accuracy 75.358%\n",
      "Epoch 13, Batch 60, LR 0.000081 Loss 7.657058, Accuracy 75.443%\n",
      "Epoch 13, Batch 61, LR 0.000081 Loss 7.657738, Accuracy 75.359%\n",
      "Epoch 13, Batch 62, LR 0.000081 Loss 7.648546, Accuracy 75.441%\n",
      "Epoch 13, Batch 63, LR 0.000081 Loss 7.651151, Accuracy 75.335%\n",
      "Epoch 13, Batch 64, LR 0.000081 Loss 7.660785, Accuracy 75.256%\n",
      "Epoch 13, Batch 65, LR 0.000081 Loss 7.647148, Accuracy 75.361%\n",
      "Epoch 13, Batch 66, LR 0.000081 Loss 7.651281, Accuracy 75.284%\n",
      "Epoch 13, Batch 67, LR 0.000081 Loss 7.670249, Accuracy 75.163%\n",
      "Epoch 13, Batch 68, LR 0.000080 Loss 7.666874, Accuracy 75.138%\n",
      "Epoch 13, Batch 69, LR 0.000080 Loss 7.664258, Accuracy 75.147%\n",
      "Epoch 13, Batch 70, LR 0.000080 Loss 7.662380, Accuracy 75.145%\n",
      "Epoch 13, Batch 71, LR 0.000080 Loss 7.666636, Accuracy 75.198%\n",
      "Epoch 13, Batch 72, LR 0.000080 Loss 7.661982, Accuracy 75.271%\n",
      "Epoch 13, Batch 73, LR 0.000080 Loss 7.657711, Accuracy 75.289%\n",
      "Epoch 13, Batch 74, LR 0.000080 Loss 7.652970, Accuracy 75.306%\n",
      "Epoch 13, Batch 75, LR 0.000080 Loss 7.647350, Accuracy 75.354%\n",
      "Epoch 13, Batch 76, LR 0.000080 Loss 7.650150, Accuracy 75.360%\n",
      "Epoch 13, Batch 77, LR 0.000080 Loss 7.647698, Accuracy 75.345%\n",
      "Epoch 13, Batch 78, LR 0.000080 Loss 7.645076, Accuracy 75.341%\n",
      "Epoch 13, Batch 79, LR 0.000080 Loss 7.657841, Accuracy 75.257%\n",
      "Epoch 13, Batch 80, LR 0.000080 Loss 7.662782, Accuracy 75.215%\n",
      "Epoch 13, Batch 81, LR 0.000080 Loss 7.672410, Accuracy 75.193%\n",
      "Epoch 13, Batch 82, LR 0.000080 Loss 7.675770, Accuracy 75.191%\n",
      "Epoch 13, Batch 83, LR 0.000080 Loss 7.683900, Accuracy 75.160%\n",
      "Epoch 13, Batch 84, LR 0.000080 Loss 7.673826, Accuracy 75.260%\n",
      "Epoch 13, Batch 85, LR 0.000080 Loss 7.679343, Accuracy 75.285%\n",
      "Epoch 13, Batch 86, LR 0.000080 Loss 7.688278, Accuracy 75.282%\n",
      "Epoch 13, Batch 87, LR 0.000080 Loss 7.688564, Accuracy 75.278%\n",
      "Epoch 13, Batch 88, LR 0.000080 Loss 7.688449, Accuracy 75.302%\n",
      "Epoch 13, Batch 89, LR 0.000080 Loss 7.686142, Accuracy 75.307%\n",
      "Epoch 13, Batch 90, LR 0.000080 Loss 7.690978, Accuracy 75.269%\n",
      "Epoch 13, Batch 91, LR 0.000080 Loss 7.687523, Accuracy 75.275%\n",
      "Epoch 13, Batch 92, LR 0.000080 Loss 7.680870, Accuracy 75.289%\n",
      "Epoch 13, Batch 93, LR 0.000080 Loss 7.688781, Accuracy 75.252%\n",
      "Epoch 13, Batch 94, LR 0.000080 Loss 7.687006, Accuracy 75.291%\n",
      "Epoch 13, Batch 95, LR 0.000080 Loss 7.681793, Accuracy 75.304%\n",
      "Epoch 13, Batch 96, LR 0.000080 Loss 7.694983, Accuracy 75.195%\n",
      "Epoch 13, Batch 97, LR 0.000080 Loss 7.696950, Accuracy 75.137%\n",
      "Epoch 13, Batch 98, LR 0.000080 Loss 7.698122, Accuracy 75.136%\n",
      "Epoch 13, Batch 99, LR 0.000080 Loss 7.695039, Accuracy 75.182%\n",
      "Epoch 13, Batch 100, LR 0.000080 Loss 7.697567, Accuracy 75.141%\n",
      "Epoch 13, Batch 101, LR 0.000080 Loss 7.697070, Accuracy 75.170%\n",
      "Epoch 13, Batch 102, LR 0.000080 Loss 7.696701, Accuracy 75.169%\n",
      "Epoch 13, Batch 103, LR 0.000080 Loss 7.699273, Accuracy 75.159%\n",
      "Epoch 13, Batch 104, LR 0.000080 Loss 7.702844, Accuracy 75.120%\n",
      "Epoch 13, Batch 105, LR 0.000080 Loss 7.699145, Accuracy 75.126%\n",
      "Epoch 13, Batch 106, LR 0.000080 Loss 7.706014, Accuracy 75.074%\n",
      "Epoch 13, Batch 107, LR 0.000080 Loss 7.697064, Accuracy 75.131%\n",
      "Epoch 13, Batch 108, LR 0.000080 Loss 7.702662, Accuracy 75.116%\n",
      "Epoch 13, Batch 109, LR 0.000080 Loss 7.702944, Accuracy 75.108%\n",
      "Epoch 13, Batch 110, LR 0.000080 Loss 7.703387, Accuracy 75.107%\n",
      "Epoch 13, Batch 111, LR 0.000080 Loss 7.696370, Accuracy 75.148%\n",
      "Epoch 13, Batch 112, LR 0.000080 Loss 7.699507, Accuracy 75.126%\n",
      "Epoch 13, Batch 113, LR 0.000080 Loss 7.697497, Accuracy 75.118%\n",
      "Epoch 13, Batch 114, LR 0.000080 Loss 7.692149, Accuracy 75.158%\n",
      "Epoch 13, Batch 115, LR 0.000080 Loss 7.687825, Accuracy 75.211%\n",
      "Epoch 13, Batch 116, LR 0.000080 Loss 7.686247, Accuracy 75.249%\n",
      "Epoch 13, Batch 117, LR 0.000080 Loss 7.689707, Accuracy 75.234%\n",
      "Epoch 13, Batch 118, LR 0.000080 Loss 7.694283, Accuracy 75.238%\n",
      "Epoch 13, Batch 119, LR 0.000080 Loss 7.693125, Accuracy 75.249%\n",
      "Epoch 13, Batch 120, LR 0.000080 Loss 7.699682, Accuracy 75.241%\n",
      "Epoch 13, Batch 121, LR 0.000080 Loss 7.698235, Accuracy 75.291%\n",
      "Epoch 13, Batch 122, LR 0.000080 Loss 7.696030, Accuracy 75.307%\n",
      "Epoch 13, Batch 123, LR 0.000080 Loss 7.696094, Accuracy 75.279%\n",
      "Epoch 13, Batch 124, LR 0.000080 Loss 7.691756, Accuracy 75.321%\n",
      "Epoch 13, Batch 125, LR 0.000080 Loss 7.688207, Accuracy 75.344%\n",
      "Epoch 13, Batch 126, LR 0.000080 Loss 7.690297, Accuracy 75.335%\n",
      "Epoch 13, Batch 127, LR 0.000080 Loss 7.689202, Accuracy 75.357%\n",
      "Epoch 13, Batch 128, LR 0.000080 Loss 7.688779, Accuracy 75.366%\n",
      "Epoch 13, Batch 129, LR 0.000080 Loss 7.697726, Accuracy 75.303%\n",
      "Epoch 13, Batch 130, LR 0.000080 Loss 7.697077, Accuracy 75.306%\n",
      "Epoch 13, Batch 131, LR 0.000080 Loss 7.690694, Accuracy 75.352%\n",
      "Epoch 13, Batch 132, LR 0.000080 Loss 7.688248, Accuracy 75.391%\n",
      "Epoch 13, Batch 133, LR 0.000080 Loss 7.688994, Accuracy 75.364%\n",
      "Epoch 13, Batch 134, LR 0.000080 Loss 7.686082, Accuracy 75.414%\n",
      "Epoch 13, Batch 135, LR 0.000080 Loss 7.681482, Accuracy 75.399%\n",
      "Epoch 13, Batch 136, LR 0.000080 Loss 7.681514, Accuracy 75.373%\n",
      "Epoch 13, Batch 137, LR 0.000080 Loss 7.681995, Accuracy 75.371%\n",
      "Epoch 13, Batch 138, LR 0.000080 Loss 7.682083, Accuracy 75.385%\n",
      "Epoch 13, Batch 139, LR 0.000080 Loss 7.683659, Accuracy 75.354%\n",
      "Epoch 13, Batch 140, LR 0.000080 Loss 7.683177, Accuracy 75.357%\n",
      "Epoch 13, Batch 141, LR 0.000080 Loss 7.684690, Accuracy 75.344%\n",
      "Epoch 13, Batch 142, LR 0.000080 Loss 7.683234, Accuracy 75.347%\n",
      "Epoch 13, Batch 143, LR 0.000080 Loss 7.684517, Accuracy 75.366%\n",
      "Epoch 13, Batch 144, LR 0.000080 Loss 7.687693, Accuracy 75.304%\n",
      "Epoch 13, Batch 145, LR 0.000080 Loss 7.685183, Accuracy 75.302%\n",
      "Epoch 13, Batch 146, LR 0.000080 Loss 7.687726, Accuracy 75.294%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 147, LR 0.000080 Loss 7.684239, Accuracy 75.319%\n",
      "Epoch 13, Batch 148, LR 0.000080 Loss 7.684072, Accuracy 75.306%\n",
      "Epoch 13, Batch 149, LR 0.000080 Loss 7.686660, Accuracy 75.304%\n",
      "Epoch 13, Batch 150, LR 0.000080 Loss 7.688347, Accuracy 75.286%\n",
      "Epoch 13, Batch 151, LR 0.000080 Loss 7.688259, Accuracy 75.290%\n",
      "Epoch 13, Batch 152, LR 0.000080 Loss 7.683222, Accuracy 75.350%\n",
      "Epoch 13, Batch 153, LR 0.000080 Loss 7.682270, Accuracy 75.337%\n",
      "Epoch 13, Batch 154, LR 0.000080 Loss 7.680622, Accuracy 75.340%\n",
      "Epoch 13, Batch 155, LR 0.000080 Loss 7.679460, Accuracy 75.343%\n",
      "Epoch 13, Batch 156, LR 0.000080 Loss 7.680594, Accuracy 75.341%\n",
      "Epoch 13, Batch 157, LR 0.000080 Loss 7.679275, Accuracy 75.328%\n",
      "Epoch 13, Batch 158, LR 0.000080 Loss 7.679076, Accuracy 75.336%\n",
      "Epoch 13, Batch 159, LR 0.000080 Loss 7.677623, Accuracy 75.354%\n",
      "Epoch 13, Batch 160, LR 0.000080 Loss 7.675914, Accuracy 75.327%\n",
      "Epoch 13, Batch 161, LR 0.000080 Loss 7.681574, Accuracy 75.291%\n",
      "Epoch 13, Batch 162, LR 0.000080 Loss 7.689105, Accuracy 75.241%\n",
      "Epoch 13, Batch 163, LR 0.000080 Loss 7.689967, Accuracy 75.240%\n",
      "Epoch 13, Batch 164, LR 0.000080 Loss 7.684300, Accuracy 75.262%\n",
      "Epoch 13, Batch 165, LR 0.000080 Loss 7.686395, Accuracy 75.232%\n",
      "Epoch 13, Batch 166, LR 0.000080 Loss 7.688826, Accuracy 75.245%\n",
      "Epoch 13, Batch 167, LR 0.000080 Loss 7.688523, Accuracy 75.271%\n",
      "Epoch 13, Batch 168, LR 0.000080 Loss 7.685766, Accuracy 75.326%\n",
      "Epoch 13, Batch 169, LR 0.000080 Loss 7.688348, Accuracy 75.328%\n",
      "Epoch 13, Batch 170, LR 0.000080 Loss 7.689135, Accuracy 75.326%\n",
      "Epoch 13, Batch 171, LR 0.000080 Loss 7.685273, Accuracy 75.329%\n",
      "Epoch 13, Batch 172, LR 0.000080 Loss 7.686445, Accuracy 75.309%\n",
      "Epoch 13, Batch 173, LR 0.000080 Loss 7.683404, Accuracy 75.352%\n",
      "Epoch 13, Batch 174, LR 0.000080 Loss 7.682862, Accuracy 75.355%\n",
      "Epoch 13, Batch 175, LR 0.000080 Loss 7.683981, Accuracy 75.357%\n",
      "Epoch 13, Batch 176, LR 0.000080 Loss 7.681049, Accuracy 75.373%\n",
      "Epoch 13, Batch 177, LR 0.000080 Loss 7.676050, Accuracy 75.410%\n",
      "Epoch 13, Batch 178, LR 0.000080 Loss 7.677342, Accuracy 75.404%\n",
      "Epoch 13, Batch 179, LR 0.000080 Loss 7.674715, Accuracy 75.393%\n",
      "Epoch 13, Batch 180, LR 0.000080 Loss 7.674073, Accuracy 75.399%\n",
      "Epoch 13, Batch 181, LR 0.000080 Loss 7.677245, Accuracy 75.384%\n",
      "Epoch 13, Batch 182, LR 0.000080 Loss 7.673980, Accuracy 75.412%\n",
      "Epoch 13, Batch 183, LR 0.000080 Loss 7.675599, Accuracy 75.384%\n",
      "Epoch 13, Batch 184, LR 0.000080 Loss 7.676271, Accuracy 75.378%\n",
      "Epoch 13, Batch 185, LR 0.000080 Loss 7.673468, Accuracy 75.401%\n",
      "Epoch 13, Batch 186, LR 0.000080 Loss 7.672944, Accuracy 75.420%\n",
      "Epoch 13, Batch 187, LR 0.000080 Loss 7.671699, Accuracy 75.434%\n",
      "Epoch 13, Batch 188, LR 0.000080 Loss 7.668531, Accuracy 75.445%\n",
      "Epoch 13, Batch 189, LR 0.000080 Loss 7.664783, Accuracy 75.442%\n",
      "Epoch 13, Batch 190, LR 0.000080 Loss 7.663045, Accuracy 75.444%\n",
      "Epoch 13, Batch 191, LR 0.000080 Loss 7.659611, Accuracy 75.462%\n",
      "Epoch 13, Batch 192, LR 0.000080 Loss 7.658281, Accuracy 75.456%\n",
      "Epoch 13, Batch 193, LR 0.000080 Loss 7.652847, Accuracy 75.470%\n",
      "Epoch 13, Batch 194, LR 0.000080 Loss 7.656629, Accuracy 75.447%\n",
      "Epoch 13, Batch 195, LR 0.000080 Loss 7.656583, Accuracy 75.445%\n",
      "Epoch 13, Batch 196, LR 0.000080 Loss 7.651256, Accuracy 75.462%\n",
      "Epoch 13, Batch 197, LR 0.000080 Loss 7.652137, Accuracy 75.464%\n",
      "Epoch 13, Batch 198, LR 0.000080 Loss 7.656702, Accuracy 75.454%\n",
      "Epoch 13, Batch 199, LR 0.000080 Loss 7.655028, Accuracy 75.463%\n",
      "Epoch 13, Batch 200, LR 0.000080 Loss 7.654143, Accuracy 75.477%\n",
      "Epoch 13, Batch 201, LR 0.000080 Loss 7.657960, Accuracy 75.466%\n",
      "Epoch 13, Batch 202, LR 0.000080 Loss 7.661883, Accuracy 75.445%\n",
      "Epoch 13, Batch 203, LR 0.000080 Loss 7.665320, Accuracy 75.416%\n",
      "Epoch 13, Batch 204, LR 0.000080 Loss 7.666245, Accuracy 75.417%\n",
      "Epoch 13, Batch 205, LR 0.000080 Loss 7.668184, Accuracy 75.400%\n",
      "Epoch 13, Batch 206, LR 0.000080 Loss 7.667341, Accuracy 75.406%\n",
      "Epoch 13, Batch 207, LR 0.000080 Loss 7.664752, Accuracy 75.445%\n",
      "Epoch 13, Batch 208, LR 0.000080 Loss 7.662780, Accuracy 75.462%\n",
      "Epoch 13, Batch 209, LR 0.000080 Loss 7.662622, Accuracy 75.441%\n",
      "Epoch 13, Batch 210, LR 0.000080 Loss 7.664857, Accuracy 75.428%\n",
      "Epoch 13, Batch 211, LR 0.000080 Loss 7.662185, Accuracy 75.463%\n",
      "Epoch 13, Batch 212, LR 0.000080 Loss 7.662126, Accuracy 75.472%\n",
      "Epoch 13, Batch 213, LR 0.000080 Loss 7.661617, Accuracy 75.477%\n",
      "Epoch 13, Batch 214, LR 0.000080 Loss 7.660646, Accuracy 75.478%\n",
      "Epoch 13, Batch 215, LR 0.000080 Loss 7.663253, Accuracy 75.469%\n",
      "Epoch 13, Batch 216, LR 0.000080 Loss 7.665185, Accuracy 75.452%\n",
      "Epoch 13, Batch 217, LR 0.000080 Loss 7.663117, Accuracy 75.472%\n",
      "Epoch 13, Batch 218, LR 0.000080 Loss 7.661302, Accuracy 75.484%\n",
      "Epoch 13, Batch 219, LR 0.000080 Loss 7.661021, Accuracy 75.503%\n",
      "Epoch 13, Batch 220, LR 0.000080 Loss 7.661895, Accuracy 75.494%\n",
      "Epoch 13, Batch 221, LR 0.000080 Loss 7.666022, Accuracy 75.467%\n",
      "Epoch 13, Batch 222, LR 0.000080 Loss 7.658411, Accuracy 75.500%\n",
      "Epoch 13, Batch 223, LR 0.000080 Loss 7.660331, Accuracy 75.483%\n",
      "Epoch 13, Batch 224, LR 0.000080 Loss 7.666756, Accuracy 75.429%\n",
      "Epoch 13, Batch 225, LR 0.000080 Loss 7.669421, Accuracy 75.413%\n",
      "Epoch 13, Batch 226, LR 0.000080 Loss 7.667814, Accuracy 75.422%\n",
      "Epoch 13, Batch 227, LR 0.000080 Loss 7.670433, Accuracy 75.403%\n",
      "Epoch 13, Batch 228, LR 0.000080 Loss 7.671866, Accuracy 75.384%\n",
      "Epoch 13, Batch 229, LR 0.000080 Loss 7.671279, Accuracy 75.396%\n",
      "Epoch 13, Batch 230, LR 0.000080 Loss 7.673671, Accuracy 75.387%\n",
      "Epoch 13, Batch 231, LR 0.000080 Loss 7.672135, Accuracy 75.409%\n",
      "Epoch 13, Batch 232, LR 0.000080 Loss 7.668666, Accuracy 75.431%\n",
      "Epoch 13, Batch 233, LR 0.000080 Loss 7.664164, Accuracy 75.456%\n",
      "Epoch 13, Batch 234, LR 0.000080 Loss 7.666479, Accuracy 75.434%\n",
      "Epoch 13, Batch 235, LR 0.000080 Loss 7.666080, Accuracy 75.439%\n",
      "Epoch 13, Batch 236, LR 0.000080 Loss 7.665761, Accuracy 75.440%\n",
      "Epoch 13, Batch 237, LR 0.000080 Loss 7.666613, Accuracy 75.432%\n",
      "Epoch 13, Batch 238, LR 0.000080 Loss 7.662767, Accuracy 75.453%\n",
      "Epoch 13, Batch 239, LR 0.000080 Loss 7.666311, Accuracy 75.441%\n",
      "Epoch 13, Batch 240, LR 0.000080 Loss 7.663297, Accuracy 75.459%\n",
      "Epoch 13, Batch 241, LR 0.000080 Loss 7.661793, Accuracy 75.467%\n",
      "Epoch 13, Batch 242, LR 0.000080 Loss 7.663286, Accuracy 75.471%\n",
      "Epoch 13, Batch 243, LR 0.000080 Loss 7.662480, Accuracy 75.482%\n",
      "Epoch 13, Batch 244, LR 0.000080 Loss 7.663534, Accuracy 75.477%\n",
      "Epoch 13, Batch 245, LR 0.000080 Loss 7.663579, Accuracy 75.475%\n",
      "Epoch 13, Batch 246, LR 0.000080 Loss 7.664869, Accuracy 75.467%\n",
      "Epoch 13, Batch 247, LR 0.000080 Loss 7.663727, Accuracy 75.474%\n",
      "Epoch 13, Batch 248, LR 0.000080 Loss 7.665946, Accuracy 75.488%\n",
      "Epoch 13, Batch 249, LR 0.000080 Loss 7.664711, Accuracy 75.493%\n",
      "Epoch 13, Batch 250, LR 0.000080 Loss 7.667455, Accuracy 75.466%\n",
      "Epoch 13, Batch 251, LR 0.000080 Loss 7.667337, Accuracy 75.461%\n",
      "Epoch 13, Batch 252, LR 0.000080 Loss 7.667877, Accuracy 75.453%\n",
      "Epoch 13, Batch 253, LR 0.000080 Loss 7.669034, Accuracy 75.442%\n",
      "Epoch 13, Batch 254, LR 0.000080 Loss 7.667404, Accuracy 75.458%\n",
      "Epoch 13, Batch 255, LR 0.000080 Loss 7.667982, Accuracy 75.463%\n",
      "Epoch 13, Batch 256, LR 0.000080 Loss 7.666688, Accuracy 75.491%\n",
      "Epoch 13, Batch 257, LR 0.000080 Loss 7.665310, Accuracy 75.492%\n",
      "Epoch 13, Batch 258, LR 0.000080 Loss 7.666985, Accuracy 75.506%\n",
      "Epoch 13, Batch 259, LR 0.000080 Loss 7.665511, Accuracy 75.507%\n",
      "Epoch 13, Batch 260, LR 0.000080 Loss 7.666654, Accuracy 75.487%\n",
      "Epoch 13, Batch 261, LR 0.000080 Loss 7.665462, Accuracy 75.494%\n",
      "Epoch 13, Batch 262, LR 0.000080 Loss 7.663756, Accuracy 75.504%\n",
      "Epoch 13, Batch 263, LR 0.000080 Loss 7.665243, Accuracy 75.502%\n",
      "Epoch 13, Batch 264, LR 0.000080 Loss 7.662970, Accuracy 75.497%\n",
      "Epoch 13, Batch 265, LR 0.000080 Loss 7.667235, Accuracy 75.483%\n",
      "Epoch 13, Batch 266, LR 0.000080 Loss 7.666552, Accuracy 75.508%\n",
      "Epoch 13, Batch 267, LR 0.000080 Loss 7.666183, Accuracy 75.506%\n",
      "Epoch 13, Batch 268, LR 0.000080 Loss 7.664438, Accuracy 75.522%\n",
      "Epoch 13, Batch 269, LR 0.000080 Loss 7.664128, Accuracy 75.514%\n",
      "Epoch 13, Batch 270, LR 0.000080 Loss 7.661405, Accuracy 75.535%\n",
      "Epoch 13, Batch 271, LR 0.000080 Loss 7.661338, Accuracy 75.528%\n",
      "Epoch 13, Batch 272, LR 0.000080 Loss 7.662612, Accuracy 75.528%\n",
      "Epoch 13, Batch 273, LR 0.000080 Loss 7.662678, Accuracy 75.535%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 274, LR 0.000080 Loss 7.664892, Accuracy 75.508%\n",
      "Epoch 13, Batch 275, LR 0.000080 Loss 7.662118, Accuracy 75.511%\n",
      "Epoch 13, Batch 276, LR 0.000080 Loss 7.662402, Accuracy 75.521%\n",
      "Epoch 13, Batch 277, LR 0.000080 Loss 7.663087, Accuracy 75.499%\n",
      "Epoch 13, Batch 278, LR 0.000080 Loss 7.666063, Accuracy 75.478%\n",
      "Epoch 13, Batch 279, LR 0.000080 Loss 7.664061, Accuracy 75.498%\n",
      "Epoch 13, Batch 280, LR 0.000080 Loss 7.661714, Accuracy 75.516%\n",
      "Epoch 13, Batch 281, LR 0.000080 Loss 7.661729, Accuracy 75.506%\n",
      "Epoch 13, Batch 282, LR 0.000080 Loss 7.659109, Accuracy 75.521%\n",
      "Epoch 13, Batch 283, LR 0.000080 Loss 7.656904, Accuracy 75.527%\n",
      "Epoch 13, Batch 284, LR 0.000080 Loss 7.657765, Accuracy 75.520%\n",
      "Epoch 13, Batch 285, LR 0.000080 Loss 7.656783, Accuracy 75.526%\n",
      "Epoch 13, Batch 286, LR 0.000080 Loss 7.658715, Accuracy 75.522%\n",
      "Epoch 13, Batch 287, LR 0.000080 Loss 7.659107, Accuracy 75.520%\n",
      "Epoch 13, Batch 288, LR 0.000080 Loss 7.658207, Accuracy 75.515%\n",
      "Epoch 13, Batch 289, LR 0.000080 Loss 7.657738, Accuracy 75.516%\n",
      "Epoch 13, Batch 290, LR 0.000080 Loss 7.662157, Accuracy 75.496%\n",
      "Epoch 13, Batch 291, LR 0.000080 Loss 7.663100, Accuracy 75.478%\n",
      "Epoch 13, Batch 292, LR 0.000080 Loss 7.666593, Accuracy 75.455%\n",
      "Epoch 13, Batch 293, LR 0.000080 Loss 7.666408, Accuracy 75.453%\n",
      "Epoch 13, Batch 294, LR 0.000080 Loss 7.667768, Accuracy 75.444%\n",
      "Epoch 13, Batch 295, LR 0.000080 Loss 7.667589, Accuracy 75.432%\n",
      "Epoch 13, Batch 296, LR 0.000080 Loss 7.669209, Accuracy 75.414%\n",
      "Epoch 13, Batch 297, LR 0.000080 Loss 7.669560, Accuracy 75.418%\n",
      "Epoch 13, Batch 298, LR 0.000080 Loss 7.669604, Accuracy 75.430%\n",
      "Epoch 13, Batch 299, LR 0.000080 Loss 7.666354, Accuracy 75.449%\n",
      "Epoch 13, Batch 300, LR 0.000080 Loss 7.667512, Accuracy 75.438%\n",
      "Epoch 13, Batch 301, LR 0.000080 Loss 7.666082, Accuracy 75.441%\n",
      "Epoch 13, Batch 302, LR 0.000080 Loss 7.666096, Accuracy 75.424%\n",
      "Epoch 13, Batch 303, LR 0.000080 Loss 7.667121, Accuracy 75.415%\n",
      "Epoch 13, Batch 304, LR 0.000080 Loss 7.667094, Accuracy 75.409%\n",
      "Epoch 13, Batch 305, LR 0.000080 Loss 7.667218, Accuracy 75.412%\n",
      "Epoch 13, Batch 306, LR 0.000080 Loss 7.665260, Accuracy 75.414%\n",
      "Epoch 13, Batch 307, LR 0.000080 Loss 7.664495, Accuracy 75.417%\n",
      "Epoch 13, Batch 308, LR 0.000080 Loss 7.664362, Accuracy 75.426%\n",
      "Epoch 13, Batch 309, LR 0.000080 Loss 7.661737, Accuracy 75.453%\n",
      "Epoch 13, Batch 310, LR 0.000080 Loss 7.663181, Accuracy 75.451%\n",
      "Epoch 13, Batch 311, LR 0.000080 Loss 7.660474, Accuracy 75.482%\n",
      "Epoch 13, Batch 312, LR 0.000080 Loss 7.659774, Accuracy 75.488%\n",
      "Epoch 13, Batch 313, LR 0.000080 Loss 7.659363, Accuracy 75.482%\n",
      "Epoch 13, Batch 314, LR 0.000080 Loss 7.659260, Accuracy 75.488%\n",
      "Epoch 13, Batch 315, LR 0.000080 Loss 7.661368, Accuracy 75.471%\n",
      "Epoch 13, Batch 316, LR 0.000080 Loss 7.660238, Accuracy 75.470%\n",
      "Epoch 13, Batch 317, LR 0.000080 Loss 7.659552, Accuracy 75.468%\n",
      "Epoch 13, Batch 318, LR 0.000080 Loss 7.660100, Accuracy 75.469%\n",
      "Epoch 13, Batch 319, LR 0.000080 Loss 7.661420, Accuracy 75.478%\n",
      "Epoch 13, Batch 320, LR 0.000080 Loss 7.659968, Accuracy 75.479%\n",
      "Epoch 13, Batch 321, LR 0.000080 Loss 7.660702, Accuracy 75.482%\n",
      "Epoch 13, Batch 322, LR 0.000080 Loss 7.661016, Accuracy 75.471%\n",
      "Epoch 13, Batch 323, LR 0.000080 Loss 7.660349, Accuracy 75.486%\n",
      "Epoch 13, Batch 324, LR 0.000080 Loss 7.659754, Accuracy 75.502%\n",
      "Epoch 13, Batch 325, LR 0.000080 Loss 7.660076, Accuracy 75.498%\n",
      "Epoch 13, Batch 326, LR 0.000080 Loss 7.662705, Accuracy 75.477%\n",
      "Epoch 13, Batch 327, LR 0.000080 Loss 7.662850, Accuracy 75.475%\n",
      "Epoch 13, Batch 328, LR 0.000080 Loss 7.663020, Accuracy 75.476%\n",
      "Epoch 13, Batch 329, LR 0.000080 Loss 7.662182, Accuracy 75.508%\n",
      "Epoch 13, Batch 330, LR 0.000080 Loss 7.661319, Accuracy 75.518%\n",
      "Epoch 13, Batch 331, LR 0.000080 Loss 7.662566, Accuracy 75.505%\n",
      "Epoch 13, Batch 332, LR 0.000080 Loss 7.661234, Accuracy 75.506%\n",
      "Epoch 13, Batch 333, LR 0.000080 Loss 7.659643, Accuracy 75.521%\n",
      "Epoch 13, Batch 334, LR 0.000080 Loss 7.661266, Accuracy 75.519%\n",
      "Epoch 13, Batch 335, LR 0.000080 Loss 7.660112, Accuracy 75.518%\n",
      "Epoch 13, Batch 336, LR 0.000080 Loss 7.661816, Accuracy 75.491%\n",
      "Epoch 13, Batch 337, LR 0.000080 Loss 7.662531, Accuracy 75.496%\n",
      "Epoch 13, Batch 338, LR 0.000080 Loss 7.663160, Accuracy 75.497%\n",
      "Epoch 13, Batch 339, LR 0.000080 Loss 7.663548, Accuracy 75.495%\n",
      "Epoch 13, Batch 340, LR 0.000080 Loss 7.662143, Accuracy 75.506%\n",
      "Epoch 13, Batch 341, LR 0.000080 Loss 7.660037, Accuracy 75.518%\n",
      "Epoch 13, Batch 342, LR 0.000080 Loss 7.660918, Accuracy 75.519%\n",
      "Epoch 13, Batch 343, LR 0.000080 Loss 7.661419, Accuracy 75.517%\n",
      "Epoch 13, Batch 344, LR 0.000080 Loss 7.662533, Accuracy 75.511%\n",
      "Epoch 13, Batch 345, LR 0.000080 Loss 7.661537, Accuracy 75.512%\n",
      "Epoch 13, Batch 346, LR 0.000080 Loss 7.659894, Accuracy 75.544%\n",
      "Epoch 13, Batch 347, LR 0.000080 Loss 7.659378, Accuracy 75.545%\n",
      "Epoch 13, Batch 348, LR 0.000080 Loss 7.660229, Accuracy 75.541%\n",
      "Epoch 13, Batch 349, LR 0.000080 Loss 7.661037, Accuracy 75.533%\n",
      "Epoch 13, Batch 350, LR 0.000080 Loss 7.660713, Accuracy 75.540%\n",
      "Epoch 13, Batch 351, LR 0.000080 Loss 7.659949, Accuracy 75.556%\n",
      "Epoch 13, Batch 352, LR 0.000080 Loss 7.658395, Accuracy 75.557%\n",
      "Epoch 13, Batch 353, LR 0.000080 Loss 7.656595, Accuracy 75.560%\n",
      "Epoch 13, Batch 354, LR 0.000080 Loss 7.655043, Accuracy 75.565%\n",
      "Epoch 13, Batch 355, LR 0.000080 Loss 7.656291, Accuracy 75.552%\n",
      "Epoch 13, Batch 356, LR 0.000080 Loss 7.654969, Accuracy 75.551%\n",
      "Epoch 13, Batch 357, LR 0.000080 Loss 7.654516, Accuracy 75.549%\n",
      "Epoch 13, Batch 358, LR 0.000080 Loss 7.654334, Accuracy 75.550%\n",
      "Epoch 13, Batch 359, LR 0.000080 Loss 7.653195, Accuracy 75.555%\n",
      "Epoch 13, Batch 360, LR 0.000080 Loss 7.651309, Accuracy 75.556%\n",
      "Epoch 13, Batch 361, LR 0.000080 Loss 7.653027, Accuracy 75.550%\n",
      "Epoch 13, Batch 362, LR 0.000080 Loss 7.651245, Accuracy 75.561%\n",
      "Epoch 13, Batch 363, LR 0.000080 Loss 7.651553, Accuracy 75.560%\n",
      "Epoch 13, Batch 364, LR 0.000080 Loss 7.651523, Accuracy 75.547%\n",
      "Epoch 13, Batch 365, LR 0.000080 Loss 7.651162, Accuracy 75.539%\n",
      "Epoch 13, Batch 366, LR 0.000080 Loss 7.653478, Accuracy 75.527%\n",
      "Epoch 13, Batch 367, LR 0.000080 Loss 7.653378, Accuracy 75.526%\n",
      "Epoch 13, Batch 368, LR 0.000080 Loss 7.651412, Accuracy 75.539%\n",
      "Epoch 13, Batch 369, LR 0.000080 Loss 7.651781, Accuracy 75.536%\n",
      "Epoch 13, Batch 370, LR 0.000080 Loss 7.651180, Accuracy 75.547%\n",
      "Epoch 13, Batch 371, LR 0.000080 Loss 7.649925, Accuracy 75.558%\n",
      "Epoch 13, Batch 372, LR 0.000080 Loss 7.649957, Accuracy 75.563%\n",
      "Epoch 13, Batch 373, LR 0.000080 Loss 7.649643, Accuracy 75.559%\n",
      "Epoch 13, Batch 374, LR 0.000080 Loss 7.650568, Accuracy 75.554%\n",
      "Epoch 13, Batch 375, LR 0.000080 Loss 7.650379, Accuracy 75.548%\n",
      "Epoch 13, Batch 376, LR 0.000080 Loss 7.647093, Accuracy 75.571%\n",
      "Epoch 13, Batch 377, LR 0.000080 Loss 7.647308, Accuracy 75.570%\n",
      "Epoch 13, Batch 378, LR 0.000080 Loss 7.644606, Accuracy 75.583%\n",
      "Epoch 13, Batch 379, LR 0.000080 Loss 7.642907, Accuracy 75.596%\n",
      "Epoch 13, Batch 380, LR 0.000080 Loss 7.642108, Accuracy 75.604%\n",
      "Epoch 13, Batch 381, LR 0.000080 Loss 7.641107, Accuracy 75.613%\n",
      "Epoch 13, Batch 382, LR 0.000080 Loss 7.640019, Accuracy 75.614%\n",
      "Epoch 13, Batch 383, LR 0.000080 Loss 7.639056, Accuracy 75.618%\n",
      "Epoch 13, Batch 384, LR 0.000080 Loss 7.639949, Accuracy 75.623%\n",
      "Epoch 13, Batch 385, LR 0.000079 Loss 7.641887, Accuracy 75.599%\n",
      "Epoch 13, Batch 386, LR 0.000079 Loss 7.640798, Accuracy 75.613%\n",
      "Epoch 13, Batch 387, LR 0.000079 Loss 7.640704, Accuracy 75.608%\n",
      "Epoch 13, Batch 388, LR 0.000079 Loss 7.643492, Accuracy 75.590%\n",
      "Epoch 13, Batch 389, LR 0.000079 Loss 7.641815, Accuracy 75.605%\n",
      "Epoch 13, Batch 390, LR 0.000079 Loss 7.640963, Accuracy 75.607%\n",
      "Epoch 13, Batch 391, LR 0.000079 Loss 7.641582, Accuracy 75.609%\n",
      "Epoch 13, Batch 392, LR 0.000079 Loss 7.641165, Accuracy 75.608%\n",
      "Epoch 13, Batch 393, LR 0.000079 Loss 7.642267, Accuracy 75.600%\n",
      "Epoch 13, Batch 394, LR 0.000079 Loss 7.642806, Accuracy 75.597%\n",
      "Epoch 13, Batch 395, LR 0.000079 Loss 7.643747, Accuracy 75.591%\n",
      "Epoch 13, Batch 396, LR 0.000079 Loss 7.644983, Accuracy 75.590%\n",
      "Epoch 13, Batch 397, LR 0.000079 Loss 7.646025, Accuracy 75.581%\n",
      "Epoch 13, Batch 398, LR 0.000079 Loss 7.646610, Accuracy 75.585%\n",
      "Epoch 13, Batch 399, LR 0.000079 Loss 7.648707, Accuracy 75.572%\n",
      "Epoch 13, Batch 400, LR 0.000079 Loss 7.647134, Accuracy 75.576%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 401, LR 0.000079 Loss 7.644677, Accuracy 75.598%\n",
      "Epoch 13, Batch 402, LR 0.000079 Loss 7.643240, Accuracy 75.602%\n",
      "Epoch 13, Batch 403, LR 0.000079 Loss 7.641986, Accuracy 75.616%\n",
      "Epoch 13, Batch 404, LR 0.000079 Loss 7.642076, Accuracy 75.619%\n",
      "Epoch 13, Batch 405, LR 0.000079 Loss 7.641262, Accuracy 75.627%\n",
      "Epoch 13, Batch 406, LR 0.000079 Loss 7.639780, Accuracy 75.637%\n",
      "Epoch 13, Batch 407, LR 0.000079 Loss 7.640611, Accuracy 75.630%\n",
      "Epoch 13, Batch 408, LR 0.000079 Loss 7.642795, Accuracy 75.632%\n",
      "Epoch 13, Batch 409, LR 0.000079 Loss 7.641292, Accuracy 75.638%\n",
      "Epoch 13, Batch 410, LR 0.000079 Loss 7.640919, Accuracy 75.644%\n",
      "Epoch 13, Batch 411, LR 0.000079 Loss 7.638992, Accuracy 75.656%\n",
      "Epoch 13, Batch 412, LR 0.000079 Loss 7.639952, Accuracy 75.643%\n",
      "Epoch 13, Batch 413, LR 0.000079 Loss 7.639464, Accuracy 75.641%\n",
      "Epoch 13, Batch 414, LR 0.000079 Loss 7.639333, Accuracy 75.653%\n",
      "Epoch 13, Batch 415, LR 0.000079 Loss 7.640782, Accuracy 75.636%\n",
      "Epoch 13, Batch 416, LR 0.000079 Loss 7.641446, Accuracy 75.633%\n",
      "Epoch 13, Batch 417, LR 0.000079 Loss 7.640990, Accuracy 75.629%\n",
      "Epoch 13, Batch 418, LR 0.000079 Loss 7.639817, Accuracy 75.635%\n",
      "Epoch 13, Batch 419, LR 0.000079 Loss 7.642176, Accuracy 75.628%\n",
      "Epoch 13, Batch 420, LR 0.000079 Loss 7.640905, Accuracy 75.642%\n",
      "Epoch 13, Batch 421, LR 0.000079 Loss 7.640235, Accuracy 75.648%\n",
      "Epoch 13, Batch 422, LR 0.000079 Loss 7.640960, Accuracy 75.648%\n",
      "Epoch 13, Batch 423, LR 0.000079 Loss 7.642804, Accuracy 75.633%\n",
      "Epoch 13, Batch 424, LR 0.000079 Loss 7.643192, Accuracy 75.630%\n",
      "Epoch 13, Batch 425, LR 0.000079 Loss 7.643930, Accuracy 75.631%\n",
      "Epoch 13, Batch 426, LR 0.000079 Loss 7.642950, Accuracy 75.635%\n",
      "Epoch 13, Batch 427, LR 0.000079 Loss 7.642892, Accuracy 75.631%\n",
      "Epoch 13, Batch 428, LR 0.000079 Loss 7.643023, Accuracy 75.628%\n",
      "Epoch 13, Batch 429, LR 0.000079 Loss 7.644914, Accuracy 75.614%\n",
      "Epoch 13, Batch 430, LR 0.000079 Loss 7.642940, Accuracy 75.625%\n",
      "Epoch 13, Batch 431, LR 0.000079 Loss 7.643524, Accuracy 75.616%\n",
      "Epoch 13, Batch 432, LR 0.000079 Loss 7.643278, Accuracy 75.617%\n",
      "Epoch 13, Batch 433, LR 0.000079 Loss 7.642542, Accuracy 75.630%\n",
      "Epoch 13, Batch 434, LR 0.000079 Loss 7.640810, Accuracy 75.639%\n",
      "Epoch 13, Batch 435, LR 0.000079 Loss 7.643221, Accuracy 75.625%\n",
      "Epoch 13, Batch 436, LR 0.000079 Loss 7.643008, Accuracy 75.625%\n",
      "Epoch 13, Batch 437, LR 0.000079 Loss 7.642469, Accuracy 75.631%\n",
      "Epoch 13, Batch 438, LR 0.000079 Loss 7.643370, Accuracy 75.628%\n",
      "Epoch 13, Batch 439, LR 0.000079 Loss 7.643970, Accuracy 75.626%\n",
      "Epoch 13, Batch 440, LR 0.000079 Loss 7.644147, Accuracy 75.618%\n",
      "Epoch 13, Batch 441, LR 0.000079 Loss 7.643692, Accuracy 75.622%\n",
      "Epoch 13, Batch 442, LR 0.000079 Loss 7.642765, Accuracy 75.631%\n",
      "Epoch 13, Batch 443, LR 0.000079 Loss 7.642696, Accuracy 75.633%\n",
      "Epoch 13, Batch 444, LR 0.000079 Loss 7.643872, Accuracy 75.621%\n",
      "Epoch 13, Batch 445, LR 0.000079 Loss 7.645116, Accuracy 75.607%\n",
      "Epoch 13, Batch 446, LR 0.000079 Loss 7.645162, Accuracy 75.613%\n",
      "Epoch 13, Batch 447, LR 0.000079 Loss 7.646567, Accuracy 75.606%\n",
      "Epoch 13, Batch 448, LR 0.000079 Loss 7.645312, Accuracy 75.614%\n",
      "Epoch 13, Batch 449, LR 0.000079 Loss 7.646583, Accuracy 75.609%\n",
      "Epoch 13, Batch 450, LR 0.000079 Loss 7.647212, Accuracy 75.604%\n",
      "Epoch 13, Batch 451, LR 0.000079 Loss 7.645995, Accuracy 75.617%\n",
      "Epoch 13, Batch 452, LR 0.000079 Loss 7.647328, Accuracy 75.608%\n",
      "Epoch 13, Batch 453, LR 0.000079 Loss 7.648856, Accuracy 75.607%\n",
      "Epoch 13, Batch 454, LR 0.000079 Loss 7.647702, Accuracy 75.613%\n",
      "Epoch 13, Batch 455, LR 0.000079 Loss 7.646660, Accuracy 75.623%\n",
      "Epoch 13, Batch 456, LR 0.000079 Loss 7.646159, Accuracy 75.630%\n",
      "Epoch 13, Batch 457, LR 0.000079 Loss 7.646191, Accuracy 75.624%\n",
      "Epoch 13, Batch 458, LR 0.000079 Loss 7.647662, Accuracy 75.611%\n",
      "Epoch 13, Batch 459, LR 0.000079 Loss 7.648450, Accuracy 75.620%\n",
      "Epoch 13, Batch 460, LR 0.000079 Loss 7.647617, Accuracy 75.615%\n",
      "Epoch 13, Batch 461, LR 0.000079 Loss 7.650017, Accuracy 75.598%\n",
      "Epoch 13, Batch 462, LR 0.000079 Loss 7.650602, Accuracy 75.592%\n",
      "Epoch 13, Batch 463, LR 0.000079 Loss 7.650766, Accuracy 75.575%\n",
      "Epoch 13, Batch 464, LR 0.000079 Loss 7.649943, Accuracy 75.574%\n",
      "Epoch 13, Batch 465, LR 0.000079 Loss 7.648659, Accuracy 75.585%\n",
      "Epoch 13, Batch 466, LR 0.000079 Loss 7.647093, Accuracy 75.593%\n",
      "Epoch 13, Batch 467, LR 0.000079 Loss 7.645981, Accuracy 75.607%\n",
      "Epoch 13, Batch 468, LR 0.000079 Loss 7.647563, Accuracy 75.601%\n",
      "Epoch 13, Batch 469, LR 0.000079 Loss 7.648348, Accuracy 75.598%\n",
      "Epoch 13, Batch 470, LR 0.000079 Loss 7.646930, Accuracy 75.598%\n",
      "Epoch 13, Batch 471, LR 0.000079 Loss 7.647088, Accuracy 75.597%\n",
      "Epoch 13, Batch 472, LR 0.000079 Loss 7.647025, Accuracy 75.591%\n",
      "Epoch 13, Batch 473, LR 0.000079 Loss 7.646801, Accuracy 75.593%\n",
      "Epoch 13, Batch 474, LR 0.000079 Loss 7.646501, Accuracy 75.598%\n",
      "Epoch 13, Batch 475, LR 0.000079 Loss 7.647723, Accuracy 75.584%\n",
      "Epoch 13, Batch 476, LR 0.000079 Loss 7.648687, Accuracy 75.576%\n",
      "Epoch 13, Batch 477, LR 0.000079 Loss 7.648272, Accuracy 75.573%\n",
      "Epoch 13, Batch 478, LR 0.000079 Loss 7.647047, Accuracy 75.577%\n",
      "Epoch 13, Batch 479, LR 0.000079 Loss 7.646269, Accuracy 75.576%\n",
      "Epoch 13, Batch 480, LR 0.000079 Loss 7.646354, Accuracy 75.576%\n",
      "Epoch 13, Batch 481, LR 0.000079 Loss 7.646846, Accuracy 75.583%\n",
      "Epoch 13, Batch 482, LR 0.000079 Loss 7.647086, Accuracy 75.575%\n",
      "Epoch 13, Batch 483, LR 0.000079 Loss 7.648079, Accuracy 75.576%\n",
      "Epoch 13, Batch 484, LR 0.000079 Loss 7.648335, Accuracy 75.571%\n",
      "Epoch 13, Batch 485, LR 0.000079 Loss 7.646182, Accuracy 75.586%\n",
      "Epoch 13, Batch 486, LR 0.000079 Loss 7.645069, Accuracy 75.595%\n",
      "Epoch 13, Batch 487, LR 0.000079 Loss 7.644956, Accuracy 75.598%\n",
      "Epoch 13, Batch 488, LR 0.000079 Loss 7.643915, Accuracy 75.605%\n",
      "Epoch 13, Batch 489, LR 0.000079 Loss 7.643370, Accuracy 75.607%\n",
      "Epoch 13, Batch 490, LR 0.000079 Loss 7.644057, Accuracy 75.606%\n",
      "Epoch 13, Batch 491, LR 0.000079 Loss 7.644412, Accuracy 75.609%\n",
      "Epoch 13, Batch 492, LR 0.000079 Loss 7.644433, Accuracy 75.618%\n",
      "Epoch 13, Batch 493, LR 0.000079 Loss 7.645685, Accuracy 75.609%\n",
      "Epoch 13, Batch 494, LR 0.000079 Loss 7.644872, Accuracy 75.604%\n",
      "Epoch 13, Batch 495, LR 0.000079 Loss 7.643764, Accuracy 75.616%\n",
      "Epoch 13, Batch 496, LR 0.000079 Loss 7.644176, Accuracy 75.613%\n",
      "Epoch 13, Batch 497, LR 0.000079 Loss 7.642391, Accuracy 75.616%\n",
      "Epoch 13, Batch 498, LR 0.000079 Loss 7.642979, Accuracy 75.618%\n",
      "Epoch 13, Batch 499, LR 0.000079 Loss 7.642067, Accuracy 75.625%\n",
      "Epoch 13, Batch 500, LR 0.000079 Loss 7.642815, Accuracy 75.617%\n",
      "Epoch 13, Batch 501, LR 0.000079 Loss 7.642181, Accuracy 75.616%\n",
      "Epoch 13, Batch 502, LR 0.000079 Loss 7.642263, Accuracy 75.615%\n",
      "Epoch 13, Batch 503, LR 0.000079 Loss 7.642006, Accuracy 75.624%\n",
      "Epoch 13, Batch 504, LR 0.000079 Loss 7.642774, Accuracy 75.617%\n",
      "Epoch 13, Batch 505, LR 0.000079 Loss 7.643160, Accuracy 75.616%\n",
      "Epoch 13, Batch 506, LR 0.000079 Loss 7.643529, Accuracy 75.613%\n",
      "Epoch 13, Batch 507, LR 0.000079 Loss 7.644843, Accuracy 75.609%\n",
      "Epoch 13, Batch 508, LR 0.000079 Loss 7.644863, Accuracy 75.612%\n",
      "Epoch 13, Batch 509, LR 0.000079 Loss 7.645192, Accuracy 75.614%\n",
      "Epoch 13, Batch 510, LR 0.000079 Loss 7.644063, Accuracy 75.619%\n",
      "Epoch 13, Batch 511, LR 0.000079 Loss 7.644112, Accuracy 75.613%\n",
      "Epoch 13, Batch 512, LR 0.000079 Loss 7.644559, Accuracy 75.609%\n",
      "Epoch 13, Batch 513, LR 0.000079 Loss 7.646181, Accuracy 75.600%\n",
      "Epoch 13, Batch 514, LR 0.000079 Loss 7.646415, Accuracy 75.605%\n",
      "Epoch 13, Batch 515, LR 0.000079 Loss 7.645725, Accuracy 75.611%\n",
      "Epoch 13, Batch 516, LR 0.000079 Loss 7.647080, Accuracy 75.601%\n",
      "Epoch 13, Batch 517, LR 0.000079 Loss 7.646469, Accuracy 75.612%\n",
      "Epoch 13, Batch 518, LR 0.000079 Loss 7.646108, Accuracy 75.617%\n",
      "Epoch 13, Batch 519, LR 0.000079 Loss 7.646453, Accuracy 75.616%\n",
      "Epoch 13, Batch 520, LR 0.000079 Loss 7.646789, Accuracy 75.614%\n",
      "Epoch 13, Batch 521, LR 0.000079 Loss 7.646877, Accuracy 75.616%\n",
      "Epoch 13, Batch 522, LR 0.000079 Loss 7.644926, Accuracy 75.618%\n",
      "Epoch 13, Batch 523, LR 0.000079 Loss 7.644849, Accuracy 75.618%\n",
      "Epoch 13, Batch 524, LR 0.000079 Loss 7.643854, Accuracy 75.626%\n",
      "Epoch 13, Batch 525, LR 0.000079 Loss 7.643669, Accuracy 75.626%\n",
      "Epoch 13, Batch 526, LR 0.000079 Loss 7.643365, Accuracy 75.636%\n",
      "Epoch 13, Batch 527, LR 0.000079 Loss 7.642931, Accuracy 75.634%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 528, LR 0.000079 Loss 7.641999, Accuracy 75.636%\n",
      "Epoch 13, Batch 529, LR 0.000079 Loss 7.641024, Accuracy 75.634%\n",
      "Epoch 13, Batch 530, LR 0.000079 Loss 7.640843, Accuracy 75.632%\n",
      "Epoch 13, Batch 531, LR 0.000079 Loss 7.640762, Accuracy 75.624%\n",
      "Epoch 13, Batch 532, LR 0.000079 Loss 7.639716, Accuracy 75.636%\n",
      "Epoch 13, Batch 533, LR 0.000079 Loss 7.638664, Accuracy 75.642%\n",
      "Epoch 13, Batch 534, LR 0.000079 Loss 7.637833, Accuracy 75.645%\n",
      "Epoch 13, Batch 535, LR 0.000079 Loss 7.640002, Accuracy 75.631%\n",
      "Epoch 13, Batch 536, LR 0.000079 Loss 7.639600, Accuracy 75.637%\n",
      "Epoch 13, Batch 537, LR 0.000079 Loss 7.638376, Accuracy 75.640%\n",
      "Epoch 13, Batch 538, LR 0.000079 Loss 7.637090, Accuracy 75.655%\n",
      "Epoch 13, Batch 539, LR 0.000079 Loss 7.636103, Accuracy 75.661%\n",
      "Epoch 13, Batch 540, LR 0.000079 Loss 7.636917, Accuracy 75.655%\n",
      "Epoch 13, Batch 541, LR 0.000079 Loss 7.638469, Accuracy 75.646%\n",
      "Epoch 13, Batch 542, LR 0.000079 Loss 7.640745, Accuracy 75.634%\n",
      "Epoch 13, Batch 543, LR 0.000079 Loss 7.639060, Accuracy 75.642%\n",
      "Epoch 13, Batch 544, LR 0.000079 Loss 7.639133, Accuracy 75.636%\n",
      "Epoch 13, Batch 545, LR 0.000079 Loss 7.638825, Accuracy 75.639%\n",
      "Epoch 13, Batch 546, LR 0.000079 Loss 7.640378, Accuracy 75.632%\n",
      "Epoch 13, Batch 547, LR 0.000079 Loss 7.642205, Accuracy 75.628%\n",
      "Epoch 13, Batch 548, LR 0.000079 Loss 7.643564, Accuracy 75.624%\n",
      "Epoch 13, Batch 549, LR 0.000079 Loss 7.644038, Accuracy 75.622%\n",
      "Epoch 13, Batch 550, LR 0.000079 Loss 7.642052, Accuracy 75.643%\n",
      "Epoch 13, Batch 551, LR 0.000079 Loss 7.640476, Accuracy 75.645%\n",
      "Epoch 13, Batch 552, LR 0.000079 Loss 7.639990, Accuracy 75.640%\n",
      "Epoch 13, Batch 553, LR 0.000079 Loss 7.639927, Accuracy 75.639%\n",
      "Epoch 13, Batch 554, LR 0.000079 Loss 7.640111, Accuracy 75.633%\n",
      "Epoch 13, Batch 555, LR 0.000079 Loss 7.640703, Accuracy 75.632%\n",
      "Epoch 13, Batch 556, LR 0.000079 Loss 7.640840, Accuracy 75.639%\n",
      "Epoch 13, Batch 557, LR 0.000079 Loss 7.640682, Accuracy 75.647%\n",
      "Epoch 13, Batch 558, LR 0.000079 Loss 7.639858, Accuracy 75.658%\n",
      "Epoch 13, Batch 559, LR 0.000079 Loss 7.638794, Accuracy 75.662%\n",
      "Epoch 13, Batch 560, LR 0.000079 Loss 7.638573, Accuracy 75.664%\n",
      "Epoch 13, Batch 561, LR 0.000079 Loss 7.638039, Accuracy 75.668%\n",
      "Epoch 13, Batch 562, LR 0.000079 Loss 7.639239, Accuracy 75.660%\n",
      "Epoch 13, Batch 563, LR 0.000079 Loss 7.641520, Accuracy 75.644%\n",
      "Epoch 13, Batch 564, LR 0.000079 Loss 7.640065, Accuracy 75.648%\n",
      "Epoch 13, Batch 565, LR 0.000079 Loss 7.641134, Accuracy 75.642%\n",
      "Epoch 13, Batch 566, LR 0.000079 Loss 7.640469, Accuracy 75.646%\n",
      "Epoch 13, Batch 567, LR 0.000079 Loss 7.641166, Accuracy 75.639%\n",
      "Epoch 13, Batch 568, LR 0.000079 Loss 7.640738, Accuracy 75.646%\n",
      "Epoch 13, Batch 569, LR 0.000079 Loss 7.640776, Accuracy 75.647%\n",
      "Epoch 13, Batch 570, LR 0.000079 Loss 7.640442, Accuracy 75.648%\n",
      "Epoch 13, Batch 571, LR 0.000079 Loss 7.639055, Accuracy 75.657%\n",
      "Epoch 13, Batch 572, LR 0.000079 Loss 7.638077, Accuracy 75.668%\n",
      "Epoch 13, Batch 573, LR 0.000079 Loss 7.637677, Accuracy 75.665%\n",
      "Epoch 13, Batch 574, LR 0.000079 Loss 7.636374, Accuracy 75.667%\n",
      "Epoch 13, Batch 575, LR 0.000079 Loss 7.635541, Accuracy 75.674%\n",
      "Epoch 13, Batch 576, LR 0.000079 Loss 7.635314, Accuracy 75.674%\n",
      "Epoch 13, Batch 577, LR 0.000079 Loss 7.635494, Accuracy 75.673%\n",
      "Epoch 13, Batch 578, LR 0.000079 Loss 7.637337, Accuracy 75.654%\n",
      "Epoch 13, Batch 579, LR 0.000079 Loss 7.636907, Accuracy 75.657%\n",
      "Epoch 13, Batch 580, LR 0.000079 Loss 7.637347, Accuracy 75.655%\n",
      "Epoch 13, Batch 581, LR 0.000079 Loss 7.637460, Accuracy 75.660%\n",
      "Epoch 13, Batch 582, LR 0.000079 Loss 7.637894, Accuracy 75.656%\n",
      "Epoch 13, Batch 583, LR 0.000079 Loss 7.637670, Accuracy 75.657%\n",
      "Epoch 13, Batch 584, LR 0.000079 Loss 7.638711, Accuracy 75.649%\n",
      "Epoch 13, Batch 585, LR 0.000079 Loss 7.638773, Accuracy 75.642%\n",
      "Epoch 13, Batch 586, LR 0.000079 Loss 7.637465, Accuracy 75.647%\n",
      "Epoch 13, Batch 587, LR 0.000079 Loss 7.636532, Accuracy 75.648%\n",
      "Epoch 13, Batch 588, LR 0.000079 Loss 7.636110, Accuracy 75.654%\n",
      "Epoch 13, Batch 589, LR 0.000079 Loss 7.634161, Accuracy 75.654%\n",
      "Epoch 13, Batch 590, LR 0.000079 Loss 7.633202, Accuracy 75.654%\n",
      "Epoch 13, Batch 591, LR 0.000079 Loss 7.632654, Accuracy 75.656%\n",
      "Epoch 13, Batch 592, LR 0.000079 Loss 7.632452, Accuracy 75.669%\n",
      "Epoch 13, Batch 593, LR 0.000079 Loss 7.633077, Accuracy 75.661%\n",
      "Epoch 13, Batch 594, LR 0.000079 Loss 7.633323, Accuracy 75.658%\n",
      "Epoch 13, Batch 595, LR 0.000079 Loss 7.633605, Accuracy 75.654%\n",
      "Epoch 13, Batch 596, LR 0.000079 Loss 7.633473, Accuracy 75.654%\n",
      "Epoch 13, Batch 597, LR 0.000079 Loss 7.633999, Accuracy 75.649%\n",
      "Epoch 13, Batch 598, LR 0.000079 Loss 7.634455, Accuracy 75.644%\n",
      "Epoch 13, Batch 599, LR 0.000079 Loss 7.634442, Accuracy 75.638%\n",
      "Epoch 13, Batch 600, LR 0.000079 Loss 7.635336, Accuracy 75.626%\n",
      "Epoch 13, Batch 601, LR 0.000079 Loss 7.634036, Accuracy 75.634%\n",
      "Epoch 13, Batch 602, LR 0.000079 Loss 7.634040, Accuracy 75.636%\n",
      "Epoch 13, Batch 603, LR 0.000079 Loss 7.634103, Accuracy 75.636%\n",
      "Epoch 13, Batch 604, LR 0.000079 Loss 7.632771, Accuracy 75.648%\n",
      "Epoch 13, Batch 605, LR 0.000079 Loss 7.632379, Accuracy 75.644%\n",
      "Epoch 13, Batch 606, LR 0.000079 Loss 7.632736, Accuracy 75.641%\n",
      "Epoch 13, Batch 607, LR 0.000079 Loss 7.633165, Accuracy 75.638%\n",
      "Epoch 13, Batch 608, LR 0.000079 Loss 7.632465, Accuracy 75.645%\n",
      "Epoch 13, Batch 609, LR 0.000079 Loss 7.632335, Accuracy 75.644%\n",
      "Epoch 13, Batch 610, LR 0.000079 Loss 7.632698, Accuracy 75.639%\n",
      "Epoch 13, Batch 611, LR 0.000079 Loss 7.633383, Accuracy 75.632%\n",
      "Epoch 13, Batch 612, LR 0.000079 Loss 7.634561, Accuracy 75.629%\n",
      "Epoch 13, Batch 613, LR 0.000079 Loss 7.634938, Accuracy 75.628%\n",
      "Epoch 13, Batch 614, LR 0.000079 Loss 7.633387, Accuracy 75.637%\n",
      "Epoch 13, Batch 615, LR 0.000079 Loss 7.634027, Accuracy 75.634%\n",
      "Epoch 13, Batch 616, LR 0.000079 Loss 7.633511, Accuracy 75.646%\n",
      "Epoch 13, Batch 617, LR 0.000079 Loss 7.634232, Accuracy 75.643%\n",
      "Epoch 13, Batch 618, LR 0.000079 Loss 7.632740, Accuracy 75.656%\n",
      "Epoch 13, Batch 619, LR 0.000079 Loss 7.633160, Accuracy 75.647%\n",
      "Epoch 13, Batch 620, LR 0.000079 Loss 7.633286, Accuracy 75.650%\n",
      "Epoch 13, Batch 621, LR 0.000079 Loss 7.632587, Accuracy 75.644%\n",
      "Epoch 13, Batch 622, LR 0.000079 Loss 7.632482, Accuracy 75.649%\n",
      "Epoch 13, Batch 623, LR 0.000079 Loss 7.633702, Accuracy 75.647%\n",
      "Epoch 13, Batch 624, LR 0.000079 Loss 7.635610, Accuracy 75.639%\n",
      "Epoch 13, Batch 625, LR 0.000079 Loss 7.634241, Accuracy 75.647%\n",
      "Epoch 13, Batch 626, LR 0.000079 Loss 7.633961, Accuracy 75.649%\n",
      "Epoch 13, Batch 627, LR 0.000079 Loss 7.635143, Accuracy 75.647%\n",
      "Epoch 13, Batch 628, LR 0.000079 Loss 7.635438, Accuracy 75.639%\n",
      "Epoch 13, Batch 629, LR 0.000079 Loss 7.635202, Accuracy 75.638%\n",
      "Epoch 13, Batch 630, LR 0.000079 Loss 7.634063, Accuracy 75.645%\n",
      "Epoch 13, Batch 631, LR 0.000079 Loss 7.632568, Accuracy 75.652%\n",
      "Epoch 13, Batch 632, LR 0.000079 Loss 7.633022, Accuracy 75.650%\n",
      "Epoch 13, Batch 633, LR 0.000079 Loss 7.632819, Accuracy 75.652%\n",
      "Epoch 13, Batch 634, LR 0.000079 Loss 7.632576, Accuracy 75.654%\n",
      "Epoch 13, Batch 635, LR 0.000079 Loss 7.631337, Accuracy 75.666%\n",
      "Epoch 13, Batch 636, LR 0.000079 Loss 7.631747, Accuracy 75.662%\n",
      "Epoch 13, Batch 637, LR 0.000079 Loss 7.631879, Accuracy 75.666%\n",
      "Epoch 13, Batch 638, LR 0.000079 Loss 7.631912, Accuracy 75.667%\n",
      "Epoch 13, Batch 639, LR 0.000079 Loss 7.631905, Accuracy 75.661%\n",
      "Epoch 13, Batch 640, LR 0.000079 Loss 7.633318, Accuracy 75.651%\n",
      "Epoch 13, Batch 641, LR 0.000079 Loss 7.633186, Accuracy 75.651%\n",
      "Epoch 13, Batch 642, LR 0.000079 Loss 7.632249, Accuracy 75.661%\n",
      "Epoch 13, Batch 643, LR 0.000079 Loss 7.631923, Accuracy 75.655%\n",
      "Epoch 13, Batch 644, LR 0.000079 Loss 7.631676, Accuracy 75.648%\n",
      "Epoch 13, Batch 645, LR 0.000079 Loss 7.631311, Accuracy 75.653%\n",
      "Epoch 13, Batch 646, LR 0.000079 Loss 7.631335, Accuracy 75.660%\n",
      "Epoch 13, Batch 647, LR 0.000079 Loss 7.631198, Accuracy 75.663%\n",
      "Epoch 13, Batch 648, LR 0.000079 Loss 7.630176, Accuracy 75.674%\n",
      "Epoch 13, Batch 649, LR 0.000079 Loss 7.631221, Accuracy 75.661%\n",
      "Epoch 13, Batch 650, LR 0.000079 Loss 7.632122, Accuracy 75.656%\n",
      "Epoch 13, Batch 651, LR 0.000079 Loss 7.633123, Accuracy 75.650%\n",
      "Epoch 13, Batch 652, LR 0.000079 Loss 7.632840, Accuracy 75.655%\n",
      "Epoch 13, Batch 653, LR 0.000079 Loss 7.632447, Accuracy 75.660%\n",
      "Epoch 13, Batch 654, LR 0.000079 Loss 7.632606, Accuracy 75.661%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 655, LR 0.000079 Loss 7.632080, Accuracy 75.662%\n",
      "Epoch 13, Batch 656, LR 0.000079 Loss 7.632033, Accuracy 75.660%\n",
      "Epoch 13, Batch 657, LR 0.000079 Loss 7.632439, Accuracy 75.658%\n",
      "Epoch 13, Batch 658, LR 0.000079 Loss 7.631923, Accuracy 75.653%\n",
      "Epoch 13, Batch 659, LR 0.000079 Loss 7.631417, Accuracy 75.660%\n",
      "Epoch 13, Batch 660, LR 0.000079 Loss 7.631770, Accuracy 75.658%\n",
      "Epoch 13, Batch 661, LR 0.000079 Loss 7.631921, Accuracy 75.654%\n",
      "Epoch 13, Batch 662, LR 0.000079 Loss 7.631096, Accuracy 75.657%\n",
      "Epoch 13, Batch 663, LR 0.000079 Loss 7.631315, Accuracy 75.659%\n",
      "Epoch 13, Batch 664, LR 0.000079 Loss 7.631912, Accuracy 75.654%\n",
      "Epoch 13, Batch 665, LR 0.000079 Loss 7.633048, Accuracy 75.647%\n",
      "Epoch 13, Batch 666, LR 0.000079 Loss 7.631620, Accuracy 75.651%\n",
      "Epoch 13, Batch 667, LR 0.000079 Loss 7.633048, Accuracy 75.638%\n",
      "Epoch 13, Batch 668, LR 0.000079 Loss 7.634108, Accuracy 75.628%\n",
      "Epoch 13, Batch 669, LR 0.000079 Loss 7.634400, Accuracy 75.619%\n",
      "Epoch 13, Batch 670, LR 0.000079 Loss 7.635262, Accuracy 75.611%\n",
      "Epoch 13, Batch 671, LR 0.000079 Loss 7.633804, Accuracy 75.615%\n",
      "Epoch 13, Batch 672, LR 0.000079 Loss 7.634037, Accuracy 75.614%\n",
      "Epoch 13, Batch 673, LR 0.000079 Loss 7.632771, Accuracy 75.628%\n",
      "Epoch 13, Batch 674, LR 0.000079 Loss 7.633022, Accuracy 75.628%\n",
      "Epoch 13, Batch 675, LR 0.000079 Loss 7.632678, Accuracy 75.627%\n",
      "Epoch 13, Batch 676, LR 0.000079 Loss 7.633721, Accuracy 75.626%\n",
      "Epoch 13, Batch 677, LR 0.000079 Loss 7.633671, Accuracy 75.625%\n",
      "Epoch 13, Batch 678, LR 0.000079 Loss 7.633128, Accuracy 75.623%\n",
      "Epoch 13, Batch 679, LR 0.000079 Loss 7.632357, Accuracy 75.627%\n",
      "Epoch 13, Batch 680, LR 0.000079 Loss 7.632485, Accuracy 75.626%\n",
      "Epoch 13, Batch 681, LR 0.000079 Loss 7.632705, Accuracy 75.619%\n",
      "Epoch 13, Batch 682, LR 0.000079 Loss 7.631717, Accuracy 75.624%\n",
      "Epoch 13, Batch 683, LR 0.000079 Loss 7.630426, Accuracy 75.629%\n",
      "Epoch 13, Batch 684, LR 0.000079 Loss 7.631137, Accuracy 75.626%\n",
      "Epoch 13, Batch 685, LR 0.000079 Loss 7.630873, Accuracy 75.631%\n",
      "Epoch 13, Batch 686, LR 0.000079 Loss 7.631127, Accuracy 75.634%\n",
      "Epoch 13, Batch 687, LR 0.000079 Loss 7.630866, Accuracy 75.637%\n",
      "Epoch 13, Batch 688, LR 0.000079 Loss 7.631100, Accuracy 75.637%\n",
      "Epoch 13, Batch 689, LR 0.000079 Loss 7.630456, Accuracy 75.638%\n",
      "Epoch 13, Batch 690, LR 0.000079 Loss 7.628813, Accuracy 75.648%\n",
      "Epoch 13, Batch 691, LR 0.000079 Loss 7.628430, Accuracy 75.646%\n",
      "Epoch 13, Batch 692, LR 0.000079 Loss 7.629612, Accuracy 75.641%\n",
      "Epoch 13, Batch 693, LR 0.000079 Loss 7.628820, Accuracy 75.646%\n",
      "Epoch 13, Batch 694, LR 0.000079 Loss 7.628918, Accuracy 75.641%\n",
      "Epoch 13, Batch 695, LR 0.000079 Loss 7.628519, Accuracy 75.645%\n",
      "Epoch 13, Batch 696, LR 0.000078 Loss 7.628337, Accuracy 75.643%\n",
      "Epoch 13, Batch 697, LR 0.000078 Loss 7.629094, Accuracy 75.642%\n",
      "Epoch 13, Batch 698, LR 0.000078 Loss 7.629020, Accuracy 75.640%\n",
      "Epoch 13, Batch 699, LR 0.000078 Loss 7.629553, Accuracy 75.636%\n",
      "Epoch 13, Batch 700, LR 0.000078 Loss 7.629497, Accuracy 75.633%\n",
      "Epoch 13, Batch 701, LR 0.000078 Loss 7.629996, Accuracy 75.630%\n",
      "Epoch 13, Batch 702, LR 0.000078 Loss 7.630790, Accuracy 75.624%\n",
      "Epoch 13, Batch 703, LR 0.000078 Loss 7.630168, Accuracy 75.626%\n",
      "Epoch 13, Batch 704, LR 0.000078 Loss 7.631247, Accuracy 75.623%\n",
      "Epoch 13, Batch 705, LR 0.000078 Loss 7.631586, Accuracy 75.625%\n",
      "Epoch 13, Batch 706, LR 0.000078 Loss 7.632331, Accuracy 75.622%\n",
      "Epoch 13, Batch 707, LR 0.000078 Loss 7.632977, Accuracy 75.619%\n",
      "Epoch 13, Batch 708, LR 0.000078 Loss 7.633526, Accuracy 75.611%\n",
      "Epoch 13, Batch 709, LR 0.000078 Loss 7.632690, Accuracy 75.614%\n",
      "Epoch 13, Batch 710, LR 0.000078 Loss 7.630996, Accuracy 75.624%\n",
      "Epoch 13, Batch 711, LR 0.000078 Loss 7.632140, Accuracy 75.613%\n",
      "Epoch 13, Batch 712, LR 0.000078 Loss 7.632223, Accuracy 75.619%\n",
      "Epoch 13, Batch 713, LR 0.000078 Loss 7.632633, Accuracy 75.622%\n",
      "Epoch 13, Batch 714, LR 0.000078 Loss 7.632849, Accuracy 75.623%\n",
      "Epoch 13, Batch 715, LR 0.000078 Loss 7.633044, Accuracy 75.620%\n",
      "Epoch 13, Batch 716, LR 0.000078 Loss 7.633295, Accuracy 75.621%\n",
      "Epoch 13, Batch 717, LR 0.000078 Loss 7.632094, Accuracy 75.624%\n",
      "Epoch 13, Batch 718, LR 0.000078 Loss 7.632781, Accuracy 75.623%\n",
      "Epoch 13, Batch 719, LR 0.000078 Loss 7.633454, Accuracy 75.622%\n",
      "Epoch 13, Batch 720, LR 0.000078 Loss 7.632016, Accuracy 75.630%\n",
      "Epoch 13, Batch 721, LR 0.000078 Loss 7.632351, Accuracy 75.631%\n",
      "Epoch 13, Batch 722, LR 0.000078 Loss 7.631497, Accuracy 75.631%\n",
      "Epoch 13, Batch 723, LR 0.000078 Loss 7.631625, Accuracy 75.631%\n",
      "Epoch 13, Batch 724, LR 0.000078 Loss 7.632152, Accuracy 75.628%\n",
      "Epoch 13, Batch 725, LR 0.000078 Loss 7.631346, Accuracy 75.627%\n",
      "Epoch 13, Batch 726, LR 0.000078 Loss 7.630913, Accuracy 75.625%\n",
      "Epoch 13, Batch 727, LR 0.000078 Loss 7.630405, Accuracy 75.633%\n",
      "Epoch 13, Batch 728, LR 0.000078 Loss 7.629826, Accuracy 75.633%\n",
      "Epoch 13, Batch 729, LR 0.000078 Loss 7.630168, Accuracy 75.626%\n",
      "Epoch 13, Batch 730, LR 0.000078 Loss 7.630089, Accuracy 75.625%\n",
      "Epoch 13, Batch 731, LR 0.000078 Loss 7.629047, Accuracy 75.624%\n",
      "Epoch 13, Batch 732, LR 0.000078 Loss 7.629354, Accuracy 75.620%\n",
      "Epoch 13, Batch 733, LR 0.000078 Loss 7.629429, Accuracy 75.618%\n",
      "Epoch 13, Batch 734, LR 0.000078 Loss 7.630122, Accuracy 75.611%\n",
      "Epoch 13, Batch 735, LR 0.000078 Loss 7.630554, Accuracy 75.608%\n",
      "Epoch 13, Batch 736, LR 0.000078 Loss 7.629578, Accuracy 75.612%\n",
      "Epoch 13, Batch 737, LR 0.000078 Loss 7.629706, Accuracy 75.612%\n",
      "Epoch 13, Batch 738, LR 0.000078 Loss 7.628999, Accuracy 75.614%\n",
      "Epoch 13, Batch 739, LR 0.000078 Loss 7.630178, Accuracy 75.611%\n",
      "Epoch 13, Batch 740, LR 0.000078 Loss 7.630679, Accuracy 75.605%\n",
      "Epoch 13, Batch 741, LR 0.000078 Loss 7.630339, Accuracy 75.601%\n",
      "Epoch 13, Batch 742, LR 0.000078 Loss 7.630313, Accuracy 75.599%\n",
      "Epoch 13, Batch 743, LR 0.000078 Loss 7.629740, Accuracy 75.604%\n",
      "Epoch 13, Batch 744, LR 0.000078 Loss 7.629700, Accuracy 75.603%\n",
      "Epoch 13, Batch 745, LR 0.000078 Loss 7.629686, Accuracy 75.607%\n",
      "Epoch 13, Batch 746, LR 0.000078 Loss 7.629892, Accuracy 75.603%\n",
      "Epoch 13, Batch 747, LR 0.000078 Loss 7.629171, Accuracy 75.609%\n",
      "Epoch 13, Batch 748, LR 0.000078 Loss 7.630003, Accuracy 75.602%\n",
      "Epoch 13, Batch 749, LR 0.000078 Loss 7.630134, Accuracy 75.598%\n",
      "Epoch 13, Batch 750, LR 0.000078 Loss 7.630119, Accuracy 75.597%\n",
      "Epoch 13, Batch 751, LR 0.000078 Loss 7.631060, Accuracy 75.592%\n",
      "Epoch 13, Batch 752, LR 0.000078 Loss 7.631454, Accuracy 75.590%\n",
      "Epoch 13, Batch 753, LR 0.000078 Loss 7.631031, Accuracy 75.589%\n",
      "Epoch 13, Batch 754, LR 0.000078 Loss 7.631703, Accuracy 75.584%\n",
      "Epoch 13, Batch 755, LR 0.000078 Loss 7.632327, Accuracy 75.577%\n",
      "Epoch 13, Batch 756, LR 0.000078 Loss 7.631955, Accuracy 75.578%\n",
      "Epoch 13, Batch 757, LR 0.000078 Loss 7.631771, Accuracy 75.573%\n",
      "Epoch 13, Batch 758, LR 0.000078 Loss 7.631900, Accuracy 75.572%\n",
      "Epoch 13, Batch 759, LR 0.000078 Loss 7.631322, Accuracy 75.574%\n",
      "Epoch 13, Batch 760, LR 0.000078 Loss 7.631604, Accuracy 75.573%\n",
      "Epoch 13, Batch 761, LR 0.000078 Loss 7.631351, Accuracy 75.568%\n",
      "Epoch 13, Batch 762, LR 0.000078 Loss 7.631622, Accuracy 75.569%\n",
      "Epoch 13, Batch 763, LR 0.000078 Loss 7.631344, Accuracy 75.579%\n",
      "Epoch 13, Batch 764, LR 0.000078 Loss 7.631390, Accuracy 75.585%\n",
      "Epoch 13, Batch 765, LR 0.000078 Loss 7.630303, Accuracy 75.592%\n",
      "Epoch 13, Batch 766, LR 0.000078 Loss 7.630559, Accuracy 75.592%\n",
      "Epoch 13, Batch 767, LR 0.000078 Loss 7.631037, Accuracy 75.589%\n",
      "Epoch 13, Batch 768, LR 0.000078 Loss 7.630997, Accuracy 75.589%\n",
      "Epoch 13, Batch 769, LR 0.000078 Loss 7.632269, Accuracy 75.584%\n",
      "Epoch 13, Batch 770, LR 0.000078 Loss 7.632217, Accuracy 75.581%\n",
      "Epoch 13, Batch 771, LR 0.000078 Loss 7.631894, Accuracy 75.582%\n",
      "Epoch 13, Batch 772, LR 0.000078 Loss 7.633276, Accuracy 75.571%\n",
      "Epoch 13, Batch 773, LR 0.000078 Loss 7.633793, Accuracy 75.561%\n",
      "Epoch 13, Batch 774, LR 0.000078 Loss 7.632348, Accuracy 75.566%\n",
      "Epoch 13, Batch 775, LR 0.000078 Loss 7.631740, Accuracy 75.567%\n",
      "Epoch 13, Batch 776, LR 0.000078 Loss 7.630600, Accuracy 75.570%\n",
      "Epoch 13, Batch 777, LR 0.000078 Loss 7.629670, Accuracy 75.573%\n",
      "Epoch 13, Batch 778, LR 0.000078 Loss 7.627980, Accuracy 75.580%\n",
      "Epoch 13, Batch 779, LR 0.000078 Loss 7.627962, Accuracy 75.585%\n",
      "Epoch 13, Batch 780, LR 0.000078 Loss 7.627606, Accuracy 75.591%\n",
      "Epoch 13, Batch 781, LR 0.000078 Loss 7.627773, Accuracy 75.589%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 782, LR 0.000078 Loss 7.626656, Accuracy 75.597%\n",
      "Epoch 13, Batch 783, LR 0.000078 Loss 7.626881, Accuracy 75.599%\n",
      "Epoch 13, Batch 784, LR 0.000078 Loss 7.626501, Accuracy 75.604%\n",
      "Epoch 13, Batch 785, LR 0.000078 Loss 7.626424, Accuracy 75.605%\n",
      "Epoch 13, Batch 786, LR 0.000078 Loss 7.626340, Accuracy 75.604%\n",
      "Epoch 13, Batch 787, LR 0.000078 Loss 7.626116, Accuracy 75.610%\n",
      "Epoch 13, Batch 788, LR 0.000078 Loss 7.625245, Accuracy 75.617%\n",
      "Epoch 13, Batch 789, LR 0.000078 Loss 7.625144, Accuracy 75.613%\n",
      "Epoch 13, Batch 790, LR 0.000078 Loss 7.626548, Accuracy 75.601%\n",
      "Epoch 13, Batch 791, LR 0.000078 Loss 7.626847, Accuracy 75.598%\n",
      "Epoch 13, Batch 792, LR 0.000078 Loss 7.627141, Accuracy 75.599%\n",
      "Epoch 13, Batch 793, LR 0.000078 Loss 7.626638, Accuracy 75.600%\n",
      "Epoch 13, Batch 794, LR 0.000078 Loss 7.626699, Accuracy 75.598%\n",
      "Epoch 13, Batch 795, LR 0.000078 Loss 7.626613, Accuracy 75.597%\n",
      "Epoch 13, Batch 796, LR 0.000078 Loss 7.626475, Accuracy 75.599%\n",
      "Epoch 13, Batch 797, LR 0.000078 Loss 7.625632, Accuracy 75.604%\n",
      "Epoch 13, Batch 798, LR 0.000078 Loss 7.625151, Accuracy 75.606%\n",
      "Epoch 13, Batch 799, LR 0.000078 Loss 7.625370, Accuracy 75.604%\n",
      "Epoch 13, Batch 800, LR 0.000078 Loss 7.625712, Accuracy 75.603%\n",
      "Epoch 13, Batch 801, LR 0.000078 Loss 7.624359, Accuracy 75.611%\n",
      "Epoch 13, Batch 802, LR 0.000078 Loss 7.624817, Accuracy 75.608%\n",
      "Epoch 13, Batch 803, LR 0.000078 Loss 7.623808, Accuracy 75.614%\n",
      "Epoch 13, Batch 804, LR 0.000078 Loss 7.622506, Accuracy 75.620%\n",
      "Epoch 13, Batch 805, LR 0.000078 Loss 7.622135, Accuracy 75.624%\n",
      "Epoch 13, Batch 806, LR 0.000078 Loss 7.622963, Accuracy 75.622%\n",
      "Epoch 13, Batch 807, LR 0.000078 Loss 7.623052, Accuracy 75.621%\n",
      "Epoch 13, Batch 808, LR 0.000078 Loss 7.623880, Accuracy 75.614%\n",
      "Epoch 13, Batch 809, LR 0.000078 Loss 7.624022, Accuracy 75.613%\n",
      "Epoch 13, Batch 810, LR 0.000078 Loss 7.624165, Accuracy 75.614%\n",
      "Epoch 13, Batch 811, LR 0.000078 Loss 7.623410, Accuracy 75.615%\n",
      "Epoch 13, Batch 812, LR 0.000078 Loss 7.624509, Accuracy 75.606%\n",
      "Epoch 13, Batch 813, LR 0.000078 Loss 7.623430, Accuracy 75.609%\n",
      "Epoch 13, Batch 814, LR 0.000078 Loss 7.623083, Accuracy 75.612%\n",
      "Epoch 13, Batch 815, LR 0.000078 Loss 7.622435, Accuracy 75.615%\n",
      "Epoch 13, Batch 816, LR 0.000078 Loss 7.622177, Accuracy 75.623%\n",
      "Epoch 13, Batch 817, LR 0.000078 Loss 7.621627, Accuracy 75.626%\n",
      "Epoch 13, Batch 818, LR 0.000078 Loss 7.621112, Accuracy 75.628%\n",
      "Epoch 13, Batch 819, LR 0.000078 Loss 7.621033, Accuracy 75.628%\n",
      "Epoch 13, Batch 820, LR 0.000078 Loss 7.620470, Accuracy 75.628%\n",
      "Epoch 13, Batch 821, LR 0.000078 Loss 7.621264, Accuracy 75.619%\n",
      "Epoch 13, Batch 822, LR 0.000078 Loss 7.621216, Accuracy 75.624%\n",
      "Epoch 13, Batch 823, LR 0.000078 Loss 7.621470, Accuracy 75.620%\n",
      "Epoch 13, Batch 824, LR 0.000078 Loss 7.621406, Accuracy 75.616%\n",
      "Epoch 13, Batch 825, LR 0.000078 Loss 7.622139, Accuracy 75.610%\n",
      "Epoch 13, Batch 826, LR 0.000078 Loss 7.621869, Accuracy 75.614%\n",
      "Epoch 13, Batch 827, LR 0.000078 Loss 7.622749, Accuracy 75.612%\n",
      "Epoch 13, Batch 828, LR 0.000078 Loss 7.622459, Accuracy 75.613%\n",
      "Epoch 13, Batch 829, LR 0.000078 Loss 7.623010, Accuracy 75.611%\n",
      "Epoch 13, Batch 830, LR 0.000078 Loss 7.622089, Accuracy 75.608%\n",
      "Epoch 13, Batch 831, LR 0.000078 Loss 7.621313, Accuracy 75.608%\n",
      "Epoch 13, Batch 832, LR 0.000078 Loss 7.620760, Accuracy 75.610%\n",
      "Epoch 13, Batch 833, LR 0.000078 Loss 7.620790, Accuracy 75.608%\n",
      "Epoch 13, Batch 834, LR 0.000078 Loss 7.620145, Accuracy 75.612%\n",
      "Epoch 13, Batch 835, LR 0.000078 Loss 7.619754, Accuracy 75.612%\n",
      "Epoch 13, Batch 836, LR 0.000078 Loss 7.619558, Accuracy 75.611%\n",
      "Epoch 13, Batch 837, LR 0.000078 Loss 7.619467, Accuracy 75.618%\n",
      "Epoch 13, Batch 838, LR 0.000078 Loss 7.618799, Accuracy 75.623%\n",
      "Epoch 13, Batch 839, LR 0.000078 Loss 7.618190, Accuracy 75.629%\n",
      "Epoch 13, Batch 840, LR 0.000078 Loss 7.617579, Accuracy 75.632%\n",
      "Epoch 13, Batch 841, LR 0.000078 Loss 7.617338, Accuracy 75.630%\n",
      "Epoch 13, Batch 842, LR 0.000078 Loss 7.617007, Accuracy 75.632%\n",
      "Epoch 13, Batch 843, LR 0.000078 Loss 7.616078, Accuracy 75.638%\n",
      "Epoch 13, Batch 844, LR 0.000078 Loss 7.614797, Accuracy 75.647%\n",
      "Epoch 13, Batch 845, LR 0.000078 Loss 7.613773, Accuracy 75.650%\n",
      "Epoch 13, Batch 846, LR 0.000078 Loss 7.612659, Accuracy 75.657%\n",
      "Epoch 13, Batch 847, LR 0.000078 Loss 7.612230, Accuracy 75.659%\n",
      "Epoch 13, Batch 848, LR 0.000078 Loss 7.612117, Accuracy 75.655%\n",
      "Epoch 13, Batch 849, LR 0.000078 Loss 7.612177, Accuracy 75.657%\n",
      "Epoch 13, Batch 850, LR 0.000078 Loss 7.611408, Accuracy 75.668%\n",
      "Epoch 13, Batch 851, LR 0.000078 Loss 7.610547, Accuracy 75.672%\n",
      "Epoch 13, Batch 852, LR 0.000078 Loss 7.609336, Accuracy 75.678%\n",
      "Epoch 13, Batch 853, LR 0.000078 Loss 7.608625, Accuracy 75.680%\n",
      "Epoch 13, Batch 854, LR 0.000078 Loss 7.609073, Accuracy 75.673%\n",
      "Epoch 13, Batch 855, LR 0.000078 Loss 7.608210, Accuracy 75.675%\n",
      "Epoch 13, Batch 856, LR 0.000078 Loss 7.607940, Accuracy 75.676%\n",
      "Epoch 13, Batch 857, LR 0.000078 Loss 7.608354, Accuracy 75.676%\n",
      "Epoch 13, Batch 858, LR 0.000078 Loss 7.607548, Accuracy 75.678%\n",
      "Epoch 13, Batch 859, LR 0.000078 Loss 7.607572, Accuracy 75.680%\n",
      "Epoch 13, Batch 860, LR 0.000078 Loss 7.608562, Accuracy 75.680%\n",
      "Epoch 13, Batch 861, LR 0.000078 Loss 7.609109, Accuracy 75.680%\n",
      "Epoch 13, Batch 862, LR 0.000078 Loss 7.608688, Accuracy 75.680%\n",
      "Epoch 13, Batch 863, LR 0.000078 Loss 7.607677, Accuracy 75.688%\n",
      "Epoch 13, Batch 864, LR 0.000078 Loss 7.608042, Accuracy 75.684%\n",
      "Epoch 13, Batch 865, LR 0.000078 Loss 7.606911, Accuracy 75.693%\n",
      "Epoch 13, Batch 866, LR 0.000078 Loss 7.607216, Accuracy 75.693%\n",
      "Epoch 13, Batch 867, LR 0.000078 Loss 7.607549, Accuracy 75.693%\n",
      "Epoch 13, Batch 868, LR 0.000078 Loss 7.607849, Accuracy 75.688%\n",
      "Epoch 13, Batch 869, LR 0.000078 Loss 7.607607, Accuracy 75.692%\n",
      "Epoch 13, Batch 870, LR 0.000078 Loss 7.607163, Accuracy 75.694%\n",
      "Epoch 13, Batch 871, LR 0.000078 Loss 7.605867, Accuracy 75.703%\n",
      "Epoch 13, Batch 872, LR 0.000078 Loss 7.605858, Accuracy 75.700%\n",
      "Epoch 13, Batch 873, LR 0.000078 Loss 7.605985, Accuracy 75.702%\n",
      "Epoch 13, Batch 874, LR 0.000078 Loss 7.606068, Accuracy 75.700%\n",
      "Epoch 13, Batch 875, LR 0.000078 Loss 7.606181, Accuracy 75.696%\n",
      "Epoch 13, Batch 876, LR 0.000078 Loss 7.605339, Accuracy 75.701%\n",
      "Epoch 13, Batch 877, LR 0.000078 Loss 7.604959, Accuracy 75.703%\n",
      "Epoch 13, Batch 878, LR 0.000078 Loss 7.603929, Accuracy 75.708%\n",
      "Epoch 13, Batch 879, LR 0.000078 Loss 7.602534, Accuracy 75.715%\n",
      "Epoch 13, Batch 880, LR 0.000078 Loss 7.603211, Accuracy 75.707%\n",
      "Epoch 13, Batch 881, LR 0.000078 Loss 7.602646, Accuracy 75.709%\n",
      "Epoch 13, Batch 882, LR 0.000078 Loss 7.602523, Accuracy 75.706%\n",
      "Epoch 13, Batch 883, LR 0.000078 Loss 7.601487, Accuracy 75.711%\n",
      "Epoch 13, Batch 884, LR 0.000078 Loss 7.601626, Accuracy 75.710%\n",
      "Epoch 13, Batch 885, LR 0.000078 Loss 7.601889, Accuracy 75.712%\n",
      "Epoch 13, Batch 886, LR 0.000078 Loss 7.600758, Accuracy 75.717%\n",
      "Epoch 13, Batch 887, LR 0.000078 Loss 7.601018, Accuracy 75.713%\n",
      "Epoch 13, Batch 888, LR 0.000078 Loss 7.600999, Accuracy 75.710%\n",
      "Epoch 13, Batch 889, LR 0.000078 Loss 7.601339, Accuracy 75.707%\n",
      "Epoch 13, Batch 890, LR 0.000078 Loss 7.600509, Accuracy 75.713%\n",
      "Epoch 13, Batch 891, LR 0.000078 Loss 7.599935, Accuracy 75.722%\n",
      "Epoch 13, Batch 892, LR 0.000078 Loss 7.600191, Accuracy 75.724%\n",
      "Epoch 13, Batch 893, LR 0.000078 Loss 7.599786, Accuracy 75.724%\n",
      "Epoch 13, Batch 894, LR 0.000078 Loss 7.599160, Accuracy 75.731%\n",
      "Epoch 13, Batch 895, LR 0.000078 Loss 7.599529, Accuracy 75.729%\n",
      "Epoch 13, Batch 896, LR 0.000078 Loss 7.599809, Accuracy 75.726%\n",
      "Epoch 13, Batch 897, LR 0.000078 Loss 7.599862, Accuracy 75.732%\n",
      "Epoch 13, Batch 898, LR 0.000078 Loss 7.599894, Accuracy 75.728%\n",
      "Epoch 13, Batch 899, LR 0.000078 Loss 7.599975, Accuracy 75.727%\n",
      "Epoch 13, Batch 900, LR 0.000078 Loss 7.599438, Accuracy 75.728%\n",
      "Epoch 13, Batch 901, LR 0.000078 Loss 7.598490, Accuracy 75.735%\n",
      "Epoch 13, Batch 902, LR 0.000078 Loss 7.598711, Accuracy 75.730%\n",
      "Epoch 13, Batch 903, LR 0.000078 Loss 7.598618, Accuracy 75.734%\n",
      "Epoch 13, Batch 904, LR 0.000078 Loss 7.598235, Accuracy 75.735%\n",
      "Epoch 13, Batch 905, LR 0.000078 Loss 7.598463, Accuracy 75.731%\n",
      "Epoch 13, Batch 906, LR 0.000078 Loss 7.598511, Accuracy 75.728%\n",
      "Epoch 13, Batch 907, LR 0.000078 Loss 7.599218, Accuracy 75.725%\n",
      "Epoch 13, Batch 908, LR 0.000078 Loss 7.598628, Accuracy 75.733%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 909, LR 0.000078 Loss 7.598899, Accuracy 75.731%\n",
      "Epoch 13, Batch 910, LR 0.000078 Loss 7.598956, Accuracy 75.733%\n",
      "Epoch 13, Batch 911, LR 0.000078 Loss 7.597850, Accuracy 75.741%\n",
      "Epoch 13, Batch 912, LR 0.000078 Loss 7.597557, Accuracy 75.737%\n",
      "Epoch 13, Batch 913, LR 0.000078 Loss 7.597969, Accuracy 75.738%\n",
      "Epoch 13, Batch 914, LR 0.000078 Loss 7.597424, Accuracy 75.741%\n",
      "Epoch 13, Batch 915, LR 0.000078 Loss 7.595771, Accuracy 75.753%\n",
      "Epoch 13, Batch 916, LR 0.000078 Loss 7.595864, Accuracy 75.750%\n",
      "Epoch 13, Batch 917, LR 0.000078 Loss 7.595931, Accuracy 75.750%\n",
      "Epoch 13, Batch 918, LR 0.000078 Loss 7.596194, Accuracy 75.744%\n",
      "Epoch 13, Batch 919, LR 0.000078 Loss 7.595923, Accuracy 75.741%\n",
      "Epoch 13, Batch 920, LR 0.000078 Loss 7.594838, Accuracy 75.749%\n",
      "Epoch 13, Batch 921, LR 0.000078 Loss 7.595158, Accuracy 75.752%\n",
      "Epoch 13, Batch 922, LR 0.000078 Loss 7.594924, Accuracy 75.757%\n",
      "Epoch 13, Batch 923, LR 0.000078 Loss 7.594985, Accuracy 75.754%\n",
      "Epoch 13, Batch 924, LR 0.000078 Loss 7.594741, Accuracy 75.753%\n",
      "Epoch 13, Batch 925, LR 0.000078 Loss 7.594430, Accuracy 75.756%\n",
      "Epoch 13, Batch 926, LR 0.000078 Loss 7.594177, Accuracy 75.758%\n",
      "Epoch 13, Batch 927, LR 0.000078 Loss 7.594360, Accuracy 75.759%\n",
      "Epoch 13, Batch 928, LR 0.000078 Loss 7.594923, Accuracy 75.755%\n",
      "Epoch 13, Batch 929, LR 0.000078 Loss 7.594859, Accuracy 75.755%\n",
      "Epoch 13, Batch 930, LR 0.000078 Loss 7.595290, Accuracy 75.753%\n",
      "Epoch 13, Batch 931, LR 0.000078 Loss 7.595449, Accuracy 75.754%\n",
      "Epoch 13, Batch 932, LR 0.000078 Loss 7.595187, Accuracy 75.759%\n",
      "Epoch 13, Batch 933, LR 0.000078 Loss 7.594995, Accuracy 75.759%\n",
      "Epoch 13, Batch 934, LR 0.000078 Loss 7.595406, Accuracy 75.760%\n",
      "Epoch 13, Batch 935, LR 0.000078 Loss 7.595469, Accuracy 75.762%\n",
      "Epoch 13, Batch 936, LR 0.000078 Loss 7.595437, Accuracy 75.760%\n",
      "Epoch 13, Batch 937, LR 0.000078 Loss 7.595653, Accuracy 75.759%\n",
      "Epoch 13, Batch 938, LR 0.000078 Loss 7.595554, Accuracy 75.760%\n",
      "Epoch 13, Batch 939, LR 0.000078 Loss 7.595172, Accuracy 75.762%\n",
      "Epoch 13, Batch 940, LR 0.000078 Loss 7.595153, Accuracy 75.767%\n",
      "Epoch 13, Batch 941, LR 0.000078 Loss 7.594874, Accuracy 75.765%\n",
      "Epoch 13, Batch 942, LR 0.000078 Loss 7.594781, Accuracy 75.764%\n",
      "Epoch 13, Batch 943, LR 0.000078 Loss 7.594718, Accuracy 75.766%\n",
      "Epoch 13, Batch 944, LR 0.000078 Loss 7.595245, Accuracy 75.760%\n",
      "Epoch 13, Batch 945, LR 0.000078 Loss 7.594398, Accuracy 75.764%\n",
      "Epoch 13, Batch 946, LR 0.000078 Loss 7.594835, Accuracy 75.760%\n",
      "Epoch 13, Batch 947, LR 0.000078 Loss 7.594517, Accuracy 75.764%\n",
      "Epoch 13, Batch 948, LR 0.000078 Loss 7.593604, Accuracy 75.769%\n",
      "Epoch 13, Batch 949, LR 0.000078 Loss 7.593377, Accuracy 75.766%\n",
      "Epoch 13, Batch 950, LR 0.000078 Loss 7.593641, Accuracy 75.762%\n",
      "Epoch 13, Batch 951, LR 0.000078 Loss 7.592858, Accuracy 75.767%\n",
      "Epoch 13, Batch 952, LR 0.000078 Loss 7.593496, Accuracy 75.766%\n",
      "Epoch 13, Batch 953, LR 0.000078 Loss 7.593181, Accuracy 75.770%\n",
      "Epoch 13, Batch 954, LR 0.000078 Loss 7.593222, Accuracy 75.771%\n",
      "Epoch 13, Batch 955, LR 0.000078 Loss 7.593502, Accuracy 75.771%\n",
      "Epoch 13, Batch 956, LR 0.000078 Loss 7.593105, Accuracy 75.779%\n",
      "Epoch 13, Batch 957, LR 0.000078 Loss 7.593900, Accuracy 75.776%\n",
      "Epoch 13, Batch 958, LR 0.000078 Loss 7.595053, Accuracy 75.769%\n",
      "Epoch 13, Batch 959, LR 0.000078 Loss 7.595566, Accuracy 75.762%\n",
      "Epoch 13, Batch 960, LR 0.000078 Loss 7.596380, Accuracy 75.756%\n",
      "Epoch 13, Batch 961, LR 0.000078 Loss 7.595131, Accuracy 75.760%\n",
      "Epoch 13, Batch 962, LR 0.000078 Loss 7.593330, Accuracy 75.766%\n",
      "Epoch 13, Batch 963, LR 0.000078 Loss 7.593840, Accuracy 75.766%\n",
      "Epoch 13, Batch 964, LR 0.000078 Loss 7.594263, Accuracy 75.766%\n",
      "Epoch 13, Batch 965, LR 0.000078 Loss 7.593432, Accuracy 75.767%\n",
      "Epoch 13, Batch 966, LR 0.000078 Loss 7.593767, Accuracy 75.767%\n",
      "Epoch 13, Batch 967, LR 0.000078 Loss 7.593883, Accuracy 75.770%\n",
      "Epoch 13, Batch 968, LR 0.000078 Loss 7.594335, Accuracy 75.774%\n",
      "Epoch 13, Batch 969, LR 0.000078 Loss 7.594445, Accuracy 75.769%\n",
      "Epoch 13, Batch 970, LR 0.000078 Loss 7.593879, Accuracy 75.773%\n",
      "Epoch 13, Batch 971, LR 0.000078 Loss 7.593952, Accuracy 75.776%\n",
      "Epoch 13, Batch 972, LR 0.000078 Loss 7.593957, Accuracy 75.776%\n",
      "Epoch 13, Batch 973, LR 0.000078 Loss 7.593906, Accuracy 75.780%\n",
      "Epoch 13, Batch 974, LR 0.000078 Loss 7.593211, Accuracy 75.785%\n",
      "Epoch 13, Batch 975, LR 0.000078 Loss 7.592448, Accuracy 75.792%\n",
      "Epoch 13, Batch 976, LR 0.000078 Loss 7.592987, Accuracy 75.795%\n",
      "Epoch 13, Batch 977, LR 0.000078 Loss 7.593353, Accuracy 75.791%\n",
      "Epoch 13, Batch 978, LR 0.000078 Loss 7.594461, Accuracy 75.785%\n",
      "Epoch 13, Batch 979, LR 0.000078 Loss 7.594685, Accuracy 75.780%\n",
      "Epoch 13, Batch 980, LR 0.000078 Loss 7.594254, Accuracy 75.784%\n",
      "Epoch 13, Batch 981, LR 0.000078 Loss 7.593981, Accuracy 75.783%\n",
      "Epoch 13, Batch 982, LR 0.000078 Loss 7.593510, Accuracy 75.783%\n",
      "Epoch 13, Batch 983, LR 0.000078 Loss 7.593218, Accuracy 75.780%\n",
      "Epoch 13, Batch 984, LR 0.000078 Loss 7.592685, Accuracy 75.784%\n",
      "Epoch 13, Batch 985, LR 0.000078 Loss 7.593530, Accuracy 75.778%\n",
      "Epoch 13, Batch 986, LR 0.000078 Loss 7.593880, Accuracy 75.782%\n",
      "Epoch 13, Batch 987, LR 0.000078 Loss 7.593257, Accuracy 75.788%\n",
      "Epoch 13, Batch 988, LR 0.000078 Loss 7.593445, Accuracy 75.783%\n",
      "Epoch 13, Batch 989, LR 0.000078 Loss 7.593612, Accuracy 75.782%\n",
      "Epoch 13, Batch 990, LR 0.000078 Loss 7.593724, Accuracy 75.780%\n",
      "Epoch 13, Batch 991, LR 0.000078 Loss 7.593682, Accuracy 75.777%\n",
      "Epoch 13, Batch 992, LR 0.000078 Loss 7.593119, Accuracy 75.782%\n",
      "Epoch 13, Batch 993, LR 0.000078 Loss 7.593353, Accuracy 75.783%\n",
      "Epoch 13, Batch 994, LR 0.000078 Loss 7.593355, Accuracy 75.781%\n",
      "Epoch 13, Batch 995, LR 0.000078 Loss 7.593191, Accuracy 75.779%\n",
      "Epoch 13, Batch 996, LR 0.000078 Loss 7.592077, Accuracy 75.783%\n",
      "Epoch 13, Batch 997, LR 0.000078 Loss 7.592146, Accuracy 75.783%\n",
      "Epoch 13, Batch 998, LR 0.000078 Loss 7.592157, Accuracy 75.783%\n",
      "Epoch 13, Batch 999, LR 0.000078 Loss 7.591492, Accuracy 75.785%\n",
      "Epoch 13, Batch 1000, LR 0.000078 Loss 7.591761, Accuracy 75.787%\n",
      "Epoch 13, Batch 1001, LR 0.000077 Loss 7.590970, Accuracy 75.791%\n",
      "Epoch 13, Batch 1002, LR 0.000077 Loss 7.591088, Accuracy 75.787%\n",
      "Epoch 13, Batch 1003, LR 0.000077 Loss 7.591095, Accuracy 75.787%\n",
      "Epoch 13, Batch 1004, LR 0.000077 Loss 7.590658, Accuracy 75.793%\n",
      "Epoch 13, Batch 1005, LR 0.000077 Loss 7.591241, Accuracy 75.787%\n",
      "Epoch 13, Batch 1006, LR 0.000077 Loss 7.590270, Accuracy 75.791%\n",
      "Epoch 13, Batch 1007, LR 0.000077 Loss 7.589800, Accuracy 75.791%\n",
      "Epoch 13, Batch 1008, LR 0.000077 Loss 7.590190, Accuracy 75.792%\n",
      "Epoch 13, Batch 1009, LR 0.000077 Loss 7.590143, Accuracy 75.786%\n",
      "Epoch 13, Batch 1010, LR 0.000077 Loss 7.590521, Accuracy 75.785%\n",
      "Epoch 13, Batch 1011, LR 0.000077 Loss 7.590355, Accuracy 75.789%\n",
      "Epoch 13, Batch 1012, LR 0.000077 Loss 7.590955, Accuracy 75.784%\n",
      "Epoch 13, Batch 1013, LR 0.000077 Loss 7.591617, Accuracy 75.781%\n",
      "Epoch 13, Batch 1014, LR 0.000077 Loss 7.591861, Accuracy 75.784%\n",
      "Epoch 13, Batch 1015, LR 0.000077 Loss 7.591888, Accuracy 75.782%\n",
      "Epoch 13, Batch 1016, LR 0.000077 Loss 7.591919, Accuracy 75.777%\n",
      "Epoch 13, Batch 1017, LR 0.000077 Loss 7.591792, Accuracy 75.778%\n",
      "Epoch 13, Batch 1018, LR 0.000077 Loss 7.592112, Accuracy 75.780%\n",
      "Epoch 13, Batch 1019, LR 0.000077 Loss 7.593100, Accuracy 75.772%\n",
      "Epoch 13, Batch 1020, LR 0.000077 Loss 7.593849, Accuracy 75.767%\n",
      "Epoch 13, Batch 1021, LR 0.000077 Loss 7.593984, Accuracy 75.768%\n",
      "Epoch 13, Batch 1022, LR 0.000077 Loss 7.594204, Accuracy 75.764%\n",
      "Epoch 13, Batch 1023, LR 0.000077 Loss 7.594286, Accuracy 75.768%\n",
      "Epoch 13, Batch 1024, LR 0.000077 Loss 7.594110, Accuracy 75.772%\n",
      "Epoch 13, Batch 1025, LR 0.000077 Loss 7.594109, Accuracy 75.774%\n",
      "Epoch 13, Batch 1026, LR 0.000077 Loss 7.593710, Accuracy 75.774%\n",
      "Epoch 13, Batch 1027, LR 0.000077 Loss 7.593224, Accuracy 75.777%\n",
      "Epoch 13, Batch 1028, LR 0.000077 Loss 7.592648, Accuracy 75.780%\n",
      "Epoch 13, Batch 1029, LR 0.000077 Loss 7.591854, Accuracy 75.786%\n",
      "Epoch 13, Batch 1030, LR 0.000077 Loss 7.591309, Accuracy 75.794%\n",
      "Epoch 13, Batch 1031, LR 0.000077 Loss 7.591394, Accuracy 75.792%\n",
      "Epoch 13, Batch 1032, LR 0.000077 Loss 7.590617, Accuracy 75.795%\n",
      "Epoch 13, Batch 1033, LR 0.000077 Loss 7.591040, Accuracy 75.794%\n",
      "Epoch 13, Batch 1034, LR 0.000077 Loss 7.591446, Accuracy 75.792%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 1035, LR 0.000077 Loss 7.591879, Accuracy 75.788%\n",
      "Epoch 13, Batch 1036, LR 0.000077 Loss 7.592746, Accuracy 75.784%\n",
      "Epoch 13, Batch 1037, LR 0.000077 Loss 7.593682, Accuracy 75.782%\n",
      "Epoch 13, Batch 1038, LR 0.000077 Loss 7.593731, Accuracy 75.781%\n",
      "Epoch 13, Batch 1039, LR 0.000077 Loss 7.594476, Accuracy 75.771%\n",
      "Epoch 13, Batch 1040, LR 0.000077 Loss 7.594629, Accuracy 75.770%\n",
      "Epoch 13, Batch 1041, LR 0.000077 Loss 7.594467, Accuracy 75.772%\n",
      "Epoch 13, Batch 1042, LR 0.000077 Loss 7.593747, Accuracy 75.776%\n",
      "Epoch 13, Batch 1043, LR 0.000077 Loss 7.593067, Accuracy 75.777%\n",
      "Epoch 13, Batch 1044, LR 0.000077 Loss 7.592521, Accuracy 75.781%\n",
      "Epoch 13, Batch 1045, LR 0.000077 Loss 7.592080, Accuracy 75.786%\n",
      "Epoch 13, Batch 1046, LR 0.000077 Loss 7.591641, Accuracy 75.787%\n",
      "Epoch 13, Batch 1047, LR 0.000077 Loss 7.591261, Accuracy 75.786%\n",
      "Epoch 13, Loss (train set) 7.591261, Accuracy (train set) 75.786%\n",
      "Epoch 14, Batch 1, LR 0.000077 Loss 7.466892, Accuracy 77.344%\n",
      "Epoch 14, Batch 2, LR 0.000077 Loss 6.922348, Accuracy 79.688%\n",
      "Epoch 14, Batch 3, LR 0.000077 Loss 7.212105, Accuracy 77.865%\n",
      "Epoch 14, Batch 4, LR 0.000077 Loss 7.071881, Accuracy 78.711%\n",
      "Epoch 14, Batch 5, LR 0.000077 Loss 7.234427, Accuracy 78.750%\n",
      "Epoch 14, Batch 6, LR 0.000077 Loss 7.131989, Accuracy 79.036%\n",
      "Epoch 14, Batch 7, LR 0.000077 Loss 7.142584, Accuracy 78.460%\n",
      "Epoch 14, Batch 8, LR 0.000077 Loss 7.104665, Accuracy 78.711%\n",
      "Epoch 14, Batch 9, LR 0.000077 Loss 7.093830, Accuracy 78.646%\n",
      "Epoch 14, Batch 10, LR 0.000077 Loss 7.139488, Accuracy 78.359%\n",
      "Epoch 14, Batch 11, LR 0.000077 Loss 7.053785, Accuracy 78.551%\n",
      "Epoch 14, Batch 12, LR 0.000077 Loss 7.068658, Accuracy 78.451%\n",
      "Epoch 14, Batch 13, LR 0.000077 Loss 7.092350, Accuracy 78.185%\n",
      "Epoch 14, Batch 14, LR 0.000077 Loss 7.102457, Accuracy 78.292%\n",
      "Epoch 14, Batch 15, LR 0.000077 Loss 7.069894, Accuracy 78.385%\n",
      "Epoch 14, Batch 16, LR 0.000077 Loss 7.080283, Accuracy 78.467%\n",
      "Epoch 14, Batch 17, LR 0.000077 Loss 7.078881, Accuracy 78.539%\n",
      "Epoch 14, Batch 18, LR 0.000077 Loss 7.072608, Accuracy 78.733%\n",
      "Epoch 14, Batch 19, LR 0.000077 Loss 7.098868, Accuracy 78.495%\n",
      "Epoch 14, Batch 20, LR 0.000077 Loss 7.149737, Accuracy 78.086%\n",
      "Epoch 14, Batch 21, LR 0.000077 Loss 7.180714, Accuracy 77.976%\n",
      "Epoch 14, Batch 22, LR 0.000077 Loss 7.163395, Accuracy 78.161%\n",
      "Epoch 14, Batch 23, LR 0.000077 Loss 7.137555, Accuracy 78.397%\n",
      "Epoch 14, Batch 24, LR 0.000077 Loss 7.148572, Accuracy 78.320%\n",
      "Epoch 14, Batch 25, LR 0.000077 Loss 7.170920, Accuracy 78.281%\n",
      "Epoch 14, Batch 26, LR 0.000077 Loss 7.197825, Accuracy 78.095%\n",
      "Epoch 14, Batch 27, LR 0.000077 Loss 7.191125, Accuracy 78.009%\n",
      "Epoch 14, Batch 28, LR 0.000077 Loss 7.184256, Accuracy 78.125%\n",
      "Epoch 14, Batch 29, LR 0.000077 Loss 7.198026, Accuracy 78.071%\n",
      "Epoch 14, Batch 30, LR 0.000077 Loss 7.221159, Accuracy 77.865%\n",
      "Epoch 14, Batch 31, LR 0.000077 Loss 7.226998, Accuracy 77.848%\n",
      "Epoch 14, Batch 32, LR 0.000077 Loss 7.203590, Accuracy 77.954%\n",
      "Epoch 14, Batch 33, LR 0.000077 Loss 7.185796, Accuracy 78.054%\n",
      "Epoch 14, Batch 34, LR 0.000077 Loss 7.166915, Accuracy 78.194%\n",
      "Epoch 14, Batch 35, LR 0.000077 Loss 7.151960, Accuracy 78.348%\n",
      "Epoch 14, Batch 36, LR 0.000077 Loss 7.166905, Accuracy 78.277%\n",
      "Epoch 14, Batch 37, LR 0.000077 Loss 7.151391, Accuracy 78.442%\n",
      "Epoch 14, Batch 38, LR 0.000077 Loss 7.173550, Accuracy 78.310%\n",
      "Epoch 14, Batch 39, LR 0.000077 Loss 7.174634, Accuracy 78.325%\n",
      "Epoch 14, Batch 40, LR 0.000077 Loss 7.182290, Accuracy 78.301%\n",
      "Epoch 14, Batch 41, LR 0.000077 Loss 7.172689, Accuracy 78.373%\n",
      "Epoch 14, Batch 42, LR 0.000077 Loss 7.176691, Accuracy 78.237%\n",
      "Epoch 14, Batch 43, LR 0.000077 Loss 7.189726, Accuracy 78.270%\n",
      "Epoch 14, Batch 44, LR 0.000077 Loss 7.190293, Accuracy 78.232%\n",
      "Epoch 14, Batch 45, LR 0.000077 Loss 7.203851, Accuracy 78.073%\n",
      "Epoch 14, Batch 46, LR 0.000077 Loss 7.204358, Accuracy 78.142%\n",
      "Epoch 14, Batch 47, LR 0.000077 Loss 7.190933, Accuracy 78.175%\n",
      "Epoch 14, Batch 48, LR 0.000077 Loss 7.188410, Accuracy 78.206%\n",
      "Epoch 14, Batch 49, LR 0.000077 Loss 7.194661, Accuracy 78.189%\n",
      "Epoch 14, Batch 50, LR 0.000077 Loss 7.184262, Accuracy 78.344%\n",
      "Epoch 14, Batch 51, LR 0.000077 Loss 7.200346, Accuracy 78.294%\n",
      "Epoch 14, Batch 52, LR 0.000077 Loss 7.184710, Accuracy 78.395%\n",
      "Epoch 14, Batch 53, LR 0.000077 Loss 7.177570, Accuracy 78.435%\n",
      "Epoch 14, Batch 54, LR 0.000077 Loss 7.175933, Accuracy 78.414%\n",
      "Epoch 14, Batch 55, LR 0.000077 Loss 7.182318, Accuracy 78.395%\n",
      "Epoch 14, Batch 56, LR 0.000077 Loss 7.189474, Accuracy 78.390%\n",
      "Epoch 14, Batch 57, LR 0.000077 Loss 7.200898, Accuracy 78.344%\n",
      "Epoch 14, Batch 58, LR 0.000077 Loss 7.204205, Accuracy 78.341%\n",
      "Epoch 14, Batch 59, LR 0.000077 Loss 7.212732, Accuracy 78.257%\n",
      "Epoch 14, Batch 60, LR 0.000077 Loss 7.220224, Accuracy 78.242%\n",
      "Epoch 14, Batch 61, LR 0.000077 Loss 7.225039, Accuracy 78.176%\n",
      "Epoch 14, Batch 62, LR 0.000077 Loss 7.225969, Accuracy 78.150%\n",
      "Epoch 14, Batch 63, LR 0.000077 Loss 7.235567, Accuracy 77.989%\n",
      "Epoch 14, Batch 64, LR 0.000077 Loss 7.243630, Accuracy 77.966%\n",
      "Epoch 14, Batch 65, LR 0.000077 Loss 7.240903, Accuracy 77.873%\n",
      "Epoch 14, Batch 66, LR 0.000077 Loss 7.256122, Accuracy 77.758%\n",
      "Epoch 14, Batch 67, LR 0.000077 Loss 7.248606, Accuracy 77.810%\n",
      "Epoch 14, Batch 68, LR 0.000077 Loss 7.249225, Accuracy 77.780%\n",
      "Epoch 14, Batch 69, LR 0.000077 Loss 7.249508, Accuracy 77.797%\n",
      "Epoch 14, Batch 70, LR 0.000077 Loss 7.244517, Accuracy 77.801%\n",
      "Epoch 14, Batch 71, LR 0.000077 Loss 7.250291, Accuracy 77.773%\n",
      "Epoch 14, Batch 72, LR 0.000077 Loss 7.260542, Accuracy 77.745%\n",
      "Epoch 14, Batch 73, LR 0.000077 Loss 7.243985, Accuracy 77.815%\n",
      "Epoch 14, Batch 74, LR 0.000077 Loss 7.249744, Accuracy 77.755%\n",
      "Epoch 14, Batch 75, LR 0.000077 Loss 7.252681, Accuracy 77.760%\n",
      "Epoch 14, Batch 76, LR 0.000077 Loss 7.249876, Accuracy 77.786%\n",
      "Epoch 14, Batch 77, LR 0.000077 Loss 7.254785, Accuracy 77.780%\n",
      "Epoch 14, Batch 78, LR 0.000077 Loss 7.259147, Accuracy 77.734%\n",
      "Epoch 14, Batch 79, LR 0.000077 Loss 7.253352, Accuracy 77.779%\n",
      "Epoch 14, Batch 80, LR 0.000077 Loss 7.258114, Accuracy 77.764%\n",
      "Epoch 14, Batch 81, LR 0.000077 Loss 7.265502, Accuracy 77.749%\n",
      "Epoch 14, Batch 82, LR 0.000077 Loss 7.263317, Accuracy 77.792%\n",
      "Epoch 14, Batch 83, LR 0.000077 Loss 7.252964, Accuracy 77.833%\n",
      "Epoch 14, Batch 84, LR 0.000077 Loss 7.259035, Accuracy 77.865%\n",
      "Epoch 14, Batch 85, LR 0.000077 Loss 7.257208, Accuracy 77.849%\n",
      "Epoch 14, Batch 86, LR 0.000077 Loss 7.256540, Accuracy 77.852%\n",
      "Epoch 14, Batch 87, LR 0.000077 Loss 7.266029, Accuracy 77.793%\n",
      "Epoch 14, Batch 88, LR 0.000077 Loss 7.261411, Accuracy 77.761%\n",
      "Epoch 14, Batch 89, LR 0.000077 Loss 7.257894, Accuracy 77.800%\n",
      "Epoch 14, Batch 90, LR 0.000077 Loss 7.258660, Accuracy 77.778%\n",
      "Epoch 14, Batch 91, LR 0.000077 Loss 7.258567, Accuracy 77.782%\n",
      "Epoch 14, Batch 92, LR 0.000077 Loss 7.259262, Accuracy 77.768%\n",
      "Epoch 14, Batch 93, LR 0.000077 Loss 7.259396, Accuracy 77.755%\n",
      "Epoch 14, Batch 94, LR 0.000077 Loss 7.262115, Accuracy 77.751%\n",
      "Epoch 14, Batch 95, LR 0.000077 Loss 7.255499, Accuracy 77.771%\n",
      "Epoch 14, Batch 96, LR 0.000077 Loss 7.257608, Accuracy 77.775%\n",
      "Epoch 14, Batch 97, LR 0.000077 Loss 7.257682, Accuracy 77.779%\n",
      "Epoch 14, Batch 98, LR 0.000077 Loss 7.254541, Accuracy 77.838%\n",
      "Epoch 14, Batch 99, LR 0.000077 Loss 7.251375, Accuracy 77.833%\n",
      "Epoch 14, Batch 100, LR 0.000077 Loss 7.243467, Accuracy 77.922%\n",
      "Epoch 14, Batch 101, LR 0.000077 Loss 7.234065, Accuracy 77.994%\n",
      "Epoch 14, Batch 102, LR 0.000077 Loss 7.235301, Accuracy 77.987%\n",
      "Epoch 14, Batch 103, LR 0.000077 Loss 7.232687, Accuracy 77.973%\n",
      "Epoch 14, Batch 104, LR 0.000077 Loss 7.228630, Accuracy 77.982%\n",
      "Epoch 14, Batch 105, LR 0.000077 Loss 7.232788, Accuracy 77.902%\n",
      "Epoch 14, Batch 106, LR 0.000077 Loss 7.231517, Accuracy 77.919%\n",
      "Epoch 14, Batch 107, LR 0.000077 Loss 7.232306, Accuracy 77.928%\n",
      "Epoch 14, Batch 108, LR 0.000077 Loss 7.232609, Accuracy 77.886%\n",
      "Epoch 14, Batch 109, LR 0.000077 Loss 7.234632, Accuracy 77.903%\n",
      "Epoch 14, Batch 110, LR 0.000077 Loss 7.227970, Accuracy 77.962%\n",
      "Epoch 14, Batch 111, LR 0.000077 Loss 7.227342, Accuracy 77.914%\n",
      "Epoch 14, Batch 112, LR 0.000077 Loss 7.220793, Accuracy 77.972%\n",
      "Epoch 14, Batch 113, LR 0.000077 Loss 7.219131, Accuracy 77.980%\n",
      "Epoch 14, Batch 114, LR 0.000077 Loss 7.219979, Accuracy 77.961%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 115, LR 0.000077 Loss 7.227490, Accuracy 77.887%\n",
      "Epoch 14, Batch 116, LR 0.000077 Loss 7.224960, Accuracy 77.909%\n",
      "Epoch 14, Batch 117, LR 0.000077 Loss 7.231103, Accuracy 77.845%\n",
      "Epoch 14, Batch 118, LR 0.000077 Loss 7.235405, Accuracy 77.807%\n",
      "Epoch 14, Batch 119, LR 0.000077 Loss 7.237129, Accuracy 77.810%\n",
      "Epoch 14, Batch 120, LR 0.000077 Loss 7.239557, Accuracy 77.806%\n",
      "Epoch 14, Batch 121, LR 0.000077 Loss 7.241714, Accuracy 77.834%\n",
      "Epoch 14, Batch 122, LR 0.000077 Loss 7.234076, Accuracy 77.837%\n",
      "Epoch 14, Batch 123, LR 0.000077 Loss 7.238334, Accuracy 77.833%\n",
      "Epoch 14, Batch 124, LR 0.000077 Loss 7.245067, Accuracy 77.804%\n",
      "Epoch 14, Batch 125, LR 0.000077 Loss 7.248266, Accuracy 77.744%\n",
      "Epoch 14, Batch 126, LR 0.000077 Loss 7.243244, Accuracy 77.778%\n",
      "Epoch 14, Batch 127, LR 0.000077 Loss 7.238877, Accuracy 77.817%\n",
      "Epoch 14, Batch 128, LR 0.000077 Loss 7.243966, Accuracy 77.759%\n",
      "Epoch 14, Batch 129, LR 0.000077 Loss 7.246344, Accuracy 77.750%\n",
      "Epoch 14, Batch 130, LR 0.000077 Loss 7.246295, Accuracy 77.776%\n",
      "Epoch 14, Batch 131, LR 0.000077 Loss 7.250206, Accuracy 77.749%\n",
      "Epoch 14, Batch 132, LR 0.000077 Loss 7.251861, Accuracy 77.740%\n",
      "Epoch 14, Batch 133, LR 0.000077 Loss 7.247696, Accuracy 77.778%\n",
      "Epoch 14, Batch 134, LR 0.000077 Loss 7.247741, Accuracy 77.758%\n",
      "Epoch 14, Batch 135, LR 0.000077 Loss 7.244586, Accuracy 77.784%\n",
      "Epoch 14, Batch 136, LR 0.000077 Loss 7.244641, Accuracy 77.780%\n",
      "Epoch 14, Batch 137, LR 0.000077 Loss 7.234042, Accuracy 77.823%\n",
      "Epoch 14, Batch 138, LR 0.000077 Loss 7.231811, Accuracy 77.836%\n",
      "Epoch 14, Batch 139, LR 0.000077 Loss 7.228733, Accuracy 77.855%\n",
      "Epoch 14, Batch 140, LR 0.000077 Loss 7.231411, Accuracy 77.818%\n",
      "Epoch 14, Batch 141, LR 0.000077 Loss 7.234717, Accuracy 77.826%\n",
      "Epoch 14, Batch 142, LR 0.000077 Loss 7.239497, Accuracy 77.811%\n",
      "Epoch 14, Batch 143, LR 0.000077 Loss 7.237941, Accuracy 77.841%\n",
      "Epoch 14, Batch 144, LR 0.000077 Loss 7.235500, Accuracy 77.870%\n",
      "Epoch 14, Batch 145, LR 0.000077 Loss 7.235533, Accuracy 77.872%\n",
      "Epoch 14, Batch 146, LR 0.000077 Loss 7.230438, Accuracy 77.884%\n",
      "Epoch 14, Batch 147, LR 0.000077 Loss 7.227707, Accuracy 77.939%\n",
      "Epoch 14, Batch 148, LR 0.000077 Loss 7.231497, Accuracy 77.898%\n",
      "Epoch 14, Batch 149, LR 0.000077 Loss 7.228495, Accuracy 77.889%\n",
      "Epoch 14, Batch 150, LR 0.000077 Loss 7.226144, Accuracy 77.906%\n",
      "Epoch 14, Batch 151, LR 0.000077 Loss 7.225935, Accuracy 77.877%\n",
      "Epoch 14, Batch 152, LR 0.000077 Loss 7.223320, Accuracy 77.919%\n",
      "Epoch 14, Batch 153, LR 0.000077 Loss 7.221331, Accuracy 77.931%\n",
      "Epoch 14, Batch 154, LR 0.000077 Loss 7.221821, Accuracy 77.932%\n",
      "Epoch 14, Batch 155, LR 0.000077 Loss 7.215335, Accuracy 77.974%\n",
      "Epoch 14, Batch 156, LR 0.000077 Loss 7.212309, Accuracy 78.000%\n",
      "Epoch 14, Batch 157, LR 0.000077 Loss 7.214144, Accuracy 77.966%\n",
      "Epoch 14, Batch 158, LR 0.000077 Loss 7.216586, Accuracy 77.932%\n",
      "Epoch 14, Batch 159, LR 0.000077 Loss 7.217660, Accuracy 77.894%\n",
      "Epoch 14, Batch 160, LR 0.000077 Loss 7.218862, Accuracy 77.886%\n",
      "Epoch 14, Batch 161, LR 0.000077 Loss 7.219188, Accuracy 77.887%\n",
      "Epoch 14, Batch 162, LR 0.000077 Loss 7.216158, Accuracy 77.908%\n",
      "Epoch 14, Batch 163, LR 0.000077 Loss 7.218774, Accuracy 77.890%\n",
      "Epoch 14, Batch 164, LR 0.000077 Loss 7.220412, Accuracy 77.853%\n",
      "Epoch 14, Batch 165, LR 0.000077 Loss 7.224496, Accuracy 77.841%\n",
      "Epoch 14, Batch 166, LR 0.000077 Loss 7.231764, Accuracy 77.805%\n",
      "Epoch 14, Batch 167, LR 0.000077 Loss 7.231648, Accuracy 77.788%\n",
      "Epoch 14, Batch 168, LR 0.000077 Loss 7.232431, Accuracy 77.776%\n",
      "Epoch 14, Batch 169, LR 0.000077 Loss 7.233831, Accuracy 77.778%\n",
      "Epoch 14, Batch 170, LR 0.000077 Loss 7.232767, Accuracy 77.780%\n",
      "Epoch 14, Batch 171, LR 0.000077 Loss 7.231999, Accuracy 77.787%\n",
      "Epoch 14, Batch 172, LR 0.000077 Loss 7.235087, Accuracy 77.757%\n",
      "Epoch 14, Batch 173, LR 0.000077 Loss 7.233764, Accuracy 77.764%\n",
      "Epoch 14, Batch 174, LR 0.000077 Loss 7.233975, Accuracy 77.757%\n",
      "Epoch 14, Batch 175, LR 0.000077 Loss 7.234429, Accuracy 77.763%\n",
      "Epoch 14, Batch 176, LR 0.000077 Loss 7.237168, Accuracy 77.752%\n",
      "Epoch 14, Batch 177, LR 0.000077 Loss 7.236392, Accuracy 77.745%\n",
      "Epoch 14, Batch 178, LR 0.000077 Loss 7.237767, Accuracy 77.712%\n",
      "Epoch 14, Batch 179, LR 0.000077 Loss 7.236871, Accuracy 77.710%\n",
      "Epoch 14, Batch 180, LR 0.000077 Loss 7.234963, Accuracy 77.704%\n",
      "Epoch 14, Batch 181, LR 0.000077 Loss 7.237661, Accuracy 77.680%\n",
      "Epoch 14, Batch 182, LR 0.000077 Loss 7.233581, Accuracy 77.704%\n",
      "Epoch 14, Batch 183, LR 0.000077 Loss 7.230882, Accuracy 77.698%\n",
      "Epoch 14, Batch 184, LR 0.000077 Loss 7.232142, Accuracy 77.692%\n",
      "Epoch 14, Batch 185, LR 0.000077 Loss 7.232498, Accuracy 77.682%\n",
      "Epoch 14, Batch 186, LR 0.000077 Loss 7.236974, Accuracy 77.646%\n",
      "Epoch 14, Batch 187, LR 0.000077 Loss 7.241464, Accuracy 77.653%\n",
      "Epoch 14, Batch 188, LR 0.000077 Loss 7.242668, Accuracy 77.651%\n",
      "Epoch 14, Batch 189, LR 0.000077 Loss 7.245561, Accuracy 77.646%\n",
      "Epoch 14, Batch 190, LR 0.000077 Loss 7.245995, Accuracy 77.648%\n",
      "Epoch 14, Batch 191, LR 0.000077 Loss 7.244661, Accuracy 77.646%\n",
      "Epoch 14, Batch 192, LR 0.000077 Loss 7.241299, Accuracy 77.681%\n",
      "Epoch 14, Batch 193, LR 0.000077 Loss 7.240579, Accuracy 77.692%\n",
      "Epoch 14, Batch 194, LR 0.000077 Loss 7.244862, Accuracy 77.682%\n",
      "Epoch 14, Batch 195, LR 0.000077 Loss 7.246042, Accuracy 77.668%\n",
      "Epoch 14, Batch 196, LR 0.000077 Loss 7.248247, Accuracy 77.667%\n",
      "Epoch 14, Batch 197, LR 0.000077 Loss 7.254463, Accuracy 77.613%\n",
      "Epoch 14, Batch 198, LR 0.000077 Loss 7.253991, Accuracy 77.624%\n",
      "Epoch 14, Batch 199, LR 0.000077 Loss 7.254713, Accuracy 77.622%\n",
      "Epoch 14, Batch 200, LR 0.000077 Loss 7.254056, Accuracy 77.625%\n",
      "Epoch 14, Batch 201, LR 0.000077 Loss 7.257024, Accuracy 77.596%\n",
      "Epoch 14, Batch 202, LR 0.000077 Loss 7.256811, Accuracy 77.595%\n",
      "Epoch 14, Batch 203, LR 0.000077 Loss 7.253456, Accuracy 77.598%\n",
      "Epoch 14, Batch 204, LR 0.000077 Loss 7.252859, Accuracy 77.585%\n",
      "Epoch 14, Batch 205, LR 0.000077 Loss 7.254084, Accuracy 77.580%\n",
      "Epoch 14, Batch 206, LR 0.000077 Loss 7.254879, Accuracy 77.575%\n",
      "Epoch 14, Batch 207, LR 0.000077 Loss 7.257505, Accuracy 77.563%\n",
      "Epoch 14, Batch 208, LR 0.000077 Loss 7.260737, Accuracy 77.532%\n",
      "Epoch 14, Batch 209, LR 0.000077 Loss 7.260178, Accuracy 77.504%\n",
      "Epoch 14, Batch 210, LR 0.000077 Loss 7.262539, Accuracy 77.496%\n",
      "Epoch 14, Batch 211, LR 0.000077 Loss 7.260808, Accuracy 77.510%\n",
      "Epoch 14, Batch 212, LR 0.000077 Loss 7.261787, Accuracy 77.513%\n",
      "Epoch 14, Batch 213, LR 0.000077 Loss 7.261762, Accuracy 77.523%\n",
      "Epoch 14, Batch 214, LR 0.000077 Loss 7.267007, Accuracy 77.515%\n",
      "Epoch 14, Batch 215, LR 0.000077 Loss 7.263627, Accuracy 77.540%\n",
      "Epoch 14, Batch 216, LR 0.000077 Loss 7.261162, Accuracy 77.543%\n",
      "Epoch 14, Batch 217, LR 0.000077 Loss 7.262684, Accuracy 77.531%\n",
      "Epoch 14, Batch 218, LR 0.000077 Loss 7.266168, Accuracy 77.512%\n",
      "Epoch 14, Batch 219, LR 0.000077 Loss 7.269692, Accuracy 77.479%\n",
      "Epoch 14, Batch 220, LR 0.000077 Loss 7.273415, Accuracy 77.443%\n",
      "Epoch 14, Batch 221, LR 0.000077 Loss 7.273692, Accuracy 77.450%\n",
      "Epoch 14, Batch 222, LR 0.000077 Loss 7.277382, Accuracy 77.446%\n",
      "Epoch 14, Batch 223, LR 0.000077 Loss 7.278018, Accuracy 77.449%\n",
      "Epoch 14, Batch 224, LR 0.000077 Loss 7.278032, Accuracy 77.441%\n",
      "Epoch 14, Batch 225, LR 0.000077 Loss 7.279647, Accuracy 77.424%\n",
      "Epoch 14, Batch 226, LR 0.000077 Loss 7.279940, Accuracy 77.423%\n",
      "Epoch 14, Batch 227, LR 0.000077 Loss 7.278260, Accuracy 77.447%\n",
      "Epoch 14, Batch 228, LR 0.000077 Loss 7.278453, Accuracy 77.447%\n",
      "Epoch 14, Batch 229, LR 0.000077 Loss 7.275309, Accuracy 77.477%\n",
      "Epoch 14, Batch 230, LR 0.000077 Loss 7.274686, Accuracy 77.476%\n",
      "Epoch 14, Batch 231, LR 0.000077 Loss 7.275551, Accuracy 77.459%\n",
      "Epoch 14, Batch 232, LR 0.000077 Loss 7.273677, Accuracy 77.465%\n",
      "Epoch 14, Batch 233, LR 0.000077 Loss 7.274596, Accuracy 77.468%\n",
      "Epoch 14, Batch 234, LR 0.000077 Loss 7.275904, Accuracy 77.461%\n",
      "Epoch 14, Batch 235, LR 0.000077 Loss 7.277078, Accuracy 77.463%\n",
      "Epoch 14, Batch 236, LR 0.000077 Loss 7.274894, Accuracy 77.483%\n",
      "Epoch 14, Batch 237, LR 0.000077 Loss 7.275196, Accuracy 77.479%\n",
      "Epoch 14, Batch 238, LR 0.000077 Loss 7.276403, Accuracy 77.465%\n",
      "Epoch 14, Batch 239, LR 0.000077 Loss 7.278634, Accuracy 77.465%\n",
      "Epoch 14, Batch 240, LR 0.000077 Loss 7.278480, Accuracy 77.451%\n",
      "Epoch 14, Batch 241, LR 0.000077 Loss 7.278352, Accuracy 77.457%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 242, LR 0.000077 Loss 7.277938, Accuracy 77.460%\n",
      "Epoch 14, Batch 243, LR 0.000077 Loss 7.278626, Accuracy 77.453%\n",
      "Epoch 14, Batch 244, LR 0.000077 Loss 7.282818, Accuracy 77.401%\n",
      "Epoch 14, Batch 245, LR 0.000077 Loss 7.281331, Accuracy 77.423%\n",
      "Epoch 14, Batch 246, LR 0.000077 Loss 7.279811, Accuracy 77.433%\n",
      "Epoch 14, Batch 247, LR 0.000077 Loss 7.280364, Accuracy 77.445%\n",
      "Epoch 14, Batch 248, LR 0.000077 Loss 7.281702, Accuracy 77.426%\n",
      "Epoch 14, Batch 249, LR 0.000077 Loss 7.281733, Accuracy 77.422%\n",
      "Epoch 14, Batch 250, LR 0.000077 Loss 7.281420, Accuracy 77.438%\n",
      "Epoch 14, Batch 251, LR 0.000077 Loss 7.282386, Accuracy 77.434%\n",
      "Epoch 14, Batch 252, LR 0.000077 Loss 7.285089, Accuracy 77.412%\n",
      "Epoch 14, Batch 253, LR 0.000077 Loss 7.286888, Accuracy 77.393%\n",
      "Epoch 14, Batch 254, LR 0.000077 Loss 7.286402, Accuracy 77.384%\n",
      "Epoch 14, Batch 255, LR 0.000076 Loss 7.286279, Accuracy 77.371%\n",
      "Epoch 14, Batch 256, LR 0.000076 Loss 7.283693, Accuracy 77.383%\n",
      "Epoch 14, Batch 257, LR 0.000076 Loss 7.283855, Accuracy 77.380%\n",
      "Epoch 14, Batch 258, LR 0.000076 Loss 7.282193, Accuracy 77.380%\n",
      "Epoch 14, Batch 259, LR 0.000076 Loss 7.281401, Accuracy 77.398%\n",
      "Epoch 14, Batch 260, LR 0.000076 Loss 7.281289, Accuracy 77.395%\n",
      "Epoch 14, Batch 261, LR 0.000076 Loss 7.281306, Accuracy 77.404%\n",
      "Epoch 14, Batch 262, LR 0.000076 Loss 7.280628, Accuracy 77.394%\n",
      "Epoch 14, Batch 263, LR 0.000076 Loss 7.280256, Accuracy 77.394%\n",
      "Epoch 14, Batch 264, LR 0.000076 Loss 7.282373, Accuracy 77.370%\n",
      "Epoch 14, Batch 265, LR 0.000076 Loss 7.281722, Accuracy 77.373%\n",
      "Epoch 14, Batch 266, LR 0.000076 Loss 7.280365, Accuracy 77.379%\n",
      "Epoch 14, Batch 267, LR 0.000076 Loss 7.276800, Accuracy 77.379%\n",
      "Epoch 14, Batch 268, LR 0.000076 Loss 7.275519, Accuracy 77.393%\n",
      "Epoch 14, Batch 269, LR 0.000076 Loss 7.273671, Accuracy 77.402%\n",
      "Epoch 14, Batch 270, LR 0.000076 Loss 7.273719, Accuracy 77.399%\n",
      "Epoch 14, Batch 271, LR 0.000076 Loss 7.273165, Accuracy 77.384%\n",
      "Epoch 14, Batch 272, LR 0.000076 Loss 7.269998, Accuracy 77.413%\n",
      "Epoch 14, Batch 273, LR 0.000076 Loss 7.271461, Accuracy 77.410%\n",
      "Epoch 14, Batch 274, LR 0.000076 Loss 7.272090, Accuracy 77.392%\n",
      "Epoch 14, Batch 275, LR 0.000076 Loss 7.271270, Accuracy 77.392%\n",
      "Epoch 14, Batch 276, LR 0.000076 Loss 7.268036, Accuracy 77.400%\n",
      "Epoch 14, Batch 277, LR 0.000076 Loss 7.263805, Accuracy 77.411%\n",
      "Epoch 14, Batch 278, LR 0.000076 Loss 7.265122, Accuracy 77.400%\n",
      "Epoch 14, Batch 279, LR 0.000076 Loss 7.265170, Accuracy 77.408%\n",
      "Epoch 14, Batch 280, LR 0.000076 Loss 7.264097, Accuracy 77.416%\n",
      "Epoch 14, Batch 281, LR 0.000076 Loss 7.263794, Accuracy 77.410%\n",
      "Epoch 14, Batch 282, LR 0.000076 Loss 7.262935, Accuracy 77.427%\n",
      "Epoch 14, Batch 283, LR 0.000076 Loss 7.264481, Accuracy 77.440%\n",
      "Epoch 14, Batch 284, LR 0.000076 Loss 7.262653, Accuracy 77.451%\n",
      "Epoch 14, Batch 285, LR 0.000076 Loss 7.260911, Accuracy 77.464%\n",
      "Epoch 14, Batch 286, LR 0.000076 Loss 7.260302, Accuracy 77.480%\n",
      "Epoch 14, Batch 287, LR 0.000076 Loss 7.261384, Accuracy 77.469%\n",
      "Epoch 14, Batch 288, LR 0.000076 Loss 7.259905, Accuracy 77.474%\n",
      "Epoch 14, Batch 289, LR 0.000076 Loss 7.259957, Accuracy 77.476%\n",
      "Epoch 14, Batch 290, LR 0.000076 Loss 7.261017, Accuracy 77.460%\n",
      "Epoch 14, Batch 291, LR 0.000076 Loss 7.262565, Accuracy 77.446%\n",
      "Epoch 14, Batch 292, LR 0.000076 Loss 7.263382, Accuracy 77.464%\n",
      "Epoch 14, Batch 293, LR 0.000076 Loss 7.261202, Accuracy 77.485%\n",
      "Epoch 14, Batch 294, LR 0.000076 Loss 7.259765, Accuracy 77.501%\n",
      "Epoch 14, Batch 295, LR 0.000076 Loss 7.260892, Accuracy 77.489%\n",
      "Epoch 14, Batch 296, LR 0.000076 Loss 7.260778, Accuracy 77.486%\n",
      "Epoch 14, Batch 297, LR 0.000076 Loss 7.262355, Accuracy 77.478%\n",
      "Epoch 14, Batch 298, LR 0.000076 Loss 7.261979, Accuracy 77.472%\n",
      "Epoch 14, Batch 299, LR 0.000076 Loss 7.259935, Accuracy 77.495%\n",
      "Epoch 14, Batch 300, LR 0.000076 Loss 7.258874, Accuracy 77.497%\n",
      "Epoch 14, Batch 301, LR 0.000076 Loss 7.257293, Accuracy 77.510%\n",
      "Epoch 14, Batch 302, LR 0.000076 Loss 7.257014, Accuracy 77.494%\n",
      "Epoch 14, Batch 303, LR 0.000076 Loss 7.256783, Accuracy 77.506%\n",
      "Epoch 14, Batch 304, LR 0.000076 Loss 7.255307, Accuracy 77.506%\n",
      "Epoch 14, Batch 305, LR 0.000076 Loss 7.254657, Accuracy 77.505%\n",
      "Epoch 14, Batch 306, LR 0.000076 Loss 7.256297, Accuracy 77.489%\n",
      "Epoch 14, Batch 307, LR 0.000076 Loss 7.257792, Accuracy 77.484%\n",
      "Epoch 14, Batch 308, LR 0.000076 Loss 7.257927, Accuracy 77.504%\n",
      "Epoch 14, Batch 309, LR 0.000076 Loss 7.259649, Accuracy 77.506%\n",
      "Epoch 14, Batch 310, LR 0.000076 Loss 7.260401, Accuracy 77.513%\n",
      "Epoch 14, Batch 311, LR 0.000076 Loss 7.261070, Accuracy 77.515%\n",
      "Epoch 14, Batch 312, LR 0.000076 Loss 7.259103, Accuracy 77.527%\n",
      "Epoch 14, Batch 313, LR 0.000076 Loss 7.261850, Accuracy 77.513%\n",
      "Epoch 14, Batch 314, LR 0.000076 Loss 7.262894, Accuracy 77.503%\n",
      "Epoch 14, Batch 315, LR 0.000076 Loss 7.263016, Accuracy 77.498%\n",
      "Epoch 14, Batch 316, LR 0.000076 Loss 7.266400, Accuracy 77.480%\n",
      "Epoch 14, Batch 317, LR 0.000076 Loss 7.266959, Accuracy 77.484%\n",
      "Epoch 14, Batch 318, LR 0.000076 Loss 7.266484, Accuracy 77.484%\n",
      "Epoch 14, Batch 319, LR 0.000076 Loss 7.267229, Accuracy 77.476%\n",
      "Epoch 14, Batch 320, LR 0.000076 Loss 7.267804, Accuracy 77.471%\n",
      "Epoch 14, Batch 321, LR 0.000076 Loss 7.266200, Accuracy 77.482%\n",
      "Epoch 14, Batch 322, LR 0.000076 Loss 7.264976, Accuracy 77.484%\n",
      "Epoch 14, Batch 323, LR 0.000076 Loss 7.267922, Accuracy 77.465%\n",
      "Epoch 14, Batch 324, LR 0.000076 Loss 7.269497, Accuracy 77.452%\n",
      "Epoch 14, Batch 325, LR 0.000076 Loss 7.271017, Accuracy 77.447%\n",
      "Epoch 14, Batch 326, LR 0.000076 Loss 7.274319, Accuracy 77.437%\n",
      "Epoch 14, Batch 327, LR 0.000076 Loss 7.273665, Accuracy 77.446%\n",
      "Epoch 14, Batch 328, LR 0.000076 Loss 7.274433, Accuracy 77.451%\n",
      "Epoch 14, Batch 329, LR 0.000076 Loss 7.275344, Accuracy 77.443%\n",
      "Epoch 14, Batch 330, LR 0.000076 Loss 7.277339, Accuracy 77.431%\n",
      "Epoch 14, Batch 331, LR 0.000076 Loss 7.277200, Accuracy 77.431%\n",
      "Epoch 14, Batch 332, LR 0.000076 Loss 7.274010, Accuracy 77.443%\n",
      "Epoch 14, Batch 333, LR 0.000076 Loss 7.272020, Accuracy 77.468%\n",
      "Epoch 14, Batch 334, LR 0.000076 Loss 7.269912, Accuracy 77.477%\n",
      "Epoch 14, Batch 335, LR 0.000076 Loss 7.271024, Accuracy 77.477%\n",
      "Epoch 14, Batch 336, LR 0.000076 Loss 7.271933, Accuracy 77.469%\n",
      "Epoch 14, Batch 337, LR 0.000076 Loss 7.272746, Accuracy 77.457%\n",
      "Epoch 14, Batch 338, LR 0.000076 Loss 7.272800, Accuracy 77.450%\n",
      "Epoch 14, Batch 339, LR 0.000076 Loss 7.270646, Accuracy 77.454%\n",
      "Epoch 14, Batch 340, LR 0.000076 Loss 7.268488, Accuracy 77.470%\n",
      "Epoch 14, Batch 341, LR 0.000076 Loss 7.269519, Accuracy 77.470%\n",
      "Epoch 14, Batch 342, LR 0.000076 Loss 7.270423, Accuracy 77.474%\n",
      "Epoch 14, Batch 343, LR 0.000076 Loss 7.269751, Accuracy 77.492%\n",
      "Epoch 14, Batch 344, LR 0.000076 Loss 7.268426, Accuracy 77.500%\n",
      "Epoch 14, Batch 345, LR 0.000076 Loss 7.264406, Accuracy 77.518%\n",
      "Epoch 14, Batch 346, LR 0.000076 Loss 7.263089, Accuracy 77.531%\n",
      "Epoch 14, Batch 347, LR 0.000076 Loss 7.265849, Accuracy 77.524%\n",
      "Epoch 14, Batch 348, LR 0.000076 Loss 7.265313, Accuracy 77.528%\n",
      "Epoch 14, Batch 349, LR 0.000076 Loss 7.267128, Accuracy 77.509%\n",
      "Epoch 14, Batch 350, LR 0.000076 Loss 7.267803, Accuracy 77.513%\n",
      "Epoch 14, Batch 351, LR 0.000076 Loss 7.267001, Accuracy 77.495%\n",
      "Epoch 14, Batch 352, LR 0.000076 Loss 7.265746, Accuracy 77.499%\n",
      "Epoch 14, Batch 353, LR 0.000076 Loss 7.267462, Accuracy 77.488%\n",
      "Epoch 14, Batch 354, LR 0.000076 Loss 7.266861, Accuracy 77.496%\n",
      "Epoch 14, Batch 355, LR 0.000076 Loss 7.269803, Accuracy 77.474%\n",
      "Epoch 14, Batch 356, LR 0.000076 Loss 7.269080, Accuracy 77.471%\n",
      "Epoch 14, Batch 357, LR 0.000076 Loss 7.271598, Accuracy 77.451%\n",
      "Epoch 14, Batch 358, LR 0.000076 Loss 7.273033, Accuracy 77.451%\n",
      "Epoch 14, Batch 359, LR 0.000076 Loss 7.274037, Accuracy 77.440%\n",
      "Epoch 14, Batch 360, LR 0.000076 Loss 7.273851, Accuracy 77.428%\n",
      "Epoch 14, Batch 361, LR 0.000076 Loss 7.273398, Accuracy 77.437%\n",
      "Epoch 14, Batch 362, LR 0.000076 Loss 7.272662, Accuracy 77.439%\n",
      "Epoch 14, Batch 363, LR 0.000076 Loss 7.272462, Accuracy 77.438%\n",
      "Epoch 14, Batch 364, LR 0.000076 Loss 7.275599, Accuracy 77.425%\n",
      "Epoch 14, Batch 365, LR 0.000076 Loss 7.275585, Accuracy 77.427%\n",
      "Epoch 14, Batch 366, LR 0.000076 Loss 7.274096, Accuracy 77.438%\n",
      "Epoch 14, Batch 367, LR 0.000076 Loss 7.272662, Accuracy 77.433%\n",
      "Epoch 14, Batch 368, LR 0.000076 Loss 7.272569, Accuracy 77.433%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 369, LR 0.000076 Loss 7.270951, Accuracy 77.439%\n",
      "Epoch 14, Batch 370, LR 0.000076 Loss 7.273077, Accuracy 77.426%\n",
      "Epoch 14, Batch 371, LR 0.000076 Loss 7.273614, Accuracy 77.426%\n",
      "Epoch 14, Batch 372, LR 0.000076 Loss 7.275063, Accuracy 77.417%\n",
      "Epoch 14, Batch 373, LR 0.000076 Loss 7.275366, Accuracy 77.411%\n",
      "Epoch 14, Batch 374, LR 0.000076 Loss 7.275298, Accuracy 77.415%\n",
      "Epoch 14, Batch 375, LR 0.000076 Loss 7.275588, Accuracy 77.415%\n",
      "Epoch 14, Batch 376, LR 0.000076 Loss 7.276738, Accuracy 77.404%\n",
      "Epoch 14, Batch 377, LR 0.000076 Loss 7.276452, Accuracy 77.408%\n",
      "Epoch 14, Batch 378, LR 0.000076 Loss 7.275087, Accuracy 77.414%\n",
      "Epoch 14, Batch 379, LR 0.000076 Loss 7.275421, Accuracy 77.414%\n",
      "Epoch 14, Batch 380, LR 0.000076 Loss 7.276615, Accuracy 77.399%\n",
      "Epoch 14, Batch 381, LR 0.000076 Loss 7.276935, Accuracy 77.411%\n",
      "Epoch 14, Batch 382, LR 0.000076 Loss 7.276926, Accuracy 77.409%\n",
      "Epoch 14, Batch 383, LR 0.000076 Loss 7.275358, Accuracy 77.417%\n",
      "Epoch 14, Batch 384, LR 0.000076 Loss 7.276443, Accuracy 77.425%\n",
      "Epoch 14, Batch 385, LR 0.000076 Loss 7.278210, Accuracy 77.429%\n",
      "Epoch 14, Batch 386, LR 0.000076 Loss 7.279311, Accuracy 77.411%\n",
      "Epoch 14, Batch 387, LR 0.000076 Loss 7.277168, Accuracy 77.418%\n",
      "Epoch 14, Batch 388, LR 0.000076 Loss 7.277115, Accuracy 77.414%\n",
      "Epoch 14, Batch 389, LR 0.000076 Loss 7.278203, Accuracy 77.416%\n",
      "Epoch 14, Batch 390, LR 0.000076 Loss 7.277812, Accuracy 77.418%\n",
      "Epoch 14, Batch 391, LR 0.000076 Loss 7.277889, Accuracy 77.420%\n",
      "Epoch 14, Batch 392, LR 0.000076 Loss 7.277194, Accuracy 77.427%\n",
      "Epoch 14, Batch 393, LR 0.000076 Loss 7.276170, Accuracy 77.437%\n",
      "Epoch 14, Batch 394, LR 0.000076 Loss 7.276939, Accuracy 77.433%\n",
      "Epoch 14, Batch 395, LR 0.000076 Loss 7.277157, Accuracy 77.439%\n",
      "Epoch 14, Batch 396, LR 0.000076 Loss 7.278542, Accuracy 77.438%\n",
      "Epoch 14, Batch 397, LR 0.000076 Loss 7.278747, Accuracy 77.444%\n",
      "Epoch 14, Batch 398, LR 0.000076 Loss 7.280427, Accuracy 77.432%\n",
      "Epoch 14, Batch 399, LR 0.000076 Loss 7.278784, Accuracy 77.434%\n",
      "Epoch 14, Batch 400, LR 0.000076 Loss 7.279983, Accuracy 77.424%\n",
      "Epoch 14, Batch 401, LR 0.000076 Loss 7.279850, Accuracy 77.428%\n",
      "Epoch 14, Batch 402, LR 0.000076 Loss 7.277019, Accuracy 77.437%\n",
      "Epoch 14, Batch 403, LR 0.000076 Loss 7.276348, Accuracy 77.445%\n",
      "Epoch 14, Batch 404, LR 0.000076 Loss 7.275481, Accuracy 77.450%\n",
      "Epoch 14, Batch 405, LR 0.000076 Loss 7.273898, Accuracy 77.452%\n",
      "Epoch 14, Batch 406, LR 0.000076 Loss 7.272025, Accuracy 77.459%\n",
      "Epoch 14, Batch 407, LR 0.000076 Loss 7.273437, Accuracy 77.449%\n",
      "Epoch 14, Batch 408, LR 0.000076 Loss 7.271486, Accuracy 77.459%\n",
      "Epoch 14, Batch 409, LR 0.000076 Loss 7.271676, Accuracy 77.455%\n",
      "Epoch 14, Batch 410, LR 0.000076 Loss 7.271045, Accuracy 77.452%\n",
      "Epoch 14, Batch 411, LR 0.000076 Loss 7.272312, Accuracy 77.439%\n",
      "Epoch 14, Batch 412, LR 0.000076 Loss 7.273053, Accuracy 77.437%\n",
      "Epoch 14, Batch 413, LR 0.000076 Loss 7.273336, Accuracy 77.444%\n",
      "Epoch 14, Batch 414, LR 0.000076 Loss 7.270853, Accuracy 77.451%\n",
      "Epoch 14, Batch 415, LR 0.000076 Loss 7.269297, Accuracy 77.457%\n",
      "Epoch 14, Batch 416, LR 0.000076 Loss 7.268723, Accuracy 77.462%\n",
      "Epoch 14, Batch 417, LR 0.000076 Loss 7.270900, Accuracy 77.451%\n",
      "Epoch 14, Batch 418, LR 0.000076 Loss 7.270186, Accuracy 77.461%\n",
      "Epoch 14, Batch 419, LR 0.000076 Loss 7.270415, Accuracy 77.461%\n",
      "Epoch 14, Batch 420, LR 0.000076 Loss 7.271341, Accuracy 77.455%\n",
      "Epoch 14, Batch 421, LR 0.000076 Loss 7.271317, Accuracy 77.453%\n",
      "Epoch 14, Batch 422, LR 0.000076 Loss 7.272800, Accuracy 77.446%\n",
      "Epoch 14, Batch 423, LR 0.000076 Loss 7.275178, Accuracy 77.443%\n",
      "Epoch 14, Batch 424, LR 0.000076 Loss 7.273742, Accuracy 77.452%\n",
      "Epoch 14, Batch 425, LR 0.000076 Loss 7.273541, Accuracy 77.456%\n",
      "Epoch 14, Batch 426, LR 0.000076 Loss 7.275177, Accuracy 77.454%\n",
      "Epoch 14, Batch 427, LR 0.000076 Loss 7.276105, Accuracy 77.443%\n",
      "Epoch 14, Batch 428, LR 0.000076 Loss 7.278485, Accuracy 77.437%\n",
      "Epoch 14, Batch 429, LR 0.000076 Loss 7.279860, Accuracy 77.428%\n",
      "Epoch 14, Batch 430, LR 0.000076 Loss 7.279748, Accuracy 77.426%\n",
      "Epoch 14, Batch 431, LR 0.000076 Loss 7.278949, Accuracy 77.427%\n",
      "Epoch 14, Batch 432, LR 0.000076 Loss 7.278584, Accuracy 77.432%\n",
      "Epoch 14, Batch 433, LR 0.000076 Loss 7.278219, Accuracy 77.436%\n",
      "Epoch 14, Batch 434, LR 0.000076 Loss 7.277034, Accuracy 77.446%\n",
      "Epoch 14, Batch 435, LR 0.000076 Loss 7.275628, Accuracy 77.466%\n",
      "Epoch 14, Batch 436, LR 0.000076 Loss 7.275185, Accuracy 77.475%\n",
      "Epoch 14, Batch 437, LR 0.000076 Loss 7.277914, Accuracy 77.447%\n",
      "Epoch 14, Batch 438, LR 0.000076 Loss 7.278390, Accuracy 77.440%\n",
      "Epoch 14, Batch 439, LR 0.000076 Loss 7.278025, Accuracy 77.447%\n",
      "Epoch 14, Batch 440, LR 0.000076 Loss 7.279620, Accuracy 77.436%\n",
      "Epoch 14, Batch 441, LR 0.000076 Loss 7.278648, Accuracy 77.438%\n",
      "Epoch 14, Batch 442, LR 0.000076 Loss 7.279890, Accuracy 77.436%\n",
      "Epoch 14, Batch 443, LR 0.000076 Loss 7.278250, Accuracy 77.448%\n",
      "Epoch 14, Batch 444, LR 0.000076 Loss 7.278038, Accuracy 77.451%\n",
      "Epoch 14, Batch 445, LR 0.000076 Loss 7.276357, Accuracy 77.458%\n",
      "Epoch 14, Batch 446, LR 0.000076 Loss 7.276470, Accuracy 77.456%\n",
      "Epoch 14, Batch 447, LR 0.000076 Loss 7.275332, Accuracy 77.461%\n",
      "Epoch 14, Batch 448, LR 0.000076 Loss 7.274544, Accuracy 77.462%\n",
      "Epoch 14, Batch 449, LR 0.000076 Loss 7.275291, Accuracy 77.452%\n",
      "Epoch 14, Batch 450, LR 0.000076 Loss 7.276591, Accuracy 77.450%\n",
      "Epoch 14, Batch 451, LR 0.000076 Loss 7.277252, Accuracy 77.451%\n",
      "Epoch 14, Batch 452, LR 0.000076 Loss 7.279754, Accuracy 77.437%\n",
      "Epoch 14, Batch 453, LR 0.000076 Loss 7.278857, Accuracy 77.444%\n",
      "Epoch 14, Batch 454, LR 0.000076 Loss 7.280601, Accuracy 77.435%\n",
      "Epoch 14, Batch 455, LR 0.000076 Loss 7.280793, Accuracy 77.430%\n",
      "Epoch 14, Batch 456, LR 0.000076 Loss 7.280511, Accuracy 77.421%\n",
      "Epoch 14, Batch 457, LR 0.000076 Loss 7.280087, Accuracy 77.417%\n",
      "Epoch 14, Batch 458, LR 0.000076 Loss 7.280901, Accuracy 77.414%\n",
      "Epoch 14, Batch 459, LR 0.000076 Loss 7.279013, Accuracy 77.420%\n",
      "Epoch 14, Batch 460, LR 0.000076 Loss 7.278142, Accuracy 77.413%\n",
      "Epoch 14, Batch 461, LR 0.000076 Loss 7.277877, Accuracy 77.423%\n",
      "Epoch 14, Batch 462, LR 0.000076 Loss 7.277938, Accuracy 77.416%\n",
      "Epoch 14, Batch 463, LR 0.000076 Loss 7.277552, Accuracy 77.423%\n",
      "Epoch 14, Batch 464, LR 0.000076 Loss 7.277205, Accuracy 77.433%\n",
      "Epoch 14, Batch 465, LR 0.000076 Loss 7.277620, Accuracy 77.424%\n",
      "Epoch 14, Batch 466, LR 0.000076 Loss 7.276129, Accuracy 77.429%\n",
      "Epoch 14, Batch 467, LR 0.000076 Loss 7.275184, Accuracy 77.437%\n",
      "Epoch 14, Batch 468, LR 0.000076 Loss 7.277306, Accuracy 77.431%\n",
      "Epoch 14, Batch 469, LR 0.000076 Loss 7.278111, Accuracy 77.425%\n",
      "Epoch 14, Batch 470, LR 0.000076 Loss 7.278116, Accuracy 77.429%\n",
      "Epoch 14, Batch 471, LR 0.000076 Loss 7.278620, Accuracy 77.425%\n",
      "Epoch 14, Batch 472, LR 0.000076 Loss 7.277976, Accuracy 77.436%\n",
      "Epoch 14, Batch 473, LR 0.000076 Loss 7.278512, Accuracy 77.433%\n",
      "Epoch 14, Batch 474, LR 0.000076 Loss 7.278461, Accuracy 77.436%\n",
      "Epoch 14, Batch 475, LR 0.000076 Loss 7.278906, Accuracy 77.438%\n",
      "Epoch 14, Batch 476, LR 0.000076 Loss 7.279432, Accuracy 77.436%\n",
      "Epoch 14, Batch 477, LR 0.000076 Loss 7.279995, Accuracy 77.434%\n",
      "Epoch 14, Batch 478, LR 0.000076 Loss 7.281034, Accuracy 77.435%\n",
      "Epoch 14, Batch 479, LR 0.000076 Loss 7.280658, Accuracy 77.438%\n",
      "Epoch 14, Batch 480, LR 0.000076 Loss 7.280164, Accuracy 77.435%\n",
      "Epoch 14, Batch 481, LR 0.000076 Loss 7.279224, Accuracy 77.444%\n",
      "Epoch 14, Batch 482, LR 0.000076 Loss 7.279018, Accuracy 77.443%\n",
      "Epoch 14, Batch 483, LR 0.000076 Loss 7.280077, Accuracy 77.441%\n",
      "Epoch 14, Batch 484, LR 0.000076 Loss 7.278546, Accuracy 77.455%\n",
      "Epoch 14, Batch 485, LR 0.000076 Loss 7.277642, Accuracy 77.458%\n",
      "Epoch 14, Batch 486, LR 0.000076 Loss 7.276181, Accuracy 77.458%\n",
      "Epoch 14, Batch 487, LR 0.000076 Loss 7.276974, Accuracy 77.456%\n",
      "Epoch 14, Batch 488, LR 0.000076 Loss 7.276564, Accuracy 77.456%\n",
      "Epoch 14, Batch 489, LR 0.000076 Loss 7.276892, Accuracy 77.460%\n",
      "Epoch 14, Batch 490, LR 0.000076 Loss 7.276412, Accuracy 77.474%\n",
      "Epoch 14, Batch 491, LR 0.000076 Loss 7.277468, Accuracy 77.473%\n",
      "Epoch 14, Batch 492, LR 0.000076 Loss 7.278171, Accuracy 77.468%\n",
      "Epoch 14, Batch 493, LR 0.000076 Loss 7.279808, Accuracy 77.455%\n",
      "Epoch 14, Batch 494, LR 0.000076 Loss 7.279120, Accuracy 77.454%\n",
      "Epoch 14, Batch 495, LR 0.000076 Loss 7.279868, Accuracy 77.445%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 496, LR 0.000076 Loss 7.279415, Accuracy 77.448%\n",
      "Epoch 14, Batch 497, LR 0.000076 Loss 7.278576, Accuracy 77.451%\n",
      "Epoch 14, Batch 498, LR 0.000076 Loss 7.278478, Accuracy 77.455%\n",
      "Epoch 14, Batch 499, LR 0.000076 Loss 7.277619, Accuracy 77.458%\n",
      "Epoch 14, Batch 500, LR 0.000076 Loss 7.278549, Accuracy 77.442%\n",
      "Epoch 14, Batch 501, LR 0.000076 Loss 7.276934, Accuracy 77.456%\n",
      "Epoch 14, Batch 502, LR 0.000076 Loss 7.277579, Accuracy 77.446%\n",
      "Epoch 14, Batch 503, LR 0.000076 Loss 7.278091, Accuracy 77.442%\n",
      "Epoch 14, Batch 504, LR 0.000076 Loss 7.276274, Accuracy 77.451%\n",
      "Epoch 14, Batch 505, LR 0.000076 Loss 7.275907, Accuracy 77.450%\n",
      "Epoch 14, Batch 506, LR 0.000076 Loss 7.275278, Accuracy 77.456%\n",
      "Epoch 14, Batch 507, LR 0.000076 Loss 7.275052, Accuracy 77.470%\n",
      "Epoch 14, Batch 508, LR 0.000076 Loss 7.273533, Accuracy 77.487%\n",
      "Epoch 14, Batch 509, LR 0.000076 Loss 7.273206, Accuracy 77.482%\n",
      "Epoch 14, Batch 510, LR 0.000076 Loss 7.272022, Accuracy 77.486%\n",
      "Epoch 14, Batch 511, LR 0.000076 Loss 7.272805, Accuracy 77.492%\n",
      "Epoch 14, Batch 512, LR 0.000076 Loss 7.271195, Accuracy 77.501%\n",
      "Epoch 14, Batch 513, LR 0.000076 Loss 7.272394, Accuracy 77.491%\n",
      "Epoch 14, Batch 514, LR 0.000076 Loss 7.271073, Accuracy 77.500%\n",
      "Epoch 14, Batch 515, LR 0.000076 Loss 7.271536, Accuracy 77.497%\n",
      "Epoch 14, Batch 516, LR 0.000076 Loss 7.272599, Accuracy 77.491%\n",
      "Epoch 14, Batch 517, LR 0.000076 Loss 7.273416, Accuracy 77.481%\n",
      "Epoch 14, Batch 518, LR 0.000076 Loss 7.273252, Accuracy 77.479%\n",
      "Epoch 14, Batch 519, LR 0.000076 Loss 7.273290, Accuracy 77.472%\n",
      "Epoch 14, Batch 520, LR 0.000076 Loss 7.273390, Accuracy 77.473%\n",
      "Epoch 14, Batch 521, LR 0.000076 Loss 7.273482, Accuracy 77.477%\n",
      "Epoch 14, Batch 522, LR 0.000076 Loss 7.273080, Accuracy 77.481%\n",
      "Epoch 14, Batch 523, LR 0.000076 Loss 7.272505, Accuracy 77.483%\n",
      "Epoch 14, Batch 524, LR 0.000076 Loss 7.272524, Accuracy 77.479%\n",
      "Epoch 14, Batch 525, LR 0.000076 Loss 7.269694, Accuracy 77.497%\n",
      "Epoch 14, Batch 526, LR 0.000076 Loss 7.267653, Accuracy 77.509%\n",
      "Epoch 14, Batch 527, LR 0.000076 Loss 7.267095, Accuracy 77.511%\n",
      "Epoch 14, Batch 528, LR 0.000076 Loss 7.267716, Accuracy 77.514%\n",
      "Epoch 14, Batch 529, LR 0.000076 Loss 7.268370, Accuracy 77.502%\n",
      "Epoch 14, Batch 530, LR 0.000076 Loss 7.267663, Accuracy 77.510%\n",
      "Epoch 14, Batch 531, LR 0.000076 Loss 7.270149, Accuracy 77.491%\n",
      "Epoch 14, Batch 532, LR 0.000076 Loss 7.271218, Accuracy 77.495%\n",
      "Epoch 14, Batch 533, LR 0.000076 Loss 7.270973, Accuracy 77.495%\n",
      "Epoch 14, Batch 534, LR 0.000076 Loss 7.270713, Accuracy 77.499%\n",
      "Epoch 14, Batch 535, LR 0.000076 Loss 7.270813, Accuracy 77.493%\n",
      "Epoch 14, Batch 536, LR 0.000076 Loss 7.269505, Accuracy 77.503%\n",
      "Epoch 14, Batch 537, LR 0.000076 Loss 7.268811, Accuracy 77.510%\n",
      "Epoch 14, Batch 538, LR 0.000076 Loss 7.269969, Accuracy 77.498%\n",
      "Epoch 14, Batch 539, LR 0.000076 Loss 7.269681, Accuracy 77.497%\n",
      "Epoch 14, Batch 540, LR 0.000076 Loss 7.269755, Accuracy 77.494%\n",
      "Epoch 14, Batch 541, LR 0.000076 Loss 7.270552, Accuracy 77.487%\n",
      "Epoch 14, Batch 542, LR 0.000076 Loss 7.270498, Accuracy 77.485%\n",
      "Epoch 14, Batch 543, LR 0.000076 Loss 7.269908, Accuracy 77.491%\n",
      "Epoch 14, Batch 544, LR 0.000076 Loss 7.270823, Accuracy 77.480%\n",
      "Epoch 14, Batch 545, LR 0.000076 Loss 7.270553, Accuracy 77.489%\n",
      "Epoch 14, Batch 546, LR 0.000076 Loss 7.269019, Accuracy 77.494%\n",
      "Epoch 14, Batch 547, LR 0.000076 Loss 7.269189, Accuracy 77.488%\n",
      "Epoch 14, Batch 548, LR 0.000076 Loss 7.269952, Accuracy 77.486%\n",
      "Epoch 14, Batch 549, LR 0.000076 Loss 7.272441, Accuracy 77.468%\n",
      "Epoch 14, Batch 550, LR 0.000076 Loss 7.273854, Accuracy 77.457%\n",
      "Epoch 14, Batch 551, LR 0.000076 Loss 7.272260, Accuracy 77.460%\n",
      "Epoch 14, Batch 552, LR 0.000075 Loss 7.271586, Accuracy 77.467%\n",
      "Epoch 14, Batch 553, LR 0.000075 Loss 7.271757, Accuracy 77.462%\n",
      "Epoch 14, Batch 554, LR 0.000075 Loss 7.270733, Accuracy 77.465%\n",
      "Epoch 14, Batch 555, LR 0.000075 Loss 7.272282, Accuracy 77.449%\n",
      "Epoch 14, Batch 556, LR 0.000075 Loss 7.270866, Accuracy 77.460%\n",
      "Epoch 14, Batch 557, LR 0.000075 Loss 7.270501, Accuracy 77.463%\n",
      "Epoch 14, Batch 558, LR 0.000075 Loss 7.270633, Accuracy 77.457%\n",
      "Epoch 14, Batch 559, LR 0.000075 Loss 7.271248, Accuracy 77.453%\n",
      "Epoch 14, Batch 560, LR 0.000075 Loss 7.271053, Accuracy 77.447%\n",
      "Epoch 14, Batch 561, LR 0.000075 Loss 7.270292, Accuracy 77.450%\n",
      "Epoch 14, Batch 562, LR 0.000075 Loss 7.270109, Accuracy 77.445%\n",
      "Epoch 14, Batch 563, LR 0.000075 Loss 7.269541, Accuracy 77.438%\n",
      "Epoch 14, Batch 564, LR 0.000075 Loss 7.270229, Accuracy 77.450%\n",
      "Epoch 14, Batch 565, LR 0.000075 Loss 7.271862, Accuracy 77.438%\n",
      "Epoch 14, Batch 566, LR 0.000075 Loss 7.271372, Accuracy 77.436%\n",
      "Epoch 14, Batch 567, LR 0.000075 Loss 7.270895, Accuracy 77.444%\n",
      "Epoch 14, Batch 568, LR 0.000075 Loss 7.271039, Accuracy 77.439%\n",
      "Epoch 14, Batch 569, LR 0.000075 Loss 7.271407, Accuracy 77.429%\n",
      "Epoch 14, Batch 570, LR 0.000075 Loss 7.271036, Accuracy 77.431%\n",
      "Epoch 14, Batch 571, LR 0.000075 Loss 7.269721, Accuracy 77.442%\n",
      "Epoch 14, Batch 572, LR 0.000075 Loss 7.268646, Accuracy 77.448%\n",
      "Epoch 14, Batch 573, LR 0.000075 Loss 7.268062, Accuracy 77.454%\n",
      "Epoch 14, Batch 574, LR 0.000075 Loss 7.268573, Accuracy 77.449%\n",
      "Epoch 14, Batch 575, LR 0.000075 Loss 7.268695, Accuracy 77.448%\n",
      "Epoch 14, Batch 576, LR 0.000075 Loss 7.268020, Accuracy 77.443%\n",
      "Epoch 14, Batch 577, LR 0.000075 Loss 7.267125, Accuracy 77.441%\n",
      "Epoch 14, Batch 578, LR 0.000075 Loss 7.268002, Accuracy 77.434%\n",
      "Epoch 14, Batch 579, LR 0.000075 Loss 7.268154, Accuracy 77.437%\n",
      "Epoch 14, Batch 580, LR 0.000075 Loss 7.266929, Accuracy 77.441%\n",
      "Epoch 14, Batch 581, LR 0.000075 Loss 7.266536, Accuracy 77.437%\n",
      "Epoch 14, Batch 582, LR 0.000075 Loss 7.266505, Accuracy 77.435%\n",
      "Epoch 14, Batch 583, LR 0.000075 Loss 7.266460, Accuracy 77.439%\n",
      "Epoch 14, Batch 584, LR 0.000075 Loss 7.266553, Accuracy 77.437%\n",
      "Epoch 14, Batch 585, LR 0.000075 Loss 7.268391, Accuracy 77.429%\n",
      "Epoch 14, Batch 586, LR 0.000075 Loss 7.267164, Accuracy 77.437%\n",
      "Epoch 14, Batch 587, LR 0.000075 Loss 7.266775, Accuracy 77.434%\n",
      "Epoch 14, Batch 588, LR 0.000075 Loss 7.266136, Accuracy 77.439%\n",
      "Epoch 14, Batch 589, LR 0.000075 Loss 7.267688, Accuracy 77.433%\n",
      "Epoch 14, Batch 590, LR 0.000075 Loss 7.269434, Accuracy 77.427%\n",
      "Epoch 14, Batch 591, LR 0.000075 Loss 7.270136, Accuracy 77.416%\n",
      "Epoch 14, Batch 592, LR 0.000075 Loss 7.271906, Accuracy 77.406%\n",
      "Epoch 14, Batch 593, LR 0.000075 Loss 7.272291, Accuracy 77.403%\n",
      "Epoch 14, Batch 594, LR 0.000075 Loss 7.273809, Accuracy 77.400%\n",
      "Epoch 14, Batch 595, LR 0.000075 Loss 7.274288, Accuracy 77.398%\n",
      "Epoch 14, Batch 596, LR 0.000075 Loss 7.273190, Accuracy 77.404%\n",
      "Epoch 14, Batch 597, LR 0.000075 Loss 7.272857, Accuracy 77.403%\n",
      "Epoch 14, Batch 598, LR 0.000075 Loss 7.271468, Accuracy 77.408%\n",
      "Epoch 14, Batch 599, LR 0.000075 Loss 7.272328, Accuracy 77.400%\n",
      "Epoch 14, Batch 600, LR 0.000075 Loss 7.272979, Accuracy 77.402%\n",
      "Epoch 14, Batch 601, LR 0.000075 Loss 7.272762, Accuracy 77.404%\n",
      "Epoch 14, Batch 602, LR 0.000075 Loss 7.272733, Accuracy 77.406%\n",
      "Epoch 14, Batch 603, LR 0.000075 Loss 7.272403, Accuracy 77.407%\n",
      "Epoch 14, Batch 604, LR 0.000075 Loss 7.270779, Accuracy 77.415%\n",
      "Epoch 14, Batch 605, LR 0.000075 Loss 7.269978, Accuracy 77.423%\n",
      "Epoch 14, Batch 606, LR 0.000075 Loss 7.270117, Accuracy 77.421%\n",
      "Epoch 14, Batch 607, LR 0.000075 Loss 7.270206, Accuracy 77.417%\n",
      "Epoch 14, Batch 608, LR 0.000075 Loss 7.269450, Accuracy 77.422%\n",
      "Epoch 14, Batch 609, LR 0.000075 Loss 7.268808, Accuracy 77.425%\n",
      "Epoch 14, Batch 610, LR 0.000075 Loss 7.269839, Accuracy 77.417%\n",
      "Epoch 14, Batch 611, LR 0.000075 Loss 7.269679, Accuracy 77.418%\n",
      "Epoch 14, Batch 612, LR 0.000075 Loss 7.269411, Accuracy 77.418%\n",
      "Epoch 14, Batch 613, LR 0.000075 Loss 7.269746, Accuracy 77.419%\n",
      "Epoch 14, Batch 614, LR 0.000075 Loss 7.269853, Accuracy 77.425%\n",
      "Epoch 14, Batch 615, LR 0.000075 Loss 7.268252, Accuracy 77.433%\n",
      "Epoch 14, Batch 616, LR 0.000075 Loss 7.267941, Accuracy 77.440%\n",
      "Epoch 14, Batch 617, LR 0.000075 Loss 7.267831, Accuracy 77.440%\n",
      "Epoch 14, Batch 618, LR 0.000075 Loss 7.267543, Accuracy 77.441%\n",
      "Epoch 14, Batch 619, LR 0.000075 Loss 7.267871, Accuracy 77.446%\n",
      "Epoch 14, Batch 620, LR 0.000075 Loss 7.267741, Accuracy 77.450%\n",
      "Epoch 14, Batch 621, LR 0.000075 Loss 7.268535, Accuracy 77.446%\n",
      "Epoch 14, Batch 622, LR 0.000075 Loss 7.268599, Accuracy 77.452%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 623, LR 0.000075 Loss 7.269417, Accuracy 77.448%\n",
      "Epoch 14, Batch 624, LR 0.000075 Loss 7.269125, Accuracy 77.446%\n",
      "Epoch 14, Batch 625, LR 0.000075 Loss 7.270025, Accuracy 77.440%\n",
      "Epoch 14, Batch 626, LR 0.000075 Loss 7.270344, Accuracy 77.441%\n",
      "Epoch 14, Batch 627, LR 0.000075 Loss 7.271406, Accuracy 77.437%\n",
      "Epoch 14, Batch 628, LR 0.000075 Loss 7.270970, Accuracy 77.437%\n",
      "Epoch 14, Batch 629, LR 0.000075 Loss 7.270737, Accuracy 77.434%\n",
      "Epoch 14, Batch 630, LR 0.000075 Loss 7.270776, Accuracy 77.428%\n",
      "Epoch 14, Batch 631, LR 0.000075 Loss 7.271011, Accuracy 77.430%\n",
      "Epoch 14, Batch 632, LR 0.000075 Loss 7.270469, Accuracy 77.439%\n",
      "Epoch 14, Batch 633, LR 0.000075 Loss 7.270959, Accuracy 77.431%\n",
      "Epoch 14, Batch 634, LR 0.000075 Loss 7.271265, Accuracy 77.435%\n",
      "Epoch 14, Batch 635, LR 0.000075 Loss 7.271407, Accuracy 77.438%\n",
      "Epoch 14, Batch 636, LR 0.000075 Loss 7.270410, Accuracy 77.447%\n",
      "Epoch 14, Batch 637, LR 0.000075 Loss 7.270438, Accuracy 77.450%\n",
      "Epoch 14, Batch 638, LR 0.000075 Loss 7.270414, Accuracy 77.459%\n",
      "Epoch 14, Batch 639, LR 0.000075 Loss 7.270779, Accuracy 77.455%\n",
      "Epoch 14, Batch 640, LR 0.000075 Loss 7.271156, Accuracy 77.450%\n",
      "Epoch 14, Batch 641, LR 0.000075 Loss 7.271735, Accuracy 77.447%\n",
      "Epoch 14, Batch 642, LR 0.000075 Loss 7.271243, Accuracy 77.451%\n",
      "Epoch 14, Batch 643, LR 0.000075 Loss 7.272103, Accuracy 77.448%\n",
      "Epoch 14, Batch 644, LR 0.000075 Loss 7.273511, Accuracy 77.431%\n",
      "Epoch 14, Batch 645, LR 0.000075 Loss 7.272732, Accuracy 77.436%\n",
      "Epoch 14, Batch 646, LR 0.000075 Loss 7.271668, Accuracy 77.444%\n",
      "Epoch 14, Batch 647, LR 0.000075 Loss 7.271862, Accuracy 77.443%\n",
      "Epoch 14, Batch 648, LR 0.000075 Loss 7.271460, Accuracy 77.443%\n",
      "Epoch 14, Batch 649, LR 0.000075 Loss 7.270982, Accuracy 77.444%\n",
      "Epoch 14, Batch 650, LR 0.000075 Loss 7.271901, Accuracy 77.448%\n",
      "Epoch 14, Batch 651, LR 0.000075 Loss 7.271601, Accuracy 77.451%\n",
      "Epoch 14, Batch 652, LR 0.000075 Loss 7.272550, Accuracy 77.444%\n",
      "Epoch 14, Batch 653, LR 0.000075 Loss 7.271111, Accuracy 77.454%\n",
      "Epoch 14, Batch 654, LR 0.000075 Loss 7.271419, Accuracy 77.451%\n",
      "Epoch 14, Batch 655, LR 0.000075 Loss 7.270780, Accuracy 77.452%\n",
      "Epoch 14, Batch 656, LR 0.000075 Loss 7.270283, Accuracy 77.445%\n",
      "Epoch 14, Batch 657, LR 0.000075 Loss 7.270553, Accuracy 77.450%\n",
      "Epoch 14, Batch 658, LR 0.000075 Loss 7.269982, Accuracy 77.451%\n",
      "Epoch 14, Batch 659, LR 0.000075 Loss 7.269966, Accuracy 77.445%\n",
      "Epoch 14, Batch 660, LR 0.000075 Loss 7.271233, Accuracy 77.437%\n",
      "Epoch 14, Batch 661, LR 0.000075 Loss 7.270195, Accuracy 77.441%\n",
      "Epoch 14, Batch 662, LR 0.000075 Loss 7.269391, Accuracy 77.438%\n",
      "Epoch 14, Batch 663, LR 0.000075 Loss 7.270429, Accuracy 77.431%\n",
      "Epoch 14, Batch 664, LR 0.000075 Loss 7.269542, Accuracy 77.434%\n",
      "Epoch 14, Batch 665, LR 0.000075 Loss 7.269120, Accuracy 77.437%\n",
      "Epoch 14, Batch 666, LR 0.000075 Loss 7.269870, Accuracy 77.429%\n",
      "Epoch 14, Batch 667, LR 0.000075 Loss 7.267996, Accuracy 77.441%\n",
      "Epoch 14, Batch 668, LR 0.000075 Loss 7.267764, Accuracy 77.438%\n",
      "Epoch 14, Batch 669, LR 0.000075 Loss 7.269661, Accuracy 77.430%\n",
      "Epoch 14, Batch 670, LR 0.000075 Loss 7.270630, Accuracy 77.424%\n",
      "Epoch 14, Batch 671, LR 0.000075 Loss 7.270841, Accuracy 77.425%\n",
      "Epoch 14, Batch 672, LR 0.000075 Loss 7.270556, Accuracy 77.420%\n",
      "Epoch 14, Batch 673, LR 0.000075 Loss 7.270352, Accuracy 77.419%\n",
      "Epoch 14, Batch 674, LR 0.000075 Loss 7.270783, Accuracy 77.421%\n",
      "Epoch 14, Batch 675, LR 0.000075 Loss 7.271181, Accuracy 77.416%\n",
      "Epoch 14, Batch 676, LR 0.000075 Loss 7.271430, Accuracy 77.411%\n",
      "Epoch 14, Batch 677, LR 0.000075 Loss 7.270116, Accuracy 77.416%\n",
      "Epoch 14, Batch 678, LR 0.000075 Loss 7.268118, Accuracy 77.431%\n",
      "Epoch 14, Batch 679, LR 0.000075 Loss 7.268611, Accuracy 77.432%\n",
      "Epoch 14, Batch 680, LR 0.000075 Loss 7.269605, Accuracy 77.430%\n",
      "Epoch 14, Batch 681, LR 0.000075 Loss 7.269166, Accuracy 77.434%\n",
      "Epoch 14, Batch 682, LR 0.000075 Loss 7.268516, Accuracy 77.438%\n",
      "Epoch 14, Batch 683, LR 0.000075 Loss 7.269320, Accuracy 77.430%\n",
      "Epoch 14, Batch 684, LR 0.000075 Loss 7.268884, Accuracy 77.434%\n",
      "Epoch 14, Batch 685, LR 0.000075 Loss 7.267994, Accuracy 77.434%\n",
      "Epoch 14, Batch 686, LR 0.000075 Loss 7.267162, Accuracy 77.441%\n",
      "Epoch 14, Batch 687, LR 0.000075 Loss 7.265559, Accuracy 77.452%\n",
      "Epoch 14, Batch 688, LR 0.000075 Loss 7.263931, Accuracy 77.461%\n",
      "Epoch 14, Batch 689, LR 0.000075 Loss 7.264966, Accuracy 77.458%\n",
      "Epoch 14, Batch 690, LR 0.000075 Loss 7.264999, Accuracy 77.454%\n",
      "Epoch 14, Batch 691, LR 0.000075 Loss 7.264075, Accuracy 77.458%\n",
      "Epoch 14, Batch 692, LR 0.000075 Loss 7.263530, Accuracy 77.461%\n",
      "Epoch 14, Batch 693, LR 0.000075 Loss 7.263857, Accuracy 77.459%\n",
      "Epoch 14, Batch 694, LR 0.000075 Loss 7.264668, Accuracy 77.457%\n",
      "Epoch 14, Batch 695, LR 0.000075 Loss 7.263877, Accuracy 77.465%\n",
      "Epoch 14, Batch 696, LR 0.000075 Loss 7.263510, Accuracy 77.465%\n",
      "Epoch 14, Batch 697, LR 0.000075 Loss 7.263825, Accuracy 77.460%\n",
      "Epoch 14, Batch 698, LR 0.000075 Loss 7.263943, Accuracy 77.459%\n",
      "Epoch 14, Batch 699, LR 0.000075 Loss 7.264276, Accuracy 77.454%\n",
      "Epoch 14, Batch 700, LR 0.000075 Loss 7.264974, Accuracy 77.446%\n",
      "Epoch 14, Batch 701, LR 0.000075 Loss 7.264964, Accuracy 77.444%\n",
      "Epoch 14, Batch 702, LR 0.000075 Loss 7.263621, Accuracy 77.454%\n",
      "Epoch 14, Batch 703, LR 0.000075 Loss 7.263689, Accuracy 77.455%\n",
      "Epoch 14, Batch 704, LR 0.000075 Loss 7.263756, Accuracy 77.458%\n",
      "Epoch 14, Batch 705, LR 0.000075 Loss 7.263359, Accuracy 77.462%\n",
      "Epoch 14, Batch 706, LR 0.000075 Loss 7.262793, Accuracy 77.465%\n",
      "Epoch 14, Batch 707, LR 0.000075 Loss 7.262457, Accuracy 77.473%\n",
      "Epoch 14, Batch 708, LR 0.000075 Loss 7.263880, Accuracy 77.470%\n",
      "Epoch 14, Batch 709, LR 0.000075 Loss 7.263062, Accuracy 77.470%\n",
      "Epoch 14, Batch 710, LR 0.000075 Loss 7.263124, Accuracy 77.472%\n",
      "Epoch 14, Batch 711, LR 0.000075 Loss 7.262714, Accuracy 77.482%\n",
      "Epoch 14, Batch 712, LR 0.000075 Loss 7.263656, Accuracy 77.478%\n",
      "Epoch 14, Batch 713, LR 0.000075 Loss 7.263363, Accuracy 77.477%\n",
      "Epoch 14, Batch 714, LR 0.000075 Loss 7.263434, Accuracy 77.479%\n",
      "Epoch 14, Batch 715, LR 0.000075 Loss 7.263373, Accuracy 77.484%\n",
      "Epoch 14, Batch 716, LR 0.000075 Loss 7.263466, Accuracy 77.482%\n",
      "Epoch 14, Batch 717, LR 0.000075 Loss 7.264020, Accuracy 77.479%\n",
      "Epoch 14, Batch 718, LR 0.000075 Loss 7.265235, Accuracy 77.473%\n",
      "Epoch 14, Batch 719, LR 0.000075 Loss 7.265422, Accuracy 77.476%\n",
      "Epoch 14, Batch 720, LR 0.000075 Loss 7.266735, Accuracy 77.469%\n",
      "Epoch 14, Batch 721, LR 0.000075 Loss 7.265466, Accuracy 77.474%\n",
      "Epoch 14, Batch 722, LR 0.000075 Loss 7.266710, Accuracy 77.467%\n",
      "Epoch 14, Batch 723, LR 0.000075 Loss 7.265979, Accuracy 77.470%\n",
      "Epoch 14, Batch 724, LR 0.000075 Loss 7.266369, Accuracy 77.465%\n",
      "Epoch 14, Batch 725, LR 0.000075 Loss 7.266217, Accuracy 77.462%\n",
      "Epoch 14, Batch 726, LR 0.000075 Loss 7.267340, Accuracy 77.445%\n",
      "Epoch 14, Batch 727, LR 0.000075 Loss 7.266906, Accuracy 77.451%\n",
      "Epoch 14, Batch 728, LR 0.000075 Loss 7.266750, Accuracy 77.456%\n",
      "Epoch 14, Batch 729, LR 0.000075 Loss 7.265615, Accuracy 77.464%\n",
      "Epoch 14, Batch 730, LR 0.000075 Loss 7.265122, Accuracy 77.469%\n",
      "Epoch 14, Batch 731, LR 0.000075 Loss 7.264711, Accuracy 77.470%\n",
      "Epoch 14, Batch 732, LR 0.000075 Loss 7.264899, Accuracy 77.471%\n",
      "Epoch 14, Batch 733, LR 0.000075 Loss 7.264372, Accuracy 77.474%\n",
      "Epoch 14, Batch 734, LR 0.000075 Loss 7.264699, Accuracy 77.470%\n",
      "Epoch 14, Batch 735, LR 0.000075 Loss 7.264376, Accuracy 77.469%\n",
      "Epoch 14, Batch 736, LR 0.000075 Loss 7.264313, Accuracy 77.466%\n",
      "Epoch 14, Batch 737, LR 0.000075 Loss 7.263991, Accuracy 77.465%\n",
      "Epoch 14, Batch 738, LR 0.000075 Loss 7.264293, Accuracy 77.467%\n",
      "Epoch 14, Batch 739, LR 0.000075 Loss 7.263860, Accuracy 77.473%\n",
      "Epoch 14, Batch 740, LR 0.000075 Loss 7.263724, Accuracy 77.479%\n",
      "Epoch 14, Batch 741, LR 0.000075 Loss 7.263432, Accuracy 77.485%\n",
      "Epoch 14, Batch 742, LR 0.000075 Loss 7.263017, Accuracy 77.486%\n",
      "Epoch 14, Batch 743, LR 0.000075 Loss 7.263980, Accuracy 77.483%\n",
      "Epoch 14, Batch 744, LR 0.000075 Loss 7.262898, Accuracy 77.488%\n",
      "Epoch 14, Batch 745, LR 0.000075 Loss 7.263330, Accuracy 77.491%\n",
      "Epoch 14, Batch 746, LR 0.000075 Loss 7.263592, Accuracy 77.490%\n",
      "Epoch 14, Batch 747, LR 0.000075 Loss 7.264658, Accuracy 77.486%\n",
      "Epoch 14, Batch 748, LR 0.000075 Loss 7.265272, Accuracy 77.485%\n",
      "Epoch 14, Batch 749, LR 0.000075 Loss 7.266034, Accuracy 77.476%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 750, LR 0.000075 Loss 7.266420, Accuracy 77.477%\n",
      "Epoch 14, Batch 751, LR 0.000075 Loss 7.266043, Accuracy 77.479%\n",
      "Epoch 14, Batch 752, LR 0.000075 Loss 7.266275, Accuracy 77.486%\n",
      "Epoch 14, Batch 753, LR 0.000075 Loss 7.266171, Accuracy 77.482%\n",
      "Epoch 14, Batch 754, LR 0.000075 Loss 7.265524, Accuracy 77.484%\n",
      "Epoch 14, Batch 755, LR 0.000075 Loss 7.266759, Accuracy 77.473%\n",
      "Epoch 14, Batch 756, LR 0.000075 Loss 7.266838, Accuracy 77.475%\n",
      "Epoch 14, Batch 757, LR 0.000075 Loss 7.267701, Accuracy 77.472%\n",
      "Epoch 14, Batch 758, LR 0.000075 Loss 7.267278, Accuracy 77.468%\n",
      "Epoch 14, Batch 759, LR 0.000075 Loss 7.266287, Accuracy 77.472%\n",
      "Epoch 14, Batch 760, LR 0.000075 Loss 7.267816, Accuracy 77.473%\n",
      "Epoch 14, Batch 761, LR 0.000075 Loss 7.268372, Accuracy 77.471%\n",
      "Epoch 14, Batch 762, LR 0.000075 Loss 7.269512, Accuracy 77.468%\n",
      "Epoch 14, Batch 763, LR 0.000075 Loss 7.269986, Accuracy 77.471%\n",
      "Epoch 14, Batch 764, LR 0.000075 Loss 7.269735, Accuracy 77.469%\n",
      "Epoch 14, Batch 765, LR 0.000075 Loss 7.268620, Accuracy 77.475%\n",
      "Epoch 14, Batch 766, LR 0.000075 Loss 7.268462, Accuracy 77.473%\n",
      "Epoch 14, Batch 767, LR 0.000075 Loss 7.269051, Accuracy 77.471%\n",
      "Epoch 14, Batch 768, LR 0.000075 Loss 7.269289, Accuracy 77.473%\n",
      "Epoch 14, Batch 769, LR 0.000075 Loss 7.269173, Accuracy 77.474%\n",
      "Epoch 14, Batch 770, LR 0.000075 Loss 7.268688, Accuracy 77.476%\n",
      "Epoch 14, Batch 771, LR 0.000075 Loss 7.269453, Accuracy 77.471%\n",
      "Epoch 14, Batch 772, LR 0.000075 Loss 7.268723, Accuracy 77.479%\n",
      "Epoch 14, Batch 773, LR 0.000075 Loss 7.268810, Accuracy 77.478%\n",
      "Epoch 14, Batch 774, LR 0.000075 Loss 7.268242, Accuracy 77.477%\n",
      "Epoch 14, Batch 775, LR 0.000075 Loss 7.267509, Accuracy 77.479%\n",
      "Epoch 14, Batch 776, LR 0.000075 Loss 7.266769, Accuracy 77.484%\n",
      "Epoch 14, Batch 777, LR 0.000075 Loss 7.267236, Accuracy 77.478%\n",
      "Epoch 14, Batch 778, LR 0.000075 Loss 7.267139, Accuracy 77.476%\n",
      "Epoch 14, Batch 779, LR 0.000075 Loss 7.267435, Accuracy 77.476%\n",
      "Epoch 14, Batch 780, LR 0.000075 Loss 7.268126, Accuracy 77.470%\n",
      "Epoch 14, Batch 781, LR 0.000075 Loss 7.268122, Accuracy 77.462%\n",
      "Epoch 14, Batch 782, LR 0.000075 Loss 7.267906, Accuracy 77.462%\n",
      "Epoch 14, Batch 783, LR 0.000075 Loss 7.267463, Accuracy 77.462%\n",
      "Epoch 14, Batch 784, LR 0.000075 Loss 7.268271, Accuracy 77.464%\n",
      "Epoch 14, Batch 785, LR 0.000075 Loss 7.268692, Accuracy 77.465%\n",
      "Epoch 14, Batch 786, LR 0.000075 Loss 7.269147, Accuracy 77.457%\n",
      "Epoch 14, Batch 787, LR 0.000075 Loss 7.268821, Accuracy 77.461%\n",
      "Epoch 14, Batch 788, LR 0.000075 Loss 7.268093, Accuracy 77.466%\n",
      "Epoch 14, Batch 789, LR 0.000075 Loss 7.267857, Accuracy 77.464%\n",
      "Epoch 14, Batch 790, LR 0.000075 Loss 7.266819, Accuracy 77.472%\n",
      "Epoch 14, Batch 791, LR 0.000075 Loss 7.267206, Accuracy 77.469%\n",
      "Epoch 14, Batch 792, LR 0.000075 Loss 7.265695, Accuracy 77.477%\n",
      "Epoch 14, Batch 793, LR 0.000075 Loss 7.266296, Accuracy 77.474%\n",
      "Epoch 14, Batch 794, LR 0.000075 Loss 7.265872, Accuracy 77.474%\n",
      "Epoch 14, Batch 795, LR 0.000075 Loss 7.265618, Accuracy 77.478%\n",
      "Epoch 14, Batch 796, LR 0.000075 Loss 7.265043, Accuracy 77.481%\n",
      "Epoch 14, Batch 797, LR 0.000075 Loss 7.265152, Accuracy 77.480%\n",
      "Epoch 14, Batch 798, LR 0.000075 Loss 7.264762, Accuracy 77.483%\n",
      "Epoch 14, Batch 799, LR 0.000075 Loss 7.265124, Accuracy 77.478%\n",
      "Epoch 14, Batch 800, LR 0.000075 Loss 7.265334, Accuracy 77.473%\n",
      "Epoch 14, Batch 801, LR 0.000075 Loss 7.264284, Accuracy 77.479%\n",
      "Epoch 14, Batch 802, LR 0.000075 Loss 7.264566, Accuracy 77.477%\n",
      "Epoch 14, Batch 803, LR 0.000075 Loss 7.265091, Accuracy 77.472%\n",
      "Epoch 14, Batch 804, LR 0.000075 Loss 7.265422, Accuracy 77.468%\n",
      "Epoch 14, Batch 805, LR 0.000075 Loss 7.265679, Accuracy 77.464%\n",
      "Epoch 14, Batch 806, LR 0.000075 Loss 7.266127, Accuracy 77.457%\n",
      "Epoch 14, Batch 807, LR 0.000075 Loss 7.266653, Accuracy 77.454%\n",
      "Epoch 14, Batch 808, LR 0.000075 Loss 7.267558, Accuracy 77.449%\n",
      "Epoch 14, Batch 809, LR 0.000075 Loss 7.267561, Accuracy 77.448%\n",
      "Epoch 14, Batch 810, LR 0.000075 Loss 7.268703, Accuracy 77.437%\n",
      "Epoch 14, Batch 811, LR 0.000075 Loss 7.269020, Accuracy 77.434%\n",
      "Epoch 14, Batch 812, LR 0.000075 Loss 7.268997, Accuracy 77.436%\n",
      "Epoch 14, Batch 813, LR 0.000075 Loss 7.268427, Accuracy 77.439%\n",
      "Epoch 14, Batch 814, LR 0.000075 Loss 7.267623, Accuracy 77.445%\n",
      "Epoch 14, Batch 815, LR 0.000075 Loss 7.267545, Accuracy 77.445%\n",
      "Epoch 14, Batch 816, LR 0.000075 Loss 7.268081, Accuracy 77.440%\n",
      "Epoch 14, Batch 817, LR 0.000075 Loss 7.268008, Accuracy 77.438%\n",
      "Epoch 14, Batch 818, LR 0.000075 Loss 7.268623, Accuracy 77.433%\n",
      "Epoch 14, Batch 819, LR 0.000075 Loss 7.269860, Accuracy 77.425%\n",
      "Epoch 14, Batch 820, LR 0.000075 Loss 7.268963, Accuracy 77.427%\n",
      "Epoch 14, Batch 821, LR 0.000075 Loss 7.268468, Accuracy 77.427%\n",
      "Epoch 14, Batch 822, LR 0.000075 Loss 7.268313, Accuracy 77.433%\n",
      "Epoch 14, Batch 823, LR 0.000075 Loss 7.268936, Accuracy 77.436%\n",
      "Epoch 14, Batch 824, LR 0.000075 Loss 7.268381, Accuracy 77.438%\n",
      "Epoch 14, Batch 825, LR 0.000075 Loss 7.267939, Accuracy 77.440%\n",
      "Epoch 14, Batch 826, LR 0.000075 Loss 7.267952, Accuracy 77.441%\n",
      "Epoch 14, Batch 827, LR 0.000075 Loss 7.267280, Accuracy 77.444%\n",
      "Epoch 14, Batch 828, LR 0.000075 Loss 7.266379, Accuracy 77.448%\n",
      "Epoch 14, Batch 829, LR 0.000075 Loss 7.266569, Accuracy 77.445%\n",
      "Epoch 14, Batch 830, LR 0.000075 Loss 7.267470, Accuracy 77.434%\n",
      "Epoch 14, Batch 831, LR 0.000075 Loss 7.267947, Accuracy 77.432%\n",
      "Epoch 14, Batch 832, LR 0.000075 Loss 7.267580, Accuracy 77.437%\n",
      "Epoch 14, Batch 833, LR 0.000075 Loss 7.267958, Accuracy 77.432%\n",
      "Epoch 14, Batch 834, LR 0.000075 Loss 7.268102, Accuracy 77.436%\n",
      "Epoch 14, Batch 835, LR 0.000075 Loss 7.267809, Accuracy 77.437%\n",
      "Epoch 14, Batch 836, LR 0.000075 Loss 7.267983, Accuracy 77.432%\n",
      "Epoch 14, Batch 837, LR 0.000075 Loss 7.266840, Accuracy 77.441%\n",
      "Epoch 14, Batch 838, LR 0.000075 Loss 7.266515, Accuracy 77.436%\n",
      "Epoch 14, Batch 839, LR 0.000075 Loss 7.266260, Accuracy 77.440%\n",
      "Epoch 14, Batch 840, LR 0.000075 Loss 7.267073, Accuracy 77.433%\n",
      "Epoch 14, Batch 841, LR 0.000075 Loss 7.267301, Accuracy 77.430%\n",
      "Epoch 14, Batch 842, LR 0.000075 Loss 7.267513, Accuracy 77.428%\n",
      "Epoch 14, Batch 843, LR 0.000075 Loss 7.267047, Accuracy 77.432%\n",
      "Epoch 14, Batch 844, LR 0.000074 Loss 7.266434, Accuracy 77.439%\n",
      "Epoch 14, Batch 845, LR 0.000074 Loss 7.265858, Accuracy 77.440%\n",
      "Epoch 14, Batch 846, LR 0.000074 Loss 7.265257, Accuracy 77.443%\n",
      "Epoch 14, Batch 847, LR 0.000074 Loss 7.265122, Accuracy 77.445%\n",
      "Epoch 14, Batch 848, LR 0.000074 Loss 7.265194, Accuracy 77.443%\n",
      "Epoch 14, Batch 849, LR 0.000074 Loss 7.265004, Accuracy 77.444%\n",
      "Epoch 14, Batch 850, LR 0.000074 Loss 7.265218, Accuracy 77.441%\n",
      "Epoch 14, Batch 851, LR 0.000074 Loss 7.264801, Accuracy 77.442%\n",
      "Epoch 14, Batch 852, LR 0.000074 Loss 7.264965, Accuracy 77.444%\n",
      "Epoch 14, Batch 853, LR 0.000074 Loss 7.264123, Accuracy 77.448%\n",
      "Epoch 14, Batch 854, LR 0.000074 Loss 7.264325, Accuracy 77.444%\n",
      "Epoch 14, Batch 855, LR 0.000074 Loss 7.264108, Accuracy 77.446%\n",
      "Epoch 14, Batch 856, LR 0.000074 Loss 7.263472, Accuracy 77.450%\n",
      "Epoch 14, Batch 857, LR 0.000074 Loss 7.262985, Accuracy 77.450%\n",
      "Epoch 14, Batch 858, LR 0.000074 Loss 7.263262, Accuracy 77.448%\n",
      "Epoch 14, Batch 859, LR 0.000074 Loss 7.263892, Accuracy 77.443%\n",
      "Epoch 14, Batch 860, LR 0.000074 Loss 7.263697, Accuracy 77.445%\n",
      "Epoch 14, Batch 861, LR 0.000074 Loss 7.263093, Accuracy 77.446%\n",
      "Epoch 14, Batch 862, LR 0.000074 Loss 7.263056, Accuracy 77.451%\n",
      "Epoch 14, Batch 863, LR 0.000074 Loss 7.262667, Accuracy 77.451%\n",
      "Epoch 14, Batch 864, LR 0.000074 Loss 7.263411, Accuracy 77.448%\n",
      "Epoch 14, Batch 865, LR 0.000074 Loss 7.263631, Accuracy 77.447%\n",
      "Epoch 14, Batch 866, LR 0.000074 Loss 7.263967, Accuracy 77.445%\n",
      "Epoch 14, Batch 867, LR 0.000074 Loss 7.264602, Accuracy 77.437%\n",
      "Epoch 14, Batch 868, LR 0.000074 Loss 7.264686, Accuracy 77.433%\n",
      "Epoch 14, Batch 869, LR 0.000074 Loss 7.264904, Accuracy 77.427%\n",
      "Epoch 14, Batch 870, LR 0.000074 Loss 7.265625, Accuracy 77.421%\n",
      "Epoch 14, Batch 871, LR 0.000074 Loss 7.265159, Accuracy 77.428%\n",
      "Epoch 14, Batch 872, LR 0.000074 Loss 7.265078, Accuracy 77.428%\n",
      "Epoch 14, Batch 873, LR 0.000074 Loss 7.265463, Accuracy 77.421%\n",
      "Epoch 14, Batch 874, LR 0.000074 Loss 7.266182, Accuracy 77.422%\n",
      "Epoch 14, Batch 875, LR 0.000074 Loss 7.265789, Accuracy 77.421%\n",
      "Epoch 14, Batch 876, LR 0.000074 Loss 7.266695, Accuracy 77.416%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 877, LR 0.000074 Loss 7.266240, Accuracy 77.416%\n",
      "Epoch 14, Batch 878, LR 0.000074 Loss 7.266668, Accuracy 77.415%\n",
      "Epoch 14, Batch 879, LR 0.000074 Loss 7.267087, Accuracy 77.410%\n",
      "Epoch 14, Batch 880, LR 0.000074 Loss 7.266556, Accuracy 77.417%\n",
      "Epoch 14, Batch 881, LR 0.000074 Loss 7.266174, Accuracy 77.416%\n",
      "Epoch 14, Batch 882, LR 0.000074 Loss 7.266002, Accuracy 77.422%\n",
      "Epoch 14, Batch 883, LR 0.000074 Loss 7.266454, Accuracy 77.419%\n",
      "Epoch 14, Batch 884, LR 0.000074 Loss 7.266529, Accuracy 77.419%\n",
      "Epoch 14, Batch 885, LR 0.000074 Loss 7.267030, Accuracy 77.420%\n",
      "Epoch 14, Batch 886, LR 0.000074 Loss 7.267547, Accuracy 77.417%\n",
      "Epoch 14, Batch 887, LR 0.000074 Loss 7.267837, Accuracy 77.417%\n",
      "Epoch 14, Batch 888, LR 0.000074 Loss 7.268799, Accuracy 77.410%\n",
      "Epoch 14, Batch 889, LR 0.000074 Loss 7.268551, Accuracy 77.411%\n",
      "Epoch 14, Batch 890, LR 0.000074 Loss 7.269258, Accuracy 77.404%\n",
      "Epoch 14, Batch 891, LR 0.000074 Loss 7.269820, Accuracy 77.402%\n",
      "Epoch 14, Batch 892, LR 0.000074 Loss 7.269586, Accuracy 77.409%\n",
      "Epoch 14, Batch 893, LR 0.000074 Loss 7.269573, Accuracy 77.411%\n",
      "Epoch 14, Batch 894, LR 0.000074 Loss 7.270192, Accuracy 77.408%\n",
      "Epoch 14, Batch 895, LR 0.000074 Loss 7.270810, Accuracy 77.410%\n",
      "Epoch 14, Batch 896, LR 0.000074 Loss 7.270761, Accuracy 77.414%\n",
      "Epoch 14, Batch 897, LR 0.000074 Loss 7.270823, Accuracy 77.418%\n",
      "Epoch 14, Batch 898, LR 0.000074 Loss 7.271538, Accuracy 77.414%\n",
      "Epoch 14, Batch 899, LR 0.000074 Loss 7.271115, Accuracy 77.418%\n",
      "Epoch 14, Batch 900, LR 0.000074 Loss 7.270979, Accuracy 77.419%\n",
      "Epoch 14, Batch 901, LR 0.000074 Loss 7.270324, Accuracy 77.421%\n",
      "Epoch 14, Batch 902, LR 0.000074 Loss 7.270489, Accuracy 77.419%\n",
      "Epoch 14, Batch 903, LR 0.000074 Loss 7.270524, Accuracy 77.416%\n",
      "Epoch 14, Batch 904, LR 0.000074 Loss 7.271049, Accuracy 77.411%\n",
      "Epoch 14, Batch 905, LR 0.000074 Loss 7.271439, Accuracy 77.409%\n",
      "Epoch 14, Batch 906, LR 0.000074 Loss 7.271122, Accuracy 77.413%\n",
      "Epoch 14, Batch 907, LR 0.000074 Loss 7.271334, Accuracy 77.412%\n",
      "Epoch 14, Batch 908, LR 0.000074 Loss 7.270912, Accuracy 77.416%\n",
      "Epoch 14, Batch 909, LR 0.000074 Loss 7.270637, Accuracy 77.417%\n",
      "Epoch 14, Batch 910, LR 0.000074 Loss 7.271320, Accuracy 77.413%\n",
      "Epoch 14, Batch 911, LR 0.000074 Loss 7.271355, Accuracy 77.417%\n",
      "Epoch 14, Batch 912, LR 0.000074 Loss 7.270584, Accuracy 77.419%\n",
      "Epoch 14, Batch 913, LR 0.000074 Loss 7.270247, Accuracy 77.421%\n",
      "Epoch 14, Batch 914, LR 0.000074 Loss 7.269937, Accuracy 77.426%\n",
      "Epoch 14, Batch 915, LR 0.000074 Loss 7.270230, Accuracy 77.427%\n",
      "Epoch 14, Batch 916, LR 0.000074 Loss 7.270235, Accuracy 77.432%\n",
      "Epoch 14, Batch 917, LR 0.000074 Loss 7.270432, Accuracy 77.425%\n",
      "Epoch 14, Batch 918, LR 0.000074 Loss 7.270243, Accuracy 77.424%\n",
      "Epoch 14, Batch 919, LR 0.000074 Loss 7.270754, Accuracy 77.425%\n",
      "Epoch 14, Batch 920, LR 0.000074 Loss 7.270499, Accuracy 77.422%\n",
      "Epoch 14, Batch 921, LR 0.000074 Loss 7.269745, Accuracy 77.428%\n",
      "Epoch 14, Batch 922, LR 0.000074 Loss 7.269726, Accuracy 77.427%\n",
      "Epoch 14, Batch 923, LR 0.000074 Loss 7.269425, Accuracy 77.428%\n",
      "Epoch 14, Batch 924, LR 0.000074 Loss 7.268821, Accuracy 77.432%\n",
      "Epoch 14, Batch 925, LR 0.000074 Loss 7.268041, Accuracy 77.438%\n",
      "Epoch 14, Batch 926, LR 0.000074 Loss 7.268537, Accuracy 77.429%\n",
      "Epoch 14, Batch 927, LR 0.000074 Loss 7.267636, Accuracy 77.437%\n",
      "Epoch 14, Batch 928, LR 0.000074 Loss 7.268097, Accuracy 77.436%\n",
      "Epoch 14, Batch 929, LR 0.000074 Loss 7.268374, Accuracy 77.434%\n",
      "Epoch 14, Batch 930, LR 0.000074 Loss 7.268579, Accuracy 77.432%\n",
      "Epoch 14, Batch 931, LR 0.000074 Loss 7.267338, Accuracy 77.439%\n",
      "Epoch 14, Batch 932, LR 0.000074 Loss 7.267701, Accuracy 77.433%\n",
      "Epoch 14, Batch 933, LR 0.000074 Loss 7.268274, Accuracy 77.435%\n",
      "Epoch 14, Batch 934, LR 0.000074 Loss 7.268556, Accuracy 77.433%\n",
      "Epoch 14, Batch 935, LR 0.000074 Loss 7.267981, Accuracy 77.442%\n",
      "Epoch 14, Batch 936, LR 0.000074 Loss 7.267546, Accuracy 77.443%\n",
      "Epoch 14, Batch 937, LR 0.000074 Loss 7.267818, Accuracy 77.439%\n",
      "Epoch 14, Batch 938, LR 0.000074 Loss 7.266841, Accuracy 77.440%\n",
      "Epoch 14, Batch 939, LR 0.000074 Loss 7.266893, Accuracy 77.435%\n",
      "Epoch 14, Batch 940, LR 0.000074 Loss 7.266885, Accuracy 77.433%\n",
      "Epoch 14, Batch 941, LR 0.000074 Loss 7.267347, Accuracy 77.431%\n",
      "Epoch 14, Batch 942, LR 0.000074 Loss 7.267305, Accuracy 77.432%\n",
      "Epoch 14, Batch 943, LR 0.000074 Loss 7.266878, Accuracy 77.432%\n",
      "Epoch 14, Batch 944, LR 0.000074 Loss 7.267474, Accuracy 77.431%\n",
      "Epoch 14, Batch 945, LR 0.000074 Loss 7.267320, Accuracy 77.433%\n",
      "Epoch 14, Batch 946, LR 0.000074 Loss 7.267807, Accuracy 77.430%\n",
      "Epoch 14, Batch 947, LR 0.000074 Loss 7.268164, Accuracy 77.426%\n",
      "Epoch 14, Batch 948, LR 0.000074 Loss 7.268436, Accuracy 77.427%\n",
      "Epoch 14, Batch 949, LR 0.000074 Loss 7.268452, Accuracy 77.427%\n",
      "Epoch 14, Batch 950, LR 0.000074 Loss 7.268779, Accuracy 77.428%\n",
      "Epoch 14, Batch 951, LR 0.000074 Loss 7.267886, Accuracy 77.433%\n",
      "Epoch 14, Batch 952, LR 0.000074 Loss 7.267910, Accuracy 77.432%\n",
      "Epoch 14, Batch 953, LR 0.000074 Loss 7.268275, Accuracy 77.431%\n",
      "Epoch 14, Batch 954, LR 0.000074 Loss 7.268383, Accuracy 77.426%\n",
      "Epoch 14, Batch 955, LR 0.000074 Loss 7.268019, Accuracy 77.430%\n",
      "Epoch 14, Batch 956, LR 0.000074 Loss 7.268316, Accuracy 77.425%\n",
      "Epoch 14, Batch 957, LR 0.000074 Loss 7.268534, Accuracy 77.423%\n",
      "Epoch 14, Batch 958, LR 0.000074 Loss 7.269135, Accuracy 77.418%\n",
      "Epoch 14, Batch 959, LR 0.000074 Loss 7.269589, Accuracy 77.416%\n",
      "Epoch 14, Batch 960, LR 0.000074 Loss 7.269894, Accuracy 77.416%\n",
      "Epoch 14, Batch 961, LR 0.000074 Loss 7.270008, Accuracy 77.417%\n",
      "Epoch 14, Batch 962, LR 0.000074 Loss 7.270639, Accuracy 77.411%\n",
      "Epoch 14, Batch 963, LR 0.000074 Loss 7.270600, Accuracy 77.410%\n",
      "Epoch 14, Batch 964, LR 0.000074 Loss 7.271173, Accuracy 77.413%\n",
      "Epoch 14, Batch 965, LR 0.000074 Loss 7.270736, Accuracy 77.413%\n",
      "Epoch 14, Batch 966, LR 0.000074 Loss 7.270794, Accuracy 77.410%\n",
      "Epoch 14, Batch 967, LR 0.000074 Loss 7.270866, Accuracy 77.405%\n",
      "Epoch 14, Batch 968, LR 0.000074 Loss 7.270238, Accuracy 77.409%\n",
      "Epoch 14, Batch 969, LR 0.000074 Loss 7.270220, Accuracy 77.408%\n",
      "Epoch 14, Batch 970, LR 0.000074 Loss 7.269995, Accuracy 77.408%\n",
      "Epoch 14, Batch 971, LR 0.000074 Loss 7.269483, Accuracy 77.409%\n",
      "Epoch 14, Batch 972, LR 0.000074 Loss 7.269290, Accuracy 77.412%\n",
      "Epoch 14, Batch 973, LR 0.000074 Loss 7.269957, Accuracy 77.410%\n",
      "Epoch 14, Batch 974, LR 0.000074 Loss 7.269746, Accuracy 77.411%\n",
      "Epoch 14, Batch 975, LR 0.000074 Loss 7.270458, Accuracy 77.408%\n",
      "Epoch 14, Batch 976, LR 0.000074 Loss 7.269976, Accuracy 77.413%\n",
      "Epoch 14, Batch 977, LR 0.000074 Loss 7.269790, Accuracy 77.417%\n",
      "Epoch 14, Batch 978, LR 0.000074 Loss 7.270185, Accuracy 77.414%\n",
      "Epoch 14, Batch 979, LR 0.000074 Loss 7.269629, Accuracy 77.414%\n",
      "Epoch 14, Batch 980, LR 0.000074 Loss 7.268668, Accuracy 77.418%\n",
      "Epoch 14, Batch 981, LR 0.000074 Loss 7.268114, Accuracy 77.424%\n",
      "Epoch 14, Batch 982, LR 0.000074 Loss 7.268606, Accuracy 77.425%\n",
      "Epoch 14, Batch 983, LR 0.000074 Loss 7.268794, Accuracy 77.426%\n",
      "Epoch 14, Batch 984, LR 0.000074 Loss 7.267996, Accuracy 77.429%\n",
      "Epoch 14, Batch 985, LR 0.000074 Loss 7.267694, Accuracy 77.431%\n",
      "Epoch 14, Batch 986, LR 0.000074 Loss 7.267872, Accuracy 77.432%\n",
      "Epoch 14, Batch 987, LR 0.000074 Loss 7.267980, Accuracy 77.432%\n",
      "Epoch 14, Batch 988, LR 0.000074 Loss 7.268063, Accuracy 77.432%\n",
      "Epoch 14, Batch 989, LR 0.000074 Loss 7.267900, Accuracy 77.435%\n",
      "Epoch 14, Batch 990, LR 0.000074 Loss 7.267729, Accuracy 77.437%\n",
      "Epoch 14, Batch 991, LR 0.000074 Loss 7.268380, Accuracy 77.437%\n",
      "Epoch 14, Batch 992, LR 0.000074 Loss 7.268340, Accuracy 77.435%\n",
      "Epoch 14, Batch 993, LR 0.000074 Loss 7.267838, Accuracy 77.438%\n",
      "Epoch 14, Batch 994, LR 0.000074 Loss 7.267600, Accuracy 77.440%\n",
      "Epoch 14, Batch 995, LR 0.000074 Loss 7.267944, Accuracy 77.436%\n",
      "Epoch 14, Batch 996, LR 0.000074 Loss 7.267318, Accuracy 77.443%\n",
      "Epoch 14, Batch 997, LR 0.000074 Loss 7.266874, Accuracy 77.445%\n",
      "Epoch 14, Batch 998, LR 0.000074 Loss 7.267291, Accuracy 77.446%\n",
      "Epoch 14, Batch 999, LR 0.000074 Loss 7.267780, Accuracy 77.441%\n",
      "Epoch 14, Batch 1000, LR 0.000074 Loss 7.267919, Accuracy 77.441%\n",
      "Epoch 14, Batch 1001, LR 0.000074 Loss 7.268382, Accuracy 77.436%\n",
      "Epoch 14, Batch 1002, LR 0.000074 Loss 7.268162, Accuracy 77.436%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 1003, LR 0.000074 Loss 7.268849, Accuracy 77.431%\n",
      "Epoch 14, Batch 1004, LR 0.000074 Loss 7.269155, Accuracy 77.432%\n",
      "Epoch 14, Batch 1005, LR 0.000074 Loss 7.269137, Accuracy 77.435%\n",
      "Epoch 14, Batch 1006, LR 0.000074 Loss 7.269302, Accuracy 77.440%\n",
      "Epoch 14, Batch 1007, LR 0.000074 Loss 7.268979, Accuracy 77.440%\n",
      "Epoch 14, Batch 1008, LR 0.000074 Loss 7.268353, Accuracy 77.445%\n",
      "Epoch 14, Batch 1009, LR 0.000074 Loss 7.268532, Accuracy 77.447%\n",
      "Epoch 14, Batch 1010, LR 0.000074 Loss 7.268255, Accuracy 77.448%\n",
      "Epoch 14, Batch 1011, LR 0.000074 Loss 7.269232, Accuracy 77.452%\n",
      "Epoch 14, Batch 1012, LR 0.000074 Loss 7.269154, Accuracy 77.449%\n",
      "Epoch 14, Batch 1013, LR 0.000074 Loss 7.269302, Accuracy 77.449%\n",
      "Epoch 14, Batch 1014, LR 0.000074 Loss 7.269174, Accuracy 77.452%\n",
      "Epoch 14, Batch 1015, LR 0.000074 Loss 7.269887, Accuracy 77.450%\n",
      "Epoch 14, Batch 1016, LR 0.000074 Loss 7.270460, Accuracy 77.450%\n",
      "Epoch 14, Batch 1017, LR 0.000074 Loss 7.270616, Accuracy 77.452%\n",
      "Epoch 14, Batch 1018, LR 0.000074 Loss 7.270203, Accuracy 77.457%\n",
      "Epoch 14, Batch 1019, LR 0.000074 Loss 7.270352, Accuracy 77.456%\n",
      "Epoch 14, Batch 1020, LR 0.000074 Loss 7.269962, Accuracy 77.462%\n",
      "Epoch 14, Batch 1021, LR 0.000074 Loss 7.269531, Accuracy 77.464%\n",
      "Epoch 14, Batch 1022, LR 0.000074 Loss 7.268794, Accuracy 77.467%\n",
      "Epoch 14, Batch 1023, LR 0.000074 Loss 7.268503, Accuracy 77.473%\n",
      "Epoch 14, Batch 1024, LR 0.000074 Loss 7.269333, Accuracy 77.472%\n",
      "Epoch 14, Batch 1025, LR 0.000074 Loss 7.269968, Accuracy 77.470%\n",
      "Epoch 14, Batch 1026, LR 0.000074 Loss 7.270385, Accuracy 77.469%\n",
      "Epoch 14, Batch 1027, LR 0.000074 Loss 7.270267, Accuracy 77.468%\n",
      "Epoch 14, Batch 1028, LR 0.000074 Loss 7.270315, Accuracy 77.468%\n",
      "Epoch 14, Batch 1029, LR 0.000074 Loss 7.270419, Accuracy 77.472%\n",
      "Epoch 14, Batch 1030, LR 0.000074 Loss 7.270159, Accuracy 77.473%\n",
      "Epoch 14, Batch 1031, LR 0.000074 Loss 7.269252, Accuracy 77.479%\n",
      "Epoch 14, Batch 1032, LR 0.000074 Loss 7.269778, Accuracy 77.477%\n",
      "Epoch 14, Batch 1033, LR 0.000074 Loss 7.269952, Accuracy 77.481%\n",
      "Epoch 14, Batch 1034, LR 0.000074 Loss 7.268437, Accuracy 77.490%\n",
      "Epoch 14, Batch 1035, LR 0.000074 Loss 7.268388, Accuracy 77.491%\n",
      "Epoch 14, Batch 1036, LR 0.000074 Loss 7.266879, Accuracy 77.501%\n",
      "Epoch 14, Batch 1037, LR 0.000074 Loss 7.267098, Accuracy 77.500%\n",
      "Epoch 14, Batch 1038, LR 0.000074 Loss 7.267171, Accuracy 77.503%\n",
      "Epoch 14, Batch 1039, LR 0.000074 Loss 7.267093, Accuracy 77.502%\n",
      "Epoch 14, Batch 1040, LR 0.000074 Loss 7.266541, Accuracy 77.508%\n",
      "Epoch 14, Batch 1041, LR 0.000074 Loss 7.266718, Accuracy 77.509%\n",
      "Epoch 14, Batch 1042, LR 0.000074 Loss 7.267885, Accuracy 77.500%\n",
      "Epoch 14, Batch 1043, LR 0.000074 Loss 7.267537, Accuracy 77.502%\n",
      "Epoch 14, Batch 1044, LR 0.000074 Loss 7.268261, Accuracy 77.493%\n",
      "Epoch 14, Batch 1045, LR 0.000074 Loss 7.268637, Accuracy 77.496%\n",
      "Epoch 14, Batch 1046, LR 0.000074 Loss 7.268806, Accuracy 77.492%\n",
      "Epoch 14, Batch 1047, LR 0.000074 Loss 7.269210, Accuracy 77.488%\n",
      "Epoch 14, Loss (train set) 7.269210, Accuracy (train set) 77.488%\n",
      "Epoch 14, Accuracy (validation set) 56.659%\n",
      "Epoch 14, EER (test set) 5.648%\n",
      "Epoch 15, Batch 1, LR 0.000074 Loss 6.857263, Accuracy 81.250%\n",
      "Epoch 15, Batch 2, LR 0.000074 Loss 7.420904, Accuracy 76.953%\n",
      "Epoch 15, Batch 3, LR 0.000074 Loss 7.507146, Accuracy 76.042%\n",
      "Epoch 15, Batch 4, LR 0.000074 Loss 7.368798, Accuracy 76.367%\n",
      "Epoch 15, Batch 5, LR 0.000074 Loss 7.249175, Accuracy 77.188%\n",
      "Epoch 15, Batch 6, LR 0.000074 Loss 7.124251, Accuracy 77.604%\n",
      "Epoch 15, Batch 7, LR 0.000074 Loss 7.076550, Accuracy 78.013%\n",
      "Epoch 15, Batch 8, LR 0.000074 Loss 7.117278, Accuracy 77.051%\n",
      "Epoch 15, Batch 9, LR 0.000074 Loss 7.185871, Accuracy 76.476%\n",
      "Epoch 15, Batch 10, LR 0.000074 Loss 7.145510, Accuracy 76.875%\n",
      "Epoch 15, Batch 11, LR 0.000074 Loss 7.163562, Accuracy 76.705%\n",
      "Epoch 15, Batch 12, LR 0.000074 Loss 7.193430, Accuracy 76.497%\n",
      "Epoch 15, Batch 13, LR 0.000074 Loss 7.232979, Accuracy 76.382%\n",
      "Epoch 15, Batch 14, LR 0.000074 Loss 7.175436, Accuracy 76.897%\n",
      "Epoch 15, Batch 15, LR 0.000074 Loss 7.163680, Accuracy 77.292%\n",
      "Epoch 15, Batch 16, LR 0.000074 Loss 7.219443, Accuracy 77.051%\n",
      "Epoch 15, Batch 17, LR 0.000074 Loss 7.230804, Accuracy 77.022%\n",
      "Epoch 15, Batch 18, LR 0.000074 Loss 7.220140, Accuracy 76.823%\n",
      "Epoch 15, Batch 19, LR 0.000074 Loss 7.213536, Accuracy 77.015%\n",
      "Epoch 15, Batch 20, LR 0.000074 Loss 7.226496, Accuracy 77.148%\n",
      "Epoch 15, Batch 21, LR 0.000074 Loss 7.199256, Accuracy 77.418%\n",
      "Epoch 15, Batch 22, LR 0.000074 Loss 7.181480, Accuracy 77.486%\n",
      "Epoch 15, Batch 23, LR 0.000074 Loss 7.145898, Accuracy 77.649%\n",
      "Epoch 15, Batch 24, LR 0.000074 Loss 7.136880, Accuracy 77.734%\n",
      "Epoch 15, Batch 25, LR 0.000074 Loss 7.128051, Accuracy 77.906%\n",
      "Epoch 15, Batch 26, LR 0.000074 Loss 7.132401, Accuracy 77.734%\n",
      "Epoch 15, Batch 27, LR 0.000074 Loss 7.118870, Accuracy 77.778%\n",
      "Epoch 15, Batch 28, LR 0.000074 Loss 7.144894, Accuracy 77.595%\n",
      "Epoch 15, Batch 29, LR 0.000074 Loss 7.127604, Accuracy 77.640%\n",
      "Epoch 15, Batch 30, LR 0.000074 Loss 7.091850, Accuracy 77.839%\n",
      "Epoch 15, Batch 31, LR 0.000074 Loss 7.103562, Accuracy 77.722%\n",
      "Epoch 15, Batch 32, LR 0.000074 Loss 7.110743, Accuracy 77.710%\n",
      "Epoch 15, Batch 33, LR 0.000074 Loss 7.113925, Accuracy 77.652%\n",
      "Epoch 15, Batch 34, LR 0.000074 Loss 7.094947, Accuracy 77.734%\n",
      "Epoch 15, Batch 35, LR 0.000074 Loss 7.104560, Accuracy 77.768%\n",
      "Epoch 15, Batch 36, LR 0.000074 Loss 7.092048, Accuracy 77.778%\n",
      "Epoch 15, Batch 37, LR 0.000074 Loss 7.095941, Accuracy 77.703%\n",
      "Epoch 15, Batch 38, LR 0.000074 Loss 7.067341, Accuracy 77.858%\n",
      "Epoch 15, Batch 39, LR 0.000074 Loss 7.046707, Accuracy 78.065%\n",
      "Epoch 15, Batch 40, LR 0.000074 Loss 7.036985, Accuracy 78.145%\n",
      "Epoch 15, Batch 41, LR 0.000074 Loss 7.031225, Accuracy 78.220%\n",
      "Epoch 15, Batch 42, LR 0.000074 Loss 7.039974, Accuracy 78.144%\n",
      "Epoch 15, Batch 43, LR 0.000074 Loss 7.040794, Accuracy 78.143%\n",
      "Epoch 15, Batch 44, LR 0.000074 Loss 7.029600, Accuracy 78.196%\n",
      "Epoch 15, Batch 45, LR 0.000074 Loss 7.034159, Accuracy 78.142%\n",
      "Epoch 15, Batch 46, LR 0.000074 Loss 7.017860, Accuracy 78.346%\n",
      "Epoch 15, Batch 47, LR 0.000074 Loss 7.026138, Accuracy 78.291%\n",
      "Epoch 15, Batch 48, LR 0.000074 Loss 7.012671, Accuracy 78.288%\n",
      "Epoch 15, Batch 49, LR 0.000074 Loss 6.997865, Accuracy 78.396%\n",
      "Epoch 15, Batch 50, LR 0.000074 Loss 6.982892, Accuracy 78.531%\n",
      "Epoch 15, Batch 51, LR 0.000074 Loss 6.985159, Accuracy 78.508%\n",
      "Epoch 15, Batch 52, LR 0.000074 Loss 6.995485, Accuracy 78.441%\n",
      "Epoch 15, Batch 53, LR 0.000074 Loss 6.984562, Accuracy 78.494%\n",
      "Epoch 15, Batch 54, LR 0.000074 Loss 7.003054, Accuracy 78.429%\n",
      "Epoch 15, Batch 55, LR 0.000074 Loss 7.009953, Accuracy 78.381%\n",
      "Epoch 15, Batch 56, LR 0.000074 Loss 7.006772, Accuracy 78.432%\n",
      "Epoch 15, Batch 57, LR 0.000074 Loss 7.016085, Accuracy 78.399%\n",
      "Epoch 15, Batch 58, LR 0.000074 Loss 7.016519, Accuracy 78.435%\n",
      "Epoch 15, Batch 59, LR 0.000074 Loss 7.001101, Accuracy 78.456%\n",
      "Epoch 15, Batch 60, LR 0.000074 Loss 6.996226, Accuracy 78.516%\n",
      "Epoch 15, Batch 61, LR 0.000074 Loss 6.981166, Accuracy 78.612%\n",
      "Epoch 15, Batch 62, LR 0.000074 Loss 6.968095, Accuracy 78.705%\n",
      "Epoch 15, Batch 63, LR 0.000074 Loss 6.968727, Accuracy 78.733%\n",
      "Epoch 15, Batch 64, LR 0.000074 Loss 6.960318, Accuracy 78.748%\n",
      "Epoch 15, Batch 65, LR 0.000074 Loss 6.956519, Accuracy 78.714%\n",
      "Epoch 15, Batch 66, LR 0.000074 Loss 6.952129, Accuracy 78.717%\n",
      "Epoch 15, Batch 67, LR 0.000074 Loss 6.950130, Accuracy 78.696%\n",
      "Epoch 15, Batch 68, LR 0.000074 Loss 6.959324, Accuracy 78.631%\n",
      "Epoch 15, Batch 69, LR 0.000074 Loss 6.963806, Accuracy 78.567%\n",
      "Epoch 15, Batch 70, LR 0.000074 Loss 6.957356, Accuracy 78.638%\n",
      "Epoch 15, Batch 71, LR 0.000074 Loss 6.957229, Accuracy 78.642%\n",
      "Epoch 15, Batch 72, LR 0.000074 Loss 6.973541, Accuracy 78.635%\n",
      "Epoch 15, Batch 73, LR 0.000074 Loss 6.971227, Accuracy 78.671%\n",
      "Epoch 15, Batch 74, LR 0.000074 Loss 6.981884, Accuracy 78.600%\n",
      "Epoch 15, Batch 75, LR 0.000074 Loss 6.977311, Accuracy 78.615%\n",
      "Epoch 15, Batch 76, LR 0.000074 Loss 6.976614, Accuracy 78.680%\n",
      "Epoch 15, Batch 77, LR 0.000074 Loss 6.972478, Accuracy 78.703%\n",
      "Epoch 15, Batch 78, LR 0.000074 Loss 6.960951, Accuracy 78.756%\n",
      "Epoch 15, Batch 79, LR 0.000074 Loss 6.958081, Accuracy 78.797%\n",
      "Epoch 15, Batch 80, LR 0.000074 Loss 6.954864, Accuracy 78.789%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 81, LR 0.000074 Loss 6.950314, Accuracy 78.839%\n",
      "Epoch 15, Batch 82, LR 0.000074 Loss 6.947141, Accuracy 78.830%\n",
      "Epoch 15, Batch 83, LR 0.000074 Loss 6.948109, Accuracy 78.812%\n",
      "Epoch 15, Batch 84, LR 0.000074 Loss 6.953315, Accuracy 78.813%\n",
      "Epoch 15, Batch 85, LR 0.000074 Loss 6.947920, Accuracy 78.796%\n",
      "Epoch 15, Batch 86, LR 0.000073 Loss 6.944794, Accuracy 78.788%\n",
      "Epoch 15, Batch 87, LR 0.000073 Loss 6.942385, Accuracy 78.798%\n",
      "Epoch 15, Batch 88, LR 0.000073 Loss 6.946924, Accuracy 78.729%\n",
      "Epoch 15, Batch 89, LR 0.000073 Loss 6.940796, Accuracy 78.748%\n",
      "Epoch 15, Batch 90, LR 0.000073 Loss 6.942909, Accuracy 78.793%\n",
      "Epoch 15, Batch 91, LR 0.000073 Loss 6.950698, Accuracy 78.743%\n",
      "Epoch 15, Batch 92, LR 0.000073 Loss 6.959156, Accuracy 78.668%\n",
      "Epoch 15, Batch 93, LR 0.000073 Loss 6.968901, Accuracy 78.562%\n",
      "Epoch 15, Batch 94, LR 0.000073 Loss 6.972981, Accuracy 78.607%\n",
      "Epoch 15, Batch 95, LR 0.000073 Loss 6.974013, Accuracy 78.594%\n",
      "Epoch 15, Batch 96, LR 0.000073 Loss 6.976651, Accuracy 78.581%\n",
      "Epoch 15, Batch 97, LR 0.000073 Loss 6.984193, Accuracy 78.544%\n",
      "Epoch 15, Batch 98, LR 0.000073 Loss 6.988427, Accuracy 78.524%\n",
      "Epoch 15, Batch 99, LR 0.000073 Loss 6.988084, Accuracy 78.480%\n",
      "Epoch 15, Batch 100, LR 0.000073 Loss 6.984718, Accuracy 78.508%\n",
      "Epoch 15, Batch 101, LR 0.000073 Loss 6.987565, Accuracy 78.519%\n",
      "Epoch 15, Batch 102, LR 0.000073 Loss 6.994161, Accuracy 78.516%\n",
      "Epoch 15, Batch 103, LR 0.000073 Loss 6.996234, Accuracy 78.481%\n",
      "Epoch 15, Batch 104, LR 0.000073 Loss 6.994423, Accuracy 78.478%\n",
      "Epoch 15, Batch 105, LR 0.000073 Loss 7.001103, Accuracy 78.408%\n",
      "Epoch 15, Batch 106, LR 0.000073 Loss 7.000650, Accuracy 78.449%\n",
      "Epoch 15, Batch 107, LR 0.000073 Loss 6.999311, Accuracy 78.446%\n",
      "Epoch 15, Batch 108, LR 0.000073 Loss 7.004777, Accuracy 78.407%\n",
      "Epoch 15, Batch 109, LR 0.000073 Loss 6.998157, Accuracy 78.469%\n",
      "Epoch 15, Batch 110, LR 0.000073 Loss 7.000624, Accuracy 78.452%\n",
      "Epoch 15, Batch 111, LR 0.000073 Loss 7.001338, Accuracy 78.407%\n",
      "Epoch 15, Batch 112, LR 0.000073 Loss 7.001424, Accuracy 78.404%\n",
      "Epoch 15, Batch 113, LR 0.000073 Loss 6.993192, Accuracy 78.408%\n",
      "Epoch 15, Batch 114, LR 0.000073 Loss 6.995615, Accuracy 78.385%\n",
      "Epoch 15, Batch 115, LR 0.000073 Loss 6.996266, Accuracy 78.397%\n",
      "Epoch 15, Batch 116, LR 0.000073 Loss 6.994714, Accuracy 78.435%\n",
      "Epoch 15, Batch 117, LR 0.000073 Loss 6.999157, Accuracy 78.385%\n",
      "Epoch 15, Batch 118, LR 0.000073 Loss 6.999699, Accuracy 78.390%\n",
      "Epoch 15, Batch 119, LR 0.000073 Loss 7.000173, Accuracy 78.381%\n",
      "Epoch 15, Batch 120, LR 0.000073 Loss 6.996708, Accuracy 78.392%\n",
      "Epoch 15, Batch 121, LR 0.000073 Loss 6.993425, Accuracy 78.364%\n",
      "Epoch 15, Batch 122, LR 0.000073 Loss 7.000208, Accuracy 78.285%\n",
      "Epoch 15, Batch 123, LR 0.000073 Loss 7.001067, Accuracy 78.296%\n",
      "Epoch 15, Batch 124, LR 0.000073 Loss 6.997149, Accuracy 78.301%\n",
      "Epoch 15, Batch 125, LR 0.000073 Loss 6.993692, Accuracy 78.344%\n",
      "Epoch 15, Batch 126, LR 0.000073 Loss 6.988003, Accuracy 78.410%\n",
      "Epoch 15, Batch 127, LR 0.000073 Loss 6.987603, Accuracy 78.420%\n",
      "Epoch 15, Batch 128, LR 0.000073 Loss 6.983712, Accuracy 78.461%\n",
      "Epoch 15, Batch 129, LR 0.000073 Loss 6.981238, Accuracy 78.500%\n",
      "Epoch 15, Batch 130, LR 0.000073 Loss 6.980403, Accuracy 78.492%\n",
      "Epoch 15, Batch 131, LR 0.000073 Loss 6.975718, Accuracy 78.531%\n",
      "Epoch 15, Batch 132, LR 0.000073 Loss 6.973011, Accuracy 78.581%\n",
      "Epoch 15, Batch 133, LR 0.000073 Loss 6.973220, Accuracy 78.577%\n",
      "Epoch 15, Batch 134, LR 0.000073 Loss 6.964704, Accuracy 78.609%\n",
      "Epoch 15, Batch 135, LR 0.000073 Loss 6.967512, Accuracy 78.600%\n",
      "Epoch 15, Batch 136, LR 0.000073 Loss 6.963497, Accuracy 78.625%\n",
      "Epoch 15, Batch 137, LR 0.000073 Loss 6.964049, Accuracy 78.627%\n",
      "Epoch 15, Batch 138, LR 0.000073 Loss 6.957380, Accuracy 78.708%\n",
      "Epoch 15, Batch 139, LR 0.000073 Loss 6.958251, Accuracy 78.693%\n",
      "Epoch 15, Batch 140, LR 0.000073 Loss 6.958504, Accuracy 78.683%\n",
      "Epoch 15, Batch 141, LR 0.000073 Loss 6.954150, Accuracy 78.712%\n",
      "Epoch 15, Batch 142, LR 0.000073 Loss 6.951919, Accuracy 78.741%\n",
      "Epoch 15, Batch 143, LR 0.000073 Loss 6.952648, Accuracy 78.726%\n",
      "Epoch 15, Batch 144, LR 0.000073 Loss 6.950677, Accuracy 78.749%\n",
      "Epoch 15, Batch 145, LR 0.000073 Loss 6.952291, Accuracy 78.739%\n",
      "Epoch 15, Batch 146, LR 0.000073 Loss 6.950761, Accuracy 78.762%\n",
      "Epoch 15, Batch 147, LR 0.000073 Loss 6.946838, Accuracy 78.784%\n",
      "Epoch 15, Batch 148, LR 0.000073 Loss 6.950417, Accuracy 78.758%\n",
      "Epoch 15, Batch 149, LR 0.000073 Loss 6.943146, Accuracy 78.786%\n",
      "Epoch 15, Batch 150, LR 0.000073 Loss 6.945647, Accuracy 78.766%\n",
      "Epoch 15, Batch 151, LR 0.000073 Loss 6.946887, Accuracy 78.767%\n",
      "Epoch 15, Batch 152, LR 0.000073 Loss 6.946382, Accuracy 78.778%\n",
      "Epoch 15, Batch 153, LR 0.000073 Loss 6.944163, Accuracy 78.825%\n",
      "Epoch 15, Batch 154, LR 0.000073 Loss 6.949755, Accuracy 78.784%\n",
      "Epoch 15, Batch 155, LR 0.000073 Loss 6.949195, Accuracy 78.775%\n",
      "Epoch 15, Batch 156, LR 0.000073 Loss 6.947887, Accuracy 78.776%\n",
      "Epoch 15, Batch 157, LR 0.000073 Loss 6.951723, Accuracy 78.757%\n",
      "Epoch 15, Batch 158, LR 0.000073 Loss 6.957038, Accuracy 78.763%\n",
      "Epoch 15, Batch 159, LR 0.000073 Loss 6.960580, Accuracy 78.739%\n",
      "Epoch 15, Batch 160, LR 0.000073 Loss 6.963909, Accuracy 78.735%\n",
      "Epoch 15, Batch 161, LR 0.000073 Loss 6.963547, Accuracy 78.722%\n",
      "Epoch 15, Batch 162, LR 0.000073 Loss 6.969916, Accuracy 78.689%\n",
      "Epoch 15, Batch 163, LR 0.000073 Loss 6.967753, Accuracy 78.719%\n",
      "Epoch 15, Batch 164, LR 0.000073 Loss 6.962671, Accuracy 78.749%\n",
      "Epoch 15, Batch 165, LR 0.000073 Loss 6.963643, Accuracy 78.745%\n",
      "Epoch 15, Batch 166, LR 0.000073 Loss 6.963436, Accuracy 78.765%\n",
      "Epoch 15, Batch 167, LR 0.000073 Loss 6.960060, Accuracy 78.775%\n",
      "Epoch 15, Batch 168, LR 0.000073 Loss 6.958782, Accuracy 78.790%\n",
      "Epoch 15, Batch 169, LR 0.000073 Loss 6.960516, Accuracy 78.772%\n",
      "Epoch 15, Batch 170, LR 0.000073 Loss 6.967232, Accuracy 78.727%\n",
      "Epoch 15, Batch 171, LR 0.000073 Loss 6.963631, Accuracy 78.742%\n",
      "Epoch 15, Batch 172, LR 0.000073 Loss 6.964978, Accuracy 78.743%\n",
      "Epoch 15, Batch 173, LR 0.000073 Loss 6.964063, Accuracy 78.757%\n",
      "Epoch 15, Batch 174, LR 0.000073 Loss 6.969347, Accuracy 78.740%\n",
      "Epoch 15, Batch 175, LR 0.000073 Loss 6.968943, Accuracy 78.741%\n",
      "Epoch 15, Batch 176, LR 0.000073 Loss 6.971025, Accuracy 78.711%\n",
      "Epoch 15, Batch 177, LR 0.000073 Loss 6.974303, Accuracy 78.708%\n",
      "Epoch 15, Batch 178, LR 0.000073 Loss 6.973241, Accuracy 78.722%\n",
      "Epoch 15, Batch 179, LR 0.000073 Loss 6.973500, Accuracy 78.719%\n",
      "Epoch 15, Batch 180, LR 0.000073 Loss 6.973773, Accuracy 78.720%\n",
      "Epoch 15, Batch 181, LR 0.000073 Loss 6.974614, Accuracy 78.734%\n",
      "Epoch 15, Batch 182, LR 0.000073 Loss 6.974226, Accuracy 78.765%\n",
      "Epoch 15, Batch 183, LR 0.000073 Loss 6.976064, Accuracy 78.765%\n",
      "Epoch 15, Batch 184, LR 0.000073 Loss 6.975191, Accuracy 78.762%\n",
      "Epoch 15, Batch 185, LR 0.000073 Loss 6.975836, Accuracy 78.737%\n",
      "Epoch 15, Batch 186, LR 0.000073 Loss 6.976800, Accuracy 78.721%\n",
      "Epoch 15, Batch 187, LR 0.000073 Loss 6.974169, Accuracy 78.739%\n",
      "Epoch 15, Batch 188, LR 0.000073 Loss 6.974949, Accuracy 78.748%\n",
      "Epoch 15, Batch 189, LR 0.000073 Loss 6.976445, Accuracy 78.729%\n",
      "Epoch 15, Batch 190, LR 0.000073 Loss 6.973673, Accuracy 78.738%\n",
      "Epoch 15, Batch 191, LR 0.000073 Loss 6.972768, Accuracy 78.739%\n",
      "Epoch 15, Batch 192, LR 0.000073 Loss 6.976670, Accuracy 78.719%\n",
      "Epoch 15, Batch 193, LR 0.000073 Loss 6.974632, Accuracy 78.728%\n",
      "Epoch 15, Batch 194, LR 0.000073 Loss 6.973385, Accuracy 78.745%\n",
      "Epoch 15, Batch 195, LR 0.000073 Loss 6.974217, Accuracy 78.726%\n",
      "Epoch 15, Batch 196, LR 0.000073 Loss 6.972985, Accuracy 78.707%\n",
      "Epoch 15, Batch 197, LR 0.000073 Loss 6.973919, Accuracy 78.720%\n",
      "Epoch 15, Batch 198, LR 0.000073 Loss 6.973871, Accuracy 78.709%\n",
      "Epoch 15, Batch 199, LR 0.000073 Loss 6.973629, Accuracy 78.718%\n",
      "Epoch 15, Batch 200, LR 0.000073 Loss 6.971357, Accuracy 78.746%\n",
      "Epoch 15, Batch 201, LR 0.000073 Loss 6.968468, Accuracy 78.755%\n",
      "Epoch 15, Batch 202, LR 0.000073 Loss 6.968965, Accuracy 78.755%\n",
      "Epoch 15, Batch 203, LR 0.000073 Loss 6.969678, Accuracy 78.748%\n",
      "Epoch 15, Batch 204, LR 0.000073 Loss 6.972816, Accuracy 78.730%\n",
      "Epoch 15, Batch 205, LR 0.000073 Loss 6.970783, Accuracy 78.765%\n",
      "Epoch 15, Batch 206, LR 0.000073 Loss 6.971637, Accuracy 78.758%\n",
      "Epoch 15, Batch 207, LR 0.000073 Loss 6.970922, Accuracy 78.778%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 208, LR 0.000073 Loss 6.969109, Accuracy 78.805%\n",
      "Epoch 15, Batch 209, LR 0.000073 Loss 6.972002, Accuracy 78.779%\n",
      "Epoch 15, Batch 210, LR 0.000073 Loss 6.970778, Accuracy 78.795%\n",
      "Epoch 15, Batch 211, LR 0.000073 Loss 6.968847, Accuracy 78.814%\n",
      "Epoch 15, Batch 212, LR 0.000073 Loss 6.970464, Accuracy 78.799%\n",
      "Epoch 15, Batch 213, LR 0.000073 Loss 6.973351, Accuracy 78.804%\n",
      "Epoch 15, Batch 214, LR 0.000073 Loss 6.972728, Accuracy 78.815%\n",
      "Epoch 15, Batch 215, LR 0.000073 Loss 6.968813, Accuracy 78.841%\n",
      "Epoch 15, Batch 216, LR 0.000073 Loss 6.969784, Accuracy 78.827%\n",
      "Epoch 15, Batch 217, LR 0.000073 Loss 6.969136, Accuracy 78.820%\n",
      "Epoch 15, Batch 218, LR 0.000073 Loss 6.967952, Accuracy 78.842%\n",
      "Epoch 15, Batch 219, LR 0.000073 Loss 6.967712, Accuracy 78.863%\n",
      "Epoch 15, Batch 220, LR 0.000073 Loss 6.968737, Accuracy 78.842%\n",
      "Epoch 15, Batch 221, LR 0.000073 Loss 6.968461, Accuracy 78.860%\n",
      "Epoch 15, Batch 222, LR 0.000073 Loss 6.971511, Accuracy 78.857%\n",
      "Epoch 15, Batch 223, LR 0.000073 Loss 6.968695, Accuracy 78.892%\n",
      "Epoch 15, Batch 224, LR 0.000073 Loss 6.967698, Accuracy 78.903%\n",
      "Epoch 15, Batch 225, LR 0.000073 Loss 6.969222, Accuracy 78.899%\n",
      "Epoch 15, Batch 226, LR 0.000073 Loss 6.965894, Accuracy 78.892%\n",
      "Epoch 15, Batch 227, LR 0.000073 Loss 6.963294, Accuracy 78.903%\n",
      "Epoch 15, Batch 228, LR 0.000073 Loss 6.962806, Accuracy 78.893%\n",
      "Epoch 15, Batch 229, LR 0.000073 Loss 6.964751, Accuracy 78.869%\n",
      "Epoch 15, Batch 230, LR 0.000073 Loss 6.964371, Accuracy 78.872%\n",
      "Epoch 15, Batch 231, LR 0.000073 Loss 6.965787, Accuracy 78.869%\n",
      "Epoch 15, Batch 232, LR 0.000073 Loss 6.967302, Accuracy 78.862%\n",
      "Epoch 15, Batch 233, LR 0.000073 Loss 6.965059, Accuracy 78.876%\n",
      "Epoch 15, Batch 234, LR 0.000073 Loss 6.966484, Accuracy 78.876%\n",
      "Epoch 15, Batch 235, LR 0.000073 Loss 6.966546, Accuracy 78.870%\n",
      "Epoch 15, Batch 236, LR 0.000073 Loss 6.967932, Accuracy 78.857%\n",
      "Epoch 15, Batch 237, LR 0.000073 Loss 6.968363, Accuracy 78.867%\n",
      "Epoch 15, Batch 238, LR 0.000073 Loss 6.969015, Accuracy 78.847%\n",
      "Epoch 15, Batch 239, LR 0.000073 Loss 6.969542, Accuracy 78.841%\n",
      "Epoch 15, Batch 240, LR 0.000073 Loss 6.970866, Accuracy 78.825%\n",
      "Epoch 15, Batch 241, LR 0.000073 Loss 6.971891, Accuracy 78.828%\n",
      "Epoch 15, Batch 242, LR 0.000073 Loss 6.970567, Accuracy 78.832%\n",
      "Epoch 15, Batch 243, LR 0.000073 Loss 6.970402, Accuracy 78.836%\n",
      "Epoch 15, Batch 244, LR 0.000073 Loss 6.972872, Accuracy 78.833%\n",
      "Epoch 15, Batch 245, LR 0.000073 Loss 6.972526, Accuracy 78.833%\n",
      "Epoch 15, Batch 246, LR 0.000073 Loss 6.972671, Accuracy 78.836%\n",
      "Epoch 15, Batch 247, LR 0.000073 Loss 6.969704, Accuracy 78.865%\n",
      "Epoch 15, Batch 248, LR 0.000073 Loss 6.971357, Accuracy 78.850%\n",
      "Epoch 15, Batch 249, LR 0.000073 Loss 6.969103, Accuracy 78.850%\n",
      "Epoch 15, Batch 250, LR 0.000073 Loss 6.965821, Accuracy 78.862%\n",
      "Epoch 15, Batch 251, LR 0.000073 Loss 6.965976, Accuracy 78.856%\n",
      "Epoch 15, Batch 252, LR 0.000073 Loss 6.966004, Accuracy 78.847%\n",
      "Epoch 15, Batch 253, LR 0.000073 Loss 6.969312, Accuracy 78.832%\n",
      "Epoch 15, Batch 254, LR 0.000073 Loss 6.968309, Accuracy 78.829%\n",
      "Epoch 15, Batch 255, LR 0.000073 Loss 6.966371, Accuracy 78.817%\n",
      "Epoch 15, Batch 256, LR 0.000073 Loss 6.969612, Accuracy 78.809%\n",
      "Epoch 15, Batch 257, LR 0.000073 Loss 6.970650, Accuracy 78.791%\n",
      "Epoch 15, Batch 258, LR 0.000073 Loss 6.968267, Accuracy 78.806%\n",
      "Epoch 15, Batch 259, LR 0.000073 Loss 6.966189, Accuracy 78.804%\n",
      "Epoch 15, Batch 260, LR 0.000073 Loss 6.967284, Accuracy 78.798%\n",
      "Epoch 15, Batch 261, LR 0.000073 Loss 6.964128, Accuracy 78.810%\n",
      "Epoch 15, Batch 262, LR 0.000073 Loss 6.962054, Accuracy 78.826%\n",
      "Epoch 15, Batch 263, LR 0.000073 Loss 6.961332, Accuracy 78.826%\n",
      "Epoch 15, Batch 264, LR 0.000073 Loss 6.962203, Accuracy 78.812%\n",
      "Epoch 15, Batch 265, LR 0.000073 Loss 6.962009, Accuracy 78.803%\n",
      "Epoch 15, Batch 266, LR 0.000073 Loss 6.962658, Accuracy 78.809%\n",
      "Epoch 15, Batch 267, LR 0.000073 Loss 6.964618, Accuracy 78.810%\n",
      "Epoch 15, Batch 268, LR 0.000073 Loss 6.964181, Accuracy 78.804%\n",
      "Epoch 15, Batch 269, LR 0.000073 Loss 6.967796, Accuracy 78.787%\n",
      "Epoch 15, Batch 270, LR 0.000073 Loss 6.970311, Accuracy 78.785%\n",
      "Epoch 15, Batch 271, LR 0.000073 Loss 6.969672, Accuracy 78.791%\n",
      "Epoch 15, Batch 272, LR 0.000073 Loss 6.968482, Accuracy 78.800%\n",
      "Epoch 15, Batch 273, LR 0.000073 Loss 6.969006, Accuracy 78.800%\n",
      "Epoch 15, Batch 274, LR 0.000073 Loss 6.966891, Accuracy 78.815%\n",
      "Epoch 15, Batch 275, LR 0.000073 Loss 6.969753, Accuracy 78.790%\n",
      "Epoch 15, Batch 276, LR 0.000073 Loss 6.972823, Accuracy 78.790%\n",
      "Epoch 15, Batch 277, LR 0.000073 Loss 6.972338, Accuracy 78.799%\n",
      "Epoch 15, Batch 278, LR 0.000073 Loss 6.974961, Accuracy 78.808%\n",
      "Epoch 15, Batch 279, LR 0.000073 Loss 6.974955, Accuracy 78.828%\n",
      "Epoch 15, Batch 280, LR 0.000073 Loss 6.977151, Accuracy 78.806%\n",
      "Epoch 15, Batch 281, LR 0.000073 Loss 6.977255, Accuracy 78.812%\n",
      "Epoch 15, Batch 282, LR 0.000073 Loss 6.978061, Accuracy 78.801%\n",
      "Epoch 15, Batch 283, LR 0.000073 Loss 6.978693, Accuracy 78.796%\n",
      "Epoch 15, Batch 284, LR 0.000073 Loss 6.976435, Accuracy 78.818%\n",
      "Epoch 15, Batch 285, LR 0.000073 Loss 6.974782, Accuracy 78.829%\n",
      "Epoch 15, Batch 286, LR 0.000073 Loss 6.974683, Accuracy 78.841%\n",
      "Epoch 15, Batch 287, LR 0.000073 Loss 6.974601, Accuracy 78.841%\n",
      "Epoch 15, Batch 288, LR 0.000073 Loss 6.973006, Accuracy 78.857%\n",
      "Epoch 15, Batch 289, LR 0.000073 Loss 6.973291, Accuracy 78.844%\n",
      "Epoch 15, Batch 290, LR 0.000073 Loss 6.975181, Accuracy 78.825%\n",
      "Epoch 15, Batch 291, LR 0.000073 Loss 6.973219, Accuracy 78.836%\n",
      "Epoch 15, Batch 292, LR 0.000073 Loss 6.972812, Accuracy 78.842%\n",
      "Epoch 15, Batch 293, LR 0.000073 Loss 6.970997, Accuracy 78.853%\n",
      "Epoch 15, Batch 294, LR 0.000073 Loss 6.971956, Accuracy 78.840%\n",
      "Epoch 15, Batch 295, LR 0.000073 Loss 6.972281, Accuracy 78.835%\n",
      "Epoch 15, Batch 296, LR 0.000073 Loss 6.971220, Accuracy 78.846%\n",
      "Epoch 15, Batch 297, LR 0.000073 Loss 6.971493, Accuracy 78.840%\n",
      "Epoch 15, Batch 298, LR 0.000073 Loss 6.971519, Accuracy 78.843%\n",
      "Epoch 15, Batch 299, LR 0.000073 Loss 6.972153, Accuracy 78.844%\n",
      "Epoch 15, Batch 300, LR 0.000073 Loss 6.974555, Accuracy 78.826%\n",
      "Epoch 15, Batch 301, LR 0.000073 Loss 6.976786, Accuracy 78.808%\n",
      "Epoch 15, Batch 302, LR 0.000073 Loss 6.975640, Accuracy 78.803%\n",
      "Epoch 15, Batch 303, LR 0.000073 Loss 6.977275, Accuracy 78.793%\n",
      "Epoch 15, Batch 304, LR 0.000073 Loss 6.980965, Accuracy 78.767%\n",
      "Epoch 15, Batch 305, LR 0.000073 Loss 6.981610, Accuracy 78.776%\n",
      "Epoch 15, Batch 306, LR 0.000073 Loss 6.981068, Accuracy 78.791%\n",
      "Epoch 15, Batch 307, LR 0.000073 Loss 6.979376, Accuracy 78.807%\n",
      "Epoch 15, Batch 308, LR 0.000073 Loss 6.980467, Accuracy 78.805%\n",
      "Epoch 15, Batch 309, LR 0.000073 Loss 6.982415, Accuracy 78.790%\n",
      "Epoch 15, Batch 310, LR 0.000073 Loss 6.983119, Accuracy 78.783%\n",
      "Epoch 15, Batch 311, LR 0.000073 Loss 6.982605, Accuracy 78.796%\n",
      "Epoch 15, Batch 312, LR 0.000073 Loss 6.980843, Accuracy 78.804%\n",
      "Epoch 15, Batch 313, LR 0.000073 Loss 6.979133, Accuracy 78.819%\n",
      "Epoch 15, Batch 314, LR 0.000073 Loss 6.978799, Accuracy 78.824%\n",
      "Epoch 15, Batch 315, LR 0.000073 Loss 6.978920, Accuracy 78.832%\n",
      "Epoch 15, Batch 316, LR 0.000073 Loss 6.978194, Accuracy 78.825%\n",
      "Epoch 15, Batch 317, LR 0.000073 Loss 6.978514, Accuracy 78.822%\n",
      "Epoch 15, Batch 318, LR 0.000073 Loss 6.979267, Accuracy 78.810%\n",
      "Epoch 15, Batch 319, LR 0.000073 Loss 6.979664, Accuracy 78.796%\n",
      "Epoch 15, Batch 320, LR 0.000073 Loss 6.980445, Accuracy 78.801%\n",
      "Epoch 15, Batch 321, LR 0.000073 Loss 6.980608, Accuracy 78.797%\n",
      "Epoch 15, Batch 322, LR 0.000073 Loss 6.980378, Accuracy 78.792%\n",
      "Epoch 15, Batch 323, LR 0.000073 Loss 6.981793, Accuracy 78.785%\n",
      "Epoch 15, Batch 324, LR 0.000073 Loss 6.983518, Accuracy 78.771%\n",
      "Epoch 15, Batch 325, LR 0.000073 Loss 6.984757, Accuracy 78.760%\n",
      "Epoch 15, Batch 326, LR 0.000073 Loss 6.982921, Accuracy 78.770%\n",
      "Epoch 15, Batch 327, LR 0.000073 Loss 6.980556, Accuracy 78.784%\n",
      "Epoch 15, Batch 328, LR 0.000073 Loss 6.980339, Accuracy 78.792%\n",
      "Epoch 15, Batch 329, LR 0.000073 Loss 6.978474, Accuracy 78.814%\n",
      "Epoch 15, Batch 330, LR 0.000073 Loss 6.977544, Accuracy 78.819%\n",
      "Epoch 15, Batch 331, LR 0.000073 Loss 6.977269, Accuracy 78.821%\n",
      "Epoch 15, Batch 332, LR 0.000073 Loss 6.973687, Accuracy 78.833%\n",
      "Epoch 15, Batch 333, LR 0.000073 Loss 6.973907, Accuracy 78.822%\n",
      "Epoch 15, Batch 334, LR 0.000073 Loss 6.972431, Accuracy 78.831%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 335, LR 0.000073 Loss 6.972486, Accuracy 78.832%\n",
      "Epoch 15, Batch 336, LR 0.000073 Loss 6.972421, Accuracy 78.848%\n",
      "Epoch 15, Batch 337, LR 0.000073 Loss 6.971655, Accuracy 78.858%\n",
      "Epoch 15, Batch 338, LR 0.000073 Loss 6.972585, Accuracy 78.855%\n",
      "Epoch 15, Batch 339, LR 0.000073 Loss 6.971689, Accuracy 78.856%\n",
      "Epoch 15, Batch 340, LR 0.000073 Loss 6.972433, Accuracy 78.853%\n",
      "Epoch 15, Batch 341, LR 0.000073 Loss 6.973141, Accuracy 78.838%\n",
      "Epoch 15, Batch 342, LR 0.000073 Loss 6.972467, Accuracy 78.851%\n",
      "Epoch 15, Batch 343, LR 0.000073 Loss 6.973554, Accuracy 78.847%\n",
      "Epoch 15, Batch 344, LR 0.000073 Loss 6.972215, Accuracy 78.863%\n",
      "Epoch 15, Batch 345, LR 0.000073 Loss 6.970885, Accuracy 78.881%\n",
      "Epoch 15, Batch 346, LR 0.000073 Loss 6.968227, Accuracy 78.890%\n",
      "Epoch 15, Batch 347, LR 0.000073 Loss 6.968267, Accuracy 78.893%\n",
      "Epoch 15, Batch 348, LR 0.000073 Loss 6.968675, Accuracy 78.888%\n",
      "Epoch 15, Batch 349, LR 0.000073 Loss 6.970092, Accuracy 78.873%\n",
      "Epoch 15, Batch 350, LR 0.000073 Loss 6.969585, Accuracy 78.877%\n",
      "Epoch 15, Batch 351, LR 0.000073 Loss 6.969336, Accuracy 78.873%\n",
      "Epoch 15, Batch 352, LR 0.000073 Loss 6.972903, Accuracy 78.853%\n",
      "Epoch 15, Batch 353, LR 0.000073 Loss 6.973934, Accuracy 78.862%\n",
      "Epoch 15, Batch 354, LR 0.000073 Loss 6.974951, Accuracy 78.855%\n",
      "Epoch 15, Batch 355, LR 0.000073 Loss 6.976776, Accuracy 78.851%\n",
      "Epoch 15, Batch 356, LR 0.000073 Loss 6.976225, Accuracy 78.851%\n",
      "Epoch 15, Batch 357, LR 0.000073 Loss 6.976674, Accuracy 78.860%\n",
      "Epoch 15, Batch 358, LR 0.000073 Loss 6.976586, Accuracy 78.871%\n",
      "Epoch 15, Batch 359, LR 0.000073 Loss 6.974527, Accuracy 78.880%\n",
      "Epoch 15, Batch 360, LR 0.000073 Loss 6.973060, Accuracy 78.889%\n",
      "Epoch 15, Batch 361, LR 0.000073 Loss 6.973285, Accuracy 78.889%\n",
      "Epoch 15, Batch 362, LR 0.000073 Loss 6.972101, Accuracy 78.889%\n",
      "Epoch 15, Batch 363, LR 0.000073 Loss 6.970886, Accuracy 78.911%\n",
      "Epoch 15, Batch 364, LR 0.000073 Loss 6.970249, Accuracy 78.921%\n",
      "Epoch 15, Batch 365, LR 0.000073 Loss 6.969029, Accuracy 78.936%\n",
      "Epoch 15, Batch 366, LR 0.000073 Loss 6.970581, Accuracy 78.930%\n",
      "Epoch 15, Batch 367, LR 0.000073 Loss 6.969546, Accuracy 78.938%\n",
      "Epoch 15, Batch 368, LR 0.000073 Loss 6.971152, Accuracy 78.925%\n",
      "Epoch 15, Batch 369, LR 0.000073 Loss 6.968948, Accuracy 78.923%\n",
      "Epoch 15, Batch 370, LR 0.000073 Loss 6.968357, Accuracy 78.925%\n",
      "Epoch 15, Batch 371, LR 0.000072 Loss 6.965933, Accuracy 78.936%\n",
      "Epoch 15, Batch 372, LR 0.000072 Loss 6.965456, Accuracy 78.940%\n",
      "Epoch 15, Batch 373, LR 0.000072 Loss 6.965028, Accuracy 78.942%\n",
      "Epoch 15, Batch 374, LR 0.000072 Loss 6.966843, Accuracy 78.931%\n",
      "Epoch 15, Batch 375, LR 0.000072 Loss 6.971203, Accuracy 78.904%\n",
      "Epoch 15, Batch 376, LR 0.000072 Loss 6.970155, Accuracy 78.906%\n",
      "Epoch 15, Batch 377, LR 0.000072 Loss 6.971324, Accuracy 78.910%\n",
      "Epoch 15, Batch 378, LR 0.000072 Loss 6.970344, Accuracy 78.919%\n",
      "Epoch 15, Batch 379, LR 0.000072 Loss 6.969301, Accuracy 78.925%\n",
      "Epoch 15, Batch 380, LR 0.000072 Loss 6.968219, Accuracy 78.923%\n",
      "Epoch 15, Batch 381, LR 0.000072 Loss 6.968534, Accuracy 78.925%\n",
      "Epoch 15, Batch 382, LR 0.000072 Loss 6.968817, Accuracy 78.921%\n",
      "Epoch 15, Batch 383, LR 0.000072 Loss 6.967114, Accuracy 78.925%\n",
      "Epoch 15, Batch 384, LR 0.000072 Loss 6.966245, Accuracy 78.929%\n",
      "Epoch 15, Batch 385, LR 0.000072 Loss 6.966508, Accuracy 78.920%\n",
      "Epoch 15, Batch 386, LR 0.000072 Loss 6.965145, Accuracy 78.939%\n",
      "Epoch 15, Batch 387, LR 0.000072 Loss 6.963686, Accuracy 78.945%\n",
      "Epoch 15, Batch 388, LR 0.000072 Loss 6.961717, Accuracy 78.945%\n",
      "Epoch 15, Batch 389, LR 0.000072 Loss 6.964372, Accuracy 78.934%\n",
      "Epoch 15, Batch 390, LR 0.000072 Loss 6.966159, Accuracy 78.924%\n",
      "Epoch 15, Batch 391, LR 0.000072 Loss 6.965634, Accuracy 78.924%\n",
      "Epoch 15, Batch 392, LR 0.000072 Loss 6.966498, Accuracy 78.918%\n",
      "Epoch 15, Batch 393, LR 0.000072 Loss 6.966419, Accuracy 78.920%\n",
      "Epoch 15, Batch 394, LR 0.000072 Loss 6.967244, Accuracy 78.914%\n",
      "Epoch 15, Batch 395, LR 0.000072 Loss 6.968179, Accuracy 78.908%\n",
      "Epoch 15, Batch 396, LR 0.000072 Loss 6.966640, Accuracy 78.922%\n",
      "Epoch 15, Batch 397, LR 0.000072 Loss 6.966186, Accuracy 78.932%\n",
      "Epoch 15, Batch 398, LR 0.000072 Loss 6.965112, Accuracy 78.946%\n",
      "Epoch 15, Batch 399, LR 0.000072 Loss 6.965483, Accuracy 78.940%\n",
      "Epoch 15, Batch 400, LR 0.000072 Loss 6.966108, Accuracy 78.934%\n",
      "Epoch 15, Batch 401, LR 0.000072 Loss 6.964981, Accuracy 78.935%\n",
      "Epoch 15, Batch 402, LR 0.000072 Loss 6.964267, Accuracy 78.943%\n",
      "Epoch 15, Batch 403, LR 0.000072 Loss 6.963433, Accuracy 78.953%\n",
      "Epoch 15, Batch 404, LR 0.000072 Loss 6.964354, Accuracy 78.935%\n",
      "Epoch 15, Batch 405, LR 0.000072 Loss 6.963499, Accuracy 78.947%\n",
      "Epoch 15, Batch 406, LR 0.000072 Loss 6.961600, Accuracy 78.956%\n",
      "Epoch 15, Batch 407, LR 0.000072 Loss 6.960248, Accuracy 78.977%\n",
      "Epoch 15, Batch 408, LR 0.000072 Loss 6.960360, Accuracy 78.971%\n",
      "Epoch 15, Batch 409, LR 0.000072 Loss 6.958075, Accuracy 78.986%\n",
      "Epoch 15, Batch 410, LR 0.000072 Loss 6.958750, Accuracy 78.990%\n",
      "Epoch 15, Batch 411, LR 0.000072 Loss 6.957499, Accuracy 79.011%\n",
      "Epoch 15, Batch 412, LR 0.000072 Loss 6.959275, Accuracy 78.993%\n",
      "Epoch 15, Batch 413, LR 0.000072 Loss 6.960708, Accuracy 78.993%\n",
      "Epoch 15, Batch 414, LR 0.000072 Loss 6.961089, Accuracy 78.989%\n",
      "Epoch 15, Batch 415, LR 0.000072 Loss 6.959814, Accuracy 78.989%\n",
      "Epoch 15, Batch 416, LR 0.000072 Loss 6.959943, Accuracy 78.983%\n",
      "Epoch 15, Batch 417, LR 0.000072 Loss 6.960435, Accuracy 78.983%\n",
      "Epoch 15, Batch 418, LR 0.000072 Loss 6.959600, Accuracy 78.985%\n",
      "Epoch 15, Batch 419, LR 0.000072 Loss 6.958534, Accuracy 78.985%\n",
      "Epoch 15, Batch 420, LR 0.000072 Loss 6.957754, Accuracy 78.992%\n",
      "Epoch 15, Batch 421, LR 0.000072 Loss 6.958585, Accuracy 78.984%\n",
      "Epoch 15, Batch 422, LR 0.000072 Loss 6.958295, Accuracy 78.971%\n",
      "Epoch 15, Batch 423, LR 0.000072 Loss 6.958593, Accuracy 78.969%\n",
      "Epoch 15, Batch 424, LR 0.000072 Loss 6.956245, Accuracy 78.980%\n",
      "Epoch 15, Batch 425, LR 0.000072 Loss 6.957224, Accuracy 78.987%\n",
      "Epoch 15, Batch 426, LR 0.000072 Loss 6.957692, Accuracy 78.992%\n",
      "Epoch 15, Batch 427, LR 0.000072 Loss 6.957451, Accuracy 78.996%\n",
      "Epoch 15, Batch 428, LR 0.000072 Loss 6.957003, Accuracy 79.005%\n",
      "Epoch 15, Batch 429, LR 0.000072 Loss 6.955562, Accuracy 79.010%\n",
      "Epoch 15, Batch 430, LR 0.000072 Loss 6.955214, Accuracy 79.013%\n",
      "Epoch 15, Batch 431, LR 0.000072 Loss 6.954583, Accuracy 79.022%\n",
      "Epoch 15, Batch 432, LR 0.000072 Loss 6.953291, Accuracy 79.026%\n",
      "Epoch 15, Batch 433, LR 0.000072 Loss 6.955497, Accuracy 79.005%\n",
      "Epoch 15, Batch 434, LR 0.000072 Loss 6.955875, Accuracy 79.003%\n",
      "Epoch 15, Batch 435, LR 0.000072 Loss 6.957910, Accuracy 78.982%\n",
      "Epoch 15, Batch 436, LR 0.000072 Loss 6.957162, Accuracy 78.990%\n",
      "Epoch 15, Batch 437, LR 0.000072 Loss 6.959164, Accuracy 78.988%\n",
      "Epoch 15, Batch 438, LR 0.000072 Loss 6.959406, Accuracy 78.987%\n",
      "Epoch 15, Batch 439, LR 0.000072 Loss 6.959509, Accuracy 78.986%\n",
      "Epoch 15, Batch 440, LR 0.000072 Loss 6.959175, Accuracy 78.983%\n",
      "Epoch 15, Batch 441, LR 0.000072 Loss 6.957699, Accuracy 78.997%\n",
      "Epoch 15, Batch 442, LR 0.000072 Loss 6.955883, Accuracy 79.005%\n",
      "Epoch 15, Batch 443, LR 0.000072 Loss 6.955390, Accuracy 79.010%\n",
      "Epoch 15, Batch 444, LR 0.000072 Loss 6.954539, Accuracy 79.007%\n",
      "Epoch 15, Batch 445, LR 0.000072 Loss 6.955085, Accuracy 79.012%\n",
      "Epoch 15, Batch 446, LR 0.000072 Loss 6.954271, Accuracy 79.018%\n",
      "Epoch 15, Batch 447, LR 0.000072 Loss 6.954503, Accuracy 79.011%\n",
      "Epoch 15, Batch 448, LR 0.000072 Loss 6.955481, Accuracy 79.009%\n",
      "Epoch 15, Batch 449, LR 0.000072 Loss 6.955492, Accuracy 79.014%\n",
      "Epoch 15, Batch 450, LR 0.000072 Loss 6.956895, Accuracy 79.010%\n",
      "Epoch 15, Batch 451, LR 0.000072 Loss 6.956890, Accuracy 79.008%\n",
      "Epoch 15, Batch 452, LR 0.000072 Loss 6.956475, Accuracy 79.013%\n",
      "Epoch 15, Batch 453, LR 0.000072 Loss 6.955361, Accuracy 79.015%\n",
      "Epoch 15, Batch 454, LR 0.000072 Loss 6.955953, Accuracy 79.015%\n",
      "Epoch 15, Batch 455, LR 0.000072 Loss 6.955176, Accuracy 79.014%\n",
      "Epoch 15, Batch 456, LR 0.000072 Loss 6.954590, Accuracy 79.021%\n",
      "Epoch 15, Batch 457, LR 0.000072 Loss 6.955175, Accuracy 79.017%\n",
      "Epoch 15, Batch 458, LR 0.000072 Loss 6.954879, Accuracy 79.014%\n",
      "Epoch 15, Batch 459, LR 0.000072 Loss 6.955061, Accuracy 79.013%\n",
      "Epoch 15, Batch 460, LR 0.000072 Loss 6.955017, Accuracy 79.015%\n",
      "Epoch 15, Batch 461, LR 0.000072 Loss 6.954208, Accuracy 79.016%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 462, LR 0.000072 Loss 6.953861, Accuracy 79.009%\n",
      "Epoch 15, Batch 463, LR 0.000072 Loss 6.954771, Accuracy 79.007%\n",
      "Epoch 15, Batch 464, LR 0.000072 Loss 6.957981, Accuracy 78.982%\n",
      "Epoch 15, Batch 465, LR 0.000072 Loss 6.955397, Accuracy 78.992%\n",
      "Epoch 15, Batch 466, LR 0.000072 Loss 6.957031, Accuracy 78.980%\n",
      "Epoch 15, Batch 467, LR 0.000072 Loss 6.956459, Accuracy 78.990%\n",
      "Epoch 15, Batch 468, LR 0.000072 Loss 6.955699, Accuracy 78.988%\n",
      "Epoch 15, Batch 469, LR 0.000072 Loss 6.957536, Accuracy 78.975%\n",
      "Epoch 15, Batch 470, LR 0.000072 Loss 6.957443, Accuracy 78.983%\n",
      "Epoch 15, Batch 471, LR 0.000072 Loss 6.959296, Accuracy 78.964%\n",
      "Epoch 15, Batch 472, LR 0.000072 Loss 6.959182, Accuracy 78.964%\n",
      "Epoch 15, Batch 473, LR 0.000072 Loss 6.958596, Accuracy 78.964%\n",
      "Epoch 15, Batch 474, LR 0.000072 Loss 6.959225, Accuracy 78.962%\n",
      "Epoch 15, Batch 475, LR 0.000072 Loss 6.958253, Accuracy 78.972%\n",
      "Epoch 15, Batch 476, LR 0.000072 Loss 6.958870, Accuracy 78.967%\n",
      "Epoch 15, Batch 477, LR 0.000072 Loss 6.959758, Accuracy 78.960%\n",
      "Epoch 15, Batch 478, LR 0.000072 Loss 6.962135, Accuracy 78.941%\n",
      "Epoch 15, Batch 479, LR 0.000072 Loss 6.961688, Accuracy 78.936%\n",
      "Epoch 15, Batch 480, LR 0.000072 Loss 6.961078, Accuracy 78.937%\n",
      "Epoch 15, Batch 481, LR 0.000072 Loss 6.962109, Accuracy 78.944%\n",
      "Epoch 15, Batch 482, LR 0.000072 Loss 6.962578, Accuracy 78.940%\n",
      "Epoch 15, Batch 483, LR 0.000072 Loss 6.963785, Accuracy 78.929%\n",
      "Epoch 15, Batch 484, LR 0.000072 Loss 6.963309, Accuracy 78.927%\n",
      "Epoch 15, Batch 485, LR 0.000072 Loss 6.961840, Accuracy 78.934%\n",
      "Epoch 15, Batch 486, LR 0.000072 Loss 6.965078, Accuracy 78.919%\n",
      "Epoch 15, Batch 487, LR 0.000072 Loss 6.965064, Accuracy 78.913%\n",
      "Epoch 15, Batch 488, LR 0.000072 Loss 6.964598, Accuracy 78.924%\n",
      "Epoch 15, Batch 489, LR 0.000072 Loss 6.964591, Accuracy 78.917%\n",
      "Epoch 15, Batch 490, LR 0.000072 Loss 6.963730, Accuracy 78.919%\n",
      "Epoch 15, Batch 491, LR 0.000072 Loss 6.963625, Accuracy 78.927%\n",
      "Epoch 15, Batch 492, LR 0.000072 Loss 6.963625, Accuracy 78.922%\n",
      "Epoch 15, Batch 493, LR 0.000072 Loss 6.963623, Accuracy 78.924%\n",
      "Epoch 15, Batch 494, LR 0.000072 Loss 6.966821, Accuracy 78.909%\n",
      "Epoch 15, Batch 495, LR 0.000072 Loss 6.966382, Accuracy 78.909%\n",
      "Epoch 15, Batch 496, LR 0.000072 Loss 6.966095, Accuracy 78.914%\n",
      "Epoch 15, Batch 497, LR 0.000072 Loss 6.965224, Accuracy 78.917%\n",
      "Epoch 15, Batch 498, LR 0.000072 Loss 6.964522, Accuracy 78.924%\n",
      "Epoch 15, Batch 499, LR 0.000072 Loss 6.963748, Accuracy 78.931%\n",
      "Epoch 15, Batch 500, LR 0.000072 Loss 6.963003, Accuracy 78.931%\n",
      "Epoch 15, Batch 501, LR 0.000072 Loss 6.960414, Accuracy 78.953%\n",
      "Epoch 15, Batch 502, LR 0.000072 Loss 6.963348, Accuracy 78.940%\n",
      "Epoch 15, Batch 503, LR 0.000072 Loss 6.963438, Accuracy 78.939%\n",
      "Epoch 15, Batch 504, LR 0.000072 Loss 6.963122, Accuracy 78.942%\n",
      "Epoch 15, Batch 505, LR 0.000072 Loss 6.962389, Accuracy 78.943%\n",
      "Epoch 15, Batch 506, LR 0.000072 Loss 6.963033, Accuracy 78.945%\n",
      "Epoch 15, Batch 507, LR 0.000072 Loss 6.963126, Accuracy 78.952%\n",
      "Epoch 15, Batch 508, LR 0.000072 Loss 6.962663, Accuracy 78.959%\n",
      "Epoch 15, Batch 509, LR 0.000072 Loss 6.962684, Accuracy 78.952%\n",
      "Epoch 15, Batch 510, LR 0.000072 Loss 6.962736, Accuracy 78.949%\n",
      "Epoch 15, Batch 511, LR 0.000072 Loss 6.963248, Accuracy 78.946%\n",
      "Epoch 15, Batch 512, LR 0.000072 Loss 6.964261, Accuracy 78.949%\n",
      "Epoch 15, Batch 513, LR 0.000072 Loss 6.964618, Accuracy 78.940%\n",
      "Epoch 15, Batch 514, LR 0.000072 Loss 6.964925, Accuracy 78.940%\n",
      "Epoch 15, Batch 515, LR 0.000072 Loss 6.964819, Accuracy 78.943%\n",
      "Epoch 15, Batch 516, LR 0.000072 Loss 6.965377, Accuracy 78.940%\n",
      "Epoch 15, Batch 517, LR 0.000072 Loss 6.964390, Accuracy 78.943%\n",
      "Epoch 15, Batch 518, LR 0.000072 Loss 6.963390, Accuracy 78.938%\n",
      "Epoch 15, Batch 519, LR 0.000072 Loss 6.962197, Accuracy 78.953%\n",
      "Epoch 15, Batch 520, LR 0.000072 Loss 6.963824, Accuracy 78.951%\n",
      "Epoch 15, Batch 521, LR 0.000072 Loss 6.964346, Accuracy 78.944%\n",
      "Epoch 15, Batch 522, LR 0.000072 Loss 6.964641, Accuracy 78.941%\n",
      "Epoch 15, Batch 523, LR 0.000072 Loss 6.964669, Accuracy 78.938%\n",
      "Epoch 15, Batch 524, LR 0.000072 Loss 6.962917, Accuracy 78.945%\n",
      "Epoch 15, Batch 525, LR 0.000072 Loss 6.963864, Accuracy 78.940%\n",
      "Epoch 15, Batch 526, LR 0.000072 Loss 6.963104, Accuracy 78.946%\n",
      "Epoch 15, Batch 527, LR 0.000072 Loss 6.964059, Accuracy 78.940%\n",
      "Epoch 15, Batch 528, LR 0.000072 Loss 6.963683, Accuracy 78.948%\n",
      "Epoch 15, Batch 529, LR 0.000072 Loss 6.964825, Accuracy 78.946%\n",
      "Epoch 15, Batch 530, LR 0.000072 Loss 6.964000, Accuracy 78.946%\n",
      "Epoch 15, Batch 531, LR 0.000072 Loss 6.964079, Accuracy 78.953%\n",
      "Epoch 15, Batch 532, LR 0.000072 Loss 6.963266, Accuracy 78.952%\n",
      "Epoch 15, Batch 533, LR 0.000072 Loss 6.963714, Accuracy 78.955%\n",
      "Epoch 15, Batch 534, LR 0.000072 Loss 6.963769, Accuracy 78.950%\n",
      "Epoch 15, Batch 535, LR 0.000072 Loss 6.964518, Accuracy 78.944%\n",
      "Epoch 15, Batch 536, LR 0.000072 Loss 6.963614, Accuracy 78.950%\n",
      "Epoch 15, Batch 537, LR 0.000072 Loss 6.965246, Accuracy 78.932%\n",
      "Epoch 15, Batch 538, LR 0.000072 Loss 6.966455, Accuracy 78.921%\n",
      "Epoch 15, Batch 539, LR 0.000072 Loss 6.967097, Accuracy 78.913%\n",
      "Epoch 15, Batch 540, LR 0.000072 Loss 6.966753, Accuracy 78.909%\n",
      "Epoch 15, Batch 541, LR 0.000072 Loss 6.967243, Accuracy 78.905%\n",
      "Epoch 15, Batch 542, LR 0.000072 Loss 6.966558, Accuracy 78.905%\n",
      "Epoch 15, Batch 543, LR 0.000072 Loss 6.966085, Accuracy 78.909%\n",
      "Epoch 15, Batch 544, LR 0.000072 Loss 6.966670, Accuracy 78.902%\n",
      "Epoch 15, Batch 545, LR 0.000072 Loss 6.967065, Accuracy 78.911%\n",
      "Epoch 15, Batch 546, LR 0.000072 Loss 6.967131, Accuracy 78.916%\n",
      "Epoch 15, Batch 547, LR 0.000072 Loss 6.968246, Accuracy 78.906%\n",
      "Epoch 15, Batch 548, LR 0.000072 Loss 6.967170, Accuracy 78.913%\n",
      "Epoch 15, Batch 549, LR 0.000072 Loss 6.966523, Accuracy 78.916%\n",
      "Epoch 15, Batch 550, LR 0.000072 Loss 6.968131, Accuracy 78.908%\n",
      "Epoch 15, Batch 551, LR 0.000072 Loss 6.966803, Accuracy 78.905%\n",
      "Epoch 15, Batch 552, LR 0.000072 Loss 6.967859, Accuracy 78.902%\n",
      "Epoch 15, Batch 553, LR 0.000072 Loss 6.967990, Accuracy 78.899%\n",
      "Epoch 15, Batch 554, LR 0.000072 Loss 6.967813, Accuracy 78.902%\n",
      "Epoch 15, Batch 555, LR 0.000072 Loss 6.969796, Accuracy 78.885%\n",
      "Epoch 15, Batch 556, LR 0.000072 Loss 6.971867, Accuracy 78.873%\n",
      "Epoch 15, Batch 557, LR 0.000072 Loss 6.970896, Accuracy 78.878%\n",
      "Epoch 15, Batch 558, LR 0.000072 Loss 6.970441, Accuracy 78.878%\n",
      "Epoch 15, Batch 559, LR 0.000072 Loss 6.971055, Accuracy 78.878%\n",
      "Epoch 15, Batch 560, LR 0.000072 Loss 6.971870, Accuracy 78.874%\n",
      "Epoch 15, Batch 561, LR 0.000072 Loss 6.972968, Accuracy 78.867%\n",
      "Epoch 15, Batch 562, LR 0.000072 Loss 6.971941, Accuracy 78.873%\n",
      "Epoch 15, Batch 563, LR 0.000072 Loss 6.972153, Accuracy 78.876%\n",
      "Epoch 15, Batch 564, LR 0.000072 Loss 6.972063, Accuracy 78.872%\n",
      "Epoch 15, Batch 565, LR 0.000072 Loss 6.971355, Accuracy 78.876%\n",
      "Epoch 15, Batch 566, LR 0.000072 Loss 6.970860, Accuracy 78.876%\n",
      "Epoch 15, Batch 567, LR 0.000072 Loss 6.971079, Accuracy 78.877%\n",
      "Epoch 15, Batch 568, LR 0.000072 Loss 6.972372, Accuracy 78.868%\n",
      "Epoch 15, Batch 569, LR 0.000072 Loss 6.972107, Accuracy 78.871%\n",
      "Epoch 15, Batch 570, LR 0.000072 Loss 6.972141, Accuracy 78.867%\n",
      "Epoch 15, Batch 571, LR 0.000072 Loss 6.971287, Accuracy 78.869%\n",
      "Epoch 15, Batch 572, LR 0.000072 Loss 6.972818, Accuracy 78.861%\n",
      "Epoch 15, Batch 573, LR 0.000072 Loss 6.973354, Accuracy 78.856%\n",
      "Epoch 15, Batch 574, LR 0.000072 Loss 6.973622, Accuracy 78.853%\n",
      "Epoch 15, Batch 575, LR 0.000072 Loss 6.974470, Accuracy 78.849%\n",
      "Epoch 15, Batch 576, LR 0.000072 Loss 6.973482, Accuracy 78.860%\n",
      "Epoch 15, Batch 577, LR 0.000072 Loss 6.973356, Accuracy 78.862%\n",
      "Epoch 15, Batch 578, LR 0.000072 Loss 6.973328, Accuracy 78.856%\n",
      "Epoch 15, Batch 579, LR 0.000072 Loss 6.972777, Accuracy 78.856%\n",
      "Epoch 15, Batch 580, LR 0.000072 Loss 6.972688, Accuracy 78.851%\n",
      "Epoch 15, Batch 581, LR 0.000072 Loss 6.972288, Accuracy 78.852%\n",
      "Epoch 15, Batch 582, LR 0.000072 Loss 6.971172, Accuracy 78.855%\n",
      "Epoch 15, Batch 583, LR 0.000072 Loss 6.971153, Accuracy 78.855%\n",
      "Epoch 15, Batch 584, LR 0.000072 Loss 6.970602, Accuracy 78.859%\n",
      "Epoch 15, Batch 585, LR 0.000072 Loss 6.970094, Accuracy 78.866%\n",
      "Epoch 15, Batch 586, LR 0.000072 Loss 6.971257, Accuracy 78.858%\n",
      "Epoch 15, Batch 587, LR 0.000072 Loss 6.969654, Accuracy 78.864%\n",
      "Epoch 15, Batch 588, LR 0.000072 Loss 6.969152, Accuracy 78.866%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 589, LR 0.000072 Loss 6.967840, Accuracy 78.874%\n",
      "Epoch 15, Batch 590, LR 0.000072 Loss 6.968475, Accuracy 78.865%\n",
      "Epoch 15, Batch 591, LR 0.000072 Loss 6.967666, Accuracy 78.877%\n",
      "Epoch 15, Batch 592, LR 0.000072 Loss 6.969186, Accuracy 78.872%\n",
      "Epoch 15, Batch 593, LR 0.000072 Loss 6.969536, Accuracy 78.872%\n",
      "Epoch 15, Batch 594, LR 0.000072 Loss 6.967858, Accuracy 78.887%\n",
      "Epoch 15, Batch 595, LR 0.000072 Loss 6.969536, Accuracy 78.881%\n",
      "Epoch 15, Batch 596, LR 0.000072 Loss 6.968631, Accuracy 78.888%\n",
      "Epoch 15, Batch 597, LR 0.000072 Loss 6.968847, Accuracy 78.884%\n",
      "Epoch 15, Batch 598, LR 0.000072 Loss 6.968799, Accuracy 78.884%\n",
      "Epoch 15, Batch 599, LR 0.000072 Loss 6.970220, Accuracy 78.881%\n",
      "Epoch 15, Batch 600, LR 0.000072 Loss 6.970251, Accuracy 78.885%\n",
      "Epoch 15, Batch 601, LR 0.000072 Loss 6.971070, Accuracy 78.878%\n",
      "Epoch 15, Batch 602, LR 0.000072 Loss 6.971858, Accuracy 78.876%\n",
      "Epoch 15, Batch 603, LR 0.000072 Loss 6.972238, Accuracy 78.875%\n",
      "Epoch 15, Batch 604, LR 0.000072 Loss 6.972366, Accuracy 78.878%\n",
      "Epoch 15, Batch 605, LR 0.000072 Loss 6.971604, Accuracy 78.879%\n",
      "Epoch 15, Batch 606, LR 0.000072 Loss 6.972105, Accuracy 78.877%\n",
      "Epoch 15, Batch 607, LR 0.000072 Loss 6.973248, Accuracy 78.871%\n",
      "Epoch 15, Batch 608, LR 0.000072 Loss 6.972441, Accuracy 78.878%\n",
      "Epoch 15, Batch 609, LR 0.000072 Loss 6.972132, Accuracy 78.884%\n",
      "Epoch 15, Batch 610, LR 0.000072 Loss 6.972725, Accuracy 78.879%\n",
      "Epoch 15, Batch 611, LR 0.000072 Loss 6.971697, Accuracy 78.887%\n",
      "Epoch 15, Batch 612, LR 0.000072 Loss 6.972057, Accuracy 78.887%\n",
      "Epoch 15, Batch 613, LR 0.000072 Loss 6.971778, Accuracy 78.886%\n",
      "Epoch 15, Batch 614, LR 0.000072 Loss 6.972489, Accuracy 78.883%\n",
      "Epoch 15, Batch 615, LR 0.000072 Loss 6.972466, Accuracy 78.887%\n",
      "Epoch 15, Batch 616, LR 0.000072 Loss 6.970802, Accuracy 78.901%\n",
      "Epoch 15, Batch 617, LR 0.000072 Loss 6.969827, Accuracy 78.904%\n",
      "Epoch 15, Batch 618, LR 0.000072 Loss 6.969526, Accuracy 78.902%\n",
      "Epoch 15, Batch 619, LR 0.000072 Loss 6.969960, Accuracy 78.901%\n",
      "Epoch 15, Batch 620, LR 0.000072 Loss 6.969880, Accuracy 78.902%\n",
      "Epoch 15, Batch 621, LR 0.000072 Loss 6.969511, Accuracy 78.906%\n",
      "Epoch 15, Batch 622, LR 0.000072 Loss 6.971472, Accuracy 78.894%\n",
      "Epoch 15, Batch 623, LR 0.000072 Loss 6.972029, Accuracy 78.892%\n",
      "Epoch 15, Batch 624, LR 0.000072 Loss 6.973106, Accuracy 78.885%\n",
      "Epoch 15, Batch 625, LR 0.000072 Loss 6.972723, Accuracy 78.885%\n",
      "Epoch 15, Batch 626, LR 0.000072 Loss 6.972808, Accuracy 78.881%\n",
      "Epoch 15, Batch 627, LR 0.000072 Loss 6.973848, Accuracy 78.878%\n",
      "Epoch 15, Batch 628, LR 0.000072 Loss 6.974681, Accuracy 78.871%\n",
      "Epoch 15, Batch 629, LR 0.000072 Loss 6.974645, Accuracy 78.879%\n",
      "Epoch 15, Batch 630, LR 0.000072 Loss 6.972343, Accuracy 78.879%\n",
      "Epoch 15, Batch 631, LR 0.000072 Loss 6.973682, Accuracy 78.869%\n",
      "Epoch 15, Batch 632, LR 0.000072 Loss 6.974041, Accuracy 78.863%\n",
      "Epoch 15, Batch 633, LR 0.000072 Loss 6.973276, Accuracy 78.866%\n",
      "Epoch 15, Batch 634, LR 0.000072 Loss 6.972684, Accuracy 78.869%\n",
      "Epoch 15, Batch 635, LR 0.000072 Loss 6.974966, Accuracy 78.863%\n",
      "Epoch 15, Batch 636, LR 0.000072 Loss 6.975103, Accuracy 78.866%\n",
      "Epoch 15, Batch 637, LR 0.000072 Loss 6.975355, Accuracy 78.872%\n",
      "Epoch 15, Batch 638, LR 0.000072 Loss 6.975119, Accuracy 78.876%\n",
      "Epoch 15, Batch 639, LR 0.000072 Loss 6.974186, Accuracy 78.883%\n",
      "Epoch 15, Batch 640, LR 0.000072 Loss 6.974244, Accuracy 78.889%\n",
      "Epoch 15, Batch 641, LR 0.000072 Loss 6.974695, Accuracy 78.879%\n",
      "Epoch 15, Batch 642, LR 0.000072 Loss 6.974498, Accuracy 78.879%\n",
      "Epoch 15, Batch 643, LR 0.000072 Loss 6.975468, Accuracy 78.877%\n",
      "Epoch 15, Batch 644, LR 0.000072 Loss 6.975148, Accuracy 78.884%\n",
      "Epoch 15, Batch 645, LR 0.000072 Loss 6.975646, Accuracy 78.889%\n",
      "Epoch 15, Batch 646, LR 0.000072 Loss 6.974401, Accuracy 78.899%\n",
      "Epoch 15, Batch 647, LR 0.000072 Loss 6.974522, Accuracy 78.903%\n",
      "Epoch 15, Batch 648, LR 0.000072 Loss 6.975834, Accuracy 78.888%\n",
      "Epoch 15, Batch 649, LR 0.000072 Loss 6.976185, Accuracy 78.886%\n",
      "Epoch 15, Batch 650, LR 0.000072 Loss 6.976655, Accuracy 78.880%\n",
      "Epoch 15, Batch 651, LR 0.000072 Loss 6.975451, Accuracy 78.888%\n",
      "Epoch 15, Batch 652, LR 0.000072 Loss 6.975290, Accuracy 78.895%\n",
      "Epoch 15, Batch 653, LR 0.000071 Loss 6.974927, Accuracy 78.895%\n",
      "Epoch 15, Batch 654, LR 0.000071 Loss 6.974757, Accuracy 78.898%\n",
      "Epoch 15, Batch 655, LR 0.000071 Loss 6.974831, Accuracy 78.903%\n",
      "Epoch 15, Batch 656, LR 0.000071 Loss 6.973899, Accuracy 78.909%\n",
      "Epoch 15, Batch 657, LR 0.000071 Loss 6.973540, Accuracy 78.909%\n",
      "Epoch 15, Batch 658, LR 0.000071 Loss 6.973256, Accuracy 78.909%\n",
      "Epoch 15, Batch 659, LR 0.000071 Loss 6.974672, Accuracy 78.903%\n",
      "Epoch 15, Batch 660, LR 0.000071 Loss 6.974727, Accuracy 78.899%\n",
      "Epoch 15, Batch 661, LR 0.000071 Loss 6.974547, Accuracy 78.904%\n",
      "Epoch 15, Batch 662, LR 0.000071 Loss 6.974220, Accuracy 78.904%\n",
      "Epoch 15, Batch 663, LR 0.000071 Loss 6.973583, Accuracy 78.903%\n",
      "Epoch 15, Batch 664, LR 0.000071 Loss 6.972524, Accuracy 78.907%\n",
      "Epoch 15, Batch 665, LR 0.000071 Loss 6.972973, Accuracy 78.902%\n",
      "Epoch 15, Batch 666, LR 0.000071 Loss 6.974295, Accuracy 78.893%\n",
      "Epoch 15, Batch 667, LR 0.000071 Loss 6.974426, Accuracy 78.892%\n",
      "Epoch 15, Batch 668, LR 0.000071 Loss 6.974793, Accuracy 78.895%\n",
      "Epoch 15, Batch 669, LR 0.000071 Loss 6.974046, Accuracy 78.896%\n",
      "Epoch 15, Batch 670, LR 0.000071 Loss 6.973407, Accuracy 78.902%\n",
      "Epoch 15, Batch 671, LR 0.000071 Loss 6.972971, Accuracy 78.903%\n",
      "Epoch 15, Batch 672, LR 0.000071 Loss 6.973328, Accuracy 78.898%\n",
      "Epoch 15, Batch 673, LR 0.000071 Loss 6.973200, Accuracy 78.896%\n",
      "Epoch 15, Batch 674, LR 0.000071 Loss 6.973666, Accuracy 78.890%\n",
      "Epoch 15, Batch 675, LR 0.000071 Loss 6.974094, Accuracy 78.892%\n",
      "Epoch 15, Batch 676, LR 0.000071 Loss 6.973674, Accuracy 78.895%\n",
      "Epoch 15, Batch 677, LR 0.000071 Loss 6.972704, Accuracy 78.904%\n",
      "Epoch 15, Batch 678, LR 0.000071 Loss 6.971712, Accuracy 78.909%\n",
      "Epoch 15, Batch 679, LR 0.000071 Loss 6.972168, Accuracy 78.910%\n",
      "Epoch 15, Batch 680, LR 0.000071 Loss 6.972090, Accuracy 78.912%\n",
      "Epoch 15, Batch 681, LR 0.000071 Loss 6.971538, Accuracy 78.918%\n",
      "Epoch 15, Batch 682, LR 0.000071 Loss 6.970480, Accuracy 78.923%\n",
      "Epoch 15, Batch 683, LR 0.000071 Loss 6.968746, Accuracy 78.927%\n",
      "Epoch 15, Batch 684, LR 0.000071 Loss 6.967339, Accuracy 78.930%\n",
      "Epoch 15, Batch 685, LR 0.000071 Loss 6.966452, Accuracy 78.931%\n",
      "Epoch 15, Batch 686, LR 0.000071 Loss 6.965975, Accuracy 78.929%\n",
      "Epoch 15, Batch 687, LR 0.000071 Loss 6.966151, Accuracy 78.929%\n",
      "Epoch 15, Batch 688, LR 0.000071 Loss 6.965979, Accuracy 78.927%\n",
      "Epoch 15, Batch 689, LR 0.000071 Loss 6.966800, Accuracy 78.924%\n",
      "Epoch 15, Batch 690, LR 0.000071 Loss 6.967666, Accuracy 78.924%\n",
      "Epoch 15, Batch 691, LR 0.000071 Loss 6.967744, Accuracy 78.929%\n",
      "Epoch 15, Batch 692, LR 0.000071 Loss 6.967775, Accuracy 78.931%\n",
      "Epoch 15, Batch 693, LR 0.000071 Loss 6.967085, Accuracy 78.936%\n",
      "Epoch 15, Batch 694, LR 0.000071 Loss 6.966671, Accuracy 78.939%\n",
      "Epoch 15, Batch 695, LR 0.000071 Loss 6.966633, Accuracy 78.937%\n",
      "Epoch 15, Batch 696, LR 0.000071 Loss 6.966616, Accuracy 78.938%\n",
      "Epoch 15, Batch 697, LR 0.000071 Loss 6.966179, Accuracy 78.945%\n",
      "Epoch 15, Batch 698, LR 0.000071 Loss 6.965081, Accuracy 78.949%\n",
      "Epoch 15, Batch 699, LR 0.000071 Loss 6.965633, Accuracy 78.951%\n",
      "Epoch 15, Batch 700, LR 0.000071 Loss 6.965969, Accuracy 78.949%\n",
      "Epoch 15, Batch 701, LR 0.000071 Loss 6.964407, Accuracy 78.956%\n",
      "Epoch 15, Batch 702, LR 0.000071 Loss 6.963379, Accuracy 78.962%\n",
      "Epoch 15, Batch 703, LR 0.000071 Loss 6.962994, Accuracy 78.968%\n",
      "Epoch 15, Batch 704, LR 0.000071 Loss 6.961927, Accuracy 78.977%\n",
      "Epoch 15, Batch 705, LR 0.000071 Loss 6.961590, Accuracy 78.980%\n",
      "Epoch 15, Batch 706, LR 0.000071 Loss 6.961714, Accuracy 78.980%\n",
      "Epoch 15, Batch 707, LR 0.000071 Loss 6.961538, Accuracy 78.982%\n",
      "Epoch 15, Batch 708, LR 0.000071 Loss 6.962041, Accuracy 78.983%\n",
      "Epoch 15, Batch 709, LR 0.000071 Loss 6.961027, Accuracy 78.991%\n",
      "Epoch 15, Batch 710, LR 0.000071 Loss 6.960723, Accuracy 78.991%\n",
      "Epoch 15, Batch 711, LR 0.000071 Loss 6.960361, Accuracy 78.993%\n",
      "Epoch 15, Batch 712, LR 0.000071 Loss 6.960915, Accuracy 78.985%\n",
      "Epoch 15, Batch 713, LR 0.000071 Loss 6.961852, Accuracy 78.975%\n",
      "Epoch 15, Batch 714, LR 0.000071 Loss 6.961346, Accuracy 78.971%\n",
      "Epoch 15, Batch 715, LR 0.000071 Loss 6.961897, Accuracy 78.967%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 716, LR 0.000071 Loss 6.962113, Accuracy 78.971%\n",
      "Epoch 15, Batch 717, LR 0.000071 Loss 6.960489, Accuracy 78.983%\n",
      "Epoch 15, Batch 718, LR 0.000071 Loss 6.960004, Accuracy 78.988%\n",
      "Epoch 15, Batch 719, LR 0.000071 Loss 6.959123, Accuracy 78.991%\n",
      "Epoch 15, Batch 720, LR 0.000071 Loss 6.958885, Accuracy 78.990%\n",
      "Epoch 15, Batch 721, LR 0.000071 Loss 6.959103, Accuracy 78.988%\n",
      "Epoch 15, Batch 722, LR 0.000071 Loss 6.959027, Accuracy 78.986%\n",
      "Epoch 15, Batch 723, LR 0.000071 Loss 6.958551, Accuracy 78.994%\n",
      "Epoch 15, Batch 724, LR 0.000071 Loss 6.959150, Accuracy 78.991%\n",
      "Epoch 15, Batch 725, LR 0.000071 Loss 6.959350, Accuracy 78.989%\n",
      "Epoch 15, Batch 726, LR 0.000071 Loss 6.958824, Accuracy 78.993%\n",
      "Epoch 15, Batch 727, LR 0.000071 Loss 6.958954, Accuracy 78.992%\n",
      "Epoch 15, Batch 728, LR 0.000071 Loss 6.959156, Accuracy 78.989%\n",
      "Epoch 15, Batch 729, LR 0.000071 Loss 6.958431, Accuracy 78.991%\n",
      "Epoch 15, Batch 730, LR 0.000071 Loss 6.958188, Accuracy 78.991%\n",
      "Epoch 15, Batch 731, LR 0.000071 Loss 6.958898, Accuracy 78.987%\n",
      "Epoch 15, Batch 732, LR 0.000071 Loss 6.958597, Accuracy 78.989%\n",
      "Epoch 15, Batch 733, LR 0.000071 Loss 6.957920, Accuracy 78.995%\n",
      "Epoch 15, Batch 734, LR 0.000071 Loss 6.958253, Accuracy 78.990%\n",
      "Epoch 15, Batch 735, LR 0.000071 Loss 6.958231, Accuracy 78.992%\n",
      "Epoch 15, Batch 736, LR 0.000071 Loss 6.958298, Accuracy 78.995%\n",
      "Epoch 15, Batch 737, LR 0.000071 Loss 6.958521, Accuracy 78.994%\n",
      "Epoch 15, Batch 738, LR 0.000071 Loss 6.959021, Accuracy 78.988%\n",
      "Epoch 15, Batch 739, LR 0.000071 Loss 6.959059, Accuracy 78.982%\n",
      "Epoch 15, Batch 740, LR 0.000071 Loss 6.958892, Accuracy 78.984%\n",
      "Epoch 15, Batch 741, LR 0.000071 Loss 6.959741, Accuracy 78.983%\n",
      "Epoch 15, Batch 742, LR 0.000071 Loss 6.959385, Accuracy 78.981%\n",
      "Epoch 15, Batch 743, LR 0.000071 Loss 6.959244, Accuracy 78.980%\n",
      "Epoch 15, Batch 744, LR 0.000071 Loss 6.959274, Accuracy 78.982%\n",
      "Epoch 15, Batch 745, LR 0.000071 Loss 6.959472, Accuracy 78.980%\n",
      "Epoch 15, Batch 746, LR 0.000071 Loss 6.960269, Accuracy 78.976%\n",
      "Epoch 15, Batch 747, LR 0.000071 Loss 6.959746, Accuracy 78.984%\n",
      "Epoch 15, Batch 748, LR 0.000071 Loss 6.959487, Accuracy 78.982%\n",
      "Epoch 15, Batch 749, LR 0.000071 Loss 6.959441, Accuracy 78.979%\n",
      "Epoch 15, Batch 750, LR 0.000071 Loss 6.958606, Accuracy 78.986%\n",
      "Epoch 15, Batch 751, LR 0.000071 Loss 6.958740, Accuracy 78.987%\n",
      "Epoch 15, Batch 752, LR 0.000071 Loss 6.957693, Accuracy 78.991%\n",
      "Epoch 15, Batch 753, LR 0.000071 Loss 6.956956, Accuracy 78.995%\n",
      "Epoch 15, Batch 754, LR 0.000071 Loss 6.956043, Accuracy 79.001%\n",
      "Epoch 15, Batch 755, LR 0.000071 Loss 6.954606, Accuracy 79.009%\n",
      "Epoch 15, Batch 756, LR 0.000071 Loss 6.955256, Accuracy 79.004%\n",
      "Epoch 15, Batch 757, LR 0.000071 Loss 6.955985, Accuracy 78.997%\n",
      "Epoch 15, Batch 758, LR 0.000071 Loss 6.955812, Accuracy 78.994%\n",
      "Epoch 15, Batch 759, LR 0.000071 Loss 6.957786, Accuracy 78.984%\n",
      "Epoch 15, Batch 760, LR 0.000071 Loss 6.958413, Accuracy 78.990%\n",
      "Epoch 15, Batch 761, LR 0.000071 Loss 6.958958, Accuracy 78.987%\n",
      "Epoch 15, Batch 762, LR 0.000071 Loss 6.959551, Accuracy 78.986%\n",
      "Epoch 15, Batch 763, LR 0.000071 Loss 6.959073, Accuracy 78.990%\n",
      "Epoch 15, Batch 764, LR 0.000071 Loss 6.959155, Accuracy 78.988%\n",
      "Epoch 15, Batch 765, LR 0.000071 Loss 6.958669, Accuracy 78.991%\n",
      "Epoch 15, Batch 766, LR 0.000071 Loss 6.957837, Accuracy 78.994%\n",
      "Epoch 15, Batch 767, LR 0.000071 Loss 6.959203, Accuracy 78.990%\n",
      "Epoch 15, Batch 768, LR 0.000071 Loss 6.959419, Accuracy 78.996%\n",
      "Epoch 15, Batch 769, LR 0.000071 Loss 6.960342, Accuracy 78.993%\n",
      "Epoch 15, Batch 770, LR 0.000071 Loss 6.961440, Accuracy 78.989%\n",
      "Epoch 15, Batch 771, LR 0.000071 Loss 6.961300, Accuracy 78.990%\n",
      "Epoch 15, Batch 772, LR 0.000071 Loss 6.960991, Accuracy 78.991%\n",
      "Epoch 15, Batch 773, LR 0.000071 Loss 6.960102, Accuracy 79.000%\n",
      "Epoch 15, Batch 774, LR 0.000071 Loss 6.960431, Accuracy 78.996%\n",
      "Epoch 15, Batch 775, LR 0.000071 Loss 6.959339, Accuracy 79.002%\n",
      "Epoch 15, Batch 776, LR 0.000071 Loss 6.959734, Accuracy 79.006%\n",
      "Epoch 15, Batch 777, LR 0.000071 Loss 6.959416, Accuracy 79.011%\n",
      "Epoch 15, Batch 778, LR 0.000071 Loss 6.960290, Accuracy 79.004%\n",
      "Epoch 15, Batch 779, LR 0.000071 Loss 6.960590, Accuracy 79.010%\n",
      "Epoch 15, Batch 780, LR 0.000071 Loss 6.960993, Accuracy 79.007%\n",
      "Epoch 15, Batch 781, LR 0.000071 Loss 6.959724, Accuracy 79.012%\n",
      "Epoch 15, Batch 782, LR 0.000071 Loss 6.960056, Accuracy 79.012%\n",
      "Epoch 15, Batch 783, LR 0.000071 Loss 6.960232, Accuracy 79.006%\n",
      "Epoch 15, Batch 784, LR 0.000071 Loss 6.960559, Accuracy 79.004%\n",
      "Epoch 15, Batch 785, LR 0.000071 Loss 6.961025, Accuracy 79.002%\n",
      "Epoch 15, Batch 786, LR 0.000071 Loss 6.962284, Accuracy 78.993%\n",
      "Epoch 15, Batch 787, LR 0.000071 Loss 6.961754, Accuracy 78.994%\n",
      "Epoch 15, Batch 788, LR 0.000071 Loss 6.962615, Accuracy 78.994%\n",
      "Epoch 15, Batch 789, LR 0.000071 Loss 6.962859, Accuracy 78.990%\n",
      "Epoch 15, Batch 790, LR 0.000071 Loss 6.963012, Accuracy 78.993%\n",
      "Epoch 15, Batch 791, LR 0.000071 Loss 6.961799, Accuracy 79.002%\n",
      "Epoch 15, Batch 792, LR 0.000071 Loss 6.963036, Accuracy 78.988%\n",
      "Epoch 15, Batch 793, LR 0.000071 Loss 6.963609, Accuracy 78.984%\n",
      "Epoch 15, Batch 794, LR 0.000071 Loss 6.963249, Accuracy 78.988%\n",
      "Epoch 15, Batch 795, LR 0.000071 Loss 6.962897, Accuracy 78.990%\n",
      "Epoch 15, Batch 796, LR 0.000071 Loss 6.962738, Accuracy 78.992%\n",
      "Epoch 15, Batch 797, LR 0.000071 Loss 6.963552, Accuracy 78.987%\n",
      "Epoch 15, Batch 798, LR 0.000071 Loss 6.963549, Accuracy 78.987%\n",
      "Epoch 15, Batch 799, LR 0.000071 Loss 6.964285, Accuracy 78.979%\n",
      "Epoch 15, Batch 800, LR 0.000071 Loss 6.963452, Accuracy 78.985%\n",
      "Epoch 15, Batch 801, LR 0.000071 Loss 6.962253, Accuracy 78.993%\n",
      "Epoch 15, Batch 802, LR 0.000071 Loss 6.962622, Accuracy 78.991%\n",
      "Epoch 15, Batch 803, LR 0.000071 Loss 6.962666, Accuracy 78.993%\n",
      "Epoch 15, Batch 804, LR 0.000071 Loss 6.963551, Accuracy 78.987%\n",
      "Epoch 15, Batch 805, LR 0.000071 Loss 6.962325, Accuracy 78.998%\n",
      "Epoch 15, Batch 806, LR 0.000071 Loss 6.963296, Accuracy 78.994%\n",
      "Epoch 15, Batch 807, LR 0.000071 Loss 6.962879, Accuracy 78.997%\n",
      "Epoch 15, Batch 808, LR 0.000071 Loss 6.963090, Accuracy 78.996%\n",
      "Epoch 15, Batch 809, LR 0.000071 Loss 6.963112, Accuracy 78.994%\n",
      "Epoch 15, Batch 810, LR 0.000071 Loss 6.962887, Accuracy 78.995%\n",
      "Epoch 15, Batch 811, LR 0.000071 Loss 6.962933, Accuracy 78.996%\n",
      "Epoch 15, Batch 812, LR 0.000071 Loss 6.963470, Accuracy 78.995%\n",
      "Epoch 15, Batch 813, LR 0.000071 Loss 6.962475, Accuracy 79.000%\n",
      "Epoch 15, Batch 814, LR 0.000071 Loss 6.961184, Accuracy 79.006%\n",
      "Epoch 15, Batch 815, LR 0.000071 Loss 6.961751, Accuracy 79.007%\n",
      "Epoch 15, Batch 816, LR 0.000071 Loss 6.960425, Accuracy 79.014%\n",
      "Epoch 15, Batch 817, LR 0.000071 Loss 6.960312, Accuracy 79.013%\n",
      "Epoch 15, Batch 818, LR 0.000071 Loss 6.959536, Accuracy 79.015%\n",
      "Epoch 15, Batch 819, LR 0.000071 Loss 6.959839, Accuracy 79.010%\n",
      "Epoch 15, Batch 820, LR 0.000071 Loss 6.958427, Accuracy 79.019%\n",
      "Epoch 15, Batch 821, LR 0.000071 Loss 6.958829, Accuracy 79.017%\n",
      "Epoch 15, Batch 822, LR 0.000071 Loss 6.958637, Accuracy 79.020%\n",
      "Epoch 15, Batch 823, LR 0.000071 Loss 6.959174, Accuracy 79.010%\n",
      "Epoch 15, Batch 824, LR 0.000071 Loss 6.959491, Accuracy 79.008%\n",
      "Epoch 15, Batch 825, LR 0.000071 Loss 6.958452, Accuracy 79.017%\n",
      "Epoch 15, Batch 826, LR 0.000071 Loss 6.958663, Accuracy 79.014%\n",
      "Epoch 15, Batch 827, LR 0.000071 Loss 6.959244, Accuracy 79.014%\n",
      "Epoch 15, Batch 828, LR 0.000071 Loss 6.958777, Accuracy 79.016%\n",
      "Epoch 15, Batch 829, LR 0.000071 Loss 6.959016, Accuracy 79.012%\n",
      "Epoch 15, Batch 830, LR 0.000071 Loss 6.959142, Accuracy 79.012%\n",
      "Epoch 15, Batch 831, LR 0.000071 Loss 6.959121, Accuracy 79.012%\n",
      "Epoch 15, Batch 832, LR 0.000071 Loss 6.959153, Accuracy 79.012%\n",
      "Epoch 15, Batch 833, LR 0.000071 Loss 6.958307, Accuracy 79.021%\n",
      "Epoch 15, Batch 834, LR 0.000071 Loss 6.958212, Accuracy 79.021%\n",
      "Epoch 15, Batch 835, LR 0.000071 Loss 6.958570, Accuracy 79.017%\n",
      "Epoch 15, Batch 836, LR 0.000071 Loss 6.958440, Accuracy 79.017%\n",
      "Epoch 15, Batch 837, LR 0.000071 Loss 6.958148, Accuracy 79.019%\n",
      "Epoch 15, Batch 838, LR 0.000071 Loss 6.959118, Accuracy 79.012%\n",
      "Epoch 15, Batch 839, LR 0.000071 Loss 6.958226, Accuracy 79.013%\n",
      "Epoch 15, Batch 840, LR 0.000071 Loss 6.957889, Accuracy 79.018%\n",
      "Epoch 15, Batch 841, LR 0.000071 Loss 6.957809, Accuracy 79.016%\n",
      "Epoch 15, Batch 842, LR 0.000071 Loss 6.957134, Accuracy 79.025%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 843, LR 0.000071 Loss 6.956334, Accuracy 79.025%\n",
      "Epoch 15, Batch 844, LR 0.000071 Loss 6.957243, Accuracy 79.015%\n",
      "Epoch 15, Batch 845, LR 0.000071 Loss 6.957807, Accuracy 79.015%\n",
      "Epoch 15, Batch 846, LR 0.000071 Loss 6.957722, Accuracy 79.018%\n",
      "Epoch 15, Batch 847, LR 0.000071 Loss 6.957845, Accuracy 79.021%\n",
      "Epoch 15, Batch 848, LR 0.000071 Loss 6.957934, Accuracy 79.022%\n",
      "Epoch 15, Batch 849, LR 0.000071 Loss 6.957904, Accuracy 79.019%\n",
      "Epoch 15, Batch 850, LR 0.000071 Loss 6.957738, Accuracy 79.025%\n",
      "Epoch 15, Batch 851, LR 0.000071 Loss 6.957596, Accuracy 79.023%\n",
      "Epoch 15, Batch 852, LR 0.000071 Loss 6.957096, Accuracy 79.025%\n",
      "Epoch 15, Batch 853, LR 0.000071 Loss 6.957522, Accuracy 79.021%\n",
      "Epoch 15, Batch 854, LR 0.000071 Loss 6.957578, Accuracy 79.017%\n",
      "Epoch 15, Batch 855, LR 0.000071 Loss 6.956754, Accuracy 79.021%\n",
      "Epoch 15, Batch 856, LR 0.000071 Loss 6.956736, Accuracy 79.026%\n",
      "Epoch 15, Batch 857, LR 0.000071 Loss 6.956228, Accuracy 79.031%\n",
      "Epoch 15, Batch 858, LR 0.000071 Loss 6.956577, Accuracy 79.026%\n",
      "Epoch 15, Batch 859, LR 0.000071 Loss 6.956394, Accuracy 79.025%\n",
      "Epoch 15, Batch 860, LR 0.000071 Loss 6.956833, Accuracy 79.026%\n",
      "Epoch 15, Batch 861, LR 0.000071 Loss 6.956268, Accuracy 79.027%\n",
      "Epoch 15, Batch 862, LR 0.000071 Loss 6.956192, Accuracy 79.031%\n",
      "Epoch 15, Batch 863, LR 0.000071 Loss 6.955029, Accuracy 79.035%\n",
      "Epoch 15, Batch 864, LR 0.000071 Loss 6.954858, Accuracy 79.033%\n",
      "Epoch 15, Batch 865, LR 0.000071 Loss 6.954877, Accuracy 79.034%\n",
      "Epoch 15, Batch 866, LR 0.000071 Loss 6.955052, Accuracy 79.031%\n",
      "Epoch 15, Batch 867, LR 0.000071 Loss 6.954634, Accuracy 79.029%\n",
      "Epoch 15, Batch 868, LR 0.000071 Loss 6.953928, Accuracy 79.033%\n",
      "Epoch 15, Batch 869, LR 0.000071 Loss 6.954154, Accuracy 79.035%\n",
      "Epoch 15, Batch 870, LR 0.000071 Loss 6.954409, Accuracy 79.036%\n",
      "Epoch 15, Batch 871, LR 0.000071 Loss 6.953806, Accuracy 79.040%\n",
      "Epoch 15, Batch 872, LR 0.000071 Loss 6.954761, Accuracy 79.034%\n",
      "Epoch 15, Batch 873, LR 0.000071 Loss 6.954703, Accuracy 79.036%\n",
      "Epoch 15, Batch 874, LR 0.000071 Loss 6.954328, Accuracy 79.039%\n",
      "Epoch 15, Batch 875, LR 0.000071 Loss 6.955027, Accuracy 79.031%\n",
      "Epoch 15, Batch 876, LR 0.000071 Loss 6.954898, Accuracy 79.034%\n",
      "Epoch 15, Batch 877, LR 0.000071 Loss 6.954921, Accuracy 79.033%\n",
      "Epoch 15, Batch 878, LR 0.000071 Loss 6.954868, Accuracy 79.033%\n",
      "Epoch 15, Batch 879, LR 0.000071 Loss 6.954381, Accuracy 79.036%\n",
      "Epoch 15, Batch 880, LR 0.000071 Loss 6.955082, Accuracy 79.035%\n",
      "Epoch 15, Batch 881, LR 0.000071 Loss 6.954474, Accuracy 79.037%\n",
      "Epoch 15, Batch 882, LR 0.000071 Loss 6.954484, Accuracy 79.036%\n",
      "Epoch 15, Batch 883, LR 0.000071 Loss 6.953789, Accuracy 79.042%\n",
      "Epoch 15, Batch 884, LR 0.000071 Loss 6.953373, Accuracy 79.043%\n",
      "Epoch 15, Batch 885, LR 0.000071 Loss 6.954107, Accuracy 79.039%\n",
      "Epoch 15, Batch 886, LR 0.000071 Loss 6.954202, Accuracy 79.036%\n",
      "Epoch 15, Batch 887, LR 0.000071 Loss 6.953442, Accuracy 79.041%\n",
      "Epoch 15, Batch 888, LR 0.000071 Loss 6.953856, Accuracy 79.042%\n",
      "Epoch 15, Batch 889, LR 0.000071 Loss 6.953580, Accuracy 79.045%\n",
      "Epoch 15, Batch 890, LR 0.000071 Loss 6.952747, Accuracy 79.050%\n",
      "Epoch 15, Batch 891, LR 0.000071 Loss 6.954425, Accuracy 79.037%\n",
      "Epoch 15, Batch 892, LR 0.000071 Loss 6.954795, Accuracy 79.031%\n",
      "Epoch 15, Batch 893, LR 0.000071 Loss 6.955304, Accuracy 79.036%\n",
      "Epoch 15, Batch 894, LR 0.000071 Loss 6.955649, Accuracy 79.037%\n",
      "Epoch 15, Batch 895, LR 0.000071 Loss 6.955830, Accuracy 79.039%\n",
      "Epoch 15, Batch 896, LR 0.000071 Loss 6.956424, Accuracy 79.036%\n",
      "Epoch 15, Batch 897, LR 0.000071 Loss 6.956878, Accuracy 79.033%\n",
      "Epoch 15, Batch 898, LR 0.000071 Loss 6.956332, Accuracy 79.041%\n",
      "Epoch 15, Batch 899, LR 0.000071 Loss 6.956058, Accuracy 79.040%\n",
      "Epoch 15, Batch 900, LR 0.000071 Loss 6.955420, Accuracy 79.046%\n",
      "Epoch 15, Batch 901, LR 0.000071 Loss 6.955864, Accuracy 79.042%\n",
      "Epoch 15, Batch 902, LR 0.000071 Loss 6.956355, Accuracy 79.040%\n",
      "Epoch 15, Batch 903, LR 0.000071 Loss 6.956650, Accuracy 79.041%\n",
      "Epoch 15, Batch 904, LR 0.000071 Loss 6.956727, Accuracy 79.038%\n",
      "Epoch 15, Batch 905, LR 0.000071 Loss 6.955778, Accuracy 79.040%\n",
      "Epoch 15, Batch 906, LR 0.000071 Loss 6.955460, Accuracy 79.041%\n",
      "Epoch 15, Batch 907, LR 0.000071 Loss 6.954914, Accuracy 79.045%\n",
      "Epoch 15, Batch 908, LR 0.000071 Loss 6.955639, Accuracy 79.048%\n",
      "Epoch 15, Batch 909, LR 0.000071 Loss 6.955498, Accuracy 79.049%\n",
      "Epoch 15, Batch 910, LR 0.000071 Loss 6.955375, Accuracy 79.052%\n",
      "Epoch 15, Batch 911, LR 0.000071 Loss 6.956222, Accuracy 79.051%\n",
      "Epoch 15, Batch 912, LR 0.000071 Loss 6.956493, Accuracy 79.050%\n",
      "Epoch 15, Batch 913, LR 0.000071 Loss 6.956393, Accuracy 79.055%\n",
      "Epoch 15, Batch 914, LR 0.000071 Loss 6.955671, Accuracy 79.058%\n",
      "Epoch 15, Batch 915, LR 0.000071 Loss 6.955863, Accuracy 79.057%\n",
      "Epoch 15, Batch 916, LR 0.000071 Loss 6.955544, Accuracy 79.061%\n",
      "Epoch 15, Batch 917, LR 0.000071 Loss 6.955266, Accuracy 79.065%\n",
      "Epoch 15, Batch 918, LR 0.000071 Loss 6.955795, Accuracy 79.062%\n",
      "Epoch 15, Batch 919, LR 0.000071 Loss 6.955867, Accuracy 79.064%\n",
      "Epoch 15, Batch 920, LR 0.000071 Loss 6.955634, Accuracy 79.063%\n",
      "Epoch 15, Batch 921, LR 0.000071 Loss 6.955051, Accuracy 79.061%\n",
      "Epoch 15, Batch 922, LR 0.000071 Loss 6.955216, Accuracy 79.055%\n",
      "Epoch 15, Batch 923, LR 0.000071 Loss 6.955476, Accuracy 79.057%\n",
      "Epoch 15, Batch 924, LR 0.000071 Loss 6.955204, Accuracy 79.058%\n",
      "Epoch 15, Batch 925, LR 0.000071 Loss 6.955408, Accuracy 79.059%\n",
      "Epoch 15, Batch 926, LR 0.000071 Loss 6.955849, Accuracy 79.058%\n",
      "Epoch 15, Batch 927, LR 0.000071 Loss 6.956068, Accuracy 79.058%\n",
      "Epoch 15, Batch 928, LR 0.000071 Loss 6.955965, Accuracy 79.055%\n",
      "Epoch 15, Batch 929, LR 0.000071 Loss 6.955592, Accuracy 79.056%\n",
      "Epoch 15, Batch 930, LR 0.000071 Loss 6.955960, Accuracy 79.059%\n",
      "Epoch 15, Batch 931, LR 0.000071 Loss 6.956126, Accuracy 79.060%\n",
      "Epoch 15, Batch 932, LR 0.000071 Loss 6.955529, Accuracy 79.066%\n",
      "Epoch 15, Batch 933, LR 0.000070 Loss 6.955649, Accuracy 79.065%\n",
      "Epoch 15, Batch 934, LR 0.000070 Loss 6.956468, Accuracy 79.064%\n",
      "Epoch 15, Batch 935, LR 0.000070 Loss 6.956418, Accuracy 79.064%\n",
      "Epoch 15, Batch 936, LR 0.000070 Loss 6.957069, Accuracy 79.056%\n",
      "Epoch 15, Batch 937, LR 0.000070 Loss 6.957365, Accuracy 79.050%\n",
      "Epoch 15, Batch 938, LR 0.000070 Loss 6.957213, Accuracy 79.049%\n",
      "Epoch 15, Batch 939, LR 0.000070 Loss 6.956633, Accuracy 79.053%\n",
      "Epoch 15, Batch 940, LR 0.000070 Loss 6.956830, Accuracy 79.048%\n",
      "Epoch 15, Batch 941, LR 0.000070 Loss 6.956452, Accuracy 79.048%\n",
      "Epoch 15, Batch 942, LR 0.000070 Loss 6.955795, Accuracy 79.046%\n",
      "Epoch 15, Batch 943, LR 0.000070 Loss 6.956287, Accuracy 79.044%\n",
      "Epoch 15, Batch 944, LR 0.000070 Loss 6.956487, Accuracy 79.043%\n",
      "Epoch 15, Batch 945, LR 0.000070 Loss 6.957519, Accuracy 79.036%\n",
      "Epoch 15, Batch 946, LR 0.000070 Loss 6.957243, Accuracy 79.039%\n",
      "Epoch 15, Batch 947, LR 0.000070 Loss 6.956947, Accuracy 79.036%\n",
      "Epoch 15, Batch 948, LR 0.000070 Loss 6.956068, Accuracy 79.038%\n",
      "Epoch 15, Batch 949, LR 0.000070 Loss 6.956388, Accuracy 79.039%\n",
      "Epoch 15, Batch 950, LR 0.000070 Loss 6.955666, Accuracy 79.043%\n",
      "Epoch 15, Batch 951, LR 0.000070 Loss 6.955477, Accuracy 79.043%\n",
      "Epoch 15, Batch 952, LR 0.000070 Loss 6.954936, Accuracy 79.049%\n",
      "Epoch 15, Batch 953, LR 0.000070 Loss 6.955162, Accuracy 79.048%\n",
      "Epoch 15, Batch 954, LR 0.000070 Loss 6.955094, Accuracy 79.048%\n",
      "Epoch 15, Batch 955, LR 0.000070 Loss 6.955736, Accuracy 79.042%\n",
      "Epoch 15, Batch 956, LR 0.000070 Loss 6.956332, Accuracy 79.038%\n",
      "Epoch 15, Batch 957, LR 0.000070 Loss 6.956528, Accuracy 79.038%\n",
      "Epoch 15, Batch 958, LR 0.000070 Loss 6.955785, Accuracy 79.046%\n",
      "Epoch 15, Batch 959, LR 0.000070 Loss 6.955897, Accuracy 79.045%\n",
      "Epoch 15, Batch 960, LR 0.000070 Loss 6.956113, Accuracy 79.045%\n",
      "Epoch 15, Batch 961, LR 0.000070 Loss 6.954573, Accuracy 79.053%\n",
      "Epoch 15, Batch 962, LR 0.000070 Loss 6.954603, Accuracy 79.058%\n",
      "Epoch 15, Batch 963, LR 0.000070 Loss 6.954410, Accuracy 79.060%\n",
      "Epoch 15, Batch 964, LR 0.000070 Loss 6.954996, Accuracy 79.058%\n",
      "Epoch 15, Batch 965, LR 0.000070 Loss 6.954844, Accuracy 79.060%\n",
      "Epoch 15, Batch 966, LR 0.000070 Loss 6.954239, Accuracy 79.070%\n",
      "Epoch 15, Batch 967, LR 0.000070 Loss 6.954629, Accuracy 79.066%\n",
      "Epoch 15, Batch 968, LR 0.000070 Loss 6.954635, Accuracy 79.067%\n",
      "Epoch 15, Batch 969, LR 0.000070 Loss 6.955298, Accuracy 79.066%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 970, LR 0.000070 Loss 6.954904, Accuracy 79.066%\n",
      "Epoch 15, Batch 971, LR 0.000070 Loss 6.954639, Accuracy 79.074%\n",
      "Epoch 15, Batch 972, LR 0.000070 Loss 6.955159, Accuracy 79.067%\n",
      "Epoch 15, Batch 973, LR 0.000070 Loss 6.955299, Accuracy 79.065%\n",
      "Epoch 15, Batch 974, LR 0.000070 Loss 6.954928, Accuracy 79.066%\n",
      "Epoch 15, Batch 975, LR 0.000070 Loss 6.954512, Accuracy 79.066%\n",
      "Epoch 15, Batch 976, LR 0.000070 Loss 6.954681, Accuracy 79.070%\n",
      "Epoch 15, Batch 977, LR 0.000070 Loss 6.954770, Accuracy 79.067%\n",
      "Epoch 15, Batch 978, LR 0.000070 Loss 6.955266, Accuracy 79.063%\n",
      "Epoch 15, Batch 979, LR 0.000070 Loss 6.955333, Accuracy 79.063%\n",
      "Epoch 15, Batch 980, LR 0.000070 Loss 6.955103, Accuracy 79.069%\n",
      "Epoch 15, Batch 981, LR 0.000070 Loss 6.955587, Accuracy 79.065%\n",
      "Epoch 15, Batch 982, LR 0.000070 Loss 6.955943, Accuracy 79.060%\n",
      "Epoch 15, Batch 983, LR 0.000070 Loss 6.956212, Accuracy 79.054%\n",
      "Epoch 15, Batch 984, LR 0.000070 Loss 6.955691, Accuracy 79.058%\n",
      "Epoch 15, Batch 985, LR 0.000070 Loss 6.955493, Accuracy 79.060%\n",
      "Epoch 15, Batch 986, LR 0.000070 Loss 6.955305, Accuracy 79.062%\n",
      "Epoch 15, Batch 987, LR 0.000070 Loss 6.955528, Accuracy 79.063%\n",
      "Epoch 15, Batch 988, LR 0.000070 Loss 6.956341, Accuracy 79.058%\n",
      "Epoch 15, Batch 989, LR 0.000070 Loss 6.956840, Accuracy 79.059%\n",
      "Epoch 15, Batch 990, LR 0.000070 Loss 6.956635, Accuracy 79.061%\n",
      "Epoch 15, Batch 991, LR 0.000070 Loss 6.956717, Accuracy 79.059%\n",
      "Epoch 15, Batch 992, LR 0.000070 Loss 6.958296, Accuracy 79.048%\n",
      "Epoch 15, Batch 993, LR 0.000070 Loss 6.957804, Accuracy 79.048%\n",
      "Epoch 15, Batch 994, LR 0.000070 Loss 6.957893, Accuracy 79.049%\n",
      "Epoch 15, Batch 995, LR 0.000070 Loss 6.958233, Accuracy 79.044%\n",
      "Epoch 15, Batch 996, LR 0.000070 Loss 6.958855, Accuracy 79.044%\n",
      "Epoch 15, Batch 997, LR 0.000070 Loss 6.958666, Accuracy 79.047%\n",
      "Epoch 15, Batch 998, LR 0.000070 Loss 6.958661, Accuracy 79.050%\n",
      "Epoch 15, Batch 999, LR 0.000070 Loss 6.958518, Accuracy 79.052%\n",
      "Epoch 15, Batch 1000, LR 0.000070 Loss 6.957918, Accuracy 79.056%\n",
      "Epoch 15, Batch 1001, LR 0.000070 Loss 6.957630, Accuracy 79.055%\n",
      "Epoch 15, Batch 1002, LR 0.000070 Loss 6.957356, Accuracy 79.058%\n",
      "Epoch 15, Batch 1003, LR 0.000070 Loss 6.957063, Accuracy 79.060%\n",
      "Epoch 15, Batch 1004, LR 0.000070 Loss 6.957212, Accuracy 79.067%\n",
      "Epoch 15, Batch 1005, LR 0.000070 Loss 6.957314, Accuracy 79.066%\n",
      "Epoch 15, Batch 1006, LR 0.000070 Loss 6.956545, Accuracy 79.065%\n",
      "Epoch 15, Batch 1007, LR 0.000070 Loss 6.956725, Accuracy 79.067%\n",
      "Epoch 15, Batch 1008, LR 0.000070 Loss 6.956446, Accuracy 79.068%\n",
      "Epoch 15, Batch 1009, LR 0.000070 Loss 6.956385, Accuracy 79.070%\n",
      "Epoch 15, Batch 1010, LR 0.000070 Loss 6.956995, Accuracy 79.062%\n",
      "Epoch 15, Batch 1011, LR 0.000070 Loss 6.956801, Accuracy 79.066%\n",
      "Epoch 15, Batch 1012, LR 0.000070 Loss 6.956469, Accuracy 79.070%\n",
      "Epoch 15, Batch 1013, LR 0.000070 Loss 6.956723, Accuracy 79.068%\n",
      "Epoch 15, Batch 1014, LR 0.000070 Loss 6.956126, Accuracy 79.072%\n",
      "Epoch 15, Batch 1015, LR 0.000070 Loss 6.956474, Accuracy 79.068%\n",
      "Epoch 15, Batch 1016, LR 0.000070 Loss 6.956455, Accuracy 79.071%\n",
      "Epoch 15, Batch 1017, LR 0.000070 Loss 6.955680, Accuracy 79.073%\n",
      "Epoch 15, Batch 1018, LR 0.000070 Loss 6.955688, Accuracy 79.070%\n",
      "Epoch 15, Batch 1019, LR 0.000070 Loss 6.955514, Accuracy 79.066%\n",
      "Epoch 15, Batch 1020, LR 0.000070 Loss 6.954870, Accuracy 79.067%\n",
      "Epoch 15, Batch 1021, LR 0.000070 Loss 6.954245, Accuracy 79.068%\n",
      "Epoch 15, Batch 1022, LR 0.000070 Loss 6.953461, Accuracy 79.075%\n",
      "Epoch 15, Batch 1023, LR 0.000070 Loss 6.954248, Accuracy 79.073%\n",
      "Epoch 15, Batch 1024, LR 0.000070 Loss 6.954438, Accuracy 79.071%\n",
      "Epoch 15, Batch 1025, LR 0.000070 Loss 6.954352, Accuracy 79.069%\n",
      "Epoch 15, Batch 1026, LR 0.000070 Loss 6.955211, Accuracy 79.069%\n",
      "Epoch 15, Batch 1027, LR 0.000070 Loss 6.955490, Accuracy 79.072%\n",
      "Epoch 15, Batch 1028, LR 0.000070 Loss 6.955779, Accuracy 79.072%\n",
      "Epoch 15, Batch 1029, LR 0.000070 Loss 6.955581, Accuracy 79.071%\n",
      "Epoch 15, Batch 1030, LR 0.000070 Loss 6.955296, Accuracy 79.075%\n",
      "Epoch 15, Batch 1031, LR 0.000070 Loss 6.955325, Accuracy 79.071%\n",
      "Epoch 15, Batch 1032, LR 0.000070 Loss 6.955296, Accuracy 79.071%\n",
      "Epoch 15, Batch 1033, LR 0.000070 Loss 6.954743, Accuracy 79.078%\n",
      "Epoch 15, Batch 1034, LR 0.000070 Loss 6.954470, Accuracy 79.079%\n",
      "Epoch 15, Batch 1035, LR 0.000070 Loss 6.955060, Accuracy 79.076%\n",
      "Epoch 15, Batch 1036, LR 0.000070 Loss 6.954565, Accuracy 79.077%\n",
      "Epoch 15, Batch 1037, LR 0.000070 Loss 6.954798, Accuracy 79.077%\n",
      "Epoch 15, Batch 1038, LR 0.000070 Loss 6.955122, Accuracy 79.073%\n",
      "Epoch 15, Batch 1039, LR 0.000070 Loss 6.955049, Accuracy 79.073%\n",
      "Epoch 15, Batch 1040, LR 0.000070 Loss 6.955114, Accuracy 79.071%\n",
      "Epoch 15, Batch 1041, LR 0.000070 Loss 6.954739, Accuracy 79.073%\n",
      "Epoch 15, Batch 1042, LR 0.000070 Loss 6.954467, Accuracy 79.073%\n",
      "Epoch 15, Batch 1043, LR 0.000070 Loss 6.954395, Accuracy 79.076%\n",
      "Epoch 15, Batch 1044, LR 0.000070 Loss 6.954199, Accuracy 79.075%\n",
      "Epoch 15, Batch 1045, LR 0.000070 Loss 6.954260, Accuracy 79.080%\n",
      "Epoch 15, Batch 1046, LR 0.000070 Loss 6.954116, Accuracy 79.079%\n",
      "Epoch 15, Batch 1047, LR 0.000070 Loss 6.952600, Accuracy 79.089%\n",
      "Epoch 15, Loss (train set) 6.952600, Accuracy (train set) 79.089%\n",
      "Epoch 16, Batch 1, LR 0.000070 Loss 6.543510, Accuracy 82.812%\n",
      "Epoch 16, Batch 2, LR 0.000070 Loss 6.403898, Accuracy 82.812%\n",
      "Epoch 16, Batch 3, LR 0.000070 Loss 6.650277, Accuracy 82.031%\n",
      "Epoch 16, Batch 4, LR 0.000070 Loss 6.780709, Accuracy 81.055%\n",
      "Epoch 16, Batch 5, LR 0.000070 Loss 6.794512, Accuracy 81.406%\n",
      "Epoch 16, Batch 6, LR 0.000070 Loss 6.924203, Accuracy 80.859%\n",
      "Epoch 16, Batch 7, LR 0.000070 Loss 6.952021, Accuracy 81.138%\n",
      "Epoch 16, Batch 8, LR 0.000070 Loss 6.841521, Accuracy 81.543%\n",
      "Epoch 16, Batch 9, LR 0.000070 Loss 6.760863, Accuracy 81.684%\n",
      "Epoch 16, Batch 10, LR 0.000070 Loss 6.702624, Accuracy 81.797%\n",
      "Epoch 16, Batch 11, LR 0.000070 Loss 6.593773, Accuracy 82.031%\n",
      "Epoch 16, Batch 12, LR 0.000070 Loss 6.577497, Accuracy 82.096%\n",
      "Epoch 16, Batch 13, LR 0.000070 Loss 6.546833, Accuracy 82.332%\n",
      "Epoch 16, Batch 14, LR 0.000070 Loss 6.606665, Accuracy 81.641%\n",
      "Epoch 16, Batch 15, LR 0.000070 Loss 6.627354, Accuracy 81.198%\n",
      "Epoch 16, Batch 16, LR 0.000070 Loss 6.661420, Accuracy 81.104%\n",
      "Epoch 16, Batch 17, LR 0.000070 Loss 6.649208, Accuracy 81.020%\n",
      "Epoch 16, Batch 18, LR 0.000070 Loss 6.681627, Accuracy 80.903%\n",
      "Epoch 16, Batch 19, LR 0.000070 Loss 6.654268, Accuracy 81.209%\n",
      "Epoch 16, Batch 20, LR 0.000070 Loss 6.672810, Accuracy 81.055%\n",
      "Epoch 16, Batch 21, LR 0.000070 Loss 6.687071, Accuracy 81.101%\n",
      "Epoch 16, Batch 22, LR 0.000070 Loss 6.694381, Accuracy 81.001%\n",
      "Epoch 16, Batch 23, LR 0.000070 Loss 6.698313, Accuracy 80.774%\n",
      "Epoch 16, Batch 24, LR 0.000070 Loss 6.699249, Accuracy 80.990%\n",
      "Epoch 16, Batch 25, LR 0.000070 Loss 6.696842, Accuracy 81.188%\n",
      "Epoch 16, Batch 26, LR 0.000070 Loss 6.689075, Accuracy 81.250%\n",
      "Epoch 16, Batch 27, LR 0.000070 Loss 6.673162, Accuracy 81.221%\n",
      "Epoch 16, Batch 28, LR 0.000070 Loss 6.689923, Accuracy 81.055%\n",
      "Epoch 16, Batch 29, LR 0.000070 Loss 6.695661, Accuracy 81.034%\n",
      "Epoch 16, Batch 30, LR 0.000070 Loss 6.700046, Accuracy 80.911%\n",
      "Epoch 16, Batch 31, LR 0.000070 Loss 6.716042, Accuracy 80.696%\n",
      "Epoch 16, Batch 32, LR 0.000070 Loss 6.714039, Accuracy 80.615%\n",
      "Epoch 16, Batch 33, LR 0.000070 Loss 6.695235, Accuracy 80.800%\n",
      "Epoch 16, Batch 34, LR 0.000070 Loss 6.711789, Accuracy 80.744%\n",
      "Epoch 16, Batch 35, LR 0.000070 Loss 6.710500, Accuracy 80.804%\n",
      "Epoch 16, Batch 36, LR 0.000070 Loss 6.717573, Accuracy 80.686%\n",
      "Epoch 16, Batch 37, LR 0.000070 Loss 6.704044, Accuracy 80.743%\n",
      "Epoch 16, Batch 38, LR 0.000070 Loss 6.715269, Accuracy 80.674%\n",
      "Epoch 16, Batch 39, LR 0.000070 Loss 6.721079, Accuracy 80.589%\n",
      "Epoch 16, Batch 40, LR 0.000070 Loss 6.717491, Accuracy 80.605%\n",
      "Epoch 16, Batch 41, LR 0.000070 Loss 6.718428, Accuracy 80.621%\n",
      "Epoch 16, Batch 42, LR 0.000070 Loss 6.715529, Accuracy 80.655%\n",
      "Epoch 16, Batch 43, LR 0.000070 Loss 6.722687, Accuracy 80.560%\n",
      "Epoch 16, Batch 44, LR 0.000070 Loss 6.745292, Accuracy 80.451%\n",
      "Epoch 16, Batch 45, LR 0.000070 Loss 6.748899, Accuracy 80.347%\n",
      "Epoch 16, Batch 46, LR 0.000070 Loss 6.756647, Accuracy 80.350%\n",
      "Epoch 16, Batch 47, LR 0.000070 Loss 6.747736, Accuracy 80.369%\n",
      "Epoch 16, Batch 48, LR 0.000070 Loss 6.752170, Accuracy 80.355%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 49, LR 0.000070 Loss 6.767640, Accuracy 80.357%\n",
      "Epoch 16, Batch 50, LR 0.000070 Loss 6.764390, Accuracy 80.359%\n",
      "Epoch 16, Batch 51, LR 0.000070 Loss 6.773435, Accuracy 80.300%\n",
      "Epoch 16, Batch 52, LR 0.000070 Loss 6.769866, Accuracy 80.349%\n",
      "Epoch 16, Batch 53, LR 0.000070 Loss 6.769716, Accuracy 80.321%\n",
      "Epoch 16, Batch 54, LR 0.000070 Loss 6.767891, Accuracy 80.295%\n",
      "Epoch 16, Batch 55, LR 0.000070 Loss 6.754778, Accuracy 80.384%\n",
      "Epoch 16, Batch 56, LR 0.000070 Loss 6.745498, Accuracy 80.427%\n",
      "Epoch 16, Batch 57, LR 0.000070 Loss 6.757757, Accuracy 80.277%\n",
      "Epoch 16, Batch 58, LR 0.000070 Loss 6.767948, Accuracy 80.253%\n",
      "Epoch 16, Batch 59, LR 0.000070 Loss 6.776572, Accuracy 80.217%\n",
      "Epoch 16, Batch 60, LR 0.000070 Loss 6.765477, Accuracy 80.273%\n",
      "Epoch 16, Batch 61, LR 0.000070 Loss 6.748039, Accuracy 80.392%\n",
      "Epoch 16, Batch 62, LR 0.000070 Loss 6.755622, Accuracy 80.406%\n",
      "Epoch 16, Batch 63, LR 0.000070 Loss 6.761990, Accuracy 80.382%\n",
      "Epoch 16, Batch 64, LR 0.000070 Loss 6.745836, Accuracy 80.457%\n",
      "Epoch 16, Batch 65, LR 0.000070 Loss 6.746943, Accuracy 80.373%\n",
      "Epoch 16, Batch 66, LR 0.000070 Loss 6.759889, Accuracy 80.315%\n",
      "Epoch 16, Batch 67, LR 0.000070 Loss 6.768792, Accuracy 80.271%\n",
      "Epoch 16, Batch 68, LR 0.000070 Loss 6.757847, Accuracy 80.285%\n",
      "Epoch 16, Batch 69, LR 0.000070 Loss 6.744114, Accuracy 80.378%\n",
      "Epoch 16, Batch 70, LR 0.000070 Loss 6.740614, Accuracy 80.413%\n",
      "Epoch 16, Batch 71, LR 0.000070 Loss 6.744458, Accuracy 80.403%\n",
      "Epoch 16, Batch 72, LR 0.000070 Loss 6.746211, Accuracy 80.339%\n",
      "Epoch 16, Batch 73, LR 0.000070 Loss 6.744533, Accuracy 80.330%\n",
      "Epoch 16, Batch 74, LR 0.000070 Loss 6.761059, Accuracy 80.226%\n",
      "Epoch 16, Batch 75, LR 0.000070 Loss 6.755409, Accuracy 80.271%\n",
      "Epoch 16, Batch 76, LR 0.000070 Loss 6.756003, Accuracy 80.304%\n",
      "Epoch 16, Batch 77, LR 0.000070 Loss 6.753824, Accuracy 80.367%\n",
      "Epoch 16, Batch 78, LR 0.000070 Loss 6.754609, Accuracy 80.339%\n",
      "Epoch 16, Batch 79, LR 0.000070 Loss 6.760947, Accuracy 80.320%\n",
      "Epoch 16, Batch 80, LR 0.000070 Loss 6.762591, Accuracy 80.312%\n",
      "Epoch 16, Batch 81, LR 0.000070 Loss 6.767986, Accuracy 80.295%\n",
      "Epoch 16, Batch 82, LR 0.000070 Loss 6.765168, Accuracy 80.278%\n",
      "Epoch 16, Batch 83, LR 0.000070 Loss 6.764992, Accuracy 80.290%\n",
      "Epoch 16, Batch 84, LR 0.000070 Loss 6.764743, Accuracy 80.273%\n",
      "Epoch 16, Batch 85, LR 0.000070 Loss 6.764092, Accuracy 80.303%\n",
      "Epoch 16, Batch 86, LR 0.000070 Loss 6.762497, Accuracy 80.305%\n",
      "Epoch 16, Batch 87, LR 0.000070 Loss 6.766882, Accuracy 80.289%\n",
      "Epoch 16, Batch 88, LR 0.000070 Loss 6.779754, Accuracy 80.265%\n",
      "Epoch 16, Batch 89, LR 0.000070 Loss 6.788026, Accuracy 80.258%\n",
      "Epoch 16, Batch 90, LR 0.000070 Loss 6.783178, Accuracy 80.321%\n",
      "Epoch 16, Batch 91, LR 0.000070 Loss 6.774453, Accuracy 80.340%\n",
      "Epoch 16, Batch 92, LR 0.000070 Loss 6.768682, Accuracy 80.367%\n",
      "Epoch 16, Batch 93, LR 0.000070 Loss 6.765301, Accuracy 80.376%\n",
      "Epoch 16, Batch 94, LR 0.000070 Loss 6.770026, Accuracy 80.352%\n",
      "Epoch 16, Batch 95, LR 0.000070 Loss 6.767452, Accuracy 80.403%\n",
      "Epoch 16, Batch 96, LR 0.000070 Loss 6.773599, Accuracy 80.363%\n",
      "Epoch 16, Batch 97, LR 0.000070 Loss 6.769466, Accuracy 80.348%\n",
      "Epoch 16, Batch 98, LR 0.000070 Loss 6.771477, Accuracy 80.373%\n",
      "Epoch 16, Batch 99, LR 0.000070 Loss 6.771327, Accuracy 80.406%\n",
      "Epoch 16, Batch 100, LR 0.000070 Loss 6.774222, Accuracy 80.367%\n",
      "Epoch 16, Batch 101, LR 0.000070 Loss 6.780674, Accuracy 80.330%\n",
      "Epoch 16, Batch 102, LR 0.000070 Loss 6.778405, Accuracy 80.346%\n",
      "Epoch 16, Batch 103, LR 0.000070 Loss 6.783942, Accuracy 80.325%\n",
      "Epoch 16, Batch 104, LR 0.000070 Loss 6.790693, Accuracy 80.258%\n",
      "Epoch 16, Batch 105, LR 0.000070 Loss 6.791034, Accuracy 80.201%\n",
      "Epoch 16, Batch 106, LR 0.000070 Loss 6.796734, Accuracy 80.167%\n",
      "Epoch 16, Batch 107, LR 0.000070 Loss 6.803496, Accuracy 80.126%\n",
      "Epoch 16, Batch 108, LR 0.000070 Loss 6.797703, Accuracy 80.208%\n",
      "Epoch 16, Batch 109, LR 0.000070 Loss 6.800036, Accuracy 80.182%\n",
      "Epoch 16, Batch 110, LR 0.000070 Loss 6.806350, Accuracy 80.156%\n",
      "Epoch 16, Batch 111, LR 0.000070 Loss 6.801201, Accuracy 80.187%\n",
      "Epoch 16, Batch 112, LR 0.000070 Loss 6.802010, Accuracy 80.190%\n",
      "Epoch 16, Batch 113, LR 0.000070 Loss 6.796645, Accuracy 80.206%\n",
      "Epoch 16, Batch 114, LR 0.000070 Loss 6.789324, Accuracy 80.249%\n",
      "Epoch 16, Batch 115, LR 0.000070 Loss 6.797443, Accuracy 80.170%\n",
      "Epoch 16, Batch 116, LR 0.000070 Loss 6.800829, Accuracy 80.132%\n",
      "Epoch 16, Batch 117, LR 0.000070 Loss 6.801810, Accuracy 80.108%\n",
      "Epoch 16, Batch 118, LR 0.000070 Loss 6.805903, Accuracy 80.091%\n",
      "Epoch 16, Batch 119, LR 0.000070 Loss 6.815777, Accuracy 80.062%\n",
      "Epoch 16, Batch 120, LR 0.000070 Loss 6.806777, Accuracy 80.111%\n",
      "Epoch 16, Batch 121, LR 0.000070 Loss 6.808448, Accuracy 80.088%\n",
      "Epoch 16, Batch 122, LR 0.000070 Loss 6.806985, Accuracy 80.091%\n",
      "Epoch 16, Batch 123, LR 0.000070 Loss 6.805699, Accuracy 80.081%\n",
      "Epoch 16, Batch 124, LR 0.000070 Loss 6.804528, Accuracy 80.084%\n",
      "Epoch 16, Batch 125, LR 0.000070 Loss 6.802017, Accuracy 80.106%\n",
      "Epoch 16, Batch 126, LR 0.000070 Loss 6.797719, Accuracy 80.128%\n",
      "Epoch 16, Batch 127, LR 0.000070 Loss 6.799312, Accuracy 80.149%\n",
      "Epoch 16, Batch 128, LR 0.000070 Loss 6.801418, Accuracy 80.127%\n",
      "Epoch 16, Batch 129, LR 0.000070 Loss 6.799960, Accuracy 80.130%\n",
      "Epoch 16, Batch 130, LR 0.000070 Loss 6.804149, Accuracy 80.108%\n",
      "Epoch 16, Batch 131, LR 0.000070 Loss 6.803980, Accuracy 80.099%\n",
      "Epoch 16, Batch 132, LR 0.000070 Loss 6.804641, Accuracy 80.102%\n",
      "Epoch 16, Batch 133, LR 0.000070 Loss 6.802763, Accuracy 80.093%\n",
      "Epoch 16, Batch 134, LR 0.000070 Loss 6.801705, Accuracy 80.113%\n",
      "Epoch 16, Batch 135, LR 0.000070 Loss 6.793921, Accuracy 80.145%\n",
      "Epoch 16, Batch 136, LR 0.000070 Loss 6.792740, Accuracy 80.159%\n",
      "Epoch 16, Batch 137, LR 0.000070 Loss 6.792262, Accuracy 80.167%\n",
      "Epoch 16, Batch 138, LR 0.000070 Loss 6.794504, Accuracy 80.174%\n",
      "Epoch 16, Batch 139, LR 0.000070 Loss 6.793699, Accuracy 80.154%\n",
      "Epoch 16, Batch 140, LR 0.000070 Loss 6.793408, Accuracy 80.145%\n",
      "Epoch 16, Batch 141, LR 0.000070 Loss 6.790524, Accuracy 80.153%\n",
      "Epoch 16, Batch 142, LR 0.000070 Loss 6.787762, Accuracy 80.155%\n",
      "Epoch 16, Batch 143, LR 0.000070 Loss 6.786090, Accuracy 80.152%\n",
      "Epoch 16, Batch 144, LR 0.000070 Loss 6.779568, Accuracy 80.143%\n",
      "Epoch 16, Batch 145, LR 0.000070 Loss 6.779422, Accuracy 80.129%\n",
      "Epoch 16, Batch 146, LR 0.000070 Loss 6.780192, Accuracy 80.137%\n",
      "Epoch 16, Batch 147, LR 0.000070 Loss 6.779344, Accuracy 80.150%\n",
      "Epoch 16, Batch 148, LR 0.000070 Loss 6.775241, Accuracy 80.178%\n",
      "Epoch 16, Batch 149, LR 0.000070 Loss 6.773787, Accuracy 80.175%\n",
      "Epoch 16, Batch 150, LR 0.000070 Loss 6.774899, Accuracy 80.172%\n",
      "Epoch 16, Batch 151, LR 0.000070 Loss 6.768482, Accuracy 80.200%\n",
      "Epoch 16, Batch 152, LR 0.000070 Loss 6.767456, Accuracy 80.176%\n",
      "Epoch 16, Batch 153, LR 0.000070 Loss 6.767284, Accuracy 80.178%\n",
      "Epoch 16, Batch 154, LR 0.000070 Loss 6.766469, Accuracy 80.200%\n",
      "Epoch 16, Batch 155, LR 0.000070 Loss 6.770148, Accuracy 80.166%\n",
      "Epoch 16, Batch 156, LR 0.000070 Loss 6.769664, Accuracy 80.178%\n",
      "Epoch 16, Batch 157, LR 0.000070 Loss 6.766719, Accuracy 80.190%\n",
      "Epoch 16, Batch 158, LR 0.000070 Loss 6.765508, Accuracy 80.207%\n",
      "Epoch 16, Batch 159, LR 0.000070 Loss 6.761156, Accuracy 80.213%\n",
      "Epoch 16, Batch 160, LR 0.000070 Loss 6.764761, Accuracy 80.195%\n",
      "Epoch 16, Batch 161, LR 0.000070 Loss 6.773311, Accuracy 80.134%\n",
      "Epoch 16, Batch 162, LR 0.000069 Loss 6.775122, Accuracy 80.112%\n",
      "Epoch 16, Batch 163, LR 0.000069 Loss 6.774543, Accuracy 80.100%\n",
      "Epoch 16, Batch 164, LR 0.000069 Loss 6.777904, Accuracy 80.078%\n",
      "Epoch 16, Batch 165, LR 0.000069 Loss 6.774260, Accuracy 80.099%\n",
      "Epoch 16, Batch 166, LR 0.000069 Loss 6.771651, Accuracy 80.120%\n",
      "Epoch 16, Batch 167, LR 0.000069 Loss 6.773886, Accuracy 80.090%\n",
      "Epoch 16, Batch 168, LR 0.000069 Loss 6.776493, Accuracy 80.073%\n",
      "Epoch 16, Batch 169, LR 0.000069 Loss 6.773524, Accuracy 80.117%\n",
      "Epoch 16, Batch 170, LR 0.000069 Loss 6.773141, Accuracy 80.119%\n",
      "Epoch 16, Batch 171, LR 0.000069 Loss 6.774256, Accuracy 80.112%\n",
      "Epoch 16, Batch 172, LR 0.000069 Loss 6.772983, Accuracy 80.092%\n",
      "Epoch 16, Batch 173, LR 0.000069 Loss 6.773349, Accuracy 80.080%\n",
      "Epoch 16, Batch 174, LR 0.000069 Loss 6.772064, Accuracy 80.087%\n",
      "Epoch 16, Batch 175, LR 0.000069 Loss 6.771687, Accuracy 80.067%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 176, LR 0.000069 Loss 6.768149, Accuracy 80.083%\n",
      "Epoch 16, Batch 177, LR 0.000069 Loss 6.768803, Accuracy 80.080%\n",
      "Epoch 16, Batch 178, LR 0.000069 Loss 6.770726, Accuracy 80.056%\n",
      "Epoch 16, Batch 179, LR 0.000069 Loss 6.770766, Accuracy 80.072%\n",
      "Epoch 16, Batch 180, LR 0.000069 Loss 6.769769, Accuracy 80.043%\n",
      "Epoch 16, Batch 181, LR 0.000069 Loss 6.764350, Accuracy 80.093%\n",
      "Epoch 16, Batch 182, LR 0.000069 Loss 6.759969, Accuracy 80.117%\n",
      "Epoch 16, Batch 183, LR 0.000069 Loss 6.754716, Accuracy 80.149%\n",
      "Epoch 16, Batch 184, LR 0.000069 Loss 6.752591, Accuracy 80.176%\n",
      "Epoch 16, Batch 185, LR 0.000069 Loss 6.748262, Accuracy 80.215%\n",
      "Epoch 16, Batch 186, LR 0.000069 Loss 6.751926, Accuracy 80.192%\n",
      "Epoch 16, Batch 187, LR 0.000069 Loss 6.750380, Accuracy 80.168%\n",
      "Epoch 16, Batch 188, LR 0.000069 Loss 6.754204, Accuracy 80.128%\n",
      "Epoch 16, Batch 189, LR 0.000069 Loss 6.754554, Accuracy 80.138%\n",
      "Epoch 16, Batch 190, LR 0.000069 Loss 6.751449, Accuracy 80.140%\n",
      "Epoch 16, Batch 191, LR 0.000069 Loss 6.750820, Accuracy 80.129%\n",
      "Epoch 16, Batch 192, LR 0.000069 Loss 6.748547, Accuracy 80.147%\n",
      "Epoch 16, Batch 193, LR 0.000069 Loss 6.749815, Accuracy 80.149%\n",
      "Epoch 16, Batch 194, LR 0.000069 Loss 6.755851, Accuracy 80.118%\n",
      "Epoch 16, Batch 195, LR 0.000069 Loss 6.754690, Accuracy 80.136%\n",
      "Epoch 16, Batch 196, LR 0.000069 Loss 6.749044, Accuracy 80.170%\n",
      "Epoch 16, Batch 197, LR 0.000069 Loss 6.748607, Accuracy 80.151%\n",
      "Epoch 16, Batch 198, LR 0.000069 Loss 6.747955, Accuracy 80.161%\n",
      "Epoch 16, Batch 199, LR 0.000069 Loss 6.749124, Accuracy 80.135%\n",
      "Epoch 16, Batch 200, LR 0.000069 Loss 6.746949, Accuracy 80.148%\n",
      "Epoch 16, Batch 201, LR 0.000069 Loss 6.748952, Accuracy 80.127%\n",
      "Epoch 16, Batch 202, LR 0.000069 Loss 6.744518, Accuracy 80.167%\n",
      "Epoch 16, Batch 203, LR 0.000069 Loss 6.744043, Accuracy 80.169%\n",
      "Epoch 16, Batch 204, LR 0.000069 Loss 6.743231, Accuracy 80.166%\n",
      "Epoch 16, Batch 205, LR 0.000069 Loss 6.746321, Accuracy 80.152%\n",
      "Epoch 16, Batch 206, LR 0.000069 Loss 6.751197, Accuracy 80.150%\n",
      "Epoch 16, Batch 207, LR 0.000069 Loss 6.749004, Accuracy 80.152%\n",
      "Epoch 16, Batch 208, LR 0.000069 Loss 6.748690, Accuracy 80.146%\n",
      "Epoch 16, Batch 209, LR 0.000069 Loss 6.749782, Accuracy 80.121%\n",
      "Epoch 16, Batch 210, LR 0.000069 Loss 6.751377, Accuracy 80.123%\n",
      "Epoch 16, Batch 211, LR 0.000069 Loss 6.752295, Accuracy 80.113%\n",
      "Epoch 16, Batch 212, LR 0.000069 Loss 6.753020, Accuracy 80.119%\n",
      "Epoch 16, Batch 213, LR 0.000069 Loss 6.752674, Accuracy 80.124%\n",
      "Epoch 16, Batch 214, LR 0.000069 Loss 6.753227, Accuracy 80.104%\n",
      "Epoch 16, Batch 215, LR 0.000069 Loss 6.752880, Accuracy 80.113%\n",
      "Epoch 16, Batch 216, LR 0.000069 Loss 6.754701, Accuracy 80.111%\n",
      "Epoch 16, Batch 217, LR 0.000069 Loss 6.754038, Accuracy 80.109%\n",
      "Epoch 16, Batch 218, LR 0.000069 Loss 6.753377, Accuracy 80.114%\n",
      "Epoch 16, Batch 219, LR 0.000069 Loss 6.752637, Accuracy 80.116%\n",
      "Epoch 16, Batch 220, LR 0.000069 Loss 6.754613, Accuracy 80.107%\n",
      "Epoch 16, Batch 221, LR 0.000069 Loss 6.754610, Accuracy 80.112%\n",
      "Epoch 16, Batch 222, LR 0.000069 Loss 6.758944, Accuracy 80.096%\n",
      "Epoch 16, Batch 223, LR 0.000069 Loss 6.763770, Accuracy 80.055%\n",
      "Epoch 16, Batch 224, LR 0.000069 Loss 6.759940, Accuracy 80.096%\n",
      "Epoch 16, Batch 225, LR 0.000069 Loss 6.762177, Accuracy 80.087%\n",
      "Epoch 16, Batch 226, LR 0.000069 Loss 6.758229, Accuracy 80.106%\n",
      "Epoch 16, Batch 227, LR 0.000069 Loss 6.761065, Accuracy 80.094%\n",
      "Epoch 16, Batch 228, LR 0.000069 Loss 6.762862, Accuracy 80.058%\n",
      "Epoch 16, Batch 229, LR 0.000069 Loss 6.765052, Accuracy 80.042%\n",
      "Epoch 16, Batch 230, LR 0.000069 Loss 6.764095, Accuracy 80.044%\n",
      "Epoch 16, Batch 231, LR 0.000069 Loss 6.762945, Accuracy 80.053%\n",
      "Epoch 16, Batch 232, LR 0.000069 Loss 6.762395, Accuracy 80.055%\n",
      "Epoch 16, Batch 233, LR 0.000069 Loss 6.762039, Accuracy 80.043%\n",
      "Epoch 16, Batch 234, LR 0.000069 Loss 6.761832, Accuracy 80.045%\n",
      "Epoch 16, Batch 235, LR 0.000069 Loss 6.761143, Accuracy 80.073%\n",
      "Epoch 16, Batch 236, LR 0.000069 Loss 6.758303, Accuracy 80.091%\n",
      "Epoch 16, Batch 237, LR 0.000069 Loss 6.760825, Accuracy 80.070%\n",
      "Epoch 16, Batch 238, LR 0.000069 Loss 6.759253, Accuracy 80.075%\n",
      "Epoch 16, Batch 239, LR 0.000069 Loss 6.762410, Accuracy 80.034%\n",
      "Epoch 16, Batch 240, LR 0.000069 Loss 6.761958, Accuracy 80.033%\n",
      "Epoch 16, Batch 241, LR 0.000069 Loss 6.761162, Accuracy 80.028%\n",
      "Epoch 16, Batch 242, LR 0.000069 Loss 6.761827, Accuracy 80.014%\n",
      "Epoch 16, Batch 243, LR 0.000069 Loss 6.764262, Accuracy 79.986%\n",
      "Epoch 16, Batch 244, LR 0.000069 Loss 6.767538, Accuracy 79.972%\n",
      "Epoch 16, Batch 245, LR 0.000069 Loss 6.766756, Accuracy 79.978%\n",
      "Epoch 16, Batch 246, LR 0.000069 Loss 6.765790, Accuracy 79.996%\n",
      "Epoch 16, Batch 247, LR 0.000069 Loss 6.769550, Accuracy 79.956%\n",
      "Epoch 16, Batch 248, LR 0.000069 Loss 6.770528, Accuracy 79.952%\n",
      "Epoch 16, Batch 249, LR 0.000069 Loss 6.769671, Accuracy 79.954%\n",
      "Epoch 16, Batch 250, LR 0.000069 Loss 6.769954, Accuracy 79.938%\n",
      "Epoch 16, Batch 251, LR 0.000069 Loss 6.768412, Accuracy 79.955%\n",
      "Epoch 16, Batch 252, LR 0.000069 Loss 6.765735, Accuracy 79.973%\n",
      "Epoch 16, Batch 253, LR 0.000069 Loss 6.764475, Accuracy 79.978%\n",
      "Epoch 16, Batch 254, LR 0.000069 Loss 6.760363, Accuracy 80.007%\n",
      "Epoch 16, Batch 255, LR 0.000069 Loss 6.762363, Accuracy 79.991%\n",
      "Epoch 16, Batch 256, LR 0.000069 Loss 6.763496, Accuracy 79.980%\n",
      "Epoch 16, Batch 257, LR 0.000069 Loss 6.763794, Accuracy 79.967%\n",
      "Epoch 16, Batch 258, LR 0.000069 Loss 6.761791, Accuracy 79.990%\n",
      "Epoch 16, Batch 259, LR 0.000069 Loss 6.761091, Accuracy 79.983%\n",
      "Epoch 16, Batch 260, LR 0.000069 Loss 6.760868, Accuracy 79.988%\n",
      "Epoch 16, Batch 261, LR 0.000069 Loss 6.760421, Accuracy 79.984%\n",
      "Epoch 16, Batch 262, LR 0.000069 Loss 6.763794, Accuracy 79.971%\n",
      "Epoch 16, Batch 263, LR 0.000069 Loss 6.763014, Accuracy 79.952%\n",
      "Epoch 16, Batch 264, LR 0.000069 Loss 6.763050, Accuracy 79.936%\n",
      "Epoch 16, Batch 265, LR 0.000069 Loss 6.765482, Accuracy 79.917%\n",
      "Epoch 16, Batch 266, LR 0.000069 Loss 6.764785, Accuracy 79.905%\n",
      "Epoch 16, Batch 267, LR 0.000069 Loss 6.765084, Accuracy 79.901%\n",
      "Epoch 16, Batch 268, LR 0.000069 Loss 6.764198, Accuracy 79.900%\n",
      "Epoch 16, Batch 269, LR 0.000069 Loss 6.764116, Accuracy 79.891%\n",
      "Epoch 16, Batch 270, LR 0.000069 Loss 6.761626, Accuracy 79.905%\n",
      "Epoch 16, Batch 271, LR 0.000069 Loss 6.764650, Accuracy 79.904%\n",
      "Epoch 16, Batch 272, LR 0.000069 Loss 6.764806, Accuracy 79.891%\n",
      "Epoch 16, Batch 273, LR 0.000069 Loss 6.764813, Accuracy 79.885%\n",
      "Epoch 16, Batch 274, LR 0.000069 Loss 6.763364, Accuracy 79.901%\n",
      "Epoch 16, Batch 275, LR 0.000069 Loss 6.760613, Accuracy 79.926%\n",
      "Epoch 16, Batch 276, LR 0.000069 Loss 6.760571, Accuracy 79.928%\n",
      "Epoch 16, Batch 277, LR 0.000069 Loss 6.763317, Accuracy 79.905%\n",
      "Epoch 16, Batch 278, LR 0.000069 Loss 6.765561, Accuracy 79.895%\n",
      "Epoch 16, Batch 279, LR 0.000069 Loss 6.763758, Accuracy 79.903%\n",
      "Epoch 16, Batch 280, LR 0.000069 Loss 6.764438, Accuracy 79.905%\n",
      "Epoch 16, Batch 281, LR 0.000069 Loss 6.764371, Accuracy 79.904%\n",
      "Epoch 16, Batch 282, LR 0.000069 Loss 6.762435, Accuracy 79.909%\n",
      "Epoch 16, Batch 283, LR 0.000069 Loss 6.761223, Accuracy 79.914%\n",
      "Epoch 16, Batch 284, LR 0.000069 Loss 6.763563, Accuracy 79.894%\n",
      "Epoch 16, Batch 285, LR 0.000069 Loss 6.762961, Accuracy 79.923%\n",
      "Epoch 16, Batch 286, LR 0.000069 Loss 6.764600, Accuracy 79.909%\n",
      "Epoch 16, Batch 287, LR 0.000069 Loss 6.761409, Accuracy 79.930%\n",
      "Epoch 16, Batch 288, LR 0.000069 Loss 6.761975, Accuracy 79.929%\n",
      "Epoch 16, Batch 289, LR 0.000069 Loss 6.759564, Accuracy 79.933%\n",
      "Epoch 16, Batch 290, LR 0.000069 Loss 6.757514, Accuracy 79.941%\n",
      "Epoch 16, Batch 291, LR 0.000069 Loss 6.756979, Accuracy 79.943%\n",
      "Epoch 16, Batch 292, LR 0.000069 Loss 6.760471, Accuracy 79.934%\n",
      "Epoch 16, Batch 293, LR 0.000069 Loss 6.762559, Accuracy 79.922%\n",
      "Epoch 16, Batch 294, LR 0.000069 Loss 6.761033, Accuracy 79.927%\n",
      "Epoch 16, Batch 295, LR 0.000069 Loss 6.762346, Accuracy 79.921%\n",
      "Epoch 16, Batch 296, LR 0.000069 Loss 6.759460, Accuracy 79.925%\n",
      "Epoch 16, Batch 297, LR 0.000069 Loss 6.757365, Accuracy 79.940%\n",
      "Epoch 16, Batch 298, LR 0.000069 Loss 6.756514, Accuracy 79.937%\n",
      "Epoch 16, Batch 299, LR 0.000069 Loss 6.757726, Accuracy 79.925%\n",
      "Epoch 16, Batch 300, LR 0.000069 Loss 6.755633, Accuracy 79.927%\n",
      "Epoch 16, Batch 301, LR 0.000069 Loss 6.755935, Accuracy 79.929%\n",
      "Epoch 16, Batch 302, LR 0.000069 Loss 6.755054, Accuracy 79.933%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 303, LR 0.000069 Loss 6.754037, Accuracy 79.930%\n",
      "Epoch 16, Batch 304, LR 0.000069 Loss 6.753910, Accuracy 79.927%\n",
      "Epoch 16, Batch 305, LR 0.000069 Loss 6.752640, Accuracy 79.944%\n",
      "Epoch 16, Batch 306, LR 0.000069 Loss 6.751052, Accuracy 79.943%\n",
      "Epoch 16, Batch 307, LR 0.000069 Loss 6.752113, Accuracy 79.937%\n",
      "Epoch 16, Batch 308, LR 0.000069 Loss 6.752894, Accuracy 79.941%\n",
      "Epoch 16, Batch 309, LR 0.000069 Loss 6.754750, Accuracy 79.935%\n",
      "Epoch 16, Batch 310, LR 0.000069 Loss 6.757545, Accuracy 79.929%\n",
      "Epoch 16, Batch 311, LR 0.000069 Loss 6.754631, Accuracy 79.946%\n",
      "Epoch 16, Batch 312, LR 0.000069 Loss 6.755043, Accuracy 79.945%\n",
      "Epoch 16, Batch 313, LR 0.000069 Loss 6.756647, Accuracy 79.937%\n",
      "Epoch 16, Batch 314, LR 0.000069 Loss 6.757077, Accuracy 79.939%\n",
      "Epoch 16, Batch 315, LR 0.000069 Loss 6.755470, Accuracy 79.936%\n",
      "Epoch 16, Batch 316, LR 0.000069 Loss 6.756278, Accuracy 79.935%\n",
      "Epoch 16, Batch 317, LR 0.000069 Loss 6.755124, Accuracy 79.939%\n",
      "Epoch 16, Batch 318, LR 0.000069 Loss 6.754249, Accuracy 79.948%\n",
      "Epoch 16, Batch 319, LR 0.000069 Loss 6.753060, Accuracy 79.959%\n",
      "Epoch 16, Batch 320, LR 0.000069 Loss 6.754650, Accuracy 79.963%\n",
      "Epoch 16, Batch 321, LR 0.000069 Loss 6.753736, Accuracy 79.970%\n",
      "Epoch 16, Batch 322, LR 0.000069 Loss 6.753910, Accuracy 79.959%\n",
      "Epoch 16, Batch 323, LR 0.000069 Loss 6.754354, Accuracy 79.958%\n",
      "Epoch 16, Batch 324, LR 0.000069 Loss 6.752819, Accuracy 79.958%\n",
      "Epoch 16, Batch 325, LR 0.000069 Loss 6.752754, Accuracy 79.947%\n",
      "Epoch 16, Batch 326, LR 0.000069 Loss 6.753909, Accuracy 79.942%\n",
      "Epoch 16, Batch 327, LR 0.000069 Loss 6.753776, Accuracy 79.946%\n",
      "Epoch 16, Batch 328, LR 0.000069 Loss 6.756348, Accuracy 79.938%\n",
      "Epoch 16, Batch 329, LR 0.000069 Loss 6.759599, Accuracy 79.920%\n",
      "Epoch 16, Batch 330, LR 0.000069 Loss 6.760100, Accuracy 79.915%\n",
      "Epoch 16, Batch 331, LR 0.000069 Loss 6.760931, Accuracy 79.912%\n",
      "Epoch 16, Batch 332, LR 0.000069 Loss 6.759625, Accuracy 79.916%\n",
      "Epoch 16, Batch 333, LR 0.000069 Loss 6.760259, Accuracy 79.913%\n",
      "Epoch 16, Batch 334, LR 0.000069 Loss 6.762244, Accuracy 79.905%\n",
      "Epoch 16, Batch 335, LR 0.000069 Loss 6.763061, Accuracy 79.911%\n",
      "Epoch 16, Batch 336, LR 0.000069 Loss 6.760995, Accuracy 79.927%\n",
      "Epoch 16, Batch 337, LR 0.000069 Loss 6.760192, Accuracy 79.936%\n",
      "Epoch 16, Batch 338, LR 0.000069 Loss 6.759363, Accuracy 79.939%\n",
      "Epoch 16, Batch 339, LR 0.000069 Loss 6.761953, Accuracy 79.923%\n",
      "Epoch 16, Batch 340, LR 0.000069 Loss 6.760465, Accuracy 79.931%\n",
      "Epoch 16, Batch 341, LR 0.000069 Loss 6.758903, Accuracy 79.937%\n",
      "Epoch 16, Batch 342, LR 0.000069 Loss 6.759032, Accuracy 79.939%\n",
      "Epoch 16, Batch 343, LR 0.000069 Loss 6.758437, Accuracy 79.959%\n",
      "Epoch 16, Batch 344, LR 0.000069 Loss 6.758584, Accuracy 79.962%\n",
      "Epoch 16, Batch 345, LR 0.000069 Loss 6.757786, Accuracy 79.962%\n",
      "Epoch 16, Batch 346, LR 0.000069 Loss 6.757905, Accuracy 79.961%\n",
      "Epoch 16, Batch 347, LR 0.000069 Loss 6.756284, Accuracy 79.980%\n",
      "Epoch 16, Batch 348, LR 0.000069 Loss 6.755800, Accuracy 79.993%\n",
      "Epoch 16, Batch 349, LR 0.000069 Loss 6.756643, Accuracy 79.992%\n",
      "Epoch 16, Batch 350, LR 0.000069 Loss 6.757597, Accuracy 79.982%\n",
      "Epoch 16, Batch 351, LR 0.000069 Loss 6.759118, Accuracy 79.968%\n",
      "Epoch 16, Batch 352, LR 0.000069 Loss 6.759656, Accuracy 79.963%\n",
      "Epoch 16, Batch 353, LR 0.000069 Loss 6.760134, Accuracy 79.955%\n",
      "Epoch 16, Batch 354, LR 0.000069 Loss 6.761351, Accuracy 79.948%\n",
      "Epoch 16, Batch 355, LR 0.000069 Loss 6.760085, Accuracy 79.952%\n",
      "Epoch 16, Batch 356, LR 0.000069 Loss 6.760252, Accuracy 79.951%\n",
      "Epoch 16, Batch 357, LR 0.000069 Loss 6.757589, Accuracy 79.985%\n",
      "Epoch 16, Batch 358, LR 0.000069 Loss 6.757241, Accuracy 79.997%\n",
      "Epoch 16, Batch 359, LR 0.000069 Loss 6.756730, Accuracy 80.001%\n",
      "Epoch 16, Batch 360, LR 0.000069 Loss 6.757471, Accuracy 79.993%\n",
      "Epoch 16, Batch 361, LR 0.000069 Loss 6.757496, Accuracy 79.993%\n",
      "Epoch 16, Batch 362, LR 0.000069 Loss 6.758530, Accuracy 79.972%\n",
      "Epoch 16, Batch 363, LR 0.000069 Loss 6.756889, Accuracy 79.978%\n",
      "Epoch 16, Batch 364, LR 0.000069 Loss 6.759200, Accuracy 79.964%\n",
      "Epoch 16, Batch 365, LR 0.000069 Loss 6.760552, Accuracy 79.944%\n",
      "Epoch 16, Batch 366, LR 0.000069 Loss 6.761199, Accuracy 79.939%\n",
      "Epoch 16, Batch 367, LR 0.000069 Loss 6.763354, Accuracy 79.926%\n",
      "Epoch 16, Batch 368, LR 0.000069 Loss 6.762790, Accuracy 79.934%\n",
      "Epoch 16, Batch 369, LR 0.000069 Loss 6.763233, Accuracy 79.933%\n",
      "Epoch 16, Batch 370, LR 0.000069 Loss 6.763488, Accuracy 79.943%\n",
      "Epoch 16, Batch 371, LR 0.000069 Loss 6.764388, Accuracy 79.942%\n",
      "Epoch 16, Batch 372, LR 0.000069 Loss 6.764076, Accuracy 79.946%\n",
      "Epoch 16, Batch 373, LR 0.000069 Loss 6.767191, Accuracy 79.922%\n",
      "Epoch 16, Batch 374, LR 0.000069 Loss 6.766688, Accuracy 79.921%\n",
      "Epoch 16, Batch 375, LR 0.000069 Loss 6.766052, Accuracy 79.915%\n",
      "Epoch 16, Batch 376, LR 0.000069 Loss 6.765761, Accuracy 79.922%\n",
      "Epoch 16, Batch 377, LR 0.000069 Loss 6.765571, Accuracy 79.915%\n",
      "Epoch 16, Batch 378, LR 0.000069 Loss 6.764686, Accuracy 79.915%\n",
      "Epoch 16, Batch 379, LR 0.000069 Loss 6.763211, Accuracy 79.931%\n",
      "Epoch 16, Batch 380, LR 0.000069 Loss 6.762375, Accuracy 79.918%\n",
      "Epoch 16, Batch 381, LR 0.000069 Loss 6.762197, Accuracy 79.915%\n",
      "Epoch 16, Batch 382, LR 0.000069 Loss 6.764178, Accuracy 79.908%\n",
      "Epoch 16, Batch 383, LR 0.000069 Loss 6.762841, Accuracy 79.918%\n",
      "Epoch 16, Batch 384, LR 0.000069 Loss 6.763295, Accuracy 79.911%\n",
      "Epoch 16, Batch 385, LR 0.000069 Loss 6.763799, Accuracy 79.905%\n",
      "Epoch 16, Batch 386, LR 0.000069 Loss 6.761404, Accuracy 79.912%\n",
      "Epoch 16, Batch 387, LR 0.000069 Loss 6.761942, Accuracy 79.922%\n",
      "Epoch 16, Batch 388, LR 0.000069 Loss 6.761791, Accuracy 79.929%\n",
      "Epoch 16, Batch 389, LR 0.000069 Loss 6.760578, Accuracy 79.937%\n",
      "Epoch 16, Batch 390, LR 0.000069 Loss 6.759102, Accuracy 79.952%\n",
      "Epoch 16, Batch 391, LR 0.000069 Loss 6.757661, Accuracy 79.957%\n",
      "Epoch 16, Batch 392, LR 0.000069 Loss 6.759506, Accuracy 79.941%\n",
      "Epoch 16, Batch 393, LR 0.000069 Loss 6.760711, Accuracy 79.930%\n",
      "Epoch 16, Batch 394, LR 0.000069 Loss 6.762612, Accuracy 79.921%\n",
      "Epoch 16, Batch 395, LR 0.000069 Loss 6.764519, Accuracy 79.909%\n",
      "Epoch 16, Batch 396, LR 0.000069 Loss 6.764967, Accuracy 79.914%\n",
      "Epoch 16, Batch 397, LR 0.000069 Loss 6.765891, Accuracy 79.910%\n",
      "Epoch 16, Batch 398, LR 0.000069 Loss 6.765724, Accuracy 79.915%\n",
      "Epoch 16, Batch 399, LR 0.000069 Loss 6.764570, Accuracy 79.921%\n",
      "Epoch 16, Batch 400, LR 0.000069 Loss 6.765294, Accuracy 79.908%\n",
      "Epoch 16, Batch 401, LR 0.000069 Loss 6.764763, Accuracy 79.919%\n",
      "Epoch 16, Batch 402, LR 0.000069 Loss 6.762251, Accuracy 79.936%\n",
      "Epoch 16, Batch 403, LR 0.000069 Loss 6.763682, Accuracy 79.922%\n",
      "Epoch 16, Batch 404, LR 0.000069 Loss 6.762846, Accuracy 79.925%\n",
      "Epoch 16, Batch 405, LR 0.000069 Loss 6.760400, Accuracy 79.934%\n",
      "Epoch 16, Batch 406, LR 0.000069 Loss 6.759360, Accuracy 79.940%\n",
      "Epoch 16, Batch 407, LR 0.000069 Loss 6.760263, Accuracy 79.933%\n",
      "Epoch 16, Batch 408, LR 0.000069 Loss 6.760919, Accuracy 79.933%\n",
      "Epoch 16, Batch 409, LR 0.000069 Loss 6.761216, Accuracy 79.930%\n",
      "Epoch 16, Batch 410, LR 0.000069 Loss 6.760244, Accuracy 79.929%\n",
      "Epoch 16, Batch 411, LR 0.000069 Loss 6.759169, Accuracy 79.935%\n",
      "Epoch 16, Batch 412, LR 0.000069 Loss 6.759720, Accuracy 79.932%\n",
      "Epoch 16, Batch 413, LR 0.000069 Loss 6.760722, Accuracy 79.926%\n",
      "Epoch 16, Batch 414, LR 0.000069 Loss 6.762419, Accuracy 79.918%\n",
      "Epoch 16, Batch 415, LR 0.000069 Loss 6.763521, Accuracy 79.912%\n",
      "Epoch 16, Batch 416, LR 0.000069 Loss 6.762643, Accuracy 79.911%\n",
      "Epoch 16, Batch 417, LR 0.000069 Loss 6.760108, Accuracy 79.927%\n",
      "Epoch 16, Batch 418, LR 0.000069 Loss 6.759973, Accuracy 79.927%\n",
      "Epoch 16, Batch 419, LR 0.000069 Loss 6.760832, Accuracy 79.913%\n",
      "Epoch 16, Batch 420, LR 0.000069 Loss 6.761400, Accuracy 79.922%\n",
      "Epoch 16, Batch 421, LR 0.000069 Loss 6.761733, Accuracy 79.919%\n",
      "Epoch 16, Batch 422, LR 0.000069 Loss 6.761524, Accuracy 79.923%\n",
      "Epoch 16, Batch 423, LR 0.000069 Loss 6.760996, Accuracy 79.926%\n",
      "Epoch 16, Batch 424, LR 0.000069 Loss 6.762226, Accuracy 79.920%\n",
      "Epoch 16, Batch 425, LR 0.000069 Loss 6.764054, Accuracy 79.912%\n",
      "Epoch 16, Batch 426, LR 0.000069 Loss 6.762782, Accuracy 79.922%\n",
      "Epoch 16, Batch 427, LR 0.000069 Loss 6.763320, Accuracy 79.918%\n",
      "Epoch 16, Batch 428, LR 0.000069 Loss 6.762544, Accuracy 79.916%\n",
      "Epoch 16, Batch 429, LR 0.000069 Loss 6.763585, Accuracy 79.915%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 430, LR 0.000069 Loss 6.761470, Accuracy 79.922%\n",
      "Epoch 16, Batch 431, LR 0.000069 Loss 6.760623, Accuracy 79.930%\n",
      "Epoch 16, Batch 432, LR 0.000069 Loss 6.761586, Accuracy 79.932%\n",
      "Epoch 16, Batch 433, LR 0.000069 Loss 6.760800, Accuracy 79.940%\n",
      "Epoch 16, Batch 434, LR 0.000069 Loss 6.761902, Accuracy 79.929%\n",
      "Epoch 16, Batch 435, LR 0.000069 Loss 6.764856, Accuracy 79.907%\n",
      "Epoch 16, Batch 436, LR 0.000068 Loss 6.764787, Accuracy 79.917%\n",
      "Epoch 16, Batch 437, LR 0.000068 Loss 6.765541, Accuracy 79.907%\n",
      "Epoch 16, Batch 438, LR 0.000068 Loss 6.762800, Accuracy 79.923%\n",
      "Epoch 16, Batch 439, LR 0.000068 Loss 6.763656, Accuracy 79.921%\n",
      "Epoch 16, Batch 440, LR 0.000068 Loss 6.762814, Accuracy 79.929%\n",
      "Epoch 16, Batch 441, LR 0.000068 Loss 6.765146, Accuracy 79.911%\n",
      "Epoch 16, Batch 442, LR 0.000068 Loss 6.765319, Accuracy 79.891%\n",
      "Epoch 16, Batch 443, LR 0.000068 Loss 6.764190, Accuracy 79.892%\n",
      "Epoch 16, Batch 444, LR 0.000068 Loss 6.765353, Accuracy 79.886%\n",
      "Epoch 16, Batch 445, LR 0.000068 Loss 6.765258, Accuracy 79.888%\n",
      "Epoch 16, Batch 446, LR 0.000068 Loss 6.766457, Accuracy 79.877%\n",
      "Epoch 16, Batch 447, LR 0.000068 Loss 6.764752, Accuracy 79.885%\n",
      "Epoch 16, Batch 448, LR 0.000068 Loss 6.764669, Accuracy 79.893%\n",
      "Epoch 16, Batch 449, LR 0.000068 Loss 6.764704, Accuracy 79.888%\n",
      "Epoch 16, Batch 450, LR 0.000068 Loss 6.763576, Accuracy 79.891%\n",
      "Epoch 16, Batch 451, LR 0.000068 Loss 6.765248, Accuracy 79.876%\n",
      "Epoch 16, Batch 452, LR 0.000068 Loss 6.764930, Accuracy 79.876%\n",
      "Epoch 16, Batch 453, LR 0.000068 Loss 6.764507, Accuracy 79.879%\n",
      "Epoch 16, Batch 454, LR 0.000068 Loss 6.761358, Accuracy 79.891%\n",
      "Epoch 16, Batch 455, LR 0.000068 Loss 6.759754, Accuracy 79.900%\n",
      "Epoch 16, Batch 456, LR 0.000068 Loss 6.761818, Accuracy 79.891%\n",
      "Epoch 16, Batch 457, LR 0.000068 Loss 6.763404, Accuracy 79.886%\n",
      "Epoch 16, Batch 458, LR 0.000068 Loss 6.762510, Accuracy 79.899%\n",
      "Epoch 16, Batch 459, LR 0.000068 Loss 6.761108, Accuracy 79.904%\n",
      "Epoch 16, Batch 460, LR 0.000068 Loss 6.760768, Accuracy 79.905%\n",
      "Epoch 16, Batch 461, LR 0.000068 Loss 6.760685, Accuracy 79.898%\n",
      "Epoch 16, Batch 462, LR 0.000068 Loss 6.759984, Accuracy 79.895%\n",
      "Epoch 16, Batch 463, LR 0.000068 Loss 6.759252, Accuracy 79.900%\n",
      "Epoch 16, Batch 464, LR 0.000068 Loss 6.757071, Accuracy 79.913%\n",
      "Epoch 16, Batch 465, LR 0.000068 Loss 6.755845, Accuracy 79.916%\n",
      "Epoch 16, Batch 466, LR 0.000068 Loss 6.755455, Accuracy 79.919%\n",
      "Epoch 16, Batch 467, LR 0.000068 Loss 6.755317, Accuracy 79.928%\n",
      "Epoch 16, Batch 468, LR 0.000068 Loss 6.755471, Accuracy 79.925%\n",
      "Epoch 16, Batch 469, LR 0.000068 Loss 6.756236, Accuracy 79.927%\n",
      "Epoch 16, Batch 470, LR 0.000068 Loss 6.756905, Accuracy 79.927%\n",
      "Epoch 16, Batch 471, LR 0.000068 Loss 6.757435, Accuracy 79.928%\n",
      "Epoch 16, Batch 472, LR 0.000068 Loss 6.755571, Accuracy 79.932%\n",
      "Epoch 16, Batch 473, LR 0.000068 Loss 6.754037, Accuracy 79.937%\n",
      "Epoch 16, Batch 474, LR 0.000068 Loss 6.751822, Accuracy 79.950%\n",
      "Epoch 16, Batch 475, LR 0.000068 Loss 6.751512, Accuracy 79.954%\n",
      "Epoch 16, Batch 476, LR 0.000068 Loss 6.751699, Accuracy 79.958%\n",
      "Epoch 16, Batch 477, LR 0.000068 Loss 6.751965, Accuracy 79.951%\n",
      "Epoch 16, Batch 478, LR 0.000068 Loss 6.751897, Accuracy 79.960%\n",
      "Epoch 16, Batch 479, LR 0.000068 Loss 6.752230, Accuracy 79.957%\n",
      "Epoch 16, Batch 480, LR 0.000068 Loss 6.753252, Accuracy 79.953%\n",
      "Epoch 16, Batch 481, LR 0.000068 Loss 6.752224, Accuracy 79.955%\n",
      "Epoch 16, Batch 482, LR 0.000068 Loss 6.753678, Accuracy 79.944%\n",
      "Epoch 16, Batch 483, LR 0.000068 Loss 6.754816, Accuracy 79.937%\n",
      "Epoch 16, Batch 484, LR 0.000068 Loss 6.754755, Accuracy 79.938%\n",
      "Epoch 16, Batch 485, LR 0.000068 Loss 6.754178, Accuracy 79.944%\n",
      "Epoch 16, Batch 486, LR 0.000068 Loss 6.753501, Accuracy 79.951%\n",
      "Epoch 16, Batch 487, LR 0.000068 Loss 6.753963, Accuracy 79.951%\n",
      "Epoch 16, Batch 488, LR 0.000068 Loss 6.751963, Accuracy 79.968%\n",
      "Epoch 16, Batch 489, LR 0.000068 Loss 6.750003, Accuracy 79.980%\n",
      "Epoch 16, Batch 490, LR 0.000068 Loss 6.749203, Accuracy 79.982%\n",
      "Epoch 16, Batch 491, LR 0.000068 Loss 6.749907, Accuracy 79.971%\n",
      "Epoch 16, Batch 492, LR 0.000068 Loss 6.749850, Accuracy 79.980%\n",
      "Epoch 16, Batch 493, LR 0.000068 Loss 6.749195, Accuracy 79.976%\n",
      "Epoch 16, Batch 494, LR 0.000068 Loss 6.749077, Accuracy 79.974%\n",
      "Epoch 16, Batch 495, LR 0.000068 Loss 6.747319, Accuracy 79.983%\n",
      "Epoch 16, Batch 496, LR 0.000068 Loss 6.746762, Accuracy 79.998%\n",
      "Epoch 16, Batch 497, LR 0.000068 Loss 6.746814, Accuracy 80.000%\n",
      "Epoch 16, Batch 498, LR 0.000068 Loss 6.747289, Accuracy 79.997%\n",
      "Epoch 16, Batch 499, LR 0.000068 Loss 6.747281, Accuracy 79.996%\n",
      "Epoch 16, Batch 500, LR 0.000068 Loss 6.746867, Accuracy 80.002%\n",
      "Epoch 16, Batch 501, LR 0.000068 Loss 6.746109, Accuracy 80.009%\n",
      "Epoch 16, Batch 502, LR 0.000068 Loss 6.746472, Accuracy 80.010%\n",
      "Epoch 16, Batch 503, LR 0.000068 Loss 6.747244, Accuracy 80.011%\n",
      "Epoch 16, Batch 504, LR 0.000068 Loss 6.748566, Accuracy 79.998%\n",
      "Epoch 16, Batch 505, LR 0.000068 Loss 6.748142, Accuracy 79.994%\n",
      "Epoch 16, Batch 506, LR 0.000068 Loss 6.747519, Accuracy 79.995%\n",
      "Epoch 16, Batch 507, LR 0.000068 Loss 6.747849, Accuracy 79.991%\n",
      "Epoch 16, Batch 508, LR 0.000068 Loss 6.748188, Accuracy 79.983%\n",
      "Epoch 16, Batch 509, LR 0.000068 Loss 6.747386, Accuracy 79.990%\n",
      "Epoch 16, Batch 510, LR 0.000068 Loss 6.747955, Accuracy 79.980%\n",
      "Epoch 16, Batch 511, LR 0.000068 Loss 6.745768, Accuracy 79.996%\n",
      "Epoch 16, Batch 512, LR 0.000068 Loss 6.745493, Accuracy 80.000%\n",
      "Epoch 16, Batch 513, LR 0.000068 Loss 6.745206, Accuracy 79.989%\n",
      "Epoch 16, Batch 514, LR 0.000068 Loss 6.744277, Accuracy 79.996%\n",
      "Epoch 16, Batch 515, LR 0.000068 Loss 6.744340, Accuracy 80.000%\n",
      "Epoch 16, Batch 516, LR 0.000068 Loss 6.745371, Accuracy 80.002%\n",
      "Epoch 16, Batch 517, LR 0.000068 Loss 6.747448, Accuracy 79.991%\n",
      "Epoch 16, Batch 518, LR 0.000068 Loss 6.748846, Accuracy 79.979%\n",
      "Epoch 16, Batch 519, LR 0.000068 Loss 6.748422, Accuracy 79.981%\n",
      "Epoch 16, Batch 520, LR 0.000068 Loss 6.748929, Accuracy 79.976%\n",
      "Epoch 16, Batch 521, LR 0.000068 Loss 6.750259, Accuracy 79.968%\n",
      "Epoch 16, Batch 522, LR 0.000068 Loss 6.749679, Accuracy 79.970%\n",
      "Epoch 16, Batch 523, LR 0.000068 Loss 6.749036, Accuracy 79.968%\n",
      "Epoch 16, Batch 524, LR 0.000068 Loss 6.749760, Accuracy 79.962%\n",
      "Epoch 16, Batch 525, LR 0.000068 Loss 6.749359, Accuracy 79.969%\n",
      "Epoch 16, Batch 526, LR 0.000068 Loss 6.749519, Accuracy 79.974%\n",
      "Epoch 16, Batch 527, LR 0.000068 Loss 6.748220, Accuracy 79.985%\n",
      "Epoch 16, Batch 528, LR 0.000068 Loss 6.749350, Accuracy 79.986%\n",
      "Epoch 16, Batch 529, LR 0.000068 Loss 6.749491, Accuracy 79.990%\n",
      "Epoch 16, Batch 530, LR 0.000068 Loss 6.749133, Accuracy 79.991%\n",
      "Epoch 16, Batch 531, LR 0.000068 Loss 6.747674, Accuracy 79.996%\n",
      "Epoch 16, Batch 532, LR 0.000068 Loss 6.750062, Accuracy 79.975%\n",
      "Epoch 16, Batch 533, LR 0.000068 Loss 6.749792, Accuracy 79.981%\n",
      "Epoch 16, Batch 534, LR 0.000068 Loss 6.750996, Accuracy 79.968%\n",
      "Epoch 16, Batch 535, LR 0.000068 Loss 6.752052, Accuracy 79.965%\n",
      "Epoch 16, Batch 536, LR 0.000068 Loss 6.752038, Accuracy 79.963%\n",
      "Epoch 16, Batch 537, LR 0.000068 Loss 6.751975, Accuracy 79.958%\n",
      "Epoch 16, Batch 538, LR 0.000068 Loss 6.751767, Accuracy 79.965%\n",
      "Epoch 16, Batch 539, LR 0.000068 Loss 6.751200, Accuracy 79.967%\n",
      "Epoch 16, Batch 540, LR 0.000068 Loss 6.750813, Accuracy 79.971%\n",
      "Epoch 16, Batch 541, LR 0.000068 Loss 6.750480, Accuracy 79.975%\n",
      "Epoch 16, Batch 542, LR 0.000068 Loss 6.751245, Accuracy 79.970%\n",
      "Epoch 16, Batch 543, LR 0.000068 Loss 6.752129, Accuracy 79.972%\n",
      "Epoch 16, Batch 544, LR 0.000068 Loss 6.752049, Accuracy 79.969%\n",
      "Epoch 16, Batch 545, LR 0.000068 Loss 6.752535, Accuracy 79.966%\n",
      "Epoch 16, Batch 546, LR 0.000068 Loss 6.752493, Accuracy 79.971%\n",
      "Epoch 16, Batch 547, LR 0.000068 Loss 6.752112, Accuracy 79.967%\n",
      "Epoch 16, Batch 548, LR 0.000068 Loss 6.752441, Accuracy 79.965%\n",
      "Epoch 16, Batch 549, LR 0.000068 Loss 6.754596, Accuracy 79.949%\n",
      "Epoch 16, Batch 550, LR 0.000068 Loss 6.754668, Accuracy 79.942%\n",
      "Epoch 16, Batch 551, LR 0.000068 Loss 6.755348, Accuracy 79.937%\n",
      "Epoch 16, Batch 552, LR 0.000068 Loss 6.756148, Accuracy 79.934%\n",
      "Epoch 16, Batch 553, LR 0.000068 Loss 6.757047, Accuracy 79.933%\n",
      "Epoch 16, Batch 554, LR 0.000068 Loss 6.754910, Accuracy 79.943%\n",
      "Epoch 16, Batch 555, LR 0.000068 Loss 6.754885, Accuracy 79.939%\n",
      "Epoch 16, Batch 556, LR 0.000068 Loss 6.754721, Accuracy 79.935%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 557, LR 0.000068 Loss 6.754615, Accuracy 79.933%\n",
      "Epoch 16, Batch 558, LR 0.000068 Loss 6.755946, Accuracy 79.928%\n",
      "Epoch 16, Batch 559, LR 0.000068 Loss 6.755858, Accuracy 79.924%\n",
      "Epoch 16, Batch 560, LR 0.000068 Loss 6.754708, Accuracy 79.930%\n",
      "Epoch 16, Batch 561, LR 0.000068 Loss 6.753924, Accuracy 79.933%\n",
      "Epoch 16, Batch 562, LR 0.000068 Loss 6.754011, Accuracy 79.925%\n",
      "Epoch 16, Batch 563, LR 0.000068 Loss 6.755849, Accuracy 79.916%\n",
      "Epoch 16, Batch 564, LR 0.000068 Loss 6.754489, Accuracy 79.919%\n",
      "Epoch 16, Batch 565, LR 0.000068 Loss 6.753978, Accuracy 79.921%\n",
      "Epoch 16, Batch 566, LR 0.000068 Loss 6.753918, Accuracy 79.926%\n",
      "Epoch 16, Batch 567, LR 0.000068 Loss 6.753769, Accuracy 79.930%\n",
      "Epoch 16, Batch 568, LR 0.000068 Loss 6.754733, Accuracy 79.931%\n",
      "Epoch 16, Batch 569, LR 0.000068 Loss 6.754059, Accuracy 79.935%\n",
      "Epoch 16, Batch 570, LR 0.000068 Loss 6.754248, Accuracy 79.931%\n",
      "Epoch 16, Batch 571, LR 0.000068 Loss 6.752941, Accuracy 79.941%\n",
      "Epoch 16, Batch 572, LR 0.000068 Loss 6.752338, Accuracy 79.947%\n",
      "Epoch 16, Batch 573, LR 0.000068 Loss 6.751943, Accuracy 79.941%\n",
      "Epoch 16, Batch 574, LR 0.000068 Loss 6.750903, Accuracy 79.939%\n",
      "Epoch 16, Batch 575, LR 0.000068 Loss 6.750347, Accuracy 79.943%\n",
      "Epoch 16, Batch 576, LR 0.000068 Loss 6.749482, Accuracy 79.942%\n",
      "Epoch 16, Batch 577, LR 0.000068 Loss 6.750128, Accuracy 79.934%\n",
      "Epoch 16, Batch 578, LR 0.000068 Loss 6.749387, Accuracy 79.938%\n",
      "Epoch 16, Batch 579, LR 0.000068 Loss 6.749109, Accuracy 79.936%\n",
      "Epoch 16, Batch 580, LR 0.000068 Loss 6.747860, Accuracy 79.952%\n",
      "Epoch 16, Batch 581, LR 0.000068 Loss 6.747483, Accuracy 79.955%\n",
      "Epoch 16, Batch 582, LR 0.000068 Loss 6.746259, Accuracy 79.968%\n",
      "Epoch 16, Batch 583, LR 0.000068 Loss 6.745252, Accuracy 79.972%\n",
      "Epoch 16, Batch 584, LR 0.000068 Loss 6.743886, Accuracy 79.983%\n",
      "Epoch 16, Batch 585, LR 0.000068 Loss 6.743782, Accuracy 79.976%\n",
      "Epoch 16, Batch 586, LR 0.000068 Loss 6.744409, Accuracy 79.969%\n",
      "Epoch 16, Batch 587, LR 0.000068 Loss 6.745268, Accuracy 79.962%\n",
      "Epoch 16, Batch 588, LR 0.000068 Loss 6.745738, Accuracy 79.964%\n",
      "Epoch 16, Batch 589, LR 0.000068 Loss 6.747220, Accuracy 79.958%\n",
      "Epoch 16, Batch 590, LR 0.000068 Loss 6.746503, Accuracy 79.962%\n",
      "Epoch 16, Batch 591, LR 0.000068 Loss 6.745912, Accuracy 79.969%\n",
      "Epoch 16, Batch 592, LR 0.000068 Loss 6.745645, Accuracy 79.971%\n",
      "Epoch 16, Batch 593, LR 0.000068 Loss 6.744947, Accuracy 79.971%\n",
      "Epoch 16, Batch 594, LR 0.000068 Loss 6.744539, Accuracy 79.977%\n",
      "Epoch 16, Batch 595, LR 0.000068 Loss 6.743876, Accuracy 79.978%\n",
      "Epoch 16, Batch 596, LR 0.000068 Loss 6.744440, Accuracy 79.971%\n",
      "Epoch 16, Batch 597, LR 0.000068 Loss 6.745460, Accuracy 79.968%\n",
      "Epoch 16, Batch 598, LR 0.000068 Loss 6.745897, Accuracy 79.955%\n",
      "Epoch 16, Batch 599, LR 0.000068 Loss 6.745386, Accuracy 79.959%\n",
      "Epoch 16, Batch 600, LR 0.000068 Loss 6.746076, Accuracy 79.952%\n",
      "Epoch 16, Batch 601, LR 0.000068 Loss 6.745906, Accuracy 79.947%\n",
      "Epoch 16, Batch 602, LR 0.000068 Loss 6.746157, Accuracy 79.951%\n",
      "Epoch 16, Batch 603, LR 0.000068 Loss 6.746874, Accuracy 79.944%\n",
      "Epoch 16, Batch 604, LR 0.000068 Loss 6.746919, Accuracy 79.945%\n",
      "Epoch 16, Batch 605, LR 0.000068 Loss 6.748290, Accuracy 79.932%\n",
      "Epoch 16, Batch 606, LR 0.000068 Loss 6.749309, Accuracy 79.930%\n",
      "Epoch 16, Batch 607, LR 0.000068 Loss 6.749522, Accuracy 79.931%\n",
      "Epoch 16, Batch 608, LR 0.000068 Loss 6.749382, Accuracy 79.933%\n",
      "Epoch 16, Batch 609, LR 0.000068 Loss 6.749200, Accuracy 79.927%\n",
      "Epoch 16, Batch 610, LR 0.000068 Loss 6.748778, Accuracy 79.933%\n",
      "Epoch 16, Batch 611, LR 0.000068 Loss 6.748120, Accuracy 79.947%\n",
      "Epoch 16, Batch 612, LR 0.000068 Loss 6.749496, Accuracy 79.940%\n",
      "Epoch 16, Batch 613, LR 0.000068 Loss 6.748876, Accuracy 79.946%\n",
      "Epoch 16, Batch 614, LR 0.000068 Loss 6.748453, Accuracy 79.947%\n",
      "Epoch 16, Batch 615, LR 0.000068 Loss 6.749472, Accuracy 79.940%\n",
      "Epoch 16, Batch 616, LR 0.000068 Loss 6.748251, Accuracy 79.945%\n",
      "Epoch 16, Batch 617, LR 0.000068 Loss 6.748469, Accuracy 79.941%\n",
      "Epoch 16, Batch 618, LR 0.000068 Loss 6.747302, Accuracy 79.949%\n",
      "Epoch 16, Batch 619, LR 0.000068 Loss 6.748085, Accuracy 79.953%\n",
      "Epoch 16, Batch 620, LR 0.000068 Loss 6.749842, Accuracy 79.942%\n",
      "Epoch 16, Batch 621, LR 0.000068 Loss 6.750667, Accuracy 79.937%\n",
      "Epoch 16, Batch 622, LR 0.000068 Loss 6.751460, Accuracy 79.930%\n",
      "Epoch 16, Batch 623, LR 0.000068 Loss 6.751620, Accuracy 79.928%\n",
      "Epoch 16, Batch 624, LR 0.000068 Loss 6.751355, Accuracy 79.933%\n",
      "Epoch 16, Batch 625, LR 0.000068 Loss 6.752529, Accuracy 79.926%\n",
      "Epoch 16, Batch 626, LR 0.000068 Loss 6.752229, Accuracy 79.931%\n",
      "Epoch 16, Batch 627, LR 0.000068 Loss 6.752913, Accuracy 79.928%\n",
      "Epoch 16, Batch 628, LR 0.000068 Loss 6.753835, Accuracy 79.928%\n",
      "Epoch 16, Batch 629, LR 0.000068 Loss 6.754278, Accuracy 79.923%\n",
      "Epoch 16, Batch 630, LR 0.000068 Loss 6.754283, Accuracy 79.921%\n",
      "Epoch 16, Batch 631, LR 0.000068 Loss 6.754004, Accuracy 79.923%\n",
      "Epoch 16, Batch 632, LR 0.000068 Loss 6.754594, Accuracy 79.920%\n",
      "Epoch 16, Batch 633, LR 0.000068 Loss 6.753191, Accuracy 79.921%\n",
      "Epoch 16, Batch 634, LR 0.000068 Loss 6.753342, Accuracy 79.917%\n",
      "Epoch 16, Batch 635, LR 0.000068 Loss 6.754092, Accuracy 79.920%\n",
      "Epoch 16, Batch 636, LR 0.000068 Loss 6.753668, Accuracy 79.920%\n",
      "Epoch 16, Batch 637, LR 0.000068 Loss 6.754361, Accuracy 79.917%\n",
      "Epoch 16, Batch 638, LR 0.000068 Loss 6.752568, Accuracy 79.924%\n",
      "Epoch 16, Batch 639, LR 0.000068 Loss 6.753367, Accuracy 79.920%\n",
      "Epoch 16, Batch 640, LR 0.000068 Loss 6.753212, Accuracy 79.924%\n",
      "Epoch 16, Batch 641, LR 0.000068 Loss 6.754042, Accuracy 79.915%\n",
      "Epoch 16, Batch 642, LR 0.000068 Loss 6.753567, Accuracy 79.911%\n",
      "Epoch 16, Batch 643, LR 0.000068 Loss 6.752859, Accuracy 79.916%\n",
      "Epoch 16, Batch 644, LR 0.000068 Loss 6.753224, Accuracy 79.924%\n",
      "Epoch 16, Batch 645, LR 0.000068 Loss 6.752736, Accuracy 79.919%\n",
      "Epoch 16, Batch 646, LR 0.000068 Loss 6.752307, Accuracy 79.916%\n",
      "Epoch 16, Batch 647, LR 0.000068 Loss 6.753604, Accuracy 79.910%\n",
      "Epoch 16, Batch 648, LR 0.000068 Loss 6.753994, Accuracy 79.917%\n",
      "Epoch 16, Batch 649, LR 0.000068 Loss 6.753605, Accuracy 79.914%\n",
      "Epoch 16, Batch 650, LR 0.000068 Loss 6.752984, Accuracy 79.918%\n",
      "Epoch 16, Batch 651, LR 0.000068 Loss 6.751901, Accuracy 79.918%\n",
      "Epoch 16, Batch 652, LR 0.000068 Loss 6.752250, Accuracy 79.918%\n",
      "Epoch 16, Batch 653, LR 0.000068 Loss 6.751023, Accuracy 79.927%\n",
      "Epoch 16, Batch 654, LR 0.000068 Loss 6.749994, Accuracy 79.931%\n",
      "Epoch 16, Batch 655, LR 0.000068 Loss 6.750928, Accuracy 79.932%\n",
      "Epoch 16, Batch 656, LR 0.000068 Loss 6.749611, Accuracy 79.930%\n",
      "Epoch 16, Batch 657, LR 0.000068 Loss 6.748699, Accuracy 79.936%\n",
      "Epoch 16, Batch 658, LR 0.000068 Loss 6.747943, Accuracy 79.944%\n",
      "Epoch 16, Batch 659, LR 0.000068 Loss 6.748391, Accuracy 79.939%\n",
      "Epoch 16, Batch 660, LR 0.000068 Loss 6.747569, Accuracy 79.946%\n",
      "Epoch 16, Batch 661, LR 0.000068 Loss 6.748147, Accuracy 79.946%\n",
      "Epoch 16, Batch 662, LR 0.000068 Loss 6.748458, Accuracy 79.939%\n",
      "Epoch 16, Batch 663, LR 0.000068 Loss 6.748469, Accuracy 79.943%\n",
      "Epoch 16, Batch 664, LR 0.000068 Loss 6.748298, Accuracy 79.946%\n",
      "Epoch 16, Batch 665, LR 0.000068 Loss 6.748181, Accuracy 79.951%\n",
      "Epoch 16, Batch 666, LR 0.000068 Loss 6.747999, Accuracy 79.956%\n",
      "Epoch 16, Batch 667, LR 0.000068 Loss 6.747929, Accuracy 79.965%\n",
      "Epoch 16, Batch 668, LR 0.000068 Loss 6.746813, Accuracy 79.975%\n",
      "Epoch 16, Batch 669, LR 0.000068 Loss 6.745416, Accuracy 79.985%\n",
      "Epoch 16, Batch 670, LR 0.000068 Loss 6.744407, Accuracy 79.991%\n",
      "Epoch 16, Batch 671, LR 0.000068 Loss 6.743264, Accuracy 79.997%\n",
      "Epoch 16, Batch 672, LR 0.000068 Loss 6.743215, Accuracy 79.996%\n",
      "Epoch 16, Batch 673, LR 0.000068 Loss 6.742326, Accuracy 79.999%\n",
      "Epoch 16, Batch 674, LR 0.000068 Loss 6.741414, Accuracy 80.004%\n",
      "Epoch 16, Batch 675, LR 0.000068 Loss 6.742572, Accuracy 79.997%\n",
      "Epoch 16, Batch 676, LR 0.000068 Loss 6.742782, Accuracy 80.000%\n",
      "Epoch 16, Batch 677, LR 0.000068 Loss 6.742030, Accuracy 80.000%\n",
      "Epoch 16, Batch 678, LR 0.000068 Loss 6.741080, Accuracy 80.002%\n",
      "Epoch 16, Batch 679, LR 0.000068 Loss 6.741174, Accuracy 80.002%\n",
      "Epoch 16, Batch 680, LR 0.000068 Loss 6.740344, Accuracy 80.003%\n",
      "Epoch 16, Batch 681, LR 0.000068 Loss 6.739777, Accuracy 80.012%\n",
      "Epoch 16, Batch 682, LR 0.000068 Loss 6.739078, Accuracy 80.013%\n",
      "Epoch 16, Batch 683, LR 0.000068 Loss 6.739027, Accuracy 80.010%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 684, LR 0.000068 Loss 6.738726, Accuracy 80.015%\n",
      "Epoch 16, Batch 685, LR 0.000068 Loss 6.737960, Accuracy 80.016%\n",
      "Epoch 16, Batch 686, LR 0.000068 Loss 6.737073, Accuracy 80.025%\n",
      "Epoch 16, Batch 687, LR 0.000068 Loss 6.735869, Accuracy 80.029%\n",
      "Epoch 16, Batch 688, LR 0.000068 Loss 6.736618, Accuracy 80.024%\n",
      "Epoch 16, Batch 689, LR 0.000068 Loss 6.737606, Accuracy 80.021%\n",
      "Epoch 16, Batch 690, LR 0.000068 Loss 6.736113, Accuracy 80.024%\n",
      "Epoch 16, Batch 691, LR 0.000068 Loss 6.736685, Accuracy 80.021%\n",
      "Epoch 16, Batch 692, LR 0.000068 Loss 6.737159, Accuracy 80.014%\n",
      "Epoch 16, Batch 693, LR 0.000068 Loss 6.737200, Accuracy 80.017%\n",
      "Epoch 16, Batch 694, LR 0.000068 Loss 6.737198, Accuracy 80.026%\n",
      "Epoch 16, Batch 695, LR 0.000068 Loss 6.738335, Accuracy 80.017%\n",
      "Epoch 16, Batch 696, LR 0.000068 Loss 6.738492, Accuracy 80.013%\n",
      "Epoch 16, Batch 697, LR 0.000068 Loss 6.738634, Accuracy 80.009%\n",
      "Epoch 16, Batch 698, LR 0.000068 Loss 6.738439, Accuracy 80.006%\n",
      "Epoch 16, Batch 699, LR 0.000068 Loss 6.739253, Accuracy 79.996%\n",
      "Epoch 16, Batch 700, LR 0.000068 Loss 6.739614, Accuracy 79.998%\n",
      "Epoch 16, Batch 701, LR 0.000068 Loss 6.739293, Accuracy 80.000%\n",
      "Epoch 16, Batch 702, LR 0.000068 Loss 6.739759, Accuracy 79.999%\n",
      "Epoch 16, Batch 703, LR 0.000068 Loss 6.739536, Accuracy 79.995%\n",
      "Epoch 16, Batch 704, LR 0.000068 Loss 6.738823, Accuracy 79.993%\n",
      "Epoch 16, Batch 705, LR 0.000068 Loss 6.737759, Accuracy 80.000%\n",
      "Epoch 16, Batch 706, LR 0.000068 Loss 6.738085, Accuracy 80.002%\n",
      "Epoch 16, Batch 707, LR 0.000067 Loss 6.738869, Accuracy 79.991%\n",
      "Epoch 16, Batch 708, LR 0.000067 Loss 6.737475, Accuracy 79.996%\n",
      "Epoch 16, Batch 709, LR 0.000067 Loss 6.736689, Accuracy 80.002%\n",
      "Epoch 16, Batch 710, LR 0.000067 Loss 6.734824, Accuracy 80.009%\n",
      "Epoch 16, Batch 711, LR 0.000067 Loss 6.735564, Accuracy 80.011%\n",
      "Epoch 16, Batch 712, LR 0.000067 Loss 6.735878, Accuracy 80.012%\n",
      "Epoch 16, Batch 713, LR 0.000067 Loss 6.736010, Accuracy 80.011%\n",
      "Epoch 16, Batch 714, LR 0.000067 Loss 6.736347, Accuracy 80.007%\n",
      "Epoch 16, Batch 715, LR 0.000067 Loss 6.736456, Accuracy 80.004%\n",
      "Epoch 16, Batch 716, LR 0.000067 Loss 6.735511, Accuracy 80.005%\n",
      "Epoch 16, Batch 717, LR 0.000067 Loss 6.736185, Accuracy 80.003%\n",
      "Epoch 16, Batch 718, LR 0.000067 Loss 6.734978, Accuracy 80.007%\n",
      "Epoch 16, Batch 719, LR 0.000067 Loss 6.735940, Accuracy 80.004%\n",
      "Epoch 16, Batch 720, LR 0.000067 Loss 6.735565, Accuracy 80.008%\n",
      "Epoch 16, Batch 721, LR 0.000067 Loss 6.734123, Accuracy 80.020%\n",
      "Epoch 16, Batch 722, LR 0.000067 Loss 6.734654, Accuracy 80.016%\n",
      "Epoch 16, Batch 723, LR 0.000067 Loss 6.735375, Accuracy 80.011%\n",
      "Epoch 16, Batch 724, LR 0.000067 Loss 6.735647, Accuracy 80.007%\n",
      "Epoch 16, Batch 725, LR 0.000067 Loss 6.735110, Accuracy 80.009%\n",
      "Epoch 16, Batch 726, LR 0.000067 Loss 6.734713, Accuracy 80.010%\n",
      "Epoch 16, Batch 727, LR 0.000067 Loss 6.735794, Accuracy 80.007%\n",
      "Epoch 16, Batch 728, LR 0.000067 Loss 6.736329, Accuracy 80.006%\n",
      "Epoch 16, Batch 729, LR 0.000067 Loss 6.735636, Accuracy 80.004%\n",
      "Epoch 16, Batch 730, LR 0.000067 Loss 6.737173, Accuracy 79.993%\n",
      "Epoch 16, Batch 731, LR 0.000067 Loss 6.736571, Accuracy 80.000%\n",
      "Epoch 16, Batch 732, LR 0.000067 Loss 6.735709, Accuracy 80.008%\n",
      "Epoch 16, Batch 733, LR 0.000067 Loss 6.735870, Accuracy 80.005%\n",
      "Epoch 16, Batch 734, LR 0.000067 Loss 6.735724, Accuracy 80.013%\n",
      "Epoch 16, Batch 735, LR 0.000067 Loss 6.736536, Accuracy 80.004%\n",
      "Epoch 16, Batch 736, LR 0.000067 Loss 6.736876, Accuracy 80.002%\n",
      "Epoch 16, Batch 737, LR 0.000067 Loss 6.736104, Accuracy 80.004%\n",
      "Epoch 16, Batch 738, LR 0.000067 Loss 6.735334, Accuracy 80.015%\n",
      "Epoch 16, Batch 739, LR 0.000067 Loss 6.733700, Accuracy 80.017%\n",
      "Epoch 16, Batch 740, LR 0.000067 Loss 6.734101, Accuracy 80.010%\n",
      "Epoch 16, Batch 741, LR 0.000067 Loss 6.734328, Accuracy 80.008%\n",
      "Epoch 16, Batch 742, LR 0.000067 Loss 6.734311, Accuracy 80.010%\n",
      "Epoch 16, Batch 743, LR 0.000067 Loss 6.734950, Accuracy 80.013%\n",
      "Epoch 16, Batch 744, LR 0.000067 Loss 6.735767, Accuracy 80.005%\n",
      "Epoch 16, Batch 745, LR 0.000067 Loss 6.735787, Accuracy 79.999%\n",
      "Epoch 16, Batch 746, LR 0.000067 Loss 6.735463, Accuracy 80.007%\n",
      "Epoch 16, Batch 747, LR 0.000067 Loss 6.734371, Accuracy 80.012%\n",
      "Epoch 16, Batch 748, LR 0.000067 Loss 6.735012, Accuracy 80.011%\n",
      "Epoch 16, Batch 749, LR 0.000067 Loss 6.734444, Accuracy 80.009%\n",
      "Epoch 16, Batch 750, LR 0.000067 Loss 6.733223, Accuracy 80.018%\n",
      "Epoch 16, Batch 751, LR 0.000067 Loss 6.733687, Accuracy 80.018%\n",
      "Epoch 16, Batch 752, LR 0.000067 Loss 6.732686, Accuracy 80.024%\n",
      "Epoch 16, Batch 753, LR 0.000067 Loss 6.732286, Accuracy 80.029%\n",
      "Epoch 16, Batch 754, LR 0.000067 Loss 6.732094, Accuracy 80.027%\n",
      "Epoch 16, Batch 755, LR 0.000067 Loss 6.731914, Accuracy 80.029%\n",
      "Epoch 16, Batch 756, LR 0.000067 Loss 6.730871, Accuracy 80.034%\n",
      "Epoch 16, Batch 757, LR 0.000067 Loss 6.729906, Accuracy 80.039%\n",
      "Epoch 16, Batch 758, LR 0.000067 Loss 6.730024, Accuracy 80.042%\n",
      "Epoch 16, Batch 759, LR 0.000067 Loss 6.729333, Accuracy 80.048%\n",
      "Epoch 16, Batch 760, LR 0.000067 Loss 6.729628, Accuracy 80.050%\n",
      "Epoch 16, Batch 761, LR 0.000067 Loss 6.730554, Accuracy 80.043%\n",
      "Epoch 16, Batch 762, LR 0.000067 Loss 6.731232, Accuracy 80.036%\n",
      "Epoch 16, Batch 763, LR 0.000067 Loss 6.731119, Accuracy 80.035%\n",
      "Epoch 16, Batch 764, LR 0.000067 Loss 6.731879, Accuracy 80.033%\n",
      "Epoch 16, Batch 765, LR 0.000067 Loss 6.732131, Accuracy 80.032%\n",
      "Epoch 16, Batch 766, LR 0.000067 Loss 6.731453, Accuracy 80.036%\n",
      "Epoch 16, Batch 767, LR 0.000067 Loss 6.730248, Accuracy 80.040%\n",
      "Epoch 16, Batch 768, LR 0.000067 Loss 6.730123, Accuracy 80.038%\n",
      "Epoch 16, Batch 769, LR 0.000067 Loss 6.729958, Accuracy 80.038%\n",
      "Epoch 16, Batch 770, LR 0.000067 Loss 6.730001, Accuracy 80.038%\n",
      "Epoch 16, Batch 771, LR 0.000067 Loss 6.729788, Accuracy 80.038%\n",
      "Epoch 16, Batch 772, LR 0.000067 Loss 6.730082, Accuracy 80.034%\n",
      "Epoch 16, Batch 773, LR 0.000067 Loss 6.730280, Accuracy 80.035%\n",
      "Epoch 16, Batch 774, LR 0.000067 Loss 6.730332, Accuracy 80.034%\n",
      "Epoch 16, Batch 775, LR 0.000067 Loss 6.730442, Accuracy 80.027%\n",
      "Epoch 16, Batch 776, LR 0.000067 Loss 6.730469, Accuracy 80.030%\n",
      "Epoch 16, Batch 777, LR 0.000067 Loss 6.731418, Accuracy 80.028%\n",
      "Epoch 16, Batch 778, LR 0.000067 Loss 6.731753, Accuracy 80.026%\n",
      "Epoch 16, Batch 779, LR 0.000067 Loss 6.731550, Accuracy 80.022%\n",
      "Epoch 16, Batch 780, LR 0.000067 Loss 6.730730, Accuracy 80.026%\n",
      "Epoch 16, Batch 781, LR 0.000067 Loss 6.730113, Accuracy 80.031%\n",
      "Epoch 16, Batch 782, LR 0.000067 Loss 6.730520, Accuracy 80.028%\n",
      "Epoch 16, Batch 783, LR 0.000067 Loss 6.729446, Accuracy 80.038%\n",
      "Epoch 16, Batch 784, LR 0.000067 Loss 6.729474, Accuracy 80.037%\n",
      "Epoch 16, Batch 785, LR 0.000067 Loss 6.729397, Accuracy 80.035%\n",
      "Epoch 16, Batch 786, LR 0.000067 Loss 6.728987, Accuracy 80.036%\n",
      "Epoch 16, Batch 787, LR 0.000067 Loss 6.728527, Accuracy 80.041%\n",
      "Epoch 16, Batch 788, LR 0.000067 Loss 6.728325, Accuracy 80.040%\n",
      "Epoch 16, Batch 789, LR 0.000067 Loss 6.727586, Accuracy 80.040%\n",
      "Epoch 16, Batch 790, LR 0.000067 Loss 6.726688, Accuracy 80.044%\n",
      "Epoch 16, Batch 791, LR 0.000067 Loss 6.727094, Accuracy 80.042%\n",
      "Epoch 16, Batch 792, LR 0.000067 Loss 6.726649, Accuracy 80.048%\n",
      "Epoch 16, Batch 793, LR 0.000067 Loss 6.726623, Accuracy 80.051%\n",
      "Epoch 16, Batch 794, LR 0.000067 Loss 6.726198, Accuracy 80.055%\n",
      "Epoch 16, Batch 795, LR 0.000067 Loss 6.726306, Accuracy 80.058%\n",
      "Epoch 16, Batch 796, LR 0.000067 Loss 6.726861, Accuracy 80.060%\n",
      "Epoch 16, Batch 797, LR 0.000067 Loss 6.726715, Accuracy 80.062%\n",
      "Epoch 16, Batch 798, LR 0.000067 Loss 6.727208, Accuracy 80.059%\n",
      "Epoch 16, Batch 799, LR 0.000067 Loss 6.726878, Accuracy 80.059%\n",
      "Epoch 16, Batch 800, LR 0.000067 Loss 6.726781, Accuracy 80.061%\n",
      "Epoch 16, Batch 801, LR 0.000067 Loss 6.726148, Accuracy 80.062%\n",
      "Epoch 16, Batch 802, LR 0.000067 Loss 6.725835, Accuracy 80.064%\n",
      "Epoch 16, Batch 803, LR 0.000067 Loss 6.726564, Accuracy 80.063%\n",
      "Epoch 16, Batch 804, LR 0.000067 Loss 6.727006, Accuracy 80.058%\n",
      "Epoch 16, Batch 805, LR 0.000067 Loss 6.727791, Accuracy 80.057%\n",
      "Epoch 16, Batch 806, LR 0.000067 Loss 6.727757, Accuracy 80.054%\n",
      "Epoch 16, Batch 807, LR 0.000067 Loss 6.726753, Accuracy 80.060%\n",
      "Epoch 16, Batch 808, LR 0.000067 Loss 6.725767, Accuracy 80.064%\n",
      "Epoch 16, Batch 809, LR 0.000067 Loss 6.724904, Accuracy 80.063%\n",
      "Epoch 16, Batch 810, LR 0.000067 Loss 6.724370, Accuracy 80.067%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 811, LR 0.000067 Loss 6.723471, Accuracy 80.072%\n",
      "Epoch 16, Batch 812, LR 0.000067 Loss 6.722793, Accuracy 80.079%\n",
      "Epoch 16, Batch 813, LR 0.000067 Loss 6.721836, Accuracy 80.087%\n",
      "Epoch 16, Batch 814, LR 0.000067 Loss 6.722158, Accuracy 80.076%\n",
      "Epoch 16, Batch 815, LR 0.000067 Loss 6.721912, Accuracy 80.076%\n",
      "Epoch 16, Batch 816, LR 0.000067 Loss 6.721320, Accuracy 80.076%\n",
      "Epoch 16, Batch 817, LR 0.000067 Loss 6.722384, Accuracy 80.067%\n",
      "Epoch 16, Batch 818, LR 0.000067 Loss 6.722122, Accuracy 80.064%\n",
      "Epoch 16, Batch 819, LR 0.000067 Loss 6.722346, Accuracy 80.062%\n",
      "Epoch 16, Batch 820, LR 0.000067 Loss 6.721329, Accuracy 80.064%\n",
      "Epoch 16, Batch 821, LR 0.000067 Loss 6.720882, Accuracy 80.067%\n",
      "Epoch 16, Batch 822, LR 0.000067 Loss 6.720419, Accuracy 80.065%\n",
      "Epoch 16, Batch 823, LR 0.000067 Loss 6.719784, Accuracy 80.069%\n",
      "Epoch 16, Batch 824, LR 0.000067 Loss 6.719111, Accuracy 80.078%\n",
      "Epoch 16, Batch 825, LR 0.000067 Loss 6.718458, Accuracy 80.081%\n",
      "Epoch 16, Batch 826, LR 0.000067 Loss 6.717468, Accuracy 80.089%\n",
      "Epoch 16, Batch 827, LR 0.000067 Loss 6.717258, Accuracy 80.093%\n",
      "Epoch 16, Batch 828, LR 0.000067 Loss 6.717260, Accuracy 80.097%\n",
      "Epoch 16, Batch 829, LR 0.000067 Loss 6.716977, Accuracy 80.097%\n",
      "Epoch 16, Batch 830, LR 0.000067 Loss 6.717711, Accuracy 80.092%\n",
      "Epoch 16, Batch 831, LR 0.000067 Loss 6.718157, Accuracy 80.092%\n",
      "Epoch 16, Batch 832, LR 0.000067 Loss 6.717814, Accuracy 80.094%\n",
      "Epoch 16, Batch 833, LR 0.000067 Loss 6.717933, Accuracy 80.097%\n",
      "Epoch 16, Batch 834, LR 0.000067 Loss 6.718579, Accuracy 80.094%\n",
      "Epoch 16, Batch 835, LR 0.000067 Loss 6.717972, Accuracy 80.095%\n",
      "Epoch 16, Batch 836, LR 0.000067 Loss 6.717655, Accuracy 80.097%\n",
      "Epoch 16, Batch 837, LR 0.000067 Loss 6.717140, Accuracy 80.100%\n",
      "Epoch 16, Batch 838, LR 0.000067 Loss 6.716484, Accuracy 80.105%\n",
      "Epoch 16, Batch 839, LR 0.000067 Loss 6.715975, Accuracy 80.103%\n",
      "Epoch 16, Batch 840, LR 0.000067 Loss 6.714733, Accuracy 80.111%\n",
      "Epoch 16, Batch 841, LR 0.000067 Loss 6.715680, Accuracy 80.100%\n",
      "Epoch 16, Batch 842, LR 0.000067 Loss 6.715771, Accuracy 80.099%\n",
      "Epoch 16, Batch 843, LR 0.000067 Loss 6.715677, Accuracy 80.103%\n",
      "Epoch 16, Batch 844, LR 0.000067 Loss 6.716107, Accuracy 80.099%\n",
      "Epoch 16, Batch 845, LR 0.000067 Loss 6.716144, Accuracy 80.092%\n",
      "Epoch 16, Batch 846, LR 0.000067 Loss 6.715545, Accuracy 80.096%\n",
      "Epoch 16, Batch 847, LR 0.000067 Loss 6.715735, Accuracy 80.096%\n",
      "Epoch 16, Batch 848, LR 0.000067 Loss 6.715484, Accuracy 80.097%\n",
      "Epoch 16, Batch 849, LR 0.000067 Loss 6.715814, Accuracy 80.103%\n",
      "Epoch 16, Batch 850, LR 0.000067 Loss 6.715450, Accuracy 80.106%\n",
      "Epoch 16, Batch 851, LR 0.000067 Loss 6.715274, Accuracy 80.107%\n",
      "Epoch 16, Batch 852, LR 0.000067 Loss 6.715703, Accuracy 80.104%\n",
      "Epoch 16, Batch 853, LR 0.000067 Loss 6.714799, Accuracy 80.108%\n",
      "Epoch 16, Batch 854, LR 0.000067 Loss 6.715421, Accuracy 80.098%\n",
      "Epoch 16, Batch 855, LR 0.000067 Loss 6.714989, Accuracy 80.104%\n",
      "Epoch 16, Batch 856, LR 0.000067 Loss 6.714395, Accuracy 80.103%\n",
      "Epoch 16, Batch 857, LR 0.000067 Loss 6.714223, Accuracy 80.099%\n",
      "Epoch 16, Batch 858, LR 0.000067 Loss 6.713304, Accuracy 80.101%\n",
      "Epoch 16, Batch 859, LR 0.000067 Loss 6.713770, Accuracy 80.098%\n",
      "Epoch 16, Batch 860, LR 0.000067 Loss 6.713446, Accuracy 80.094%\n",
      "Epoch 16, Batch 861, LR 0.000067 Loss 6.713680, Accuracy 80.094%\n",
      "Epoch 16, Batch 862, LR 0.000067 Loss 6.714697, Accuracy 80.089%\n",
      "Epoch 16, Batch 863, LR 0.000067 Loss 6.714281, Accuracy 80.092%\n",
      "Epoch 16, Batch 864, LR 0.000067 Loss 6.714466, Accuracy 80.094%\n",
      "Epoch 16, Batch 865, LR 0.000067 Loss 6.713901, Accuracy 80.099%\n",
      "Epoch 16, Batch 866, LR 0.000067 Loss 6.713295, Accuracy 80.101%\n",
      "Epoch 16, Batch 867, LR 0.000067 Loss 6.713162, Accuracy 80.097%\n",
      "Epoch 16, Batch 868, LR 0.000067 Loss 6.713777, Accuracy 80.099%\n",
      "Epoch 16, Batch 869, LR 0.000067 Loss 6.713846, Accuracy 80.096%\n",
      "Epoch 16, Batch 870, LR 0.000067 Loss 6.713295, Accuracy 80.095%\n",
      "Epoch 16, Batch 871, LR 0.000067 Loss 6.712927, Accuracy 80.097%\n",
      "Epoch 16, Batch 872, LR 0.000067 Loss 6.712863, Accuracy 80.098%\n",
      "Epoch 16, Batch 873, LR 0.000067 Loss 6.713175, Accuracy 80.096%\n",
      "Epoch 16, Batch 874, LR 0.000067 Loss 6.713778, Accuracy 80.092%\n",
      "Epoch 16, Batch 875, LR 0.000067 Loss 6.713690, Accuracy 80.088%\n",
      "Epoch 16, Batch 876, LR 0.000067 Loss 6.714111, Accuracy 80.083%\n",
      "Epoch 16, Batch 877, LR 0.000067 Loss 6.713799, Accuracy 80.088%\n",
      "Epoch 16, Batch 878, LR 0.000067 Loss 6.713858, Accuracy 80.088%\n",
      "Epoch 16, Batch 879, LR 0.000067 Loss 6.713237, Accuracy 80.092%\n",
      "Epoch 16, Batch 880, LR 0.000067 Loss 6.714075, Accuracy 80.085%\n",
      "Epoch 16, Batch 881, LR 0.000067 Loss 6.713409, Accuracy 80.085%\n",
      "Epoch 16, Batch 882, LR 0.000067 Loss 6.712761, Accuracy 80.089%\n",
      "Epoch 16, Batch 883, LR 0.000067 Loss 6.713697, Accuracy 80.083%\n",
      "Epoch 16, Batch 884, LR 0.000067 Loss 6.713711, Accuracy 80.086%\n",
      "Epoch 16, Batch 885, LR 0.000067 Loss 6.713571, Accuracy 80.088%\n",
      "Epoch 16, Batch 886, LR 0.000067 Loss 6.713112, Accuracy 80.091%\n",
      "Epoch 16, Batch 887, LR 0.000067 Loss 6.712382, Accuracy 80.090%\n",
      "Epoch 16, Batch 888, LR 0.000067 Loss 6.712673, Accuracy 80.086%\n",
      "Epoch 16, Batch 889, LR 0.000067 Loss 6.713085, Accuracy 80.087%\n",
      "Epoch 16, Batch 890, LR 0.000067 Loss 6.712891, Accuracy 80.089%\n",
      "Epoch 16, Batch 891, LR 0.000067 Loss 6.712966, Accuracy 80.090%\n",
      "Epoch 16, Batch 892, LR 0.000067 Loss 6.711939, Accuracy 80.101%\n",
      "Epoch 16, Batch 893, LR 0.000067 Loss 6.711621, Accuracy 80.102%\n",
      "Epoch 16, Batch 894, LR 0.000067 Loss 6.711699, Accuracy 80.104%\n",
      "Epoch 16, Batch 895, LR 0.000067 Loss 6.711111, Accuracy 80.110%\n",
      "Epoch 16, Batch 896, LR 0.000067 Loss 6.711110, Accuracy 80.108%\n",
      "Epoch 16, Batch 897, LR 0.000067 Loss 6.710498, Accuracy 80.109%\n",
      "Epoch 16, Batch 898, LR 0.000067 Loss 6.708802, Accuracy 80.118%\n",
      "Epoch 16, Batch 899, LR 0.000067 Loss 6.708665, Accuracy 80.119%\n",
      "Epoch 16, Batch 900, LR 0.000067 Loss 6.709608, Accuracy 80.116%\n",
      "Epoch 16, Batch 901, LR 0.000067 Loss 6.709429, Accuracy 80.118%\n",
      "Epoch 16, Batch 902, LR 0.000067 Loss 6.709575, Accuracy 80.122%\n",
      "Epoch 16, Batch 903, LR 0.000067 Loss 6.710342, Accuracy 80.117%\n",
      "Epoch 16, Batch 904, LR 0.000067 Loss 6.710203, Accuracy 80.116%\n",
      "Epoch 16, Batch 905, LR 0.000067 Loss 6.710156, Accuracy 80.118%\n",
      "Epoch 16, Batch 906, LR 0.000067 Loss 6.710670, Accuracy 80.119%\n",
      "Epoch 16, Batch 907, LR 0.000067 Loss 6.710769, Accuracy 80.118%\n",
      "Epoch 16, Batch 908, LR 0.000067 Loss 6.710902, Accuracy 80.116%\n",
      "Epoch 16, Batch 909, LR 0.000067 Loss 6.711758, Accuracy 80.117%\n",
      "Epoch 16, Batch 910, LR 0.000067 Loss 6.712280, Accuracy 80.115%\n",
      "Epoch 16, Batch 911, LR 0.000067 Loss 6.712926, Accuracy 80.109%\n",
      "Epoch 16, Batch 912, LR 0.000067 Loss 6.712672, Accuracy 80.112%\n",
      "Epoch 16, Batch 913, LR 0.000067 Loss 6.711450, Accuracy 80.119%\n",
      "Epoch 16, Batch 914, LR 0.000067 Loss 6.711424, Accuracy 80.118%\n",
      "Epoch 16, Batch 915, LR 0.000067 Loss 6.711288, Accuracy 80.123%\n",
      "Epoch 16, Batch 916, LR 0.000067 Loss 6.711815, Accuracy 80.121%\n",
      "Epoch 16, Batch 917, LR 0.000067 Loss 6.711720, Accuracy 80.120%\n",
      "Epoch 16, Batch 918, LR 0.000067 Loss 6.711649, Accuracy 80.118%\n",
      "Epoch 16, Batch 919, LR 0.000067 Loss 6.711871, Accuracy 80.117%\n",
      "Epoch 16, Batch 920, LR 0.000067 Loss 6.711593, Accuracy 80.120%\n",
      "Epoch 16, Batch 921, LR 0.000067 Loss 6.711744, Accuracy 80.122%\n",
      "Epoch 16, Batch 922, LR 0.000067 Loss 6.711989, Accuracy 80.115%\n",
      "Epoch 16, Batch 923, LR 0.000067 Loss 6.711740, Accuracy 80.118%\n",
      "Epoch 16, Batch 924, LR 0.000067 Loss 6.712947, Accuracy 80.117%\n",
      "Epoch 16, Batch 925, LR 0.000067 Loss 6.713567, Accuracy 80.109%\n",
      "Epoch 16, Batch 926, LR 0.000067 Loss 6.712516, Accuracy 80.115%\n",
      "Epoch 16, Batch 927, LR 0.000067 Loss 6.712784, Accuracy 80.110%\n",
      "Epoch 16, Batch 928, LR 0.000067 Loss 6.712661, Accuracy 80.108%\n",
      "Epoch 16, Batch 929, LR 0.000067 Loss 6.712738, Accuracy 80.105%\n",
      "Epoch 16, Batch 930, LR 0.000067 Loss 6.712879, Accuracy 80.105%\n",
      "Epoch 16, Batch 931, LR 0.000067 Loss 6.712741, Accuracy 80.110%\n",
      "Epoch 16, Batch 932, LR 0.000067 Loss 6.712857, Accuracy 80.110%\n",
      "Epoch 16, Batch 933, LR 0.000067 Loss 6.712747, Accuracy 80.110%\n",
      "Epoch 16, Batch 934, LR 0.000067 Loss 6.712074, Accuracy 80.113%\n",
      "Epoch 16, Batch 935, LR 0.000067 Loss 6.711801, Accuracy 80.114%\n",
      "Epoch 16, Batch 936, LR 0.000067 Loss 6.712384, Accuracy 80.108%\n",
      "Epoch 16, Batch 937, LR 0.000067 Loss 6.712429, Accuracy 80.112%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 938, LR 0.000067 Loss 6.712283, Accuracy 80.114%\n",
      "Epoch 16, Batch 939, LR 0.000067 Loss 6.711501, Accuracy 80.118%\n",
      "Epoch 16, Batch 940, LR 0.000067 Loss 6.711382, Accuracy 80.125%\n",
      "Epoch 16, Batch 941, LR 0.000067 Loss 6.711044, Accuracy 80.126%\n",
      "Epoch 16, Batch 942, LR 0.000067 Loss 6.710698, Accuracy 80.130%\n",
      "Epoch 16, Batch 943, LR 0.000067 Loss 6.710998, Accuracy 80.127%\n",
      "Epoch 16, Batch 944, LR 0.000067 Loss 6.711394, Accuracy 80.125%\n",
      "Epoch 16, Batch 945, LR 0.000067 Loss 6.710996, Accuracy 80.127%\n",
      "Epoch 16, Batch 946, LR 0.000067 Loss 6.711009, Accuracy 80.127%\n",
      "Epoch 16, Batch 947, LR 0.000067 Loss 6.711232, Accuracy 80.131%\n",
      "Epoch 16, Batch 948, LR 0.000067 Loss 6.711587, Accuracy 80.127%\n",
      "Epoch 16, Batch 949, LR 0.000067 Loss 6.710177, Accuracy 80.135%\n",
      "Epoch 16, Batch 950, LR 0.000067 Loss 6.709919, Accuracy 80.141%\n",
      "Epoch 16, Batch 951, LR 0.000067 Loss 6.708944, Accuracy 80.144%\n",
      "Epoch 16, Batch 952, LR 0.000067 Loss 6.708774, Accuracy 80.146%\n",
      "Epoch 16, Batch 953, LR 0.000067 Loss 6.707866, Accuracy 80.155%\n",
      "Epoch 16, Batch 954, LR 0.000067 Loss 6.707978, Accuracy 80.151%\n",
      "Epoch 16, Batch 955, LR 0.000067 Loss 6.707381, Accuracy 80.156%\n",
      "Epoch 16, Batch 956, LR 0.000067 Loss 6.707375, Accuracy 80.159%\n",
      "Epoch 16, Batch 957, LR 0.000067 Loss 6.708529, Accuracy 80.154%\n",
      "Epoch 16, Batch 958, LR 0.000067 Loss 6.707857, Accuracy 80.158%\n",
      "Epoch 16, Batch 959, LR 0.000067 Loss 6.707340, Accuracy 80.162%\n",
      "Epoch 16, Batch 960, LR 0.000067 Loss 6.707200, Accuracy 80.161%\n",
      "Epoch 16, Batch 961, LR 0.000067 Loss 6.707800, Accuracy 80.159%\n",
      "Epoch 16, Batch 962, LR 0.000067 Loss 6.707495, Accuracy 80.162%\n",
      "Epoch 16, Batch 963, LR 0.000067 Loss 6.706464, Accuracy 80.165%\n",
      "Epoch 16, Batch 964, LR 0.000067 Loss 6.706060, Accuracy 80.166%\n",
      "Epoch 16, Batch 965, LR 0.000067 Loss 6.705685, Accuracy 80.168%\n",
      "Epoch 16, Batch 966, LR 0.000067 Loss 6.705032, Accuracy 80.169%\n",
      "Epoch 16, Batch 967, LR 0.000067 Loss 6.705120, Accuracy 80.163%\n",
      "Epoch 16, Batch 968, LR 0.000067 Loss 6.705578, Accuracy 80.164%\n",
      "Epoch 16, Batch 969, LR 0.000067 Loss 6.704945, Accuracy 80.167%\n",
      "Epoch 16, Batch 970, LR 0.000067 Loss 6.704999, Accuracy 80.168%\n",
      "Epoch 16, Batch 971, LR 0.000067 Loss 6.705343, Accuracy 80.169%\n",
      "Epoch 16, Batch 972, LR 0.000067 Loss 6.704537, Accuracy 80.172%\n",
      "Epoch 16, Batch 973, LR 0.000067 Loss 6.703690, Accuracy 80.172%\n",
      "Epoch 16, Batch 974, LR 0.000067 Loss 6.703348, Accuracy 80.178%\n",
      "Epoch 16, Batch 975, LR 0.000067 Loss 6.703649, Accuracy 80.178%\n",
      "Epoch 16, Batch 976, LR 0.000067 Loss 6.702915, Accuracy 80.184%\n",
      "Epoch 16, Batch 977, LR 0.000066 Loss 6.701379, Accuracy 80.192%\n",
      "Epoch 16, Batch 978, LR 0.000066 Loss 6.701768, Accuracy 80.190%\n",
      "Epoch 16, Batch 979, LR 0.000066 Loss 6.701435, Accuracy 80.190%\n",
      "Epoch 16, Batch 980, LR 0.000066 Loss 6.700919, Accuracy 80.194%\n",
      "Epoch 16, Batch 981, LR 0.000066 Loss 6.700106, Accuracy 80.201%\n",
      "Epoch 16, Batch 982, LR 0.000066 Loss 6.699914, Accuracy 80.202%\n",
      "Epoch 16, Batch 983, LR 0.000066 Loss 6.699645, Accuracy 80.203%\n",
      "Epoch 16, Batch 984, LR 0.000066 Loss 6.699206, Accuracy 80.205%\n",
      "Epoch 16, Batch 985, LR 0.000066 Loss 6.698886, Accuracy 80.203%\n",
      "Epoch 16, Batch 986, LR 0.000066 Loss 6.699441, Accuracy 80.205%\n",
      "Epoch 16, Batch 987, LR 0.000066 Loss 6.698621, Accuracy 80.204%\n",
      "Epoch 16, Batch 988, LR 0.000066 Loss 6.697924, Accuracy 80.205%\n",
      "Epoch 16, Batch 989, LR 0.000066 Loss 6.697336, Accuracy 80.210%\n",
      "Epoch 16, Batch 990, LR 0.000066 Loss 6.697146, Accuracy 80.214%\n",
      "Epoch 16, Batch 991, LR 0.000066 Loss 6.696971, Accuracy 80.218%\n",
      "Epoch 16, Batch 992, LR 0.000066 Loss 6.696472, Accuracy 80.221%\n",
      "Epoch 16, Batch 993, LR 0.000066 Loss 6.697021, Accuracy 80.214%\n",
      "Epoch 16, Batch 994, LR 0.000066 Loss 6.696459, Accuracy 80.213%\n",
      "Epoch 16, Batch 995, LR 0.000066 Loss 6.695893, Accuracy 80.217%\n",
      "Epoch 16, Batch 996, LR 0.000066 Loss 6.694911, Accuracy 80.220%\n",
      "Epoch 16, Batch 997, LR 0.000066 Loss 6.695358, Accuracy 80.214%\n",
      "Epoch 16, Batch 998, LR 0.000066 Loss 6.694735, Accuracy 80.225%\n",
      "Epoch 16, Batch 999, LR 0.000066 Loss 6.694384, Accuracy 80.226%\n",
      "Epoch 16, Batch 1000, LR 0.000066 Loss 6.694630, Accuracy 80.223%\n",
      "Epoch 16, Batch 1001, LR 0.000066 Loss 6.695170, Accuracy 80.219%\n",
      "Epoch 16, Batch 1002, LR 0.000066 Loss 6.694048, Accuracy 80.228%\n",
      "Epoch 16, Batch 1003, LR 0.000066 Loss 6.694263, Accuracy 80.227%\n",
      "Epoch 16, Batch 1004, LR 0.000066 Loss 6.694252, Accuracy 80.224%\n",
      "Epoch 16, Batch 1005, LR 0.000066 Loss 6.693977, Accuracy 80.230%\n",
      "Epoch 16, Batch 1006, LR 0.000066 Loss 6.694357, Accuracy 80.229%\n",
      "Epoch 16, Batch 1007, LR 0.000066 Loss 6.693978, Accuracy 80.231%\n",
      "Epoch 16, Batch 1008, LR 0.000066 Loss 6.694548, Accuracy 80.225%\n",
      "Epoch 16, Batch 1009, LR 0.000066 Loss 6.694356, Accuracy 80.222%\n",
      "Epoch 16, Batch 1010, LR 0.000066 Loss 6.693588, Accuracy 80.226%\n",
      "Epoch 16, Batch 1011, LR 0.000066 Loss 6.693623, Accuracy 80.220%\n",
      "Epoch 16, Batch 1012, LR 0.000066 Loss 6.692905, Accuracy 80.222%\n",
      "Epoch 16, Batch 1013, LR 0.000066 Loss 6.693199, Accuracy 80.222%\n",
      "Epoch 16, Batch 1014, LR 0.000066 Loss 6.692685, Accuracy 80.227%\n",
      "Epoch 16, Batch 1015, LR 0.000066 Loss 6.693048, Accuracy 80.229%\n",
      "Epoch 16, Batch 1016, LR 0.000066 Loss 6.693009, Accuracy 80.227%\n",
      "Epoch 16, Batch 1017, LR 0.000066 Loss 6.692433, Accuracy 80.231%\n",
      "Epoch 16, Batch 1018, LR 0.000066 Loss 6.692275, Accuracy 80.231%\n",
      "Epoch 16, Batch 1019, LR 0.000066 Loss 6.691809, Accuracy 80.227%\n",
      "Epoch 16, Batch 1020, LR 0.000066 Loss 6.691714, Accuracy 80.225%\n",
      "Epoch 16, Batch 1021, LR 0.000066 Loss 6.692456, Accuracy 80.223%\n",
      "Epoch 16, Batch 1022, LR 0.000066 Loss 6.692857, Accuracy 80.223%\n",
      "Epoch 16, Batch 1023, LR 0.000066 Loss 6.693008, Accuracy 80.221%\n",
      "Epoch 16, Batch 1024, LR 0.000066 Loss 6.692799, Accuracy 80.222%\n",
      "Epoch 16, Batch 1025, LR 0.000066 Loss 6.692895, Accuracy 80.222%\n",
      "Epoch 16, Batch 1026, LR 0.000066 Loss 6.693435, Accuracy 80.215%\n",
      "Epoch 16, Batch 1027, LR 0.000066 Loss 6.693620, Accuracy 80.215%\n",
      "Epoch 16, Batch 1028, LR 0.000066 Loss 6.693390, Accuracy 80.216%\n",
      "Epoch 16, Batch 1029, LR 0.000066 Loss 6.692160, Accuracy 80.226%\n",
      "Epoch 16, Batch 1030, LR 0.000066 Loss 6.692917, Accuracy 80.223%\n",
      "Epoch 16, Batch 1031, LR 0.000066 Loss 6.692941, Accuracy 80.223%\n",
      "Epoch 16, Batch 1032, LR 0.000066 Loss 6.692741, Accuracy 80.222%\n",
      "Epoch 16, Batch 1033, LR 0.000066 Loss 6.692741, Accuracy 80.220%\n",
      "Epoch 16, Batch 1034, LR 0.000066 Loss 6.692639, Accuracy 80.221%\n",
      "Epoch 16, Batch 1035, LR 0.000066 Loss 6.692787, Accuracy 80.215%\n",
      "Epoch 16, Batch 1036, LR 0.000066 Loss 6.692135, Accuracy 80.217%\n",
      "Epoch 16, Batch 1037, LR 0.000066 Loss 6.692037, Accuracy 80.220%\n",
      "Epoch 16, Batch 1038, LR 0.000066 Loss 6.691784, Accuracy 80.223%\n",
      "Epoch 16, Batch 1039, LR 0.000066 Loss 6.691385, Accuracy 80.223%\n",
      "Epoch 16, Batch 1040, LR 0.000066 Loss 6.690834, Accuracy 80.227%\n",
      "Epoch 16, Batch 1041, LR 0.000066 Loss 6.690688, Accuracy 80.230%\n",
      "Epoch 16, Batch 1042, LR 0.000066 Loss 6.689999, Accuracy 80.236%\n",
      "Epoch 16, Batch 1043, LR 0.000066 Loss 6.690323, Accuracy 80.234%\n",
      "Epoch 16, Batch 1044, LR 0.000066 Loss 6.690514, Accuracy 80.232%\n",
      "Epoch 16, Batch 1045, LR 0.000066 Loss 6.690470, Accuracy 80.233%\n",
      "Epoch 16, Batch 1046, LR 0.000066 Loss 6.690340, Accuracy 80.233%\n",
      "Epoch 16, Batch 1047, LR 0.000066 Loss 6.689844, Accuracy 80.238%\n",
      "Epoch 16, Loss (train set) 6.689844, Accuracy (train set) 80.238%\n",
      "Epoch 17, Batch 1, LR 0.000066 Loss 6.472213, Accuracy 83.594%\n",
      "Epoch 17, Batch 2, LR 0.000066 Loss 6.583711, Accuracy 80.469%\n",
      "Epoch 17, Batch 3, LR 0.000066 Loss 6.263506, Accuracy 82.812%\n",
      "Epoch 17, Batch 4, LR 0.000066 Loss 6.170919, Accuracy 83.594%\n",
      "Epoch 17, Batch 5, LR 0.000066 Loss 6.227731, Accuracy 82.188%\n",
      "Epoch 17, Batch 6, LR 0.000066 Loss 6.232916, Accuracy 82.552%\n",
      "Epoch 17, Batch 7, LR 0.000066 Loss 6.334317, Accuracy 82.589%\n",
      "Epoch 17, Batch 8, LR 0.000066 Loss 6.424805, Accuracy 82.812%\n",
      "Epoch 17, Batch 9, LR 0.000066 Loss 6.375274, Accuracy 82.726%\n",
      "Epoch 17, Batch 10, LR 0.000066 Loss 6.380100, Accuracy 82.734%\n",
      "Epoch 17, Batch 11, LR 0.000066 Loss 6.415569, Accuracy 82.457%\n",
      "Epoch 17, Batch 12, LR 0.000066 Loss 6.437956, Accuracy 82.422%\n",
      "Epoch 17, Batch 13, LR 0.000066 Loss 6.455253, Accuracy 82.212%\n",
      "Epoch 17, Batch 14, LR 0.000066 Loss 6.542414, Accuracy 81.696%\n",
      "Epoch 17, Batch 15, LR 0.000066 Loss 6.511211, Accuracy 81.719%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 16, LR 0.000066 Loss 6.505215, Accuracy 81.592%\n",
      "Epoch 17, Batch 17, LR 0.000066 Loss 6.502756, Accuracy 81.618%\n",
      "Epoch 17, Batch 18, LR 0.000066 Loss 6.494617, Accuracy 81.727%\n",
      "Epoch 17, Batch 19, LR 0.000066 Loss 6.542055, Accuracy 81.497%\n",
      "Epoch 17, Batch 20, LR 0.000066 Loss 6.534566, Accuracy 81.445%\n",
      "Epoch 17, Batch 21, LR 0.000066 Loss 6.518940, Accuracy 81.585%\n",
      "Epoch 17, Batch 22, LR 0.000066 Loss 6.560802, Accuracy 81.357%\n",
      "Epoch 17, Batch 23, LR 0.000066 Loss 6.536711, Accuracy 81.454%\n",
      "Epoch 17, Batch 24, LR 0.000066 Loss 6.550005, Accuracy 81.217%\n",
      "Epoch 17, Batch 25, LR 0.000066 Loss 6.525759, Accuracy 81.531%\n",
      "Epoch 17, Batch 26, LR 0.000066 Loss 6.521179, Accuracy 81.520%\n",
      "Epoch 17, Batch 27, LR 0.000066 Loss 6.515839, Accuracy 81.481%\n",
      "Epoch 17, Batch 28, LR 0.000066 Loss 6.532307, Accuracy 81.306%\n",
      "Epoch 17, Batch 29, LR 0.000066 Loss 6.534945, Accuracy 81.358%\n",
      "Epoch 17, Batch 30, LR 0.000066 Loss 6.509268, Accuracy 81.484%\n",
      "Epoch 17, Batch 31, LR 0.000066 Loss 6.498644, Accuracy 81.376%\n",
      "Epoch 17, Batch 32, LR 0.000066 Loss 6.518401, Accuracy 81.226%\n",
      "Epoch 17, Batch 33, LR 0.000066 Loss 6.503094, Accuracy 81.250%\n",
      "Epoch 17, Batch 34, LR 0.000066 Loss 6.518293, Accuracy 81.273%\n",
      "Epoch 17, Batch 35, LR 0.000066 Loss 6.508866, Accuracy 81.339%\n",
      "Epoch 17, Batch 36, LR 0.000066 Loss 6.526476, Accuracy 81.185%\n",
      "Epoch 17, Batch 37, LR 0.000066 Loss 6.520476, Accuracy 81.250%\n",
      "Epoch 17, Batch 38, LR 0.000066 Loss 6.512203, Accuracy 81.312%\n",
      "Epoch 17, Batch 39, LR 0.000066 Loss 6.506512, Accuracy 81.350%\n",
      "Epoch 17, Batch 40, LR 0.000066 Loss 6.505460, Accuracy 81.387%\n",
      "Epoch 17, Batch 41, LR 0.000066 Loss 6.504701, Accuracy 81.269%\n",
      "Epoch 17, Batch 42, LR 0.000066 Loss 6.496056, Accuracy 81.306%\n",
      "Epoch 17, Batch 43, LR 0.000066 Loss 6.498852, Accuracy 81.214%\n",
      "Epoch 17, Batch 44, LR 0.000066 Loss 6.527826, Accuracy 80.930%\n",
      "Epoch 17, Batch 45, LR 0.000066 Loss 6.521412, Accuracy 80.955%\n",
      "Epoch 17, Batch 46, LR 0.000066 Loss 6.522131, Accuracy 80.995%\n",
      "Epoch 17, Batch 47, LR 0.000066 Loss 6.532886, Accuracy 80.951%\n",
      "Epoch 17, Batch 48, LR 0.000066 Loss 6.527375, Accuracy 81.136%\n",
      "Epoch 17, Batch 49, LR 0.000066 Loss 6.537650, Accuracy 81.107%\n",
      "Epoch 17, Batch 50, LR 0.000066 Loss 6.548128, Accuracy 81.047%\n",
      "Epoch 17, Batch 51, LR 0.000066 Loss 6.548955, Accuracy 81.097%\n",
      "Epoch 17, Batch 52, LR 0.000066 Loss 6.565062, Accuracy 80.950%\n",
      "Epoch 17, Batch 53, LR 0.000066 Loss 6.563742, Accuracy 80.940%\n",
      "Epoch 17, Batch 54, LR 0.000066 Loss 6.549261, Accuracy 81.033%\n",
      "Epoch 17, Batch 55, LR 0.000066 Loss 6.544140, Accuracy 81.122%\n",
      "Epoch 17, Batch 56, LR 0.000066 Loss 6.554121, Accuracy 81.041%\n",
      "Epoch 17, Batch 57, LR 0.000066 Loss 6.560812, Accuracy 80.948%\n",
      "Epoch 17, Batch 58, LR 0.000066 Loss 6.563790, Accuracy 80.927%\n",
      "Epoch 17, Batch 59, LR 0.000066 Loss 6.552550, Accuracy 80.959%\n",
      "Epoch 17, Batch 60, LR 0.000066 Loss 6.557672, Accuracy 80.898%\n",
      "Epoch 17, Batch 61, LR 0.000066 Loss 6.551174, Accuracy 80.930%\n",
      "Epoch 17, Batch 62, LR 0.000066 Loss 6.558580, Accuracy 80.872%\n",
      "Epoch 17, Batch 63, LR 0.000066 Loss 6.567232, Accuracy 80.853%\n",
      "Epoch 17, Batch 64, LR 0.000066 Loss 6.557551, Accuracy 80.884%\n",
      "Epoch 17, Batch 65, LR 0.000066 Loss 6.542456, Accuracy 80.950%\n",
      "Epoch 17, Batch 66, LR 0.000066 Loss 6.552469, Accuracy 80.895%\n",
      "Epoch 17, Batch 67, LR 0.000066 Loss 6.546043, Accuracy 80.877%\n",
      "Epoch 17, Batch 68, LR 0.000066 Loss 6.542056, Accuracy 80.882%\n",
      "Epoch 17, Batch 69, LR 0.000066 Loss 6.534977, Accuracy 80.888%\n",
      "Epoch 17, Batch 70, LR 0.000066 Loss 6.543377, Accuracy 80.915%\n",
      "Epoch 17, Batch 71, LR 0.000066 Loss 6.559611, Accuracy 80.887%\n",
      "Epoch 17, Batch 72, LR 0.000066 Loss 6.549748, Accuracy 80.957%\n",
      "Epoch 17, Batch 73, LR 0.000066 Loss 6.535563, Accuracy 80.972%\n",
      "Epoch 17, Batch 74, LR 0.000066 Loss 6.538297, Accuracy 80.965%\n",
      "Epoch 17, Batch 75, LR 0.000066 Loss 6.541614, Accuracy 80.906%\n",
      "Epoch 17, Batch 76, LR 0.000066 Loss 6.542399, Accuracy 80.890%\n",
      "Epoch 17, Batch 77, LR 0.000066 Loss 6.541186, Accuracy 80.885%\n",
      "Epoch 17, Batch 78, LR 0.000066 Loss 6.534144, Accuracy 80.940%\n",
      "Epoch 17, Batch 79, LR 0.000066 Loss 6.533415, Accuracy 80.973%\n",
      "Epoch 17, Batch 80, LR 0.000066 Loss 6.528463, Accuracy 80.986%\n",
      "Epoch 17, Batch 81, LR 0.000066 Loss 6.532044, Accuracy 80.941%\n",
      "Epoch 17, Batch 82, LR 0.000066 Loss 6.520508, Accuracy 81.021%\n",
      "Epoch 17, Batch 83, LR 0.000066 Loss 6.508270, Accuracy 81.099%\n",
      "Epoch 17, Batch 84, LR 0.000066 Loss 6.503591, Accuracy 81.148%\n",
      "Epoch 17, Batch 85, LR 0.000066 Loss 6.500795, Accuracy 81.186%\n",
      "Epoch 17, Batch 86, LR 0.000066 Loss 6.486156, Accuracy 81.305%\n",
      "Epoch 17, Batch 87, LR 0.000066 Loss 6.486938, Accuracy 81.313%\n",
      "Epoch 17, Batch 88, LR 0.000066 Loss 6.485614, Accuracy 81.374%\n",
      "Epoch 17, Batch 89, LR 0.000066 Loss 6.483473, Accuracy 81.408%\n",
      "Epoch 17, Batch 90, LR 0.000066 Loss 6.486916, Accuracy 81.380%\n",
      "Epoch 17, Batch 91, LR 0.000066 Loss 6.485707, Accuracy 81.396%\n",
      "Epoch 17, Batch 92, LR 0.000066 Loss 6.480300, Accuracy 81.420%\n",
      "Epoch 17, Batch 93, LR 0.000066 Loss 6.486051, Accuracy 81.384%\n",
      "Epoch 17, Batch 94, LR 0.000066 Loss 6.483591, Accuracy 81.400%\n",
      "Epoch 17, Batch 95, LR 0.000066 Loss 6.484454, Accuracy 81.373%\n",
      "Epoch 17, Batch 96, LR 0.000066 Loss 6.488327, Accuracy 81.372%\n",
      "Epoch 17, Batch 97, LR 0.000066 Loss 6.483463, Accuracy 81.379%\n",
      "Epoch 17, Batch 98, LR 0.000066 Loss 6.488816, Accuracy 81.322%\n",
      "Epoch 17, Batch 99, LR 0.000066 Loss 6.491835, Accuracy 81.313%\n",
      "Epoch 17, Batch 100, LR 0.000066 Loss 6.494402, Accuracy 81.312%\n",
      "Epoch 17, Batch 101, LR 0.000066 Loss 6.498476, Accuracy 81.289%\n",
      "Epoch 17, Batch 102, LR 0.000066 Loss 6.497867, Accuracy 81.281%\n",
      "Epoch 17, Batch 103, LR 0.000066 Loss 6.502170, Accuracy 81.273%\n",
      "Epoch 17, Batch 104, LR 0.000066 Loss 6.510438, Accuracy 81.235%\n",
      "Epoch 17, Batch 105, LR 0.000066 Loss 6.516033, Accuracy 81.198%\n",
      "Epoch 17, Batch 106, LR 0.000066 Loss 6.524847, Accuracy 81.125%\n",
      "Epoch 17, Batch 107, LR 0.000066 Loss 6.527986, Accuracy 81.104%\n",
      "Epoch 17, Batch 108, LR 0.000066 Loss 6.523003, Accuracy 81.149%\n",
      "Epoch 17, Batch 109, LR 0.000066 Loss 6.525369, Accuracy 81.164%\n",
      "Epoch 17, Batch 110, LR 0.000066 Loss 6.534241, Accuracy 81.087%\n",
      "Epoch 17, Batch 111, LR 0.000066 Loss 6.536314, Accuracy 81.102%\n",
      "Epoch 17, Batch 112, LR 0.000066 Loss 6.532517, Accuracy 81.090%\n",
      "Epoch 17, Batch 113, LR 0.000066 Loss 6.533761, Accuracy 81.119%\n",
      "Epoch 17, Batch 114, LR 0.000066 Loss 6.533561, Accuracy 81.154%\n",
      "Epoch 17, Batch 115, LR 0.000066 Loss 6.536208, Accuracy 81.148%\n",
      "Epoch 17, Batch 116, LR 0.000066 Loss 6.535984, Accuracy 81.169%\n",
      "Epoch 17, Batch 117, LR 0.000066 Loss 6.533397, Accuracy 81.157%\n",
      "Epoch 17, Batch 118, LR 0.000066 Loss 6.536416, Accuracy 81.164%\n",
      "Epoch 17, Batch 119, LR 0.000066 Loss 6.533649, Accuracy 81.158%\n",
      "Epoch 17, Batch 120, LR 0.000066 Loss 6.533446, Accuracy 81.146%\n",
      "Epoch 17, Batch 121, LR 0.000066 Loss 6.535382, Accuracy 81.114%\n",
      "Epoch 17, Batch 122, LR 0.000066 Loss 6.539399, Accuracy 81.122%\n",
      "Epoch 17, Batch 123, LR 0.000066 Loss 6.539123, Accuracy 81.104%\n",
      "Epoch 17, Batch 124, LR 0.000066 Loss 6.536819, Accuracy 81.124%\n",
      "Epoch 17, Batch 125, LR 0.000066 Loss 6.541388, Accuracy 81.088%\n",
      "Epoch 17, Batch 126, LR 0.000066 Loss 6.547865, Accuracy 81.058%\n",
      "Epoch 17, Batch 127, LR 0.000066 Loss 6.544347, Accuracy 81.029%\n",
      "Epoch 17, Batch 128, LR 0.000066 Loss 6.540399, Accuracy 81.085%\n",
      "Epoch 17, Batch 129, LR 0.000066 Loss 6.532291, Accuracy 81.117%\n",
      "Epoch 17, Batch 130, LR 0.000066 Loss 6.523245, Accuracy 81.172%\n",
      "Epoch 17, Batch 131, LR 0.000066 Loss 6.520924, Accuracy 81.184%\n",
      "Epoch 17, Batch 132, LR 0.000066 Loss 6.522666, Accuracy 81.149%\n",
      "Epoch 17, Batch 133, LR 0.000066 Loss 6.520723, Accuracy 81.133%\n",
      "Epoch 17, Batch 134, LR 0.000066 Loss 6.514922, Accuracy 81.151%\n",
      "Epoch 17, Batch 135, LR 0.000066 Loss 6.518683, Accuracy 81.111%\n",
      "Epoch 17, Batch 136, LR 0.000066 Loss 6.513297, Accuracy 81.135%\n",
      "Epoch 17, Batch 137, LR 0.000066 Loss 6.513308, Accuracy 81.136%\n",
      "Epoch 17, Batch 138, LR 0.000066 Loss 6.515480, Accuracy 81.165%\n",
      "Epoch 17, Batch 139, LR 0.000066 Loss 6.514151, Accuracy 81.194%\n",
      "Epoch 17, Batch 140, LR 0.000066 Loss 6.506578, Accuracy 81.233%\n",
      "Epoch 17, Batch 141, LR 0.000066 Loss 6.508422, Accuracy 81.250%\n",
      "Epoch 17, Batch 142, LR 0.000066 Loss 6.511755, Accuracy 81.233%\n",
      "Epoch 17, Batch 143, LR 0.000066 Loss 6.514250, Accuracy 81.228%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 144, LR 0.000066 Loss 6.509359, Accuracy 81.234%\n",
      "Epoch 17, Batch 145, LR 0.000066 Loss 6.508346, Accuracy 81.261%\n",
      "Epoch 17, Batch 146, LR 0.000066 Loss 6.505624, Accuracy 81.277%\n",
      "Epoch 17, Batch 147, LR 0.000066 Loss 6.508643, Accuracy 81.261%\n",
      "Epoch 17, Batch 148, LR 0.000066 Loss 6.507541, Accuracy 81.250%\n",
      "Epoch 17, Batch 149, LR 0.000066 Loss 6.507590, Accuracy 81.250%\n",
      "Epoch 17, Batch 150, LR 0.000066 Loss 6.505059, Accuracy 81.281%\n",
      "Epoch 17, Batch 151, LR 0.000066 Loss 6.504336, Accuracy 81.266%\n",
      "Epoch 17, Batch 152, LR 0.000066 Loss 6.497925, Accuracy 81.271%\n",
      "Epoch 17, Batch 153, LR 0.000066 Loss 6.501886, Accuracy 81.240%\n",
      "Epoch 17, Batch 154, LR 0.000066 Loss 6.503477, Accuracy 81.250%\n",
      "Epoch 17, Batch 155, LR 0.000066 Loss 6.507639, Accuracy 81.195%\n",
      "Epoch 17, Batch 156, LR 0.000066 Loss 6.499357, Accuracy 81.245%\n",
      "Epoch 17, Batch 157, LR 0.000066 Loss 6.493912, Accuracy 81.270%\n",
      "Epoch 17, Batch 158, LR 0.000066 Loss 6.491733, Accuracy 81.250%\n",
      "Epoch 17, Batch 159, LR 0.000066 Loss 6.490881, Accuracy 81.265%\n",
      "Epoch 17, Batch 160, LR 0.000066 Loss 6.496929, Accuracy 81.221%\n",
      "Epoch 17, Batch 161, LR 0.000066 Loss 6.498292, Accuracy 81.206%\n",
      "Epoch 17, Batch 162, LR 0.000066 Loss 6.502699, Accuracy 81.197%\n",
      "Epoch 17, Batch 163, LR 0.000066 Loss 6.506392, Accuracy 81.154%\n",
      "Epoch 17, Batch 164, LR 0.000066 Loss 6.504802, Accuracy 81.159%\n",
      "Epoch 17, Batch 165, LR 0.000066 Loss 6.501988, Accuracy 81.184%\n",
      "Epoch 17, Batch 166, LR 0.000066 Loss 6.498089, Accuracy 81.189%\n",
      "Epoch 17, Batch 167, LR 0.000066 Loss 6.487280, Accuracy 81.245%\n",
      "Epoch 17, Batch 168, LR 0.000066 Loss 6.484737, Accuracy 81.241%\n",
      "Epoch 17, Batch 169, LR 0.000066 Loss 6.485223, Accuracy 81.222%\n",
      "Epoch 17, Batch 170, LR 0.000066 Loss 6.478317, Accuracy 81.250%\n",
      "Epoch 17, Batch 171, LR 0.000066 Loss 6.479542, Accuracy 81.245%\n",
      "Epoch 17, Batch 172, LR 0.000066 Loss 6.479931, Accuracy 81.245%\n",
      "Epoch 17, Batch 173, LR 0.000066 Loss 6.483375, Accuracy 81.236%\n",
      "Epoch 17, Batch 174, LR 0.000066 Loss 6.487315, Accuracy 81.232%\n",
      "Epoch 17, Batch 175, LR 0.000066 Loss 6.488552, Accuracy 81.232%\n",
      "Epoch 17, Batch 176, LR 0.000066 Loss 6.485257, Accuracy 81.228%\n",
      "Epoch 17, Batch 177, LR 0.000066 Loss 6.483282, Accuracy 81.241%\n",
      "Epoch 17, Batch 178, LR 0.000066 Loss 6.483650, Accuracy 81.250%\n",
      "Epoch 17, Batch 179, LR 0.000066 Loss 6.485816, Accuracy 81.228%\n",
      "Epoch 17, Batch 180, LR 0.000066 Loss 6.485099, Accuracy 81.237%\n",
      "Epoch 17, Batch 181, LR 0.000066 Loss 6.489166, Accuracy 81.241%\n",
      "Epoch 17, Batch 182, LR 0.000066 Loss 6.493024, Accuracy 81.220%\n",
      "Epoch 17, Batch 183, LR 0.000066 Loss 6.493103, Accuracy 81.233%\n",
      "Epoch 17, Batch 184, LR 0.000066 Loss 6.495218, Accuracy 81.229%\n",
      "Epoch 17, Batch 185, LR 0.000066 Loss 6.492915, Accuracy 81.237%\n",
      "Epoch 17, Batch 186, LR 0.000066 Loss 6.497065, Accuracy 81.208%\n",
      "Epoch 17, Batch 187, LR 0.000066 Loss 6.496706, Accuracy 81.221%\n",
      "Epoch 17, Batch 188, LR 0.000066 Loss 6.495994, Accuracy 81.246%\n",
      "Epoch 17, Batch 189, LR 0.000066 Loss 6.497062, Accuracy 81.229%\n",
      "Epoch 17, Batch 190, LR 0.000066 Loss 6.496450, Accuracy 81.229%\n",
      "Epoch 17, Batch 191, LR 0.000066 Loss 6.496955, Accuracy 81.234%\n",
      "Epoch 17, Batch 192, LR 0.000066 Loss 6.495128, Accuracy 81.238%\n",
      "Epoch 17, Batch 193, LR 0.000066 Loss 6.495283, Accuracy 81.234%\n",
      "Epoch 17, Batch 194, LR 0.000066 Loss 6.492022, Accuracy 81.258%\n",
      "Epoch 17, Batch 195, LR 0.000066 Loss 6.493173, Accuracy 81.242%\n",
      "Epoch 17, Batch 196, LR 0.000066 Loss 6.496563, Accuracy 81.250%\n",
      "Epoch 17, Batch 197, LR 0.000065 Loss 6.497234, Accuracy 81.254%\n",
      "Epoch 17, Batch 198, LR 0.000065 Loss 6.495128, Accuracy 81.262%\n",
      "Epoch 17, Batch 199, LR 0.000065 Loss 6.490000, Accuracy 81.266%\n",
      "Epoch 17, Batch 200, LR 0.000065 Loss 6.492735, Accuracy 81.258%\n",
      "Epoch 17, Batch 201, LR 0.000065 Loss 6.489943, Accuracy 81.273%\n",
      "Epoch 17, Batch 202, LR 0.000065 Loss 6.489244, Accuracy 81.273%\n",
      "Epoch 17, Batch 203, LR 0.000065 Loss 6.493365, Accuracy 81.242%\n",
      "Epoch 17, Batch 204, LR 0.000065 Loss 6.492847, Accuracy 81.250%\n",
      "Epoch 17, Batch 205, LR 0.000065 Loss 6.494514, Accuracy 81.242%\n",
      "Epoch 17, Batch 206, LR 0.000065 Loss 6.492245, Accuracy 81.246%\n",
      "Epoch 17, Batch 207, LR 0.000065 Loss 6.493101, Accuracy 81.235%\n",
      "Epoch 17, Batch 208, LR 0.000065 Loss 6.494255, Accuracy 81.216%\n",
      "Epoch 17, Batch 209, LR 0.000065 Loss 6.488775, Accuracy 81.243%\n",
      "Epoch 17, Batch 210, LR 0.000065 Loss 6.490624, Accuracy 81.231%\n",
      "Epoch 17, Batch 211, LR 0.000065 Loss 6.491395, Accuracy 81.239%\n",
      "Epoch 17, Batch 212, LR 0.000065 Loss 6.491526, Accuracy 81.224%\n",
      "Epoch 17, Batch 213, LR 0.000065 Loss 6.492865, Accuracy 81.210%\n",
      "Epoch 17, Batch 214, LR 0.000065 Loss 6.488980, Accuracy 81.221%\n",
      "Epoch 17, Batch 215, LR 0.000065 Loss 6.486833, Accuracy 81.217%\n",
      "Epoch 17, Batch 216, LR 0.000065 Loss 6.487868, Accuracy 81.214%\n",
      "Epoch 17, Batch 217, LR 0.000065 Loss 6.488163, Accuracy 81.221%\n",
      "Epoch 17, Batch 218, LR 0.000065 Loss 6.490011, Accuracy 81.236%\n",
      "Epoch 17, Batch 219, LR 0.000065 Loss 6.492489, Accuracy 81.214%\n",
      "Epoch 17, Batch 220, LR 0.000065 Loss 6.492215, Accuracy 81.197%\n",
      "Epoch 17, Batch 221, LR 0.000065 Loss 6.491084, Accuracy 81.193%\n",
      "Epoch 17, Batch 222, LR 0.000065 Loss 6.496501, Accuracy 81.173%\n",
      "Epoch 17, Batch 223, LR 0.000065 Loss 6.497831, Accuracy 81.166%\n",
      "Epoch 17, Batch 224, LR 0.000065 Loss 6.497255, Accuracy 81.170%\n",
      "Epoch 17, Batch 225, LR 0.000065 Loss 6.493719, Accuracy 81.198%\n",
      "Epoch 17, Batch 226, LR 0.000065 Loss 6.492441, Accuracy 81.195%\n",
      "Epoch 17, Batch 227, LR 0.000065 Loss 6.492262, Accuracy 81.188%\n",
      "Epoch 17, Batch 228, LR 0.000065 Loss 6.491064, Accuracy 81.205%\n",
      "Epoch 17, Batch 229, LR 0.000065 Loss 6.493976, Accuracy 81.202%\n",
      "Epoch 17, Batch 230, LR 0.000065 Loss 6.493585, Accuracy 81.209%\n",
      "Epoch 17, Batch 231, LR 0.000065 Loss 6.493584, Accuracy 81.209%\n",
      "Epoch 17, Batch 232, LR 0.000065 Loss 6.492605, Accuracy 81.210%\n",
      "Epoch 17, Batch 233, LR 0.000065 Loss 6.490410, Accuracy 81.213%\n",
      "Epoch 17, Batch 234, LR 0.000065 Loss 6.491557, Accuracy 81.207%\n",
      "Epoch 17, Batch 235, LR 0.000065 Loss 6.490648, Accuracy 81.207%\n",
      "Epoch 17, Batch 236, LR 0.000065 Loss 6.492492, Accuracy 81.210%\n",
      "Epoch 17, Batch 237, LR 0.000065 Loss 6.489657, Accuracy 81.220%\n",
      "Epoch 17, Batch 238, LR 0.000065 Loss 6.485226, Accuracy 81.243%\n",
      "Epoch 17, Batch 239, LR 0.000065 Loss 6.485633, Accuracy 81.250%\n",
      "Epoch 17, Batch 240, LR 0.000065 Loss 6.486461, Accuracy 81.250%\n",
      "Epoch 17, Batch 241, LR 0.000065 Loss 6.487571, Accuracy 81.240%\n",
      "Epoch 17, Batch 242, LR 0.000065 Loss 6.488293, Accuracy 81.244%\n",
      "Epoch 17, Batch 243, LR 0.000065 Loss 6.488900, Accuracy 81.231%\n",
      "Epoch 17, Batch 244, LR 0.000065 Loss 6.489071, Accuracy 81.221%\n",
      "Epoch 17, Batch 245, LR 0.000065 Loss 6.489027, Accuracy 81.234%\n",
      "Epoch 17, Batch 246, LR 0.000065 Loss 6.490497, Accuracy 81.225%\n",
      "Epoch 17, Batch 247, LR 0.000065 Loss 6.489109, Accuracy 81.234%\n",
      "Epoch 17, Batch 248, LR 0.000065 Loss 6.489152, Accuracy 81.231%\n",
      "Epoch 17, Batch 249, LR 0.000065 Loss 6.487233, Accuracy 81.228%\n",
      "Epoch 17, Batch 250, LR 0.000065 Loss 6.485989, Accuracy 81.241%\n",
      "Epoch 17, Batch 251, LR 0.000065 Loss 6.483937, Accuracy 81.244%\n",
      "Epoch 17, Batch 252, LR 0.000065 Loss 6.483128, Accuracy 81.253%\n",
      "Epoch 17, Batch 253, LR 0.000065 Loss 6.482462, Accuracy 81.265%\n",
      "Epoch 17, Batch 254, LR 0.000065 Loss 6.477562, Accuracy 81.287%\n",
      "Epoch 17, Batch 255, LR 0.000065 Loss 6.474819, Accuracy 81.302%\n",
      "Epoch 17, Batch 256, LR 0.000065 Loss 6.473052, Accuracy 81.308%\n",
      "Epoch 17, Batch 257, LR 0.000065 Loss 6.472903, Accuracy 81.296%\n",
      "Epoch 17, Batch 258, LR 0.000065 Loss 6.475773, Accuracy 81.277%\n",
      "Epoch 17, Batch 259, LR 0.000065 Loss 6.476262, Accuracy 81.283%\n",
      "Epoch 17, Batch 260, LR 0.000065 Loss 6.475192, Accuracy 81.301%\n",
      "Epoch 17, Batch 261, LR 0.000065 Loss 6.477746, Accuracy 81.292%\n",
      "Epoch 17, Batch 262, LR 0.000065 Loss 6.477197, Accuracy 81.286%\n",
      "Epoch 17, Batch 263, LR 0.000065 Loss 6.475979, Accuracy 81.295%\n",
      "Epoch 17, Batch 264, LR 0.000065 Loss 6.475227, Accuracy 81.309%\n",
      "Epoch 17, Batch 265, LR 0.000065 Loss 6.476730, Accuracy 81.291%\n",
      "Epoch 17, Batch 266, LR 0.000065 Loss 6.477831, Accuracy 81.288%\n",
      "Epoch 17, Batch 267, LR 0.000065 Loss 6.479093, Accuracy 81.279%\n",
      "Epoch 17, Batch 268, LR 0.000065 Loss 6.480596, Accuracy 81.265%\n",
      "Epoch 17, Batch 269, LR 0.000065 Loss 6.481891, Accuracy 81.267%\n",
      "Epoch 17, Batch 270, LR 0.000065 Loss 6.477588, Accuracy 81.302%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 271, LR 0.000065 Loss 6.481914, Accuracy 81.270%\n",
      "Epoch 17, Batch 272, LR 0.000065 Loss 6.483237, Accuracy 81.264%\n",
      "Epoch 17, Batch 273, LR 0.000065 Loss 6.480924, Accuracy 81.279%\n",
      "Epoch 17, Batch 274, LR 0.000065 Loss 6.481736, Accuracy 81.284%\n",
      "Epoch 17, Batch 275, LR 0.000065 Loss 6.479268, Accuracy 81.290%\n",
      "Epoch 17, Batch 276, LR 0.000065 Loss 6.478351, Accuracy 81.307%\n",
      "Epoch 17, Batch 277, LR 0.000065 Loss 6.478878, Accuracy 81.289%\n",
      "Epoch 17, Batch 278, LR 0.000065 Loss 6.476194, Accuracy 81.303%\n",
      "Epoch 17, Batch 279, LR 0.000065 Loss 6.479513, Accuracy 81.286%\n",
      "Epoch 17, Batch 280, LR 0.000065 Loss 6.481198, Accuracy 81.270%\n",
      "Epoch 17, Batch 281, LR 0.000065 Loss 6.480183, Accuracy 81.269%\n",
      "Epoch 17, Batch 282, LR 0.000065 Loss 6.481758, Accuracy 81.264%\n",
      "Epoch 17, Batch 283, LR 0.000065 Loss 6.480161, Accuracy 81.278%\n",
      "Epoch 17, Batch 284, LR 0.000065 Loss 6.480206, Accuracy 81.280%\n",
      "Epoch 17, Batch 285, LR 0.000065 Loss 6.479304, Accuracy 81.275%\n",
      "Epoch 17, Batch 286, LR 0.000065 Loss 6.480558, Accuracy 81.269%\n",
      "Epoch 17, Batch 287, LR 0.000065 Loss 6.479538, Accuracy 81.266%\n",
      "Epoch 17, Batch 288, LR 0.000065 Loss 6.478050, Accuracy 81.269%\n",
      "Epoch 17, Batch 289, LR 0.000065 Loss 6.475333, Accuracy 81.274%\n",
      "Epoch 17, Batch 290, LR 0.000065 Loss 6.474951, Accuracy 81.266%\n",
      "Epoch 17, Batch 291, LR 0.000065 Loss 6.475854, Accuracy 81.261%\n",
      "Epoch 17, Batch 292, LR 0.000065 Loss 6.478081, Accuracy 81.258%\n",
      "Epoch 17, Batch 293, LR 0.000065 Loss 6.476538, Accuracy 81.263%\n",
      "Epoch 17, Batch 294, LR 0.000065 Loss 6.475850, Accuracy 81.263%\n",
      "Epoch 17, Batch 295, LR 0.000065 Loss 6.477876, Accuracy 81.242%\n",
      "Epoch 17, Batch 296, LR 0.000065 Loss 6.476763, Accuracy 81.229%\n",
      "Epoch 17, Batch 297, LR 0.000065 Loss 6.475168, Accuracy 81.237%\n",
      "Epoch 17, Batch 298, LR 0.000065 Loss 6.473079, Accuracy 81.240%\n",
      "Epoch 17, Batch 299, LR 0.000065 Loss 6.472961, Accuracy 81.234%\n",
      "Epoch 17, Batch 300, LR 0.000065 Loss 6.474272, Accuracy 81.229%\n",
      "Epoch 17, Batch 301, LR 0.000065 Loss 6.473311, Accuracy 81.237%\n",
      "Epoch 17, Batch 302, LR 0.000065 Loss 6.474409, Accuracy 81.227%\n",
      "Epoch 17, Batch 303, LR 0.000065 Loss 6.472895, Accuracy 81.240%\n",
      "Epoch 17, Batch 304, LR 0.000065 Loss 6.472362, Accuracy 81.237%\n",
      "Epoch 17, Batch 305, LR 0.000065 Loss 6.471488, Accuracy 81.237%\n",
      "Epoch 17, Batch 306, LR 0.000065 Loss 6.470543, Accuracy 81.250%\n",
      "Epoch 17, Batch 307, LR 0.000065 Loss 6.472008, Accuracy 81.242%\n",
      "Epoch 17, Batch 308, LR 0.000065 Loss 6.470306, Accuracy 81.260%\n",
      "Epoch 17, Batch 309, LR 0.000065 Loss 6.468930, Accuracy 81.270%\n",
      "Epoch 17, Batch 310, LR 0.000065 Loss 6.466831, Accuracy 81.285%\n",
      "Epoch 17, Batch 311, LR 0.000065 Loss 6.467799, Accuracy 81.283%\n",
      "Epoch 17, Batch 312, LR 0.000065 Loss 6.468549, Accuracy 81.278%\n",
      "Epoch 17, Batch 313, LR 0.000065 Loss 6.472190, Accuracy 81.262%\n",
      "Epoch 17, Batch 314, LR 0.000065 Loss 6.471758, Accuracy 81.267%\n",
      "Epoch 17, Batch 315, LR 0.000065 Loss 6.471896, Accuracy 81.280%\n",
      "Epoch 17, Batch 316, LR 0.000065 Loss 6.470356, Accuracy 81.295%\n",
      "Epoch 17, Batch 317, LR 0.000065 Loss 6.470214, Accuracy 81.297%\n",
      "Epoch 17, Batch 318, LR 0.000065 Loss 6.471706, Accuracy 81.267%\n",
      "Epoch 17, Batch 319, LR 0.000065 Loss 6.473634, Accuracy 81.260%\n",
      "Epoch 17, Batch 320, LR 0.000065 Loss 6.472750, Accuracy 81.274%\n",
      "Epoch 17, Batch 321, LR 0.000065 Loss 6.471893, Accuracy 81.291%\n",
      "Epoch 17, Batch 322, LR 0.000065 Loss 6.476043, Accuracy 81.265%\n",
      "Epoch 17, Batch 323, LR 0.000065 Loss 6.478234, Accuracy 81.250%\n",
      "Epoch 17, Batch 324, LR 0.000065 Loss 6.480430, Accuracy 81.236%\n",
      "Epoch 17, Batch 325, LR 0.000065 Loss 6.479842, Accuracy 81.255%\n",
      "Epoch 17, Batch 326, LR 0.000065 Loss 6.481631, Accuracy 81.245%\n",
      "Epoch 17, Batch 327, LR 0.000065 Loss 6.478696, Accuracy 81.264%\n",
      "Epoch 17, Batch 328, LR 0.000065 Loss 6.478522, Accuracy 81.264%\n",
      "Epoch 17, Batch 329, LR 0.000065 Loss 6.475698, Accuracy 81.281%\n",
      "Epoch 17, Batch 330, LR 0.000065 Loss 6.475286, Accuracy 81.281%\n",
      "Epoch 17, Batch 331, LR 0.000065 Loss 6.478086, Accuracy 81.274%\n",
      "Epoch 17, Batch 332, LR 0.000065 Loss 6.477295, Accuracy 81.276%\n",
      "Epoch 17, Batch 333, LR 0.000065 Loss 6.478969, Accuracy 81.271%\n",
      "Epoch 17, Batch 334, LR 0.000065 Loss 6.478321, Accuracy 81.285%\n",
      "Epoch 17, Batch 335, LR 0.000065 Loss 6.476554, Accuracy 81.297%\n",
      "Epoch 17, Batch 336, LR 0.000065 Loss 6.476412, Accuracy 81.301%\n",
      "Epoch 17, Batch 337, LR 0.000065 Loss 6.475381, Accuracy 81.310%\n",
      "Epoch 17, Batch 338, LR 0.000065 Loss 6.476083, Accuracy 81.296%\n",
      "Epoch 17, Batch 339, LR 0.000065 Loss 6.477026, Accuracy 81.291%\n",
      "Epoch 17, Batch 340, LR 0.000065 Loss 6.477045, Accuracy 81.284%\n",
      "Epoch 17, Batch 341, LR 0.000065 Loss 6.478962, Accuracy 81.277%\n",
      "Epoch 17, Batch 342, LR 0.000065 Loss 6.479855, Accuracy 81.266%\n",
      "Epoch 17, Batch 343, LR 0.000065 Loss 6.479592, Accuracy 81.257%\n",
      "Epoch 17, Batch 344, LR 0.000065 Loss 6.482529, Accuracy 81.243%\n",
      "Epoch 17, Batch 345, LR 0.000065 Loss 6.481938, Accuracy 81.250%\n",
      "Epoch 17, Batch 346, LR 0.000065 Loss 6.482805, Accuracy 81.241%\n",
      "Epoch 17, Batch 347, LR 0.000065 Loss 6.483633, Accuracy 81.223%\n",
      "Epoch 17, Batch 348, LR 0.000065 Loss 6.481454, Accuracy 81.230%\n",
      "Epoch 17, Batch 349, LR 0.000065 Loss 6.479128, Accuracy 81.237%\n",
      "Epoch 17, Batch 350, LR 0.000065 Loss 6.478160, Accuracy 81.250%\n",
      "Epoch 17, Batch 351, LR 0.000065 Loss 6.479000, Accuracy 81.257%\n",
      "Epoch 17, Batch 352, LR 0.000065 Loss 6.480386, Accuracy 81.257%\n",
      "Epoch 17, Batch 353, LR 0.000065 Loss 6.480989, Accuracy 81.250%\n",
      "Epoch 17, Batch 354, LR 0.000065 Loss 6.479611, Accuracy 81.252%\n",
      "Epoch 17, Batch 355, LR 0.000065 Loss 6.479954, Accuracy 81.254%\n",
      "Epoch 17, Batch 356, LR 0.000065 Loss 6.481427, Accuracy 81.246%\n",
      "Epoch 17, Batch 357, LR 0.000065 Loss 6.482882, Accuracy 81.232%\n",
      "Epoch 17, Batch 358, LR 0.000065 Loss 6.483440, Accuracy 81.219%\n",
      "Epoch 17, Batch 359, LR 0.000065 Loss 6.481619, Accuracy 81.239%\n",
      "Epoch 17, Batch 360, LR 0.000065 Loss 6.481727, Accuracy 81.239%\n",
      "Epoch 17, Batch 361, LR 0.000065 Loss 6.479954, Accuracy 81.239%\n",
      "Epoch 17, Batch 362, LR 0.000065 Loss 6.482098, Accuracy 81.228%\n",
      "Epoch 17, Batch 363, LR 0.000065 Loss 6.481448, Accuracy 81.233%\n",
      "Epoch 17, Batch 364, LR 0.000065 Loss 6.479569, Accuracy 81.237%\n",
      "Epoch 17, Batch 365, LR 0.000065 Loss 6.478981, Accuracy 81.246%\n",
      "Epoch 17, Batch 366, LR 0.000065 Loss 6.480190, Accuracy 81.235%\n",
      "Epoch 17, Batch 367, LR 0.000065 Loss 6.480017, Accuracy 81.229%\n",
      "Epoch 17, Batch 368, LR 0.000065 Loss 6.480429, Accuracy 81.225%\n",
      "Epoch 17, Batch 369, LR 0.000065 Loss 6.479307, Accuracy 81.227%\n",
      "Epoch 17, Batch 370, LR 0.000065 Loss 6.476862, Accuracy 81.242%\n",
      "Epoch 17, Batch 371, LR 0.000065 Loss 6.479648, Accuracy 81.231%\n",
      "Epoch 17, Batch 372, LR 0.000065 Loss 6.480299, Accuracy 81.229%\n",
      "Epoch 17, Batch 373, LR 0.000065 Loss 6.481755, Accuracy 81.233%\n",
      "Epoch 17, Batch 374, LR 0.000065 Loss 6.481495, Accuracy 81.229%\n",
      "Epoch 17, Batch 375, LR 0.000065 Loss 6.482934, Accuracy 81.217%\n",
      "Epoch 17, Batch 376, LR 0.000065 Loss 6.483080, Accuracy 81.217%\n",
      "Epoch 17, Batch 377, LR 0.000065 Loss 6.480751, Accuracy 81.225%\n",
      "Epoch 17, Batch 378, LR 0.000065 Loss 6.479188, Accuracy 81.227%\n",
      "Epoch 17, Batch 379, LR 0.000065 Loss 6.479482, Accuracy 81.217%\n",
      "Epoch 17, Batch 380, LR 0.000065 Loss 6.477086, Accuracy 81.229%\n",
      "Epoch 17, Batch 381, LR 0.000065 Loss 6.478257, Accuracy 81.219%\n",
      "Epoch 17, Batch 382, LR 0.000065 Loss 6.480740, Accuracy 81.215%\n",
      "Epoch 17, Batch 383, LR 0.000065 Loss 6.480047, Accuracy 81.221%\n",
      "Epoch 17, Batch 384, LR 0.000065 Loss 6.479574, Accuracy 81.232%\n",
      "Epoch 17, Batch 385, LR 0.000065 Loss 6.479604, Accuracy 81.228%\n",
      "Epoch 17, Batch 386, LR 0.000065 Loss 6.479947, Accuracy 81.224%\n",
      "Epoch 17, Batch 387, LR 0.000065 Loss 6.480965, Accuracy 81.216%\n",
      "Epoch 17, Batch 388, LR 0.000065 Loss 6.479846, Accuracy 81.220%\n",
      "Epoch 17, Batch 389, LR 0.000065 Loss 6.478630, Accuracy 81.226%\n",
      "Epoch 17, Batch 390, LR 0.000065 Loss 6.481719, Accuracy 81.210%\n",
      "Epoch 17, Batch 391, LR 0.000065 Loss 6.480143, Accuracy 81.212%\n",
      "Epoch 17, Batch 392, LR 0.000065 Loss 6.480533, Accuracy 81.216%\n",
      "Epoch 17, Batch 393, LR 0.000065 Loss 6.478145, Accuracy 81.228%\n",
      "Epoch 17, Batch 394, LR 0.000065 Loss 6.477520, Accuracy 81.234%\n",
      "Epoch 17, Batch 395, LR 0.000065 Loss 6.476902, Accuracy 81.244%\n",
      "Epoch 17, Batch 396, LR 0.000065 Loss 6.477589, Accuracy 81.242%\n",
      "Epoch 17, Batch 397, LR 0.000065 Loss 6.477700, Accuracy 81.242%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 398, LR 0.000065 Loss 6.476156, Accuracy 81.254%\n",
      "Epoch 17, Batch 399, LR 0.000065 Loss 6.475509, Accuracy 81.262%\n",
      "Epoch 17, Batch 400, LR 0.000065 Loss 6.474660, Accuracy 81.260%\n",
      "Epoch 17, Batch 401, LR 0.000065 Loss 6.474700, Accuracy 81.254%\n",
      "Epoch 17, Batch 402, LR 0.000065 Loss 6.475152, Accuracy 81.242%\n",
      "Epoch 17, Batch 403, LR 0.000065 Loss 6.476253, Accuracy 81.238%\n",
      "Epoch 17, Batch 404, LR 0.000065 Loss 6.475051, Accuracy 81.240%\n",
      "Epoch 17, Batch 405, LR 0.000065 Loss 6.474620, Accuracy 81.240%\n",
      "Epoch 17, Batch 406, LR 0.000065 Loss 6.474010, Accuracy 81.246%\n",
      "Epoch 17, Batch 407, LR 0.000065 Loss 6.474888, Accuracy 81.244%\n",
      "Epoch 17, Batch 408, LR 0.000065 Loss 6.475546, Accuracy 81.227%\n",
      "Epoch 17, Batch 409, LR 0.000065 Loss 6.472047, Accuracy 81.237%\n",
      "Epoch 17, Batch 410, LR 0.000065 Loss 6.472076, Accuracy 81.233%\n",
      "Epoch 17, Batch 411, LR 0.000065 Loss 6.471727, Accuracy 81.237%\n",
      "Epoch 17, Batch 412, LR 0.000065 Loss 6.471391, Accuracy 81.248%\n",
      "Epoch 17, Batch 413, LR 0.000065 Loss 6.472090, Accuracy 81.248%\n",
      "Epoch 17, Batch 414, LR 0.000065 Loss 6.469970, Accuracy 81.248%\n",
      "Epoch 17, Batch 415, LR 0.000065 Loss 6.470094, Accuracy 81.244%\n",
      "Epoch 17, Batch 416, LR 0.000065 Loss 6.470461, Accuracy 81.227%\n",
      "Epoch 17, Batch 417, LR 0.000065 Loss 6.469130, Accuracy 81.237%\n",
      "Epoch 17, Batch 418, LR 0.000065 Loss 6.468165, Accuracy 81.241%\n",
      "Epoch 17, Batch 419, LR 0.000065 Loss 6.465293, Accuracy 81.256%\n",
      "Epoch 17, Batch 420, LR 0.000065 Loss 6.466412, Accuracy 81.239%\n",
      "Epoch 17, Batch 421, LR 0.000065 Loss 6.465028, Accuracy 81.241%\n",
      "Epoch 17, Batch 422, LR 0.000065 Loss 6.464059, Accuracy 81.252%\n",
      "Epoch 17, Batch 423, LR 0.000065 Loss 6.464238, Accuracy 81.248%\n",
      "Epoch 17, Batch 424, LR 0.000065 Loss 6.463654, Accuracy 81.252%\n",
      "Epoch 17, Batch 425, LR 0.000065 Loss 6.464195, Accuracy 81.237%\n",
      "Epoch 17, Batch 426, LR 0.000065 Loss 6.463131, Accuracy 81.244%\n",
      "Epoch 17, Batch 427, LR 0.000065 Loss 6.461003, Accuracy 81.263%\n",
      "Epoch 17, Batch 428, LR 0.000065 Loss 6.459633, Accuracy 81.272%\n",
      "Epoch 17, Batch 429, LR 0.000065 Loss 6.458590, Accuracy 81.286%\n",
      "Epoch 17, Batch 430, LR 0.000065 Loss 6.457531, Accuracy 81.285%\n",
      "Epoch 17, Batch 431, LR 0.000065 Loss 6.455114, Accuracy 81.297%\n",
      "Epoch 17, Batch 432, LR 0.000065 Loss 6.457148, Accuracy 81.290%\n",
      "Epoch 17, Batch 433, LR 0.000065 Loss 6.457175, Accuracy 81.291%\n",
      "Epoch 17, Batch 434, LR 0.000065 Loss 6.457598, Accuracy 81.288%\n",
      "Epoch 17, Batch 435, LR 0.000065 Loss 6.456436, Accuracy 81.295%\n",
      "Epoch 17, Batch 436, LR 0.000065 Loss 6.456721, Accuracy 81.286%\n",
      "Epoch 17, Batch 437, LR 0.000065 Loss 6.456963, Accuracy 81.280%\n",
      "Epoch 17, Batch 438, LR 0.000065 Loss 6.457573, Accuracy 81.273%\n",
      "Epoch 17, Batch 439, LR 0.000065 Loss 6.458974, Accuracy 81.262%\n",
      "Epoch 17, Batch 440, LR 0.000065 Loss 6.456628, Accuracy 81.270%\n",
      "Epoch 17, Batch 441, LR 0.000065 Loss 6.455347, Accuracy 81.273%\n",
      "Epoch 17, Batch 442, LR 0.000065 Loss 6.454637, Accuracy 81.280%\n",
      "Epoch 17, Batch 443, LR 0.000065 Loss 6.455682, Accuracy 81.284%\n",
      "Epoch 17, Batch 444, LR 0.000065 Loss 6.456039, Accuracy 81.289%\n",
      "Epoch 17, Batch 445, LR 0.000065 Loss 6.455590, Accuracy 81.294%\n",
      "Epoch 17, Batch 446, LR 0.000065 Loss 6.455905, Accuracy 81.290%\n",
      "Epoch 17, Batch 447, LR 0.000065 Loss 6.455067, Accuracy 81.295%\n",
      "Epoch 17, Batch 448, LR 0.000065 Loss 6.455509, Accuracy 81.299%\n",
      "Epoch 17, Batch 449, LR 0.000065 Loss 6.453709, Accuracy 81.318%\n",
      "Epoch 17, Batch 450, LR 0.000065 Loss 6.452440, Accuracy 81.319%\n",
      "Epoch 17, Batch 451, LR 0.000065 Loss 6.452456, Accuracy 81.319%\n",
      "Epoch 17, Batch 452, LR 0.000065 Loss 6.451358, Accuracy 81.317%\n",
      "Epoch 17, Batch 453, LR 0.000065 Loss 6.449901, Accuracy 81.333%\n",
      "Epoch 17, Batch 454, LR 0.000065 Loss 6.449762, Accuracy 81.324%\n",
      "Epoch 17, Batch 455, LR 0.000065 Loss 6.450184, Accuracy 81.314%\n",
      "Epoch 17, Batch 456, LR 0.000065 Loss 6.450648, Accuracy 81.320%\n",
      "Epoch 17, Batch 457, LR 0.000065 Loss 6.452558, Accuracy 81.313%\n",
      "Epoch 17, Batch 458, LR 0.000065 Loss 6.452123, Accuracy 81.315%\n",
      "Epoch 17, Batch 459, LR 0.000065 Loss 6.450486, Accuracy 81.320%\n",
      "Epoch 17, Batch 460, LR 0.000065 Loss 6.450491, Accuracy 81.318%\n",
      "Epoch 17, Batch 461, LR 0.000065 Loss 6.450392, Accuracy 81.306%\n",
      "Epoch 17, Batch 462, LR 0.000064 Loss 6.449667, Accuracy 81.309%\n",
      "Epoch 17, Batch 463, LR 0.000064 Loss 6.450078, Accuracy 81.299%\n",
      "Epoch 17, Batch 464, LR 0.000064 Loss 6.451390, Accuracy 81.285%\n",
      "Epoch 17, Batch 465, LR 0.000064 Loss 6.452126, Accuracy 81.282%\n",
      "Epoch 17, Batch 466, LR 0.000064 Loss 6.449569, Accuracy 81.302%\n",
      "Epoch 17, Batch 467, LR 0.000064 Loss 6.449353, Accuracy 81.299%\n",
      "Epoch 17, Batch 468, LR 0.000064 Loss 6.449701, Accuracy 81.297%\n",
      "Epoch 17, Batch 469, LR 0.000064 Loss 6.449876, Accuracy 81.305%\n",
      "Epoch 17, Batch 470, LR 0.000064 Loss 6.451645, Accuracy 81.283%\n",
      "Epoch 17, Batch 471, LR 0.000064 Loss 6.449334, Accuracy 81.295%\n",
      "Epoch 17, Batch 472, LR 0.000064 Loss 6.447716, Accuracy 81.296%\n",
      "Epoch 17, Batch 473, LR 0.000064 Loss 6.446784, Accuracy 81.300%\n",
      "Epoch 17, Batch 474, LR 0.000064 Loss 6.446546, Accuracy 81.298%\n",
      "Epoch 17, Batch 475, LR 0.000064 Loss 6.447233, Accuracy 81.293%\n",
      "Epoch 17, Batch 476, LR 0.000064 Loss 6.448709, Accuracy 81.278%\n",
      "Epoch 17, Batch 477, LR 0.000064 Loss 6.448635, Accuracy 81.283%\n",
      "Epoch 17, Batch 478, LR 0.000064 Loss 6.448016, Accuracy 81.296%\n",
      "Epoch 17, Batch 479, LR 0.000064 Loss 6.448308, Accuracy 81.286%\n",
      "Epoch 17, Batch 480, LR 0.000064 Loss 6.448251, Accuracy 81.284%\n",
      "Epoch 17, Batch 481, LR 0.000064 Loss 6.447667, Accuracy 81.287%\n",
      "Epoch 17, Batch 482, LR 0.000064 Loss 6.448732, Accuracy 81.287%\n",
      "Epoch 17, Batch 483, LR 0.000064 Loss 6.449546, Accuracy 81.290%\n",
      "Epoch 17, Batch 484, LR 0.000064 Loss 6.450850, Accuracy 81.289%\n",
      "Epoch 17, Batch 485, LR 0.000064 Loss 6.451426, Accuracy 81.287%\n",
      "Epoch 17, Batch 486, LR 0.000064 Loss 6.450802, Accuracy 81.290%\n",
      "Epoch 17, Batch 487, LR 0.000064 Loss 6.451667, Accuracy 81.289%\n",
      "Epoch 17, Batch 488, LR 0.000064 Loss 6.451633, Accuracy 81.287%\n",
      "Epoch 17, Batch 489, LR 0.000064 Loss 6.451901, Accuracy 81.284%\n",
      "Epoch 17, Batch 490, LR 0.000064 Loss 6.451211, Accuracy 81.291%\n",
      "Epoch 17, Batch 491, LR 0.000064 Loss 6.450422, Accuracy 81.293%\n",
      "Epoch 17, Batch 492, LR 0.000064 Loss 6.451207, Accuracy 81.291%\n",
      "Epoch 17, Batch 493, LR 0.000064 Loss 6.450828, Accuracy 81.296%\n",
      "Epoch 17, Batch 494, LR 0.000064 Loss 6.450746, Accuracy 81.297%\n",
      "Epoch 17, Batch 495, LR 0.000064 Loss 6.450511, Accuracy 81.301%\n",
      "Epoch 17, Batch 496, LR 0.000064 Loss 6.450229, Accuracy 81.305%\n",
      "Epoch 17, Batch 497, LR 0.000064 Loss 6.449992, Accuracy 81.310%\n",
      "Epoch 17, Batch 498, LR 0.000064 Loss 6.448712, Accuracy 81.324%\n",
      "Epoch 17, Batch 499, LR 0.000064 Loss 6.448305, Accuracy 81.322%\n",
      "Epoch 17, Batch 500, LR 0.000064 Loss 6.448287, Accuracy 81.327%\n",
      "Epoch 17, Batch 501, LR 0.000064 Loss 6.447760, Accuracy 81.330%\n",
      "Epoch 17, Batch 502, LR 0.000064 Loss 6.447727, Accuracy 81.323%\n",
      "Epoch 17, Batch 503, LR 0.000064 Loss 6.447784, Accuracy 81.329%\n",
      "Epoch 17, Batch 504, LR 0.000064 Loss 6.448866, Accuracy 81.321%\n",
      "Epoch 17, Batch 505, LR 0.000064 Loss 6.451366, Accuracy 81.306%\n",
      "Epoch 17, Batch 506, LR 0.000064 Loss 6.453029, Accuracy 81.306%\n",
      "Epoch 17, Batch 507, LR 0.000064 Loss 6.452390, Accuracy 81.316%\n",
      "Epoch 17, Batch 508, LR 0.000064 Loss 6.451976, Accuracy 81.322%\n",
      "Epoch 17, Batch 509, LR 0.000064 Loss 6.451505, Accuracy 81.325%\n",
      "Epoch 17, Batch 510, LR 0.000064 Loss 6.452446, Accuracy 81.316%\n",
      "Epoch 17, Batch 511, LR 0.000064 Loss 6.453015, Accuracy 81.308%\n",
      "Epoch 17, Batch 512, LR 0.000064 Loss 6.451538, Accuracy 81.323%\n",
      "Epoch 17, Batch 513, LR 0.000064 Loss 6.451554, Accuracy 81.328%\n",
      "Epoch 17, Batch 514, LR 0.000064 Loss 6.453154, Accuracy 81.308%\n",
      "Epoch 17, Batch 515, LR 0.000064 Loss 6.453531, Accuracy 81.314%\n",
      "Epoch 17, Batch 516, LR 0.000064 Loss 6.454438, Accuracy 81.305%\n",
      "Epoch 17, Batch 517, LR 0.000064 Loss 6.454329, Accuracy 81.303%\n",
      "Epoch 17, Batch 518, LR 0.000064 Loss 6.454299, Accuracy 81.301%\n",
      "Epoch 17, Batch 519, LR 0.000064 Loss 6.453783, Accuracy 81.303%\n",
      "Epoch 17, Batch 520, LR 0.000064 Loss 6.452052, Accuracy 81.313%\n",
      "Epoch 17, Batch 521, LR 0.000064 Loss 6.451191, Accuracy 81.313%\n",
      "Epoch 17, Batch 522, LR 0.000064 Loss 6.451358, Accuracy 81.316%\n",
      "Epoch 17, Batch 523, LR 0.000064 Loss 6.451803, Accuracy 81.311%\n",
      "Epoch 17, Batch 524, LR 0.000064 Loss 6.453674, Accuracy 81.302%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 525, LR 0.000064 Loss 6.455319, Accuracy 81.289%\n",
      "Epoch 17, Batch 526, LR 0.000064 Loss 6.455374, Accuracy 81.286%\n",
      "Epoch 17, Batch 527, LR 0.000064 Loss 6.454229, Accuracy 81.292%\n",
      "Epoch 17, Batch 528, LR 0.000064 Loss 6.452656, Accuracy 81.294%\n",
      "Epoch 17, Batch 529, LR 0.000064 Loss 6.451149, Accuracy 81.299%\n",
      "Epoch 17, Batch 530, LR 0.000064 Loss 6.450627, Accuracy 81.305%\n",
      "Epoch 17, Batch 531, LR 0.000064 Loss 6.450230, Accuracy 81.307%\n",
      "Epoch 17, Batch 532, LR 0.000064 Loss 6.450202, Accuracy 81.301%\n",
      "Epoch 17, Batch 533, LR 0.000064 Loss 6.450500, Accuracy 81.298%\n",
      "Epoch 17, Batch 534, LR 0.000064 Loss 6.450425, Accuracy 81.301%\n",
      "Epoch 17, Batch 535, LR 0.000064 Loss 6.450328, Accuracy 81.298%\n",
      "Epoch 17, Batch 536, LR 0.000064 Loss 6.450428, Accuracy 81.297%\n",
      "Epoch 17, Batch 537, LR 0.000064 Loss 6.450932, Accuracy 81.295%\n",
      "Epoch 17, Batch 538, LR 0.000064 Loss 6.450013, Accuracy 81.299%\n",
      "Epoch 17, Batch 539, LR 0.000064 Loss 6.449202, Accuracy 81.309%\n",
      "Epoch 17, Batch 540, LR 0.000064 Loss 6.448857, Accuracy 81.309%\n",
      "Epoch 17, Batch 541, LR 0.000064 Loss 6.448647, Accuracy 81.309%\n",
      "Epoch 17, Batch 542, LR 0.000064 Loss 6.448655, Accuracy 81.308%\n",
      "Epoch 17, Batch 543, LR 0.000064 Loss 6.447100, Accuracy 81.309%\n",
      "Epoch 17, Batch 544, LR 0.000064 Loss 6.447731, Accuracy 81.303%\n",
      "Epoch 17, Batch 545, LR 0.000064 Loss 6.447323, Accuracy 81.317%\n",
      "Epoch 17, Batch 546, LR 0.000064 Loss 6.446479, Accuracy 81.322%\n",
      "Epoch 17, Batch 547, LR 0.000064 Loss 6.446543, Accuracy 81.317%\n",
      "Epoch 17, Batch 548, LR 0.000064 Loss 6.444546, Accuracy 81.334%\n",
      "Epoch 17, Batch 549, LR 0.000064 Loss 6.444960, Accuracy 81.338%\n",
      "Epoch 17, Batch 550, LR 0.000064 Loss 6.444934, Accuracy 81.335%\n",
      "Epoch 17, Batch 551, LR 0.000064 Loss 6.444319, Accuracy 81.334%\n",
      "Epoch 17, Batch 552, LR 0.000064 Loss 6.444642, Accuracy 81.331%\n",
      "Epoch 17, Batch 553, LR 0.000064 Loss 6.446358, Accuracy 81.328%\n",
      "Epoch 17, Batch 554, LR 0.000064 Loss 6.445701, Accuracy 81.330%\n",
      "Epoch 17, Batch 555, LR 0.000064 Loss 6.445579, Accuracy 81.330%\n",
      "Epoch 17, Batch 556, LR 0.000064 Loss 6.444477, Accuracy 81.331%\n",
      "Epoch 17, Batch 557, LR 0.000064 Loss 6.443989, Accuracy 81.334%\n",
      "Epoch 17, Batch 558, LR 0.000064 Loss 6.443833, Accuracy 81.331%\n",
      "Epoch 17, Batch 559, LR 0.000064 Loss 6.443830, Accuracy 81.327%\n",
      "Epoch 17, Batch 560, LR 0.000064 Loss 6.443062, Accuracy 81.335%\n",
      "Epoch 17, Batch 561, LR 0.000064 Loss 6.442318, Accuracy 81.338%\n",
      "Epoch 17, Batch 562, LR 0.000064 Loss 6.441934, Accuracy 81.346%\n",
      "Epoch 17, Batch 563, LR 0.000064 Loss 6.441454, Accuracy 81.344%\n",
      "Epoch 17, Batch 564, LR 0.000064 Loss 6.442350, Accuracy 81.330%\n",
      "Epoch 17, Batch 565, LR 0.000064 Loss 6.441706, Accuracy 81.343%\n",
      "Epoch 17, Batch 566, LR 0.000064 Loss 6.441332, Accuracy 81.342%\n",
      "Epoch 17, Batch 567, LR 0.000064 Loss 6.442203, Accuracy 81.345%\n",
      "Epoch 17, Batch 568, LR 0.000064 Loss 6.441370, Accuracy 81.346%\n",
      "Epoch 17, Batch 569, LR 0.000064 Loss 6.440760, Accuracy 81.347%\n",
      "Epoch 17, Batch 570, LR 0.000064 Loss 6.441960, Accuracy 81.345%\n",
      "Epoch 17, Batch 571, LR 0.000064 Loss 6.441444, Accuracy 81.353%\n",
      "Epoch 17, Batch 572, LR 0.000064 Loss 6.442609, Accuracy 81.337%\n",
      "Epoch 17, Batch 573, LR 0.000064 Loss 6.442816, Accuracy 81.330%\n",
      "Epoch 17, Batch 574, LR 0.000064 Loss 6.442585, Accuracy 81.329%\n",
      "Epoch 17, Batch 575, LR 0.000064 Loss 6.441434, Accuracy 81.338%\n",
      "Epoch 17, Batch 576, LR 0.000064 Loss 6.442213, Accuracy 81.335%\n",
      "Epoch 17, Batch 577, LR 0.000064 Loss 6.442020, Accuracy 81.335%\n",
      "Epoch 17, Batch 578, LR 0.000064 Loss 6.442781, Accuracy 81.327%\n",
      "Epoch 17, Batch 579, LR 0.000064 Loss 6.442770, Accuracy 81.327%\n",
      "Epoch 17, Batch 580, LR 0.000064 Loss 6.441561, Accuracy 81.332%\n",
      "Epoch 17, Batch 581, LR 0.000064 Loss 6.443292, Accuracy 81.315%\n",
      "Epoch 17, Batch 582, LR 0.000064 Loss 6.442672, Accuracy 81.320%\n",
      "Epoch 17, Batch 583, LR 0.000064 Loss 6.441630, Accuracy 81.324%\n",
      "Epoch 17, Batch 584, LR 0.000064 Loss 6.441575, Accuracy 81.325%\n",
      "Epoch 17, Batch 585, LR 0.000064 Loss 6.441205, Accuracy 81.327%\n",
      "Epoch 17, Batch 586, LR 0.000064 Loss 6.441296, Accuracy 81.331%\n",
      "Epoch 17, Batch 587, LR 0.000064 Loss 6.441422, Accuracy 81.326%\n",
      "Epoch 17, Batch 588, LR 0.000064 Loss 6.442787, Accuracy 81.312%\n",
      "Epoch 17, Batch 589, LR 0.000064 Loss 6.442194, Accuracy 81.312%\n",
      "Epoch 17, Batch 590, LR 0.000064 Loss 6.442365, Accuracy 81.311%\n",
      "Epoch 17, Batch 591, LR 0.000064 Loss 6.442151, Accuracy 81.306%\n",
      "Epoch 17, Batch 592, LR 0.000064 Loss 6.441613, Accuracy 81.315%\n",
      "Epoch 17, Batch 593, LR 0.000064 Loss 6.441389, Accuracy 81.313%\n",
      "Epoch 17, Batch 594, LR 0.000064 Loss 6.442574, Accuracy 81.311%\n",
      "Epoch 17, Batch 595, LR 0.000064 Loss 6.441909, Accuracy 81.320%\n",
      "Epoch 17, Batch 596, LR 0.000064 Loss 6.443355, Accuracy 81.310%\n",
      "Epoch 17, Batch 597, LR 0.000064 Loss 6.444447, Accuracy 81.300%\n",
      "Epoch 17, Batch 598, LR 0.000064 Loss 6.443986, Accuracy 81.304%\n",
      "Epoch 17, Batch 599, LR 0.000064 Loss 6.443411, Accuracy 81.298%\n",
      "Epoch 17, Batch 600, LR 0.000064 Loss 6.442946, Accuracy 81.299%\n",
      "Epoch 17, Batch 601, LR 0.000064 Loss 6.443976, Accuracy 81.289%\n",
      "Epoch 17, Batch 602, LR 0.000064 Loss 6.443350, Accuracy 81.293%\n",
      "Epoch 17, Batch 603, LR 0.000064 Loss 6.443232, Accuracy 81.297%\n",
      "Epoch 17, Batch 604, LR 0.000064 Loss 6.444164, Accuracy 81.294%\n",
      "Epoch 17, Batch 605, LR 0.000064 Loss 6.445464, Accuracy 81.281%\n",
      "Epoch 17, Batch 606, LR 0.000064 Loss 6.445845, Accuracy 81.284%\n",
      "Epoch 17, Batch 607, LR 0.000064 Loss 6.445872, Accuracy 81.280%\n",
      "Epoch 17, Batch 608, LR 0.000064 Loss 6.446333, Accuracy 81.274%\n",
      "Epoch 17, Batch 609, LR 0.000064 Loss 6.446775, Accuracy 81.278%\n",
      "Epoch 17, Batch 610, LR 0.000064 Loss 6.446410, Accuracy 81.283%\n",
      "Epoch 17, Batch 611, LR 0.000064 Loss 6.448386, Accuracy 81.279%\n",
      "Epoch 17, Batch 612, LR 0.000064 Loss 6.449329, Accuracy 81.277%\n",
      "Epoch 17, Batch 613, LR 0.000064 Loss 6.448446, Accuracy 81.275%\n",
      "Epoch 17, Batch 614, LR 0.000064 Loss 6.447793, Accuracy 81.278%\n",
      "Epoch 17, Batch 615, LR 0.000064 Loss 6.448026, Accuracy 81.277%\n",
      "Epoch 17, Batch 616, LR 0.000064 Loss 6.449514, Accuracy 81.266%\n",
      "Epoch 17, Batch 617, LR 0.000064 Loss 6.450548, Accuracy 81.261%\n",
      "Epoch 17, Batch 618, LR 0.000064 Loss 6.451313, Accuracy 81.261%\n",
      "Epoch 17, Batch 619, LR 0.000064 Loss 6.452039, Accuracy 81.259%\n",
      "Epoch 17, Batch 620, LR 0.000064 Loss 6.452589, Accuracy 81.258%\n",
      "Epoch 17, Batch 621, LR 0.000064 Loss 6.451890, Accuracy 81.258%\n",
      "Epoch 17, Batch 622, LR 0.000064 Loss 6.451961, Accuracy 81.253%\n",
      "Epoch 17, Batch 623, LR 0.000064 Loss 6.452284, Accuracy 81.258%\n",
      "Epoch 17, Batch 624, LR 0.000064 Loss 6.453000, Accuracy 81.253%\n",
      "Epoch 17, Batch 625, LR 0.000064 Loss 6.453535, Accuracy 81.252%\n",
      "Epoch 17, Batch 626, LR 0.000064 Loss 6.453912, Accuracy 81.251%\n",
      "Epoch 17, Batch 627, LR 0.000064 Loss 6.454356, Accuracy 81.256%\n",
      "Epoch 17, Batch 628, LR 0.000064 Loss 6.455189, Accuracy 81.250%\n",
      "Epoch 17, Batch 629, LR 0.000064 Loss 6.455453, Accuracy 81.244%\n",
      "Epoch 17, Batch 630, LR 0.000064 Loss 6.455720, Accuracy 81.243%\n",
      "Epoch 17, Batch 631, LR 0.000064 Loss 6.454490, Accuracy 81.249%\n",
      "Epoch 17, Batch 632, LR 0.000064 Loss 6.454289, Accuracy 81.257%\n",
      "Epoch 17, Batch 633, LR 0.000064 Loss 6.453356, Accuracy 81.257%\n",
      "Epoch 17, Batch 634, LR 0.000064 Loss 6.453296, Accuracy 81.260%\n",
      "Epoch 17, Batch 635, LR 0.000064 Loss 6.452895, Accuracy 81.267%\n",
      "Epoch 17, Batch 636, LR 0.000064 Loss 6.452558, Accuracy 81.268%\n",
      "Epoch 17, Batch 637, LR 0.000064 Loss 6.453712, Accuracy 81.261%\n",
      "Epoch 17, Batch 638, LR 0.000064 Loss 6.453370, Accuracy 81.263%\n",
      "Epoch 17, Batch 639, LR 0.000064 Loss 6.454221, Accuracy 81.274%\n",
      "Epoch 17, Batch 640, LR 0.000064 Loss 6.454695, Accuracy 81.272%\n",
      "Epoch 17, Batch 641, LR 0.000064 Loss 6.453883, Accuracy 81.277%\n",
      "Epoch 17, Batch 642, LR 0.000064 Loss 6.453175, Accuracy 81.276%\n",
      "Epoch 17, Batch 643, LR 0.000064 Loss 6.453002, Accuracy 81.274%\n",
      "Epoch 17, Batch 644, LR 0.000064 Loss 6.452605, Accuracy 81.278%\n",
      "Epoch 17, Batch 645, LR 0.000064 Loss 6.452723, Accuracy 81.277%\n",
      "Epoch 17, Batch 646, LR 0.000064 Loss 6.451752, Accuracy 81.284%\n",
      "Epoch 17, Batch 647, LR 0.000064 Loss 6.452856, Accuracy 81.283%\n",
      "Epoch 17, Batch 648, LR 0.000064 Loss 6.450894, Accuracy 81.289%\n",
      "Epoch 17, Batch 649, LR 0.000064 Loss 6.450545, Accuracy 81.291%\n",
      "Epoch 17, Batch 650, LR 0.000064 Loss 6.451075, Accuracy 81.287%\n",
      "Epoch 17, Batch 651, LR 0.000064 Loss 6.452498, Accuracy 81.274%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 652, LR 0.000064 Loss 6.451940, Accuracy 81.279%\n",
      "Epoch 17, Batch 653, LR 0.000064 Loss 6.451193, Accuracy 81.283%\n",
      "Epoch 17, Batch 654, LR 0.000064 Loss 6.450894, Accuracy 81.285%\n",
      "Epoch 17, Batch 655, LR 0.000064 Loss 6.450130, Accuracy 81.288%\n",
      "Epoch 17, Batch 656, LR 0.000064 Loss 6.451358, Accuracy 81.282%\n",
      "Epoch 17, Batch 657, LR 0.000064 Loss 6.449295, Accuracy 81.295%\n",
      "Epoch 17, Batch 658, LR 0.000064 Loss 6.447595, Accuracy 81.307%\n",
      "Epoch 17, Batch 659, LR 0.000064 Loss 6.447397, Accuracy 81.308%\n",
      "Epoch 17, Batch 660, LR 0.000064 Loss 6.447940, Accuracy 81.309%\n",
      "Epoch 17, Batch 661, LR 0.000064 Loss 6.447466, Accuracy 81.314%\n",
      "Epoch 17, Batch 662, LR 0.000064 Loss 6.448279, Accuracy 81.307%\n",
      "Epoch 17, Batch 663, LR 0.000064 Loss 6.448268, Accuracy 81.305%\n",
      "Epoch 17, Batch 664, LR 0.000064 Loss 6.448139, Accuracy 81.301%\n",
      "Epoch 17, Batch 665, LR 0.000064 Loss 6.448347, Accuracy 81.299%\n",
      "Epoch 17, Batch 666, LR 0.000064 Loss 6.447944, Accuracy 81.299%\n",
      "Epoch 17, Batch 667, LR 0.000064 Loss 6.448438, Accuracy 81.298%\n",
      "Epoch 17, Batch 668, LR 0.000064 Loss 6.448456, Accuracy 81.300%\n",
      "Epoch 17, Batch 669, LR 0.000064 Loss 6.447379, Accuracy 81.306%\n",
      "Epoch 17, Batch 670, LR 0.000064 Loss 6.446807, Accuracy 81.312%\n",
      "Epoch 17, Batch 671, LR 0.000064 Loss 6.447943, Accuracy 81.307%\n",
      "Epoch 17, Batch 672, LR 0.000064 Loss 6.448463, Accuracy 81.307%\n",
      "Epoch 17, Batch 673, LR 0.000064 Loss 6.448940, Accuracy 81.305%\n",
      "Epoch 17, Batch 674, LR 0.000064 Loss 6.449338, Accuracy 81.304%\n",
      "Epoch 17, Batch 675, LR 0.000064 Loss 6.448933, Accuracy 81.304%\n",
      "Epoch 17, Batch 676, LR 0.000064 Loss 6.447537, Accuracy 81.309%\n",
      "Epoch 17, Batch 677, LR 0.000064 Loss 6.447571, Accuracy 81.308%\n",
      "Epoch 17, Batch 678, LR 0.000064 Loss 6.446914, Accuracy 81.313%\n",
      "Epoch 17, Batch 679, LR 0.000064 Loss 6.447473, Accuracy 81.318%\n",
      "Epoch 17, Batch 680, LR 0.000064 Loss 6.447193, Accuracy 81.313%\n",
      "Epoch 17, Batch 681, LR 0.000064 Loss 6.445904, Accuracy 81.322%\n",
      "Epoch 17, Batch 682, LR 0.000064 Loss 6.446154, Accuracy 81.328%\n",
      "Epoch 17, Batch 683, LR 0.000064 Loss 6.445325, Accuracy 81.334%\n",
      "Epoch 17, Batch 684, LR 0.000064 Loss 6.446229, Accuracy 81.329%\n",
      "Epoch 17, Batch 685, LR 0.000064 Loss 6.446225, Accuracy 81.328%\n",
      "Epoch 17, Batch 686, LR 0.000064 Loss 6.446014, Accuracy 81.331%\n",
      "Epoch 17, Batch 687, LR 0.000064 Loss 6.445541, Accuracy 81.334%\n",
      "Epoch 17, Batch 688, LR 0.000064 Loss 6.444983, Accuracy 81.337%\n",
      "Epoch 17, Batch 689, LR 0.000064 Loss 6.444356, Accuracy 81.335%\n",
      "Epoch 17, Batch 690, LR 0.000064 Loss 6.445012, Accuracy 81.330%\n",
      "Epoch 17, Batch 691, LR 0.000064 Loss 6.444669, Accuracy 81.335%\n",
      "Epoch 17, Batch 692, LR 0.000064 Loss 6.444720, Accuracy 81.335%\n",
      "Epoch 17, Batch 693, LR 0.000064 Loss 6.444651, Accuracy 81.336%\n",
      "Epoch 17, Batch 694, LR 0.000064 Loss 6.445459, Accuracy 81.336%\n",
      "Epoch 17, Batch 695, LR 0.000064 Loss 6.445701, Accuracy 81.338%\n",
      "Epoch 17, Batch 696, LR 0.000064 Loss 6.444579, Accuracy 81.348%\n",
      "Epoch 17, Batch 697, LR 0.000064 Loss 6.444771, Accuracy 81.346%\n",
      "Epoch 17, Batch 698, LR 0.000064 Loss 6.444149, Accuracy 81.347%\n",
      "Epoch 17, Batch 699, LR 0.000064 Loss 6.444029, Accuracy 81.353%\n",
      "Epoch 17, Batch 700, LR 0.000064 Loss 6.444164, Accuracy 81.354%\n",
      "Epoch 17, Batch 701, LR 0.000064 Loss 6.444089, Accuracy 81.357%\n",
      "Epoch 17, Batch 702, LR 0.000064 Loss 6.443225, Accuracy 81.361%\n",
      "Epoch 17, Batch 703, LR 0.000064 Loss 6.442562, Accuracy 81.364%\n",
      "Epoch 17, Batch 704, LR 0.000064 Loss 6.442228, Accuracy 81.364%\n",
      "Epoch 17, Batch 705, LR 0.000064 Loss 6.442595, Accuracy 81.355%\n",
      "Epoch 17, Batch 706, LR 0.000064 Loss 6.442791, Accuracy 81.354%\n",
      "Epoch 17, Batch 707, LR 0.000064 Loss 6.442523, Accuracy 81.357%\n",
      "Epoch 17, Batch 708, LR 0.000064 Loss 6.442003, Accuracy 81.364%\n",
      "Epoch 17, Batch 709, LR 0.000064 Loss 6.442132, Accuracy 81.356%\n",
      "Epoch 17, Batch 710, LR 0.000064 Loss 6.441644, Accuracy 81.359%\n",
      "Epoch 17, Batch 711, LR 0.000064 Loss 6.440737, Accuracy 81.362%\n",
      "Epoch 17, Batch 712, LR 0.000064 Loss 6.440779, Accuracy 81.361%\n",
      "Epoch 17, Batch 713, LR 0.000064 Loss 6.440182, Accuracy 81.364%\n",
      "Epoch 17, Batch 714, LR 0.000064 Loss 6.440473, Accuracy 81.368%\n",
      "Epoch 17, Batch 715, LR 0.000064 Loss 6.440685, Accuracy 81.366%\n",
      "Epoch 17, Batch 716, LR 0.000064 Loss 6.440047, Accuracy 81.363%\n",
      "Epoch 17, Batch 717, LR 0.000064 Loss 6.439606, Accuracy 81.361%\n",
      "Epoch 17, Batch 718, LR 0.000064 Loss 6.438261, Accuracy 81.369%\n",
      "Epoch 17, Batch 719, LR 0.000064 Loss 6.438737, Accuracy 81.365%\n",
      "Epoch 17, Batch 720, LR 0.000064 Loss 6.439674, Accuracy 81.357%\n",
      "Epoch 17, Batch 721, LR 0.000064 Loss 6.439068, Accuracy 81.359%\n",
      "Epoch 17, Batch 722, LR 0.000064 Loss 6.439385, Accuracy 81.357%\n",
      "Epoch 17, Batch 723, LR 0.000064 Loss 6.438973, Accuracy 81.353%\n",
      "Epoch 17, Batch 724, LR 0.000064 Loss 6.439643, Accuracy 81.350%\n",
      "Epoch 17, Batch 725, LR 0.000064 Loss 6.440263, Accuracy 81.349%\n",
      "Epoch 17, Batch 726, LR 0.000063 Loss 6.440878, Accuracy 81.350%\n",
      "Epoch 17, Batch 727, LR 0.000063 Loss 6.441199, Accuracy 81.343%\n",
      "Epoch 17, Batch 728, LR 0.000063 Loss 6.440382, Accuracy 81.354%\n",
      "Epoch 17, Batch 729, LR 0.000063 Loss 6.440067, Accuracy 81.348%\n",
      "Epoch 17, Batch 730, LR 0.000063 Loss 6.440788, Accuracy 81.341%\n",
      "Epoch 17, Batch 731, LR 0.000063 Loss 6.440103, Accuracy 81.338%\n",
      "Epoch 17, Batch 732, LR 0.000063 Loss 6.439151, Accuracy 81.342%\n",
      "Epoch 17, Batch 733, LR 0.000063 Loss 6.437912, Accuracy 81.350%\n",
      "Epoch 17, Batch 734, LR 0.000063 Loss 6.438151, Accuracy 81.343%\n",
      "Epoch 17, Batch 735, LR 0.000063 Loss 6.439181, Accuracy 81.338%\n",
      "Epoch 17, Batch 736, LR 0.000063 Loss 6.438840, Accuracy 81.334%\n",
      "Epoch 17, Batch 737, LR 0.000063 Loss 6.440481, Accuracy 81.326%\n",
      "Epoch 17, Batch 738, LR 0.000063 Loss 6.440882, Accuracy 81.323%\n",
      "Epoch 17, Batch 739, LR 0.000063 Loss 6.441501, Accuracy 81.321%\n",
      "Epoch 17, Batch 740, LR 0.000063 Loss 6.441617, Accuracy 81.319%\n",
      "Epoch 17, Batch 741, LR 0.000063 Loss 6.442993, Accuracy 81.311%\n",
      "Epoch 17, Batch 742, LR 0.000063 Loss 6.443974, Accuracy 81.304%\n",
      "Epoch 17, Batch 743, LR 0.000063 Loss 6.444654, Accuracy 81.304%\n",
      "Epoch 17, Batch 744, LR 0.000063 Loss 6.445775, Accuracy 81.296%\n",
      "Epoch 17, Batch 745, LR 0.000063 Loss 6.445686, Accuracy 81.295%\n",
      "Epoch 17, Batch 746, LR 0.000063 Loss 6.444470, Accuracy 81.301%\n",
      "Epoch 17, Batch 747, LR 0.000063 Loss 6.444334, Accuracy 81.302%\n",
      "Epoch 17, Batch 748, LR 0.000063 Loss 6.445047, Accuracy 81.301%\n",
      "Epoch 17, Batch 749, LR 0.000063 Loss 6.444954, Accuracy 81.306%\n",
      "Epoch 17, Batch 750, LR 0.000063 Loss 6.445367, Accuracy 81.305%\n",
      "Epoch 17, Batch 751, LR 0.000063 Loss 6.445686, Accuracy 81.305%\n",
      "Epoch 17, Batch 752, LR 0.000063 Loss 6.445450, Accuracy 81.304%\n",
      "Epoch 17, Batch 753, LR 0.000063 Loss 6.444545, Accuracy 81.311%\n",
      "Epoch 17, Batch 754, LR 0.000063 Loss 6.444731, Accuracy 81.315%\n",
      "Epoch 17, Batch 755, LR 0.000063 Loss 6.444702, Accuracy 81.314%\n",
      "Epoch 17, Batch 756, LR 0.000063 Loss 6.444746, Accuracy 81.314%\n",
      "Epoch 17, Batch 757, LR 0.000063 Loss 6.444973, Accuracy 81.306%\n",
      "Epoch 17, Batch 758, LR 0.000063 Loss 6.445606, Accuracy 81.302%\n",
      "Epoch 17, Batch 759, LR 0.000063 Loss 6.445476, Accuracy 81.300%\n",
      "Epoch 17, Batch 760, LR 0.000063 Loss 6.446186, Accuracy 81.301%\n",
      "Epoch 17, Batch 761, LR 0.000063 Loss 6.446605, Accuracy 81.298%\n",
      "Epoch 17, Batch 762, LR 0.000063 Loss 6.446765, Accuracy 81.295%\n",
      "Epoch 17, Batch 763, LR 0.000063 Loss 6.446404, Accuracy 81.297%\n",
      "Epoch 17, Batch 764, LR 0.000063 Loss 6.447528, Accuracy 81.291%\n",
      "Epoch 17, Batch 765, LR 0.000063 Loss 6.447598, Accuracy 81.290%\n",
      "Epoch 17, Batch 766, LR 0.000063 Loss 6.447724, Accuracy 81.293%\n",
      "Epoch 17, Batch 767, LR 0.000063 Loss 6.447542, Accuracy 81.300%\n",
      "Epoch 17, Batch 768, LR 0.000063 Loss 6.447515, Accuracy 81.301%\n",
      "Epoch 17, Batch 769, LR 0.000063 Loss 6.447902, Accuracy 81.298%\n",
      "Epoch 17, Batch 770, LR 0.000063 Loss 6.447780, Accuracy 81.298%\n",
      "Epoch 17, Batch 771, LR 0.000063 Loss 6.446618, Accuracy 81.305%\n",
      "Epoch 17, Batch 772, LR 0.000063 Loss 6.446162, Accuracy 81.308%\n",
      "Epoch 17, Batch 773, LR 0.000063 Loss 6.446529, Accuracy 81.311%\n",
      "Epoch 17, Batch 774, LR 0.000063 Loss 6.447292, Accuracy 81.307%\n",
      "Epoch 17, Batch 775, LR 0.000063 Loss 6.448010, Accuracy 81.304%\n",
      "Epoch 17, Batch 776, LR 0.000063 Loss 6.447624, Accuracy 81.302%\n",
      "Epoch 17, Batch 777, LR 0.000063 Loss 6.447783, Accuracy 81.300%\n",
      "Epoch 17, Batch 778, LR 0.000063 Loss 6.447841, Accuracy 81.300%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 779, LR 0.000063 Loss 6.449271, Accuracy 81.296%\n",
      "Epoch 17, Batch 780, LR 0.000063 Loss 6.450045, Accuracy 81.290%\n",
      "Epoch 17, Batch 781, LR 0.000063 Loss 6.450373, Accuracy 81.287%\n",
      "Epoch 17, Batch 782, LR 0.000063 Loss 6.450971, Accuracy 81.278%\n",
      "Epoch 17, Batch 783, LR 0.000063 Loss 6.450952, Accuracy 81.274%\n",
      "Epoch 17, Batch 784, LR 0.000063 Loss 6.451428, Accuracy 81.273%\n",
      "Epoch 17, Batch 785, LR 0.000063 Loss 6.451398, Accuracy 81.277%\n",
      "Epoch 17, Batch 786, LR 0.000063 Loss 6.452335, Accuracy 81.278%\n",
      "Epoch 17, Batch 787, LR 0.000063 Loss 6.452157, Accuracy 81.282%\n",
      "Epoch 17, Batch 788, LR 0.000063 Loss 6.452513, Accuracy 81.281%\n",
      "Epoch 17, Batch 789, LR 0.000063 Loss 6.452469, Accuracy 81.278%\n",
      "Epoch 17, Batch 790, LR 0.000063 Loss 6.451670, Accuracy 81.280%\n",
      "Epoch 17, Batch 791, LR 0.000063 Loss 6.451446, Accuracy 81.283%\n",
      "Epoch 17, Batch 792, LR 0.000063 Loss 6.451699, Accuracy 81.280%\n",
      "Epoch 17, Batch 793, LR 0.000063 Loss 6.451416, Accuracy 81.283%\n",
      "Epoch 17, Batch 794, LR 0.000063 Loss 6.451513, Accuracy 81.280%\n",
      "Epoch 17, Batch 795, LR 0.000063 Loss 6.452254, Accuracy 81.275%\n",
      "Epoch 17, Batch 796, LR 0.000063 Loss 6.452318, Accuracy 81.272%\n",
      "Epoch 17, Batch 797, LR 0.000063 Loss 6.451414, Accuracy 81.279%\n",
      "Epoch 17, Batch 798, LR 0.000063 Loss 6.450302, Accuracy 81.290%\n",
      "Epoch 17, Batch 799, LR 0.000063 Loss 6.451110, Accuracy 81.279%\n",
      "Epoch 17, Batch 800, LR 0.000063 Loss 6.451032, Accuracy 81.282%\n",
      "Epoch 17, Batch 801, LR 0.000063 Loss 6.449960, Accuracy 81.287%\n",
      "Epoch 17, Batch 802, LR 0.000063 Loss 6.449427, Accuracy 81.287%\n",
      "Epoch 17, Batch 803, LR 0.000063 Loss 6.448344, Accuracy 81.293%\n",
      "Epoch 17, Batch 804, LR 0.000063 Loss 6.448409, Accuracy 81.297%\n",
      "Epoch 17, Batch 805, LR 0.000063 Loss 6.448946, Accuracy 81.299%\n",
      "Epoch 17, Batch 806, LR 0.000063 Loss 6.449405, Accuracy 81.297%\n",
      "Epoch 17, Batch 807, LR 0.000063 Loss 6.448644, Accuracy 81.296%\n",
      "Epoch 17, Batch 808, LR 0.000063 Loss 6.448399, Accuracy 81.296%\n",
      "Epoch 17, Batch 809, LR 0.000063 Loss 6.448608, Accuracy 81.293%\n",
      "Epoch 17, Batch 810, LR 0.000063 Loss 6.448511, Accuracy 81.299%\n",
      "Epoch 17, Batch 811, LR 0.000063 Loss 6.448015, Accuracy 81.299%\n",
      "Epoch 17, Batch 812, LR 0.000063 Loss 6.447850, Accuracy 81.301%\n",
      "Epoch 17, Batch 813, LR 0.000063 Loss 6.448941, Accuracy 81.291%\n",
      "Epoch 17, Batch 814, LR 0.000063 Loss 6.448917, Accuracy 81.290%\n",
      "Epoch 17, Batch 815, LR 0.000063 Loss 6.448635, Accuracy 81.297%\n",
      "Epoch 17, Batch 816, LR 0.000063 Loss 6.448246, Accuracy 81.297%\n",
      "Epoch 17, Batch 817, LR 0.000063 Loss 6.449152, Accuracy 81.294%\n",
      "Epoch 17, Batch 818, LR 0.000063 Loss 6.448969, Accuracy 81.298%\n",
      "Epoch 17, Batch 819, LR 0.000063 Loss 6.448870, Accuracy 81.299%\n",
      "Epoch 17, Batch 820, LR 0.000063 Loss 6.448545, Accuracy 81.300%\n",
      "Epoch 17, Batch 821, LR 0.000063 Loss 6.448453, Accuracy 81.299%\n",
      "Epoch 17, Batch 822, LR 0.000063 Loss 6.448512, Accuracy 81.298%\n",
      "Epoch 17, Batch 823, LR 0.000063 Loss 6.448641, Accuracy 81.298%\n",
      "Epoch 17, Batch 824, LR 0.000063 Loss 6.447621, Accuracy 81.304%\n",
      "Epoch 17, Batch 825, LR 0.000063 Loss 6.447114, Accuracy 81.305%\n",
      "Epoch 17, Batch 826, LR 0.000063 Loss 6.446135, Accuracy 81.310%\n",
      "Epoch 17, Batch 827, LR 0.000063 Loss 6.446467, Accuracy 81.307%\n",
      "Epoch 17, Batch 828, LR 0.000063 Loss 6.446229, Accuracy 81.306%\n",
      "Epoch 17, Batch 829, LR 0.000063 Loss 6.446504, Accuracy 81.300%\n",
      "Epoch 17, Batch 830, LR 0.000063 Loss 6.446197, Accuracy 81.300%\n",
      "Epoch 17, Batch 831, LR 0.000063 Loss 6.446143, Accuracy 81.304%\n",
      "Epoch 17, Batch 832, LR 0.000063 Loss 6.446070, Accuracy 81.301%\n",
      "Epoch 17, Batch 833, LR 0.000063 Loss 6.444992, Accuracy 81.304%\n",
      "Epoch 17, Batch 834, LR 0.000063 Loss 6.445742, Accuracy 81.301%\n",
      "Epoch 17, Batch 835, LR 0.000063 Loss 6.446130, Accuracy 81.301%\n",
      "Epoch 17, Batch 836, LR 0.000063 Loss 6.445160, Accuracy 81.310%\n",
      "Epoch 17, Batch 837, LR 0.000063 Loss 6.445147, Accuracy 81.306%\n",
      "Epoch 17, Batch 838, LR 0.000063 Loss 6.445858, Accuracy 81.304%\n",
      "Epoch 17, Batch 839, LR 0.000063 Loss 6.444932, Accuracy 81.309%\n",
      "Epoch 17, Batch 840, LR 0.000063 Loss 6.445109, Accuracy 81.313%\n",
      "Epoch 17, Batch 841, LR 0.000063 Loss 6.445390, Accuracy 81.310%\n",
      "Epoch 17, Batch 842, LR 0.000063 Loss 6.445098, Accuracy 81.310%\n",
      "Epoch 17, Batch 843, LR 0.000063 Loss 6.445278, Accuracy 81.315%\n",
      "Epoch 17, Batch 844, LR 0.000063 Loss 6.443830, Accuracy 81.322%\n",
      "Epoch 17, Batch 845, LR 0.000063 Loss 6.443598, Accuracy 81.322%\n",
      "Epoch 17, Batch 846, LR 0.000063 Loss 6.443892, Accuracy 81.325%\n",
      "Epoch 17, Batch 847, LR 0.000063 Loss 6.443386, Accuracy 81.326%\n",
      "Epoch 17, Batch 848, LR 0.000063 Loss 6.444620, Accuracy 81.325%\n",
      "Epoch 17, Batch 849, LR 0.000063 Loss 6.443986, Accuracy 81.326%\n",
      "Epoch 17, Batch 850, LR 0.000063 Loss 6.443865, Accuracy 81.324%\n",
      "Epoch 17, Batch 851, LR 0.000063 Loss 6.442669, Accuracy 81.330%\n",
      "Epoch 17, Batch 852, LR 0.000063 Loss 6.443288, Accuracy 81.329%\n",
      "Epoch 17, Batch 853, LR 0.000063 Loss 6.443120, Accuracy 81.331%\n",
      "Epoch 17, Batch 854, LR 0.000063 Loss 6.443321, Accuracy 81.335%\n",
      "Epoch 17, Batch 855, LR 0.000063 Loss 6.443717, Accuracy 81.333%\n",
      "Epoch 17, Batch 856, LR 0.000063 Loss 6.444584, Accuracy 81.331%\n",
      "Epoch 17, Batch 857, LR 0.000063 Loss 6.444608, Accuracy 81.327%\n",
      "Epoch 17, Batch 858, LR 0.000063 Loss 6.443100, Accuracy 81.339%\n",
      "Epoch 17, Batch 859, LR 0.000063 Loss 6.444025, Accuracy 81.334%\n",
      "Epoch 17, Batch 860, LR 0.000063 Loss 6.443071, Accuracy 81.338%\n",
      "Epoch 17, Batch 861, LR 0.000063 Loss 6.443656, Accuracy 81.329%\n",
      "Epoch 17, Batch 862, LR 0.000063 Loss 6.443742, Accuracy 81.330%\n",
      "Epoch 17, Batch 863, LR 0.000063 Loss 6.443499, Accuracy 81.331%\n",
      "Epoch 17, Batch 864, LR 0.000063 Loss 6.444332, Accuracy 81.325%\n",
      "Epoch 17, Batch 865, LR 0.000063 Loss 6.444650, Accuracy 81.320%\n",
      "Epoch 17, Batch 866, LR 0.000063 Loss 6.445432, Accuracy 81.323%\n",
      "Epoch 17, Batch 867, LR 0.000063 Loss 6.445513, Accuracy 81.323%\n",
      "Epoch 17, Batch 868, LR 0.000063 Loss 6.445342, Accuracy 81.327%\n",
      "Epoch 17, Batch 869, LR 0.000063 Loss 6.445730, Accuracy 81.323%\n",
      "Epoch 17, Batch 870, LR 0.000063 Loss 6.445069, Accuracy 81.328%\n",
      "Epoch 17, Batch 871, LR 0.000063 Loss 6.445440, Accuracy 81.327%\n",
      "Epoch 17, Batch 872, LR 0.000063 Loss 6.445799, Accuracy 81.325%\n",
      "Epoch 17, Batch 873, LR 0.000063 Loss 6.445981, Accuracy 81.325%\n",
      "Epoch 17, Batch 874, LR 0.000063 Loss 6.445226, Accuracy 81.330%\n",
      "Epoch 17, Batch 875, LR 0.000063 Loss 6.444389, Accuracy 81.333%\n",
      "Epoch 17, Batch 876, LR 0.000063 Loss 6.444636, Accuracy 81.334%\n",
      "Epoch 17, Batch 877, LR 0.000063 Loss 6.443809, Accuracy 81.336%\n",
      "Epoch 17, Batch 878, LR 0.000063 Loss 6.443549, Accuracy 81.336%\n",
      "Epoch 17, Batch 879, LR 0.000063 Loss 6.443419, Accuracy 81.338%\n",
      "Epoch 17, Batch 880, LR 0.000063 Loss 6.444410, Accuracy 81.333%\n",
      "Epoch 17, Batch 881, LR 0.000063 Loss 6.443651, Accuracy 81.341%\n",
      "Epoch 17, Batch 882, LR 0.000063 Loss 6.443187, Accuracy 81.350%\n",
      "Epoch 17, Batch 883, LR 0.000063 Loss 6.443351, Accuracy 81.349%\n",
      "Epoch 17, Batch 884, LR 0.000063 Loss 6.442160, Accuracy 81.354%\n",
      "Epoch 17, Batch 885, LR 0.000063 Loss 6.441257, Accuracy 81.357%\n",
      "Epoch 17, Batch 886, LR 0.000063 Loss 6.440868, Accuracy 81.354%\n",
      "Epoch 17, Batch 887, LR 0.000063 Loss 6.440471, Accuracy 81.359%\n",
      "Epoch 17, Batch 888, LR 0.000063 Loss 6.439891, Accuracy 81.359%\n",
      "Epoch 17, Batch 889, LR 0.000063 Loss 6.439738, Accuracy 81.361%\n",
      "Epoch 17, Batch 890, LR 0.000063 Loss 6.439547, Accuracy 81.361%\n",
      "Epoch 17, Batch 891, LR 0.000063 Loss 6.439725, Accuracy 81.361%\n",
      "Epoch 17, Batch 892, LR 0.000063 Loss 6.439508, Accuracy 81.365%\n",
      "Epoch 17, Batch 893, LR 0.000063 Loss 6.438991, Accuracy 81.369%\n",
      "Epoch 17, Batch 894, LR 0.000063 Loss 6.439079, Accuracy 81.373%\n",
      "Epoch 17, Batch 895, LR 0.000063 Loss 6.439124, Accuracy 81.373%\n",
      "Epoch 17, Batch 896, LR 0.000063 Loss 6.439413, Accuracy 81.367%\n",
      "Epoch 17, Batch 897, LR 0.000063 Loss 6.439379, Accuracy 81.365%\n",
      "Epoch 17, Batch 898, LR 0.000063 Loss 6.439296, Accuracy 81.367%\n",
      "Epoch 17, Batch 899, LR 0.000063 Loss 6.438801, Accuracy 81.368%\n",
      "Epoch 17, Batch 900, LR 0.000063 Loss 6.438733, Accuracy 81.370%\n",
      "Epoch 17, Batch 901, LR 0.000063 Loss 6.438664, Accuracy 81.370%\n",
      "Epoch 17, Batch 902, LR 0.000063 Loss 6.439005, Accuracy 81.366%\n",
      "Epoch 17, Batch 903, LR 0.000063 Loss 6.438801, Accuracy 81.371%\n",
      "Epoch 17, Batch 904, LR 0.000063 Loss 6.439105, Accuracy 81.373%\n",
      "Epoch 17, Batch 905, LR 0.000063 Loss 6.440104, Accuracy 81.367%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 906, LR 0.000063 Loss 6.439172, Accuracy 81.373%\n",
      "Epoch 17, Batch 907, LR 0.000063 Loss 6.438828, Accuracy 81.376%\n",
      "Epoch 17, Batch 908, LR 0.000063 Loss 6.439160, Accuracy 81.375%\n",
      "Epoch 17, Batch 909, LR 0.000063 Loss 6.439171, Accuracy 81.371%\n",
      "Epoch 17, Batch 910, LR 0.000063 Loss 6.438853, Accuracy 81.373%\n",
      "Epoch 17, Batch 911, LR 0.000063 Loss 6.438442, Accuracy 81.370%\n",
      "Epoch 17, Batch 912, LR 0.000063 Loss 6.437934, Accuracy 81.376%\n",
      "Epoch 17, Batch 913, LR 0.000063 Loss 6.437386, Accuracy 81.382%\n",
      "Epoch 17, Batch 914, LR 0.000063 Loss 6.436079, Accuracy 81.388%\n",
      "Epoch 17, Batch 915, LR 0.000063 Loss 6.436615, Accuracy 81.386%\n",
      "Epoch 17, Batch 916, LR 0.000063 Loss 6.436801, Accuracy 81.385%\n",
      "Epoch 17, Batch 917, LR 0.000063 Loss 6.437127, Accuracy 81.380%\n",
      "Epoch 17, Batch 918, LR 0.000063 Loss 6.437369, Accuracy 81.379%\n",
      "Epoch 17, Batch 919, LR 0.000063 Loss 6.436865, Accuracy 81.383%\n",
      "Epoch 17, Batch 920, LR 0.000063 Loss 6.436326, Accuracy 81.382%\n",
      "Epoch 17, Batch 921, LR 0.000063 Loss 6.436805, Accuracy 81.381%\n",
      "Epoch 17, Batch 922, LR 0.000063 Loss 6.437146, Accuracy 81.375%\n",
      "Epoch 17, Batch 923, LR 0.000063 Loss 6.437588, Accuracy 81.368%\n",
      "Epoch 17, Batch 924, LR 0.000063 Loss 6.436952, Accuracy 81.369%\n",
      "Epoch 17, Batch 925, LR 0.000063 Loss 6.437569, Accuracy 81.367%\n",
      "Epoch 17, Batch 926, LR 0.000063 Loss 6.437101, Accuracy 81.372%\n",
      "Epoch 17, Batch 927, LR 0.000063 Loss 6.437033, Accuracy 81.376%\n",
      "Epoch 17, Batch 928, LR 0.000063 Loss 6.436967, Accuracy 81.375%\n",
      "Epoch 17, Batch 929, LR 0.000063 Loss 6.436669, Accuracy 81.376%\n",
      "Epoch 17, Batch 930, LR 0.000063 Loss 6.436857, Accuracy 81.372%\n",
      "Epoch 17, Batch 931, LR 0.000063 Loss 6.437647, Accuracy 81.367%\n",
      "Epoch 17, Batch 932, LR 0.000063 Loss 6.437271, Accuracy 81.366%\n",
      "Epoch 17, Batch 933, LR 0.000063 Loss 6.437446, Accuracy 81.364%\n",
      "Epoch 17, Batch 934, LR 0.000063 Loss 6.437022, Accuracy 81.365%\n",
      "Epoch 17, Batch 935, LR 0.000063 Loss 6.437154, Accuracy 81.365%\n",
      "Epoch 17, Batch 936, LR 0.000063 Loss 6.437224, Accuracy 81.365%\n",
      "Epoch 17, Batch 937, LR 0.000063 Loss 6.437910, Accuracy 81.361%\n",
      "Epoch 17, Batch 938, LR 0.000063 Loss 6.437774, Accuracy 81.361%\n",
      "Epoch 17, Batch 939, LR 0.000063 Loss 6.437178, Accuracy 81.367%\n",
      "Epoch 17, Batch 940, LR 0.000063 Loss 6.437183, Accuracy 81.367%\n",
      "Epoch 17, Batch 941, LR 0.000063 Loss 6.437018, Accuracy 81.366%\n",
      "Epoch 17, Batch 942, LR 0.000063 Loss 6.436637, Accuracy 81.374%\n",
      "Epoch 17, Batch 943, LR 0.000063 Loss 6.436369, Accuracy 81.369%\n",
      "Epoch 17, Batch 944, LR 0.000063 Loss 6.436584, Accuracy 81.369%\n",
      "Epoch 17, Batch 945, LR 0.000063 Loss 6.437240, Accuracy 81.361%\n",
      "Epoch 17, Batch 946, LR 0.000063 Loss 6.437745, Accuracy 81.357%\n",
      "Epoch 17, Batch 947, LR 0.000063 Loss 6.437339, Accuracy 81.352%\n",
      "Epoch 17, Batch 948, LR 0.000063 Loss 6.436885, Accuracy 81.359%\n",
      "Epoch 17, Batch 949, LR 0.000063 Loss 6.436474, Accuracy 81.360%\n",
      "Epoch 17, Batch 950, LR 0.000063 Loss 6.436782, Accuracy 81.354%\n",
      "Epoch 17, Batch 951, LR 0.000063 Loss 6.437072, Accuracy 81.350%\n",
      "Epoch 17, Batch 952, LR 0.000063 Loss 6.437514, Accuracy 81.348%\n",
      "Epoch 17, Batch 953, LR 0.000063 Loss 6.436771, Accuracy 81.353%\n",
      "Epoch 17, Batch 954, LR 0.000063 Loss 6.436141, Accuracy 81.358%\n",
      "Epoch 17, Batch 955, LR 0.000063 Loss 6.435276, Accuracy 81.364%\n",
      "Epoch 17, Batch 956, LR 0.000063 Loss 6.435135, Accuracy 81.364%\n",
      "Epoch 17, Batch 957, LR 0.000063 Loss 6.435096, Accuracy 81.367%\n",
      "Epoch 17, Batch 958, LR 0.000063 Loss 6.434671, Accuracy 81.367%\n",
      "Epoch 17, Batch 959, LR 0.000063 Loss 6.434444, Accuracy 81.369%\n",
      "Epoch 17, Batch 960, LR 0.000063 Loss 6.435031, Accuracy 81.361%\n",
      "Epoch 17, Batch 961, LR 0.000063 Loss 6.435419, Accuracy 81.359%\n",
      "Epoch 17, Batch 962, LR 0.000063 Loss 6.435502, Accuracy 81.356%\n",
      "Epoch 17, Batch 963, LR 0.000063 Loss 6.435161, Accuracy 81.358%\n",
      "Epoch 17, Batch 964, LR 0.000063 Loss 6.435076, Accuracy 81.363%\n",
      "Epoch 17, Batch 965, LR 0.000063 Loss 6.435648, Accuracy 81.361%\n",
      "Epoch 17, Batch 966, LR 0.000063 Loss 6.436108, Accuracy 81.359%\n",
      "Epoch 17, Batch 967, LR 0.000063 Loss 6.437175, Accuracy 81.351%\n",
      "Epoch 17, Batch 968, LR 0.000063 Loss 6.437564, Accuracy 81.348%\n",
      "Epoch 17, Batch 969, LR 0.000063 Loss 6.437073, Accuracy 81.352%\n",
      "Epoch 17, Batch 970, LR 0.000063 Loss 6.437460, Accuracy 81.354%\n",
      "Epoch 17, Batch 971, LR 0.000063 Loss 6.437824, Accuracy 81.352%\n",
      "Epoch 17, Batch 972, LR 0.000063 Loss 6.437985, Accuracy 81.347%\n",
      "Epoch 17, Batch 973, LR 0.000063 Loss 6.437339, Accuracy 81.351%\n",
      "Epoch 17, Batch 974, LR 0.000063 Loss 6.437841, Accuracy 81.353%\n",
      "Epoch 17, Batch 975, LR 0.000063 Loss 6.437091, Accuracy 81.359%\n",
      "Epoch 17, Batch 976, LR 0.000063 Loss 6.437406, Accuracy 81.356%\n",
      "Epoch 17, Batch 977, LR 0.000063 Loss 6.437312, Accuracy 81.358%\n",
      "Epoch 17, Batch 978, LR 0.000063 Loss 6.437789, Accuracy 81.359%\n",
      "Epoch 17, Batch 979, LR 0.000063 Loss 6.438077, Accuracy 81.361%\n",
      "Epoch 17, Batch 980, LR 0.000063 Loss 6.438165, Accuracy 81.359%\n",
      "Epoch 17, Batch 981, LR 0.000063 Loss 6.437710, Accuracy 81.361%\n",
      "Epoch 17, Batch 982, LR 0.000063 Loss 6.437459, Accuracy 81.363%\n",
      "Epoch 17, Batch 983, LR 0.000063 Loss 6.438056, Accuracy 81.360%\n",
      "Epoch 17, Batch 984, LR 0.000063 Loss 6.439051, Accuracy 81.356%\n",
      "Epoch 17, Batch 985, LR 0.000063 Loss 6.438556, Accuracy 81.356%\n",
      "Epoch 17, Batch 986, LR 0.000063 Loss 6.437391, Accuracy 81.360%\n",
      "Epoch 17, Batch 987, LR 0.000063 Loss 6.437765, Accuracy 81.354%\n",
      "Epoch 17, Batch 988, LR 0.000063 Loss 6.438438, Accuracy 81.352%\n",
      "Epoch 17, Batch 989, LR 0.000062 Loss 6.438171, Accuracy 81.354%\n",
      "Epoch 17, Batch 990, LR 0.000062 Loss 6.437583, Accuracy 81.357%\n",
      "Epoch 17, Batch 991, LR 0.000062 Loss 6.437868, Accuracy 81.356%\n",
      "Epoch 17, Batch 992, LR 0.000062 Loss 6.437176, Accuracy 81.363%\n",
      "Epoch 17, Batch 993, LR 0.000062 Loss 6.436650, Accuracy 81.366%\n",
      "Epoch 17, Batch 994, LR 0.000062 Loss 6.436578, Accuracy 81.366%\n",
      "Epoch 17, Batch 995, LR 0.000062 Loss 6.436441, Accuracy 81.366%\n",
      "Epoch 17, Batch 996, LR 0.000062 Loss 6.437167, Accuracy 81.362%\n",
      "Epoch 17, Batch 997, LR 0.000062 Loss 6.437137, Accuracy 81.360%\n",
      "Epoch 17, Batch 998, LR 0.000062 Loss 6.437667, Accuracy 81.357%\n",
      "Epoch 17, Batch 999, LR 0.000062 Loss 6.437882, Accuracy 81.360%\n",
      "Epoch 17, Batch 1000, LR 0.000062 Loss 6.438333, Accuracy 81.360%\n",
      "Epoch 17, Batch 1001, LR 0.000062 Loss 6.438576, Accuracy 81.364%\n",
      "Epoch 17, Batch 1002, LR 0.000062 Loss 6.438795, Accuracy 81.361%\n",
      "Epoch 17, Batch 1003, LR 0.000062 Loss 6.438557, Accuracy 81.361%\n",
      "Epoch 17, Batch 1004, LR 0.000062 Loss 6.438586, Accuracy 81.358%\n",
      "Epoch 17, Batch 1005, LR 0.000062 Loss 6.438299, Accuracy 81.361%\n",
      "Epoch 17, Batch 1006, LR 0.000062 Loss 6.438821, Accuracy 81.361%\n",
      "Epoch 17, Batch 1007, LR 0.000062 Loss 6.438615, Accuracy 81.366%\n",
      "Epoch 17, Batch 1008, LR 0.000062 Loss 6.439188, Accuracy 81.360%\n",
      "Epoch 17, Batch 1009, LR 0.000062 Loss 6.438922, Accuracy 81.366%\n",
      "Epoch 17, Batch 1010, LR 0.000062 Loss 6.438517, Accuracy 81.365%\n",
      "Epoch 17, Batch 1011, LR 0.000062 Loss 6.438834, Accuracy 81.365%\n",
      "Epoch 17, Batch 1012, LR 0.000062 Loss 6.439628, Accuracy 81.360%\n",
      "Epoch 17, Batch 1013, LR 0.000062 Loss 6.439070, Accuracy 81.363%\n",
      "Epoch 17, Batch 1014, LR 0.000062 Loss 6.438823, Accuracy 81.365%\n",
      "Epoch 17, Batch 1015, LR 0.000062 Loss 6.438861, Accuracy 81.367%\n",
      "Epoch 17, Batch 1016, LR 0.000062 Loss 6.438959, Accuracy 81.365%\n",
      "Epoch 17, Batch 1017, LR 0.000062 Loss 6.439162, Accuracy 81.365%\n",
      "Epoch 17, Batch 1018, LR 0.000062 Loss 6.439230, Accuracy 81.362%\n",
      "Epoch 17, Batch 1019, LR 0.000062 Loss 6.439686, Accuracy 81.360%\n",
      "Epoch 17, Batch 1020, LR 0.000062 Loss 6.439781, Accuracy 81.358%\n",
      "Epoch 17, Batch 1021, LR 0.000062 Loss 6.439959, Accuracy 81.356%\n",
      "Epoch 17, Batch 1022, LR 0.000062 Loss 6.439357, Accuracy 81.359%\n",
      "Epoch 17, Batch 1023, LR 0.000062 Loss 6.440285, Accuracy 81.357%\n",
      "Epoch 17, Batch 1024, LR 0.000062 Loss 6.440315, Accuracy 81.361%\n",
      "Epoch 17, Batch 1025, LR 0.000062 Loss 6.440166, Accuracy 81.359%\n",
      "Epoch 17, Batch 1026, LR 0.000062 Loss 6.440172, Accuracy 81.358%\n",
      "Epoch 17, Batch 1027, LR 0.000062 Loss 6.439454, Accuracy 81.366%\n",
      "Epoch 17, Batch 1028, LR 0.000062 Loss 6.438659, Accuracy 81.370%\n",
      "Epoch 17, Batch 1029, LR 0.000062 Loss 6.438657, Accuracy 81.371%\n",
      "Epoch 17, Batch 1030, LR 0.000062 Loss 6.438642, Accuracy 81.370%\n",
      "Epoch 17, Batch 1031, LR 0.000062 Loss 6.439302, Accuracy 81.369%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 1032, LR 0.000062 Loss 6.438641, Accuracy 81.373%\n",
      "Epoch 17, Batch 1033, LR 0.000062 Loss 6.438705, Accuracy 81.372%\n",
      "Epoch 17, Batch 1034, LR 0.000062 Loss 6.438009, Accuracy 81.375%\n",
      "Epoch 17, Batch 1035, LR 0.000062 Loss 6.438271, Accuracy 81.372%\n",
      "Epoch 17, Batch 1036, LR 0.000062 Loss 6.438218, Accuracy 81.372%\n",
      "Epoch 17, Batch 1037, LR 0.000062 Loss 6.438329, Accuracy 81.370%\n",
      "Epoch 17, Batch 1038, LR 0.000062 Loss 6.438375, Accuracy 81.368%\n",
      "Epoch 17, Batch 1039, LR 0.000062 Loss 6.437804, Accuracy 81.374%\n",
      "Epoch 17, Batch 1040, LR 0.000062 Loss 6.437704, Accuracy 81.372%\n",
      "Epoch 17, Batch 1041, LR 0.000062 Loss 6.437410, Accuracy 81.374%\n",
      "Epoch 17, Batch 1042, LR 0.000062 Loss 6.437354, Accuracy 81.376%\n",
      "Epoch 17, Batch 1043, LR 0.000062 Loss 6.436606, Accuracy 81.380%\n",
      "Epoch 17, Batch 1044, LR 0.000062 Loss 6.436319, Accuracy 81.379%\n",
      "Epoch 17, Batch 1045, LR 0.000062 Loss 6.436651, Accuracy 81.377%\n",
      "Epoch 17, Batch 1046, LR 0.000062 Loss 6.436504, Accuracy 81.376%\n",
      "Epoch 17, Batch 1047, LR 0.000062 Loss 6.436106, Accuracy 81.380%\n",
      "Epoch 17, Loss (train set) 6.436106, Accuracy (train set) 81.380%\n",
      "Epoch 18, Batch 1, LR 0.000062 Loss 5.691449, Accuracy 88.281%\n",
      "Epoch 18, Batch 2, LR 0.000062 Loss 5.599547, Accuracy 88.281%\n",
      "Epoch 18, Batch 3, LR 0.000062 Loss 5.905757, Accuracy 85.938%\n",
      "Epoch 18, Batch 4, LR 0.000062 Loss 5.785020, Accuracy 85.156%\n",
      "Epoch 18, Batch 5, LR 0.000062 Loss 6.017684, Accuracy 82.969%\n",
      "Epoch 18, Batch 6, LR 0.000062 Loss 6.075384, Accuracy 82.161%\n",
      "Epoch 18, Batch 7, LR 0.000062 Loss 6.072164, Accuracy 81.362%\n",
      "Epoch 18, Batch 8, LR 0.000062 Loss 6.061530, Accuracy 81.348%\n",
      "Epoch 18, Batch 9, LR 0.000062 Loss 6.016810, Accuracy 81.944%\n",
      "Epoch 18, Batch 10, LR 0.000062 Loss 6.072610, Accuracy 81.953%\n",
      "Epoch 18, Batch 11, LR 0.000062 Loss 6.135103, Accuracy 81.392%\n",
      "Epoch 18, Batch 12, LR 0.000062 Loss 6.077879, Accuracy 81.576%\n",
      "Epoch 18, Batch 13, LR 0.000062 Loss 6.076174, Accuracy 81.791%\n",
      "Epoch 18, Batch 14, LR 0.000062 Loss 6.145954, Accuracy 81.306%\n",
      "Epoch 18, Batch 15, LR 0.000062 Loss 6.134836, Accuracy 81.823%\n",
      "Epoch 18, Batch 16, LR 0.000062 Loss 6.165801, Accuracy 81.787%\n",
      "Epoch 18, Batch 17, LR 0.000062 Loss 6.207534, Accuracy 81.388%\n",
      "Epoch 18, Batch 18, LR 0.000062 Loss 6.196601, Accuracy 81.467%\n",
      "Epoch 18, Batch 19, LR 0.000062 Loss 6.200166, Accuracy 81.538%\n",
      "Epoch 18, Batch 20, LR 0.000062 Loss 6.152064, Accuracy 81.758%\n",
      "Epoch 18, Batch 21, LR 0.000062 Loss 6.202417, Accuracy 81.362%\n",
      "Epoch 18, Batch 22, LR 0.000062 Loss 6.220285, Accuracy 81.321%\n",
      "Epoch 18, Batch 23, LR 0.000062 Loss 6.208425, Accuracy 81.488%\n",
      "Epoch 18, Batch 24, LR 0.000062 Loss 6.180549, Accuracy 81.706%\n",
      "Epoch 18, Batch 25, LR 0.000062 Loss 6.211734, Accuracy 81.719%\n",
      "Epoch 18, Batch 26, LR 0.000062 Loss 6.236852, Accuracy 81.611%\n",
      "Epoch 18, Batch 27, LR 0.000062 Loss 6.274194, Accuracy 81.481%\n",
      "Epoch 18, Batch 28, LR 0.000062 Loss 6.228891, Accuracy 81.724%\n",
      "Epoch 18, Batch 29, LR 0.000062 Loss 6.227985, Accuracy 81.870%\n",
      "Epoch 18, Batch 30, LR 0.000062 Loss 6.244909, Accuracy 81.797%\n",
      "Epoch 18, Batch 31, LR 0.000062 Loss 6.216067, Accuracy 81.956%\n",
      "Epoch 18, Batch 32, LR 0.000062 Loss 6.214734, Accuracy 82.031%\n",
      "Epoch 18, Batch 33, LR 0.000062 Loss 6.220062, Accuracy 82.126%\n",
      "Epoch 18, Batch 34, LR 0.000062 Loss 6.240531, Accuracy 82.077%\n",
      "Epoch 18, Batch 35, LR 0.000062 Loss 6.239801, Accuracy 82.009%\n",
      "Epoch 18, Batch 36, LR 0.000062 Loss 6.231905, Accuracy 82.010%\n",
      "Epoch 18, Batch 37, LR 0.000062 Loss 6.249221, Accuracy 81.883%\n",
      "Epoch 18, Batch 38, LR 0.000062 Loss 6.264917, Accuracy 81.785%\n",
      "Epoch 18, Batch 39, LR 0.000062 Loss 6.256737, Accuracy 81.831%\n",
      "Epoch 18, Batch 40, LR 0.000062 Loss 6.262022, Accuracy 81.816%\n",
      "Epoch 18, Batch 41, LR 0.000062 Loss 6.257601, Accuracy 81.860%\n",
      "Epoch 18, Batch 42, LR 0.000062 Loss 6.266556, Accuracy 81.864%\n",
      "Epoch 18, Batch 43, LR 0.000062 Loss 6.290228, Accuracy 81.850%\n",
      "Epoch 18, Batch 44, LR 0.000062 Loss 6.300356, Accuracy 81.800%\n",
      "Epoch 18, Batch 45, LR 0.000062 Loss 6.301111, Accuracy 81.788%\n",
      "Epoch 18, Batch 46, LR 0.000062 Loss 6.293048, Accuracy 81.912%\n",
      "Epoch 18, Batch 47, LR 0.000062 Loss 6.311932, Accuracy 81.815%\n",
      "Epoch 18, Batch 48, LR 0.000062 Loss 6.295830, Accuracy 81.868%\n",
      "Epoch 18, Batch 49, LR 0.000062 Loss 6.297316, Accuracy 81.888%\n",
      "Epoch 18, Batch 50, LR 0.000062 Loss 6.295448, Accuracy 82.000%\n",
      "Epoch 18, Batch 51, LR 0.000062 Loss 6.301994, Accuracy 81.955%\n",
      "Epoch 18, Batch 52, LR 0.000062 Loss 6.313596, Accuracy 81.911%\n",
      "Epoch 18, Batch 53, LR 0.000062 Loss 6.308530, Accuracy 81.987%\n",
      "Epoch 18, Batch 54, LR 0.000062 Loss 6.286642, Accuracy 82.104%\n",
      "Epoch 18, Batch 55, LR 0.000062 Loss 6.292642, Accuracy 82.088%\n",
      "Epoch 18, Batch 56, LR 0.000062 Loss 6.295316, Accuracy 82.115%\n",
      "Epoch 18, Batch 57, LR 0.000062 Loss 6.292457, Accuracy 82.155%\n",
      "Epoch 18, Batch 58, LR 0.000062 Loss 6.277056, Accuracy 82.260%\n",
      "Epoch 18, Batch 59, LR 0.000062 Loss 6.282484, Accuracy 82.256%\n",
      "Epoch 18, Batch 60, LR 0.000062 Loss 6.291695, Accuracy 82.240%\n",
      "Epoch 18, Batch 61, LR 0.000062 Loss 6.283751, Accuracy 82.287%\n",
      "Epoch 18, Batch 62, LR 0.000062 Loss 6.267184, Accuracy 82.397%\n",
      "Epoch 18, Batch 63, LR 0.000062 Loss 6.264903, Accuracy 82.354%\n",
      "Epoch 18, Batch 64, LR 0.000062 Loss 6.274611, Accuracy 82.263%\n",
      "Epoch 18, Batch 65, LR 0.000062 Loss 6.277358, Accuracy 82.248%\n",
      "Epoch 18, Batch 66, LR 0.000062 Loss 6.278717, Accuracy 82.256%\n",
      "Epoch 18, Batch 67, LR 0.000062 Loss 6.272863, Accuracy 82.323%\n",
      "Epoch 18, Batch 68, LR 0.000062 Loss 6.271990, Accuracy 82.341%\n",
      "Epoch 18, Batch 69, LR 0.000062 Loss 6.259655, Accuracy 82.360%\n",
      "Epoch 18, Batch 70, LR 0.000062 Loss 6.255606, Accuracy 82.377%\n",
      "Epoch 18, Batch 71, LR 0.000062 Loss 6.242431, Accuracy 82.460%\n",
      "Epoch 18, Batch 72, LR 0.000062 Loss 6.246555, Accuracy 82.411%\n",
      "Epoch 18, Batch 73, LR 0.000062 Loss 6.252184, Accuracy 82.310%\n",
      "Epoch 18, Batch 74, LR 0.000062 Loss 6.261975, Accuracy 82.274%\n",
      "Epoch 18, Batch 75, LR 0.000062 Loss 6.263059, Accuracy 82.281%\n",
      "Epoch 18, Batch 76, LR 0.000062 Loss 6.263368, Accuracy 82.319%\n",
      "Epoch 18, Batch 77, LR 0.000062 Loss 6.252202, Accuracy 82.346%\n",
      "Epoch 18, Batch 78, LR 0.000062 Loss 6.264471, Accuracy 82.252%\n",
      "Epoch 18, Batch 79, LR 0.000062 Loss 6.266137, Accuracy 82.229%\n",
      "Epoch 18, Batch 80, LR 0.000062 Loss 6.255852, Accuracy 82.305%\n",
      "Epoch 18, Batch 81, LR 0.000062 Loss 6.256114, Accuracy 82.301%\n",
      "Epoch 18, Batch 82, LR 0.000062 Loss 6.255032, Accuracy 82.346%\n",
      "Epoch 18, Batch 83, LR 0.000062 Loss 6.260085, Accuracy 82.314%\n",
      "Epoch 18, Batch 84, LR 0.000062 Loss 6.257073, Accuracy 82.375%\n",
      "Epoch 18, Batch 85, LR 0.000062 Loss 6.251504, Accuracy 82.390%\n",
      "Epoch 18, Batch 86, LR 0.000062 Loss 6.256368, Accuracy 82.376%\n",
      "Epoch 18, Batch 87, LR 0.000062 Loss 6.258875, Accuracy 82.355%\n",
      "Epoch 18, Batch 88, LR 0.000062 Loss 6.259009, Accuracy 82.377%\n",
      "Epoch 18, Batch 89, LR 0.000062 Loss 6.263727, Accuracy 82.312%\n",
      "Epoch 18, Batch 90, LR 0.000062 Loss 6.263796, Accuracy 82.292%\n",
      "Epoch 18, Batch 91, LR 0.000062 Loss 6.278178, Accuracy 82.212%\n",
      "Epoch 18, Batch 92, LR 0.000062 Loss 6.269931, Accuracy 82.261%\n",
      "Epoch 18, Batch 93, LR 0.000062 Loss 6.264709, Accuracy 82.241%\n",
      "Epoch 18, Batch 94, LR 0.000062 Loss 6.267665, Accuracy 82.214%\n",
      "Epoch 18, Batch 95, LR 0.000062 Loss 6.263283, Accuracy 82.262%\n",
      "Epoch 18, Batch 96, LR 0.000062 Loss 6.267052, Accuracy 82.251%\n",
      "Epoch 18, Batch 97, LR 0.000062 Loss 6.269314, Accuracy 82.241%\n",
      "Epoch 18, Batch 98, LR 0.000062 Loss 6.269485, Accuracy 82.254%\n",
      "Epoch 18, Batch 99, LR 0.000062 Loss 6.273099, Accuracy 82.229%\n",
      "Epoch 18, Batch 100, LR 0.000062 Loss 6.270704, Accuracy 82.258%\n",
      "Epoch 18, Batch 101, LR 0.000062 Loss 6.275307, Accuracy 82.263%\n",
      "Epoch 18, Batch 102, LR 0.000062 Loss 6.263387, Accuracy 82.307%\n",
      "Epoch 18, Batch 103, LR 0.000062 Loss 6.264816, Accuracy 82.282%\n",
      "Epoch 18, Batch 104, LR 0.000062 Loss 6.261748, Accuracy 82.279%\n",
      "Epoch 18, Batch 105, LR 0.000062 Loss 6.267054, Accuracy 82.292%\n",
      "Epoch 18, Batch 106, LR 0.000062 Loss 6.267260, Accuracy 82.289%\n",
      "Epoch 18, Batch 107, LR 0.000062 Loss 6.266445, Accuracy 82.265%\n",
      "Epoch 18, Batch 108, LR 0.000062 Loss 6.261192, Accuracy 82.292%\n",
      "Epoch 18, Batch 109, LR 0.000062 Loss 6.259907, Accuracy 82.311%\n",
      "Epoch 18, Batch 110, LR 0.000062 Loss 6.264304, Accuracy 82.280%\n",
      "Epoch 18, Batch 111, LR 0.000062 Loss 6.264473, Accuracy 82.264%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 112, LR 0.000062 Loss 6.265402, Accuracy 82.261%\n",
      "Epoch 18, Batch 113, LR 0.000062 Loss 6.266715, Accuracy 82.218%\n",
      "Epoch 18, Batch 114, LR 0.000062 Loss 6.274465, Accuracy 82.168%\n",
      "Epoch 18, Batch 115, LR 0.000062 Loss 6.275277, Accuracy 82.147%\n",
      "Epoch 18, Batch 116, LR 0.000062 Loss 6.267275, Accuracy 82.206%\n",
      "Epoch 18, Batch 117, LR 0.000062 Loss 6.273943, Accuracy 82.218%\n",
      "Epoch 18, Batch 118, LR 0.000062 Loss 6.274034, Accuracy 82.177%\n",
      "Epoch 18, Batch 119, LR 0.000062 Loss 6.267098, Accuracy 82.209%\n",
      "Epoch 18, Batch 120, LR 0.000062 Loss 6.258607, Accuracy 82.233%\n",
      "Epoch 18, Batch 121, LR 0.000062 Loss 6.259007, Accuracy 82.238%\n",
      "Epoch 18, Batch 122, LR 0.000062 Loss 6.264539, Accuracy 82.243%\n",
      "Epoch 18, Batch 123, LR 0.000062 Loss 6.266583, Accuracy 82.235%\n",
      "Epoch 18, Batch 124, LR 0.000062 Loss 6.267209, Accuracy 82.227%\n",
      "Epoch 18, Batch 125, LR 0.000062 Loss 6.267143, Accuracy 82.213%\n",
      "Epoch 18, Batch 126, LR 0.000062 Loss 6.266794, Accuracy 82.205%\n",
      "Epoch 18, Batch 127, LR 0.000062 Loss 6.266750, Accuracy 82.197%\n",
      "Epoch 18, Batch 128, LR 0.000062 Loss 6.262081, Accuracy 82.245%\n",
      "Epoch 18, Batch 129, LR 0.000062 Loss 6.260261, Accuracy 82.231%\n",
      "Epoch 18, Batch 130, LR 0.000062 Loss 6.255171, Accuracy 82.260%\n",
      "Epoch 18, Batch 131, LR 0.000062 Loss 6.260700, Accuracy 82.264%\n",
      "Epoch 18, Batch 132, LR 0.000062 Loss 6.263726, Accuracy 82.262%\n",
      "Epoch 18, Batch 133, LR 0.000062 Loss 6.261469, Accuracy 82.249%\n",
      "Epoch 18, Batch 134, LR 0.000062 Loss 6.263174, Accuracy 82.229%\n",
      "Epoch 18, Batch 135, LR 0.000062 Loss 6.266354, Accuracy 82.211%\n",
      "Epoch 18, Batch 136, LR 0.000062 Loss 6.268560, Accuracy 82.204%\n",
      "Epoch 18, Batch 137, LR 0.000062 Loss 6.273662, Accuracy 82.174%\n",
      "Epoch 18, Batch 138, LR 0.000062 Loss 6.271396, Accuracy 82.156%\n",
      "Epoch 18, Batch 139, LR 0.000062 Loss 6.271661, Accuracy 82.161%\n",
      "Epoch 18, Batch 140, LR 0.000062 Loss 6.271982, Accuracy 82.165%\n",
      "Epoch 18, Batch 141, LR 0.000062 Loss 6.267378, Accuracy 82.192%\n",
      "Epoch 18, Batch 142, LR 0.000062 Loss 6.267657, Accuracy 82.207%\n",
      "Epoch 18, Batch 143, LR 0.000062 Loss 6.263431, Accuracy 82.233%\n",
      "Epoch 18, Batch 144, LR 0.000062 Loss 6.265007, Accuracy 82.232%\n",
      "Epoch 18, Batch 145, LR 0.000062 Loss 6.269665, Accuracy 82.225%\n",
      "Epoch 18, Batch 146, LR 0.000062 Loss 6.265046, Accuracy 82.272%\n",
      "Epoch 18, Batch 147, LR 0.000062 Loss 6.257974, Accuracy 82.297%\n",
      "Epoch 18, Batch 148, LR 0.000062 Loss 6.256879, Accuracy 82.285%\n",
      "Epoch 18, Batch 149, LR 0.000062 Loss 6.258795, Accuracy 82.288%\n",
      "Epoch 18, Batch 150, LR 0.000062 Loss 6.261898, Accuracy 82.271%\n",
      "Epoch 18, Batch 151, LR 0.000062 Loss 6.259499, Accuracy 82.290%\n",
      "Epoch 18, Batch 152, LR 0.000062 Loss 6.255080, Accuracy 82.314%\n",
      "Epoch 18, Batch 153, LR 0.000062 Loss 6.253715, Accuracy 82.322%\n",
      "Epoch 18, Batch 154, LR 0.000062 Loss 6.253089, Accuracy 82.336%\n",
      "Epoch 18, Batch 155, LR 0.000062 Loss 6.250057, Accuracy 82.349%\n",
      "Epoch 18, Batch 156, LR 0.000062 Loss 6.249325, Accuracy 82.337%\n",
      "Epoch 18, Batch 157, LR 0.000062 Loss 6.253974, Accuracy 82.315%\n",
      "Epoch 18, Batch 158, LR 0.000062 Loss 6.253945, Accuracy 82.318%\n",
      "Epoch 18, Batch 159, LR 0.000062 Loss 6.256172, Accuracy 82.311%\n",
      "Epoch 18, Batch 160, LR 0.000062 Loss 6.255129, Accuracy 82.310%\n",
      "Epoch 18, Batch 161, LR 0.000062 Loss 6.257014, Accuracy 82.293%\n",
      "Epoch 18, Batch 162, LR 0.000062 Loss 6.257405, Accuracy 82.287%\n",
      "Epoch 18, Batch 163, LR 0.000062 Loss 6.258270, Accuracy 82.280%\n",
      "Epoch 18, Batch 164, LR 0.000062 Loss 6.259076, Accuracy 82.269%\n",
      "Epoch 18, Batch 165, LR 0.000062 Loss 6.257038, Accuracy 82.292%\n",
      "Epoch 18, Batch 166, LR 0.000062 Loss 6.256956, Accuracy 82.295%\n",
      "Epoch 18, Batch 167, LR 0.000062 Loss 6.256689, Accuracy 82.312%\n",
      "Epoch 18, Batch 168, LR 0.000062 Loss 6.254062, Accuracy 82.347%\n",
      "Epoch 18, Batch 169, LR 0.000062 Loss 6.252705, Accuracy 82.378%\n",
      "Epoch 18, Batch 170, LR 0.000062 Loss 6.251647, Accuracy 82.367%\n",
      "Epoch 18, Batch 171, LR 0.000062 Loss 6.255571, Accuracy 82.365%\n",
      "Epoch 18, Batch 172, LR 0.000062 Loss 6.255339, Accuracy 82.381%\n",
      "Epoch 18, Batch 173, LR 0.000062 Loss 6.256873, Accuracy 82.379%\n",
      "Epoch 18, Batch 174, LR 0.000062 Loss 6.256014, Accuracy 82.368%\n",
      "Epoch 18, Batch 175, LR 0.000062 Loss 6.251250, Accuracy 82.384%\n",
      "Epoch 18, Batch 176, LR 0.000062 Loss 6.252017, Accuracy 82.400%\n",
      "Epoch 18, Batch 177, LR 0.000062 Loss 6.258700, Accuracy 82.349%\n",
      "Epoch 18, Batch 178, LR 0.000062 Loss 6.258773, Accuracy 82.347%\n",
      "Epoch 18, Batch 179, LR 0.000062 Loss 6.258583, Accuracy 82.354%\n",
      "Epoch 18, Batch 180, LR 0.000062 Loss 6.256200, Accuracy 82.348%\n",
      "Epoch 18, Batch 181, LR 0.000062 Loss 6.251371, Accuracy 82.351%\n",
      "Epoch 18, Batch 182, LR 0.000062 Loss 6.253120, Accuracy 82.332%\n",
      "Epoch 18, Batch 183, LR 0.000062 Loss 6.257665, Accuracy 82.304%\n",
      "Epoch 18, Batch 184, LR 0.000062 Loss 6.259223, Accuracy 82.307%\n",
      "Epoch 18, Batch 185, LR 0.000062 Loss 6.259695, Accuracy 82.310%\n",
      "Epoch 18, Batch 186, LR 0.000062 Loss 6.259650, Accuracy 82.317%\n",
      "Epoch 18, Batch 187, LR 0.000062 Loss 6.261795, Accuracy 82.294%\n",
      "Epoch 18, Batch 188, LR 0.000062 Loss 6.263767, Accuracy 82.285%\n",
      "Epoch 18, Batch 189, LR 0.000062 Loss 6.263093, Accuracy 82.292%\n",
      "Epoch 18, Batch 190, LR 0.000062 Loss 6.261541, Accuracy 82.282%\n",
      "Epoch 18, Batch 191, LR 0.000062 Loss 6.260733, Accuracy 82.277%\n",
      "Epoch 18, Batch 192, LR 0.000062 Loss 6.264536, Accuracy 82.259%\n",
      "Epoch 18, Batch 193, LR 0.000062 Loss 6.266492, Accuracy 82.254%\n",
      "Epoch 18, Batch 194, LR 0.000062 Loss 6.263278, Accuracy 82.269%\n",
      "Epoch 18, Batch 195, LR 0.000062 Loss 6.268167, Accuracy 82.256%\n",
      "Epoch 18, Batch 196, LR 0.000062 Loss 6.270648, Accuracy 82.250%\n",
      "Epoch 18, Batch 197, LR 0.000062 Loss 6.270931, Accuracy 82.241%\n",
      "Epoch 18, Batch 198, LR 0.000062 Loss 6.276137, Accuracy 82.225%\n",
      "Epoch 18, Batch 199, LR 0.000062 Loss 6.274600, Accuracy 82.212%\n",
      "Epoch 18, Batch 200, LR 0.000062 Loss 6.275102, Accuracy 82.211%\n",
      "Epoch 18, Batch 201, LR 0.000062 Loss 6.273741, Accuracy 82.210%\n",
      "Epoch 18, Batch 202, LR 0.000062 Loss 6.278119, Accuracy 82.190%\n",
      "Epoch 18, Batch 203, LR 0.000061 Loss 6.281161, Accuracy 82.193%\n",
      "Epoch 18, Batch 204, LR 0.000061 Loss 6.283687, Accuracy 82.188%\n",
      "Epoch 18, Batch 205, LR 0.000061 Loss 6.285165, Accuracy 82.180%\n",
      "Epoch 18, Batch 206, LR 0.000061 Loss 6.282839, Accuracy 82.202%\n",
      "Epoch 18, Batch 207, LR 0.000061 Loss 6.281782, Accuracy 82.220%\n",
      "Epoch 18, Batch 208, LR 0.000061 Loss 6.283199, Accuracy 82.204%\n",
      "Epoch 18, Batch 209, LR 0.000061 Loss 6.279578, Accuracy 82.222%\n",
      "Epoch 18, Batch 210, LR 0.000061 Loss 6.281450, Accuracy 82.217%\n",
      "Epoch 18, Batch 211, LR 0.000061 Loss 6.279418, Accuracy 82.242%\n",
      "Epoch 18, Batch 212, LR 0.000061 Loss 6.279545, Accuracy 82.245%\n",
      "Epoch 18, Batch 213, LR 0.000061 Loss 6.283739, Accuracy 82.215%\n",
      "Epoch 18, Batch 214, LR 0.000061 Loss 6.282617, Accuracy 82.232%\n",
      "Epoch 18, Batch 215, LR 0.000061 Loss 6.285738, Accuracy 82.220%\n",
      "Epoch 18, Batch 216, LR 0.000061 Loss 6.287928, Accuracy 82.234%\n",
      "Epoch 18, Batch 217, LR 0.000061 Loss 6.287572, Accuracy 82.233%\n",
      "Epoch 18, Batch 218, LR 0.000061 Loss 6.286913, Accuracy 82.232%\n",
      "Epoch 18, Batch 219, LR 0.000061 Loss 6.289631, Accuracy 82.210%\n",
      "Epoch 18, Batch 220, LR 0.000061 Loss 6.287480, Accuracy 82.234%\n",
      "Epoch 18, Batch 221, LR 0.000061 Loss 6.286961, Accuracy 82.229%\n",
      "Epoch 18, Batch 222, LR 0.000061 Loss 6.289652, Accuracy 82.207%\n",
      "Epoch 18, Batch 223, LR 0.000061 Loss 6.290660, Accuracy 82.192%\n",
      "Epoch 18, Batch 224, LR 0.000061 Loss 6.292613, Accuracy 82.174%\n",
      "Epoch 18, Batch 225, LR 0.000061 Loss 6.292617, Accuracy 82.181%\n",
      "Epoch 18, Batch 226, LR 0.000061 Loss 6.289203, Accuracy 82.190%\n",
      "Epoch 18, Batch 227, LR 0.000061 Loss 6.291607, Accuracy 82.165%\n",
      "Epoch 18, Batch 228, LR 0.000061 Loss 6.291815, Accuracy 82.165%\n",
      "Epoch 18, Batch 229, LR 0.000061 Loss 6.289922, Accuracy 82.178%\n",
      "Epoch 18, Batch 230, LR 0.000061 Loss 6.291857, Accuracy 82.171%\n",
      "Epoch 18, Batch 231, LR 0.000061 Loss 6.290921, Accuracy 82.194%\n",
      "Epoch 18, Batch 232, LR 0.000061 Loss 6.294982, Accuracy 82.166%\n",
      "Epoch 18, Batch 233, LR 0.000061 Loss 6.296074, Accuracy 82.165%\n",
      "Epoch 18, Batch 234, LR 0.000061 Loss 6.297879, Accuracy 82.148%\n",
      "Epoch 18, Batch 235, LR 0.000061 Loss 6.297030, Accuracy 82.141%\n",
      "Epoch 18, Batch 236, LR 0.000061 Loss 6.294944, Accuracy 82.137%\n",
      "Epoch 18, Batch 237, LR 0.000061 Loss 6.295558, Accuracy 82.127%\n",
      "Epoch 18, Batch 238, LR 0.000061 Loss 6.297975, Accuracy 82.107%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 239, LR 0.000061 Loss 6.300744, Accuracy 82.077%\n",
      "Epoch 18, Batch 240, LR 0.000061 Loss 6.296933, Accuracy 82.093%\n",
      "Epoch 18, Batch 241, LR 0.000061 Loss 6.294919, Accuracy 82.096%\n",
      "Epoch 18, Batch 242, LR 0.000061 Loss 6.296377, Accuracy 82.080%\n",
      "Epoch 18, Batch 243, LR 0.000061 Loss 6.299535, Accuracy 82.063%\n",
      "Epoch 18, Batch 244, LR 0.000061 Loss 6.300601, Accuracy 82.066%\n",
      "Epoch 18, Batch 245, LR 0.000061 Loss 6.301258, Accuracy 82.044%\n",
      "Epoch 18, Batch 246, LR 0.000061 Loss 6.303755, Accuracy 82.038%\n",
      "Epoch 18, Batch 247, LR 0.000061 Loss 6.300283, Accuracy 82.060%\n",
      "Epoch 18, Batch 248, LR 0.000061 Loss 6.300238, Accuracy 82.072%\n",
      "Epoch 18, Batch 249, LR 0.000061 Loss 6.302917, Accuracy 82.038%\n",
      "Epoch 18, Batch 250, LR 0.000061 Loss 6.302520, Accuracy 82.028%\n",
      "Epoch 18, Batch 251, LR 0.000061 Loss 6.304225, Accuracy 82.019%\n",
      "Epoch 18, Batch 252, LR 0.000061 Loss 6.303694, Accuracy 82.034%\n",
      "Epoch 18, Batch 253, LR 0.000061 Loss 6.303193, Accuracy 82.031%\n",
      "Epoch 18, Batch 254, LR 0.000061 Loss 6.303992, Accuracy 82.037%\n",
      "Epoch 18, Batch 255, LR 0.000061 Loss 6.303055, Accuracy 82.047%\n",
      "Epoch 18, Batch 256, LR 0.000061 Loss 6.304076, Accuracy 82.043%\n",
      "Epoch 18, Batch 257, LR 0.000061 Loss 6.306032, Accuracy 82.053%\n",
      "Epoch 18, Batch 258, LR 0.000061 Loss 6.308237, Accuracy 82.031%\n",
      "Epoch 18, Batch 259, LR 0.000061 Loss 6.308705, Accuracy 82.025%\n",
      "Epoch 18, Batch 260, LR 0.000061 Loss 6.309138, Accuracy 82.031%\n",
      "Epoch 18, Batch 261, LR 0.000061 Loss 6.308924, Accuracy 82.031%\n",
      "Epoch 18, Batch 262, LR 0.000061 Loss 6.308707, Accuracy 82.040%\n",
      "Epoch 18, Batch 263, LR 0.000061 Loss 6.309271, Accuracy 82.034%\n",
      "Epoch 18, Batch 264, LR 0.000061 Loss 6.309241, Accuracy 82.037%\n",
      "Epoch 18, Batch 265, LR 0.000061 Loss 6.310619, Accuracy 82.043%\n",
      "Epoch 18, Batch 266, LR 0.000061 Loss 6.310916, Accuracy 82.043%\n",
      "Epoch 18, Batch 267, LR 0.000061 Loss 6.308060, Accuracy 82.046%\n",
      "Epoch 18, Batch 268, LR 0.000061 Loss 6.310652, Accuracy 82.028%\n",
      "Epoch 18, Batch 269, LR 0.000061 Loss 6.311624, Accuracy 82.023%\n",
      "Epoch 18, Batch 270, LR 0.000061 Loss 6.312383, Accuracy 82.025%\n",
      "Epoch 18, Batch 271, LR 0.000061 Loss 6.311808, Accuracy 82.040%\n",
      "Epoch 18, Batch 272, LR 0.000061 Loss 6.312233, Accuracy 82.051%\n",
      "Epoch 18, Batch 273, LR 0.000061 Loss 6.312330, Accuracy 82.051%\n",
      "Epoch 18, Batch 274, LR 0.000061 Loss 6.312451, Accuracy 82.046%\n",
      "Epoch 18, Batch 275, LR 0.000061 Loss 6.309868, Accuracy 82.060%\n",
      "Epoch 18, Batch 276, LR 0.000061 Loss 6.307683, Accuracy 82.074%\n",
      "Epoch 18, Batch 277, LR 0.000061 Loss 6.306198, Accuracy 82.082%\n",
      "Epoch 18, Batch 278, LR 0.000061 Loss 6.306908, Accuracy 82.068%\n",
      "Epoch 18, Batch 279, LR 0.000061 Loss 6.306852, Accuracy 82.065%\n",
      "Epoch 18, Batch 280, LR 0.000061 Loss 6.308657, Accuracy 82.056%\n",
      "Epoch 18, Batch 281, LR 0.000061 Loss 6.307202, Accuracy 82.051%\n",
      "Epoch 18, Batch 282, LR 0.000061 Loss 6.308765, Accuracy 82.045%\n",
      "Epoch 18, Batch 283, LR 0.000061 Loss 6.306451, Accuracy 82.051%\n",
      "Epoch 18, Batch 284, LR 0.000061 Loss 6.304886, Accuracy 82.062%\n",
      "Epoch 18, Batch 285, LR 0.000061 Loss 6.304515, Accuracy 82.056%\n",
      "Epoch 18, Batch 286, LR 0.000061 Loss 6.304436, Accuracy 82.045%\n",
      "Epoch 18, Batch 287, LR 0.000061 Loss 6.304715, Accuracy 82.050%\n",
      "Epoch 18, Batch 288, LR 0.000061 Loss 6.303370, Accuracy 82.067%\n",
      "Epoch 18, Batch 289, LR 0.000061 Loss 6.305163, Accuracy 82.053%\n",
      "Epoch 18, Batch 290, LR 0.000061 Loss 6.302075, Accuracy 82.072%\n",
      "Epoch 18, Batch 291, LR 0.000061 Loss 6.300284, Accuracy 82.074%\n",
      "Epoch 18, Batch 292, LR 0.000061 Loss 6.299075, Accuracy 82.079%\n",
      "Epoch 18, Batch 293, LR 0.000061 Loss 6.299033, Accuracy 82.074%\n",
      "Epoch 18, Batch 294, LR 0.000061 Loss 6.295546, Accuracy 82.098%\n",
      "Epoch 18, Batch 295, LR 0.000061 Loss 6.298087, Accuracy 82.074%\n",
      "Epoch 18, Batch 296, LR 0.000061 Loss 6.300195, Accuracy 82.068%\n",
      "Epoch 18, Batch 297, LR 0.000061 Loss 6.299270, Accuracy 82.065%\n",
      "Epoch 18, Batch 298, LR 0.000061 Loss 6.298893, Accuracy 82.073%\n",
      "Epoch 18, Batch 299, LR 0.000061 Loss 6.298082, Accuracy 82.081%\n",
      "Epoch 18, Batch 300, LR 0.000061 Loss 6.298108, Accuracy 82.078%\n",
      "Epoch 18, Batch 301, LR 0.000061 Loss 6.301000, Accuracy 82.052%\n",
      "Epoch 18, Batch 302, LR 0.000061 Loss 6.301713, Accuracy 82.049%\n",
      "Epoch 18, Batch 303, LR 0.000061 Loss 6.300926, Accuracy 82.057%\n",
      "Epoch 18, Batch 304, LR 0.000061 Loss 6.300160, Accuracy 82.062%\n",
      "Epoch 18, Batch 305, LR 0.000061 Loss 6.299893, Accuracy 82.075%\n",
      "Epoch 18, Batch 306, LR 0.000061 Loss 6.298624, Accuracy 82.087%\n",
      "Epoch 18, Batch 307, LR 0.000061 Loss 6.297465, Accuracy 82.090%\n",
      "Epoch 18, Batch 308, LR 0.000061 Loss 6.296724, Accuracy 82.097%\n",
      "Epoch 18, Batch 309, LR 0.000061 Loss 6.294133, Accuracy 82.107%\n",
      "Epoch 18, Batch 310, LR 0.000061 Loss 6.294147, Accuracy 82.097%\n",
      "Epoch 18, Batch 311, LR 0.000061 Loss 6.294782, Accuracy 82.092%\n",
      "Epoch 18, Batch 312, LR 0.000061 Loss 6.292517, Accuracy 82.101%\n",
      "Epoch 18, Batch 313, LR 0.000061 Loss 6.292317, Accuracy 82.114%\n",
      "Epoch 18, Batch 314, LR 0.000061 Loss 6.295537, Accuracy 82.106%\n",
      "Epoch 18, Batch 315, LR 0.000061 Loss 6.295100, Accuracy 82.113%\n",
      "Epoch 18, Batch 316, LR 0.000061 Loss 6.295140, Accuracy 82.130%\n",
      "Epoch 18, Batch 317, LR 0.000061 Loss 6.295432, Accuracy 82.127%\n",
      "Epoch 18, Batch 318, LR 0.000061 Loss 6.294292, Accuracy 82.127%\n",
      "Epoch 18, Batch 319, LR 0.000061 Loss 6.295381, Accuracy 82.122%\n",
      "Epoch 18, Batch 320, LR 0.000061 Loss 6.296192, Accuracy 82.122%\n",
      "Epoch 18, Batch 321, LR 0.000061 Loss 6.296494, Accuracy 82.124%\n",
      "Epoch 18, Batch 322, LR 0.000061 Loss 6.297474, Accuracy 82.128%\n",
      "Epoch 18, Batch 323, LR 0.000061 Loss 6.297060, Accuracy 82.138%\n",
      "Epoch 18, Batch 324, LR 0.000061 Loss 6.294424, Accuracy 82.154%\n",
      "Epoch 18, Batch 325, LR 0.000061 Loss 6.294136, Accuracy 82.166%\n",
      "Epoch 18, Batch 326, LR 0.000061 Loss 6.292695, Accuracy 82.177%\n",
      "Epoch 18, Batch 327, LR 0.000061 Loss 6.292620, Accuracy 82.179%\n",
      "Epoch 18, Batch 328, LR 0.000061 Loss 6.290431, Accuracy 82.196%\n",
      "Epoch 18, Batch 329, LR 0.000061 Loss 6.290362, Accuracy 82.202%\n",
      "Epoch 18, Batch 330, LR 0.000061 Loss 6.289695, Accuracy 82.202%\n",
      "Epoch 18, Batch 331, LR 0.000061 Loss 6.288426, Accuracy 82.204%\n",
      "Epoch 18, Batch 332, LR 0.000061 Loss 6.288140, Accuracy 82.205%\n",
      "Epoch 18, Batch 333, LR 0.000061 Loss 6.289317, Accuracy 82.195%\n",
      "Epoch 18, Batch 334, LR 0.000061 Loss 6.289359, Accuracy 82.195%\n",
      "Epoch 18, Batch 335, LR 0.000061 Loss 6.290659, Accuracy 82.188%\n",
      "Epoch 18, Batch 336, LR 0.000061 Loss 6.289943, Accuracy 82.187%\n",
      "Epoch 18, Batch 337, LR 0.000061 Loss 6.289224, Accuracy 82.205%\n",
      "Epoch 18, Batch 338, LR 0.000061 Loss 6.287955, Accuracy 82.228%\n",
      "Epoch 18, Batch 339, LR 0.000061 Loss 6.286159, Accuracy 82.239%\n",
      "Epoch 18, Batch 340, LR 0.000061 Loss 6.284158, Accuracy 82.250%\n",
      "Epoch 18, Batch 341, LR 0.000061 Loss 6.284524, Accuracy 82.251%\n",
      "Epoch 18, Batch 342, LR 0.000061 Loss 6.284937, Accuracy 82.251%\n",
      "Epoch 18, Batch 343, LR 0.000061 Loss 6.288447, Accuracy 82.234%\n",
      "Epoch 18, Batch 344, LR 0.000061 Loss 6.287433, Accuracy 82.245%\n",
      "Epoch 18, Batch 345, LR 0.000061 Loss 6.289222, Accuracy 82.226%\n",
      "Epoch 18, Batch 346, LR 0.000061 Loss 6.289403, Accuracy 82.219%\n",
      "Epoch 18, Batch 347, LR 0.000061 Loss 6.288413, Accuracy 82.225%\n",
      "Epoch 18, Batch 348, LR 0.000061 Loss 6.285974, Accuracy 82.233%\n",
      "Epoch 18, Batch 349, LR 0.000061 Loss 6.285816, Accuracy 82.233%\n",
      "Epoch 18, Batch 350, LR 0.000061 Loss 6.287473, Accuracy 82.214%\n",
      "Epoch 18, Batch 351, LR 0.000061 Loss 6.287444, Accuracy 82.209%\n",
      "Epoch 18, Batch 352, LR 0.000061 Loss 6.287687, Accuracy 82.198%\n",
      "Epoch 18, Batch 353, LR 0.000061 Loss 6.286709, Accuracy 82.204%\n",
      "Epoch 18, Batch 354, LR 0.000061 Loss 6.283264, Accuracy 82.232%\n",
      "Epoch 18, Batch 355, LR 0.000061 Loss 6.285025, Accuracy 82.227%\n",
      "Epoch 18, Batch 356, LR 0.000061 Loss 6.283721, Accuracy 82.233%\n",
      "Epoch 18, Batch 357, LR 0.000061 Loss 6.284916, Accuracy 82.222%\n",
      "Epoch 18, Batch 358, LR 0.000061 Loss 6.284351, Accuracy 82.230%\n",
      "Epoch 18, Batch 359, LR 0.000061 Loss 6.283798, Accuracy 82.234%\n",
      "Epoch 18, Batch 360, LR 0.000061 Loss 6.284068, Accuracy 82.240%\n",
      "Epoch 18, Batch 361, LR 0.000061 Loss 6.282279, Accuracy 82.241%\n",
      "Epoch 18, Batch 362, LR 0.000061 Loss 6.284263, Accuracy 82.243%\n",
      "Epoch 18, Batch 363, LR 0.000061 Loss 6.283912, Accuracy 82.246%\n",
      "Epoch 18, Batch 364, LR 0.000061 Loss 6.282948, Accuracy 82.250%\n",
      "Epoch 18, Batch 365, LR 0.000061 Loss 6.280290, Accuracy 82.262%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 366, LR 0.000061 Loss 6.280504, Accuracy 82.268%\n",
      "Epoch 18, Batch 367, LR 0.000061 Loss 6.281440, Accuracy 82.261%\n",
      "Epoch 18, Batch 368, LR 0.000061 Loss 6.282980, Accuracy 82.254%\n",
      "Epoch 18, Batch 369, LR 0.000061 Loss 6.285015, Accuracy 82.239%\n",
      "Epoch 18, Batch 370, LR 0.000061 Loss 6.284267, Accuracy 82.251%\n",
      "Epoch 18, Batch 371, LR 0.000061 Loss 6.284799, Accuracy 82.248%\n",
      "Epoch 18, Batch 372, LR 0.000061 Loss 6.285342, Accuracy 82.245%\n",
      "Epoch 18, Batch 373, LR 0.000061 Loss 6.284779, Accuracy 82.251%\n",
      "Epoch 18, Batch 374, LR 0.000061 Loss 6.281700, Accuracy 82.263%\n",
      "Epoch 18, Batch 375, LR 0.000061 Loss 6.279256, Accuracy 82.269%\n",
      "Epoch 18, Batch 376, LR 0.000061 Loss 6.278591, Accuracy 82.274%\n",
      "Epoch 18, Batch 377, LR 0.000061 Loss 6.277190, Accuracy 82.284%\n",
      "Epoch 18, Batch 378, LR 0.000061 Loss 6.279848, Accuracy 82.277%\n",
      "Epoch 18, Batch 379, LR 0.000061 Loss 6.279184, Accuracy 82.289%\n",
      "Epoch 18, Batch 380, LR 0.000061 Loss 6.278864, Accuracy 82.296%\n",
      "Epoch 18, Batch 381, LR 0.000061 Loss 6.277589, Accuracy 82.290%\n",
      "Epoch 18, Batch 382, LR 0.000061 Loss 6.277260, Accuracy 82.293%\n",
      "Epoch 18, Batch 383, LR 0.000061 Loss 6.277050, Accuracy 82.284%\n",
      "Epoch 18, Batch 384, LR 0.000061 Loss 6.277396, Accuracy 82.277%\n",
      "Epoch 18, Batch 385, LR 0.000061 Loss 6.278709, Accuracy 82.261%\n",
      "Epoch 18, Batch 386, LR 0.000061 Loss 6.279145, Accuracy 82.242%\n",
      "Epoch 18, Batch 387, LR 0.000061 Loss 6.280169, Accuracy 82.239%\n",
      "Epoch 18, Batch 388, LR 0.000061 Loss 6.281012, Accuracy 82.225%\n",
      "Epoch 18, Batch 389, LR 0.000061 Loss 6.281044, Accuracy 82.228%\n",
      "Epoch 18, Batch 390, LR 0.000061 Loss 6.282280, Accuracy 82.228%\n",
      "Epoch 18, Batch 391, LR 0.000061 Loss 6.283655, Accuracy 82.225%\n",
      "Epoch 18, Batch 392, LR 0.000061 Loss 6.283460, Accuracy 82.231%\n",
      "Epoch 18, Batch 393, LR 0.000061 Loss 6.284860, Accuracy 82.230%\n",
      "Epoch 18, Batch 394, LR 0.000061 Loss 6.283193, Accuracy 82.241%\n",
      "Epoch 18, Batch 395, LR 0.000061 Loss 6.285390, Accuracy 82.241%\n",
      "Epoch 18, Batch 396, LR 0.000061 Loss 6.282580, Accuracy 82.250%\n",
      "Epoch 18, Batch 397, LR 0.000061 Loss 6.282523, Accuracy 82.256%\n",
      "Epoch 18, Batch 398, LR 0.000061 Loss 6.281757, Accuracy 82.255%\n",
      "Epoch 18, Batch 399, LR 0.000061 Loss 6.279721, Accuracy 82.268%\n",
      "Epoch 18, Batch 400, LR 0.000061 Loss 6.278950, Accuracy 82.279%\n",
      "Epoch 18, Batch 401, LR 0.000061 Loss 6.282051, Accuracy 82.261%\n",
      "Epoch 18, Batch 402, LR 0.000061 Loss 6.281642, Accuracy 82.261%\n",
      "Epoch 18, Batch 403, LR 0.000061 Loss 6.281308, Accuracy 82.258%\n",
      "Epoch 18, Batch 404, LR 0.000061 Loss 6.278575, Accuracy 82.267%\n",
      "Epoch 18, Batch 405, LR 0.000061 Loss 6.277053, Accuracy 82.280%\n",
      "Epoch 18, Batch 406, LR 0.000061 Loss 6.276287, Accuracy 82.279%\n",
      "Epoch 18, Batch 407, LR 0.000061 Loss 6.275988, Accuracy 82.273%\n",
      "Epoch 18, Batch 408, LR 0.000061 Loss 6.276530, Accuracy 82.269%\n",
      "Epoch 18, Batch 409, LR 0.000061 Loss 6.276502, Accuracy 82.266%\n",
      "Epoch 18, Batch 410, LR 0.000061 Loss 6.277013, Accuracy 82.260%\n",
      "Epoch 18, Batch 411, LR 0.000061 Loss 6.277548, Accuracy 82.259%\n",
      "Epoch 18, Batch 412, LR 0.000061 Loss 6.278331, Accuracy 82.257%\n",
      "Epoch 18, Batch 413, LR 0.000061 Loss 6.280521, Accuracy 82.239%\n",
      "Epoch 18, Batch 414, LR 0.000061 Loss 6.278848, Accuracy 82.250%\n",
      "Epoch 18, Batch 415, LR 0.000061 Loss 6.279948, Accuracy 82.233%\n",
      "Epoch 18, Batch 416, LR 0.000061 Loss 6.280555, Accuracy 82.234%\n",
      "Epoch 18, Batch 417, LR 0.000061 Loss 6.278952, Accuracy 82.247%\n",
      "Epoch 18, Batch 418, LR 0.000061 Loss 6.278692, Accuracy 82.259%\n",
      "Epoch 18, Batch 419, LR 0.000061 Loss 6.279462, Accuracy 82.253%\n",
      "Epoch 18, Batch 420, LR 0.000061 Loss 6.277631, Accuracy 82.262%\n",
      "Epoch 18, Batch 421, LR 0.000061 Loss 6.275919, Accuracy 82.280%\n",
      "Epoch 18, Batch 422, LR 0.000061 Loss 6.276083, Accuracy 82.276%\n",
      "Epoch 18, Batch 423, LR 0.000061 Loss 6.274493, Accuracy 82.284%\n",
      "Epoch 18, Batch 424, LR 0.000061 Loss 6.275556, Accuracy 82.282%\n",
      "Epoch 18, Batch 425, LR 0.000061 Loss 6.274945, Accuracy 82.285%\n",
      "Epoch 18, Batch 426, LR 0.000061 Loss 6.275003, Accuracy 82.284%\n",
      "Epoch 18, Batch 427, LR 0.000061 Loss 6.272960, Accuracy 82.295%\n",
      "Epoch 18, Batch 428, LR 0.000061 Loss 6.274469, Accuracy 82.283%\n",
      "Epoch 18, Batch 429, LR 0.000061 Loss 6.272608, Accuracy 82.288%\n",
      "Epoch 18, Batch 430, LR 0.000061 Loss 6.273071, Accuracy 82.295%\n",
      "Epoch 18, Batch 431, LR 0.000061 Loss 6.273034, Accuracy 82.300%\n",
      "Epoch 18, Batch 432, LR 0.000061 Loss 6.274651, Accuracy 82.292%\n",
      "Epoch 18, Batch 433, LR 0.000061 Loss 6.274057, Accuracy 82.293%\n",
      "Epoch 18, Batch 434, LR 0.000061 Loss 6.274574, Accuracy 82.285%\n",
      "Epoch 18, Batch 435, LR 0.000061 Loss 6.274490, Accuracy 82.283%\n",
      "Epoch 18, Batch 436, LR 0.000061 Loss 6.275473, Accuracy 82.284%\n",
      "Epoch 18, Batch 437, LR 0.000061 Loss 6.275117, Accuracy 82.294%\n",
      "Epoch 18, Batch 438, LR 0.000061 Loss 6.273128, Accuracy 82.304%\n",
      "Epoch 18, Batch 439, LR 0.000061 Loss 6.272861, Accuracy 82.305%\n",
      "Epoch 18, Batch 440, LR 0.000061 Loss 6.271722, Accuracy 82.312%\n",
      "Epoch 18, Batch 441, LR 0.000061 Loss 6.270933, Accuracy 82.316%\n",
      "Epoch 18, Batch 442, LR 0.000061 Loss 6.270341, Accuracy 82.311%\n",
      "Epoch 18, Batch 443, LR 0.000061 Loss 6.271234, Accuracy 82.306%\n",
      "Epoch 18, Batch 444, LR 0.000061 Loss 6.270618, Accuracy 82.308%\n",
      "Epoch 18, Batch 445, LR 0.000061 Loss 6.271650, Accuracy 82.305%\n",
      "Epoch 18, Batch 446, LR 0.000061 Loss 6.272312, Accuracy 82.303%\n",
      "Epoch 18, Batch 447, LR 0.000061 Loss 6.274546, Accuracy 82.286%\n",
      "Epoch 18, Batch 448, LR 0.000061 Loss 6.273633, Accuracy 82.300%\n",
      "Epoch 18, Batch 449, LR 0.000061 Loss 6.274073, Accuracy 82.297%\n",
      "Epoch 18, Batch 450, LR 0.000061 Loss 6.273193, Accuracy 82.307%\n",
      "Epoch 18, Batch 451, LR 0.000061 Loss 6.273639, Accuracy 82.307%\n",
      "Epoch 18, Batch 452, LR 0.000061 Loss 6.275548, Accuracy 82.301%\n",
      "Epoch 18, Batch 453, LR 0.000061 Loss 6.275132, Accuracy 82.297%\n",
      "Epoch 18, Batch 454, LR 0.000061 Loss 6.276082, Accuracy 82.293%\n",
      "Epoch 18, Batch 455, LR 0.000061 Loss 6.275102, Accuracy 82.294%\n",
      "Epoch 18, Batch 456, LR 0.000061 Loss 6.275479, Accuracy 82.297%\n",
      "Epoch 18, Batch 457, LR 0.000061 Loss 6.274822, Accuracy 82.291%\n",
      "Epoch 18, Batch 458, LR 0.000061 Loss 6.274342, Accuracy 82.299%\n",
      "Epoch 18, Batch 459, LR 0.000061 Loss 6.273614, Accuracy 82.307%\n",
      "Epoch 18, Batch 460, LR 0.000061 Loss 6.274083, Accuracy 82.311%\n",
      "Epoch 18, Batch 461, LR 0.000061 Loss 6.271943, Accuracy 82.323%\n",
      "Epoch 18, Batch 462, LR 0.000060 Loss 6.271893, Accuracy 82.322%\n",
      "Epoch 18, Batch 463, LR 0.000060 Loss 6.272852, Accuracy 82.320%\n",
      "Epoch 18, Batch 464, LR 0.000060 Loss 6.275098, Accuracy 82.316%\n",
      "Epoch 18, Batch 465, LR 0.000060 Loss 6.274219, Accuracy 82.322%\n",
      "Epoch 18, Batch 466, LR 0.000060 Loss 6.273784, Accuracy 82.326%\n",
      "Epoch 18, Batch 467, LR 0.000060 Loss 6.275677, Accuracy 82.317%\n",
      "Epoch 18, Batch 468, LR 0.000060 Loss 6.274056, Accuracy 82.328%\n",
      "Epoch 18, Batch 469, LR 0.000060 Loss 6.274793, Accuracy 82.323%\n",
      "Epoch 18, Batch 470, LR 0.000060 Loss 6.275265, Accuracy 82.327%\n",
      "Epoch 18, Batch 471, LR 0.000060 Loss 6.276732, Accuracy 82.317%\n",
      "Epoch 18, Batch 472, LR 0.000060 Loss 6.276243, Accuracy 82.329%\n",
      "Epoch 18, Batch 473, LR 0.000060 Loss 6.276932, Accuracy 82.325%\n",
      "Epoch 18, Batch 474, LR 0.000060 Loss 6.277612, Accuracy 82.313%\n",
      "Epoch 18, Batch 475, LR 0.000060 Loss 6.278208, Accuracy 82.311%\n",
      "Epoch 18, Batch 476, LR 0.000060 Loss 6.278383, Accuracy 82.310%\n",
      "Epoch 18, Batch 477, LR 0.000060 Loss 6.277959, Accuracy 82.313%\n",
      "Epoch 18, Batch 478, LR 0.000060 Loss 6.277052, Accuracy 82.321%\n",
      "Epoch 18, Batch 479, LR 0.000060 Loss 6.275702, Accuracy 82.318%\n",
      "Epoch 18, Batch 480, LR 0.000060 Loss 6.274726, Accuracy 82.323%\n",
      "Epoch 18, Batch 481, LR 0.000060 Loss 6.274817, Accuracy 82.324%\n",
      "Epoch 18, Batch 482, LR 0.000060 Loss 6.273592, Accuracy 82.328%\n",
      "Epoch 18, Batch 483, LR 0.000060 Loss 6.273068, Accuracy 82.330%\n",
      "Epoch 18, Batch 484, LR 0.000060 Loss 6.274112, Accuracy 82.328%\n",
      "Epoch 18, Batch 485, LR 0.000060 Loss 6.273397, Accuracy 82.332%\n",
      "Epoch 18, Batch 486, LR 0.000060 Loss 6.273508, Accuracy 82.337%\n",
      "Epoch 18, Batch 487, LR 0.000060 Loss 6.272201, Accuracy 82.339%\n",
      "Epoch 18, Batch 488, LR 0.000060 Loss 6.271625, Accuracy 82.347%\n",
      "Epoch 18, Batch 489, LR 0.000060 Loss 6.271068, Accuracy 82.346%\n",
      "Epoch 18, Batch 490, LR 0.000060 Loss 6.270760, Accuracy 82.357%\n",
      "Epoch 18, Batch 491, LR 0.000060 Loss 6.272663, Accuracy 82.348%\n",
      "Epoch 18, Batch 492, LR 0.000060 Loss 6.272309, Accuracy 82.350%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 493, LR 0.000060 Loss 6.272014, Accuracy 82.362%\n",
      "Epoch 18, Batch 494, LR 0.000060 Loss 6.271030, Accuracy 82.367%\n",
      "Epoch 18, Batch 495, LR 0.000060 Loss 6.272543, Accuracy 82.350%\n",
      "Epoch 18, Batch 496, LR 0.000060 Loss 6.273831, Accuracy 82.343%\n",
      "Epoch 18, Batch 497, LR 0.000060 Loss 6.273170, Accuracy 82.338%\n",
      "Epoch 18, Batch 498, LR 0.000060 Loss 6.271376, Accuracy 82.343%\n",
      "Epoch 18, Batch 499, LR 0.000060 Loss 6.270551, Accuracy 82.349%\n",
      "Epoch 18, Batch 500, LR 0.000060 Loss 6.270775, Accuracy 82.345%\n",
      "Epoch 18, Batch 501, LR 0.000060 Loss 6.270425, Accuracy 82.343%\n",
      "Epoch 18, Batch 502, LR 0.000060 Loss 6.271468, Accuracy 82.336%\n",
      "Epoch 18, Batch 503, LR 0.000060 Loss 6.271091, Accuracy 82.323%\n",
      "Epoch 18, Batch 504, LR 0.000060 Loss 6.270695, Accuracy 82.318%\n",
      "Epoch 18, Batch 505, LR 0.000060 Loss 6.270079, Accuracy 82.319%\n",
      "Epoch 18, Batch 506, LR 0.000060 Loss 6.270459, Accuracy 82.320%\n",
      "Epoch 18, Batch 507, LR 0.000060 Loss 6.269847, Accuracy 82.322%\n",
      "Epoch 18, Batch 508, LR 0.000060 Loss 6.270316, Accuracy 82.317%\n",
      "Epoch 18, Batch 509, LR 0.000060 Loss 6.269205, Accuracy 82.321%\n",
      "Epoch 18, Batch 510, LR 0.000060 Loss 6.269697, Accuracy 82.318%\n",
      "Epoch 18, Batch 511, LR 0.000060 Loss 6.269597, Accuracy 82.323%\n",
      "Epoch 18, Batch 512, LR 0.000060 Loss 6.269319, Accuracy 82.326%\n",
      "Epoch 18, Batch 513, LR 0.000060 Loss 6.268748, Accuracy 82.327%\n",
      "Epoch 18, Batch 514, LR 0.000060 Loss 6.268061, Accuracy 82.329%\n",
      "Epoch 18, Batch 515, LR 0.000060 Loss 6.268427, Accuracy 82.329%\n",
      "Epoch 18, Batch 516, LR 0.000060 Loss 6.268110, Accuracy 82.331%\n",
      "Epoch 18, Batch 517, LR 0.000060 Loss 6.268322, Accuracy 82.332%\n",
      "Epoch 18, Batch 518, LR 0.000060 Loss 6.267376, Accuracy 82.331%\n",
      "Epoch 18, Batch 519, LR 0.000060 Loss 6.267123, Accuracy 82.328%\n",
      "Epoch 18, Batch 520, LR 0.000060 Loss 6.266266, Accuracy 82.335%\n",
      "Epoch 18, Batch 521, LR 0.000060 Loss 6.266533, Accuracy 82.336%\n",
      "Epoch 18, Batch 522, LR 0.000060 Loss 6.268289, Accuracy 82.325%\n",
      "Epoch 18, Batch 523, LR 0.000060 Loss 6.267984, Accuracy 82.326%\n",
      "Epoch 18, Batch 524, LR 0.000060 Loss 6.267829, Accuracy 82.323%\n",
      "Epoch 18, Batch 525, LR 0.000060 Loss 6.267369, Accuracy 82.326%\n",
      "Epoch 18, Batch 526, LR 0.000060 Loss 6.266390, Accuracy 82.324%\n",
      "Epoch 18, Batch 527, LR 0.000060 Loss 6.266574, Accuracy 82.320%\n",
      "Epoch 18, Batch 528, LR 0.000060 Loss 6.267188, Accuracy 82.321%\n",
      "Epoch 18, Batch 529, LR 0.000060 Loss 6.266222, Accuracy 82.328%\n",
      "Epoch 18, Batch 530, LR 0.000060 Loss 6.266155, Accuracy 82.328%\n",
      "Epoch 18, Batch 531, LR 0.000060 Loss 6.265514, Accuracy 82.331%\n",
      "Epoch 18, Batch 532, LR 0.000060 Loss 6.264630, Accuracy 82.338%\n",
      "Epoch 18, Batch 533, LR 0.000060 Loss 6.263125, Accuracy 82.342%\n",
      "Epoch 18, Batch 534, LR 0.000060 Loss 6.263734, Accuracy 82.341%\n",
      "Epoch 18, Batch 535, LR 0.000060 Loss 6.264022, Accuracy 82.344%\n",
      "Epoch 18, Batch 536, LR 0.000060 Loss 6.265077, Accuracy 82.336%\n",
      "Epoch 18, Batch 537, LR 0.000060 Loss 6.264976, Accuracy 82.332%\n",
      "Epoch 18, Batch 538, LR 0.000060 Loss 6.263490, Accuracy 82.342%\n",
      "Epoch 18, Batch 539, LR 0.000060 Loss 6.262189, Accuracy 82.341%\n",
      "Epoch 18, Batch 540, LR 0.000060 Loss 6.261868, Accuracy 82.344%\n",
      "Epoch 18, Batch 541, LR 0.000060 Loss 6.260969, Accuracy 82.343%\n",
      "Epoch 18, Batch 542, LR 0.000060 Loss 6.262843, Accuracy 82.334%\n",
      "Epoch 18, Batch 543, LR 0.000060 Loss 6.264258, Accuracy 82.322%\n",
      "Epoch 18, Batch 544, LR 0.000060 Loss 6.263616, Accuracy 82.323%\n",
      "Epoch 18, Batch 545, LR 0.000060 Loss 6.263565, Accuracy 82.324%\n",
      "Epoch 18, Batch 546, LR 0.000060 Loss 6.262941, Accuracy 82.329%\n",
      "Epoch 18, Batch 547, LR 0.000060 Loss 6.262468, Accuracy 82.333%\n",
      "Epoch 18, Batch 548, LR 0.000060 Loss 6.262955, Accuracy 82.324%\n",
      "Epoch 18, Batch 549, LR 0.000060 Loss 6.261077, Accuracy 82.329%\n",
      "Epoch 18, Batch 550, LR 0.000060 Loss 6.260254, Accuracy 82.337%\n",
      "Epoch 18, Batch 551, LR 0.000060 Loss 6.259905, Accuracy 82.336%\n",
      "Epoch 18, Batch 552, LR 0.000060 Loss 6.260657, Accuracy 82.330%\n",
      "Epoch 18, Batch 553, LR 0.000060 Loss 6.260352, Accuracy 82.339%\n",
      "Epoch 18, Batch 554, LR 0.000060 Loss 6.260985, Accuracy 82.334%\n",
      "Epoch 18, Batch 555, LR 0.000060 Loss 6.261545, Accuracy 82.327%\n",
      "Epoch 18, Batch 556, LR 0.000060 Loss 6.263242, Accuracy 82.314%\n",
      "Epoch 18, Batch 557, LR 0.000060 Loss 6.264873, Accuracy 82.303%\n",
      "Epoch 18, Batch 558, LR 0.000060 Loss 6.263447, Accuracy 82.311%\n",
      "Epoch 18, Batch 559, LR 0.000060 Loss 6.263985, Accuracy 82.307%\n",
      "Epoch 18, Batch 560, LR 0.000060 Loss 6.264660, Accuracy 82.306%\n",
      "Epoch 18, Batch 561, LR 0.000060 Loss 6.264599, Accuracy 82.308%\n",
      "Epoch 18, Batch 562, LR 0.000060 Loss 6.263597, Accuracy 82.315%\n",
      "Epoch 18, Batch 563, LR 0.000060 Loss 6.263658, Accuracy 82.314%\n",
      "Epoch 18, Batch 564, LR 0.000060 Loss 6.263844, Accuracy 82.310%\n",
      "Epoch 18, Batch 565, LR 0.000060 Loss 6.263421, Accuracy 82.312%\n",
      "Epoch 18, Batch 566, LR 0.000060 Loss 6.264040, Accuracy 82.316%\n",
      "Epoch 18, Batch 567, LR 0.000060 Loss 6.262761, Accuracy 82.321%\n",
      "Epoch 18, Batch 568, LR 0.000060 Loss 6.262578, Accuracy 82.321%\n",
      "Epoch 18, Batch 569, LR 0.000060 Loss 6.262158, Accuracy 82.326%\n",
      "Epoch 18, Batch 570, LR 0.000060 Loss 6.260995, Accuracy 82.327%\n",
      "Epoch 18, Batch 571, LR 0.000060 Loss 6.261576, Accuracy 82.324%\n",
      "Epoch 18, Batch 572, LR 0.000060 Loss 6.262549, Accuracy 82.317%\n",
      "Epoch 18, Batch 573, LR 0.000060 Loss 6.262341, Accuracy 82.330%\n",
      "Epoch 18, Batch 574, LR 0.000060 Loss 6.261195, Accuracy 82.329%\n",
      "Epoch 18, Batch 575, LR 0.000060 Loss 6.261549, Accuracy 82.323%\n",
      "Epoch 18, Batch 576, LR 0.000060 Loss 6.261736, Accuracy 82.317%\n",
      "Epoch 18, Batch 577, LR 0.000060 Loss 6.260214, Accuracy 82.332%\n",
      "Epoch 18, Batch 578, LR 0.000060 Loss 6.260303, Accuracy 82.337%\n",
      "Epoch 18, Batch 579, LR 0.000060 Loss 6.260564, Accuracy 82.332%\n",
      "Epoch 18, Batch 580, LR 0.000060 Loss 6.260239, Accuracy 82.330%\n",
      "Epoch 18, Batch 581, LR 0.000060 Loss 6.260380, Accuracy 82.331%\n",
      "Epoch 18, Batch 582, LR 0.000060 Loss 6.262518, Accuracy 82.313%\n",
      "Epoch 18, Batch 583, LR 0.000060 Loss 6.262885, Accuracy 82.305%\n",
      "Epoch 18, Batch 584, LR 0.000060 Loss 6.263141, Accuracy 82.301%\n",
      "Epoch 18, Batch 585, LR 0.000060 Loss 6.261515, Accuracy 82.310%\n",
      "Epoch 18, Batch 586, LR 0.000060 Loss 6.262769, Accuracy 82.301%\n",
      "Epoch 18, Batch 587, LR 0.000060 Loss 6.262549, Accuracy 82.305%\n",
      "Epoch 18, Batch 588, LR 0.000060 Loss 6.262282, Accuracy 82.305%\n",
      "Epoch 18, Batch 589, LR 0.000060 Loss 6.261716, Accuracy 82.308%\n",
      "Epoch 18, Batch 590, LR 0.000060 Loss 6.261041, Accuracy 82.312%\n",
      "Epoch 18, Batch 591, LR 0.000060 Loss 6.261598, Accuracy 82.313%\n",
      "Epoch 18, Batch 592, LR 0.000060 Loss 6.260042, Accuracy 82.320%\n",
      "Epoch 18, Batch 593, LR 0.000060 Loss 6.258336, Accuracy 82.325%\n",
      "Epoch 18, Batch 594, LR 0.000060 Loss 6.257065, Accuracy 82.332%\n",
      "Epoch 18, Batch 595, LR 0.000060 Loss 6.256927, Accuracy 82.331%\n",
      "Epoch 18, Batch 596, LR 0.000060 Loss 6.256724, Accuracy 82.333%\n",
      "Epoch 18, Batch 597, LR 0.000060 Loss 6.256438, Accuracy 82.332%\n",
      "Epoch 18, Batch 598, LR 0.000060 Loss 6.256941, Accuracy 82.321%\n",
      "Epoch 18, Batch 599, LR 0.000060 Loss 6.256555, Accuracy 82.322%\n",
      "Epoch 18, Batch 600, LR 0.000060 Loss 6.256777, Accuracy 82.318%\n",
      "Epoch 18, Batch 601, LR 0.000060 Loss 6.256614, Accuracy 82.319%\n",
      "Epoch 18, Batch 602, LR 0.000060 Loss 6.256936, Accuracy 82.319%\n",
      "Epoch 18, Batch 603, LR 0.000060 Loss 6.256923, Accuracy 82.318%\n",
      "Epoch 18, Batch 604, LR 0.000060 Loss 6.257650, Accuracy 82.309%\n",
      "Epoch 18, Batch 605, LR 0.000060 Loss 6.258273, Accuracy 82.305%\n",
      "Epoch 18, Batch 606, LR 0.000060 Loss 6.256783, Accuracy 82.314%\n",
      "Epoch 18, Batch 607, LR 0.000060 Loss 6.255015, Accuracy 82.323%\n",
      "Epoch 18, Batch 608, LR 0.000060 Loss 6.255297, Accuracy 82.329%\n",
      "Epoch 18, Batch 609, LR 0.000060 Loss 6.255045, Accuracy 82.329%\n",
      "Epoch 18, Batch 610, LR 0.000060 Loss 6.255028, Accuracy 82.326%\n",
      "Epoch 18, Batch 611, LR 0.000060 Loss 6.254182, Accuracy 82.325%\n",
      "Epoch 18, Batch 612, LR 0.000060 Loss 6.253496, Accuracy 82.331%\n",
      "Epoch 18, Batch 613, LR 0.000060 Loss 6.252094, Accuracy 82.341%\n",
      "Epoch 18, Batch 614, LR 0.000060 Loss 6.252411, Accuracy 82.334%\n",
      "Epoch 18, Batch 615, LR 0.000060 Loss 6.252305, Accuracy 82.332%\n",
      "Epoch 18, Batch 616, LR 0.000060 Loss 6.251094, Accuracy 82.339%\n",
      "Epoch 18, Batch 617, LR 0.000060 Loss 6.251263, Accuracy 82.341%\n",
      "Epoch 18, Batch 618, LR 0.000060 Loss 6.252376, Accuracy 82.331%\n",
      "Epoch 18, Batch 619, LR 0.000060 Loss 6.252071, Accuracy 82.330%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 620, LR 0.000060 Loss 6.251516, Accuracy 82.332%\n",
      "Epoch 18, Batch 621, LR 0.000060 Loss 6.252242, Accuracy 82.329%\n",
      "Epoch 18, Batch 622, LR 0.000060 Loss 6.251926, Accuracy 82.339%\n",
      "Epoch 18, Batch 623, LR 0.000060 Loss 6.251814, Accuracy 82.336%\n",
      "Epoch 18, Batch 624, LR 0.000060 Loss 6.251170, Accuracy 82.337%\n",
      "Epoch 18, Batch 625, LR 0.000060 Loss 6.250314, Accuracy 82.335%\n",
      "Epoch 18, Batch 626, LR 0.000060 Loss 6.251082, Accuracy 82.328%\n",
      "Epoch 18, Batch 627, LR 0.000060 Loss 6.249931, Accuracy 82.332%\n",
      "Epoch 18, Batch 628, LR 0.000060 Loss 6.251132, Accuracy 82.320%\n",
      "Epoch 18, Batch 629, LR 0.000060 Loss 6.250683, Accuracy 82.324%\n",
      "Epoch 18, Batch 630, LR 0.000060 Loss 6.250159, Accuracy 82.326%\n",
      "Epoch 18, Batch 631, LR 0.000060 Loss 6.249487, Accuracy 82.328%\n",
      "Epoch 18, Batch 632, LR 0.000060 Loss 6.249269, Accuracy 82.332%\n",
      "Epoch 18, Batch 633, LR 0.000060 Loss 6.249646, Accuracy 82.326%\n",
      "Epoch 18, Batch 634, LR 0.000060 Loss 6.248997, Accuracy 82.328%\n",
      "Epoch 18, Batch 635, LR 0.000060 Loss 6.248351, Accuracy 82.334%\n",
      "Epoch 18, Batch 636, LR 0.000060 Loss 6.248355, Accuracy 82.337%\n",
      "Epoch 18, Batch 637, LR 0.000060 Loss 6.249680, Accuracy 82.324%\n",
      "Epoch 18, Batch 638, LR 0.000060 Loss 6.250644, Accuracy 82.321%\n",
      "Epoch 18, Batch 639, LR 0.000060 Loss 6.249805, Accuracy 82.323%\n",
      "Epoch 18, Batch 640, LR 0.000060 Loss 6.249438, Accuracy 82.325%\n",
      "Epoch 18, Batch 641, LR 0.000060 Loss 6.249705, Accuracy 82.324%\n",
      "Epoch 18, Batch 642, LR 0.000060 Loss 6.249281, Accuracy 82.325%\n",
      "Epoch 18, Batch 643, LR 0.000060 Loss 6.249511, Accuracy 82.330%\n",
      "Epoch 18, Batch 644, LR 0.000060 Loss 6.250605, Accuracy 82.325%\n",
      "Epoch 18, Batch 645, LR 0.000060 Loss 6.248648, Accuracy 82.343%\n",
      "Epoch 18, Batch 646, LR 0.000060 Loss 6.248258, Accuracy 82.341%\n",
      "Epoch 18, Batch 647, LR 0.000060 Loss 6.248419, Accuracy 82.343%\n",
      "Epoch 18, Batch 648, LR 0.000060 Loss 6.248814, Accuracy 82.337%\n",
      "Epoch 18, Batch 649, LR 0.000060 Loss 6.248063, Accuracy 82.343%\n",
      "Epoch 18, Batch 650, LR 0.000060 Loss 6.248216, Accuracy 82.347%\n",
      "Epoch 18, Batch 651, LR 0.000060 Loss 6.247916, Accuracy 82.348%\n",
      "Epoch 18, Batch 652, LR 0.000060 Loss 6.247486, Accuracy 82.348%\n",
      "Epoch 18, Batch 653, LR 0.000060 Loss 6.247172, Accuracy 82.347%\n",
      "Epoch 18, Batch 654, LR 0.000060 Loss 6.247425, Accuracy 82.347%\n",
      "Epoch 18, Batch 655, LR 0.000060 Loss 6.246451, Accuracy 82.353%\n",
      "Epoch 18, Batch 656, LR 0.000060 Loss 6.246827, Accuracy 82.356%\n",
      "Epoch 18, Batch 657, LR 0.000060 Loss 6.247254, Accuracy 82.352%\n",
      "Epoch 18, Batch 658, LR 0.000060 Loss 6.248397, Accuracy 82.342%\n",
      "Epoch 18, Batch 659, LR 0.000060 Loss 6.248987, Accuracy 82.336%\n",
      "Epoch 18, Batch 660, LR 0.000060 Loss 6.249526, Accuracy 82.325%\n",
      "Epoch 18, Batch 661, LR 0.000060 Loss 6.248147, Accuracy 82.334%\n",
      "Epoch 18, Batch 662, LR 0.000060 Loss 6.248373, Accuracy 82.340%\n",
      "Epoch 18, Batch 663, LR 0.000060 Loss 6.248578, Accuracy 82.339%\n",
      "Epoch 18, Batch 664, LR 0.000060 Loss 6.248335, Accuracy 82.341%\n",
      "Epoch 18, Batch 665, LR 0.000060 Loss 6.248628, Accuracy 82.340%\n",
      "Epoch 18, Batch 666, LR 0.000060 Loss 6.249149, Accuracy 82.342%\n",
      "Epoch 18, Batch 667, LR 0.000060 Loss 6.249377, Accuracy 82.344%\n",
      "Epoch 18, Batch 668, LR 0.000060 Loss 6.249386, Accuracy 82.347%\n",
      "Epoch 18, Batch 669, LR 0.000060 Loss 6.249177, Accuracy 82.348%\n",
      "Epoch 18, Batch 670, LR 0.000060 Loss 6.247884, Accuracy 82.352%\n",
      "Epoch 18, Batch 671, LR 0.000060 Loss 6.248593, Accuracy 82.346%\n",
      "Epoch 18, Batch 672, LR 0.000060 Loss 6.249399, Accuracy 82.339%\n",
      "Epoch 18, Batch 673, LR 0.000060 Loss 6.249400, Accuracy 82.340%\n",
      "Epoch 18, Batch 674, LR 0.000060 Loss 6.249180, Accuracy 82.343%\n",
      "Epoch 18, Batch 675, LR 0.000060 Loss 6.249152, Accuracy 82.346%\n",
      "Epoch 18, Batch 676, LR 0.000060 Loss 6.249753, Accuracy 82.342%\n",
      "Epoch 18, Batch 677, LR 0.000060 Loss 6.250609, Accuracy 82.336%\n",
      "Epoch 18, Batch 678, LR 0.000060 Loss 6.252236, Accuracy 82.327%\n",
      "Epoch 18, Batch 679, LR 0.000060 Loss 6.251897, Accuracy 82.327%\n",
      "Epoch 18, Batch 680, LR 0.000060 Loss 6.251870, Accuracy 82.330%\n",
      "Epoch 18, Batch 681, LR 0.000060 Loss 6.252042, Accuracy 82.330%\n",
      "Epoch 18, Batch 682, LR 0.000060 Loss 6.251581, Accuracy 82.330%\n",
      "Epoch 18, Batch 683, LR 0.000060 Loss 6.250460, Accuracy 82.341%\n",
      "Epoch 18, Batch 684, LR 0.000060 Loss 6.249350, Accuracy 82.338%\n",
      "Epoch 18, Batch 685, LR 0.000060 Loss 6.248415, Accuracy 82.346%\n",
      "Epoch 18, Batch 686, LR 0.000060 Loss 6.249350, Accuracy 82.341%\n",
      "Epoch 18, Batch 687, LR 0.000060 Loss 6.248519, Accuracy 82.350%\n",
      "Epoch 18, Batch 688, LR 0.000060 Loss 6.248808, Accuracy 82.342%\n",
      "Epoch 18, Batch 689, LR 0.000060 Loss 6.247932, Accuracy 82.349%\n",
      "Epoch 18, Batch 690, LR 0.000060 Loss 6.248527, Accuracy 82.343%\n",
      "Epoch 18, Batch 691, LR 0.000060 Loss 6.248087, Accuracy 82.349%\n",
      "Epoch 18, Batch 692, LR 0.000060 Loss 6.247530, Accuracy 82.355%\n",
      "Epoch 18, Batch 693, LR 0.000060 Loss 6.247102, Accuracy 82.354%\n",
      "Epoch 18, Batch 694, LR 0.000060 Loss 6.247111, Accuracy 82.357%\n",
      "Epoch 18, Batch 695, LR 0.000060 Loss 6.245836, Accuracy 82.365%\n",
      "Epoch 18, Batch 696, LR 0.000060 Loss 6.246041, Accuracy 82.360%\n",
      "Epoch 18, Batch 697, LR 0.000060 Loss 6.246003, Accuracy 82.362%\n",
      "Epoch 18, Batch 698, LR 0.000060 Loss 6.245777, Accuracy 82.364%\n",
      "Epoch 18, Batch 699, LR 0.000060 Loss 6.245406, Accuracy 82.363%\n",
      "Epoch 18, Batch 700, LR 0.000060 Loss 6.245872, Accuracy 82.362%\n",
      "Epoch 18, Batch 701, LR 0.000060 Loss 6.245466, Accuracy 82.363%\n",
      "Epoch 18, Batch 702, LR 0.000060 Loss 6.244421, Accuracy 82.373%\n",
      "Epoch 18, Batch 703, LR 0.000060 Loss 6.245461, Accuracy 82.365%\n",
      "Epoch 18, Batch 704, LR 0.000060 Loss 6.245476, Accuracy 82.362%\n",
      "Epoch 18, Batch 705, LR 0.000060 Loss 6.245242, Accuracy 82.364%\n",
      "Epoch 18, Batch 706, LR 0.000060 Loss 6.245140, Accuracy 82.365%\n",
      "Epoch 18, Batch 707, LR 0.000060 Loss 6.244428, Accuracy 82.369%\n",
      "Epoch 18, Batch 708, LR 0.000060 Loss 6.243971, Accuracy 82.366%\n",
      "Epoch 18, Batch 709, LR 0.000060 Loss 6.242359, Accuracy 82.373%\n",
      "Epoch 18, Batch 710, LR 0.000060 Loss 6.242263, Accuracy 82.368%\n",
      "Epoch 18, Batch 711, LR 0.000060 Loss 6.241786, Accuracy 82.362%\n",
      "Epoch 18, Batch 712, LR 0.000060 Loss 6.241216, Accuracy 82.369%\n",
      "Epoch 18, Batch 713, LR 0.000060 Loss 6.240668, Accuracy 82.369%\n",
      "Epoch 18, Batch 714, LR 0.000060 Loss 6.239344, Accuracy 82.370%\n",
      "Epoch 18, Batch 715, LR 0.000060 Loss 6.238496, Accuracy 82.373%\n",
      "Epoch 18, Batch 716, LR 0.000060 Loss 6.238623, Accuracy 82.372%\n",
      "Epoch 18, Batch 717, LR 0.000060 Loss 6.238783, Accuracy 82.368%\n",
      "Epoch 18, Batch 718, LR 0.000060 Loss 6.238690, Accuracy 82.366%\n",
      "Epoch 18, Batch 719, LR 0.000060 Loss 6.238743, Accuracy 82.365%\n",
      "Epoch 18, Batch 720, LR 0.000060 Loss 6.239188, Accuracy 82.361%\n",
      "Epoch 18, Batch 721, LR 0.000059 Loss 6.238842, Accuracy 82.361%\n",
      "Epoch 18, Batch 722, LR 0.000059 Loss 6.238015, Accuracy 82.365%\n",
      "Epoch 18, Batch 723, LR 0.000059 Loss 6.236794, Accuracy 82.368%\n",
      "Epoch 18, Batch 724, LR 0.000059 Loss 6.236480, Accuracy 82.370%\n",
      "Epoch 18, Batch 725, LR 0.000059 Loss 6.236251, Accuracy 82.372%\n",
      "Epoch 18, Batch 726, LR 0.000059 Loss 6.235816, Accuracy 82.367%\n",
      "Epoch 18, Batch 727, LR 0.000059 Loss 6.235853, Accuracy 82.369%\n",
      "Epoch 18, Batch 728, LR 0.000059 Loss 6.236192, Accuracy 82.365%\n",
      "Epoch 18, Batch 729, LR 0.000059 Loss 6.235865, Accuracy 82.372%\n",
      "Epoch 18, Batch 730, LR 0.000059 Loss 6.236983, Accuracy 82.363%\n",
      "Epoch 18, Batch 731, LR 0.000059 Loss 6.236401, Accuracy 82.368%\n",
      "Epoch 18, Batch 732, LR 0.000059 Loss 6.236704, Accuracy 82.364%\n",
      "Epoch 18, Batch 733, LR 0.000059 Loss 6.237276, Accuracy 82.363%\n",
      "Epoch 18, Batch 734, LR 0.000059 Loss 6.238252, Accuracy 82.354%\n",
      "Epoch 18, Batch 735, LR 0.000059 Loss 6.238021, Accuracy 82.355%\n",
      "Epoch 18, Batch 736, LR 0.000059 Loss 6.237050, Accuracy 82.360%\n",
      "Epoch 18, Batch 737, LR 0.000059 Loss 6.236492, Accuracy 82.366%\n",
      "Epoch 18, Batch 738, LR 0.000059 Loss 6.237320, Accuracy 82.364%\n",
      "Epoch 18, Batch 739, LR 0.000059 Loss 6.237006, Accuracy 82.366%\n",
      "Epoch 18, Batch 740, LR 0.000059 Loss 6.237749, Accuracy 82.361%\n",
      "Epoch 18, Batch 741, LR 0.000059 Loss 6.237714, Accuracy 82.355%\n",
      "Epoch 18, Batch 742, LR 0.000059 Loss 6.236918, Accuracy 82.361%\n",
      "Epoch 18, Batch 743, LR 0.000059 Loss 6.236910, Accuracy 82.365%\n",
      "Epoch 18, Batch 744, LR 0.000059 Loss 6.237677, Accuracy 82.357%\n",
      "Epoch 18, Batch 745, LR 0.000059 Loss 6.238523, Accuracy 82.349%\n",
      "Epoch 18, Batch 746, LR 0.000059 Loss 6.239416, Accuracy 82.346%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 747, LR 0.000059 Loss 6.239959, Accuracy 82.343%\n",
      "Epoch 18, Batch 748, LR 0.000059 Loss 6.240029, Accuracy 82.346%\n",
      "Epoch 18, Batch 749, LR 0.000059 Loss 6.240649, Accuracy 82.339%\n",
      "Epoch 18, Batch 750, LR 0.000059 Loss 6.240975, Accuracy 82.331%\n",
      "Epoch 18, Batch 751, LR 0.000059 Loss 6.240777, Accuracy 82.331%\n",
      "Epoch 18, Batch 752, LR 0.000059 Loss 6.240664, Accuracy 82.328%\n",
      "Epoch 18, Batch 753, LR 0.000059 Loss 6.241418, Accuracy 82.327%\n",
      "Epoch 18, Batch 754, LR 0.000059 Loss 6.242326, Accuracy 82.327%\n",
      "Epoch 18, Batch 755, LR 0.000059 Loss 6.241206, Accuracy 82.332%\n",
      "Epoch 18, Batch 756, LR 0.000059 Loss 6.240839, Accuracy 82.331%\n",
      "Epoch 18, Batch 757, LR 0.000059 Loss 6.239986, Accuracy 82.332%\n",
      "Epoch 18, Batch 758, LR 0.000059 Loss 6.242141, Accuracy 82.323%\n",
      "Epoch 18, Batch 759, LR 0.000059 Loss 6.241464, Accuracy 82.330%\n",
      "Epoch 18, Batch 760, LR 0.000059 Loss 6.241719, Accuracy 82.327%\n",
      "Epoch 18, Batch 761, LR 0.000059 Loss 6.241576, Accuracy 82.332%\n",
      "Epoch 18, Batch 762, LR 0.000059 Loss 6.241174, Accuracy 82.337%\n",
      "Epoch 18, Batch 763, LR 0.000059 Loss 6.241336, Accuracy 82.335%\n",
      "Epoch 18, Batch 764, LR 0.000059 Loss 6.240588, Accuracy 82.336%\n",
      "Epoch 18, Batch 765, LR 0.000059 Loss 6.240806, Accuracy 82.329%\n",
      "Epoch 18, Batch 766, LR 0.000059 Loss 6.240762, Accuracy 82.330%\n",
      "Epoch 18, Batch 767, LR 0.000059 Loss 6.241215, Accuracy 82.331%\n",
      "Epoch 18, Batch 768, LR 0.000059 Loss 6.241639, Accuracy 82.330%\n",
      "Epoch 18, Batch 769, LR 0.000059 Loss 6.241320, Accuracy 82.334%\n",
      "Epoch 18, Batch 770, LR 0.000059 Loss 6.241498, Accuracy 82.338%\n",
      "Epoch 18, Batch 771, LR 0.000059 Loss 6.240871, Accuracy 82.335%\n",
      "Epoch 18, Batch 772, LR 0.000059 Loss 6.240837, Accuracy 82.334%\n",
      "Epoch 18, Batch 773, LR 0.000059 Loss 6.241029, Accuracy 82.335%\n",
      "Epoch 18, Batch 774, LR 0.000059 Loss 6.240956, Accuracy 82.339%\n",
      "Epoch 18, Batch 775, LR 0.000059 Loss 6.240951, Accuracy 82.335%\n",
      "Epoch 18, Batch 776, LR 0.000059 Loss 6.240860, Accuracy 82.336%\n",
      "Epoch 18, Batch 777, LR 0.000059 Loss 6.240344, Accuracy 82.341%\n",
      "Epoch 18, Batch 778, LR 0.000059 Loss 6.240441, Accuracy 82.342%\n",
      "Epoch 18, Batch 779, LR 0.000059 Loss 6.240235, Accuracy 82.340%\n",
      "Epoch 18, Batch 780, LR 0.000059 Loss 6.240870, Accuracy 82.335%\n",
      "Epoch 18, Batch 781, LR 0.000059 Loss 6.241116, Accuracy 82.331%\n",
      "Epoch 18, Batch 782, LR 0.000059 Loss 6.240804, Accuracy 82.340%\n",
      "Epoch 18, Batch 783, LR 0.000059 Loss 6.240366, Accuracy 82.342%\n",
      "Epoch 18, Batch 784, LR 0.000059 Loss 6.240390, Accuracy 82.339%\n",
      "Epoch 18, Batch 785, LR 0.000059 Loss 6.239320, Accuracy 82.344%\n",
      "Epoch 18, Batch 786, LR 0.000059 Loss 6.239687, Accuracy 82.342%\n",
      "Epoch 18, Batch 787, LR 0.000059 Loss 6.240014, Accuracy 82.337%\n",
      "Epoch 18, Batch 788, LR 0.000059 Loss 6.239501, Accuracy 82.340%\n",
      "Epoch 18, Batch 789, LR 0.000059 Loss 6.240143, Accuracy 82.337%\n",
      "Epoch 18, Batch 790, LR 0.000059 Loss 6.239594, Accuracy 82.341%\n",
      "Epoch 18, Batch 791, LR 0.000059 Loss 6.239510, Accuracy 82.341%\n",
      "Epoch 18, Batch 792, LR 0.000059 Loss 6.239535, Accuracy 82.339%\n",
      "Epoch 18, Batch 793, LR 0.000059 Loss 6.239390, Accuracy 82.341%\n",
      "Epoch 18, Batch 794, LR 0.000059 Loss 6.239506, Accuracy 82.336%\n",
      "Epoch 18, Batch 795, LR 0.000059 Loss 6.240199, Accuracy 82.335%\n",
      "Epoch 18, Batch 796, LR 0.000059 Loss 6.240305, Accuracy 82.334%\n",
      "Epoch 18, Batch 797, LR 0.000059 Loss 6.240691, Accuracy 82.330%\n",
      "Epoch 18, Batch 798, LR 0.000059 Loss 6.240456, Accuracy 82.334%\n",
      "Epoch 18, Batch 799, LR 0.000059 Loss 6.238662, Accuracy 82.339%\n",
      "Epoch 18, Batch 800, LR 0.000059 Loss 6.239456, Accuracy 82.336%\n",
      "Epoch 18, Batch 801, LR 0.000059 Loss 6.238373, Accuracy 82.338%\n",
      "Epoch 18, Batch 802, LR 0.000059 Loss 6.238900, Accuracy 82.337%\n",
      "Epoch 18, Batch 803, LR 0.000059 Loss 6.238646, Accuracy 82.339%\n",
      "Epoch 18, Batch 804, LR 0.000059 Loss 6.238360, Accuracy 82.337%\n",
      "Epoch 18, Batch 805, LR 0.000059 Loss 6.238590, Accuracy 82.337%\n",
      "Epoch 18, Batch 806, LR 0.000059 Loss 6.238074, Accuracy 82.337%\n",
      "Epoch 18, Batch 807, LR 0.000059 Loss 6.238344, Accuracy 82.336%\n",
      "Epoch 18, Batch 808, LR 0.000059 Loss 6.238343, Accuracy 82.334%\n",
      "Epoch 18, Batch 809, LR 0.000059 Loss 6.237404, Accuracy 82.342%\n",
      "Epoch 18, Batch 810, LR 0.000059 Loss 6.237444, Accuracy 82.344%\n",
      "Epoch 18, Batch 811, LR 0.000059 Loss 6.237654, Accuracy 82.344%\n",
      "Epoch 18, Batch 812, LR 0.000059 Loss 6.237414, Accuracy 82.348%\n",
      "Epoch 18, Batch 813, LR 0.000059 Loss 6.237554, Accuracy 82.347%\n",
      "Epoch 18, Batch 814, LR 0.000059 Loss 6.236254, Accuracy 82.355%\n",
      "Epoch 18, Batch 815, LR 0.000059 Loss 6.235616, Accuracy 82.355%\n",
      "Epoch 18, Batch 816, LR 0.000059 Loss 6.235672, Accuracy 82.355%\n",
      "Epoch 18, Batch 817, LR 0.000059 Loss 6.235816, Accuracy 82.354%\n",
      "Epoch 18, Batch 818, LR 0.000059 Loss 6.235845, Accuracy 82.355%\n",
      "Epoch 18, Batch 819, LR 0.000059 Loss 6.235946, Accuracy 82.355%\n",
      "Epoch 18, Batch 820, LR 0.000059 Loss 6.235901, Accuracy 82.358%\n",
      "Epoch 18, Batch 821, LR 0.000059 Loss 6.236201, Accuracy 82.354%\n",
      "Epoch 18, Batch 822, LR 0.000059 Loss 6.235994, Accuracy 82.358%\n",
      "Epoch 18, Batch 823, LR 0.000059 Loss 6.235683, Accuracy 82.361%\n",
      "Epoch 18, Batch 824, LR 0.000059 Loss 6.235259, Accuracy 82.358%\n",
      "Epoch 18, Batch 825, LR 0.000059 Loss 6.234829, Accuracy 82.361%\n",
      "Epoch 18, Batch 826, LR 0.000059 Loss 6.234740, Accuracy 82.364%\n",
      "Epoch 18, Batch 827, LR 0.000059 Loss 6.235380, Accuracy 82.354%\n",
      "Epoch 18, Batch 828, LR 0.000059 Loss 6.235030, Accuracy 82.350%\n",
      "Epoch 18, Batch 829, LR 0.000059 Loss 6.234986, Accuracy 82.353%\n",
      "Epoch 18, Batch 830, LR 0.000059 Loss 6.234359, Accuracy 82.352%\n",
      "Epoch 18, Batch 831, LR 0.000059 Loss 6.234607, Accuracy 82.354%\n",
      "Epoch 18, Batch 832, LR 0.000059 Loss 6.235403, Accuracy 82.347%\n",
      "Epoch 18, Batch 833, LR 0.000059 Loss 6.235203, Accuracy 82.345%\n",
      "Epoch 18, Batch 834, LR 0.000059 Loss 6.235538, Accuracy 82.343%\n",
      "Epoch 18, Batch 835, LR 0.000059 Loss 6.235171, Accuracy 82.347%\n",
      "Epoch 18, Batch 836, LR 0.000059 Loss 6.234830, Accuracy 82.347%\n",
      "Epoch 18, Batch 837, LR 0.000059 Loss 6.234391, Accuracy 82.347%\n",
      "Epoch 18, Batch 838, LR 0.000059 Loss 6.234132, Accuracy 82.352%\n",
      "Epoch 18, Batch 839, LR 0.000059 Loss 6.232965, Accuracy 82.355%\n",
      "Epoch 18, Batch 840, LR 0.000059 Loss 6.232697, Accuracy 82.360%\n",
      "Epoch 18, Batch 841, LR 0.000059 Loss 6.232731, Accuracy 82.359%\n",
      "Epoch 18, Batch 842, LR 0.000059 Loss 6.233742, Accuracy 82.351%\n",
      "Epoch 18, Batch 843, LR 0.000059 Loss 6.233143, Accuracy 82.352%\n",
      "Epoch 18, Batch 844, LR 0.000059 Loss 6.233121, Accuracy 82.352%\n",
      "Epoch 18, Batch 845, LR 0.000059 Loss 6.232925, Accuracy 82.353%\n",
      "Epoch 18, Batch 846, LR 0.000059 Loss 6.232532, Accuracy 82.355%\n",
      "Epoch 18, Batch 847, LR 0.000059 Loss 6.232624, Accuracy 82.357%\n",
      "Epoch 18, Batch 848, LR 0.000059 Loss 6.233158, Accuracy 82.351%\n",
      "Epoch 18, Batch 849, LR 0.000059 Loss 6.232769, Accuracy 82.357%\n",
      "Epoch 18, Batch 850, LR 0.000059 Loss 6.232601, Accuracy 82.358%\n",
      "Epoch 18, Batch 851, LR 0.000059 Loss 6.232169, Accuracy 82.360%\n",
      "Epoch 18, Batch 852, LR 0.000059 Loss 6.231636, Accuracy 82.361%\n",
      "Epoch 18, Batch 853, LR 0.000059 Loss 6.232329, Accuracy 82.360%\n",
      "Epoch 18, Batch 854, LR 0.000059 Loss 6.233253, Accuracy 82.355%\n",
      "Epoch 18, Batch 855, LR 0.000059 Loss 6.233405, Accuracy 82.353%\n",
      "Epoch 18, Batch 856, LR 0.000059 Loss 6.232048, Accuracy 82.365%\n",
      "Epoch 18, Batch 857, LR 0.000059 Loss 6.231751, Accuracy 82.366%\n",
      "Epoch 18, Batch 858, LR 0.000059 Loss 6.231366, Accuracy 82.363%\n",
      "Epoch 18, Batch 859, LR 0.000059 Loss 6.231017, Accuracy 82.361%\n",
      "Epoch 18, Batch 860, LR 0.000059 Loss 6.231051, Accuracy 82.357%\n",
      "Epoch 18, Batch 861, LR 0.000059 Loss 6.230387, Accuracy 82.363%\n",
      "Epoch 18, Batch 862, LR 0.000059 Loss 6.230657, Accuracy 82.367%\n",
      "Epoch 18, Batch 863, LR 0.000059 Loss 6.230559, Accuracy 82.369%\n",
      "Epoch 18, Batch 864, LR 0.000059 Loss 6.230933, Accuracy 82.364%\n",
      "Epoch 18, Batch 865, LR 0.000059 Loss 6.230741, Accuracy 82.370%\n",
      "Epoch 18, Batch 866, LR 0.000059 Loss 6.231654, Accuracy 82.365%\n",
      "Epoch 18, Batch 867, LR 0.000059 Loss 6.231683, Accuracy 82.366%\n",
      "Epoch 18, Batch 868, LR 0.000059 Loss 6.231004, Accuracy 82.371%\n",
      "Epoch 18, Batch 869, LR 0.000059 Loss 6.230669, Accuracy 82.371%\n",
      "Epoch 18, Batch 870, LR 0.000059 Loss 6.230609, Accuracy 82.372%\n",
      "Epoch 18, Batch 871, LR 0.000059 Loss 6.230584, Accuracy 82.369%\n",
      "Epoch 18, Batch 872, LR 0.000059 Loss 6.229888, Accuracy 82.372%\n",
      "Epoch 18, Batch 873, LR 0.000059 Loss 6.230431, Accuracy 82.372%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 874, LR 0.000059 Loss 6.230809, Accuracy 82.366%\n",
      "Epoch 18, Batch 875, LR 0.000059 Loss 6.230502, Accuracy 82.367%\n",
      "Epoch 18, Batch 876, LR 0.000059 Loss 6.230094, Accuracy 82.374%\n",
      "Epoch 18, Batch 877, LR 0.000059 Loss 6.230146, Accuracy 82.376%\n",
      "Epoch 18, Batch 878, LR 0.000059 Loss 6.229554, Accuracy 82.381%\n",
      "Epoch 18, Batch 879, LR 0.000059 Loss 6.230016, Accuracy 82.373%\n",
      "Epoch 18, Batch 880, LR 0.000059 Loss 6.230283, Accuracy 82.377%\n",
      "Epoch 18, Batch 881, LR 0.000059 Loss 6.230353, Accuracy 82.378%\n",
      "Epoch 18, Batch 882, LR 0.000059 Loss 6.230571, Accuracy 82.376%\n",
      "Epoch 18, Batch 883, LR 0.000059 Loss 6.230140, Accuracy 82.380%\n",
      "Epoch 18, Batch 884, LR 0.000059 Loss 6.229166, Accuracy 82.380%\n",
      "Epoch 18, Batch 885, LR 0.000059 Loss 6.229056, Accuracy 82.382%\n",
      "Epoch 18, Batch 886, LR 0.000059 Loss 6.229622, Accuracy 82.380%\n",
      "Epoch 18, Batch 887, LR 0.000059 Loss 6.229566, Accuracy 82.381%\n",
      "Epoch 18, Batch 888, LR 0.000059 Loss 6.229848, Accuracy 82.384%\n",
      "Epoch 18, Batch 889, LR 0.000059 Loss 6.230349, Accuracy 82.382%\n",
      "Epoch 18, Batch 890, LR 0.000059 Loss 6.229429, Accuracy 82.387%\n",
      "Epoch 18, Batch 891, LR 0.000059 Loss 6.229094, Accuracy 82.387%\n",
      "Epoch 18, Batch 892, LR 0.000059 Loss 6.229142, Accuracy 82.390%\n",
      "Epoch 18, Batch 893, LR 0.000059 Loss 6.228537, Accuracy 82.391%\n",
      "Epoch 18, Batch 894, LR 0.000059 Loss 6.228609, Accuracy 82.389%\n",
      "Epoch 18, Batch 895, LR 0.000059 Loss 6.229134, Accuracy 82.381%\n",
      "Epoch 18, Batch 896, LR 0.000059 Loss 6.229306, Accuracy 82.383%\n",
      "Epoch 18, Batch 897, LR 0.000059 Loss 6.229063, Accuracy 82.381%\n",
      "Epoch 18, Batch 898, LR 0.000059 Loss 6.228100, Accuracy 82.388%\n",
      "Epoch 18, Batch 899, LR 0.000059 Loss 6.228754, Accuracy 82.386%\n",
      "Epoch 18, Batch 900, LR 0.000059 Loss 6.228895, Accuracy 82.380%\n",
      "Epoch 18, Batch 901, LR 0.000059 Loss 6.228285, Accuracy 82.383%\n",
      "Epoch 18, Batch 902, LR 0.000059 Loss 6.228536, Accuracy 82.378%\n",
      "Epoch 18, Batch 903, LR 0.000059 Loss 6.228597, Accuracy 82.379%\n",
      "Epoch 18, Batch 904, LR 0.000059 Loss 6.228433, Accuracy 82.386%\n",
      "Epoch 18, Batch 905, LR 0.000059 Loss 6.228887, Accuracy 82.382%\n",
      "Epoch 18, Batch 906, LR 0.000059 Loss 6.228492, Accuracy 82.385%\n",
      "Epoch 18, Batch 907, LR 0.000059 Loss 6.229134, Accuracy 82.382%\n",
      "Epoch 18, Batch 908, LR 0.000059 Loss 6.229065, Accuracy 82.377%\n",
      "Epoch 18, Batch 909, LR 0.000059 Loss 6.228672, Accuracy 82.379%\n",
      "Epoch 18, Batch 910, LR 0.000059 Loss 6.228295, Accuracy 82.379%\n",
      "Epoch 18, Batch 911, LR 0.000059 Loss 6.227710, Accuracy 82.382%\n",
      "Epoch 18, Batch 912, LR 0.000059 Loss 6.228159, Accuracy 82.382%\n",
      "Epoch 18, Batch 913, LR 0.000059 Loss 6.227476, Accuracy 82.390%\n",
      "Epoch 18, Batch 914, LR 0.000059 Loss 6.226539, Accuracy 82.393%\n",
      "Epoch 18, Batch 915, LR 0.000059 Loss 6.225083, Accuracy 82.401%\n",
      "Epoch 18, Batch 916, LR 0.000059 Loss 6.226208, Accuracy 82.395%\n",
      "Epoch 18, Batch 917, LR 0.000059 Loss 6.225684, Accuracy 82.398%\n",
      "Epoch 18, Batch 918, LR 0.000059 Loss 6.225232, Accuracy 82.401%\n",
      "Epoch 18, Batch 919, LR 0.000059 Loss 6.224857, Accuracy 82.403%\n",
      "Epoch 18, Batch 920, LR 0.000059 Loss 6.224584, Accuracy 82.406%\n",
      "Epoch 18, Batch 921, LR 0.000059 Loss 6.225441, Accuracy 82.399%\n",
      "Epoch 18, Batch 922, LR 0.000059 Loss 6.225185, Accuracy 82.397%\n",
      "Epoch 18, Batch 923, LR 0.000059 Loss 6.225277, Accuracy 82.399%\n",
      "Epoch 18, Batch 924, LR 0.000059 Loss 6.225095, Accuracy 82.397%\n",
      "Epoch 18, Batch 925, LR 0.000059 Loss 6.224891, Accuracy 82.399%\n",
      "Epoch 18, Batch 926, LR 0.000059 Loss 6.224918, Accuracy 82.398%\n",
      "Epoch 18, Batch 927, LR 0.000059 Loss 6.224339, Accuracy 82.400%\n",
      "Epoch 18, Batch 928, LR 0.000059 Loss 6.223961, Accuracy 82.397%\n",
      "Epoch 18, Batch 929, LR 0.000059 Loss 6.223986, Accuracy 82.395%\n",
      "Epoch 18, Batch 930, LR 0.000059 Loss 6.223665, Accuracy 82.397%\n",
      "Epoch 18, Batch 931, LR 0.000059 Loss 6.223738, Accuracy 82.398%\n",
      "Epoch 18, Batch 932, LR 0.000059 Loss 6.223355, Accuracy 82.400%\n",
      "Epoch 18, Batch 933, LR 0.000059 Loss 6.221974, Accuracy 82.411%\n",
      "Epoch 18, Batch 934, LR 0.000059 Loss 6.221240, Accuracy 82.411%\n",
      "Epoch 18, Batch 935, LR 0.000059 Loss 6.221331, Accuracy 82.411%\n",
      "Epoch 18, Batch 936, LR 0.000059 Loss 6.220975, Accuracy 82.415%\n",
      "Epoch 18, Batch 937, LR 0.000059 Loss 6.220643, Accuracy 82.416%\n",
      "Epoch 18, Batch 938, LR 0.000059 Loss 6.220013, Accuracy 82.420%\n",
      "Epoch 18, Batch 939, LR 0.000059 Loss 6.219895, Accuracy 82.421%\n",
      "Epoch 18, Batch 940, LR 0.000059 Loss 6.219908, Accuracy 82.420%\n",
      "Epoch 18, Batch 941, LR 0.000059 Loss 6.219311, Accuracy 82.423%\n",
      "Epoch 18, Batch 942, LR 0.000059 Loss 6.219645, Accuracy 82.421%\n",
      "Epoch 18, Batch 943, LR 0.000059 Loss 6.218555, Accuracy 82.425%\n",
      "Epoch 18, Batch 944, LR 0.000059 Loss 6.217629, Accuracy 82.433%\n",
      "Epoch 18, Batch 945, LR 0.000059 Loss 6.217652, Accuracy 82.435%\n",
      "Epoch 18, Batch 946, LR 0.000059 Loss 6.217762, Accuracy 82.432%\n",
      "Epoch 18, Batch 947, LR 0.000059 Loss 6.217651, Accuracy 82.428%\n",
      "Epoch 18, Batch 948, LR 0.000059 Loss 6.218134, Accuracy 82.424%\n",
      "Epoch 18, Batch 949, LR 0.000059 Loss 6.218492, Accuracy 82.424%\n",
      "Epoch 18, Batch 950, LR 0.000059 Loss 6.218018, Accuracy 82.424%\n",
      "Epoch 18, Batch 951, LR 0.000059 Loss 6.218200, Accuracy 82.420%\n",
      "Epoch 18, Batch 952, LR 0.000059 Loss 6.217944, Accuracy 82.423%\n",
      "Epoch 18, Batch 953, LR 0.000059 Loss 6.216852, Accuracy 82.428%\n",
      "Epoch 18, Batch 954, LR 0.000059 Loss 6.216558, Accuracy 82.429%\n",
      "Epoch 18, Batch 955, LR 0.000059 Loss 6.215636, Accuracy 82.430%\n",
      "Epoch 18, Batch 956, LR 0.000059 Loss 6.215267, Accuracy 82.435%\n",
      "Epoch 18, Batch 957, LR 0.000059 Loss 6.215419, Accuracy 82.434%\n",
      "Epoch 18, Batch 958, LR 0.000059 Loss 6.215395, Accuracy 82.432%\n",
      "Epoch 18, Batch 959, LR 0.000059 Loss 6.215153, Accuracy 82.431%\n",
      "Epoch 18, Batch 960, LR 0.000059 Loss 6.215147, Accuracy 82.429%\n",
      "Epoch 18, Batch 961, LR 0.000059 Loss 6.214755, Accuracy 82.430%\n",
      "Epoch 18, Batch 962, LR 0.000059 Loss 6.214815, Accuracy 82.430%\n",
      "Epoch 18, Batch 963, LR 0.000059 Loss 6.216153, Accuracy 82.422%\n",
      "Epoch 18, Batch 964, LR 0.000059 Loss 6.216455, Accuracy 82.421%\n",
      "Epoch 18, Batch 965, LR 0.000059 Loss 6.217193, Accuracy 82.416%\n",
      "Epoch 18, Batch 966, LR 0.000059 Loss 6.216081, Accuracy 82.420%\n",
      "Epoch 18, Batch 967, LR 0.000059 Loss 6.216293, Accuracy 82.421%\n",
      "Epoch 18, Batch 968, LR 0.000059 Loss 6.216601, Accuracy 82.414%\n",
      "Epoch 18, Batch 969, LR 0.000059 Loss 6.217132, Accuracy 82.414%\n",
      "Epoch 18, Batch 970, LR 0.000059 Loss 6.217140, Accuracy 82.419%\n",
      "Epoch 18, Batch 971, LR 0.000059 Loss 6.216075, Accuracy 82.423%\n",
      "Epoch 18, Batch 972, LR 0.000059 Loss 6.216573, Accuracy 82.421%\n",
      "Epoch 18, Batch 973, LR 0.000059 Loss 6.216800, Accuracy 82.418%\n",
      "Epoch 18, Batch 974, LR 0.000059 Loss 6.217217, Accuracy 82.416%\n",
      "Epoch 18, Batch 975, LR 0.000059 Loss 6.215880, Accuracy 82.422%\n",
      "Epoch 18, Batch 976, LR 0.000059 Loss 6.215887, Accuracy 82.422%\n",
      "Epoch 18, Batch 977, LR 0.000059 Loss 6.215740, Accuracy 82.423%\n",
      "Epoch 18, Batch 978, LR 0.000058 Loss 6.215949, Accuracy 82.424%\n",
      "Epoch 18, Batch 979, LR 0.000058 Loss 6.215851, Accuracy 82.427%\n",
      "Epoch 18, Batch 980, LR 0.000058 Loss 6.215780, Accuracy 82.427%\n",
      "Epoch 18, Batch 981, LR 0.000058 Loss 6.215761, Accuracy 82.429%\n",
      "Epoch 18, Batch 982, LR 0.000058 Loss 6.215510, Accuracy 82.430%\n",
      "Epoch 18, Batch 983, LR 0.000058 Loss 6.214691, Accuracy 82.438%\n",
      "Epoch 18, Batch 984, LR 0.000058 Loss 6.214215, Accuracy 82.441%\n",
      "Epoch 18, Batch 985, LR 0.000058 Loss 6.215231, Accuracy 82.435%\n",
      "Epoch 18, Batch 986, LR 0.000058 Loss 6.215093, Accuracy 82.434%\n",
      "Epoch 18, Batch 987, LR 0.000058 Loss 6.214480, Accuracy 82.438%\n",
      "Epoch 18, Batch 988, LR 0.000058 Loss 6.213438, Accuracy 82.446%\n",
      "Epoch 18, Batch 989, LR 0.000058 Loss 6.213602, Accuracy 82.445%\n",
      "Epoch 18, Batch 990, LR 0.000058 Loss 6.213648, Accuracy 82.446%\n",
      "Epoch 18, Batch 991, LR 0.000058 Loss 6.214275, Accuracy 82.439%\n",
      "Epoch 18, Batch 992, LR 0.000058 Loss 6.214872, Accuracy 82.440%\n",
      "Epoch 18, Batch 993, LR 0.000058 Loss 6.215199, Accuracy 82.433%\n",
      "Epoch 18, Batch 994, LR 0.000058 Loss 6.216052, Accuracy 82.430%\n",
      "Epoch 18, Batch 995, LR 0.000058 Loss 6.216452, Accuracy 82.425%\n",
      "Epoch 18, Batch 996, LR 0.000058 Loss 6.216576, Accuracy 82.425%\n",
      "Epoch 18, Batch 997, LR 0.000058 Loss 6.216359, Accuracy 82.425%\n",
      "Epoch 18, Batch 998, LR 0.000058 Loss 6.216398, Accuracy 82.420%\n",
      "Epoch 18, Batch 999, LR 0.000058 Loss 6.217067, Accuracy 82.414%\n",
      "Epoch 18, Batch 1000, LR 0.000058 Loss 6.216797, Accuracy 82.414%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 1001, LR 0.000058 Loss 6.216072, Accuracy 82.419%\n",
      "Epoch 18, Batch 1002, LR 0.000058 Loss 6.215699, Accuracy 82.425%\n",
      "Epoch 18, Batch 1003, LR 0.000058 Loss 6.215084, Accuracy 82.423%\n",
      "Epoch 18, Batch 1004, LR 0.000058 Loss 6.214982, Accuracy 82.423%\n",
      "Epoch 18, Batch 1005, LR 0.000058 Loss 6.214221, Accuracy 82.425%\n",
      "Epoch 18, Batch 1006, LR 0.000058 Loss 6.214065, Accuracy 82.429%\n",
      "Epoch 18, Batch 1007, LR 0.000058 Loss 6.213692, Accuracy 82.429%\n",
      "Epoch 18, Batch 1008, LR 0.000058 Loss 6.213793, Accuracy 82.430%\n",
      "Epoch 18, Batch 1009, LR 0.000058 Loss 6.213558, Accuracy 82.431%\n",
      "Epoch 18, Batch 1010, LR 0.000058 Loss 6.214519, Accuracy 82.425%\n",
      "Epoch 18, Batch 1011, LR 0.000058 Loss 6.215283, Accuracy 82.424%\n",
      "Epoch 18, Batch 1012, LR 0.000058 Loss 6.216060, Accuracy 82.424%\n",
      "Epoch 18, Batch 1013, LR 0.000058 Loss 6.215493, Accuracy 82.424%\n",
      "Epoch 18, Batch 1014, LR 0.000058 Loss 6.215027, Accuracy 82.425%\n",
      "Epoch 18, Batch 1015, LR 0.000058 Loss 6.215248, Accuracy 82.426%\n",
      "Epoch 18, Batch 1016, LR 0.000058 Loss 6.215303, Accuracy 82.426%\n",
      "Epoch 18, Batch 1017, LR 0.000058 Loss 6.214913, Accuracy 82.428%\n",
      "Epoch 18, Batch 1018, LR 0.000058 Loss 6.214878, Accuracy 82.432%\n",
      "Epoch 18, Batch 1019, LR 0.000058 Loss 6.215668, Accuracy 82.421%\n",
      "Epoch 18, Batch 1020, LR 0.000058 Loss 6.216134, Accuracy 82.420%\n",
      "Epoch 18, Batch 1021, LR 0.000058 Loss 6.215693, Accuracy 82.421%\n",
      "Epoch 18, Batch 1022, LR 0.000058 Loss 6.215922, Accuracy 82.420%\n",
      "Epoch 18, Batch 1023, LR 0.000058 Loss 6.215977, Accuracy 82.415%\n",
      "Epoch 18, Batch 1024, LR 0.000058 Loss 6.216236, Accuracy 82.410%\n",
      "Epoch 18, Batch 1025, LR 0.000058 Loss 6.215647, Accuracy 82.413%\n",
      "Epoch 18, Batch 1026, LR 0.000058 Loss 6.215529, Accuracy 82.415%\n",
      "Epoch 18, Batch 1027, LR 0.000058 Loss 6.215526, Accuracy 82.415%\n",
      "Epoch 18, Batch 1028, LR 0.000058 Loss 6.215583, Accuracy 82.411%\n",
      "Epoch 18, Batch 1029, LR 0.000058 Loss 6.214893, Accuracy 82.415%\n",
      "Epoch 18, Batch 1030, LR 0.000058 Loss 6.214293, Accuracy 82.420%\n",
      "Epoch 18, Batch 1031, LR 0.000058 Loss 6.213687, Accuracy 82.422%\n",
      "Epoch 18, Batch 1032, LR 0.000058 Loss 6.213691, Accuracy 82.425%\n",
      "Epoch 18, Batch 1033, LR 0.000058 Loss 6.213355, Accuracy 82.424%\n",
      "Epoch 18, Batch 1034, LR 0.000058 Loss 6.213071, Accuracy 82.428%\n",
      "Epoch 18, Batch 1035, LR 0.000058 Loss 6.212713, Accuracy 82.428%\n",
      "Epoch 18, Batch 1036, LR 0.000058 Loss 6.212505, Accuracy 82.430%\n",
      "Epoch 18, Batch 1037, LR 0.000058 Loss 6.211686, Accuracy 82.431%\n",
      "Epoch 18, Batch 1038, LR 0.000058 Loss 6.211530, Accuracy 82.432%\n",
      "Epoch 18, Batch 1039, LR 0.000058 Loss 6.211683, Accuracy 82.432%\n",
      "Epoch 18, Batch 1040, LR 0.000058 Loss 6.212044, Accuracy 82.429%\n",
      "Epoch 18, Batch 1041, LR 0.000058 Loss 6.211651, Accuracy 82.428%\n",
      "Epoch 18, Batch 1042, LR 0.000058 Loss 6.212305, Accuracy 82.420%\n",
      "Epoch 18, Batch 1043, LR 0.000058 Loss 6.212140, Accuracy 82.418%\n",
      "Epoch 18, Batch 1044, LR 0.000058 Loss 6.212217, Accuracy 82.418%\n",
      "Epoch 18, Batch 1045, LR 0.000058 Loss 6.212488, Accuracy 82.419%\n",
      "Epoch 18, Batch 1046, LR 0.000058 Loss 6.210989, Accuracy 82.424%\n",
      "Epoch 18, Batch 1047, LR 0.000058 Loss 6.211423, Accuracy 82.424%\n",
      "Epoch 18, Loss (train set) 6.211423, Accuracy (train set) 82.424%\n",
      "Epoch 19, Batch 1, LR 0.000058 Loss 6.326054, Accuracy 79.688%\n",
      "Epoch 19, Batch 2, LR 0.000058 Loss 6.368804, Accuracy 81.250%\n",
      "Epoch 19, Batch 3, LR 0.000058 Loss 6.271679, Accuracy 81.771%\n",
      "Epoch 19, Batch 4, LR 0.000058 Loss 6.205859, Accuracy 82.812%\n",
      "Epoch 19, Batch 5, LR 0.000058 Loss 6.229699, Accuracy 82.188%\n",
      "Epoch 19, Batch 6, LR 0.000058 Loss 6.200513, Accuracy 81.771%\n",
      "Epoch 19, Batch 7, LR 0.000058 Loss 6.222554, Accuracy 81.585%\n",
      "Epoch 19, Batch 8, LR 0.000058 Loss 6.183739, Accuracy 81.836%\n",
      "Epoch 19, Batch 9, LR 0.000058 Loss 6.178389, Accuracy 82.205%\n",
      "Epoch 19, Batch 10, LR 0.000058 Loss 6.148724, Accuracy 82.578%\n",
      "Epoch 19, Batch 11, LR 0.000058 Loss 6.148224, Accuracy 83.168%\n",
      "Epoch 19, Batch 12, LR 0.000058 Loss 6.159408, Accuracy 83.529%\n",
      "Epoch 19, Batch 13, LR 0.000058 Loss 6.104929, Accuracy 83.834%\n",
      "Epoch 19, Batch 14, LR 0.000058 Loss 6.149453, Accuracy 83.315%\n",
      "Epoch 19, Batch 15, LR 0.000058 Loss 6.188997, Accuracy 83.073%\n",
      "Epoch 19, Batch 16, LR 0.000058 Loss 6.184876, Accuracy 82.910%\n",
      "Epoch 19, Batch 17, LR 0.000058 Loss 6.163815, Accuracy 82.904%\n",
      "Epoch 19, Batch 18, LR 0.000058 Loss 6.170965, Accuracy 82.856%\n",
      "Epoch 19, Batch 19, LR 0.000058 Loss 6.190563, Accuracy 82.525%\n",
      "Epoch 19, Batch 20, LR 0.000058 Loss 6.219952, Accuracy 82.266%\n",
      "Epoch 19, Batch 21, LR 0.000058 Loss 6.209852, Accuracy 82.329%\n",
      "Epoch 19, Batch 22, LR 0.000058 Loss 6.160519, Accuracy 82.599%\n",
      "Epoch 19, Batch 23, LR 0.000058 Loss 6.151813, Accuracy 82.575%\n",
      "Epoch 19, Batch 24, LR 0.000058 Loss 6.134547, Accuracy 82.617%\n",
      "Epoch 19, Batch 25, LR 0.000058 Loss 6.121279, Accuracy 82.781%\n",
      "Epoch 19, Batch 26, LR 0.000058 Loss 6.097554, Accuracy 82.903%\n",
      "Epoch 19, Batch 27, LR 0.000058 Loss 6.099178, Accuracy 82.928%\n",
      "Epoch 19, Batch 28, LR 0.000058 Loss 6.115026, Accuracy 82.757%\n",
      "Epoch 19, Batch 29, LR 0.000058 Loss 6.118581, Accuracy 82.543%\n",
      "Epoch 19, Batch 30, LR 0.000058 Loss 6.131482, Accuracy 82.422%\n",
      "Epoch 19, Batch 31, LR 0.000058 Loss 6.118494, Accuracy 82.560%\n",
      "Epoch 19, Batch 32, LR 0.000058 Loss 6.141408, Accuracy 82.471%\n",
      "Epoch 19, Batch 33, LR 0.000058 Loss 6.135034, Accuracy 82.505%\n",
      "Epoch 19, Batch 34, LR 0.000058 Loss 6.142458, Accuracy 82.537%\n",
      "Epoch 19, Batch 35, LR 0.000058 Loss 6.143563, Accuracy 82.522%\n",
      "Epoch 19, Batch 36, LR 0.000058 Loss 6.155425, Accuracy 82.444%\n",
      "Epoch 19, Batch 37, LR 0.000058 Loss 6.165991, Accuracy 82.432%\n",
      "Epoch 19, Batch 38, LR 0.000058 Loss 6.176980, Accuracy 82.442%\n",
      "Epoch 19, Batch 39, LR 0.000058 Loss 6.162962, Accuracy 82.512%\n",
      "Epoch 19, Batch 40, LR 0.000058 Loss 6.165602, Accuracy 82.559%\n",
      "Epoch 19, Batch 41, LR 0.000058 Loss 6.193399, Accuracy 82.336%\n",
      "Epoch 19, Batch 42, LR 0.000058 Loss 6.191441, Accuracy 82.273%\n",
      "Epoch 19, Batch 43, LR 0.000058 Loss 6.193025, Accuracy 82.267%\n",
      "Epoch 19, Batch 44, LR 0.000058 Loss 6.182278, Accuracy 82.262%\n",
      "Epoch 19, Batch 45, LR 0.000058 Loss 6.189956, Accuracy 82.170%\n",
      "Epoch 19, Batch 46, LR 0.000058 Loss 6.181659, Accuracy 82.269%\n",
      "Epoch 19, Batch 47, LR 0.000058 Loss 6.179996, Accuracy 82.231%\n",
      "Epoch 19, Batch 48, LR 0.000058 Loss 6.175014, Accuracy 82.259%\n",
      "Epoch 19, Batch 49, LR 0.000058 Loss 6.163944, Accuracy 82.366%\n",
      "Epoch 19, Batch 50, LR 0.000058 Loss 6.169244, Accuracy 82.266%\n",
      "Epoch 19, Batch 51, LR 0.000058 Loss 6.169749, Accuracy 82.322%\n",
      "Epoch 19, Batch 52, LR 0.000058 Loss 6.156740, Accuracy 82.437%\n",
      "Epoch 19, Batch 53, LR 0.000058 Loss 6.163296, Accuracy 82.459%\n",
      "Epoch 19, Batch 54, LR 0.000058 Loss 6.157720, Accuracy 82.465%\n",
      "Epoch 19, Batch 55, LR 0.000058 Loss 6.158314, Accuracy 82.500%\n",
      "Epoch 19, Batch 56, LR 0.000058 Loss 6.149270, Accuracy 82.547%\n",
      "Epoch 19, Batch 57, LR 0.000058 Loss 6.134061, Accuracy 82.648%\n",
      "Epoch 19, Batch 58, LR 0.000058 Loss 6.136930, Accuracy 82.516%\n",
      "Epoch 19, Batch 59, LR 0.000058 Loss 6.122842, Accuracy 82.534%\n",
      "Epoch 19, Batch 60, LR 0.000058 Loss 6.130431, Accuracy 82.422%\n",
      "Epoch 19, Batch 61, LR 0.000058 Loss 6.126779, Accuracy 82.454%\n",
      "Epoch 19, Batch 62, LR 0.000058 Loss 6.123393, Accuracy 82.510%\n",
      "Epoch 19, Batch 63, LR 0.000058 Loss 6.119529, Accuracy 82.552%\n",
      "Epoch 19, Batch 64, LR 0.000058 Loss 6.125092, Accuracy 82.483%\n",
      "Epoch 19, Batch 65, LR 0.000058 Loss 6.118811, Accuracy 82.536%\n",
      "Epoch 19, Batch 66, LR 0.000058 Loss 6.115808, Accuracy 82.588%\n",
      "Epoch 19, Batch 67, LR 0.000058 Loss 6.116714, Accuracy 82.579%\n",
      "Epoch 19, Batch 68, LR 0.000058 Loss 6.121591, Accuracy 82.583%\n",
      "Epoch 19, Batch 69, LR 0.000058 Loss 6.105242, Accuracy 82.665%\n",
      "Epoch 19, Batch 70, LR 0.000058 Loss 6.084775, Accuracy 82.790%\n",
      "Epoch 19, Batch 71, LR 0.000058 Loss 6.090704, Accuracy 82.801%\n",
      "Epoch 19, Batch 72, LR 0.000058 Loss 6.081511, Accuracy 82.899%\n",
      "Epoch 19, Batch 73, LR 0.000058 Loss 6.077871, Accuracy 82.909%\n",
      "Epoch 19, Batch 74, LR 0.000058 Loss 6.091829, Accuracy 82.823%\n",
      "Epoch 19, Batch 75, LR 0.000058 Loss 6.088841, Accuracy 82.844%\n",
      "Epoch 19, Batch 76, LR 0.000058 Loss 6.092237, Accuracy 82.812%\n",
      "Epoch 19, Batch 77, LR 0.000058 Loss 6.094153, Accuracy 82.792%\n",
      "Epoch 19, Batch 78, LR 0.000058 Loss 6.097393, Accuracy 82.782%\n",
      "Epoch 19, Batch 79, LR 0.000058 Loss 6.088379, Accuracy 82.812%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 80, LR 0.000058 Loss 6.077682, Accuracy 82.861%\n",
      "Epoch 19, Batch 81, LR 0.000058 Loss 6.074534, Accuracy 82.890%\n",
      "Epoch 19, Batch 82, LR 0.000058 Loss 6.076821, Accuracy 82.860%\n",
      "Epoch 19, Batch 83, LR 0.000058 Loss 6.071872, Accuracy 82.897%\n",
      "Epoch 19, Batch 84, LR 0.000058 Loss 6.083419, Accuracy 82.859%\n",
      "Epoch 19, Batch 85, LR 0.000058 Loss 6.080295, Accuracy 82.868%\n",
      "Epoch 19, Batch 86, LR 0.000058 Loss 6.090886, Accuracy 82.785%\n",
      "Epoch 19, Batch 87, LR 0.000058 Loss 6.088053, Accuracy 82.777%\n",
      "Epoch 19, Batch 88, LR 0.000058 Loss 6.089564, Accuracy 82.733%\n",
      "Epoch 19, Batch 89, LR 0.000058 Loss 6.093927, Accuracy 82.716%\n",
      "Epoch 19, Batch 90, LR 0.000058 Loss 6.092484, Accuracy 82.769%\n",
      "Epoch 19, Batch 91, LR 0.000058 Loss 6.093954, Accuracy 82.761%\n",
      "Epoch 19, Batch 92, LR 0.000058 Loss 6.083683, Accuracy 82.812%\n",
      "Epoch 19, Batch 93, LR 0.000058 Loss 6.075062, Accuracy 82.855%\n",
      "Epoch 19, Batch 94, LR 0.000058 Loss 6.074004, Accuracy 82.821%\n",
      "Epoch 19, Batch 95, LR 0.000058 Loss 6.078911, Accuracy 82.788%\n",
      "Epoch 19, Batch 96, LR 0.000058 Loss 6.081295, Accuracy 82.804%\n",
      "Epoch 19, Batch 97, LR 0.000058 Loss 6.081397, Accuracy 82.821%\n",
      "Epoch 19, Batch 98, LR 0.000058 Loss 6.087096, Accuracy 82.773%\n",
      "Epoch 19, Batch 99, LR 0.000058 Loss 6.085737, Accuracy 82.789%\n",
      "Epoch 19, Batch 100, LR 0.000058 Loss 6.085564, Accuracy 82.766%\n",
      "Epoch 19, Batch 101, LR 0.000058 Loss 6.085907, Accuracy 82.774%\n",
      "Epoch 19, Batch 102, LR 0.000058 Loss 6.082951, Accuracy 82.812%\n",
      "Epoch 19, Batch 103, LR 0.000058 Loss 6.085264, Accuracy 82.797%\n",
      "Epoch 19, Batch 104, LR 0.000058 Loss 6.087882, Accuracy 82.790%\n",
      "Epoch 19, Batch 105, LR 0.000058 Loss 6.086355, Accuracy 82.790%\n",
      "Epoch 19, Batch 106, LR 0.000058 Loss 6.085161, Accuracy 82.768%\n",
      "Epoch 19, Batch 107, LR 0.000058 Loss 6.087357, Accuracy 82.769%\n",
      "Epoch 19, Batch 108, LR 0.000058 Loss 6.089818, Accuracy 82.733%\n",
      "Epoch 19, Batch 109, LR 0.000058 Loss 6.087084, Accuracy 82.726%\n",
      "Epoch 19, Batch 110, LR 0.000058 Loss 6.084177, Accuracy 82.749%\n",
      "Epoch 19, Batch 111, LR 0.000058 Loss 6.082507, Accuracy 82.749%\n",
      "Epoch 19, Batch 112, LR 0.000058 Loss 6.077743, Accuracy 82.743%\n",
      "Epoch 19, Batch 113, LR 0.000058 Loss 6.078354, Accuracy 82.764%\n",
      "Epoch 19, Batch 114, LR 0.000058 Loss 6.083875, Accuracy 82.737%\n",
      "Epoch 19, Batch 115, LR 0.000058 Loss 6.078282, Accuracy 82.785%\n",
      "Epoch 19, Batch 116, LR 0.000058 Loss 6.071371, Accuracy 82.839%\n",
      "Epoch 19, Batch 117, LR 0.000058 Loss 6.074339, Accuracy 82.853%\n",
      "Epoch 19, Batch 118, LR 0.000058 Loss 6.073188, Accuracy 82.859%\n",
      "Epoch 19, Batch 119, LR 0.000058 Loss 6.080034, Accuracy 82.832%\n",
      "Epoch 19, Batch 120, LR 0.000058 Loss 6.075127, Accuracy 82.865%\n",
      "Epoch 19, Batch 121, LR 0.000058 Loss 6.079308, Accuracy 82.812%\n",
      "Epoch 19, Batch 122, LR 0.000058 Loss 6.078648, Accuracy 82.819%\n",
      "Epoch 19, Batch 123, LR 0.000058 Loss 6.082220, Accuracy 82.806%\n",
      "Epoch 19, Batch 124, LR 0.000058 Loss 6.077871, Accuracy 82.812%\n",
      "Epoch 19, Batch 125, LR 0.000058 Loss 6.081498, Accuracy 82.763%\n",
      "Epoch 19, Batch 126, LR 0.000058 Loss 6.080935, Accuracy 82.781%\n",
      "Epoch 19, Batch 127, LR 0.000058 Loss 6.077597, Accuracy 82.812%\n",
      "Epoch 19, Batch 128, LR 0.000058 Loss 6.075689, Accuracy 82.825%\n",
      "Epoch 19, Batch 129, LR 0.000058 Loss 6.079318, Accuracy 82.825%\n",
      "Epoch 19, Batch 130, LR 0.000058 Loss 6.078913, Accuracy 82.843%\n",
      "Epoch 19, Batch 131, LR 0.000058 Loss 6.079590, Accuracy 82.848%\n",
      "Epoch 19, Batch 132, LR 0.000058 Loss 6.077064, Accuracy 82.848%\n",
      "Epoch 19, Batch 133, LR 0.000058 Loss 6.082771, Accuracy 82.818%\n",
      "Epoch 19, Batch 134, LR 0.000058 Loss 6.085150, Accuracy 82.812%\n",
      "Epoch 19, Batch 135, LR 0.000058 Loss 6.078647, Accuracy 82.853%\n",
      "Epoch 19, Batch 136, LR 0.000058 Loss 6.075241, Accuracy 82.881%\n",
      "Epoch 19, Batch 137, LR 0.000058 Loss 6.070720, Accuracy 82.921%\n",
      "Epoch 19, Batch 138, LR 0.000058 Loss 6.074872, Accuracy 82.914%\n",
      "Epoch 19, Batch 139, LR 0.000058 Loss 6.075454, Accuracy 82.908%\n",
      "Epoch 19, Batch 140, LR 0.000058 Loss 6.081101, Accuracy 82.874%\n",
      "Epoch 19, Batch 141, LR 0.000058 Loss 6.081720, Accuracy 82.851%\n",
      "Epoch 19, Batch 142, LR 0.000058 Loss 6.077026, Accuracy 82.879%\n",
      "Epoch 19, Batch 143, LR 0.000058 Loss 6.076929, Accuracy 82.873%\n",
      "Epoch 19, Batch 144, LR 0.000058 Loss 6.077667, Accuracy 82.867%\n",
      "Epoch 19, Batch 145, LR 0.000058 Loss 6.079466, Accuracy 82.872%\n",
      "Epoch 19, Batch 146, LR 0.000058 Loss 6.078387, Accuracy 82.903%\n",
      "Epoch 19, Batch 147, LR 0.000058 Loss 6.074692, Accuracy 82.935%\n",
      "Epoch 19, Batch 148, LR 0.000058 Loss 6.071040, Accuracy 82.939%\n",
      "Epoch 19, Batch 149, LR 0.000058 Loss 6.074405, Accuracy 82.928%\n",
      "Epoch 19, Batch 150, LR 0.000058 Loss 6.070648, Accuracy 82.953%\n",
      "Epoch 19, Batch 151, LR 0.000058 Loss 6.076420, Accuracy 82.911%\n",
      "Epoch 19, Batch 152, LR 0.000058 Loss 6.072226, Accuracy 82.946%\n",
      "Epoch 19, Batch 153, LR 0.000058 Loss 6.069014, Accuracy 82.966%\n",
      "Epoch 19, Batch 154, LR 0.000058 Loss 6.073821, Accuracy 82.970%\n",
      "Epoch 19, Batch 155, LR 0.000058 Loss 6.076992, Accuracy 82.949%\n",
      "Epoch 19, Batch 156, LR 0.000058 Loss 6.074200, Accuracy 82.948%\n",
      "Epoch 19, Batch 157, LR 0.000058 Loss 6.067768, Accuracy 83.002%\n",
      "Epoch 19, Batch 158, LR 0.000058 Loss 6.067214, Accuracy 82.995%\n",
      "Epoch 19, Batch 159, LR 0.000058 Loss 6.067539, Accuracy 82.994%\n",
      "Epoch 19, Batch 160, LR 0.000058 Loss 6.071774, Accuracy 82.974%\n",
      "Epoch 19, Batch 161, LR 0.000058 Loss 6.070608, Accuracy 82.982%\n",
      "Epoch 19, Batch 162, LR 0.000058 Loss 6.066864, Accuracy 83.005%\n",
      "Epoch 19, Batch 163, LR 0.000058 Loss 6.067934, Accuracy 82.990%\n",
      "Epoch 19, Batch 164, LR 0.000058 Loss 6.068439, Accuracy 82.984%\n",
      "Epoch 19, Batch 165, LR 0.000058 Loss 6.068773, Accuracy 82.964%\n",
      "Epoch 19, Batch 166, LR 0.000058 Loss 6.067442, Accuracy 82.973%\n",
      "Epoch 19, Batch 167, LR 0.000058 Loss 6.068307, Accuracy 82.972%\n",
      "Epoch 19, Batch 168, LR 0.000058 Loss 6.070019, Accuracy 82.980%\n",
      "Epoch 19, Batch 169, LR 0.000058 Loss 6.064074, Accuracy 83.011%\n",
      "Epoch 19, Batch 170, LR 0.000058 Loss 6.061633, Accuracy 83.015%\n",
      "Epoch 19, Batch 171, LR 0.000058 Loss 6.060214, Accuracy 83.018%\n",
      "Epoch 19, Batch 172, LR 0.000058 Loss 6.056377, Accuracy 83.040%\n",
      "Epoch 19, Batch 173, LR 0.000058 Loss 6.054613, Accuracy 83.047%\n",
      "Epoch 19, Batch 174, LR 0.000058 Loss 6.055018, Accuracy 83.046%\n",
      "Epoch 19, Batch 175, LR 0.000058 Loss 6.054029, Accuracy 83.058%\n",
      "Epoch 19, Batch 176, LR 0.000058 Loss 6.054598, Accuracy 83.048%\n",
      "Epoch 19, Batch 177, LR 0.000058 Loss 6.057053, Accuracy 83.029%\n",
      "Epoch 19, Batch 178, LR 0.000058 Loss 6.054268, Accuracy 83.036%\n",
      "Epoch 19, Batch 179, LR 0.000058 Loss 6.055647, Accuracy 83.031%\n",
      "Epoch 19, Batch 180, LR 0.000058 Loss 6.055582, Accuracy 83.025%\n",
      "Epoch 19, Batch 181, LR 0.000058 Loss 6.053351, Accuracy 83.033%\n",
      "Epoch 19, Batch 182, LR 0.000058 Loss 6.053729, Accuracy 83.031%\n",
      "Epoch 19, Batch 183, LR 0.000058 Loss 6.055844, Accuracy 83.047%\n",
      "Epoch 19, Batch 184, LR 0.000058 Loss 6.050975, Accuracy 83.055%\n",
      "Epoch 19, Batch 185, LR 0.000058 Loss 6.048606, Accuracy 83.057%\n",
      "Epoch 19, Batch 186, LR 0.000058 Loss 6.046682, Accuracy 83.073%\n",
      "Epoch 19, Batch 187, LR 0.000058 Loss 6.048001, Accuracy 83.076%\n",
      "Epoch 19, Batch 188, LR 0.000057 Loss 6.049156, Accuracy 83.070%\n",
      "Epoch 19, Batch 189, LR 0.000057 Loss 6.047661, Accuracy 83.044%\n",
      "Epoch 19, Batch 190, LR 0.000057 Loss 6.051924, Accuracy 83.022%\n",
      "Epoch 19, Batch 191, LR 0.000057 Loss 6.051155, Accuracy 83.029%\n",
      "Epoch 19, Batch 192, LR 0.000057 Loss 6.053041, Accuracy 83.024%\n",
      "Epoch 19, Batch 193, LR 0.000057 Loss 6.052315, Accuracy 83.007%\n",
      "Epoch 19, Batch 194, LR 0.000057 Loss 6.048178, Accuracy 83.030%\n",
      "Epoch 19, Batch 195, LR 0.000057 Loss 6.049854, Accuracy 83.025%\n",
      "Epoch 19, Batch 196, LR 0.000057 Loss 6.049056, Accuracy 83.016%\n",
      "Epoch 19, Batch 197, LR 0.000057 Loss 6.047132, Accuracy 83.027%\n",
      "Epoch 19, Batch 198, LR 0.000057 Loss 6.043906, Accuracy 83.045%\n",
      "Epoch 19, Batch 199, LR 0.000057 Loss 6.040393, Accuracy 83.060%\n",
      "Epoch 19, Batch 200, LR 0.000057 Loss 6.041702, Accuracy 83.047%\n",
      "Epoch 19, Batch 201, LR 0.000057 Loss 6.044419, Accuracy 83.019%\n",
      "Epoch 19, Batch 202, LR 0.000057 Loss 6.041329, Accuracy 83.041%\n",
      "Epoch 19, Batch 203, LR 0.000057 Loss 6.039504, Accuracy 83.059%\n",
      "Epoch 19, Batch 204, LR 0.000057 Loss 6.041589, Accuracy 83.065%\n",
      "Epoch 19, Batch 205, LR 0.000057 Loss 6.046118, Accuracy 83.045%\n",
      "Epoch 19, Batch 206, LR 0.000057 Loss 6.044554, Accuracy 83.051%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 207, LR 0.000057 Loss 6.044740, Accuracy 83.069%\n",
      "Epoch 19, Batch 208, LR 0.000057 Loss 6.045657, Accuracy 83.064%\n",
      "Epoch 19, Batch 209, LR 0.000057 Loss 6.042493, Accuracy 83.085%\n",
      "Epoch 19, Batch 210, LR 0.000057 Loss 6.043171, Accuracy 83.095%\n",
      "Epoch 19, Batch 211, LR 0.000057 Loss 6.042909, Accuracy 83.094%\n",
      "Epoch 19, Batch 212, LR 0.000057 Loss 6.043490, Accuracy 83.089%\n",
      "Epoch 19, Batch 213, LR 0.000057 Loss 6.042794, Accuracy 83.095%\n",
      "Epoch 19, Batch 214, LR 0.000057 Loss 6.043621, Accuracy 83.097%\n",
      "Epoch 19, Batch 215, LR 0.000057 Loss 6.045886, Accuracy 83.081%\n",
      "Epoch 19, Batch 216, LR 0.000057 Loss 6.043574, Accuracy 83.084%\n",
      "Epoch 19, Batch 217, LR 0.000057 Loss 6.046364, Accuracy 83.072%\n",
      "Epoch 19, Batch 218, LR 0.000057 Loss 6.048601, Accuracy 83.060%\n",
      "Epoch 19, Batch 219, LR 0.000057 Loss 6.046378, Accuracy 83.055%\n",
      "Epoch 19, Batch 220, LR 0.000057 Loss 6.048207, Accuracy 83.061%\n",
      "Epoch 19, Batch 221, LR 0.000057 Loss 6.049374, Accuracy 83.039%\n",
      "Epoch 19, Batch 222, LR 0.000057 Loss 6.051738, Accuracy 83.031%\n",
      "Epoch 19, Batch 223, LR 0.000057 Loss 6.050228, Accuracy 83.044%\n",
      "Epoch 19, Batch 224, LR 0.000057 Loss 6.051805, Accuracy 83.025%\n",
      "Epoch 19, Batch 225, LR 0.000057 Loss 6.053521, Accuracy 83.021%\n",
      "Epoch 19, Batch 226, LR 0.000057 Loss 6.059852, Accuracy 82.989%\n",
      "Epoch 19, Batch 227, LR 0.000057 Loss 6.054855, Accuracy 82.998%\n",
      "Epoch 19, Batch 228, LR 0.000057 Loss 6.058234, Accuracy 82.984%\n",
      "Epoch 19, Batch 229, LR 0.000057 Loss 6.054556, Accuracy 82.980%\n",
      "Epoch 19, Batch 230, LR 0.000057 Loss 6.053387, Accuracy 82.989%\n",
      "Epoch 19, Batch 231, LR 0.000057 Loss 6.053301, Accuracy 82.999%\n",
      "Epoch 19, Batch 232, LR 0.000057 Loss 6.050416, Accuracy 83.001%\n",
      "Epoch 19, Batch 233, LR 0.000057 Loss 6.050749, Accuracy 82.990%\n",
      "Epoch 19, Batch 234, LR 0.000057 Loss 6.046943, Accuracy 83.013%\n",
      "Epoch 19, Batch 235, LR 0.000057 Loss 6.048961, Accuracy 83.009%\n",
      "Epoch 19, Batch 236, LR 0.000057 Loss 6.048475, Accuracy 83.008%\n",
      "Epoch 19, Batch 237, LR 0.000057 Loss 6.047791, Accuracy 83.020%\n",
      "Epoch 19, Batch 238, LR 0.000057 Loss 6.049635, Accuracy 83.013%\n",
      "Epoch 19, Batch 239, LR 0.000057 Loss 6.053644, Accuracy 82.986%\n",
      "Epoch 19, Batch 240, LR 0.000057 Loss 6.052730, Accuracy 82.985%\n",
      "Epoch 19, Batch 241, LR 0.000057 Loss 6.051413, Accuracy 82.994%\n",
      "Epoch 19, Batch 242, LR 0.000057 Loss 6.053032, Accuracy 82.984%\n",
      "Epoch 19, Batch 243, LR 0.000057 Loss 6.054279, Accuracy 82.973%\n",
      "Epoch 19, Batch 244, LR 0.000057 Loss 6.055365, Accuracy 82.979%\n",
      "Epoch 19, Batch 245, LR 0.000057 Loss 6.055642, Accuracy 82.962%\n",
      "Epoch 19, Batch 246, LR 0.000057 Loss 6.056076, Accuracy 82.962%\n",
      "Epoch 19, Batch 247, LR 0.000057 Loss 6.057810, Accuracy 82.945%\n",
      "Epoch 19, Batch 248, LR 0.000057 Loss 6.057699, Accuracy 82.957%\n",
      "Epoch 19, Batch 249, LR 0.000057 Loss 6.057277, Accuracy 82.973%\n",
      "Epoch 19, Batch 250, LR 0.000057 Loss 6.055737, Accuracy 82.972%\n",
      "Epoch 19, Batch 251, LR 0.000057 Loss 6.056090, Accuracy 82.956%\n",
      "Epoch 19, Batch 252, LR 0.000057 Loss 6.057073, Accuracy 82.977%\n",
      "Epoch 19, Batch 253, LR 0.000057 Loss 6.056776, Accuracy 82.970%\n",
      "Epoch 19, Batch 254, LR 0.000057 Loss 6.058542, Accuracy 82.954%\n",
      "Epoch 19, Batch 255, LR 0.000057 Loss 6.056490, Accuracy 82.960%\n",
      "Epoch 19, Batch 256, LR 0.000057 Loss 6.058384, Accuracy 82.959%\n",
      "Epoch 19, Batch 257, LR 0.000057 Loss 6.057676, Accuracy 82.968%\n",
      "Epoch 19, Batch 258, LR 0.000057 Loss 6.054019, Accuracy 82.976%\n",
      "Epoch 19, Batch 259, LR 0.000057 Loss 6.051323, Accuracy 82.981%\n",
      "Epoch 19, Batch 260, LR 0.000057 Loss 6.051776, Accuracy 82.966%\n",
      "Epoch 19, Batch 261, LR 0.000057 Loss 6.053952, Accuracy 82.968%\n",
      "Epoch 19, Batch 262, LR 0.000057 Loss 6.052205, Accuracy 82.982%\n",
      "Epoch 19, Batch 263, LR 0.000057 Loss 6.053800, Accuracy 82.958%\n",
      "Epoch 19, Batch 264, LR 0.000057 Loss 6.056212, Accuracy 82.955%\n",
      "Epoch 19, Batch 265, LR 0.000057 Loss 6.058287, Accuracy 82.942%\n",
      "Epoch 19, Batch 266, LR 0.000057 Loss 6.060517, Accuracy 82.927%\n",
      "Epoch 19, Batch 267, LR 0.000057 Loss 6.061647, Accuracy 82.930%\n",
      "Epoch 19, Batch 268, LR 0.000057 Loss 6.058825, Accuracy 82.941%\n",
      "Epoch 19, Batch 269, LR 0.000057 Loss 6.058779, Accuracy 82.946%\n",
      "Epoch 19, Batch 270, LR 0.000057 Loss 6.057859, Accuracy 82.946%\n",
      "Epoch 19, Batch 271, LR 0.000057 Loss 6.058226, Accuracy 82.939%\n",
      "Epoch 19, Batch 272, LR 0.000057 Loss 6.056305, Accuracy 82.939%\n",
      "Epoch 19, Batch 273, LR 0.000057 Loss 6.055644, Accuracy 82.938%\n",
      "Epoch 19, Batch 274, LR 0.000057 Loss 6.055732, Accuracy 82.947%\n",
      "Epoch 19, Batch 275, LR 0.000057 Loss 6.054832, Accuracy 82.960%\n",
      "Epoch 19, Batch 276, LR 0.000057 Loss 6.054843, Accuracy 82.965%\n",
      "Epoch 19, Batch 277, LR 0.000057 Loss 6.052309, Accuracy 82.979%\n",
      "Epoch 19, Batch 278, LR 0.000057 Loss 6.051309, Accuracy 82.984%\n",
      "Epoch 19, Batch 279, LR 0.000057 Loss 6.050987, Accuracy 82.989%\n",
      "Epoch 19, Batch 280, LR 0.000057 Loss 6.047067, Accuracy 83.011%\n",
      "Epoch 19, Batch 281, LR 0.000057 Loss 6.047661, Accuracy 83.015%\n",
      "Epoch 19, Batch 282, LR 0.000057 Loss 6.049025, Accuracy 83.009%\n",
      "Epoch 19, Batch 283, LR 0.000057 Loss 6.050294, Accuracy 83.011%\n",
      "Epoch 19, Batch 284, LR 0.000057 Loss 6.052727, Accuracy 82.994%\n",
      "Epoch 19, Batch 285, LR 0.000057 Loss 6.052967, Accuracy 83.004%\n",
      "Epoch 19, Batch 286, LR 0.000057 Loss 6.053049, Accuracy 83.006%\n",
      "Epoch 19, Batch 287, LR 0.000057 Loss 6.053981, Accuracy 82.992%\n",
      "Epoch 19, Batch 288, LR 0.000057 Loss 6.051153, Accuracy 83.005%\n",
      "Epoch 19, Batch 289, LR 0.000057 Loss 6.052266, Accuracy 82.980%\n",
      "Epoch 19, Batch 290, LR 0.000057 Loss 6.051844, Accuracy 82.985%\n",
      "Epoch 19, Batch 291, LR 0.000057 Loss 6.052345, Accuracy 82.976%\n",
      "Epoch 19, Batch 292, LR 0.000057 Loss 6.050771, Accuracy 82.994%\n",
      "Epoch 19, Batch 293, LR 0.000057 Loss 6.047185, Accuracy 83.015%\n",
      "Epoch 19, Batch 294, LR 0.000057 Loss 6.046662, Accuracy 83.033%\n",
      "Epoch 19, Batch 295, LR 0.000057 Loss 6.047416, Accuracy 83.022%\n",
      "Epoch 19, Batch 296, LR 0.000057 Loss 6.047882, Accuracy 83.005%\n",
      "Epoch 19, Batch 297, LR 0.000057 Loss 6.049730, Accuracy 83.005%\n",
      "Epoch 19, Batch 298, LR 0.000057 Loss 6.049130, Accuracy 83.009%\n",
      "Epoch 19, Batch 299, LR 0.000057 Loss 6.050823, Accuracy 82.998%\n",
      "Epoch 19, Batch 300, LR 0.000057 Loss 6.049986, Accuracy 83.013%\n",
      "Epoch 19, Batch 301, LR 0.000057 Loss 6.050859, Accuracy 83.018%\n",
      "Epoch 19, Batch 302, LR 0.000057 Loss 6.051060, Accuracy 83.019%\n",
      "Epoch 19, Batch 303, LR 0.000057 Loss 6.051307, Accuracy 83.008%\n",
      "Epoch 19, Batch 304, LR 0.000057 Loss 6.052930, Accuracy 82.998%\n",
      "Epoch 19, Batch 305, LR 0.000057 Loss 6.053568, Accuracy 82.987%\n",
      "Epoch 19, Batch 306, LR 0.000057 Loss 6.052201, Accuracy 82.991%\n",
      "Epoch 19, Batch 307, LR 0.000057 Loss 6.052265, Accuracy 82.993%\n",
      "Epoch 19, Batch 308, LR 0.000057 Loss 6.050675, Accuracy 83.008%\n",
      "Epoch 19, Batch 309, LR 0.000057 Loss 6.049116, Accuracy 83.012%\n",
      "Epoch 19, Batch 310, LR 0.000057 Loss 6.048898, Accuracy 83.012%\n",
      "Epoch 19, Batch 311, LR 0.000057 Loss 6.049196, Accuracy 83.013%\n",
      "Epoch 19, Batch 312, LR 0.000057 Loss 6.051116, Accuracy 82.998%\n",
      "Epoch 19, Batch 313, LR 0.000057 Loss 6.050522, Accuracy 83.002%\n",
      "Epoch 19, Batch 314, LR 0.000057 Loss 6.049205, Accuracy 83.024%\n",
      "Epoch 19, Batch 315, LR 0.000057 Loss 6.049635, Accuracy 83.018%\n",
      "Epoch 19, Batch 316, LR 0.000057 Loss 6.051714, Accuracy 83.023%\n",
      "Epoch 19, Batch 317, LR 0.000057 Loss 6.052383, Accuracy 83.010%\n",
      "Epoch 19, Batch 318, LR 0.000057 Loss 6.051697, Accuracy 83.009%\n",
      "Epoch 19, Batch 319, LR 0.000057 Loss 6.051441, Accuracy 83.008%\n",
      "Epoch 19, Batch 320, LR 0.000057 Loss 6.050478, Accuracy 83.013%\n",
      "Epoch 19, Batch 321, LR 0.000057 Loss 6.048436, Accuracy 83.019%\n",
      "Epoch 19, Batch 322, LR 0.000057 Loss 6.047057, Accuracy 83.021%\n",
      "Epoch 19, Batch 323, LR 0.000057 Loss 6.046019, Accuracy 83.030%\n",
      "Epoch 19, Batch 324, LR 0.000057 Loss 6.046025, Accuracy 83.020%\n",
      "Epoch 19, Batch 325, LR 0.000057 Loss 6.045202, Accuracy 83.024%\n",
      "Epoch 19, Batch 326, LR 0.000057 Loss 6.045858, Accuracy 83.023%\n",
      "Epoch 19, Batch 327, LR 0.000057 Loss 6.044207, Accuracy 83.039%\n",
      "Epoch 19, Batch 328, LR 0.000057 Loss 6.044414, Accuracy 83.046%\n",
      "Epoch 19, Batch 329, LR 0.000057 Loss 6.041792, Accuracy 83.062%\n",
      "Epoch 19, Batch 330, LR 0.000057 Loss 6.038896, Accuracy 83.075%\n",
      "Epoch 19, Batch 331, LR 0.000057 Loss 6.038595, Accuracy 83.067%\n",
      "Epoch 19, Batch 332, LR 0.000057 Loss 6.041324, Accuracy 83.043%\n",
      "Epoch 19, Batch 333, LR 0.000057 Loss 6.044051, Accuracy 83.026%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 334, LR 0.000057 Loss 6.044667, Accuracy 83.032%\n",
      "Epoch 19, Batch 335, LR 0.000057 Loss 6.044724, Accuracy 83.029%\n",
      "Epoch 19, Batch 336, LR 0.000057 Loss 6.045845, Accuracy 83.022%\n",
      "Epoch 19, Batch 337, LR 0.000057 Loss 6.045952, Accuracy 83.026%\n",
      "Epoch 19, Batch 338, LR 0.000057 Loss 6.047864, Accuracy 83.016%\n",
      "Epoch 19, Batch 339, LR 0.000057 Loss 6.048578, Accuracy 83.018%\n",
      "Epoch 19, Batch 340, LR 0.000057 Loss 6.049283, Accuracy 83.024%\n",
      "Epoch 19, Batch 341, LR 0.000057 Loss 6.048276, Accuracy 83.030%\n",
      "Epoch 19, Batch 342, LR 0.000057 Loss 6.048904, Accuracy 83.027%\n",
      "Epoch 19, Batch 343, LR 0.000057 Loss 6.048637, Accuracy 83.024%\n",
      "Epoch 19, Batch 344, LR 0.000057 Loss 6.045661, Accuracy 83.033%\n",
      "Epoch 19, Batch 345, LR 0.000057 Loss 6.043483, Accuracy 83.037%\n",
      "Epoch 19, Batch 346, LR 0.000057 Loss 6.044069, Accuracy 83.025%\n",
      "Epoch 19, Batch 347, LR 0.000057 Loss 6.042205, Accuracy 83.033%\n",
      "Epoch 19, Batch 348, LR 0.000057 Loss 6.041636, Accuracy 83.039%\n",
      "Epoch 19, Batch 349, LR 0.000057 Loss 6.040092, Accuracy 83.048%\n",
      "Epoch 19, Batch 350, LR 0.000057 Loss 6.039624, Accuracy 83.040%\n",
      "Epoch 19, Batch 351, LR 0.000057 Loss 6.039460, Accuracy 83.042%\n",
      "Epoch 19, Batch 352, LR 0.000057 Loss 6.040417, Accuracy 83.030%\n",
      "Epoch 19, Batch 353, LR 0.000057 Loss 6.040005, Accuracy 83.032%\n",
      "Epoch 19, Batch 354, LR 0.000057 Loss 6.042080, Accuracy 83.020%\n",
      "Epoch 19, Batch 355, LR 0.000057 Loss 6.042269, Accuracy 83.017%\n",
      "Epoch 19, Batch 356, LR 0.000057 Loss 6.039132, Accuracy 83.039%\n",
      "Epoch 19, Batch 357, LR 0.000057 Loss 6.039860, Accuracy 83.044%\n",
      "Epoch 19, Batch 358, LR 0.000057 Loss 6.038295, Accuracy 83.042%\n",
      "Epoch 19, Batch 359, LR 0.000057 Loss 6.037166, Accuracy 83.048%\n",
      "Epoch 19, Batch 360, LR 0.000057 Loss 6.039081, Accuracy 83.036%\n",
      "Epoch 19, Batch 361, LR 0.000057 Loss 6.038709, Accuracy 83.038%\n",
      "Epoch 19, Batch 362, LR 0.000057 Loss 6.038139, Accuracy 83.039%\n",
      "Epoch 19, Batch 363, LR 0.000057 Loss 6.037972, Accuracy 83.038%\n",
      "Epoch 19, Batch 364, LR 0.000057 Loss 6.036862, Accuracy 83.057%\n",
      "Epoch 19, Batch 365, LR 0.000057 Loss 6.036340, Accuracy 83.059%\n",
      "Epoch 19, Batch 366, LR 0.000057 Loss 6.035768, Accuracy 83.056%\n",
      "Epoch 19, Batch 367, LR 0.000057 Loss 6.035726, Accuracy 83.042%\n",
      "Epoch 19, Batch 368, LR 0.000057 Loss 6.036477, Accuracy 83.040%\n",
      "Epoch 19, Batch 369, LR 0.000057 Loss 6.035809, Accuracy 83.045%\n",
      "Epoch 19, Batch 370, LR 0.000057 Loss 6.035405, Accuracy 83.049%\n",
      "Epoch 19, Batch 371, LR 0.000057 Loss 6.037198, Accuracy 83.038%\n",
      "Epoch 19, Batch 372, LR 0.000057 Loss 6.037049, Accuracy 83.044%\n",
      "Epoch 19, Batch 373, LR 0.000057 Loss 6.035998, Accuracy 83.045%\n",
      "Epoch 19, Batch 374, LR 0.000057 Loss 6.035606, Accuracy 83.049%\n",
      "Epoch 19, Batch 375, LR 0.000057 Loss 6.035407, Accuracy 83.044%\n",
      "Epoch 19, Batch 376, LR 0.000057 Loss 6.034021, Accuracy 83.047%\n",
      "Epoch 19, Batch 377, LR 0.000057 Loss 6.033576, Accuracy 83.061%\n",
      "Epoch 19, Batch 378, LR 0.000057 Loss 6.032675, Accuracy 83.071%\n",
      "Epoch 19, Batch 379, LR 0.000057 Loss 6.031918, Accuracy 83.060%\n",
      "Epoch 19, Batch 380, LR 0.000057 Loss 6.032508, Accuracy 83.053%\n",
      "Epoch 19, Batch 381, LR 0.000057 Loss 6.032657, Accuracy 83.052%\n",
      "Epoch 19, Batch 382, LR 0.000057 Loss 6.034285, Accuracy 83.050%\n",
      "Epoch 19, Batch 383, LR 0.000057 Loss 6.033277, Accuracy 83.057%\n",
      "Epoch 19, Batch 384, LR 0.000057 Loss 6.034145, Accuracy 83.055%\n",
      "Epoch 19, Batch 385, LR 0.000057 Loss 6.032459, Accuracy 83.054%\n",
      "Epoch 19, Batch 386, LR 0.000057 Loss 6.030885, Accuracy 83.057%\n",
      "Epoch 19, Batch 387, LR 0.000057 Loss 6.032479, Accuracy 83.047%\n",
      "Epoch 19, Batch 388, LR 0.000057 Loss 6.033335, Accuracy 83.052%\n",
      "Epoch 19, Batch 389, LR 0.000057 Loss 6.035560, Accuracy 83.031%\n",
      "Epoch 19, Batch 390, LR 0.000057 Loss 6.034796, Accuracy 83.047%\n",
      "Epoch 19, Batch 391, LR 0.000057 Loss 6.033343, Accuracy 83.056%\n",
      "Epoch 19, Batch 392, LR 0.000057 Loss 6.035304, Accuracy 83.044%\n",
      "Epoch 19, Batch 393, LR 0.000057 Loss 6.034054, Accuracy 83.047%\n",
      "Epoch 19, Batch 394, LR 0.000057 Loss 6.036621, Accuracy 83.037%\n",
      "Epoch 19, Batch 395, LR 0.000057 Loss 6.039214, Accuracy 83.028%\n",
      "Epoch 19, Batch 396, LR 0.000057 Loss 6.040810, Accuracy 83.028%\n",
      "Epoch 19, Batch 397, LR 0.000057 Loss 6.041088, Accuracy 83.025%\n",
      "Epoch 19, Batch 398, LR 0.000057 Loss 6.042005, Accuracy 83.030%\n",
      "Epoch 19, Batch 399, LR 0.000057 Loss 6.041689, Accuracy 83.034%\n",
      "Epoch 19, Batch 400, LR 0.000057 Loss 6.041615, Accuracy 83.041%\n",
      "Epoch 19, Batch 401, LR 0.000057 Loss 6.041456, Accuracy 83.048%\n",
      "Epoch 19, Batch 402, LR 0.000057 Loss 6.042909, Accuracy 83.040%\n",
      "Epoch 19, Batch 403, LR 0.000057 Loss 6.045602, Accuracy 83.032%\n",
      "Epoch 19, Batch 404, LR 0.000057 Loss 6.046486, Accuracy 83.027%\n",
      "Epoch 19, Batch 405, LR 0.000057 Loss 6.045409, Accuracy 83.034%\n",
      "Epoch 19, Batch 406, LR 0.000057 Loss 6.046793, Accuracy 83.028%\n",
      "Epoch 19, Batch 407, LR 0.000057 Loss 6.047278, Accuracy 83.026%\n",
      "Epoch 19, Batch 408, LR 0.000057 Loss 6.047774, Accuracy 83.025%\n",
      "Epoch 19, Batch 409, LR 0.000057 Loss 6.046954, Accuracy 83.030%\n",
      "Epoch 19, Batch 410, LR 0.000057 Loss 6.048619, Accuracy 83.024%\n",
      "Epoch 19, Batch 411, LR 0.000057 Loss 6.052382, Accuracy 83.004%\n",
      "Epoch 19, Batch 412, LR 0.000057 Loss 6.053342, Accuracy 83.000%\n",
      "Epoch 19, Batch 413, LR 0.000057 Loss 6.054910, Accuracy 82.992%\n",
      "Epoch 19, Batch 414, LR 0.000057 Loss 6.054133, Accuracy 82.999%\n",
      "Epoch 19, Batch 415, LR 0.000057 Loss 6.053520, Accuracy 83.010%\n",
      "Epoch 19, Batch 416, LR 0.000057 Loss 6.053783, Accuracy 83.010%\n",
      "Epoch 19, Batch 417, LR 0.000057 Loss 6.055553, Accuracy 82.994%\n",
      "Epoch 19, Batch 418, LR 0.000057 Loss 6.056140, Accuracy 82.992%\n",
      "Epoch 19, Batch 419, LR 0.000057 Loss 6.054508, Accuracy 83.001%\n",
      "Epoch 19, Batch 420, LR 0.000057 Loss 6.054957, Accuracy 83.008%\n",
      "Epoch 19, Batch 421, LR 0.000057 Loss 6.054594, Accuracy 83.009%\n",
      "Epoch 19, Batch 422, LR 0.000057 Loss 6.056836, Accuracy 82.992%\n",
      "Epoch 19, Batch 423, LR 0.000057 Loss 6.057499, Accuracy 82.975%\n",
      "Epoch 19, Batch 424, LR 0.000057 Loss 6.056535, Accuracy 82.978%\n",
      "Epoch 19, Batch 425, LR 0.000057 Loss 6.056242, Accuracy 82.982%\n",
      "Epoch 19, Batch 426, LR 0.000057 Loss 6.055241, Accuracy 82.992%\n",
      "Epoch 19, Batch 427, LR 0.000057 Loss 6.056442, Accuracy 82.988%\n",
      "Epoch 19, Batch 428, LR 0.000057 Loss 6.058302, Accuracy 82.980%\n",
      "Epoch 19, Batch 429, LR 0.000057 Loss 6.057102, Accuracy 82.980%\n",
      "Epoch 19, Batch 430, LR 0.000057 Loss 6.057165, Accuracy 82.985%\n",
      "Epoch 19, Batch 431, LR 0.000057 Loss 6.056481, Accuracy 82.983%\n",
      "Epoch 19, Batch 432, LR 0.000057 Loss 6.055236, Accuracy 82.988%\n",
      "Epoch 19, Batch 433, LR 0.000057 Loss 6.053768, Accuracy 83.002%\n",
      "Epoch 19, Batch 434, LR 0.000057 Loss 6.053799, Accuracy 83.005%\n",
      "Epoch 19, Batch 435, LR 0.000057 Loss 6.055715, Accuracy 82.997%\n",
      "Epoch 19, Batch 436, LR 0.000057 Loss 6.054897, Accuracy 83.006%\n",
      "Epoch 19, Batch 437, LR 0.000057 Loss 6.053648, Accuracy 83.011%\n",
      "Epoch 19, Batch 438, LR 0.000057 Loss 6.054676, Accuracy 83.003%\n",
      "Epoch 19, Batch 439, LR 0.000057 Loss 6.055499, Accuracy 82.996%\n",
      "Epoch 19, Batch 440, LR 0.000057 Loss 6.053706, Accuracy 83.013%\n",
      "Epoch 19, Batch 441, LR 0.000057 Loss 6.053798, Accuracy 83.002%\n",
      "Epoch 19, Batch 442, LR 0.000057 Loss 6.053175, Accuracy 82.995%\n",
      "Epoch 19, Batch 443, LR 0.000057 Loss 6.053293, Accuracy 82.987%\n",
      "Epoch 19, Batch 444, LR 0.000056 Loss 6.055546, Accuracy 82.969%\n",
      "Epoch 19, Batch 445, LR 0.000056 Loss 6.055618, Accuracy 82.971%\n",
      "Epoch 19, Batch 446, LR 0.000056 Loss 6.056410, Accuracy 82.960%\n",
      "Epoch 19, Batch 447, LR 0.000056 Loss 6.055423, Accuracy 82.963%\n",
      "Epoch 19, Batch 448, LR 0.000056 Loss 6.054877, Accuracy 82.966%\n",
      "Epoch 19, Batch 449, LR 0.000056 Loss 6.054404, Accuracy 82.973%\n",
      "Epoch 19, Batch 450, LR 0.000056 Loss 6.051750, Accuracy 82.979%\n",
      "Epoch 19, Batch 451, LR 0.000056 Loss 6.051804, Accuracy 82.984%\n",
      "Epoch 19, Batch 452, LR 0.000056 Loss 6.049969, Accuracy 82.994%\n",
      "Epoch 19, Batch 453, LR 0.000056 Loss 6.048779, Accuracy 82.997%\n",
      "Epoch 19, Batch 454, LR 0.000056 Loss 6.048474, Accuracy 82.990%\n",
      "Epoch 19, Batch 455, LR 0.000056 Loss 6.048000, Accuracy 83.001%\n",
      "Epoch 19, Batch 456, LR 0.000056 Loss 6.050012, Accuracy 82.992%\n",
      "Epoch 19, Batch 457, LR 0.000056 Loss 6.050374, Accuracy 82.989%\n",
      "Epoch 19, Batch 458, LR 0.000056 Loss 6.049591, Accuracy 82.998%\n",
      "Epoch 19, Batch 459, LR 0.000056 Loss 6.048339, Accuracy 83.010%\n",
      "Epoch 19, Batch 460, LR 0.000056 Loss 6.048509, Accuracy 83.010%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 461, LR 0.000056 Loss 6.048366, Accuracy 83.018%\n",
      "Epoch 19, Batch 462, LR 0.000056 Loss 6.046460, Accuracy 83.022%\n",
      "Epoch 19, Batch 463, LR 0.000056 Loss 6.046737, Accuracy 83.027%\n",
      "Epoch 19, Batch 464, LR 0.000056 Loss 6.048591, Accuracy 83.013%\n",
      "Epoch 19, Batch 465, LR 0.000056 Loss 6.047529, Accuracy 83.009%\n",
      "Epoch 19, Batch 466, LR 0.000056 Loss 6.045581, Accuracy 83.019%\n",
      "Epoch 19, Batch 467, LR 0.000056 Loss 6.044147, Accuracy 83.020%\n",
      "Epoch 19, Batch 468, LR 0.000056 Loss 6.044774, Accuracy 83.013%\n",
      "Epoch 19, Batch 469, LR 0.000056 Loss 6.044714, Accuracy 83.009%\n",
      "Epoch 19, Batch 470, LR 0.000056 Loss 6.044168, Accuracy 83.017%\n",
      "Epoch 19, Batch 471, LR 0.000056 Loss 6.045122, Accuracy 83.005%\n",
      "Epoch 19, Batch 472, LR 0.000056 Loss 6.045583, Accuracy 83.005%\n",
      "Epoch 19, Batch 473, LR 0.000056 Loss 6.045530, Accuracy 83.007%\n",
      "Epoch 19, Batch 474, LR 0.000056 Loss 6.045095, Accuracy 83.004%\n",
      "Epoch 19, Batch 475, LR 0.000056 Loss 6.044165, Accuracy 83.007%\n",
      "Epoch 19, Batch 476, LR 0.000056 Loss 6.043671, Accuracy 83.008%\n",
      "Epoch 19, Batch 477, LR 0.000056 Loss 6.042759, Accuracy 83.012%\n",
      "Epoch 19, Batch 478, LR 0.000056 Loss 6.041473, Accuracy 83.009%\n",
      "Epoch 19, Batch 479, LR 0.000056 Loss 6.040312, Accuracy 83.015%\n",
      "Epoch 19, Batch 480, LR 0.000056 Loss 6.039329, Accuracy 83.018%\n",
      "Epoch 19, Batch 481, LR 0.000056 Loss 6.038458, Accuracy 83.020%\n",
      "Epoch 19, Batch 482, LR 0.000056 Loss 6.039420, Accuracy 83.020%\n",
      "Epoch 19, Batch 483, LR 0.000056 Loss 6.040531, Accuracy 83.013%\n",
      "Epoch 19, Batch 484, LR 0.000056 Loss 6.040103, Accuracy 83.016%\n",
      "Epoch 19, Batch 485, LR 0.000056 Loss 6.039126, Accuracy 83.012%\n",
      "Epoch 19, Batch 486, LR 0.000056 Loss 6.040773, Accuracy 82.997%\n",
      "Epoch 19, Batch 487, LR 0.000056 Loss 6.041839, Accuracy 82.987%\n",
      "Epoch 19, Batch 488, LR 0.000056 Loss 6.041993, Accuracy 82.990%\n",
      "Epoch 19, Batch 489, LR 0.000056 Loss 6.043080, Accuracy 82.988%\n",
      "Epoch 19, Batch 490, LR 0.000056 Loss 6.043758, Accuracy 82.985%\n",
      "Epoch 19, Batch 491, LR 0.000056 Loss 6.044058, Accuracy 82.984%\n",
      "Epoch 19, Batch 492, LR 0.000056 Loss 6.044233, Accuracy 82.990%\n",
      "Epoch 19, Batch 493, LR 0.000056 Loss 6.044373, Accuracy 82.992%\n",
      "Epoch 19, Batch 494, LR 0.000056 Loss 6.043710, Accuracy 82.982%\n",
      "Epoch 19, Batch 495, LR 0.000056 Loss 6.043257, Accuracy 82.988%\n",
      "Epoch 19, Batch 496, LR 0.000056 Loss 6.044833, Accuracy 82.981%\n",
      "Epoch 19, Batch 497, LR 0.000056 Loss 6.045592, Accuracy 82.974%\n",
      "Epoch 19, Batch 498, LR 0.000056 Loss 6.046851, Accuracy 82.973%\n",
      "Epoch 19, Batch 499, LR 0.000056 Loss 6.045964, Accuracy 82.974%\n",
      "Epoch 19, Batch 500, LR 0.000056 Loss 6.044695, Accuracy 82.987%\n",
      "Epoch 19, Batch 501, LR 0.000056 Loss 6.043527, Accuracy 83.001%\n",
      "Epoch 19, Batch 502, LR 0.000056 Loss 6.044662, Accuracy 83.002%\n",
      "Epoch 19, Batch 503, LR 0.000056 Loss 6.044502, Accuracy 83.002%\n",
      "Epoch 19, Batch 504, LR 0.000056 Loss 6.044726, Accuracy 83.002%\n",
      "Epoch 19, Batch 505, LR 0.000056 Loss 6.046571, Accuracy 82.987%\n",
      "Epoch 19, Batch 506, LR 0.000056 Loss 6.044941, Accuracy 82.998%\n",
      "Epoch 19, Batch 507, LR 0.000056 Loss 6.045055, Accuracy 82.994%\n",
      "Epoch 19, Batch 508, LR 0.000056 Loss 6.044039, Accuracy 83.006%\n",
      "Epoch 19, Batch 509, LR 0.000056 Loss 6.042159, Accuracy 83.010%\n",
      "Epoch 19, Batch 510, LR 0.000056 Loss 6.040218, Accuracy 83.021%\n",
      "Epoch 19, Batch 511, LR 0.000056 Loss 6.039128, Accuracy 83.025%\n",
      "Epoch 19, Batch 512, LR 0.000056 Loss 6.040878, Accuracy 83.014%\n",
      "Epoch 19, Batch 513, LR 0.000056 Loss 6.042116, Accuracy 83.004%\n",
      "Epoch 19, Batch 514, LR 0.000056 Loss 6.041952, Accuracy 83.009%\n",
      "Epoch 19, Batch 515, LR 0.000056 Loss 6.042864, Accuracy 83.010%\n",
      "Epoch 19, Batch 516, LR 0.000056 Loss 6.043136, Accuracy 83.003%\n",
      "Epoch 19, Batch 517, LR 0.000056 Loss 6.043100, Accuracy 82.997%\n",
      "Epoch 19, Batch 518, LR 0.000056 Loss 6.043460, Accuracy 82.995%\n",
      "Epoch 19, Batch 519, LR 0.000056 Loss 6.044835, Accuracy 82.992%\n",
      "Epoch 19, Batch 520, LR 0.000056 Loss 6.043894, Accuracy 83.000%\n",
      "Epoch 19, Batch 521, LR 0.000056 Loss 6.044190, Accuracy 82.988%\n",
      "Epoch 19, Batch 522, LR 0.000056 Loss 6.044907, Accuracy 82.983%\n",
      "Epoch 19, Batch 523, LR 0.000056 Loss 6.044659, Accuracy 82.978%\n",
      "Epoch 19, Batch 524, LR 0.000056 Loss 6.043939, Accuracy 82.975%\n",
      "Epoch 19, Batch 525, LR 0.000056 Loss 6.044945, Accuracy 82.963%\n",
      "Epoch 19, Batch 526, LR 0.000056 Loss 6.045508, Accuracy 82.963%\n",
      "Epoch 19, Batch 527, LR 0.000056 Loss 6.045119, Accuracy 82.971%\n",
      "Epoch 19, Batch 528, LR 0.000056 Loss 6.045326, Accuracy 82.966%\n",
      "Epoch 19, Batch 529, LR 0.000056 Loss 6.044182, Accuracy 82.973%\n",
      "Epoch 19, Batch 530, LR 0.000056 Loss 6.044829, Accuracy 82.969%\n",
      "Epoch 19, Batch 531, LR 0.000056 Loss 6.044420, Accuracy 82.971%\n",
      "Epoch 19, Batch 532, LR 0.000056 Loss 6.043562, Accuracy 82.983%\n",
      "Epoch 19, Batch 533, LR 0.000056 Loss 6.043534, Accuracy 82.984%\n",
      "Epoch 19, Batch 534, LR 0.000056 Loss 6.043307, Accuracy 82.984%\n",
      "Epoch 19, Batch 535, LR 0.000056 Loss 6.043976, Accuracy 82.983%\n",
      "Epoch 19, Batch 536, LR 0.000056 Loss 6.045129, Accuracy 82.980%\n",
      "Epoch 19, Batch 537, LR 0.000056 Loss 6.045910, Accuracy 82.975%\n",
      "Epoch 19, Batch 538, LR 0.000056 Loss 6.045730, Accuracy 82.974%\n",
      "Epoch 19, Batch 539, LR 0.000056 Loss 6.045426, Accuracy 82.976%\n",
      "Epoch 19, Batch 540, LR 0.000056 Loss 6.045653, Accuracy 82.980%\n",
      "Epoch 19, Batch 541, LR 0.000056 Loss 6.045608, Accuracy 82.979%\n",
      "Epoch 19, Batch 542, LR 0.000056 Loss 6.047047, Accuracy 82.967%\n",
      "Epoch 19, Batch 543, LR 0.000056 Loss 6.048870, Accuracy 82.953%\n",
      "Epoch 19, Batch 544, LR 0.000056 Loss 6.050195, Accuracy 82.956%\n",
      "Epoch 19, Batch 545, LR 0.000056 Loss 6.051407, Accuracy 82.952%\n",
      "Epoch 19, Batch 546, LR 0.000056 Loss 6.051222, Accuracy 82.961%\n",
      "Epoch 19, Batch 547, LR 0.000056 Loss 6.049857, Accuracy 82.972%\n",
      "Epoch 19, Batch 548, LR 0.000056 Loss 6.050883, Accuracy 82.965%\n",
      "Epoch 19, Batch 549, LR 0.000056 Loss 6.050041, Accuracy 82.965%\n",
      "Epoch 19, Batch 550, LR 0.000056 Loss 6.050987, Accuracy 82.957%\n",
      "Epoch 19, Batch 551, LR 0.000056 Loss 6.050479, Accuracy 82.954%\n",
      "Epoch 19, Batch 552, LR 0.000056 Loss 6.050692, Accuracy 82.961%\n",
      "Epoch 19, Batch 553, LR 0.000056 Loss 6.050637, Accuracy 82.952%\n",
      "Epoch 19, Batch 554, LR 0.000056 Loss 6.048409, Accuracy 82.961%\n",
      "Epoch 19, Batch 555, LR 0.000056 Loss 6.049245, Accuracy 82.962%\n",
      "Epoch 19, Batch 556, LR 0.000056 Loss 6.049177, Accuracy 82.970%\n",
      "Epoch 19, Batch 557, LR 0.000056 Loss 6.047528, Accuracy 82.981%\n",
      "Epoch 19, Batch 558, LR 0.000056 Loss 6.047366, Accuracy 82.978%\n",
      "Epoch 19, Batch 559, LR 0.000056 Loss 6.046914, Accuracy 82.983%\n",
      "Epoch 19, Batch 560, LR 0.000056 Loss 6.045720, Accuracy 82.984%\n",
      "Epoch 19, Batch 561, LR 0.000056 Loss 6.047272, Accuracy 82.984%\n",
      "Epoch 19, Batch 562, LR 0.000056 Loss 6.046522, Accuracy 82.990%\n",
      "Epoch 19, Batch 563, LR 0.000056 Loss 6.048095, Accuracy 82.985%\n",
      "Epoch 19, Batch 564, LR 0.000056 Loss 6.048846, Accuracy 82.983%\n",
      "Epoch 19, Batch 565, LR 0.000056 Loss 6.048478, Accuracy 82.981%\n",
      "Epoch 19, Batch 566, LR 0.000056 Loss 6.049327, Accuracy 82.974%\n",
      "Epoch 19, Batch 567, LR 0.000056 Loss 6.049409, Accuracy 82.975%\n",
      "Epoch 19, Batch 568, LR 0.000056 Loss 6.049895, Accuracy 82.964%\n",
      "Epoch 19, Batch 569, LR 0.000056 Loss 6.049202, Accuracy 82.968%\n",
      "Epoch 19, Batch 570, LR 0.000056 Loss 6.048976, Accuracy 82.974%\n",
      "Epoch 19, Batch 571, LR 0.000056 Loss 6.049384, Accuracy 82.970%\n",
      "Epoch 19, Batch 572, LR 0.000056 Loss 6.049062, Accuracy 82.967%\n",
      "Epoch 19, Batch 573, LR 0.000056 Loss 6.047890, Accuracy 82.982%\n",
      "Epoch 19, Batch 574, LR 0.000056 Loss 6.048689, Accuracy 82.985%\n",
      "Epoch 19, Batch 575, LR 0.000056 Loss 6.048918, Accuracy 82.985%\n",
      "Epoch 19, Batch 576, LR 0.000056 Loss 6.048343, Accuracy 82.983%\n",
      "Epoch 19, Batch 577, LR 0.000056 Loss 6.047361, Accuracy 82.987%\n",
      "Epoch 19, Batch 578, LR 0.000056 Loss 6.045536, Accuracy 82.999%\n",
      "Epoch 19, Batch 579, LR 0.000056 Loss 6.047086, Accuracy 82.999%\n",
      "Epoch 19, Batch 580, LR 0.000056 Loss 6.045632, Accuracy 83.012%\n",
      "Epoch 19, Batch 581, LR 0.000056 Loss 6.044918, Accuracy 83.018%\n",
      "Epoch 19, Batch 582, LR 0.000056 Loss 6.043742, Accuracy 83.021%\n",
      "Epoch 19, Batch 583, LR 0.000056 Loss 6.042458, Accuracy 83.032%\n",
      "Epoch 19, Batch 584, LR 0.000056 Loss 6.042677, Accuracy 83.028%\n",
      "Epoch 19, Batch 585, LR 0.000056 Loss 6.042116, Accuracy 83.034%\n",
      "Epoch 19, Batch 586, LR 0.000056 Loss 6.041431, Accuracy 83.035%\n",
      "Epoch 19, Batch 587, LR 0.000056 Loss 6.039757, Accuracy 83.039%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 588, LR 0.000056 Loss 6.040305, Accuracy 83.038%\n",
      "Epoch 19, Batch 589, LR 0.000056 Loss 6.042394, Accuracy 83.033%\n",
      "Epoch 19, Batch 590, LR 0.000056 Loss 6.042773, Accuracy 83.024%\n",
      "Epoch 19, Batch 591, LR 0.000056 Loss 6.042245, Accuracy 83.023%\n",
      "Epoch 19, Batch 592, LR 0.000056 Loss 6.041428, Accuracy 83.030%\n",
      "Epoch 19, Batch 593, LR 0.000056 Loss 6.040776, Accuracy 83.030%\n",
      "Epoch 19, Batch 594, LR 0.000056 Loss 6.042116, Accuracy 83.020%\n",
      "Epoch 19, Batch 595, LR 0.000056 Loss 6.040644, Accuracy 83.032%\n",
      "Epoch 19, Batch 596, LR 0.000056 Loss 6.039987, Accuracy 83.033%\n",
      "Epoch 19, Batch 597, LR 0.000056 Loss 6.040118, Accuracy 83.030%\n",
      "Epoch 19, Batch 598, LR 0.000056 Loss 6.041880, Accuracy 83.023%\n",
      "Epoch 19, Batch 599, LR 0.000056 Loss 6.043457, Accuracy 83.016%\n",
      "Epoch 19, Batch 600, LR 0.000056 Loss 6.043524, Accuracy 83.014%\n",
      "Epoch 19, Batch 601, LR 0.000056 Loss 6.045036, Accuracy 83.010%\n",
      "Epoch 19, Batch 602, LR 0.000056 Loss 6.045785, Accuracy 83.006%\n",
      "Epoch 19, Batch 603, LR 0.000056 Loss 6.046670, Accuracy 83.008%\n",
      "Epoch 19, Batch 604, LR 0.000056 Loss 6.046789, Accuracy 83.009%\n",
      "Epoch 19, Batch 605, LR 0.000056 Loss 6.048238, Accuracy 82.998%\n",
      "Epoch 19, Batch 606, LR 0.000056 Loss 6.048415, Accuracy 82.994%\n",
      "Epoch 19, Batch 607, LR 0.000056 Loss 6.050727, Accuracy 82.977%\n",
      "Epoch 19, Batch 608, LR 0.000056 Loss 6.051667, Accuracy 82.971%\n",
      "Epoch 19, Batch 609, LR 0.000056 Loss 6.051884, Accuracy 82.975%\n",
      "Epoch 19, Batch 610, LR 0.000056 Loss 6.051290, Accuracy 82.976%\n",
      "Epoch 19, Batch 611, LR 0.000056 Loss 6.050774, Accuracy 82.981%\n",
      "Epoch 19, Batch 612, LR 0.000056 Loss 6.050265, Accuracy 82.987%\n",
      "Epoch 19, Batch 613, LR 0.000056 Loss 6.050876, Accuracy 82.982%\n",
      "Epoch 19, Batch 614, LR 0.000056 Loss 6.050480, Accuracy 82.980%\n",
      "Epoch 19, Batch 615, LR 0.000056 Loss 6.050014, Accuracy 82.980%\n",
      "Epoch 19, Batch 616, LR 0.000056 Loss 6.049922, Accuracy 82.989%\n",
      "Epoch 19, Batch 617, LR 0.000056 Loss 6.047995, Accuracy 83.000%\n",
      "Epoch 19, Batch 618, LR 0.000056 Loss 6.046932, Accuracy 83.011%\n",
      "Epoch 19, Batch 619, LR 0.000056 Loss 6.048284, Accuracy 83.003%\n",
      "Epoch 19, Batch 620, LR 0.000056 Loss 6.047793, Accuracy 83.008%\n",
      "Epoch 19, Batch 621, LR 0.000056 Loss 6.048039, Accuracy 83.005%\n",
      "Epoch 19, Batch 622, LR 0.000056 Loss 6.048710, Accuracy 83.002%\n",
      "Epoch 19, Batch 623, LR 0.000056 Loss 6.050296, Accuracy 82.984%\n",
      "Epoch 19, Batch 624, LR 0.000056 Loss 6.050385, Accuracy 82.983%\n",
      "Epoch 19, Batch 625, LR 0.000056 Loss 6.050135, Accuracy 82.980%\n",
      "Epoch 19, Batch 626, LR 0.000056 Loss 6.049475, Accuracy 82.981%\n",
      "Epoch 19, Batch 627, LR 0.000056 Loss 6.048579, Accuracy 82.986%\n",
      "Epoch 19, Batch 628, LR 0.000056 Loss 6.049552, Accuracy 82.983%\n",
      "Epoch 19, Batch 629, LR 0.000056 Loss 6.049063, Accuracy 82.985%\n",
      "Epoch 19, Batch 630, LR 0.000056 Loss 6.049436, Accuracy 82.989%\n",
      "Epoch 19, Batch 631, LR 0.000056 Loss 6.049167, Accuracy 82.990%\n",
      "Epoch 19, Batch 632, LR 0.000056 Loss 6.048557, Accuracy 83.002%\n",
      "Epoch 19, Batch 633, LR 0.000056 Loss 6.048964, Accuracy 83.006%\n",
      "Epoch 19, Batch 634, LR 0.000056 Loss 6.049256, Accuracy 83.002%\n",
      "Epoch 19, Batch 635, LR 0.000056 Loss 6.048311, Accuracy 83.011%\n",
      "Epoch 19, Batch 636, LR 0.000056 Loss 6.047539, Accuracy 83.020%\n",
      "Epoch 19, Batch 637, LR 0.000056 Loss 6.048018, Accuracy 83.017%\n",
      "Epoch 19, Batch 638, LR 0.000056 Loss 6.047654, Accuracy 83.022%\n",
      "Epoch 19, Batch 639, LR 0.000056 Loss 6.047690, Accuracy 83.025%\n",
      "Epoch 19, Batch 640, LR 0.000056 Loss 6.047979, Accuracy 83.024%\n",
      "Epoch 19, Batch 641, LR 0.000056 Loss 6.048400, Accuracy 83.022%\n",
      "Epoch 19, Batch 642, LR 0.000056 Loss 6.048144, Accuracy 83.021%\n",
      "Epoch 19, Batch 643, LR 0.000056 Loss 6.048818, Accuracy 83.009%\n",
      "Epoch 19, Batch 644, LR 0.000056 Loss 6.048657, Accuracy 83.011%\n",
      "Epoch 19, Batch 645, LR 0.000056 Loss 6.049143, Accuracy 83.009%\n",
      "Epoch 19, Batch 646, LR 0.000056 Loss 6.048680, Accuracy 83.012%\n",
      "Epoch 19, Batch 647, LR 0.000056 Loss 6.049810, Accuracy 83.011%\n",
      "Epoch 19, Batch 648, LR 0.000056 Loss 6.048976, Accuracy 83.014%\n",
      "Epoch 19, Batch 649, LR 0.000056 Loss 6.048557, Accuracy 83.017%\n",
      "Epoch 19, Batch 650, LR 0.000056 Loss 6.049149, Accuracy 83.011%\n",
      "Epoch 19, Batch 651, LR 0.000056 Loss 6.049106, Accuracy 83.017%\n",
      "Epoch 19, Batch 652, LR 0.000056 Loss 6.049095, Accuracy 83.017%\n",
      "Epoch 19, Batch 653, LR 0.000056 Loss 6.049660, Accuracy 83.016%\n",
      "Epoch 19, Batch 654, LR 0.000056 Loss 6.050372, Accuracy 83.016%\n",
      "Epoch 19, Batch 655, LR 0.000056 Loss 6.049988, Accuracy 83.012%\n",
      "Epoch 19, Batch 656, LR 0.000056 Loss 6.049226, Accuracy 83.019%\n",
      "Epoch 19, Batch 657, LR 0.000056 Loss 6.049606, Accuracy 83.018%\n",
      "Epoch 19, Batch 658, LR 0.000056 Loss 6.049968, Accuracy 83.021%\n",
      "Epoch 19, Batch 659, LR 0.000056 Loss 6.049091, Accuracy 83.027%\n",
      "Epoch 19, Batch 660, LR 0.000056 Loss 6.049299, Accuracy 83.027%\n",
      "Epoch 19, Batch 661, LR 0.000056 Loss 6.048368, Accuracy 83.037%\n",
      "Epoch 19, Batch 662, LR 0.000056 Loss 6.046849, Accuracy 83.036%\n",
      "Epoch 19, Batch 663, LR 0.000056 Loss 6.047273, Accuracy 83.040%\n",
      "Epoch 19, Batch 664, LR 0.000056 Loss 6.047333, Accuracy 83.045%\n",
      "Epoch 19, Batch 665, LR 0.000056 Loss 6.047623, Accuracy 83.039%\n",
      "Epoch 19, Batch 666, LR 0.000056 Loss 6.048877, Accuracy 83.031%\n",
      "Epoch 19, Batch 667, LR 0.000056 Loss 6.049219, Accuracy 83.030%\n",
      "Epoch 19, Batch 668, LR 0.000056 Loss 6.048886, Accuracy 83.027%\n",
      "Epoch 19, Batch 669, LR 0.000056 Loss 6.048343, Accuracy 83.031%\n",
      "Epoch 19, Batch 670, LR 0.000056 Loss 6.048133, Accuracy 83.039%\n",
      "Epoch 19, Batch 671, LR 0.000056 Loss 6.046335, Accuracy 83.045%\n",
      "Epoch 19, Batch 672, LR 0.000056 Loss 6.045591, Accuracy 83.043%\n",
      "Epoch 19, Batch 673, LR 0.000056 Loss 6.045423, Accuracy 83.040%\n",
      "Epoch 19, Batch 674, LR 0.000056 Loss 6.046051, Accuracy 83.042%\n",
      "Epoch 19, Batch 675, LR 0.000056 Loss 6.045679, Accuracy 83.047%\n",
      "Epoch 19, Batch 676, LR 0.000056 Loss 6.045558, Accuracy 83.047%\n",
      "Epoch 19, Batch 677, LR 0.000056 Loss 6.046056, Accuracy 83.048%\n",
      "Epoch 19, Batch 678, LR 0.000056 Loss 6.044851, Accuracy 83.056%\n",
      "Epoch 19, Batch 679, LR 0.000056 Loss 6.044452, Accuracy 83.056%\n",
      "Epoch 19, Batch 680, LR 0.000056 Loss 6.043636, Accuracy 83.062%\n",
      "Epoch 19, Batch 681, LR 0.000056 Loss 6.042767, Accuracy 83.068%\n",
      "Epoch 19, Batch 682, LR 0.000056 Loss 6.041776, Accuracy 83.070%\n",
      "Epoch 19, Batch 683, LR 0.000056 Loss 6.041819, Accuracy 83.070%\n",
      "Epoch 19, Batch 684, LR 0.000056 Loss 6.041745, Accuracy 83.071%\n",
      "Epoch 19, Batch 685, LR 0.000056 Loss 6.041402, Accuracy 83.070%\n",
      "Epoch 19, Batch 686, LR 0.000056 Loss 6.040064, Accuracy 83.079%\n",
      "Epoch 19, Batch 687, LR 0.000056 Loss 6.040226, Accuracy 83.083%\n",
      "Epoch 19, Batch 688, LR 0.000056 Loss 6.039417, Accuracy 83.088%\n",
      "Epoch 19, Batch 689, LR 0.000056 Loss 6.039954, Accuracy 83.093%\n",
      "Epoch 19, Batch 690, LR 0.000056 Loss 6.040005, Accuracy 83.088%\n",
      "Epoch 19, Batch 691, LR 0.000056 Loss 6.040682, Accuracy 83.088%\n",
      "Epoch 19, Batch 692, LR 0.000056 Loss 6.041205, Accuracy 83.085%\n",
      "Epoch 19, Batch 693, LR 0.000056 Loss 6.042288, Accuracy 83.079%\n",
      "Epoch 19, Batch 694, LR 0.000056 Loss 6.041917, Accuracy 83.077%\n",
      "Epoch 19, Batch 695, LR 0.000056 Loss 6.041369, Accuracy 83.082%\n",
      "Epoch 19, Batch 696, LR 0.000056 Loss 6.040170, Accuracy 83.091%\n",
      "Epoch 19, Batch 697, LR 0.000056 Loss 6.039898, Accuracy 83.093%\n",
      "Epoch 19, Batch 698, LR 0.000056 Loss 6.039135, Accuracy 83.096%\n",
      "Epoch 19, Batch 699, LR 0.000055 Loss 6.040015, Accuracy 83.087%\n",
      "Epoch 19, Batch 700, LR 0.000055 Loss 6.040236, Accuracy 83.090%\n",
      "Epoch 19, Batch 701, LR 0.000055 Loss 6.039468, Accuracy 83.097%\n",
      "Epoch 19, Batch 702, LR 0.000055 Loss 6.040168, Accuracy 83.086%\n",
      "Epoch 19, Batch 703, LR 0.000055 Loss 6.039710, Accuracy 83.087%\n",
      "Epoch 19, Batch 704, LR 0.000055 Loss 6.040214, Accuracy 83.083%\n",
      "Epoch 19, Batch 705, LR 0.000055 Loss 6.040571, Accuracy 83.081%\n",
      "Epoch 19, Batch 706, LR 0.000055 Loss 6.041122, Accuracy 83.074%\n",
      "Epoch 19, Batch 707, LR 0.000055 Loss 6.040990, Accuracy 83.075%\n",
      "Epoch 19, Batch 708, LR 0.000055 Loss 6.041853, Accuracy 83.074%\n",
      "Epoch 19, Batch 709, LR 0.000055 Loss 6.041259, Accuracy 83.079%\n",
      "Epoch 19, Batch 710, LR 0.000055 Loss 6.041614, Accuracy 83.074%\n",
      "Epoch 19, Batch 711, LR 0.000055 Loss 6.040503, Accuracy 83.081%\n",
      "Epoch 19, Batch 712, LR 0.000055 Loss 6.039533, Accuracy 83.082%\n",
      "Epoch 19, Batch 713, LR 0.000055 Loss 6.037947, Accuracy 83.095%\n",
      "Epoch 19, Batch 714, LR 0.000055 Loss 6.036886, Accuracy 83.098%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 715, LR 0.000055 Loss 6.036506, Accuracy 83.098%\n",
      "Epoch 19, Batch 716, LR 0.000055 Loss 6.036907, Accuracy 83.093%\n",
      "Epoch 19, Batch 717, LR 0.000055 Loss 6.037045, Accuracy 83.091%\n",
      "Epoch 19, Batch 718, LR 0.000055 Loss 6.036684, Accuracy 83.093%\n",
      "Epoch 19, Batch 719, LR 0.000055 Loss 6.036594, Accuracy 83.094%\n",
      "Epoch 19, Batch 720, LR 0.000055 Loss 6.037049, Accuracy 83.092%\n",
      "Epoch 19, Batch 721, LR 0.000055 Loss 6.038863, Accuracy 83.079%\n",
      "Epoch 19, Batch 722, LR 0.000055 Loss 6.037511, Accuracy 83.084%\n",
      "Epoch 19, Batch 723, LR 0.000055 Loss 6.037327, Accuracy 83.079%\n",
      "Epoch 19, Batch 724, LR 0.000055 Loss 6.037073, Accuracy 83.082%\n",
      "Epoch 19, Batch 725, LR 0.000055 Loss 6.036044, Accuracy 83.091%\n",
      "Epoch 19, Batch 726, LR 0.000055 Loss 6.035163, Accuracy 83.089%\n",
      "Epoch 19, Batch 727, LR 0.000055 Loss 6.035553, Accuracy 83.088%\n",
      "Epoch 19, Batch 728, LR 0.000055 Loss 6.034417, Accuracy 83.094%\n",
      "Epoch 19, Batch 729, LR 0.000055 Loss 6.034509, Accuracy 83.096%\n",
      "Epoch 19, Batch 730, LR 0.000055 Loss 6.034405, Accuracy 83.101%\n",
      "Epoch 19, Batch 731, LR 0.000055 Loss 6.034856, Accuracy 83.097%\n",
      "Epoch 19, Batch 732, LR 0.000055 Loss 6.035663, Accuracy 83.092%\n",
      "Epoch 19, Batch 733, LR 0.000055 Loss 6.037104, Accuracy 83.092%\n",
      "Epoch 19, Batch 734, LR 0.000055 Loss 6.036170, Accuracy 83.098%\n",
      "Epoch 19, Batch 735, LR 0.000055 Loss 6.035740, Accuracy 83.101%\n",
      "Epoch 19, Batch 736, LR 0.000055 Loss 6.035617, Accuracy 83.103%\n",
      "Epoch 19, Batch 737, LR 0.000055 Loss 6.035540, Accuracy 83.103%\n",
      "Epoch 19, Batch 738, LR 0.000055 Loss 6.035932, Accuracy 83.101%\n",
      "Epoch 19, Batch 739, LR 0.000055 Loss 6.036440, Accuracy 83.104%\n",
      "Epoch 19, Batch 740, LR 0.000055 Loss 6.037067, Accuracy 83.104%\n",
      "Epoch 19, Batch 741, LR 0.000055 Loss 6.036116, Accuracy 83.108%\n",
      "Epoch 19, Batch 742, LR 0.000055 Loss 6.037304, Accuracy 83.103%\n",
      "Epoch 19, Batch 743, LR 0.000055 Loss 6.036777, Accuracy 83.103%\n",
      "Epoch 19, Batch 744, LR 0.000055 Loss 6.037255, Accuracy 83.107%\n",
      "Epoch 19, Batch 745, LR 0.000055 Loss 6.037269, Accuracy 83.107%\n",
      "Epoch 19, Batch 746, LR 0.000055 Loss 6.037527, Accuracy 83.106%\n",
      "Epoch 19, Batch 747, LR 0.000055 Loss 6.037190, Accuracy 83.113%\n",
      "Epoch 19, Batch 748, LR 0.000055 Loss 6.036582, Accuracy 83.120%\n",
      "Epoch 19, Batch 749, LR 0.000055 Loss 6.036909, Accuracy 83.117%\n",
      "Epoch 19, Batch 750, LR 0.000055 Loss 6.036669, Accuracy 83.120%\n",
      "Epoch 19, Batch 751, LR 0.000055 Loss 6.036155, Accuracy 83.121%\n",
      "Epoch 19, Batch 752, LR 0.000055 Loss 6.036941, Accuracy 83.115%\n",
      "Epoch 19, Batch 753, LR 0.000055 Loss 6.036387, Accuracy 83.121%\n",
      "Epoch 19, Batch 754, LR 0.000055 Loss 6.035083, Accuracy 83.129%\n",
      "Epoch 19, Batch 755, LR 0.000055 Loss 6.035723, Accuracy 83.120%\n",
      "Epoch 19, Batch 756, LR 0.000055 Loss 6.035569, Accuracy 83.120%\n",
      "Epoch 19, Batch 757, LR 0.000055 Loss 6.033628, Accuracy 83.130%\n",
      "Epoch 19, Batch 758, LR 0.000055 Loss 6.033667, Accuracy 83.128%\n",
      "Epoch 19, Batch 759, LR 0.000055 Loss 6.033475, Accuracy 83.127%\n",
      "Epoch 19, Batch 760, LR 0.000055 Loss 6.032762, Accuracy 83.131%\n",
      "Epoch 19, Batch 761, LR 0.000055 Loss 6.032382, Accuracy 83.135%\n",
      "Epoch 19, Batch 762, LR 0.000055 Loss 6.031084, Accuracy 83.140%\n",
      "Epoch 19, Batch 763, LR 0.000055 Loss 6.031528, Accuracy 83.139%\n",
      "Epoch 19, Batch 764, LR 0.000055 Loss 6.031195, Accuracy 83.137%\n",
      "Epoch 19, Batch 765, LR 0.000055 Loss 6.031432, Accuracy 83.142%\n",
      "Epoch 19, Batch 766, LR 0.000055 Loss 6.031227, Accuracy 83.144%\n",
      "Epoch 19, Batch 767, LR 0.000055 Loss 6.031053, Accuracy 83.147%\n",
      "Epoch 19, Batch 768, LR 0.000055 Loss 6.030859, Accuracy 83.145%\n",
      "Epoch 19, Batch 769, LR 0.000055 Loss 6.030725, Accuracy 83.144%\n",
      "Epoch 19, Batch 770, LR 0.000055 Loss 6.030860, Accuracy 83.150%\n",
      "Epoch 19, Batch 771, LR 0.000055 Loss 6.030373, Accuracy 83.152%\n",
      "Epoch 19, Batch 772, LR 0.000055 Loss 6.031202, Accuracy 83.148%\n",
      "Epoch 19, Batch 773, LR 0.000055 Loss 6.031338, Accuracy 83.151%\n",
      "Epoch 19, Batch 774, LR 0.000055 Loss 6.030634, Accuracy 83.152%\n",
      "Epoch 19, Batch 775, LR 0.000055 Loss 6.030434, Accuracy 83.149%\n",
      "Epoch 19, Batch 776, LR 0.000055 Loss 6.031400, Accuracy 83.145%\n",
      "Epoch 19, Batch 777, LR 0.000055 Loss 6.032006, Accuracy 83.140%\n",
      "Epoch 19, Batch 778, LR 0.000055 Loss 6.033111, Accuracy 83.138%\n",
      "Epoch 19, Batch 779, LR 0.000055 Loss 6.032860, Accuracy 83.141%\n",
      "Epoch 19, Batch 780, LR 0.000055 Loss 6.031680, Accuracy 83.141%\n",
      "Epoch 19, Batch 781, LR 0.000055 Loss 6.031548, Accuracy 83.143%\n",
      "Epoch 19, Batch 782, LR 0.000055 Loss 6.031669, Accuracy 83.142%\n",
      "Epoch 19, Batch 783, LR 0.000055 Loss 6.031481, Accuracy 83.147%\n",
      "Epoch 19, Batch 784, LR 0.000055 Loss 6.031970, Accuracy 83.151%\n",
      "Epoch 19, Batch 785, LR 0.000055 Loss 6.031415, Accuracy 83.153%\n",
      "Epoch 19, Batch 786, LR 0.000055 Loss 6.030846, Accuracy 83.154%\n",
      "Epoch 19, Batch 787, LR 0.000055 Loss 6.031012, Accuracy 83.156%\n",
      "Epoch 19, Batch 788, LR 0.000055 Loss 6.031110, Accuracy 83.158%\n",
      "Epoch 19, Batch 789, LR 0.000055 Loss 6.031545, Accuracy 83.152%\n",
      "Epoch 19, Batch 790, LR 0.000055 Loss 6.031191, Accuracy 83.153%\n",
      "Epoch 19, Batch 791, LR 0.000055 Loss 6.030202, Accuracy 83.158%\n",
      "Epoch 19, Batch 792, LR 0.000055 Loss 6.029092, Accuracy 83.161%\n",
      "Epoch 19, Batch 793, LR 0.000055 Loss 6.030123, Accuracy 83.157%\n",
      "Epoch 19, Batch 794, LR 0.000055 Loss 6.030789, Accuracy 83.152%\n",
      "Epoch 19, Batch 795, LR 0.000055 Loss 6.030498, Accuracy 83.152%\n",
      "Epoch 19, Batch 796, LR 0.000055 Loss 6.030717, Accuracy 83.150%\n",
      "Epoch 19, Batch 797, LR 0.000055 Loss 6.031162, Accuracy 83.150%\n",
      "Epoch 19, Batch 798, LR 0.000055 Loss 6.030575, Accuracy 83.148%\n",
      "Epoch 19, Batch 799, LR 0.000055 Loss 6.030139, Accuracy 83.151%\n",
      "Epoch 19, Batch 800, LR 0.000055 Loss 6.030122, Accuracy 83.153%\n",
      "Epoch 19, Batch 801, LR 0.000055 Loss 6.030429, Accuracy 83.151%\n",
      "Epoch 19, Batch 802, LR 0.000055 Loss 6.029984, Accuracy 83.153%\n",
      "Epoch 19, Batch 803, LR 0.000055 Loss 6.029929, Accuracy 83.155%\n",
      "Epoch 19, Batch 804, LR 0.000055 Loss 6.029774, Accuracy 83.155%\n",
      "Epoch 19, Batch 805, LR 0.000055 Loss 6.029034, Accuracy 83.158%\n",
      "Epoch 19, Batch 806, LR 0.000055 Loss 6.028517, Accuracy 83.157%\n",
      "Epoch 19, Batch 807, LR 0.000055 Loss 6.029308, Accuracy 83.149%\n",
      "Epoch 19, Batch 808, LR 0.000055 Loss 6.029351, Accuracy 83.145%\n",
      "Epoch 19, Batch 809, LR 0.000055 Loss 6.030254, Accuracy 83.143%\n",
      "Epoch 19, Batch 810, LR 0.000055 Loss 6.029441, Accuracy 83.149%\n",
      "Epoch 19, Batch 811, LR 0.000055 Loss 6.028622, Accuracy 83.147%\n",
      "Epoch 19, Batch 812, LR 0.000055 Loss 6.027093, Accuracy 83.151%\n",
      "Epoch 19, Batch 813, LR 0.000055 Loss 6.027217, Accuracy 83.147%\n",
      "Epoch 19, Batch 814, LR 0.000055 Loss 6.027507, Accuracy 83.146%\n",
      "Epoch 19, Batch 815, LR 0.000055 Loss 6.027116, Accuracy 83.151%\n",
      "Epoch 19, Batch 816, LR 0.000055 Loss 6.026977, Accuracy 83.151%\n",
      "Epoch 19, Batch 817, LR 0.000055 Loss 6.027373, Accuracy 83.140%\n",
      "Epoch 19, Batch 818, LR 0.000055 Loss 6.027759, Accuracy 83.141%\n",
      "Epoch 19, Batch 819, LR 0.000055 Loss 6.027285, Accuracy 83.142%\n",
      "Epoch 19, Batch 820, LR 0.000055 Loss 6.027208, Accuracy 83.144%\n",
      "Epoch 19, Batch 821, LR 0.000055 Loss 6.026853, Accuracy 83.149%\n",
      "Epoch 19, Batch 822, LR 0.000055 Loss 6.026511, Accuracy 83.147%\n",
      "Epoch 19, Batch 823, LR 0.000055 Loss 6.027879, Accuracy 83.142%\n",
      "Epoch 19, Batch 824, LR 0.000055 Loss 6.028215, Accuracy 83.143%\n",
      "Epoch 19, Batch 825, LR 0.000055 Loss 6.028143, Accuracy 83.145%\n",
      "Epoch 19, Batch 826, LR 0.000055 Loss 6.027872, Accuracy 83.145%\n",
      "Epoch 19, Batch 827, LR 0.000055 Loss 6.027528, Accuracy 83.150%\n",
      "Epoch 19, Batch 828, LR 0.000055 Loss 6.027393, Accuracy 83.152%\n",
      "Epoch 19, Batch 829, LR 0.000055 Loss 6.027005, Accuracy 83.155%\n",
      "Epoch 19, Batch 830, LR 0.000055 Loss 6.026804, Accuracy 83.158%\n",
      "Epoch 19, Batch 831, LR 0.000055 Loss 6.026475, Accuracy 83.159%\n",
      "Epoch 19, Batch 832, LR 0.000055 Loss 6.025928, Accuracy 83.160%\n",
      "Epoch 19, Batch 833, LR 0.000055 Loss 6.025309, Accuracy 83.167%\n",
      "Epoch 19, Batch 834, LR 0.000055 Loss 6.025953, Accuracy 83.165%\n",
      "Epoch 19, Batch 835, LR 0.000055 Loss 6.025432, Accuracy 83.162%\n",
      "Epoch 19, Batch 836, LR 0.000055 Loss 6.025484, Accuracy 83.164%\n",
      "Epoch 19, Batch 837, LR 0.000055 Loss 6.024658, Accuracy 83.172%\n",
      "Epoch 19, Batch 838, LR 0.000055 Loss 6.024317, Accuracy 83.175%\n",
      "Epoch 19, Batch 839, LR 0.000055 Loss 6.025166, Accuracy 83.168%\n",
      "Epoch 19, Batch 840, LR 0.000055 Loss 6.025389, Accuracy 83.168%\n",
      "Epoch 19, Batch 841, LR 0.000055 Loss 6.025601, Accuracy 83.167%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 842, LR 0.000055 Loss 6.025381, Accuracy 83.166%\n",
      "Epoch 19, Batch 843, LR 0.000055 Loss 6.024902, Accuracy 83.167%\n",
      "Epoch 19, Batch 844, LR 0.000055 Loss 6.024261, Accuracy 83.172%\n",
      "Epoch 19, Batch 845, LR 0.000055 Loss 6.024494, Accuracy 83.170%\n",
      "Epoch 19, Batch 846, LR 0.000055 Loss 6.023637, Accuracy 83.170%\n",
      "Epoch 19, Batch 847, LR 0.000055 Loss 6.023636, Accuracy 83.175%\n",
      "Epoch 19, Batch 848, LR 0.000055 Loss 6.024687, Accuracy 83.172%\n",
      "Epoch 19, Batch 849, LR 0.000055 Loss 6.025471, Accuracy 83.168%\n",
      "Epoch 19, Batch 850, LR 0.000055 Loss 6.025821, Accuracy 83.168%\n",
      "Epoch 19, Batch 851, LR 0.000055 Loss 6.025689, Accuracy 83.174%\n",
      "Epoch 19, Batch 852, LR 0.000055 Loss 6.025710, Accuracy 83.173%\n",
      "Epoch 19, Batch 853, LR 0.000055 Loss 6.025217, Accuracy 83.173%\n",
      "Epoch 19, Batch 854, LR 0.000055 Loss 6.025857, Accuracy 83.171%\n",
      "Epoch 19, Batch 855, LR 0.000055 Loss 6.026000, Accuracy 83.173%\n",
      "Epoch 19, Batch 856, LR 0.000055 Loss 6.026068, Accuracy 83.172%\n",
      "Epoch 19, Batch 857, LR 0.000055 Loss 6.026201, Accuracy 83.176%\n",
      "Epoch 19, Batch 858, LR 0.000055 Loss 6.026342, Accuracy 83.171%\n",
      "Epoch 19, Batch 859, LR 0.000055 Loss 6.025448, Accuracy 83.174%\n",
      "Epoch 19, Batch 860, LR 0.000055 Loss 6.025080, Accuracy 83.173%\n",
      "Epoch 19, Batch 861, LR 0.000055 Loss 6.025601, Accuracy 83.172%\n",
      "Epoch 19, Batch 862, LR 0.000055 Loss 6.025287, Accuracy 83.170%\n",
      "Epoch 19, Batch 863, LR 0.000055 Loss 6.024770, Accuracy 83.176%\n",
      "Epoch 19, Batch 864, LR 0.000055 Loss 6.024040, Accuracy 83.181%\n",
      "Epoch 19, Batch 865, LR 0.000055 Loss 6.023328, Accuracy 83.184%\n",
      "Epoch 19, Batch 866, LR 0.000055 Loss 6.023440, Accuracy 83.185%\n",
      "Epoch 19, Batch 867, LR 0.000055 Loss 6.023427, Accuracy 83.188%\n",
      "Epoch 19, Batch 868, LR 0.000055 Loss 6.022839, Accuracy 83.193%\n",
      "Epoch 19, Batch 869, LR 0.000055 Loss 6.023145, Accuracy 83.190%\n",
      "Epoch 19, Batch 870, LR 0.000055 Loss 6.023532, Accuracy 83.190%\n",
      "Epoch 19, Batch 871, LR 0.000055 Loss 6.024817, Accuracy 83.186%\n",
      "Epoch 19, Batch 872, LR 0.000055 Loss 6.023698, Accuracy 83.190%\n",
      "Epoch 19, Batch 873, LR 0.000055 Loss 6.023380, Accuracy 83.196%\n",
      "Epoch 19, Batch 874, LR 0.000055 Loss 6.022902, Accuracy 83.194%\n",
      "Epoch 19, Batch 875, LR 0.000055 Loss 6.022883, Accuracy 83.193%\n",
      "Epoch 19, Batch 876, LR 0.000055 Loss 6.023056, Accuracy 83.192%\n",
      "Epoch 19, Batch 877, LR 0.000055 Loss 6.022034, Accuracy 83.192%\n",
      "Epoch 19, Batch 878, LR 0.000055 Loss 6.022139, Accuracy 83.191%\n",
      "Epoch 19, Batch 879, LR 0.000055 Loss 6.021876, Accuracy 83.192%\n",
      "Epoch 19, Batch 880, LR 0.000055 Loss 6.021746, Accuracy 83.189%\n",
      "Epoch 19, Batch 881, LR 0.000055 Loss 6.022679, Accuracy 83.183%\n",
      "Epoch 19, Batch 882, LR 0.000055 Loss 6.021849, Accuracy 83.188%\n",
      "Epoch 19, Batch 883, LR 0.000055 Loss 6.022112, Accuracy 83.189%\n",
      "Epoch 19, Batch 884, LR 0.000055 Loss 6.022264, Accuracy 83.186%\n",
      "Epoch 19, Batch 885, LR 0.000055 Loss 6.022249, Accuracy 83.187%\n",
      "Epoch 19, Batch 886, LR 0.000055 Loss 6.022630, Accuracy 83.188%\n",
      "Epoch 19, Batch 887, LR 0.000055 Loss 6.023411, Accuracy 83.179%\n",
      "Epoch 19, Batch 888, LR 0.000055 Loss 6.023707, Accuracy 83.176%\n",
      "Epoch 19, Batch 889, LR 0.000055 Loss 6.023145, Accuracy 83.182%\n",
      "Epoch 19, Batch 890, LR 0.000055 Loss 6.023074, Accuracy 83.179%\n",
      "Epoch 19, Batch 891, LR 0.000055 Loss 6.023037, Accuracy 83.177%\n",
      "Epoch 19, Batch 892, LR 0.000055 Loss 6.022604, Accuracy 83.185%\n",
      "Epoch 19, Batch 893, LR 0.000055 Loss 6.022529, Accuracy 83.185%\n",
      "Epoch 19, Batch 894, LR 0.000055 Loss 6.022512, Accuracy 83.191%\n",
      "Epoch 19, Batch 895, LR 0.000055 Loss 6.022095, Accuracy 83.192%\n",
      "Epoch 19, Batch 896, LR 0.000055 Loss 6.021837, Accuracy 83.196%\n",
      "Epoch 19, Batch 897, LR 0.000055 Loss 6.020683, Accuracy 83.201%\n",
      "Epoch 19, Batch 898, LR 0.000055 Loss 6.021362, Accuracy 83.198%\n",
      "Epoch 19, Batch 899, LR 0.000055 Loss 6.021361, Accuracy 83.200%\n",
      "Epoch 19, Batch 900, LR 0.000055 Loss 6.020438, Accuracy 83.203%\n",
      "Epoch 19, Batch 901, LR 0.000055 Loss 6.020524, Accuracy 83.204%\n",
      "Epoch 19, Batch 902, LR 0.000055 Loss 6.020054, Accuracy 83.207%\n",
      "Epoch 19, Batch 903, LR 0.000055 Loss 6.019498, Accuracy 83.208%\n",
      "Epoch 19, Batch 904, LR 0.000055 Loss 6.019313, Accuracy 83.213%\n",
      "Epoch 19, Batch 905, LR 0.000055 Loss 6.018831, Accuracy 83.219%\n",
      "Epoch 19, Batch 906, LR 0.000055 Loss 6.018539, Accuracy 83.222%\n",
      "Epoch 19, Batch 907, LR 0.000055 Loss 6.018259, Accuracy 83.220%\n",
      "Epoch 19, Batch 908, LR 0.000055 Loss 6.018010, Accuracy 83.223%\n",
      "Epoch 19, Batch 909, LR 0.000055 Loss 6.017171, Accuracy 83.227%\n",
      "Epoch 19, Batch 910, LR 0.000055 Loss 6.017135, Accuracy 83.227%\n",
      "Epoch 19, Batch 911, LR 0.000055 Loss 6.018192, Accuracy 83.218%\n",
      "Epoch 19, Batch 912, LR 0.000055 Loss 6.018051, Accuracy 83.219%\n",
      "Epoch 19, Batch 913, LR 0.000055 Loss 6.017934, Accuracy 83.223%\n",
      "Epoch 19, Batch 914, LR 0.000055 Loss 6.018473, Accuracy 83.218%\n",
      "Epoch 19, Batch 915, LR 0.000055 Loss 6.018257, Accuracy 83.221%\n",
      "Epoch 19, Batch 916, LR 0.000055 Loss 6.018097, Accuracy 83.223%\n",
      "Epoch 19, Batch 917, LR 0.000055 Loss 6.017878, Accuracy 83.222%\n",
      "Epoch 19, Batch 918, LR 0.000055 Loss 6.017811, Accuracy 83.223%\n",
      "Epoch 19, Batch 919, LR 0.000055 Loss 6.018003, Accuracy 83.221%\n",
      "Epoch 19, Batch 920, LR 0.000055 Loss 6.017857, Accuracy 83.223%\n",
      "Epoch 19, Batch 921, LR 0.000055 Loss 6.017361, Accuracy 83.223%\n",
      "Epoch 19, Batch 922, LR 0.000055 Loss 6.018790, Accuracy 83.217%\n",
      "Epoch 19, Batch 923, LR 0.000055 Loss 6.018276, Accuracy 83.220%\n",
      "Epoch 19, Batch 924, LR 0.000055 Loss 6.018300, Accuracy 83.219%\n",
      "Epoch 19, Batch 925, LR 0.000055 Loss 6.018432, Accuracy 83.219%\n",
      "Epoch 19, Batch 926, LR 0.000055 Loss 6.018711, Accuracy 83.220%\n",
      "Epoch 19, Batch 927, LR 0.000055 Loss 6.018523, Accuracy 83.220%\n",
      "Epoch 19, Batch 928, LR 0.000055 Loss 6.017520, Accuracy 83.228%\n",
      "Epoch 19, Batch 929, LR 0.000055 Loss 6.016777, Accuracy 83.236%\n",
      "Epoch 19, Batch 930, LR 0.000055 Loss 6.016459, Accuracy 83.238%\n",
      "Epoch 19, Batch 931, LR 0.000055 Loss 6.015468, Accuracy 83.243%\n",
      "Epoch 19, Batch 932, LR 0.000055 Loss 6.015458, Accuracy 83.245%\n",
      "Epoch 19, Batch 933, LR 0.000055 Loss 6.015199, Accuracy 83.245%\n",
      "Epoch 19, Batch 934, LR 0.000055 Loss 6.015337, Accuracy 83.246%\n",
      "Epoch 19, Batch 935, LR 0.000055 Loss 6.015873, Accuracy 83.240%\n",
      "Epoch 19, Batch 936, LR 0.000055 Loss 6.015523, Accuracy 83.240%\n",
      "Epoch 19, Batch 937, LR 0.000055 Loss 6.015673, Accuracy 83.238%\n",
      "Epoch 19, Batch 938, LR 0.000055 Loss 6.015758, Accuracy 83.236%\n",
      "Epoch 19, Batch 939, LR 0.000055 Loss 6.014869, Accuracy 83.243%\n",
      "Epoch 19, Batch 940, LR 0.000055 Loss 6.015121, Accuracy 83.239%\n",
      "Epoch 19, Batch 941, LR 0.000055 Loss 6.016040, Accuracy 83.231%\n",
      "Epoch 19, Batch 942, LR 0.000055 Loss 6.015526, Accuracy 83.233%\n",
      "Epoch 19, Batch 943, LR 0.000055 Loss 6.015268, Accuracy 83.229%\n",
      "Epoch 19, Batch 944, LR 0.000055 Loss 6.015135, Accuracy 83.225%\n",
      "Epoch 19, Batch 945, LR 0.000055 Loss 6.014866, Accuracy 83.225%\n",
      "Epoch 19, Batch 946, LR 0.000055 Loss 6.014047, Accuracy 83.228%\n",
      "Epoch 19, Batch 947, LR 0.000055 Loss 6.013011, Accuracy 83.235%\n",
      "Epoch 19, Batch 948, LR 0.000055 Loss 6.012276, Accuracy 83.239%\n",
      "Epoch 19, Batch 949, LR 0.000055 Loss 6.012069, Accuracy 83.236%\n",
      "Epoch 19, Batch 950, LR 0.000055 Loss 6.012216, Accuracy 83.234%\n",
      "Epoch 19, Batch 951, LR 0.000055 Loss 6.012038, Accuracy 83.231%\n",
      "Epoch 19, Batch 952, LR 0.000055 Loss 6.011937, Accuracy 83.231%\n",
      "Epoch 19, Batch 953, LR 0.000054 Loss 6.011582, Accuracy 83.233%\n",
      "Epoch 19, Batch 954, LR 0.000054 Loss 6.010641, Accuracy 83.238%\n",
      "Epoch 19, Batch 955, LR 0.000054 Loss 6.010616, Accuracy 83.236%\n",
      "Epoch 19, Batch 956, LR 0.000054 Loss 6.009643, Accuracy 83.242%\n",
      "Epoch 19, Batch 957, LR 0.000054 Loss 6.009742, Accuracy 83.239%\n",
      "Epoch 19, Batch 958, LR 0.000054 Loss 6.009004, Accuracy 83.238%\n",
      "Epoch 19, Batch 959, LR 0.000054 Loss 6.008466, Accuracy 83.243%\n",
      "Epoch 19, Batch 960, LR 0.000054 Loss 6.008052, Accuracy 83.243%\n",
      "Epoch 19, Batch 961, LR 0.000054 Loss 6.007947, Accuracy 83.243%\n",
      "Epoch 19, Batch 962, LR 0.000054 Loss 6.008233, Accuracy 83.241%\n",
      "Epoch 19, Batch 963, LR 0.000054 Loss 6.008422, Accuracy 83.237%\n",
      "Epoch 19, Batch 964, LR 0.000054 Loss 6.008836, Accuracy 83.237%\n",
      "Epoch 19, Batch 965, LR 0.000054 Loss 6.008192, Accuracy 83.242%\n",
      "Epoch 19, Batch 966, LR 0.000054 Loss 6.007765, Accuracy 83.244%\n",
      "Epoch 19, Batch 967, LR 0.000054 Loss 6.006969, Accuracy 83.247%\n",
      "Epoch 19, Batch 968, LR 0.000054 Loss 6.007225, Accuracy 83.248%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 969, LR 0.000054 Loss 6.007090, Accuracy 83.244%\n",
      "Epoch 19, Batch 970, LR 0.000054 Loss 6.007238, Accuracy 83.247%\n",
      "Epoch 19, Batch 971, LR 0.000054 Loss 6.007044, Accuracy 83.249%\n",
      "Epoch 19, Batch 972, LR 0.000054 Loss 6.006705, Accuracy 83.254%\n",
      "Epoch 19, Batch 973, LR 0.000054 Loss 6.006829, Accuracy 83.253%\n",
      "Epoch 19, Batch 974, LR 0.000054 Loss 6.006298, Accuracy 83.253%\n",
      "Epoch 19, Batch 975, LR 0.000054 Loss 6.006594, Accuracy 83.255%\n",
      "Epoch 19, Batch 976, LR 0.000054 Loss 6.006147, Accuracy 83.258%\n",
      "Epoch 19, Batch 977, LR 0.000054 Loss 6.006054, Accuracy 83.260%\n",
      "Epoch 19, Batch 978, LR 0.000054 Loss 6.006475, Accuracy 83.257%\n",
      "Epoch 19, Batch 979, LR 0.000054 Loss 6.005345, Accuracy 83.265%\n",
      "Epoch 19, Batch 980, LR 0.000054 Loss 6.005394, Accuracy 83.265%\n",
      "Epoch 19, Batch 981, LR 0.000054 Loss 6.005143, Accuracy 83.268%\n",
      "Epoch 19, Batch 982, LR 0.000054 Loss 6.005453, Accuracy 83.263%\n",
      "Epoch 19, Batch 983, LR 0.000054 Loss 6.005561, Accuracy 83.265%\n",
      "Epoch 19, Batch 984, LR 0.000054 Loss 6.004947, Accuracy 83.268%\n",
      "Epoch 19, Batch 985, LR 0.000054 Loss 6.005084, Accuracy 83.267%\n",
      "Epoch 19, Batch 986, LR 0.000054 Loss 6.005108, Accuracy 83.267%\n",
      "Epoch 19, Batch 987, LR 0.000054 Loss 6.005370, Accuracy 83.264%\n",
      "Epoch 19, Batch 988, LR 0.000054 Loss 6.005344, Accuracy 83.264%\n",
      "Epoch 19, Batch 989, LR 0.000054 Loss 6.005439, Accuracy 83.264%\n",
      "Epoch 19, Batch 990, LR 0.000054 Loss 6.005350, Accuracy 83.259%\n",
      "Epoch 19, Batch 991, LR 0.000054 Loss 6.004327, Accuracy 83.263%\n",
      "Epoch 19, Batch 992, LR 0.000054 Loss 6.004405, Accuracy 83.266%\n",
      "Epoch 19, Batch 993, LR 0.000054 Loss 6.004202, Accuracy 83.267%\n",
      "Epoch 19, Batch 994, LR 0.000054 Loss 6.004981, Accuracy 83.264%\n",
      "Epoch 19, Batch 995, LR 0.000054 Loss 6.005234, Accuracy 83.259%\n",
      "Epoch 19, Batch 996, LR 0.000054 Loss 6.004809, Accuracy 83.262%\n",
      "Epoch 19, Batch 997, LR 0.000054 Loss 6.004812, Accuracy 83.260%\n",
      "Epoch 19, Batch 998, LR 0.000054 Loss 6.004993, Accuracy 83.256%\n",
      "Epoch 19, Batch 999, LR 0.000054 Loss 6.004331, Accuracy 83.262%\n",
      "Epoch 19, Batch 1000, LR 0.000054 Loss 6.004227, Accuracy 83.263%\n",
      "Epoch 19, Batch 1001, LR 0.000054 Loss 6.003966, Accuracy 83.263%\n",
      "Epoch 19, Batch 1002, LR 0.000054 Loss 6.003239, Accuracy 83.264%\n",
      "Epoch 19, Batch 1003, LR 0.000054 Loss 6.002768, Accuracy 83.268%\n",
      "Epoch 19, Batch 1004, LR 0.000054 Loss 6.003722, Accuracy 83.262%\n",
      "Epoch 19, Batch 1005, LR 0.000054 Loss 6.003930, Accuracy 83.266%\n",
      "Epoch 19, Batch 1006, LR 0.000054 Loss 6.004231, Accuracy 83.267%\n",
      "Epoch 19, Batch 1007, LR 0.000054 Loss 6.002899, Accuracy 83.273%\n",
      "Epoch 19, Batch 1008, LR 0.000054 Loss 6.002510, Accuracy 83.271%\n",
      "Epoch 19, Batch 1009, LR 0.000054 Loss 6.002243, Accuracy 83.272%\n",
      "Epoch 19, Batch 1010, LR 0.000054 Loss 6.001827, Accuracy 83.272%\n",
      "Epoch 19, Batch 1011, LR 0.000054 Loss 6.001933, Accuracy 83.271%\n",
      "Epoch 19, Batch 1012, LR 0.000054 Loss 6.001938, Accuracy 83.273%\n",
      "Epoch 19, Batch 1013, LR 0.000054 Loss 6.001571, Accuracy 83.273%\n",
      "Epoch 19, Batch 1014, LR 0.000054 Loss 6.001327, Accuracy 83.273%\n",
      "Epoch 19, Batch 1015, LR 0.000054 Loss 6.001500, Accuracy 83.272%\n",
      "Epoch 19, Batch 1016, LR 0.000054 Loss 6.000971, Accuracy 83.272%\n",
      "Epoch 19, Batch 1017, LR 0.000054 Loss 6.001355, Accuracy 83.270%\n",
      "Epoch 19, Batch 1018, LR 0.000054 Loss 6.001294, Accuracy 83.269%\n",
      "Epoch 19, Batch 1019, LR 0.000054 Loss 6.001834, Accuracy 83.268%\n",
      "Epoch 19, Batch 1020, LR 0.000054 Loss 6.001923, Accuracy 83.267%\n",
      "Epoch 19, Batch 1021, LR 0.000054 Loss 6.001768, Accuracy 83.266%\n",
      "Epoch 19, Batch 1022, LR 0.000054 Loss 6.002324, Accuracy 83.268%\n",
      "Epoch 19, Batch 1023, LR 0.000054 Loss 6.002703, Accuracy 83.265%\n",
      "Epoch 19, Batch 1024, LR 0.000054 Loss 6.002444, Accuracy 83.266%\n",
      "Epoch 19, Batch 1025, LR 0.000054 Loss 6.002639, Accuracy 83.266%\n",
      "Epoch 19, Batch 1026, LR 0.000054 Loss 6.002736, Accuracy 83.266%\n",
      "Epoch 19, Batch 1027, LR 0.000054 Loss 6.002443, Accuracy 83.267%\n",
      "Epoch 19, Batch 1028, LR 0.000054 Loss 6.002917, Accuracy 83.266%\n",
      "Epoch 19, Batch 1029, LR 0.000054 Loss 6.002817, Accuracy 83.267%\n",
      "Epoch 19, Batch 1030, LR 0.000054 Loss 6.002943, Accuracy 83.268%\n",
      "Epoch 19, Batch 1031, LR 0.000054 Loss 6.003310, Accuracy 83.266%\n",
      "Epoch 19, Batch 1032, LR 0.000054 Loss 6.002879, Accuracy 83.270%\n",
      "Epoch 19, Batch 1033, LR 0.000054 Loss 6.002990, Accuracy 83.270%\n",
      "Epoch 19, Batch 1034, LR 0.000054 Loss 6.003400, Accuracy 83.267%\n",
      "Epoch 19, Batch 1035, LR 0.000054 Loss 6.003313, Accuracy 83.266%\n",
      "Epoch 19, Batch 1036, LR 0.000054 Loss 6.003067, Accuracy 83.268%\n",
      "Epoch 19, Batch 1037, LR 0.000054 Loss 6.002948, Accuracy 83.271%\n",
      "Epoch 19, Batch 1038, LR 0.000054 Loss 6.002271, Accuracy 83.274%\n",
      "Epoch 19, Batch 1039, LR 0.000054 Loss 6.002066, Accuracy 83.279%\n",
      "Epoch 19, Batch 1040, LR 0.000054 Loss 6.002338, Accuracy 83.278%\n",
      "Epoch 19, Batch 1041, LR 0.000054 Loss 6.002173, Accuracy 83.279%\n",
      "Epoch 19, Batch 1042, LR 0.000054 Loss 6.002364, Accuracy 83.276%\n",
      "Epoch 19, Batch 1043, LR 0.000054 Loss 6.002138, Accuracy 83.272%\n",
      "Epoch 19, Batch 1044, LR 0.000054 Loss 6.002138, Accuracy 83.274%\n",
      "Epoch 19, Batch 1045, LR 0.000054 Loss 6.002289, Accuracy 83.275%\n",
      "Epoch 19, Batch 1046, LR 0.000054 Loss 6.001582, Accuracy 83.279%\n",
      "Epoch 19, Batch 1047, LR 0.000054 Loss 6.002083, Accuracy 83.281%\n",
      "Epoch 19, Loss (train set) 6.002083, Accuracy (train set) 83.281%\n",
      "Epoch 19, Accuracy (validation set) 63.654%\n",
      "Epoch 19, EER (test set) 5.627%\n",
      "Epoch 20, Batch 1, LR 0.000054 Loss 5.930224, Accuracy 85.156%\n",
      "Epoch 20, Batch 2, LR 0.000054 Loss 5.850719, Accuracy 83.594%\n",
      "Epoch 20, Batch 3, LR 0.000054 Loss 5.508099, Accuracy 84.896%\n",
      "Epoch 20, Batch 4, LR 0.000054 Loss 5.727984, Accuracy 84.375%\n",
      "Epoch 20, Batch 5, LR 0.000054 Loss 5.752263, Accuracy 83.906%\n",
      "Epoch 20, Batch 6, LR 0.000054 Loss 5.749263, Accuracy 83.724%\n",
      "Epoch 20, Batch 7, LR 0.000054 Loss 5.740337, Accuracy 84.040%\n",
      "Epoch 20, Batch 8, LR 0.000054 Loss 5.713729, Accuracy 84.082%\n",
      "Epoch 20, Batch 9, LR 0.000054 Loss 5.731188, Accuracy 83.941%\n",
      "Epoch 20, Batch 10, LR 0.000054 Loss 5.765272, Accuracy 83.828%\n",
      "Epoch 20, Batch 11, LR 0.000054 Loss 5.734925, Accuracy 84.020%\n",
      "Epoch 20, Batch 12, LR 0.000054 Loss 5.788067, Accuracy 83.789%\n",
      "Epoch 20, Batch 13, LR 0.000054 Loss 5.807126, Accuracy 83.654%\n",
      "Epoch 20, Batch 14, LR 0.000054 Loss 5.820676, Accuracy 83.650%\n",
      "Epoch 20, Batch 15, LR 0.000054 Loss 5.766004, Accuracy 83.906%\n",
      "Epoch 20, Batch 16, LR 0.000054 Loss 5.727031, Accuracy 83.740%\n",
      "Epoch 20, Batch 17, LR 0.000054 Loss 5.779758, Accuracy 83.134%\n",
      "Epoch 20, Batch 18, LR 0.000054 Loss 5.764941, Accuracy 83.420%\n",
      "Epoch 20, Batch 19, LR 0.000054 Loss 5.777131, Accuracy 83.347%\n",
      "Epoch 20, Batch 20, LR 0.000054 Loss 5.841136, Accuracy 83.047%\n",
      "Epoch 20, Batch 21, LR 0.000054 Loss 5.857343, Accuracy 82.887%\n",
      "Epoch 20, Batch 22, LR 0.000054 Loss 5.892442, Accuracy 82.777%\n",
      "Epoch 20, Batch 23, LR 0.000054 Loss 5.911764, Accuracy 82.779%\n",
      "Epoch 20, Batch 24, LR 0.000054 Loss 5.877777, Accuracy 82.845%\n",
      "Epoch 20, Batch 25, LR 0.000054 Loss 5.866348, Accuracy 82.906%\n",
      "Epoch 20, Batch 26, LR 0.000054 Loss 5.880375, Accuracy 82.782%\n",
      "Epoch 20, Batch 27, LR 0.000054 Loss 5.890793, Accuracy 82.726%\n",
      "Epoch 20, Batch 28, LR 0.000054 Loss 5.914369, Accuracy 82.757%\n",
      "Epoch 20, Batch 29, LR 0.000054 Loss 5.912441, Accuracy 82.759%\n",
      "Epoch 20, Batch 30, LR 0.000054 Loss 5.875048, Accuracy 83.021%\n",
      "Epoch 20, Batch 31, LR 0.000054 Loss 5.865357, Accuracy 83.014%\n",
      "Epoch 20, Batch 32, LR 0.000054 Loss 5.860174, Accuracy 83.032%\n",
      "Epoch 20, Batch 33, LR 0.000054 Loss 5.875627, Accuracy 83.097%\n",
      "Epoch 20, Batch 34, LR 0.000054 Loss 5.862723, Accuracy 83.203%\n",
      "Epoch 20, Batch 35, LR 0.000054 Loss 5.870342, Accuracy 83.214%\n",
      "Epoch 20, Batch 36, LR 0.000054 Loss 5.876816, Accuracy 83.203%\n",
      "Epoch 20, Batch 37, LR 0.000054 Loss 5.866533, Accuracy 83.298%\n",
      "Epoch 20, Batch 38, LR 0.000054 Loss 5.860281, Accuracy 83.429%\n",
      "Epoch 20, Batch 39, LR 0.000054 Loss 5.863105, Accuracy 83.494%\n",
      "Epoch 20, Batch 40, LR 0.000054 Loss 5.856967, Accuracy 83.457%\n",
      "Epoch 20, Batch 41, LR 0.000054 Loss 5.854476, Accuracy 83.537%\n",
      "Epoch 20, Batch 42, LR 0.000054 Loss 5.867824, Accuracy 83.371%\n",
      "Epoch 20, Batch 43, LR 0.000054 Loss 5.853314, Accuracy 83.448%\n",
      "Epoch 20, Batch 44, LR 0.000054 Loss 5.856403, Accuracy 83.523%\n",
      "Epoch 20, Batch 45, LR 0.000054 Loss 5.865235, Accuracy 83.507%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 46, LR 0.000054 Loss 5.864686, Accuracy 83.509%\n",
      "Epoch 20, Batch 47, LR 0.000054 Loss 5.852883, Accuracy 83.577%\n",
      "Epoch 20, Batch 48, LR 0.000054 Loss 5.871738, Accuracy 83.545%\n",
      "Epoch 20, Batch 49, LR 0.000054 Loss 5.887566, Accuracy 83.482%\n",
      "Epoch 20, Batch 50, LR 0.000054 Loss 5.883109, Accuracy 83.516%\n",
      "Epoch 20, Batch 51, LR 0.000054 Loss 5.884057, Accuracy 83.548%\n",
      "Epoch 20, Batch 52, LR 0.000054 Loss 5.886904, Accuracy 83.594%\n",
      "Epoch 20, Batch 53, LR 0.000054 Loss 5.895436, Accuracy 83.550%\n",
      "Epoch 20, Batch 54, LR 0.000054 Loss 5.912244, Accuracy 83.464%\n",
      "Epoch 20, Batch 55, LR 0.000054 Loss 5.912950, Accuracy 83.480%\n",
      "Epoch 20, Batch 56, LR 0.000054 Loss 5.902191, Accuracy 83.552%\n",
      "Epoch 20, Batch 57, LR 0.000054 Loss 5.906358, Accuracy 83.525%\n",
      "Epoch 20, Batch 58, LR 0.000054 Loss 5.899182, Accuracy 83.567%\n",
      "Epoch 20, Batch 59, LR 0.000054 Loss 5.898604, Accuracy 83.581%\n",
      "Epoch 20, Batch 60, LR 0.000054 Loss 5.898952, Accuracy 83.659%\n",
      "Epoch 20, Batch 61, LR 0.000054 Loss 5.901736, Accuracy 83.696%\n",
      "Epoch 20, Batch 62, LR 0.000054 Loss 5.902134, Accuracy 83.745%\n",
      "Epoch 20, Batch 63, LR 0.000054 Loss 5.900916, Accuracy 83.829%\n",
      "Epoch 20, Batch 64, LR 0.000054 Loss 5.901994, Accuracy 83.826%\n",
      "Epoch 20, Batch 65, LR 0.000054 Loss 5.909360, Accuracy 83.798%\n",
      "Epoch 20, Batch 66, LR 0.000054 Loss 5.897939, Accuracy 83.819%\n",
      "Epoch 20, Batch 67, LR 0.000054 Loss 5.898247, Accuracy 83.815%\n",
      "Epoch 20, Batch 68, LR 0.000054 Loss 5.896662, Accuracy 83.835%\n",
      "Epoch 20, Batch 69, LR 0.000054 Loss 5.890001, Accuracy 83.854%\n",
      "Epoch 20, Batch 70, LR 0.000054 Loss 5.880969, Accuracy 83.895%\n",
      "Epoch 20, Batch 71, LR 0.000054 Loss 5.890849, Accuracy 83.825%\n",
      "Epoch 20, Batch 72, LR 0.000054 Loss 5.898165, Accuracy 83.767%\n",
      "Epoch 20, Batch 73, LR 0.000054 Loss 5.897357, Accuracy 83.776%\n",
      "Epoch 20, Batch 74, LR 0.000054 Loss 5.892255, Accuracy 83.826%\n",
      "Epoch 20, Batch 75, LR 0.000054 Loss 5.877230, Accuracy 83.958%\n",
      "Epoch 20, Batch 76, LR 0.000054 Loss 5.869557, Accuracy 83.984%\n",
      "Epoch 20, Batch 77, LR 0.000054 Loss 5.862455, Accuracy 84.030%\n",
      "Epoch 20, Batch 78, LR 0.000054 Loss 5.863444, Accuracy 83.984%\n",
      "Epoch 20, Batch 79, LR 0.000054 Loss 5.863861, Accuracy 83.979%\n",
      "Epoch 20, Batch 80, LR 0.000054 Loss 5.866044, Accuracy 83.906%\n",
      "Epoch 20, Batch 81, LR 0.000054 Loss 5.853481, Accuracy 83.931%\n",
      "Epoch 20, Batch 82, LR 0.000054 Loss 5.850949, Accuracy 83.946%\n",
      "Epoch 20, Batch 83, LR 0.000054 Loss 5.856865, Accuracy 83.923%\n",
      "Epoch 20, Batch 84, LR 0.000054 Loss 5.856086, Accuracy 83.938%\n",
      "Epoch 20, Batch 85, LR 0.000054 Loss 5.857145, Accuracy 83.906%\n",
      "Epoch 20, Batch 86, LR 0.000054 Loss 5.853641, Accuracy 83.975%\n",
      "Epoch 20, Batch 87, LR 0.000054 Loss 5.851469, Accuracy 83.935%\n",
      "Epoch 20, Batch 88, LR 0.000054 Loss 5.847706, Accuracy 84.011%\n",
      "Epoch 20, Batch 89, LR 0.000054 Loss 5.842014, Accuracy 84.050%\n",
      "Epoch 20, Batch 90, LR 0.000054 Loss 5.848616, Accuracy 84.062%\n",
      "Epoch 20, Batch 91, LR 0.000054 Loss 5.848123, Accuracy 84.066%\n",
      "Epoch 20, Batch 92, LR 0.000054 Loss 5.860795, Accuracy 83.942%\n",
      "Epoch 20, Batch 93, LR 0.000054 Loss 5.864275, Accuracy 83.913%\n",
      "Epoch 20, Batch 94, LR 0.000054 Loss 5.862332, Accuracy 83.893%\n",
      "Epoch 20, Batch 95, LR 0.000054 Loss 5.858930, Accuracy 83.914%\n",
      "Epoch 20, Batch 96, LR 0.000054 Loss 5.864331, Accuracy 83.895%\n",
      "Epoch 20, Batch 97, LR 0.000054 Loss 5.860097, Accuracy 83.932%\n",
      "Epoch 20, Batch 98, LR 0.000054 Loss 5.857765, Accuracy 83.929%\n",
      "Epoch 20, Batch 99, LR 0.000054 Loss 5.866906, Accuracy 83.862%\n",
      "Epoch 20, Batch 100, LR 0.000054 Loss 5.863844, Accuracy 83.867%\n",
      "Epoch 20, Batch 101, LR 0.000054 Loss 5.861136, Accuracy 83.849%\n",
      "Epoch 20, Batch 102, LR 0.000054 Loss 5.861151, Accuracy 83.816%\n",
      "Epoch 20, Batch 103, LR 0.000054 Loss 5.862689, Accuracy 83.836%\n",
      "Epoch 20, Batch 104, LR 0.000054 Loss 5.853952, Accuracy 83.872%\n",
      "Epoch 20, Batch 105, LR 0.000054 Loss 5.864287, Accuracy 83.802%\n",
      "Epoch 20, Batch 106, LR 0.000054 Loss 5.857643, Accuracy 83.822%\n",
      "Epoch 20, Batch 107, LR 0.000054 Loss 5.863320, Accuracy 83.762%\n",
      "Epoch 20, Batch 108, LR 0.000054 Loss 5.864072, Accuracy 83.753%\n",
      "Epoch 20, Batch 109, LR 0.000054 Loss 5.864411, Accuracy 83.759%\n",
      "Epoch 20, Batch 110, LR 0.000054 Loss 5.861009, Accuracy 83.757%\n",
      "Epoch 20, Batch 111, LR 0.000054 Loss 5.867689, Accuracy 83.685%\n",
      "Epoch 20, Batch 112, LR 0.000054 Loss 5.869580, Accuracy 83.670%\n",
      "Epoch 20, Batch 113, LR 0.000054 Loss 5.869636, Accuracy 83.670%\n",
      "Epoch 20, Batch 114, LR 0.000054 Loss 5.867436, Accuracy 83.690%\n",
      "Epoch 20, Batch 115, LR 0.000054 Loss 5.870627, Accuracy 83.668%\n",
      "Epoch 20, Batch 116, LR 0.000054 Loss 5.869566, Accuracy 83.668%\n",
      "Epoch 20, Batch 117, LR 0.000054 Loss 5.870070, Accuracy 83.674%\n",
      "Epoch 20, Batch 118, LR 0.000054 Loss 5.868583, Accuracy 83.680%\n",
      "Epoch 20, Batch 119, LR 0.000054 Loss 5.865291, Accuracy 83.712%\n",
      "Epoch 20, Batch 120, LR 0.000054 Loss 5.860672, Accuracy 83.750%\n",
      "Epoch 20, Batch 121, LR 0.000054 Loss 5.860416, Accuracy 83.762%\n",
      "Epoch 20, Batch 122, LR 0.000054 Loss 5.866476, Accuracy 83.728%\n",
      "Epoch 20, Batch 123, LR 0.000054 Loss 5.869533, Accuracy 83.689%\n",
      "Epoch 20, Batch 124, LR 0.000054 Loss 5.870144, Accuracy 83.682%\n",
      "Epoch 20, Batch 125, LR 0.000054 Loss 5.867298, Accuracy 83.700%\n",
      "Epoch 20, Batch 126, LR 0.000054 Loss 5.868553, Accuracy 83.718%\n",
      "Epoch 20, Batch 127, LR 0.000054 Loss 5.869399, Accuracy 83.717%\n",
      "Epoch 20, Batch 128, LR 0.000054 Loss 5.869463, Accuracy 83.734%\n",
      "Epoch 20, Batch 129, LR 0.000054 Loss 5.868554, Accuracy 83.751%\n",
      "Epoch 20, Batch 130, LR 0.000054 Loss 5.872967, Accuracy 83.774%\n",
      "Epoch 20, Batch 131, LR 0.000054 Loss 5.873262, Accuracy 83.761%\n",
      "Epoch 20, Batch 132, LR 0.000054 Loss 5.875556, Accuracy 83.765%\n",
      "Epoch 20, Batch 133, LR 0.000054 Loss 5.874339, Accuracy 83.788%\n",
      "Epoch 20, Batch 134, LR 0.000054 Loss 5.874042, Accuracy 83.780%\n",
      "Epoch 20, Batch 135, LR 0.000054 Loss 5.873540, Accuracy 83.785%\n",
      "Epoch 20, Batch 136, LR 0.000054 Loss 5.871286, Accuracy 83.795%\n",
      "Epoch 20, Batch 137, LR 0.000054 Loss 5.870804, Accuracy 83.799%\n",
      "Epoch 20, Batch 138, LR 0.000054 Loss 5.867161, Accuracy 83.809%\n",
      "Epoch 20, Batch 139, LR 0.000054 Loss 5.861662, Accuracy 83.847%\n",
      "Epoch 20, Batch 140, LR 0.000054 Loss 5.861279, Accuracy 83.878%\n",
      "Epoch 20, Batch 141, LR 0.000054 Loss 5.860699, Accuracy 83.876%\n",
      "Epoch 20, Batch 142, LR 0.000054 Loss 5.855872, Accuracy 83.913%\n",
      "Epoch 20, Batch 143, LR 0.000054 Loss 5.857702, Accuracy 83.894%\n",
      "Epoch 20, Batch 144, LR 0.000054 Loss 5.856432, Accuracy 83.908%\n",
      "Epoch 20, Batch 145, LR 0.000054 Loss 5.860071, Accuracy 83.890%\n",
      "Epoch 20, Batch 146, LR 0.000054 Loss 5.867965, Accuracy 83.829%\n",
      "Epoch 20, Batch 147, LR 0.000054 Loss 5.868698, Accuracy 83.838%\n",
      "Epoch 20, Batch 148, LR 0.000054 Loss 5.867491, Accuracy 83.863%\n",
      "Epoch 20, Batch 149, LR 0.000054 Loss 5.861594, Accuracy 83.908%\n",
      "Epoch 20, Batch 150, LR 0.000054 Loss 5.863249, Accuracy 83.896%\n",
      "Epoch 20, Batch 151, LR 0.000054 Loss 5.865615, Accuracy 83.878%\n",
      "Epoch 20, Batch 152, LR 0.000054 Loss 5.866764, Accuracy 83.876%\n",
      "Epoch 20, Batch 153, LR 0.000054 Loss 5.865405, Accuracy 83.875%\n",
      "Epoch 20, Batch 154, LR 0.000054 Loss 5.856872, Accuracy 83.903%\n",
      "Epoch 20, Batch 155, LR 0.000054 Loss 5.859479, Accuracy 83.886%\n",
      "Epoch 20, Batch 156, LR 0.000054 Loss 5.862638, Accuracy 83.864%\n",
      "Epoch 20, Batch 157, LR 0.000054 Loss 5.857471, Accuracy 83.862%\n",
      "Epoch 20, Batch 158, LR 0.000054 Loss 5.855763, Accuracy 83.871%\n",
      "Epoch 20, Batch 159, LR 0.000054 Loss 5.854838, Accuracy 83.874%\n",
      "Epoch 20, Batch 160, LR 0.000053 Loss 5.854208, Accuracy 83.882%\n",
      "Epoch 20, Batch 161, LR 0.000053 Loss 5.849822, Accuracy 83.919%\n",
      "Epoch 20, Batch 162, LR 0.000053 Loss 5.852002, Accuracy 83.922%\n",
      "Epoch 20, Batch 163, LR 0.000053 Loss 5.849607, Accuracy 83.924%\n",
      "Epoch 20, Batch 164, LR 0.000053 Loss 5.846757, Accuracy 83.937%\n",
      "Epoch 20, Batch 165, LR 0.000053 Loss 5.842493, Accuracy 83.968%\n",
      "Epoch 20, Batch 166, LR 0.000053 Loss 5.843517, Accuracy 83.975%\n",
      "Epoch 20, Batch 167, LR 0.000053 Loss 5.847787, Accuracy 83.931%\n",
      "Epoch 20, Batch 168, LR 0.000053 Loss 5.845651, Accuracy 83.943%\n",
      "Epoch 20, Batch 169, LR 0.000053 Loss 5.843051, Accuracy 83.964%\n",
      "Epoch 20, Batch 170, LR 0.000053 Loss 5.844244, Accuracy 83.952%\n",
      "Epoch 20, Batch 171, LR 0.000053 Loss 5.845706, Accuracy 83.918%\n",
      "Epoch 20, Batch 172, LR 0.000053 Loss 5.843939, Accuracy 83.930%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 173, LR 0.000053 Loss 5.849116, Accuracy 83.887%\n",
      "Epoch 20, Batch 174, LR 0.000053 Loss 5.847995, Accuracy 83.881%\n",
      "Epoch 20, Batch 175, LR 0.000053 Loss 5.846390, Accuracy 83.906%\n",
      "Epoch 20, Batch 176, LR 0.000053 Loss 5.845929, Accuracy 83.913%\n",
      "Epoch 20, Batch 177, LR 0.000053 Loss 5.845187, Accuracy 83.925%\n",
      "Epoch 20, Batch 178, LR 0.000053 Loss 5.845120, Accuracy 83.927%\n",
      "Epoch 20, Batch 179, LR 0.000053 Loss 5.845733, Accuracy 83.934%\n",
      "Epoch 20, Batch 180, LR 0.000053 Loss 5.841845, Accuracy 83.941%\n",
      "Epoch 20, Batch 181, LR 0.000053 Loss 5.843171, Accuracy 83.935%\n",
      "Epoch 20, Batch 182, LR 0.000053 Loss 5.842467, Accuracy 83.950%\n",
      "Epoch 20, Batch 183, LR 0.000053 Loss 5.842199, Accuracy 83.961%\n",
      "Epoch 20, Batch 184, LR 0.000053 Loss 5.840667, Accuracy 83.967%\n",
      "Epoch 20, Batch 185, LR 0.000053 Loss 5.842787, Accuracy 83.957%\n",
      "Epoch 20, Batch 186, LR 0.000053 Loss 5.844381, Accuracy 83.947%\n",
      "Epoch 20, Batch 187, LR 0.000053 Loss 5.848758, Accuracy 83.932%\n",
      "Epoch 20, Batch 188, LR 0.000053 Loss 5.847102, Accuracy 83.930%\n",
      "Epoch 20, Batch 189, LR 0.000053 Loss 5.849039, Accuracy 83.929%\n",
      "Epoch 20, Batch 190, LR 0.000053 Loss 5.849924, Accuracy 83.923%\n",
      "Epoch 20, Batch 191, LR 0.000053 Loss 5.850720, Accuracy 83.909%\n",
      "Epoch 20, Batch 192, LR 0.000053 Loss 5.856574, Accuracy 83.887%\n",
      "Epoch 20, Batch 193, LR 0.000053 Loss 5.860867, Accuracy 83.837%\n",
      "Epoch 20, Batch 194, LR 0.000053 Loss 5.860209, Accuracy 83.847%\n",
      "Epoch 20, Batch 195, LR 0.000053 Loss 5.863647, Accuracy 83.822%\n",
      "Epoch 20, Batch 196, LR 0.000053 Loss 5.863157, Accuracy 83.825%\n",
      "Epoch 20, Batch 197, LR 0.000053 Loss 5.859845, Accuracy 83.836%\n",
      "Epoch 20, Batch 198, LR 0.000053 Loss 5.857987, Accuracy 83.846%\n",
      "Epoch 20, Batch 199, LR 0.000053 Loss 5.859352, Accuracy 83.853%\n",
      "Epoch 20, Batch 200, LR 0.000053 Loss 5.858428, Accuracy 83.895%\n",
      "Epoch 20, Batch 201, LR 0.000053 Loss 5.860682, Accuracy 83.877%\n",
      "Epoch 20, Batch 202, LR 0.000053 Loss 5.860394, Accuracy 83.884%\n",
      "Epoch 20, Batch 203, LR 0.000053 Loss 5.860833, Accuracy 83.898%\n",
      "Epoch 20, Batch 204, LR 0.000053 Loss 5.864853, Accuracy 83.873%\n",
      "Epoch 20, Batch 205, LR 0.000053 Loss 5.868400, Accuracy 83.830%\n",
      "Epoch 20, Batch 206, LR 0.000053 Loss 5.868222, Accuracy 83.833%\n",
      "Epoch 20, Batch 207, LR 0.000053 Loss 5.870708, Accuracy 83.820%\n",
      "Epoch 20, Batch 208, LR 0.000053 Loss 5.870923, Accuracy 83.838%\n",
      "Epoch 20, Batch 209, LR 0.000053 Loss 5.869926, Accuracy 83.855%\n",
      "Epoch 20, Batch 210, LR 0.000053 Loss 5.871576, Accuracy 83.839%\n",
      "Epoch 20, Batch 211, LR 0.000053 Loss 5.868050, Accuracy 83.871%\n",
      "Epoch 20, Batch 212, LR 0.000053 Loss 5.864934, Accuracy 83.911%\n",
      "Epoch 20, Batch 213, LR 0.000053 Loss 5.868590, Accuracy 83.876%\n",
      "Epoch 20, Batch 214, LR 0.000053 Loss 5.872851, Accuracy 83.857%\n",
      "Epoch 20, Batch 215, LR 0.000053 Loss 5.874314, Accuracy 83.863%\n",
      "Epoch 20, Batch 216, LR 0.000053 Loss 5.871965, Accuracy 83.869%\n",
      "Epoch 20, Batch 217, LR 0.000053 Loss 5.870543, Accuracy 83.896%\n",
      "Epoch 20, Batch 218, LR 0.000053 Loss 5.866877, Accuracy 83.880%\n",
      "Epoch 20, Batch 219, LR 0.000053 Loss 5.865956, Accuracy 83.883%\n",
      "Epoch 20, Batch 220, LR 0.000053 Loss 5.864990, Accuracy 83.878%\n",
      "Epoch 20, Batch 221, LR 0.000053 Loss 5.868468, Accuracy 83.838%\n",
      "Epoch 20, Batch 222, LR 0.000053 Loss 5.866899, Accuracy 83.830%\n",
      "Epoch 20, Batch 223, LR 0.000053 Loss 5.867111, Accuracy 83.828%\n",
      "Epoch 20, Batch 224, LR 0.000053 Loss 5.868984, Accuracy 83.820%\n",
      "Epoch 20, Batch 225, LR 0.000053 Loss 5.865589, Accuracy 83.830%\n",
      "Epoch 20, Batch 226, LR 0.000053 Loss 5.864477, Accuracy 83.818%\n",
      "Epoch 20, Batch 227, LR 0.000053 Loss 5.863073, Accuracy 83.831%\n",
      "Epoch 20, Batch 228, LR 0.000053 Loss 5.865573, Accuracy 83.816%\n",
      "Epoch 20, Batch 229, LR 0.000053 Loss 5.865888, Accuracy 83.816%\n",
      "Epoch 20, Batch 230, LR 0.000053 Loss 5.867515, Accuracy 83.815%\n",
      "Epoch 20, Batch 231, LR 0.000053 Loss 5.868462, Accuracy 83.810%\n",
      "Epoch 20, Batch 232, LR 0.000053 Loss 5.870020, Accuracy 83.799%\n",
      "Epoch 20, Batch 233, LR 0.000053 Loss 5.870101, Accuracy 83.805%\n",
      "Epoch 20, Batch 234, LR 0.000053 Loss 5.864712, Accuracy 83.834%\n",
      "Epoch 20, Batch 235, LR 0.000053 Loss 5.865913, Accuracy 83.820%\n",
      "Epoch 20, Batch 236, LR 0.000053 Loss 5.862427, Accuracy 83.822%\n",
      "Epoch 20, Batch 237, LR 0.000053 Loss 5.863141, Accuracy 83.792%\n",
      "Epoch 20, Batch 238, LR 0.000053 Loss 5.862562, Accuracy 83.791%\n",
      "Epoch 20, Batch 239, LR 0.000053 Loss 5.861327, Accuracy 83.790%\n",
      "Epoch 20, Batch 240, LR 0.000053 Loss 5.857451, Accuracy 83.809%\n",
      "Epoch 20, Batch 241, LR 0.000053 Loss 5.859496, Accuracy 83.808%\n",
      "Epoch 20, Batch 242, LR 0.000053 Loss 5.859645, Accuracy 83.813%\n",
      "Epoch 20, Batch 243, LR 0.000053 Loss 5.860226, Accuracy 83.800%\n",
      "Epoch 20, Batch 244, LR 0.000053 Loss 5.861745, Accuracy 83.795%\n",
      "Epoch 20, Batch 245, LR 0.000053 Loss 5.861478, Accuracy 83.807%\n",
      "Epoch 20, Batch 246, LR 0.000053 Loss 5.861636, Accuracy 83.810%\n",
      "Epoch 20, Batch 247, LR 0.000053 Loss 5.862103, Accuracy 83.818%\n",
      "Epoch 20, Batch 248, LR 0.000053 Loss 5.860524, Accuracy 83.821%\n",
      "Epoch 20, Batch 249, LR 0.000053 Loss 5.864199, Accuracy 83.801%\n",
      "Epoch 20, Batch 250, LR 0.000053 Loss 5.864999, Accuracy 83.797%\n",
      "Epoch 20, Batch 251, LR 0.000053 Loss 5.860797, Accuracy 83.827%\n",
      "Epoch 20, Batch 252, LR 0.000053 Loss 5.863122, Accuracy 83.814%\n",
      "Epoch 20, Batch 253, LR 0.000053 Loss 5.865353, Accuracy 83.804%\n",
      "Epoch 20, Batch 254, LR 0.000053 Loss 5.866861, Accuracy 83.794%\n",
      "Epoch 20, Batch 255, LR 0.000053 Loss 5.866351, Accuracy 83.805%\n",
      "Epoch 20, Batch 256, LR 0.000053 Loss 5.864802, Accuracy 83.810%\n",
      "Epoch 20, Batch 257, LR 0.000053 Loss 5.863309, Accuracy 83.819%\n",
      "Epoch 20, Batch 258, LR 0.000053 Loss 5.863695, Accuracy 83.815%\n",
      "Epoch 20, Batch 259, LR 0.000053 Loss 5.864271, Accuracy 83.808%\n",
      "Epoch 20, Batch 260, LR 0.000053 Loss 5.866376, Accuracy 83.789%\n",
      "Epoch 20, Batch 261, LR 0.000053 Loss 5.866601, Accuracy 83.803%\n",
      "Epoch 20, Batch 262, LR 0.000053 Loss 5.865924, Accuracy 83.802%\n",
      "Epoch 20, Batch 263, LR 0.000053 Loss 5.869129, Accuracy 83.793%\n",
      "Epoch 20, Batch 264, LR 0.000053 Loss 5.867567, Accuracy 83.777%\n",
      "Epoch 20, Batch 265, LR 0.000053 Loss 5.866555, Accuracy 83.782%\n",
      "Epoch 20, Batch 266, LR 0.000053 Loss 5.862925, Accuracy 83.802%\n",
      "Epoch 20, Batch 267, LR 0.000053 Loss 5.861734, Accuracy 83.819%\n",
      "Epoch 20, Batch 268, LR 0.000053 Loss 5.858830, Accuracy 83.827%\n",
      "Epoch 20, Batch 269, LR 0.000053 Loss 5.856896, Accuracy 83.832%\n",
      "Epoch 20, Batch 270, LR 0.000053 Loss 5.856394, Accuracy 83.831%\n",
      "Epoch 20, Batch 271, LR 0.000053 Loss 5.854675, Accuracy 83.833%\n",
      "Epoch 20, Batch 272, LR 0.000053 Loss 5.855974, Accuracy 83.824%\n",
      "Epoch 20, Batch 273, LR 0.000053 Loss 5.856200, Accuracy 83.814%\n",
      "Epoch 20, Batch 274, LR 0.000053 Loss 5.857202, Accuracy 83.808%\n",
      "Epoch 20, Batch 275, LR 0.000053 Loss 5.857606, Accuracy 83.798%\n",
      "Epoch 20, Batch 276, LR 0.000053 Loss 5.856164, Accuracy 83.812%\n",
      "Epoch 20, Batch 277, LR 0.000053 Loss 5.856219, Accuracy 83.808%\n",
      "Epoch 20, Batch 278, LR 0.000053 Loss 5.851915, Accuracy 83.824%\n",
      "Epoch 20, Batch 279, LR 0.000053 Loss 5.851814, Accuracy 83.815%\n",
      "Epoch 20, Batch 280, LR 0.000053 Loss 5.849906, Accuracy 83.817%\n",
      "Epoch 20, Batch 281, LR 0.000053 Loss 5.851841, Accuracy 83.797%\n",
      "Epoch 20, Batch 282, LR 0.000053 Loss 5.851807, Accuracy 83.785%\n",
      "Epoch 20, Batch 283, LR 0.000053 Loss 5.849823, Accuracy 83.793%\n",
      "Epoch 20, Batch 284, LR 0.000053 Loss 5.849735, Accuracy 83.803%\n",
      "Epoch 20, Batch 285, LR 0.000053 Loss 5.847440, Accuracy 83.819%\n",
      "Epoch 20, Batch 286, LR 0.000053 Loss 5.847675, Accuracy 83.831%\n",
      "Epoch 20, Batch 287, LR 0.000053 Loss 5.848747, Accuracy 83.828%\n",
      "Epoch 20, Batch 288, LR 0.000053 Loss 5.850668, Accuracy 83.811%\n",
      "Epoch 20, Batch 289, LR 0.000053 Loss 5.848615, Accuracy 83.826%\n",
      "Epoch 20, Batch 290, LR 0.000053 Loss 5.849048, Accuracy 83.834%\n",
      "Epoch 20, Batch 291, LR 0.000053 Loss 5.849355, Accuracy 83.833%\n",
      "Epoch 20, Batch 292, LR 0.000053 Loss 5.849993, Accuracy 83.840%\n",
      "Epoch 20, Batch 293, LR 0.000053 Loss 5.851099, Accuracy 83.831%\n",
      "Epoch 20, Batch 294, LR 0.000053 Loss 5.852100, Accuracy 83.828%\n",
      "Epoch 20, Batch 295, LR 0.000053 Loss 5.854926, Accuracy 83.814%\n",
      "Epoch 20, Batch 296, LR 0.000053 Loss 5.852141, Accuracy 83.834%\n",
      "Epoch 20, Batch 297, LR 0.000053 Loss 5.850382, Accuracy 83.846%\n",
      "Epoch 20, Batch 298, LR 0.000053 Loss 5.850767, Accuracy 83.851%\n",
      "Epoch 20, Batch 299, LR 0.000053 Loss 5.853246, Accuracy 83.837%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 300, LR 0.000053 Loss 5.853084, Accuracy 83.836%\n",
      "Epoch 20, Batch 301, LR 0.000053 Loss 5.853360, Accuracy 83.835%\n",
      "Epoch 20, Batch 302, LR 0.000053 Loss 5.854420, Accuracy 83.837%\n",
      "Epoch 20, Batch 303, LR 0.000053 Loss 5.856831, Accuracy 83.834%\n",
      "Epoch 20, Batch 304, LR 0.000053 Loss 5.855808, Accuracy 83.833%\n",
      "Epoch 20, Batch 305, LR 0.000053 Loss 5.856167, Accuracy 83.827%\n",
      "Epoch 20, Batch 306, LR 0.000053 Loss 5.855123, Accuracy 83.824%\n",
      "Epoch 20, Batch 307, LR 0.000053 Loss 5.855469, Accuracy 83.833%\n",
      "Epoch 20, Batch 308, LR 0.000053 Loss 5.855175, Accuracy 83.835%\n",
      "Epoch 20, Batch 309, LR 0.000053 Loss 5.854049, Accuracy 83.839%\n",
      "Epoch 20, Batch 310, LR 0.000053 Loss 5.854776, Accuracy 83.826%\n",
      "Epoch 20, Batch 311, LR 0.000053 Loss 5.855747, Accuracy 83.820%\n",
      "Epoch 20, Batch 312, LR 0.000053 Loss 5.857394, Accuracy 83.807%\n",
      "Epoch 20, Batch 313, LR 0.000053 Loss 5.857068, Accuracy 83.808%\n",
      "Epoch 20, Batch 314, LR 0.000053 Loss 5.860194, Accuracy 83.798%\n",
      "Epoch 20, Batch 315, LR 0.000053 Loss 5.860263, Accuracy 83.810%\n",
      "Epoch 20, Batch 316, LR 0.000053 Loss 5.859720, Accuracy 83.819%\n",
      "Epoch 20, Batch 317, LR 0.000053 Loss 5.860707, Accuracy 83.806%\n",
      "Epoch 20, Batch 318, LR 0.000053 Loss 5.860899, Accuracy 83.798%\n",
      "Epoch 20, Batch 319, LR 0.000053 Loss 5.862553, Accuracy 83.790%\n",
      "Epoch 20, Batch 320, LR 0.000053 Loss 5.861987, Accuracy 83.794%\n",
      "Epoch 20, Batch 321, LR 0.000053 Loss 5.859696, Accuracy 83.808%\n",
      "Epoch 20, Batch 322, LR 0.000053 Loss 5.860071, Accuracy 83.810%\n",
      "Epoch 20, Batch 323, LR 0.000053 Loss 5.858345, Accuracy 83.811%\n",
      "Epoch 20, Batch 324, LR 0.000053 Loss 5.858023, Accuracy 83.818%\n",
      "Epoch 20, Batch 325, LR 0.000053 Loss 5.858742, Accuracy 83.817%\n",
      "Epoch 20, Batch 326, LR 0.000053 Loss 5.861180, Accuracy 83.805%\n",
      "Epoch 20, Batch 327, LR 0.000053 Loss 5.861137, Accuracy 83.794%\n",
      "Epoch 20, Batch 328, LR 0.000053 Loss 5.862806, Accuracy 83.784%\n",
      "Epoch 20, Batch 329, LR 0.000053 Loss 5.864188, Accuracy 83.767%\n",
      "Epoch 20, Batch 330, LR 0.000053 Loss 5.865119, Accuracy 83.755%\n",
      "Epoch 20, Batch 331, LR 0.000053 Loss 5.865840, Accuracy 83.752%\n",
      "Epoch 20, Batch 332, LR 0.000053 Loss 5.868784, Accuracy 83.756%\n",
      "Epoch 20, Batch 333, LR 0.000053 Loss 5.868510, Accuracy 83.763%\n",
      "Epoch 20, Batch 334, LR 0.000053 Loss 5.867999, Accuracy 83.750%\n",
      "Epoch 20, Batch 335, LR 0.000053 Loss 5.865531, Accuracy 83.759%\n",
      "Epoch 20, Batch 336, LR 0.000053 Loss 5.863143, Accuracy 83.775%\n",
      "Epoch 20, Batch 337, LR 0.000053 Loss 5.863966, Accuracy 83.763%\n",
      "Epoch 20, Batch 338, LR 0.000053 Loss 5.863045, Accuracy 83.765%\n",
      "Epoch 20, Batch 339, LR 0.000053 Loss 5.862355, Accuracy 83.760%\n",
      "Epoch 20, Batch 340, LR 0.000053 Loss 5.863135, Accuracy 83.757%\n",
      "Epoch 20, Batch 341, LR 0.000053 Loss 5.863632, Accuracy 83.750%\n",
      "Epoch 20, Batch 342, LR 0.000053 Loss 5.862555, Accuracy 83.747%\n",
      "Epoch 20, Batch 343, LR 0.000053 Loss 5.862484, Accuracy 83.755%\n",
      "Epoch 20, Batch 344, LR 0.000053 Loss 5.859845, Accuracy 83.787%\n",
      "Epoch 20, Batch 345, LR 0.000053 Loss 5.859603, Accuracy 83.788%\n",
      "Epoch 20, Batch 346, LR 0.000053 Loss 5.859089, Accuracy 83.804%\n",
      "Epoch 20, Batch 347, LR 0.000053 Loss 5.860135, Accuracy 83.794%\n",
      "Epoch 20, Batch 348, LR 0.000053 Loss 5.859845, Accuracy 83.805%\n",
      "Epoch 20, Batch 349, LR 0.000053 Loss 5.859270, Accuracy 83.806%\n",
      "Epoch 20, Batch 350, LR 0.000053 Loss 5.858828, Accuracy 83.808%\n",
      "Epoch 20, Batch 351, LR 0.000053 Loss 5.859050, Accuracy 83.819%\n",
      "Epoch 20, Batch 352, LR 0.000053 Loss 5.858682, Accuracy 83.827%\n",
      "Epoch 20, Batch 353, LR 0.000053 Loss 5.854680, Accuracy 83.850%\n",
      "Epoch 20, Batch 354, LR 0.000053 Loss 5.852120, Accuracy 83.852%\n",
      "Epoch 20, Batch 355, LR 0.000053 Loss 5.852613, Accuracy 83.842%\n",
      "Epoch 20, Batch 356, LR 0.000053 Loss 5.853292, Accuracy 83.840%\n",
      "Epoch 20, Batch 357, LR 0.000053 Loss 5.853589, Accuracy 83.848%\n",
      "Epoch 20, Batch 358, LR 0.000053 Loss 5.854022, Accuracy 83.858%\n",
      "Epoch 20, Batch 359, LR 0.000053 Loss 5.855944, Accuracy 83.846%\n",
      "Epoch 20, Batch 360, LR 0.000053 Loss 5.853709, Accuracy 83.872%\n",
      "Epoch 20, Batch 361, LR 0.000053 Loss 5.851689, Accuracy 83.882%\n",
      "Epoch 20, Batch 362, LR 0.000053 Loss 5.851124, Accuracy 83.883%\n",
      "Epoch 20, Batch 363, LR 0.000053 Loss 5.852621, Accuracy 83.871%\n",
      "Epoch 20, Batch 364, LR 0.000053 Loss 5.853766, Accuracy 83.871%\n",
      "Epoch 20, Batch 365, LR 0.000053 Loss 5.854780, Accuracy 83.868%\n",
      "Epoch 20, Batch 366, LR 0.000053 Loss 5.853772, Accuracy 83.871%\n",
      "Epoch 20, Batch 367, LR 0.000053 Loss 5.854827, Accuracy 83.868%\n",
      "Epoch 20, Batch 368, LR 0.000053 Loss 5.856682, Accuracy 83.874%\n",
      "Epoch 20, Batch 369, LR 0.000053 Loss 5.857033, Accuracy 83.867%\n",
      "Epoch 20, Batch 370, LR 0.000053 Loss 5.854992, Accuracy 83.879%\n",
      "Epoch 20, Batch 371, LR 0.000053 Loss 5.853793, Accuracy 83.886%\n",
      "Epoch 20, Batch 372, LR 0.000053 Loss 5.851878, Accuracy 83.879%\n",
      "Epoch 20, Batch 373, LR 0.000053 Loss 5.851113, Accuracy 83.883%\n",
      "Epoch 20, Batch 374, LR 0.000053 Loss 5.854218, Accuracy 83.855%\n",
      "Epoch 20, Batch 375, LR 0.000053 Loss 5.853983, Accuracy 83.852%\n",
      "Epoch 20, Batch 376, LR 0.000053 Loss 5.854118, Accuracy 83.847%\n",
      "Epoch 20, Batch 377, LR 0.000053 Loss 5.854933, Accuracy 83.836%\n",
      "Epoch 20, Batch 378, LR 0.000053 Loss 5.854054, Accuracy 83.840%\n",
      "Epoch 20, Batch 379, LR 0.000053 Loss 5.855902, Accuracy 83.829%\n",
      "Epoch 20, Batch 380, LR 0.000053 Loss 5.855387, Accuracy 83.826%\n",
      "Epoch 20, Batch 381, LR 0.000053 Loss 5.856770, Accuracy 83.813%\n",
      "Epoch 20, Batch 382, LR 0.000053 Loss 5.856830, Accuracy 83.821%\n",
      "Epoch 20, Batch 383, LR 0.000053 Loss 5.855800, Accuracy 83.830%\n",
      "Epoch 20, Batch 384, LR 0.000053 Loss 5.856694, Accuracy 83.834%\n",
      "Epoch 20, Batch 385, LR 0.000053 Loss 5.855964, Accuracy 83.837%\n",
      "Epoch 20, Batch 386, LR 0.000053 Loss 5.857283, Accuracy 83.829%\n",
      "Epoch 20, Batch 387, LR 0.000053 Loss 5.858808, Accuracy 83.826%\n",
      "Epoch 20, Batch 388, LR 0.000053 Loss 5.859201, Accuracy 83.829%\n",
      "Epoch 20, Batch 389, LR 0.000053 Loss 5.857084, Accuracy 83.843%\n",
      "Epoch 20, Batch 390, LR 0.000053 Loss 5.857916, Accuracy 83.842%\n",
      "Epoch 20, Batch 391, LR 0.000053 Loss 5.859193, Accuracy 83.834%\n",
      "Epoch 20, Batch 392, LR 0.000053 Loss 5.858209, Accuracy 83.839%\n",
      "Epoch 20, Batch 393, LR 0.000053 Loss 5.858684, Accuracy 83.830%\n",
      "Epoch 20, Batch 394, LR 0.000053 Loss 5.860547, Accuracy 83.830%\n",
      "Epoch 20, Batch 395, LR 0.000053 Loss 5.860812, Accuracy 83.825%\n",
      "Epoch 20, Batch 396, LR 0.000053 Loss 5.859202, Accuracy 83.836%\n",
      "Epoch 20, Batch 397, LR 0.000053 Loss 5.861184, Accuracy 83.820%\n",
      "Epoch 20, Batch 398, LR 0.000053 Loss 5.860391, Accuracy 83.823%\n",
      "Epoch 20, Batch 399, LR 0.000053 Loss 5.862860, Accuracy 83.803%\n",
      "Epoch 20, Batch 400, LR 0.000053 Loss 5.862275, Accuracy 83.805%\n",
      "Epoch 20, Batch 401, LR 0.000053 Loss 5.862475, Accuracy 83.806%\n",
      "Epoch 20, Batch 402, LR 0.000053 Loss 5.861697, Accuracy 83.806%\n",
      "Epoch 20, Batch 403, LR 0.000053 Loss 5.861880, Accuracy 83.811%\n",
      "Epoch 20, Batch 404, LR 0.000053 Loss 5.861741, Accuracy 83.808%\n",
      "Epoch 20, Batch 405, LR 0.000053 Loss 5.861622, Accuracy 83.802%\n",
      "Epoch 20, Batch 406, LR 0.000053 Loss 5.860730, Accuracy 83.800%\n",
      "Epoch 20, Batch 407, LR 0.000053 Loss 5.861847, Accuracy 83.790%\n",
      "Epoch 20, Batch 408, LR 0.000053 Loss 5.860587, Accuracy 83.795%\n",
      "Epoch 20, Batch 409, LR 0.000053 Loss 5.859988, Accuracy 83.792%\n",
      "Epoch 20, Batch 410, LR 0.000053 Loss 5.858539, Accuracy 83.801%\n",
      "Epoch 20, Batch 411, LR 0.000053 Loss 5.860225, Accuracy 83.772%\n",
      "Epoch 20, Batch 412, LR 0.000053 Loss 5.860681, Accuracy 83.776%\n",
      "Epoch 20, Batch 413, LR 0.000053 Loss 5.861153, Accuracy 83.770%\n",
      "Epoch 20, Batch 414, LR 0.000052 Loss 5.861966, Accuracy 83.762%\n",
      "Epoch 20, Batch 415, LR 0.000052 Loss 5.862640, Accuracy 83.759%\n",
      "Epoch 20, Batch 416, LR 0.000052 Loss 5.861492, Accuracy 83.767%\n",
      "Epoch 20, Batch 417, LR 0.000052 Loss 5.862291, Accuracy 83.768%\n",
      "Epoch 20, Batch 418, LR 0.000052 Loss 5.862100, Accuracy 83.777%\n",
      "Epoch 20, Batch 419, LR 0.000052 Loss 5.863549, Accuracy 83.767%\n",
      "Epoch 20, Batch 420, LR 0.000052 Loss 5.863604, Accuracy 83.761%\n",
      "Epoch 20, Batch 421, LR 0.000052 Loss 5.864493, Accuracy 83.766%\n",
      "Epoch 20, Batch 422, LR 0.000052 Loss 5.864920, Accuracy 83.770%\n",
      "Epoch 20, Batch 423, LR 0.000052 Loss 5.863912, Accuracy 83.780%\n",
      "Epoch 20, Batch 424, LR 0.000052 Loss 5.863199, Accuracy 83.789%\n",
      "Epoch 20, Batch 425, LR 0.000052 Loss 5.862676, Accuracy 83.794%\n",
      "Epoch 20, Batch 426, LR 0.000052 Loss 5.863553, Accuracy 83.790%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 427, LR 0.000052 Loss 5.863539, Accuracy 83.790%\n",
      "Epoch 20, Batch 428, LR 0.000052 Loss 5.862991, Accuracy 83.804%\n",
      "Epoch 20, Batch 429, LR 0.000052 Loss 5.863439, Accuracy 83.805%\n",
      "Epoch 20, Batch 430, LR 0.000052 Loss 5.862473, Accuracy 83.808%\n",
      "Epoch 20, Batch 431, LR 0.000052 Loss 5.862698, Accuracy 83.809%\n",
      "Epoch 20, Batch 432, LR 0.000052 Loss 5.863009, Accuracy 83.813%\n",
      "Epoch 20, Batch 433, LR 0.000052 Loss 5.864202, Accuracy 83.810%\n",
      "Epoch 20, Batch 434, LR 0.000052 Loss 5.864763, Accuracy 83.804%\n",
      "Epoch 20, Batch 435, LR 0.000052 Loss 5.864517, Accuracy 83.804%\n",
      "Epoch 20, Batch 436, LR 0.000052 Loss 5.866902, Accuracy 83.794%\n",
      "Epoch 20, Batch 437, LR 0.000052 Loss 5.867467, Accuracy 83.790%\n",
      "Epoch 20, Batch 438, LR 0.000052 Loss 5.869918, Accuracy 83.777%\n",
      "Epoch 20, Batch 439, LR 0.000052 Loss 5.870964, Accuracy 83.781%\n",
      "Epoch 20, Batch 440, LR 0.000052 Loss 5.871124, Accuracy 83.773%\n",
      "Epoch 20, Batch 441, LR 0.000052 Loss 5.870729, Accuracy 83.773%\n",
      "Epoch 20, Batch 442, LR 0.000052 Loss 5.870588, Accuracy 83.778%\n",
      "Epoch 20, Batch 443, LR 0.000052 Loss 5.871189, Accuracy 83.777%\n",
      "Epoch 20, Batch 444, LR 0.000052 Loss 5.871074, Accuracy 83.779%\n",
      "Epoch 20, Batch 445, LR 0.000052 Loss 5.873263, Accuracy 83.768%\n",
      "Epoch 20, Batch 446, LR 0.000052 Loss 5.874528, Accuracy 83.760%\n",
      "Epoch 20, Batch 447, LR 0.000052 Loss 5.874164, Accuracy 83.758%\n",
      "Epoch 20, Batch 448, LR 0.000052 Loss 5.874209, Accuracy 83.759%\n",
      "Epoch 20, Batch 449, LR 0.000052 Loss 5.873236, Accuracy 83.766%\n",
      "Epoch 20, Batch 450, LR 0.000052 Loss 5.873209, Accuracy 83.766%\n",
      "Epoch 20, Batch 451, LR 0.000052 Loss 5.875702, Accuracy 83.753%\n",
      "Epoch 20, Batch 452, LR 0.000052 Loss 5.874134, Accuracy 83.758%\n",
      "Epoch 20, Batch 453, LR 0.000052 Loss 5.875045, Accuracy 83.747%\n",
      "Epoch 20, Batch 454, LR 0.000052 Loss 5.873575, Accuracy 83.761%\n",
      "Epoch 20, Batch 455, LR 0.000052 Loss 5.872630, Accuracy 83.760%\n",
      "Epoch 20, Batch 456, LR 0.000052 Loss 5.874105, Accuracy 83.755%\n",
      "Epoch 20, Batch 457, LR 0.000052 Loss 5.873767, Accuracy 83.763%\n",
      "Epoch 20, Batch 458, LR 0.000052 Loss 5.873808, Accuracy 83.766%\n",
      "Epoch 20, Batch 459, LR 0.000052 Loss 5.873121, Accuracy 83.774%\n",
      "Epoch 20, Batch 460, LR 0.000052 Loss 5.874073, Accuracy 83.762%\n",
      "Epoch 20, Batch 461, LR 0.000052 Loss 5.874222, Accuracy 83.758%\n",
      "Epoch 20, Batch 462, LR 0.000052 Loss 5.874487, Accuracy 83.758%\n",
      "Epoch 20, Batch 463, LR 0.000052 Loss 5.873958, Accuracy 83.756%\n",
      "Epoch 20, Batch 464, LR 0.000052 Loss 5.874484, Accuracy 83.754%\n",
      "Epoch 20, Batch 465, LR 0.000052 Loss 5.874906, Accuracy 83.753%\n",
      "Epoch 20, Batch 466, LR 0.000052 Loss 5.873308, Accuracy 83.756%\n",
      "Epoch 20, Batch 467, LR 0.000052 Loss 5.873886, Accuracy 83.748%\n",
      "Epoch 20, Batch 468, LR 0.000052 Loss 5.874868, Accuracy 83.747%\n",
      "Epoch 20, Batch 469, LR 0.000052 Loss 5.874860, Accuracy 83.742%\n",
      "Epoch 20, Batch 470, LR 0.000052 Loss 5.873497, Accuracy 83.755%\n",
      "Epoch 20, Batch 471, LR 0.000052 Loss 5.874389, Accuracy 83.753%\n",
      "Epoch 20, Batch 472, LR 0.000052 Loss 5.875883, Accuracy 83.748%\n",
      "Epoch 20, Batch 473, LR 0.000052 Loss 5.876867, Accuracy 83.741%\n",
      "Epoch 20, Batch 474, LR 0.000052 Loss 5.876973, Accuracy 83.735%\n",
      "Epoch 20, Batch 475, LR 0.000052 Loss 5.876706, Accuracy 83.729%\n",
      "Epoch 20, Batch 476, LR 0.000052 Loss 5.876017, Accuracy 83.730%\n",
      "Epoch 20, Batch 477, LR 0.000052 Loss 5.875998, Accuracy 83.730%\n",
      "Epoch 20, Batch 478, LR 0.000052 Loss 5.875120, Accuracy 83.726%\n",
      "Epoch 20, Batch 479, LR 0.000052 Loss 5.875092, Accuracy 83.727%\n",
      "Epoch 20, Batch 480, LR 0.000052 Loss 5.875347, Accuracy 83.722%\n",
      "Epoch 20, Batch 481, LR 0.000052 Loss 5.875571, Accuracy 83.722%\n",
      "Epoch 20, Batch 482, LR 0.000052 Loss 5.875700, Accuracy 83.719%\n",
      "Epoch 20, Batch 483, LR 0.000052 Loss 5.874118, Accuracy 83.733%\n",
      "Epoch 20, Batch 484, LR 0.000052 Loss 5.873266, Accuracy 83.742%\n",
      "Epoch 20, Batch 485, LR 0.000052 Loss 5.872460, Accuracy 83.748%\n",
      "Epoch 20, Batch 486, LR 0.000052 Loss 5.872730, Accuracy 83.735%\n",
      "Epoch 20, Batch 487, LR 0.000052 Loss 5.874492, Accuracy 83.738%\n",
      "Epoch 20, Batch 488, LR 0.000052 Loss 5.873044, Accuracy 83.746%\n",
      "Epoch 20, Batch 489, LR 0.000052 Loss 5.873823, Accuracy 83.746%\n",
      "Epoch 20, Batch 490, LR 0.000052 Loss 5.872773, Accuracy 83.755%\n",
      "Epoch 20, Batch 491, LR 0.000052 Loss 5.873511, Accuracy 83.756%\n",
      "Epoch 20, Batch 492, LR 0.000052 Loss 5.871780, Accuracy 83.762%\n",
      "Epoch 20, Batch 493, LR 0.000052 Loss 5.871723, Accuracy 83.762%\n",
      "Epoch 20, Batch 494, LR 0.000052 Loss 5.871666, Accuracy 83.760%\n",
      "Epoch 20, Batch 495, LR 0.000052 Loss 5.872068, Accuracy 83.758%\n",
      "Epoch 20, Batch 496, LR 0.000052 Loss 5.872232, Accuracy 83.758%\n",
      "Epoch 20, Batch 497, LR 0.000052 Loss 5.872468, Accuracy 83.756%\n",
      "Epoch 20, Batch 498, LR 0.000052 Loss 5.872657, Accuracy 83.749%\n",
      "Epoch 20, Batch 499, LR 0.000052 Loss 5.872210, Accuracy 83.746%\n",
      "Epoch 20, Batch 500, LR 0.000052 Loss 5.871735, Accuracy 83.752%\n",
      "Epoch 20, Batch 501, LR 0.000052 Loss 5.870277, Accuracy 83.757%\n",
      "Epoch 20, Batch 502, LR 0.000052 Loss 5.870801, Accuracy 83.754%\n",
      "Epoch 20, Batch 503, LR 0.000052 Loss 5.870878, Accuracy 83.751%\n",
      "Epoch 20, Batch 504, LR 0.000052 Loss 5.871766, Accuracy 83.752%\n",
      "Epoch 20, Batch 505, LR 0.000052 Loss 5.871333, Accuracy 83.748%\n",
      "Epoch 20, Batch 506, LR 0.000052 Loss 5.872848, Accuracy 83.748%\n",
      "Epoch 20, Batch 507, LR 0.000052 Loss 5.872230, Accuracy 83.748%\n",
      "Epoch 20, Batch 508, LR 0.000052 Loss 5.873311, Accuracy 83.737%\n",
      "Epoch 20, Batch 509, LR 0.000052 Loss 5.873850, Accuracy 83.738%\n",
      "Epoch 20, Batch 510, LR 0.000052 Loss 5.874543, Accuracy 83.727%\n",
      "Epoch 20, Batch 511, LR 0.000052 Loss 5.872750, Accuracy 83.741%\n",
      "Epoch 20, Batch 512, LR 0.000052 Loss 5.872275, Accuracy 83.748%\n",
      "Epoch 20, Batch 513, LR 0.000052 Loss 5.871406, Accuracy 83.752%\n",
      "Epoch 20, Batch 514, LR 0.000052 Loss 5.871715, Accuracy 83.755%\n",
      "Epoch 20, Batch 515, LR 0.000052 Loss 5.871604, Accuracy 83.753%\n",
      "Epoch 20, Batch 516, LR 0.000052 Loss 5.872650, Accuracy 83.753%\n",
      "Epoch 20, Batch 517, LR 0.000052 Loss 5.872908, Accuracy 83.746%\n",
      "Epoch 20, Batch 518, LR 0.000052 Loss 5.872973, Accuracy 83.746%\n",
      "Epoch 20, Batch 519, LR 0.000052 Loss 5.872908, Accuracy 83.750%\n",
      "Epoch 20, Batch 520, LR 0.000052 Loss 5.872100, Accuracy 83.759%\n",
      "Epoch 20, Batch 521, LR 0.000052 Loss 5.871847, Accuracy 83.765%\n",
      "Epoch 20, Batch 522, LR 0.000052 Loss 5.872422, Accuracy 83.760%\n",
      "Epoch 20, Batch 523, LR 0.000052 Loss 5.871959, Accuracy 83.773%\n",
      "Epoch 20, Batch 524, LR 0.000052 Loss 5.870344, Accuracy 83.783%\n",
      "Epoch 20, Batch 525, LR 0.000052 Loss 5.871252, Accuracy 83.774%\n",
      "Epoch 20, Batch 526, LR 0.000052 Loss 5.871094, Accuracy 83.772%\n",
      "Epoch 20, Batch 527, LR 0.000052 Loss 5.870456, Accuracy 83.775%\n",
      "Epoch 20, Batch 528, LR 0.000052 Loss 5.870856, Accuracy 83.770%\n",
      "Epoch 20, Batch 529, LR 0.000052 Loss 5.872781, Accuracy 83.765%\n",
      "Epoch 20, Batch 530, LR 0.000052 Loss 5.872821, Accuracy 83.762%\n",
      "Epoch 20, Batch 531, LR 0.000052 Loss 5.873510, Accuracy 83.753%\n",
      "Epoch 20, Batch 532, LR 0.000052 Loss 5.873184, Accuracy 83.752%\n",
      "Epoch 20, Batch 533, LR 0.000052 Loss 5.872655, Accuracy 83.755%\n",
      "Epoch 20, Batch 534, LR 0.000052 Loss 5.871626, Accuracy 83.766%\n",
      "Epoch 20, Batch 535, LR 0.000052 Loss 5.871884, Accuracy 83.773%\n",
      "Epoch 20, Batch 536, LR 0.000052 Loss 5.872400, Accuracy 83.773%\n",
      "Epoch 20, Batch 537, LR 0.000052 Loss 5.873030, Accuracy 83.765%\n",
      "Epoch 20, Batch 538, LR 0.000052 Loss 5.873638, Accuracy 83.764%\n",
      "Epoch 20, Batch 539, LR 0.000052 Loss 5.872262, Accuracy 83.762%\n",
      "Epoch 20, Batch 540, LR 0.000052 Loss 5.872741, Accuracy 83.762%\n",
      "Epoch 20, Batch 541, LR 0.000052 Loss 5.871300, Accuracy 83.768%\n",
      "Epoch 20, Batch 542, LR 0.000052 Loss 5.869865, Accuracy 83.780%\n",
      "Epoch 20, Batch 543, LR 0.000052 Loss 5.871015, Accuracy 83.771%\n",
      "Epoch 20, Batch 544, LR 0.000052 Loss 5.871959, Accuracy 83.773%\n",
      "Epoch 20, Batch 545, LR 0.000052 Loss 5.870725, Accuracy 83.787%\n",
      "Epoch 20, Batch 546, LR 0.000052 Loss 5.869517, Accuracy 83.791%\n",
      "Epoch 20, Batch 547, LR 0.000052 Loss 5.869863, Accuracy 83.782%\n",
      "Epoch 20, Batch 548, LR 0.000052 Loss 5.868756, Accuracy 83.790%\n",
      "Epoch 20, Batch 549, LR 0.000052 Loss 5.867803, Accuracy 83.793%\n",
      "Epoch 20, Batch 550, LR 0.000052 Loss 5.867766, Accuracy 83.797%\n",
      "Epoch 20, Batch 551, LR 0.000052 Loss 5.866040, Accuracy 83.806%\n",
      "Epoch 20, Batch 552, LR 0.000052 Loss 5.865869, Accuracy 83.807%\n",
      "Epoch 20, Batch 553, LR 0.000052 Loss 5.866247, Accuracy 83.811%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 554, LR 0.000052 Loss 5.865470, Accuracy 83.815%\n",
      "Epoch 20, Batch 555, LR 0.000052 Loss 5.864697, Accuracy 83.822%\n",
      "Epoch 20, Batch 556, LR 0.000052 Loss 5.864958, Accuracy 83.816%\n",
      "Epoch 20, Batch 557, LR 0.000052 Loss 5.864167, Accuracy 83.822%\n",
      "Epoch 20, Batch 558, LR 0.000052 Loss 5.864297, Accuracy 83.825%\n",
      "Epoch 20, Batch 559, LR 0.000052 Loss 5.865465, Accuracy 83.815%\n",
      "Epoch 20, Batch 560, LR 0.000052 Loss 5.864396, Accuracy 83.816%\n",
      "Epoch 20, Batch 561, LR 0.000052 Loss 5.865088, Accuracy 83.810%\n",
      "Epoch 20, Batch 562, LR 0.000052 Loss 5.863549, Accuracy 83.823%\n",
      "Epoch 20, Batch 563, LR 0.000052 Loss 5.862218, Accuracy 83.827%\n",
      "Epoch 20, Batch 564, LR 0.000052 Loss 5.862003, Accuracy 83.835%\n",
      "Epoch 20, Batch 565, LR 0.000052 Loss 5.862138, Accuracy 83.840%\n",
      "Epoch 20, Batch 566, LR 0.000052 Loss 5.862190, Accuracy 83.844%\n",
      "Epoch 20, Batch 567, LR 0.000052 Loss 5.861448, Accuracy 83.850%\n",
      "Epoch 20, Batch 568, LR 0.000052 Loss 5.861308, Accuracy 83.856%\n",
      "Epoch 20, Batch 569, LR 0.000052 Loss 5.861176, Accuracy 83.861%\n",
      "Epoch 20, Batch 570, LR 0.000052 Loss 5.860083, Accuracy 83.871%\n",
      "Epoch 20, Batch 571, LR 0.000052 Loss 5.860267, Accuracy 83.876%\n",
      "Epoch 20, Batch 572, LR 0.000052 Loss 5.861479, Accuracy 83.868%\n",
      "Epoch 20, Batch 573, LR 0.000052 Loss 5.861779, Accuracy 83.868%\n",
      "Epoch 20, Batch 574, LR 0.000052 Loss 5.862175, Accuracy 83.866%\n",
      "Epoch 20, Batch 575, LR 0.000052 Loss 5.861841, Accuracy 83.870%\n",
      "Epoch 20, Batch 576, LR 0.000052 Loss 5.860534, Accuracy 83.883%\n",
      "Epoch 20, Batch 577, LR 0.000052 Loss 5.860044, Accuracy 83.885%\n",
      "Epoch 20, Batch 578, LR 0.000052 Loss 5.859053, Accuracy 83.890%\n",
      "Epoch 20, Batch 579, LR 0.000052 Loss 5.858711, Accuracy 83.881%\n",
      "Epoch 20, Batch 580, LR 0.000052 Loss 5.858819, Accuracy 83.882%\n",
      "Epoch 20, Batch 581, LR 0.000052 Loss 5.858946, Accuracy 83.879%\n",
      "Epoch 20, Batch 582, LR 0.000052 Loss 5.858741, Accuracy 83.880%\n",
      "Epoch 20, Batch 583, LR 0.000052 Loss 5.857119, Accuracy 83.885%\n",
      "Epoch 20, Batch 584, LR 0.000052 Loss 5.857062, Accuracy 83.879%\n",
      "Epoch 20, Batch 585, LR 0.000052 Loss 5.856252, Accuracy 83.880%\n",
      "Epoch 20, Batch 586, LR 0.000052 Loss 5.856439, Accuracy 83.871%\n",
      "Epoch 20, Batch 587, LR 0.000052 Loss 5.856252, Accuracy 83.872%\n",
      "Epoch 20, Batch 588, LR 0.000052 Loss 5.856916, Accuracy 83.869%\n",
      "Epoch 20, Batch 589, LR 0.000052 Loss 5.858080, Accuracy 83.867%\n",
      "Epoch 20, Batch 590, LR 0.000052 Loss 5.857619, Accuracy 83.873%\n",
      "Epoch 20, Batch 591, LR 0.000052 Loss 5.857072, Accuracy 83.878%\n",
      "Epoch 20, Batch 592, LR 0.000052 Loss 5.856594, Accuracy 83.881%\n",
      "Epoch 20, Batch 593, LR 0.000052 Loss 5.857221, Accuracy 83.873%\n",
      "Epoch 20, Batch 594, LR 0.000052 Loss 5.856743, Accuracy 83.873%\n",
      "Epoch 20, Batch 595, LR 0.000052 Loss 5.858145, Accuracy 83.876%\n",
      "Epoch 20, Batch 596, LR 0.000052 Loss 5.857301, Accuracy 83.880%\n",
      "Epoch 20, Batch 597, LR 0.000052 Loss 5.857073, Accuracy 83.879%\n",
      "Epoch 20, Batch 598, LR 0.000052 Loss 5.857420, Accuracy 83.875%\n",
      "Epoch 20, Batch 599, LR 0.000052 Loss 5.857569, Accuracy 83.870%\n",
      "Epoch 20, Batch 600, LR 0.000052 Loss 5.857988, Accuracy 83.871%\n",
      "Epoch 20, Batch 601, LR 0.000052 Loss 5.857630, Accuracy 83.875%\n",
      "Epoch 20, Batch 602, LR 0.000052 Loss 5.857900, Accuracy 83.868%\n",
      "Epoch 20, Batch 603, LR 0.000052 Loss 5.857153, Accuracy 83.874%\n",
      "Epoch 20, Batch 604, LR 0.000052 Loss 5.858056, Accuracy 83.865%\n",
      "Epoch 20, Batch 605, LR 0.000052 Loss 5.858677, Accuracy 83.856%\n",
      "Epoch 20, Batch 606, LR 0.000052 Loss 5.857917, Accuracy 83.859%\n",
      "Epoch 20, Batch 607, LR 0.000052 Loss 5.857573, Accuracy 83.859%\n",
      "Epoch 20, Batch 608, LR 0.000052 Loss 5.856782, Accuracy 83.866%\n",
      "Epoch 20, Batch 609, LR 0.000052 Loss 5.857714, Accuracy 83.859%\n",
      "Epoch 20, Batch 610, LR 0.000052 Loss 5.859656, Accuracy 83.845%\n",
      "Epoch 20, Batch 611, LR 0.000052 Loss 5.859163, Accuracy 83.842%\n",
      "Epoch 20, Batch 612, LR 0.000052 Loss 5.858335, Accuracy 83.843%\n",
      "Epoch 20, Batch 613, LR 0.000052 Loss 5.858739, Accuracy 83.840%\n",
      "Epoch 20, Batch 614, LR 0.000052 Loss 5.858561, Accuracy 83.844%\n",
      "Epoch 20, Batch 615, LR 0.000052 Loss 5.857653, Accuracy 83.845%\n",
      "Epoch 20, Batch 616, LR 0.000052 Loss 5.856811, Accuracy 83.849%\n",
      "Epoch 20, Batch 617, LR 0.000052 Loss 5.857130, Accuracy 83.851%\n",
      "Epoch 20, Batch 618, LR 0.000052 Loss 5.856488, Accuracy 83.849%\n",
      "Epoch 20, Batch 619, LR 0.000052 Loss 5.856617, Accuracy 83.847%\n",
      "Epoch 20, Batch 620, LR 0.000052 Loss 5.857866, Accuracy 83.838%\n",
      "Epoch 20, Batch 621, LR 0.000052 Loss 5.857714, Accuracy 83.840%\n",
      "Epoch 20, Batch 622, LR 0.000052 Loss 5.858387, Accuracy 83.837%\n",
      "Epoch 20, Batch 623, LR 0.000052 Loss 5.857672, Accuracy 83.843%\n",
      "Epoch 20, Batch 624, LR 0.000052 Loss 5.857156, Accuracy 83.850%\n",
      "Epoch 20, Batch 625, LR 0.000052 Loss 5.857235, Accuracy 83.850%\n",
      "Epoch 20, Batch 626, LR 0.000052 Loss 5.856444, Accuracy 83.852%\n",
      "Epoch 20, Batch 627, LR 0.000052 Loss 5.856930, Accuracy 83.848%\n",
      "Epoch 20, Batch 628, LR 0.000052 Loss 5.855969, Accuracy 83.851%\n",
      "Epoch 20, Batch 629, LR 0.000052 Loss 5.855981, Accuracy 83.851%\n",
      "Epoch 20, Batch 630, LR 0.000052 Loss 5.856155, Accuracy 83.850%\n",
      "Epoch 20, Batch 631, LR 0.000052 Loss 5.857792, Accuracy 83.839%\n",
      "Epoch 20, Batch 632, LR 0.000052 Loss 5.858177, Accuracy 83.840%\n",
      "Epoch 20, Batch 633, LR 0.000052 Loss 5.858032, Accuracy 83.837%\n",
      "Epoch 20, Batch 634, LR 0.000052 Loss 5.857769, Accuracy 83.843%\n",
      "Epoch 20, Batch 635, LR 0.000052 Loss 5.857343, Accuracy 83.847%\n",
      "Epoch 20, Batch 636, LR 0.000052 Loss 5.857248, Accuracy 83.850%\n",
      "Epoch 20, Batch 637, LR 0.000052 Loss 5.856865, Accuracy 83.856%\n",
      "Epoch 20, Batch 638, LR 0.000052 Loss 5.857043, Accuracy 83.862%\n",
      "Epoch 20, Batch 639, LR 0.000052 Loss 5.857245, Accuracy 83.863%\n",
      "Epoch 20, Batch 640, LR 0.000052 Loss 5.856716, Accuracy 83.864%\n",
      "Epoch 20, Batch 641, LR 0.000052 Loss 5.856754, Accuracy 83.868%\n",
      "Epoch 20, Batch 642, LR 0.000052 Loss 5.856441, Accuracy 83.871%\n",
      "Epoch 20, Batch 643, LR 0.000052 Loss 5.857649, Accuracy 83.866%\n",
      "Epoch 20, Batch 644, LR 0.000052 Loss 5.857337, Accuracy 83.863%\n",
      "Epoch 20, Batch 645, LR 0.000052 Loss 5.857364, Accuracy 83.863%\n",
      "Epoch 20, Batch 646, LR 0.000052 Loss 5.856948, Accuracy 83.866%\n",
      "Epoch 20, Batch 647, LR 0.000052 Loss 5.858203, Accuracy 83.855%\n",
      "Epoch 20, Batch 648, LR 0.000052 Loss 5.859443, Accuracy 83.849%\n",
      "Epoch 20, Batch 649, LR 0.000052 Loss 5.858700, Accuracy 83.854%\n",
      "Epoch 20, Batch 650, LR 0.000052 Loss 5.859267, Accuracy 83.851%\n",
      "Epoch 20, Batch 651, LR 0.000052 Loss 5.860071, Accuracy 83.853%\n",
      "Epoch 20, Batch 652, LR 0.000052 Loss 5.859029, Accuracy 83.859%\n",
      "Epoch 20, Batch 653, LR 0.000052 Loss 5.859715, Accuracy 83.855%\n",
      "Epoch 20, Batch 654, LR 0.000052 Loss 5.860455, Accuracy 83.851%\n",
      "Epoch 20, Batch 655, LR 0.000052 Loss 5.860933, Accuracy 83.849%\n",
      "Epoch 20, Batch 656, LR 0.000052 Loss 5.860243, Accuracy 83.857%\n",
      "Epoch 20, Batch 657, LR 0.000052 Loss 5.859168, Accuracy 83.858%\n",
      "Epoch 20, Batch 658, LR 0.000052 Loss 5.859413, Accuracy 83.856%\n",
      "Epoch 20, Batch 659, LR 0.000052 Loss 5.860396, Accuracy 83.845%\n",
      "Epoch 20, Batch 660, LR 0.000052 Loss 5.859995, Accuracy 83.845%\n",
      "Epoch 20, Batch 661, LR 0.000052 Loss 5.861062, Accuracy 83.841%\n",
      "Epoch 20, Batch 662, LR 0.000052 Loss 5.860958, Accuracy 83.834%\n",
      "Epoch 20, Batch 663, LR 0.000052 Loss 5.860434, Accuracy 83.836%\n",
      "Epoch 20, Batch 664, LR 0.000052 Loss 5.859400, Accuracy 83.836%\n",
      "Epoch 20, Batch 665, LR 0.000052 Loss 5.858965, Accuracy 83.842%\n",
      "Epoch 20, Batch 666, LR 0.000052 Loss 5.858322, Accuracy 83.849%\n",
      "Epoch 20, Batch 667, LR 0.000052 Loss 5.858812, Accuracy 83.847%\n",
      "Epoch 20, Batch 668, LR 0.000051 Loss 5.859256, Accuracy 83.848%\n",
      "Epoch 20, Batch 669, LR 0.000051 Loss 5.859372, Accuracy 83.848%\n",
      "Epoch 20, Batch 670, LR 0.000051 Loss 5.859411, Accuracy 83.854%\n",
      "Epoch 20, Batch 671, LR 0.000051 Loss 5.859774, Accuracy 83.855%\n",
      "Epoch 20, Batch 672, LR 0.000051 Loss 5.858521, Accuracy 83.859%\n",
      "Epoch 20, Batch 673, LR 0.000051 Loss 5.858366, Accuracy 83.855%\n",
      "Epoch 20, Batch 674, LR 0.000051 Loss 5.858496, Accuracy 83.855%\n",
      "Epoch 20, Batch 675, LR 0.000051 Loss 5.857646, Accuracy 83.860%\n",
      "Epoch 20, Batch 676, LR 0.000051 Loss 5.857900, Accuracy 83.855%\n",
      "Epoch 20, Batch 677, LR 0.000051 Loss 5.856494, Accuracy 83.861%\n",
      "Epoch 20, Batch 678, LR 0.000051 Loss 5.856734, Accuracy 83.863%\n",
      "Epoch 20, Batch 679, LR 0.000051 Loss 5.856779, Accuracy 83.865%\n",
      "Epoch 20, Batch 680, LR 0.000051 Loss 5.857746, Accuracy 83.864%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 681, LR 0.000051 Loss 5.858366, Accuracy 83.858%\n",
      "Epoch 20, Batch 682, LR 0.000051 Loss 5.857347, Accuracy 83.861%\n",
      "Epoch 20, Batch 683, LR 0.000051 Loss 5.856603, Accuracy 83.863%\n",
      "Epoch 20, Batch 684, LR 0.000051 Loss 5.855943, Accuracy 83.867%\n",
      "Epoch 20, Batch 685, LR 0.000051 Loss 5.855020, Accuracy 83.867%\n",
      "Epoch 20, Batch 686, LR 0.000051 Loss 5.854714, Accuracy 83.867%\n",
      "Epoch 20, Batch 687, LR 0.000051 Loss 5.854866, Accuracy 83.860%\n",
      "Epoch 20, Batch 688, LR 0.000051 Loss 5.855055, Accuracy 83.862%\n",
      "Epoch 20, Batch 689, LR 0.000051 Loss 5.855269, Accuracy 83.862%\n",
      "Epoch 20, Batch 690, LR 0.000051 Loss 5.855396, Accuracy 83.853%\n",
      "Epoch 20, Batch 691, LR 0.000051 Loss 5.855653, Accuracy 83.849%\n",
      "Epoch 20, Batch 692, LR 0.000051 Loss 5.855715, Accuracy 83.852%\n",
      "Epoch 20, Batch 693, LR 0.000051 Loss 5.854499, Accuracy 83.856%\n",
      "Epoch 20, Batch 694, LR 0.000051 Loss 5.855472, Accuracy 83.854%\n",
      "Epoch 20, Batch 695, LR 0.000051 Loss 5.855320, Accuracy 83.856%\n",
      "Epoch 20, Batch 696, LR 0.000051 Loss 5.856576, Accuracy 83.844%\n",
      "Epoch 20, Batch 697, LR 0.000051 Loss 5.856363, Accuracy 83.843%\n",
      "Epoch 20, Batch 698, LR 0.000051 Loss 5.855956, Accuracy 83.849%\n",
      "Epoch 20, Batch 699, LR 0.000051 Loss 5.854995, Accuracy 83.855%\n",
      "Epoch 20, Batch 700, LR 0.000051 Loss 5.854695, Accuracy 83.857%\n",
      "Epoch 20, Batch 701, LR 0.000051 Loss 5.854839, Accuracy 83.853%\n",
      "Epoch 20, Batch 702, LR 0.000051 Loss 5.855781, Accuracy 83.846%\n",
      "Epoch 20, Batch 703, LR 0.000051 Loss 5.856135, Accuracy 83.840%\n",
      "Epoch 20, Batch 704, LR 0.000051 Loss 5.855670, Accuracy 83.840%\n",
      "Epoch 20, Batch 705, LR 0.000051 Loss 5.856024, Accuracy 83.835%\n",
      "Epoch 20, Batch 706, LR 0.000051 Loss 5.855025, Accuracy 83.837%\n",
      "Epoch 20, Batch 707, LR 0.000051 Loss 5.855376, Accuracy 83.836%\n",
      "Epoch 20, Batch 708, LR 0.000051 Loss 5.855497, Accuracy 83.838%\n",
      "Epoch 20, Batch 709, LR 0.000051 Loss 5.855422, Accuracy 83.844%\n",
      "Epoch 20, Batch 710, LR 0.000051 Loss 5.855138, Accuracy 83.845%\n",
      "Epoch 20, Batch 711, LR 0.000051 Loss 5.854831, Accuracy 83.848%\n",
      "Epoch 20, Batch 712, LR 0.000051 Loss 5.855235, Accuracy 83.852%\n",
      "Epoch 20, Batch 713, LR 0.000051 Loss 5.855667, Accuracy 83.850%\n",
      "Epoch 20, Batch 714, LR 0.000051 Loss 5.856170, Accuracy 83.844%\n",
      "Epoch 20, Batch 715, LR 0.000051 Loss 5.855217, Accuracy 83.855%\n",
      "Epoch 20, Batch 716, LR 0.000051 Loss 5.855259, Accuracy 83.856%\n",
      "Epoch 20, Batch 717, LR 0.000051 Loss 5.853736, Accuracy 83.855%\n",
      "Epoch 20, Batch 718, LR 0.000051 Loss 5.854292, Accuracy 83.853%\n",
      "Epoch 20, Batch 719, LR 0.000051 Loss 5.853848, Accuracy 83.860%\n",
      "Epoch 20, Batch 720, LR 0.000051 Loss 5.853266, Accuracy 83.861%\n",
      "Epoch 20, Batch 721, LR 0.000051 Loss 5.853336, Accuracy 83.859%\n",
      "Epoch 20, Batch 722, LR 0.000051 Loss 5.853638, Accuracy 83.858%\n",
      "Epoch 20, Batch 723, LR 0.000051 Loss 5.853552, Accuracy 83.857%\n",
      "Epoch 20, Batch 724, LR 0.000051 Loss 5.854352, Accuracy 83.860%\n",
      "Epoch 20, Batch 725, LR 0.000051 Loss 5.853722, Accuracy 83.867%\n",
      "Epoch 20, Batch 726, LR 0.000051 Loss 5.853475, Accuracy 83.872%\n",
      "Epoch 20, Batch 727, LR 0.000051 Loss 5.853719, Accuracy 83.872%\n",
      "Epoch 20, Batch 728, LR 0.000051 Loss 5.854552, Accuracy 83.870%\n",
      "Epoch 20, Batch 729, LR 0.000051 Loss 5.853374, Accuracy 83.873%\n",
      "Epoch 20, Batch 730, LR 0.000051 Loss 5.854281, Accuracy 83.867%\n",
      "Epoch 20, Batch 731, LR 0.000051 Loss 5.855000, Accuracy 83.864%\n",
      "Epoch 20, Batch 732, LR 0.000051 Loss 5.854247, Accuracy 83.868%\n",
      "Epoch 20, Batch 733, LR 0.000051 Loss 5.854398, Accuracy 83.874%\n",
      "Epoch 20, Batch 734, LR 0.000051 Loss 5.854485, Accuracy 83.872%\n",
      "Epoch 20, Batch 735, LR 0.000051 Loss 5.855069, Accuracy 83.866%\n",
      "Epoch 20, Batch 736, LR 0.000051 Loss 5.855389, Accuracy 83.858%\n",
      "Epoch 20, Batch 737, LR 0.000051 Loss 5.855047, Accuracy 83.862%\n",
      "Epoch 20, Batch 738, LR 0.000051 Loss 5.854871, Accuracy 83.856%\n",
      "Epoch 20, Batch 739, LR 0.000051 Loss 5.855752, Accuracy 83.853%\n",
      "Epoch 20, Batch 740, LR 0.000051 Loss 5.856626, Accuracy 83.843%\n",
      "Epoch 20, Batch 741, LR 0.000051 Loss 5.856114, Accuracy 83.845%\n",
      "Epoch 20, Batch 742, LR 0.000051 Loss 5.855105, Accuracy 83.849%\n",
      "Epoch 20, Batch 743, LR 0.000051 Loss 5.854809, Accuracy 83.849%\n",
      "Epoch 20, Batch 744, LR 0.000051 Loss 5.853749, Accuracy 83.851%\n",
      "Epoch 20, Batch 745, LR 0.000051 Loss 5.853626, Accuracy 83.848%\n",
      "Epoch 20, Batch 746, LR 0.000051 Loss 5.854154, Accuracy 83.845%\n",
      "Epoch 20, Batch 747, LR 0.000051 Loss 5.854200, Accuracy 83.847%\n",
      "Epoch 20, Batch 748, LR 0.000051 Loss 5.854512, Accuracy 83.847%\n",
      "Epoch 20, Batch 749, LR 0.000051 Loss 5.854808, Accuracy 83.845%\n",
      "Epoch 20, Batch 750, LR 0.000051 Loss 5.855130, Accuracy 83.844%\n",
      "Epoch 20, Batch 751, LR 0.000051 Loss 5.854615, Accuracy 83.850%\n",
      "Epoch 20, Batch 752, LR 0.000051 Loss 5.855417, Accuracy 83.848%\n",
      "Epoch 20, Batch 753, LR 0.000051 Loss 5.855829, Accuracy 83.844%\n",
      "Epoch 20, Batch 754, LR 0.000051 Loss 5.856182, Accuracy 83.848%\n",
      "Epoch 20, Batch 755, LR 0.000051 Loss 5.856583, Accuracy 83.842%\n",
      "Epoch 20, Batch 756, LR 0.000051 Loss 5.856709, Accuracy 83.840%\n",
      "Epoch 20, Batch 757, LR 0.000051 Loss 5.856914, Accuracy 83.847%\n",
      "Epoch 20, Batch 758, LR 0.000051 Loss 5.856344, Accuracy 83.849%\n",
      "Epoch 20, Batch 759, LR 0.000051 Loss 5.856089, Accuracy 83.851%\n",
      "Epoch 20, Batch 760, LR 0.000051 Loss 5.856272, Accuracy 83.852%\n",
      "Epoch 20, Batch 761, LR 0.000051 Loss 5.856720, Accuracy 83.851%\n",
      "Epoch 20, Batch 762, LR 0.000051 Loss 5.857112, Accuracy 83.849%\n",
      "Epoch 20, Batch 763, LR 0.000051 Loss 5.856941, Accuracy 83.855%\n",
      "Epoch 20, Batch 764, LR 0.000051 Loss 5.856864, Accuracy 83.853%\n",
      "Epoch 20, Batch 765, LR 0.000051 Loss 5.856339, Accuracy 83.856%\n",
      "Epoch 20, Batch 766, LR 0.000051 Loss 5.856526, Accuracy 83.854%\n",
      "Epoch 20, Batch 767, LR 0.000051 Loss 5.857143, Accuracy 83.852%\n",
      "Epoch 20, Batch 768, LR 0.000051 Loss 5.858531, Accuracy 83.844%\n",
      "Epoch 20, Batch 769, LR 0.000051 Loss 5.858535, Accuracy 83.841%\n",
      "Epoch 20, Batch 770, LR 0.000051 Loss 5.857805, Accuracy 83.844%\n",
      "Epoch 20, Batch 771, LR 0.000051 Loss 5.857391, Accuracy 83.846%\n",
      "Epoch 20, Batch 772, LR 0.000051 Loss 5.857860, Accuracy 83.847%\n",
      "Epoch 20, Batch 773, LR 0.000051 Loss 5.858438, Accuracy 83.844%\n",
      "Epoch 20, Batch 774, LR 0.000051 Loss 5.857914, Accuracy 83.849%\n",
      "Epoch 20, Batch 775, LR 0.000051 Loss 5.858779, Accuracy 83.848%\n",
      "Epoch 20, Batch 776, LR 0.000051 Loss 5.859238, Accuracy 83.845%\n",
      "Epoch 20, Batch 777, LR 0.000051 Loss 5.859850, Accuracy 83.839%\n",
      "Epoch 20, Batch 778, LR 0.000051 Loss 5.860193, Accuracy 83.843%\n",
      "Epoch 20, Batch 779, LR 0.000051 Loss 5.860796, Accuracy 83.840%\n",
      "Epoch 20, Batch 780, LR 0.000051 Loss 5.860800, Accuracy 83.846%\n",
      "Epoch 20, Batch 781, LR 0.000051 Loss 5.861138, Accuracy 83.848%\n",
      "Epoch 20, Batch 782, LR 0.000051 Loss 5.861631, Accuracy 83.848%\n",
      "Epoch 20, Batch 783, LR 0.000051 Loss 5.860739, Accuracy 83.848%\n",
      "Epoch 20, Batch 784, LR 0.000051 Loss 5.861421, Accuracy 83.851%\n",
      "Epoch 20, Batch 785, LR 0.000051 Loss 5.861720, Accuracy 83.854%\n",
      "Epoch 20, Batch 786, LR 0.000051 Loss 5.860748, Accuracy 83.858%\n",
      "Epoch 20, Batch 787, LR 0.000051 Loss 5.861119, Accuracy 83.861%\n",
      "Epoch 20, Batch 788, LR 0.000051 Loss 5.860461, Accuracy 83.861%\n",
      "Epoch 20, Batch 789, LR 0.000051 Loss 5.860655, Accuracy 83.861%\n",
      "Epoch 20, Batch 790, LR 0.000051 Loss 5.860831, Accuracy 83.861%\n",
      "Epoch 20, Batch 791, LR 0.000051 Loss 5.860974, Accuracy 83.859%\n",
      "Epoch 20, Batch 792, LR 0.000051 Loss 5.861084, Accuracy 83.862%\n",
      "Epoch 20, Batch 793, LR 0.000051 Loss 5.859824, Accuracy 83.864%\n",
      "Epoch 20, Batch 794, LR 0.000051 Loss 5.860211, Accuracy 83.864%\n",
      "Epoch 20, Batch 795, LR 0.000051 Loss 5.859674, Accuracy 83.867%\n",
      "Epoch 20, Batch 796, LR 0.000051 Loss 5.858917, Accuracy 83.873%\n",
      "Epoch 20, Batch 797, LR 0.000051 Loss 5.859494, Accuracy 83.875%\n",
      "Epoch 20, Batch 798, LR 0.000051 Loss 5.860869, Accuracy 83.867%\n",
      "Epoch 20, Batch 799, LR 0.000051 Loss 5.860099, Accuracy 83.872%\n",
      "Epoch 20, Batch 800, LR 0.000051 Loss 5.859800, Accuracy 83.874%\n",
      "Epoch 20, Batch 801, LR 0.000051 Loss 5.859775, Accuracy 83.869%\n",
      "Epoch 20, Batch 802, LR 0.000051 Loss 5.859274, Accuracy 83.871%\n",
      "Epoch 20, Batch 803, LR 0.000051 Loss 5.859904, Accuracy 83.870%\n",
      "Epoch 20, Batch 804, LR 0.000051 Loss 5.860867, Accuracy 83.859%\n",
      "Epoch 20, Batch 805, LR 0.000051 Loss 5.860389, Accuracy 83.861%\n",
      "Epoch 20, Batch 806, LR 0.000051 Loss 5.860629, Accuracy 83.857%\n",
      "Epoch 20, Batch 807, LR 0.000051 Loss 5.861205, Accuracy 83.855%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 808, LR 0.000051 Loss 5.860612, Accuracy 83.857%\n",
      "Epoch 20, Batch 809, LR 0.000051 Loss 5.860443, Accuracy 83.858%\n",
      "Epoch 20, Batch 810, LR 0.000051 Loss 5.860959, Accuracy 83.856%\n",
      "Epoch 20, Batch 811, LR 0.000051 Loss 5.859806, Accuracy 83.863%\n",
      "Epoch 20, Batch 812, LR 0.000051 Loss 5.860275, Accuracy 83.862%\n",
      "Epoch 20, Batch 813, LR 0.000051 Loss 5.860020, Accuracy 83.864%\n",
      "Epoch 20, Batch 814, LR 0.000051 Loss 5.860141, Accuracy 83.862%\n",
      "Epoch 20, Batch 815, LR 0.000051 Loss 5.859741, Accuracy 83.867%\n",
      "Epoch 20, Batch 816, LR 0.000051 Loss 5.859499, Accuracy 83.869%\n",
      "Epoch 20, Batch 817, LR 0.000051 Loss 5.858202, Accuracy 83.878%\n",
      "Epoch 20, Batch 818, LR 0.000051 Loss 5.858236, Accuracy 83.880%\n",
      "Epoch 20, Batch 819, LR 0.000051 Loss 5.859094, Accuracy 83.880%\n",
      "Epoch 20, Batch 820, LR 0.000051 Loss 5.859747, Accuracy 83.880%\n",
      "Epoch 20, Batch 821, LR 0.000051 Loss 5.859767, Accuracy 83.880%\n",
      "Epoch 20, Batch 822, LR 0.000051 Loss 5.859850, Accuracy 83.880%\n",
      "Epoch 20, Batch 823, LR 0.000051 Loss 5.859981, Accuracy 83.880%\n",
      "Epoch 20, Batch 824, LR 0.000051 Loss 5.859343, Accuracy 83.883%\n",
      "Epoch 20, Batch 825, LR 0.000051 Loss 5.859424, Accuracy 83.884%\n",
      "Epoch 20, Batch 826, LR 0.000051 Loss 5.858832, Accuracy 83.889%\n",
      "Epoch 20, Batch 827, LR 0.000051 Loss 5.859154, Accuracy 83.889%\n",
      "Epoch 20, Batch 828, LR 0.000051 Loss 5.858798, Accuracy 83.899%\n",
      "Epoch 20, Batch 829, LR 0.000051 Loss 5.858263, Accuracy 83.904%\n",
      "Epoch 20, Batch 830, LR 0.000051 Loss 5.858083, Accuracy 83.905%\n",
      "Epoch 20, Batch 831, LR 0.000051 Loss 5.857038, Accuracy 83.911%\n",
      "Epoch 20, Batch 832, LR 0.000051 Loss 5.857377, Accuracy 83.912%\n",
      "Epoch 20, Batch 833, LR 0.000051 Loss 5.858128, Accuracy 83.906%\n",
      "Epoch 20, Batch 834, LR 0.000051 Loss 5.858125, Accuracy 83.907%\n",
      "Epoch 20, Batch 835, LR 0.000051 Loss 5.857940, Accuracy 83.910%\n",
      "Epoch 20, Batch 836, LR 0.000051 Loss 5.858456, Accuracy 83.908%\n",
      "Epoch 20, Batch 837, LR 0.000051 Loss 5.857701, Accuracy 83.912%\n",
      "Epoch 20, Batch 838, LR 0.000051 Loss 5.858549, Accuracy 83.910%\n",
      "Epoch 20, Batch 839, LR 0.000051 Loss 5.858183, Accuracy 83.912%\n",
      "Epoch 20, Batch 840, LR 0.000051 Loss 5.857155, Accuracy 83.918%\n",
      "Epoch 20, Batch 841, LR 0.000051 Loss 5.857642, Accuracy 83.915%\n",
      "Epoch 20, Batch 842, LR 0.000051 Loss 5.858123, Accuracy 83.914%\n",
      "Epoch 20, Batch 843, LR 0.000051 Loss 5.857694, Accuracy 83.915%\n",
      "Epoch 20, Batch 844, LR 0.000051 Loss 5.858375, Accuracy 83.915%\n",
      "Epoch 20, Batch 845, LR 0.000051 Loss 5.858160, Accuracy 83.910%\n",
      "Epoch 20, Batch 846, LR 0.000051 Loss 5.858247, Accuracy 83.907%\n",
      "Epoch 20, Batch 847, LR 0.000051 Loss 5.858071, Accuracy 83.903%\n",
      "Epoch 20, Batch 848, LR 0.000051 Loss 5.859215, Accuracy 83.893%\n",
      "Epoch 20, Batch 849, LR 0.000051 Loss 5.860283, Accuracy 83.882%\n",
      "Epoch 20, Batch 850, LR 0.000051 Loss 5.859681, Accuracy 83.882%\n",
      "Epoch 20, Batch 851, LR 0.000051 Loss 5.859629, Accuracy 83.877%\n",
      "Epoch 20, Batch 852, LR 0.000051 Loss 5.859541, Accuracy 83.878%\n",
      "Epoch 20, Batch 853, LR 0.000051 Loss 5.860099, Accuracy 83.874%\n",
      "Epoch 20, Batch 854, LR 0.000051 Loss 5.860310, Accuracy 83.872%\n",
      "Epoch 20, Batch 855, LR 0.000051 Loss 5.859964, Accuracy 83.877%\n",
      "Epoch 20, Batch 856, LR 0.000051 Loss 5.859972, Accuracy 83.876%\n",
      "Epoch 20, Batch 857, LR 0.000051 Loss 5.859577, Accuracy 83.876%\n",
      "Epoch 20, Batch 858, LR 0.000051 Loss 5.860635, Accuracy 83.869%\n",
      "Epoch 20, Batch 859, LR 0.000051 Loss 5.861039, Accuracy 83.872%\n",
      "Epoch 20, Batch 860, LR 0.000051 Loss 5.861427, Accuracy 83.870%\n",
      "Epoch 20, Batch 861, LR 0.000051 Loss 5.861063, Accuracy 83.873%\n",
      "Epoch 20, Batch 862, LR 0.000051 Loss 5.861135, Accuracy 83.873%\n",
      "Epoch 20, Batch 863, LR 0.000051 Loss 5.861597, Accuracy 83.870%\n",
      "Epoch 20, Batch 864, LR 0.000051 Loss 5.862145, Accuracy 83.866%\n",
      "Epoch 20, Batch 865, LR 0.000051 Loss 5.861970, Accuracy 83.869%\n",
      "Epoch 20, Batch 866, LR 0.000051 Loss 5.861891, Accuracy 83.873%\n",
      "Epoch 20, Batch 867, LR 0.000051 Loss 5.862546, Accuracy 83.868%\n",
      "Epoch 20, Batch 868, LR 0.000051 Loss 5.862854, Accuracy 83.862%\n",
      "Epoch 20, Batch 869, LR 0.000051 Loss 5.863024, Accuracy 83.862%\n",
      "Epoch 20, Batch 870, LR 0.000051 Loss 5.863132, Accuracy 83.857%\n",
      "Epoch 20, Batch 871, LR 0.000051 Loss 5.863603, Accuracy 83.857%\n",
      "Epoch 20, Batch 872, LR 0.000051 Loss 5.863885, Accuracy 83.854%\n",
      "Epoch 20, Batch 873, LR 0.000051 Loss 5.862290, Accuracy 83.862%\n",
      "Epoch 20, Batch 874, LR 0.000051 Loss 5.862650, Accuracy 83.865%\n",
      "Epoch 20, Batch 875, LR 0.000051 Loss 5.861682, Accuracy 83.868%\n",
      "Epoch 20, Batch 876, LR 0.000051 Loss 5.861296, Accuracy 83.873%\n",
      "Epoch 20, Batch 877, LR 0.000051 Loss 5.861128, Accuracy 83.870%\n",
      "Epoch 20, Batch 878, LR 0.000051 Loss 5.860682, Accuracy 83.876%\n",
      "Epoch 20, Batch 879, LR 0.000051 Loss 5.860697, Accuracy 83.870%\n",
      "Epoch 20, Batch 880, LR 0.000051 Loss 5.860431, Accuracy 83.869%\n",
      "Epoch 20, Batch 881, LR 0.000051 Loss 5.859672, Accuracy 83.865%\n",
      "Epoch 20, Batch 882, LR 0.000051 Loss 5.859425, Accuracy 83.866%\n",
      "Epoch 20, Batch 883, LR 0.000051 Loss 5.860317, Accuracy 83.858%\n",
      "Epoch 20, Batch 884, LR 0.000051 Loss 5.859417, Accuracy 83.862%\n",
      "Epoch 20, Batch 885, LR 0.000051 Loss 5.858806, Accuracy 83.865%\n",
      "Epoch 20, Batch 886, LR 0.000051 Loss 5.857788, Accuracy 83.864%\n",
      "Epoch 20, Batch 887, LR 0.000051 Loss 5.859117, Accuracy 83.859%\n",
      "Epoch 20, Batch 888, LR 0.000051 Loss 5.858724, Accuracy 83.860%\n",
      "Epoch 20, Batch 889, LR 0.000051 Loss 5.858256, Accuracy 83.864%\n",
      "Epoch 20, Batch 890, LR 0.000051 Loss 5.858082, Accuracy 83.864%\n",
      "Epoch 20, Batch 891, LR 0.000051 Loss 5.858298, Accuracy 83.866%\n",
      "Epoch 20, Batch 892, LR 0.000051 Loss 5.859125, Accuracy 83.862%\n",
      "Epoch 20, Batch 893, LR 0.000051 Loss 5.859159, Accuracy 83.861%\n",
      "Epoch 20, Batch 894, LR 0.000051 Loss 5.860405, Accuracy 83.855%\n",
      "Epoch 20, Batch 895, LR 0.000051 Loss 5.859915, Accuracy 83.858%\n",
      "Epoch 20, Batch 896, LR 0.000051 Loss 5.859689, Accuracy 83.856%\n",
      "Epoch 20, Batch 897, LR 0.000051 Loss 5.859875, Accuracy 83.859%\n",
      "Epoch 20, Batch 898, LR 0.000051 Loss 5.859877, Accuracy 83.856%\n",
      "Epoch 20, Batch 899, LR 0.000051 Loss 5.860009, Accuracy 83.854%\n",
      "Epoch 20, Batch 900, LR 0.000051 Loss 5.860728, Accuracy 83.846%\n",
      "Epoch 20, Batch 901, LR 0.000051 Loss 5.860847, Accuracy 83.844%\n",
      "Epoch 20, Batch 902, LR 0.000051 Loss 5.860780, Accuracy 83.845%\n",
      "Epoch 20, Batch 903, LR 0.000051 Loss 5.860268, Accuracy 83.846%\n",
      "Epoch 20, Batch 904, LR 0.000051 Loss 5.860277, Accuracy 83.850%\n",
      "Epoch 20, Batch 905, LR 0.000051 Loss 5.859923, Accuracy 83.850%\n",
      "Epoch 20, Batch 906, LR 0.000051 Loss 5.859400, Accuracy 83.854%\n",
      "Epoch 20, Batch 907, LR 0.000051 Loss 5.860184, Accuracy 83.856%\n",
      "Epoch 20, Batch 908, LR 0.000051 Loss 5.859969, Accuracy 83.862%\n",
      "Epoch 20, Batch 909, LR 0.000051 Loss 5.860170, Accuracy 83.866%\n",
      "Epoch 20, Batch 910, LR 0.000051 Loss 5.860572, Accuracy 83.867%\n",
      "Epoch 20, Batch 911, LR 0.000051 Loss 5.860234, Accuracy 83.866%\n",
      "Epoch 20, Batch 912, LR 0.000051 Loss 5.860132, Accuracy 83.867%\n",
      "Epoch 20, Batch 913, LR 0.000051 Loss 5.860040, Accuracy 83.866%\n",
      "Epoch 20, Batch 914, LR 0.000051 Loss 5.860812, Accuracy 83.860%\n",
      "Epoch 20, Batch 915, LR 0.000051 Loss 5.861031, Accuracy 83.864%\n",
      "Epoch 20, Batch 916, LR 0.000051 Loss 5.860491, Accuracy 83.866%\n",
      "Epoch 20, Batch 917, LR 0.000051 Loss 5.861467, Accuracy 83.863%\n",
      "Epoch 20, Batch 918, LR 0.000051 Loss 5.861712, Accuracy 83.869%\n",
      "Epoch 20, Batch 919, LR 0.000051 Loss 5.861421, Accuracy 83.873%\n",
      "Epoch 20, Batch 920, LR 0.000051 Loss 5.860751, Accuracy 83.877%\n",
      "Epoch 20, Batch 921, LR 0.000050 Loss 5.861301, Accuracy 83.873%\n",
      "Epoch 20, Batch 922, LR 0.000050 Loss 5.860579, Accuracy 83.878%\n",
      "Epoch 20, Batch 923, LR 0.000050 Loss 5.859768, Accuracy 83.881%\n",
      "Epoch 20, Batch 924, LR 0.000050 Loss 5.859951, Accuracy 83.880%\n",
      "Epoch 20, Batch 925, LR 0.000050 Loss 5.859316, Accuracy 83.884%\n",
      "Epoch 20, Batch 926, LR 0.000050 Loss 5.859107, Accuracy 83.885%\n",
      "Epoch 20, Batch 927, LR 0.000050 Loss 5.859336, Accuracy 83.884%\n",
      "Epoch 20, Batch 928, LR 0.000050 Loss 5.859189, Accuracy 83.883%\n",
      "Epoch 20, Batch 929, LR 0.000050 Loss 5.858891, Accuracy 83.881%\n",
      "Epoch 20, Batch 930, LR 0.000050 Loss 5.858747, Accuracy 83.880%\n",
      "Epoch 20, Batch 931, LR 0.000050 Loss 5.859011, Accuracy 83.886%\n",
      "Epoch 20, Batch 932, LR 0.000050 Loss 5.858828, Accuracy 83.884%\n",
      "Epoch 20, Batch 933, LR 0.000050 Loss 5.858509, Accuracy 83.883%\n",
      "Epoch 20, Batch 934, LR 0.000050 Loss 5.858328, Accuracy 83.886%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 935, LR 0.000050 Loss 5.858224, Accuracy 83.885%\n",
      "Epoch 20, Batch 936, LR 0.000050 Loss 5.858236, Accuracy 83.883%\n",
      "Epoch 20, Batch 937, LR 0.000050 Loss 5.857652, Accuracy 83.887%\n",
      "Epoch 20, Batch 938, LR 0.000050 Loss 5.857361, Accuracy 83.888%\n",
      "Epoch 20, Batch 939, LR 0.000050 Loss 5.857015, Accuracy 83.888%\n",
      "Epoch 20, Batch 940, LR 0.000050 Loss 5.856969, Accuracy 83.888%\n",
      "Epoch 20, Batch 941, LR 0.000050 Loss 5.856771, Accuracy 83.892%\n",
      "Epoch 20, Batch 942, LR 0.000050 Loss 5.857267, Accuracy 83.888%\n",
      "Epoch 20, Batch 943, LR 0.000050 Loss 5.856738, Accuracy 83.894%\n",
      "Epoch 20, Batch 944, LR 0.000050 Loss 5.857325, Accuracy 83.888%\n",
      "Epoch 20, Batch 945, LR 0.000050 Loss 5.857330, Accuracy 83.886%\n",
      "Epoch 20, Batch 946, LR 0.000050 Loss 5.857493, Accuracy 83.884%\n",
      "Epoch 20, Batch 947, LR 0.000050 Loss 5.856939, Accuracy 83.885%\n",
      "Epoch 20, Batch 948, LR 0.000050 Loss 5.857043, Accuracy 83.883%\n",
      "Epoch 20, Batch 949, LR 0.000050 Loss 5.857838, Accuracy 83.875%\n",
      "Epoch 20, Batch 950, LR 0.000050 Loss 5.857516, Accuracy 83.877%\n",
      "Epoch 20, Batch 951, LR 0.000050 Loss 5.858159, Accuracy 83.876%\n",
      "Epoch 20, Batch 952, LR 0.000050 Loss 5.858059, Accuracy 83.879%\n",
      "Epoch 20, Batch 953, LR 0.000050 Loss 5.856538, Accuracy 83.887%\n",
      "Epoch 20, Batch 954, LR 0.000050 Loss 5.856341, Accuracy 83.891%\n",
      "Epoch 20, Batch 955, LR 0.000050 Loss 5.856083, Accuracy 83.896%\n",
      "Epoch 20, Batch 956, LR 0.000050 Loss 5.856118, Accuracy 83.896%\n",
      "Epoch 20, Batch 957, LR 0.000050 Loss 5.856342, Accuracy 83.892%\n",
      "Epoch 20, Batch 958, LR 0.000050 Loss 5.856721, Accuracy 83.895%\n",
      "Epoch 20, Batch 959, LR 0.000050 Loss 5.857121, Accuracy 83.892%\n",
      "Epoch 20, Batch 960, LR 0.000050 Loss 5.856989, Accuracy 83.894%\n",
      "Epoch 20, Batch 961, LR 0.000050 Loss 5.856024, Accuracy 83.899%\n",
      "Epoch 20, Batch 962, LR 0.000050 Loss 5.855910, Accuracy 83.897%\n",
      "Epoch 20, Batch 963, LR 0.000050 Loss 5.856229, Accuracy 83.894%\n",
      "Epoch 20, Batch 964, LR 0.000050 Loss 5.856998, Accuracy 83.886%\n",
      "Epoch 20, Batch 965, LR 0.000050 Loss 5.857028, Accuracy 83.886%\n",
      "Epoch 20, Batch 966, LR 0.000050 Loss 5.856681, Accuracy 83.887%\n",
      "Epoch 20, Batch 967, LR 0.000050 Loss 5.855990, Accuracy 83.890%\n",
      "Epoch 20, Batch 968, LR 0.000050 Loss 5.855824, Accuracy 83.890%\n",
      "Epoch 20, Batch 969, LR 0.000050 Loss 5.856863, Accuracy 83.890%\n",
      "Epoch 20, Batch 970, LR 0.000050 Loss 5.858145, Accuracy 83.885%\n",
      "Epoch 20, Batch 971, LR 0.000050 Loss 5.857116, Accuracy 83.891%\n",
      "Epoch 20, Batch 972, LR 0.000050 Loss 5.857461, Accuracy 83.892%\n",
      "Epoch 20, Batch 973, LR 0.000050 Loss 5.856941, Accuracy 83.901%\n",
      "Epoch 20, Batch 974, LR 0.000050 Loss 5.856362, Accuracy 83.903%\n",
      "Epoch 20, Batch 975, LR 0.000050 Loss 5.856259, Accuracy 83.904%\n",
      "Epoch 20, Batch 976, LR 0.000050 Loss 5.856831, Accuracy 83.901%\n",
      "Epoch 20, Batch 977, LR 0.000050 Loss 5.856680, Accuracy 83.902%\n",
      "Epoch 20, Batch 978, LR 0.000050 Loss 5.856222, Accuracy 83.906%\n",
      "Epoch 20, Batch 979, LR 0.000050 Loss 5.856024, Accuracy 83.910%\n",
      "Epoch 20, Batch 980, LR 0.000050 Loss 5.855829, Accuracy 83.909%\n",
      "Epoch 20, Batch 981, LR 0.000050 Loss 5.855512, Accuracy 83.913%\n",
      "Epoch 20, Batch 982, LR 0.000050 Loss 5.855069, Accuracy 83.918%\n",
      "Epoch 20, Batch 983, LR 0.000050 Loss 5.855267, Accuracy 83.918%\n",
      "Epoch 20, Batch 984, LR 0.000050 Loss 5.855025, Accuracy 83.920%\n",
      "Epoch 20, Batch 985, LR 0.000050 Loss 5.854346, Accuracy 83.923%\n",
      "Epoch 20, Batch 986, LR 0.000050 Loss 5.853922, Accuracy 83.926%\n",
      "Epoch 20, Batch 987, LR 0.000050 Loss 5.854574, Accuracy 83.920%\n",
      "Epoch 20, Batch 988, LR 0.000050 Loss 5.855035, Accuracy 83.918%\n",
      "Epoch 20, Batch 989, LR 0.000050 Loss 5.854830, Accuracy 83.922%\n",
      "Epoch 20, Batch 990, LR 0.000050 Loss 5.855095, Accuracy 83.919%\n",
      "Epoch 20, Batch 991, LR 0.000050 Loss 5.855445, Accuracy 83.921%\n",
      "Epoch 20, Batch 992, LR 0.000050 Loss 5.855130, Accuracy 83.922%\n",
      "Epoch 20, Batch 993, LR 0.000050 Loss 5.855362, Accuracy 83.923%\n",
      "Epoch 20, Batch 994, LR 0.000050 Loss 5.855847, Accuracy 83.924%\n",
      "Epoch 20, Batch 995, LR 0.000050 Loss 5.855954, Accuracy 83.922%\n",
      "Epoch 20, Batch 996, LR 0.000050 Loss 5.856459, Accuracy 83.922%\n",
      "Epoch 20, Batch 997, LR 0.000050 Loss 5.856949, Accuracy 83.917%\n",
      "Epoch 20, Batch 998, LR 0.000050 Loss 5.856816, Accuracy 83.917%\n",
      "Epoch 20, Batch 999, LR 0.000050 Loss 5.856740, Accuracy 83.918%\n",
      "Epoch 20, Batch 1000, LR 0.000050 Loss 5.856891, Accuracy 83.918%\n",
      "Epoch 20, Batch 1001, LR 0.000050 Loss 5.857172, Accuracy 83.918%\n",
      "Epoch 20, Batch 1002, LR 0.000050 Loss 5.857219, Accuracy 83.920%\n",
      "Epoch 20, Batch 1003, LR 0.000050 Loss 5.856933, Accuracy 83.921%\n",
      "Epoch 20, Batch 1004, LR 0.000050 Loss 5.857062, Accuracy 83.921%\n",
      "Epoch 20, Batch 1005, LR 0.000050 Loss 5.857294, Accuracy 83.919%\n",
      "Epoch 20, Batch 1006, LR 0.000050 Loss 5.856272, Accuracy 83.922%\n",
      "Epoch 20, Batch 1007, LR 0.000050 Loss 5.856455, Accuracy 83.919%\n",
      "Epoch 20, Batch 1008, LR 0.000050 Loss 5.855776, Accuracy 83.924%\n",
      "Epoch 20, Batch 1009, LR 0.000050 Loss 5.855452, Accuracy 83.925%\n",
      "Epoch 20, Batch 1010, LR 0.000050 Loss 5.855116, Accuracy 83.929%\n",
      "Epoch 20, Batch 1011, LR 0.000050 Loss 5.854432, Accuracy 83.933%\n",
      "Epoch 20, Batch 1012, LR 0.000050 Loss 5.853547, Accuracy 83.942%\n",
      "Epoch 20, Batch 1013, LR 0.000050 Loss 5.852935, Accuracy 83.944%\n",
      "Epoch 20, Batch 1014, LR 0.000050 Loss 5.852934, Accuracy 83.942%\n",
      "Epoch 20, Batch 1015, LR 0.000050 Loss 5.852644, Accuracy 83.946%\n",
      "Epoch 20, Batch 1016, LR 0.000050 Loss 5.852128, Accuracy 83.948%\n",
      "Epoch 20, Batch 1017, LR 0.000050 Loss 5.852444, Accuracy 83.949%\n",
      "Epoch 20, Batch 1018, LR 0.000050 Loss 5.851790, Accuracy 83.954%\n",
      "Epoch 20, Batch 1019, LR 0.000050 Loss 5.850689, Accuracy 83.957%\n",
      "Epoch 20, Batch 1020, LR 0.000050 Loss 5.850171, Accuracy 83.961%\n",
      "Epoch 20, Batch 1021, LR 0.000050 Loss 5.850371, Accuracy 83.963%\n",
      "Epoch 20, Batch 1022, LR 0.000050 Loss 5.850536, Accuracy 83.962%\n",
      "Epoch 20, Batch 1023, LR 0.000050 Loss 5.850447, Accuracy 83.958%\n",
      "Epoch 20, Batch 1024, LR 0.000050 Loss 5.849990, Accuracy 83.955%\n",
      "Epoch 20, Batch 1025, LR 0.000050 Loss 5.849605, Accuracy 83.960%\n",
      "Epoch 20, Batch 1026, LR 0.000050 Loss 5.850405, Accuracy 83.956%\n",
      "Epoch 20, Batch 1027, LR 0.000050 Loss 5.850757, Accuracy 83.956%\n",
      "Epoch 20, Batch 1028, LR 0.000050 Loss 5.851114, Accuracy 83.953%\n",
      "Epoch 20, Batch 1029, LR 0.000050 Loss 5.851233, Accuracy 83.953%\n",
      "Epoch 20, Batch 1030, LR 0.000050 Loss 5.851226, Accuracy 83.953%\n",
      "Epoch 20, Batch 1031, LR 0.000050 Loss 5.851107, Accuracy 83.951%\n",
      "Epoch 20, Batch 1032, LR 0.000050 Loss 5.850708, Accuracy 83.950%\n",
      "Epoch 20, Batch 1033, LR 0.000050 Loss 5.850422, Accuracy 83.954%\n",
      "Epoch 20, Batch 1034, LR 0.000050 Loss 5.850332, Accuracy 83.953%\n",
      "Epoch 20, Batch 1035, LR 0.000050 Loss 5.850056, Accuracy 83.956%\n",
      "Epoch 20, Batch 1036, LR 0.000050 Loss 5.850066, Accuracy 83.953%\n",
      "Epoch 20, Batch 1037, LR 0.000050 Loss 5.849668, Accuracy 83.952%\n",
      "Epoch 20, Batch 1038, LR 0.000050 Loss 5.849728, Accuracy 83.951%\n",
      "Epoch 20, Batch 1039, LR 0.000050 Loss 5.849472, Accuracy 83.955%\n",
      "Epoch 20, Batch 1040, LR 0.000050 Loss 5.849420, Accuracy 83.958%\n",
      "Epoch 20, Batch 1041, LR 0.000050 Loss 5.849973, Accuracy 83.954%\n",
      "Epoch 20, Batch 1042, LR 0.000050 Loss 5.850195, Accuracy 83.957%\n",
      "Epoch 20, Batch 1043, LR 0.000050 Loss 5.849956, Accuracy 83.961%\n",
      "Epoch 20, Batch 1044, LR 0.000050 Loss 5.850289, Accuracy 83.961%\n",
      "Epoch 20, Batch 1045, LR 0.000050 Loss 5.850364, Accuracy 83.964%\n",
      "Epoch 20, Batch 1046, LR 0.000050 Loss 5.850343, Accuracy 83.961%\n",
      "Epoch 20, Batch 1047, LR 0.000050 Loss 5.850164, Accuracy 83.963%\n",
      "Epoch 20, Loss (train set) 5.850164, Accuracy (train set) 83.963%\n",
      "Epoch 21, Batch 1, LR 0.000050 Loss 6.443252, Accuracy 83.594%\n",
      "Epoch 21, Batch 2, LR 0.000050 Loss 6.125395, Accuracy 82.812%\n",
      "Epoch 21, Batch 3, LR 0.000050 Loss 5.801313, Accuracy 83.854%\n",
      "Epoch 21, Batch 4, LR 0.000050 Loss 5.827184, Accuracy 84.375%\n",
      "Epoch 21, Batch 5, LR 0.000050 Loss 5.967949, Accuracy 83.594%\n",
      "Epoch 21, Batch 6, LR 0.000050 Loss 5.762654, Accuracy 84.245%\n",
      "Epoch 21, Batch 7, LR 0.000050 Loss 5.856446, Accuracy 83.929%\n",
      "Epoch 21, Batch 8, LR 0.000050 Loss 5.807505, Accuracy 84.277%\n",
      "Epoch 21, Batch 9, LR 0.000050 Loss 5.855885, Accuracy 84.201%\n",
      "Epoch 21, Batch 10, LR 0.000050 Loss 5.848253, Accuracy 84.141%\n",
      "Epoch 21, Batch 11, LR 0.000050 Loss 5.784589, Accuracy 84.304%\n",
      "Epoch 21, Batch 12, LR 0.000050 Loss 5.825253, Accuracy 84.049%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 13, LR 0.000050 Loss 5.750420, Accuracy 84.615%\n",
      "Epoch 21, Batch 14, LR 0.000050 Loss 5.784059, Accuracy 84.431%\n",
      "Epoch 21, Batch 15, LR 0.000050 Loss 5.800698, Accuracy 84.375%\n",
      "Epoch 21, Batch 16, LR 0.000050 Loss 5.807873, Accuracy 84.473%\n",
      "Epoch 21, Batch 17, LR 0.000050 Loss 5.795302, Accuracy 84.467%\n",
      "Epoch 21, Batch 18, LR 0.000050 Loss 5.817973, Accuracy 84.418%\n",
      "Epoch 21, Batch 19, LR 0.000050 Loss 5.818399, Accuracy 84.416%\n",
      "Epoch 21, Batch 20, LR 0.000050 Loss 5.788222, Accuracy 84.609%\n",
      "Epoch 21, Batch 21, LR 0.000050 Loss 5.801755, Accuracy 84.487%\n",
      "Epoch 21, Batch 22, LR 0.000050 Loss 5.823778, Accuracy 84.446%\n",
      "Epoch 21, Batch 23, LR 0.000050 Loss 5.833447, Accuracy 84.341%\n",
      "Epoch 21, Batch 24, LR 0.000050 Loss 5.819096, Accuracy 84.505%\n",
      "Epoch 21, Batch 25, LR 0.000050 Loss 5.821283, Accuracy 84.406%\n",
      "Epoch 21, Batch 26, LR 0.000050 Loss 5.812774, Accuracy 84.495%\n",
      "Epoch 21, Batch 27, LR 0.000050 Loss 5.822650, Accuracy 84.433%\n",
      "Epoch 21, Batch 28, LR 0.000050 Loss 5.830696, Accuracy 84.347%\n",
      "Epoch 21, Batch 29, LR 0.000050 Loss 5.839403, Accuracy 84.321%\n",
      "Epoch 21, Batch 30, LR 0.000050 Loss 5.835695, Accuracy 84.349%\n",
      "Epoch 21, Batch 31, LR 0.000050 Loss 5.814074, Accuracy 84.451%\n",
      "Epoch 21, Batch 32, LR 0.000050 Loss 5.792098, Accuracy 84.619%\n",
      "Epoch 21, Batch 33, LR 0.000050 Loss 5.806626, Accuracy 84.517%\n",
      "Epoch 21, Batch 34, LR 0.000050 Loss 5.807421, Accuracy 84.513%\n",
      "Epoch 21, Batch 35, LR 0.000050 Loss 5.832480, Accuracy 84.397%\n",
      "Epoch 21, Batch 36, LR 0.000050 Loss 5.814754, Accuracy 84.332%\n",
      "Epoch 21, Batch 37, LR 0.000050 Loss 5.818256, Accuracy 84.354%\n",
      "Epoch 21, Batch 38, LR 0.000050 Loss 5.839187, Accuracy 84.375%\n",
      "Epoch 21, Batch 39, LR 0.000050 Loss 5.849181, Accuracy 84.315%\n",
      "Epoch 21, Batch 40, LR 0.000050 Loss 5.839667, Accuracy 84.355%\n",
      "Epoch 21, Batch 41, LR 0.000050 Loss 5.840823, Accuracy 84.394%\n",
      "Epoch 21, Batch 42, LR 0.000050 Loss 5.823852, Accuracy 84.561%\n",
      "Epoch 21, Batch 43, LR 0.000050 Loss 5.822767, Accuracy 84.593%\n",
      "Epoch 21, Batch 44, LR 0.000050 Loss 5.813699, Accuracy 84.588%\n",
      "Epoch 21, Batch 45, LR 0.000050 Loss 5.809389, Accuracy 84.583%\n",
      "Epoch 21, Batch 46, LR 0.000050 Loss 5.813374, Accuracy 84.579%\n",
      "Epoch 21, Batch 47, LR 0.000050 Loss 5.810152, Accuracy 84.591%\n",
      "Epoch 21, Batch 48, LR 0.000050 Loss 5.795966, Accuracy 84.652%\n",
      "Epoch 21, Batch 49, LR 0.000050 Loss 5.778013, Accuracy 84.726%\n",
      "Epoch 21, Batch 50, LR 0.000050 Loss 5.769516, Accuracy 84.781%\n",
      "Epoch 21, Batch 51, LR 0.000050 Loss 5.783500, Accuracy 84.681%\n",
      "Epoch 21, Batch 52, LR 0.000050 Loss 5.801193, Accuracy 84.660%\n",
      "Epoch 21, Batch 53, LR 0.000050 Loss 5.794646, Accuracy 84.670%\n",
      "Epoch 21, Batch 54, LR 0.000050 Loss 5.777816, Accuracy 84.650%\n",
      "Epoch 21, Batch 55, LR 0.000050 Loss 5.786308, Accuracy 84.631%\n",
      "Epoch 21, Batch 56, LR 0.000050 Loss 5.786102, Accuracy 84.668%\n",
      "Epoch 21, Batch 57, LR 0.000050 Loss 5.791082, Accuracy 84.567%\n",
      "Epoch 21, Batch 58, LR 0.000050 Loss 5.817952, Accuracy 84.429%\n",
      "Epoch 21, Batch 59, LR 0.000050 Loss 5.815078, Accuracy 84.441%\n",
      "Epoch 21, Batch 60, LR 0.000050 Loss 5.829187, Accuracy 84.388%\n",
      "Epoch 21, Batch 61, LR 0.000050 Loss 5.842655, Accuracy 84.273%\n",
      "Epoch 21, Batch 62, LR 0.000050 Loss 5.848516, Accuracy 84.211%\n",
      "Epoch 21, Batch 63, LR 0.000050 Loss 5.843483, Accuracy 84.251%\n",
      "Epoch 21, Batch 64, LR 0.000050 Loss 5.846379, Accuracy 84.216%\n",
      "Epoch 21, Batch 65, LR 0.000050 Loss 5.832059, Accuracy 84.231%\n",
      "Epoch 21, Batch 66, LR 0.000050 Loss 5.838889, Accuracy 84.221%\n",
      "Epoch 21, Batch 67, LR 0.000050 Loss 5.834459, Accuracy 84.247%\n",
      "Epoch 21, Batch 68, LR 0.000050 Loss 5.832291, Accuracy 84.226%\n",
      "Epoch 21, Batch 69, LR 0.000050 Loss 5.828309, Accuracy 84.239%\n",
      "Epoch 21, Batch 70, LR 0.000050 Loss 5.825971, Accuracy 84.286%\n",
      "Epoch 21, Batch 71, LR 0.000050 Loss 5.819481, Accuracy 84.309%\n",
      "Epoch 21, Batch 72, LR 0.000050 Loss 5.824958, Accuracy 84.288%\n",
      "Epoch 21, Batch 73, LR 0.000050 Loss 5.826306, Accuracy 84.279%\n",
      "Epoch 21, Batch 74, LR 0.000050 Loss 5.818833, Accuracy 84.312%\n",
      "Epoch 21, Batch 75, LR 0.000050 Loss 5.812747, Accuracy 84.302%\n",
      "Epoch 21, Batch 76, LR 0.000050 Loss 5.810226, Accuracy 84.324%\n",
      "Epoch 21, Batch 77, LR 0.000050 Loss 5.805653, Accuracy 84.334%\n",
      "Epoch 21, Batch 78, LR 0.000050 Loss 5.801770, Accuracy 84.375%\n",
      "Epoch 21, Batch 79, LR 0.000050 Loss 5.807621, Accuracy 84.375%\n",
      "Epoch 21, Batch 80, LR 0.000050 Loss 5.801767, Accuracy 84.404%\n",
      "Epoch 21, Batch 81, LR 0.000050 Loss 5.799983, Accuracy 84.452%\n",
      "Epoch 21, Batch 82, LR 0.000050 Loss 5.799995, Accuracy 84.413%\n",
      "Epoch 21, Batch 83, LR 0.000050 Loss 5.802152, Accuracy 84.375%\n",
      "Epoch 21, Batch 84, LR 0.000050 Loss 5.795003, Accuracy 84.412%\n",
      "Epoch 21, Batch 85, LR 0.000050 Loss 5.782924, Accuracy 84.430%\n",
      "Epoch 21, Batch 86, LR 0.000050 Loss 5.783903, Accuracy 84.402%\n",
      "Epoch 21, Batch 87, LR 0.000050 Loss 5.781566, Accuracy 84.411%\n",
      "Epoch 21, Batch 88, LR 0.000050 Loss 5.782520, Accuracy 84.384%\n",
      "Epoch 21, Batch 89, LR 0.000050 Loss 5.779403, Accuracy 84.393%\n",
      "Epoch 21, Batch 90, LR 0.000050 Loss 5.779765, Accuracy 84.410%\n",
      "Epoch 21, Batch 91, LR 0.000050 Loss 5.778882, Accuracy 84.427%\n",
      "Epoch 21, Batch 92, LR 0.000050 Loss 5.774223, Accuracy 84.451%\n",
      "Epoch 21, Batch 93, LR 0.000050 Loss 5.766111, Accuracy 84.467%\n",
      "Epoch 21, Batch 94, LR 0.000050 Loss 5.762081, Accuracy 84.525%\n",
      "Epoch 21, Batch 95, LR 0.000050 Loss 5.761625, Accuracy 84.490%\n",
      "Epoch 21, Batch 96, LR 0.000050 Loss 5.755992, Accuracy 84.546%\n",
      "Epoch 21, Batch 97, LR 0.000050 Loss 5.757402, Accuracy 84.576%\n",
      "Epoch 21, Batch 98, LR 0.000050 Loss 5.753747, Accuracy 84.606%\n",
      "Epoch 21, Batch 99, LR 0.000050 Loss 5.748531, Accuracy 84.628%\n",
      "Epoch 21, Batch 100, LR 0.000050 Loss 5.746165, Accuracy 84.664%\n",
      "Epoch 21, Batch 101, LR 0.000050 Loss 5.745257, Accuracy 84.677%\n",
      "Epoch 21, Batch 102, LR 0.000050 Loss 5.752733, Accuracy 84.658%\n",
      "Epoch 21, Batch 103, LR 0.000050 Loss 5.754067, Accuracy 84.648%\n",
      "Epoch 21, Batch 104, LR 0.000050 Loss 5.753009, Accuracy 84.675%\n",
      "Epoch 21, Batch 105, LR 0.000050 Loss 5.747908, Accuracy 84.673%\n",
      "Epoch 21, Batch 106, LR 0.000050 Loss 5.747834, Accuracy 84.685%\n",
      "Epoch 21, Batch 107, LR 0.000050 Loss 5.743467, Accuracy 84.696%\n",
      "Epoch 21, Batch 108, LR 0.000050 Loss 5.742085, Accuracy 84.708%\n",
      "Epoch 21, Batch 109, LR 0.000050 Loss 5.734266, Accuracy 84.741%\n",
      "Epoch 21, Batch 110, LR 0.000050 Loss 5.731883, Accuracy 84.751%\n",
      "Epoch 21, Batch 111, LR 0.000050 Loss 5.730101, Accuracy 84.769%\n",
      "Epoch 21, Batch 112, LR 0.000050 Loss 5.729447, Accuracy 84.780%\n",
      "Epoch 21, Batch 113, LR 0.000050 Loss 5.736172, Accuracy 84.769%\n",
      "Epoch 21, Batch 114, LR 0.000050 Loss 5.743563, Accuracy 84.738%\n",
      "Epoch 21, Batch 115, LR 0.000050 Loss 5.743564, Accuracy 84.749%\n",
      "Epoch 21, Batch 116, LR 0.000050 Loss 5.745941, Accuracy 84.725%\n",
      "Epoch 21, Batch 117, LR 0.000050 Loss 5.745802, Accuracy 84.749%\n",
      "Epoch 21, Batch 118, LR 0.000050 Loss 5.746656, Accuracy 84.766%\n",
      "Epoch 21, Batch 119, LR 0.000050 Loss 5.743111, Accuracy 84.775%\n",
      "Epoch 21, Batch 120, LR 0.000050 Loss 5.746493, Accuracy 84.779%\n",
      "Epoch 21, Batch 121, LR 0.000050 Loss 5.746742, Accuracy 84.782%\n",
      "Epoch 21, Batch 122, LR 0.000050 Loss 5.747126, Accuracy 84.785%\n",
      "Epoch 21, Batch 123, LR 0.000050 Loss 5.743603, Accuracy 84.782%\n",
      "Epoch 21, Batch 124, LR 0.000050 Loss 5.746329, Accuracy 84.778%\n",
      "Epoch 21, Batch 125, LR 0.000050 Loss 5.742425, Accuracy 84.794%\n",
      "Epoch 21, Batch 126, LR 0.000050 Loss 5.746509, Accuracy 84.815%\n",
      "Epoch 21, Batch 127, LR 0.000049 Loss 5.750210, Accuracy 84.836%\n",
      "Epoch 21, Batch 128, LR 0.000049 Loss 5.750719, Accuracy 84.808%\n",
      "Epoch 21, Batch 129, LR 0.000049 Loss 5.753550, Accuracy 84.763%\n",
      "Epoch 21, Batch 130, LR 0.000049 Loss 5.748246, Accuracy 84.802%\n",
      "Epoch 21, Batch 131, LR 0.000049 Loss 5.743554, Accuracy 84.810%\n",
      "Epoch 21, Batch 132, LR 0.000049 Loss 5.746463, Accuracy 84.807%\n",
      "Epoch 21, Batch 133, LR 0.000049 Loss 5.746624, Accuracy 84.786%\n",
      "Epoch 21, Batch 134, LR 0.000049 Loss 5.742445, Accuracy 84.801%\n",
      "Epoch 21, Batch 135, LR 0.000049 Loss 5.743878, Accuracy 84.803%\n",
      "Epoch 21, Batch 136, LR 0.000049 Loss 5.745414, Accuracy 84.766%\n",
      "Epoch 21, Batch 137, LR 0.000049 Loss 5.744575, Accuracy 84.774%\n",
      "Epoch 21, Batch 138, LR 0.000049 Loss 5.749409, Accuracy 84.737%\n",
      "Epoch 21, Batch 139, LR 0.000049 Loss 5.743458, Accuracy 84.757%\n",
      "Epoch 21, Batch 140, LR 0.000049 Loss 5.746782, Accuracy 84.743%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 141, LR 0.000049 Loss 5.747300, Accuracy 84.735%\n",
      "Epoch 21, Batch 142, LR 0.000049 Loss 5.750596, Accuracy 84.716%\n",
      "Epoch 21, Batch 143, LR 0.000049 Loss 5.752226, Accuracy 84.725%\n",
      "Epoch 21, Batch 144, LR 0.000049 Loss 5.752933, Accuracy 84.722%\n",
      "Epoch 21, Batch 145, LR 0.000049 Loss 5.755497, Accuracy 84.720%\n",
      "Epoch 21, Batch 146, LR 0.000049 Loss 5.758560, Accuracy 84.696%\n",
      "Epoch 21, Batch 147, LR 0.000049 Loss 5.754094, Accuracy 84.715%\n",
      "Epoch 21, Batch 148, LR 0.000049 Loss 5.752171, Accuracy 84.723%\n",
      "Epoch 21, Batch 149, LR 0.000049 Loss 5.755472, Accuracy 84.705%\n",
      "Epoch 21, Batch 150, LR 0.000049 Loss 5.757389, Accuracy 84.698%\n",
      "Epoch 21, Batch 151, LR 0.000049 Loss 5.761917, Accuracy 84.670%\n",
      "Epoch 21, Batch 152, LR 0.000049 Loss 5.764700, Accuracy 84.642%\n",
      "Epoch 21, Batch 153, LR 0.000049 Loss 5.769911, Accuracy 84.620%\n",
      "Epoch 21, Batch 154, LR 0.000049 Loss 5.765097, Accuracy 84.624%\n",
      "Epoch 21, Batch 155, LR 0.000049 Loss 5.763323, Accuracy 84.637%\n",
      "Epoch 21, Batch 156, LR 0.000049 Loss 5.759364, Accuracy 84.655%\n",
      "Epoch 21, Batch 157, LR 0.000049 Loss 5.762609, Accuracy 84.669%\n",
      "Epoch 21, Batch 158, LR 0.000049 Loss 5.762199, Accuracy 84.647%\n",
      "Epoch 21, Batch 159, LR 0.000049 Loss 5.762267, Accuracy 84.635%\n",
      "Epoch 21, Batch 160, LR 0.000049 Loss 5.766255, Accuracy 84.609%\n",
      "Epoch 21, Batch 161, LR 0.000049 Loss 5.760930, Accuracy 84.618%\n",
      "Epoch 21, Batch 162, LR 0.000049 Loss 5.761892, Accuracy 84.621%\n",
      "Epoch 21, Batch 163, LR 0.000049 Loss 5.762320, Accuracy 84.634%\n",
      "Epoch 21, Batch 164, LR 0.000049 Loss 5.766680, Accuracy 84.580%\n",
      "Epoch 21, Batch 165, LR 0.000049 Loss 5.765986, Accuracy 84.598%\n",
      "Epoch 21, Batch 166, LR 0.000049 Loss 5.766513, Accuracy 84.582%\n",
      "Epoch 21, Batch 167, LR 0.000049 Loss 5.768889, Accuracy 84.576%\n",
      "Epoch 21, Batch 168, LR 0.000049 Loss 5.766663, Accuracy 84.584%\n",
      "Epoch 21, Batch 169, LR 0.000049 Loss 5.765975, Accuracy 84.583%\n",
      "Epoch 21, Batch 170, LR 0.000049 Loss 5.762964, Accuracy 84.605%\n",
      "Epoch 21, Batch 171, LR 0.000049 Loss 5.763656, Accuracy 84.590%\n",
      "Epoch 21, Batch 172, LR 0.000049 Loss 5.762666, Accuracy 84.588%\n",
      "Epoch 21, Batch 173, LR 0.000049 Loss 5.764762, Accuracy 84.560%\n",
      "Epoch 21, Batch 174, LR 0.000049 Loss 5.758501, Accuracy 84.586%\n",
      "Epoch 21, Batch 175, LR 0.000049 Loss 5.754184, Accuracy 84.603%\n",
      "Epoch 21, Batch 176, LR 0.000049 Loss 5.750646, Accuracy 84.628%\n",
      "Epoch 21, Batch 177, LR 0.000049 Loss 5.747115, Accuracy 84.653%\n",
      "Epoch 21, Batch 178, LR 0.000049 Loss 5.747932, Accuracy 84.652%\n",
      "Epoch 21, Batch 179, LR 0.000049 Loss 5.744903, Accuracy 84.676%\n",
      "Epoch 21, Batch 180, LR 0.000049 Loss 5.746128, Accuracy 84.674%\n",
      "Epoch 21, Batch 181, LR 0.000049 Loss 5.748581, Accuracy 84.664%\n",
      "Epoch 21, Batch 182, LR 0.000049 Loss 5.751082, Accuracy 84.637%\n",
      "Epoch 21, Batch 183, LR 0.000049 Loss 5.747485, Accuracy 84.648%\n",
      "Epoch 21, Batch 184, LR 0.000049 Loss 5.743939, Accuracy 84.668%\n",
      "Epoch 21, Batch 185, LR 0.000049 Loss 5.744616, Accuracy 84.662%\n",
      "Epoch 21, Batch 186, LR 0.000049 Loss 5.744013, Accuracy 84.669%\n",
      "Epoch 21, Batch 187, LR 0.000049 Loss 5.744602, Accuracy 84.667%\n",
      "Epoch 21, Batch 188, LR 0.000049 Loss 5.746020, Accuracy 84.641%\n",
      "Epoch 21, Batch 189, LR 0.000049 Loss 5.746145, Accuracy 84.652%\n",
      "Epoch 21, Batch 190, LR 0.000049 Loss 5.750089, Accuracy 84.659%\n",
      "Epoch 21, Batch 191, LR 0.000049 Loss 5.747671, Accuracy 84.682%\n",
      "Epoch 21, Batch 192, LR 0.000049 Loss 5.743895, Accuracy 84.705%\n",
      "Epoch 21, Batch 193, LR 0.000049 Loss 5.741476, Accuracy 84.723%\n",
      "Epoch 21, Batch 194, LR 0.000049 Loss 5.740449, Accuracy 84.729%\n",
      "Epoch 21, Batch 195, LR 0.000049 Loss 5.740581, Accuracy 84.740%\n",
      "Epoch 21, Batch 196, LR 0.000049 Loss 5.740744, Accuracy 84.758%\n",
      "Epoch 21, Batch 197, LR 0.000049 Loss 5.740927, Accuracy 84.768%\n",
      "Epoch 21, Batch 198, LR 0.000049 Loss 5.742801, Accuracy 84.762%\n",
      "Epoch 21, Batch 199, LR 0.000049 Loss 5.739775, Accuracy 84.768%\n",
      "Epoch 21, Batch 200, LR 0.000049 Loss 5.737916, Accuracy 84.754%\n",
      "Epoch 21, Batch 201, LR 0.000049 Loss 5.736899, Accuracy 84.764%\n",
      "Epoch 21, Batch 202, LR 0.000049 Loss 5.735373, Accuracy 84.781%\n",
      "Epoch 21, Batch 203, LR 0.000049 Loss 5.736070, Accuracy 84.771%\n",
      "Epoch 21, Batch 204, LR 0.000049 Loss 5.734454, Accuracy 84.769%\n",
      "Epoch 21, Batch 205, LR 0.000049 Loss 5.732828, Accuracy 84.798%\n",
      "Epoch 21, Batch 206, LR 0.000049 Loss 5.730013, Accuracy 84.815%\n",
      "Epoch 21, Batch 207, LR 0.000049 Loss 5.728986, Accuracy 84.832%\n",
      "Epoch 21, Batch 208, LR 0.000049 Loss 5.728083, Accuracy 84.826%\n",
      "Epoch 21, Batch 209, LR 0.000049 Loss 5.731747, Accuracy 84.805%\n",
      "Epoch 21, Batch 210, LR 0.000049 Loss 5.730193, Accuracy 84.803%\n",
      "Epoch 21, Batch 211, LR 0.000049 Loss 5.727838, Accuracy 84.797%\n",
      "Epoch 21, Batch 212, LR 0.000049 Loss 5.724561, Accuracy 84.825%\n",
      "Epoch 21, Batch 213, LR 0.000049 Loss 5.723949, Accuracy 84.822%\n",
      "Epoch 21, Batch 214, LR 0.000049 Loss 5.723850, Accuracy 84.820%\n",
      "Epoch 21, Batch 215, LR 0.000049 Loss 5.723345, Accuracy 84.811%\n",
      "Epoch 21, Batch 216, LR 0.000049 Loss 5.723118, Accuracy 84.816%\n",
      "Epoch 21, Batch 217, LR 0.000049 Loss 5.725266, Accuracy 84.793%\n",
      "Epoch 21, Batch 218, LR 0.000049 Loss 5.723505, Accuracy 84.787%\n",
      "Epoch 21, Batch 219, LR 0.000049 Loss 5.725979, Accuracy 84.775%\n",
      "Epoch 21, Batch 220, LR 0.000049 Loss 5.729533, Accuracy 84.748%\n",
      "Epoch 21, Batch 221, LR 0.000049 Loss 5.726829, Accuracy 84.757%\n",
      "Epoch 21, Batch 222, LR 0.000049 Loss 5.727424, Accuracy 84.737%\n",
      "Epoch 21, Batch 223, LR 0.000049 Loss 5.729283, Accuracy 84.729%\n",
      "Epoch 21, Batch 224, LR 0.000049 Loss 5.731090, Accuracy 84.727%\n",
      "Epoch 21, Batch 225, LR 0.000049 Loss 5.730746, Accuracy 84.729%\n",
      "Epoch 21, Batch 226, LR 0.000049 Loss 5.731509, Accuracy 84.721%\n",
      "Epoch 21, Batch 227, LR 0.000049 Loss 5.731587, Accuracy 84.719%\n",
      "Epoch 21, Batch 228, LR 0.000049 Loss 5.733894, Accuracy 84.718%\n",
      "Epoch 21, Batch 229, LR 0.000049 Loss 5.729886, Accuracy 84.743%\n",
      "Epoch 21, Batch 230, LR 0.000049 Loss 5.728821, Accuracy 84.745%\n",
      "Epoch 21, Batch 231, LR 0.000049 Loss 5.723138, Accuracy 84.771%\n",
      "Epoch 21, Batch 232, LR 0.000049 Loss 5.726507, Accuracy 84.759%\n",
      "Epoch 21, Batch 233, LR 0.000049 Loss 5.727607, Accuracy 84.747%\n",
      "Epoch 21, Batch 234, LR 0.000049 Loss 5.728110, Accuracy 84.752%\n",
      "Epoch 21, Batch 235, LR 0.000049 Loss 5.725834, Accuracy 84.777%\n",
      "Epoch 21, Batch 236, LR 0.000049 Loss 5.723599, Accuracy 84.802%\n",
      "Epoch 21, Batch 237, LR 0.000049 Loss 5.720706, Accuracy 84.800%\n",
      "Epoch 21, Batch 238, LR 0.000049 Loss 5.720729, Accuracy 84.802%\n",
      "Epoch 21, Batch 239, LR 0.000049 Loss 5.721560, Accuracy 84.793%\n",
      "Epoch 21, Batch 240, LR 0.000049 Loss 5.721072, Accuracy 84.827%\n",
      "Epoch 21, Batch 241, LR 0.000049 Loss 5.717365, Accuracy 84.826%\n",
      "Epoch 21, Batch 242, LR 0.000049 Loss 5.715901, Accuracy 84.837%\n",
      "Epoch 21, Batch 243, LR 0.000049 Loss 5.712873, Accuracy 84.851%\n",
      "Epoch 21, Batch 244, LR 0.000049 Loss 5.710198, Accuracy 84.862%\n",
      "Epoch 21, Batch 245, LR 0.000049 Loss 5.709175, Accuracy 84.866%\n",
      "Epoch 21, Batch 246, LR 0.000049 Loss 5.710724, Accuracy 84.861%\n",
      "Epoch 21, Batch 247, LR 0.000049 Loss 5.709569, Accuracy 84.884%\n",
      "Epoch 21, Batch 248, LR 0.000049 Loss 5.709379, Accuracy 84.888%\n",
      "Epoch 21, Batch 249, LR 0.000049 Loss 5.711224, Accuracy 84.877%\n",
      "Epoch 21, Batch 250, LR 0.000049 Loss 5.711827, Accuracy 84.881%\n",
      "Epoch 21, Batch 251, LR 0.000049 Loss 5.709815, Accuracy 84.879%\n",
      "Epoch 21, Batch 252, LR 0.000049 Loss 5.710241, Accuracy 84.887%\n",
      "Epoch 21, Batch 253, LR 0.000049 Loss 5.710416, Accuracy 84.897%\n",
      "Epoch 21, Batch 254, LR 0.000049 Loss 5.708980, Accuracy 84.910%\n",
      "Epoch 21, Batch 255, LR 0.000049 Loss 5.709621, Accuracy 84.899%\n",
      "Epoch 21, Batch 256, LR 0.000049 Loss 5.709012, Accuracy 84.891%\n",
      "Epoch 21, Batch 257, LR 0.000049 Loss 5.708416, Accuracy 84.904%\n",
      "Epoch 21, Batch 258, LR 0.000049 Loss 5.704098, Accuracy 84.926%\n",
      "Epoch 21, Batch 259, LR 0.000049 Loss 5.703341, Accuracy 84.912%\n",
      "Epoch 21, Batch 260, LR 0.000049 Loss 5.702517, Accuracy 84.913%\n",
      "Epoch 21, Batch 261, LR 0.000049 Loss 5.700695, Accuracy 84.932%\n",
      "Epoch 21, Batch 262, LR 0.000049 Loss 5.700792, Accuracy 84.942%\n",
      "Epoch 21, Batch 263, LR 0.000049 Loss 5.700416, Accuracy 84.936%\n",
      "Epoch 21, Batch 264, LR 0.000049 Loss 5.700832, Accuracy 84.922%\n",
      "Epoch 21, Batch 265, LR 0.000049 Loss 5.699602, Accuracy 84.915%\n",
      "Epoch 21, Batch 266, LR 0.000049 Loss 5.701225, Accuracy 84.901%\n",
      "Epoch 21, Batch 267, LR 0.000049 Loss 5.702731, Accuracy 84.896%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 268, LR 0.000049 Loss 5.701414, Accuracy 84.903%\n",
      "Epoch 21, Batch 269, LR 0.000049 Loss 5.701392, Accuracy 84.912%\n",
      "Epoch 21, Batch 270, LR 0.000049 Loss 5.699828, Accuracy 84.910%\n",
      "Epoch 21, Batch 271, LR 0.000049 Loss 5.700133, Accuracy 84.908%\n",
      "Epoch 21, Batch 272, LR 0.000049 Loss 5.702083, Accuracy 84.903%\n",
      "Epoch 21, Batch 273, LR 0.000049 Loss 5.701901, Accuracy 84.907%\n",
      "Epoch 21, Batch 274, LR 0.000049 Loss 5.704226, Accuracy 84.897%\n",
      "Epoch 21, Batch 275, LR 0.000049 Loss 5.706374, Accuracy 84.881%\n",
      "Epoch 21, Batch 276, LR 0.000049 Loss 5.705920, Accuracy 84.879%\n",
      "Epoch 21, Batch 277, LR 0.000049 Loss 5.704126, Accuracy 84.888%\n",
      "Epoch 21, Batch 278, LR 0.000049 Loss 5.702201, Accuracy 84.901%\n",
      "Epoch 21, Batch 279, LR 0.000049 Loss 5.703561, Accuracy 84.896%\n",
      "Epoch 21, Batch 280, LR 0.000049 Loss 5.704088, Accuracy 84.894%\n",
      "Epoch 21, Batch 281, LR 0.000049 Loss 5.707319, Accuracy 84.887%\n",
      "Epoch 21, Batch 282, LR 0.000049 Loss 5.706195, Accuracy 84.888%\n",
      "Epoch 21, Batch 283, LR 0.000049 Loss 5.702928, Accuracy 84.897%\n",
      "Epoch 21, Batch 284, LR 0.000049 Loss 5.702524, Accuracy 84.900%\n",
      "Epoch 21, Batch 285, LR 0.000049 Loss 5.700527, Accuracy 84.907%\n",
      "Epoch 21, Batch 286, LR 0.000049 Loss 5.701975, Accuracy 84.889%\n",
      "Epoch 21, Batch 287, LR 0.000049 Loss 5.701964, Accuracy 84.892%\n",
      "Epoch 21, Batch 288, LR 0.000049 Loss 5.702144, Accuracy 84.882%\n",
      "Epoch 21, Batch 289, LR 0.000049 Loss 5.702095, Accuracy 84.881%\n",
      "Epoch 21, Batch 290, LR 0.000049 Loss 5.698781, Accuracy 84.898%\n",
      "Epoch 21, Batch 291, LR 0.000049 Loss 5.699056, Accuracy 84.888%\n",
      "Epoch 21, Batch 292, LR 0.000049 Loss 5.701719, Accuracy 84.889%\n",
      "Epoch 21, Batch 293, LR 0.000049 Loss 5.701841, Accuracy 84.890%\n",
      "Epoch 21, Batch 294, LR 0.000049 Loss 5.703828, Accuracy 84.872%\n",
      "Epoch 21, Batch 295, LR 0.000049 Loss 5.702913, Accuracy 84.862%\n",
      "Epoch 21, Batch 296, LR 0.000049 Loss 5.701397, Accuracy 84.863%\n",
      "Epoch 21, Batch 297, LR 0.000049 Loss 5.700269, Accuracy 84.867%\n",
      "Epoch 21, Batch 298, LR 0.000049 Loss 5.700314, Accuracy 84.860%\n",
      "Epoch 21, Batch 299, LR 0.000049 Loss 5.699983, Accuracy 84.851%\n",
      "Epoch 21, Batch 300, LR 0.000049 Loss 5.699296, Accuracy 84.857%\n",
      "Epoch 21, Batch 301, LR 0.000049 Loss 5.699150, Accuracy 84.858%\n",
      "Epoch 21, Batch 302, LR 0.000049 Loss 5.697809, Accuracy 84.872%\n",
      "Epoch 21, Batch 303, LR 0.000049 Loss 5.698670, Accuracy 84.865%\n",
      "Epoch 21, Batch 304, LR 0.000049 Loss 5.699052, Accuracy 84.868%\n",
      "Epoch 21, Batch 305, LR 0.000049 Loss 5.700628, Accuracy 84.851%\n",
      "Epoch 21, Batch 306, LR 0.000049 Loss 5.699177, Accuracy 84.863%\n",
      "Epoch 21, Batch 307, LR 0.000049 Loss 5.698962, Accuracy 84.864%\n",
      "Epoch 21, Batch 308, LR 0.000049 Loss 5.698186, Accuracy 84.875%\n",
      "Epoch 21, Batch 309, LR 0.000049 Loss 5.699154, Accuracy 84.871%\n",
      "Epoch 21, Batch 310, LR 0.000049 Loss 5.698629, Accuracy 84.877%\n",
      "Epoch 21, Batch 311, LR 0.000049 Loss 5.697325, Accuracy 84.877%\n",
      "Epoch 21, Batch 312, LR 0.000049 Loss 5.696602, Accuracy 84.876%\n",
      "Epoch 21, Batch 313, LR 0.000049 Loss 5.699834, Accuracy 84.852%\n",
      "Epoch 21, Batch 314, LR 0.000049 Loss 5.701040, Accuracy 84.848%\n",
      "Epoch 21, Batch 315, LR 0.000049 Loss 5.700670, Accuracy 84.866%\n",
      "Epoch 21, Batch 316, LR 0.000049 Loss 5.699292, Accuracy 84.872%\n",
      "Epoch 21, Batch 317, LR 0.000049 Loss 5.698949, Accuracy 84.873%\n",
      "Epoch 21, Batch 318, LR 0.000049 Loss 5.698750, Accuracy 84.857%\n",
      "Epoch 21, Batch 319, LR 0.000049 Loss 5.698010, Accuracy 84.865%\n",
      "Epoch 21, Batch 320, LR 0.000049 Loss 5.694955, Accuracy 84.888%\n",
      "Epoch 21, Batch 321, LR 0.000049 Loss 5.694968, Accuracy 84.893%\n",
      "Epoch 21, Batch 322, LR 0.000049 Loss 5.693695, Accuracy 84.904%\n",
      "Epoch 21, Batch 323, LR 0.000049 Loss 5.692543, Accuracy 84.897%\n",
      "Epoch 21, Batch 324, LR 0.000049 Loss 5.694610, Accuracy 84.893%\n",
      "Epoch 21, Batch 325, LR 0.000049 Loss 5.695849, Accuracy 84.877%\n",
      "Epoch 21, Batch 326, LR 0.000049 Loss 5.692329, Accuracy 84.905%\n",
      "Epoch 21, Batch 327, LR 0.000049 Loss 5.691495, Accuracy 84.903%\n",
      "Epoch 21, Batch 328, LR 0.000049 Loss 5.691434, Accuracy 84.904%\n",
      "Epoch 21, Batch 329, LR 0.000049 Loss 5.692397, Accuracy 84.886%\n",
      "Epoch 21, Batch 330, LR 0.000049 Loss 5.692009, Accuracy 84.886%\n",
      "Epoch 21, Batch 331, LR 0.000049 Loss 5.692436, Accuracy 84.887%\n",
      "Epoch 21, Batch 332, LR 0.000049 Loss 5.690467, Accuracy 84.886%\n",
      "Epoch 21, Batch 333, LR 0.000049 Loss 5.688531, Accuracy 84.903%\n",
      "Epoch 21, Batch 334, LR 0.000049 Loss 5.691331, Accuracy 84.885%\n",
      "Epoch 21, Batch 335, LR 0.000049 Loss 5.690559, Accuracy 84.895%\n",
      "Epoch 21, Batch 336, LR 0.000049 Loss 5.688389, Accuracy 84.910%\n",
      "Epoch 21, Batch 337, LR 0.000049 Loss 5.688028, Accuracy 84.908%\n",
      "Epoch 21, Batch 338, LR 0.000049 Loss 5.687869, Accuracy 84.902%\n",
      "Epoch 21, Batch 339, LR 0.000049 Loss 5.689072, Accuracy 84.887%\n",
      "Epoch 21, Batch 340, LR 0.000049 Loss 5.688051, Accuracy 84.894%\n",
      "Epoch 21, Batch 341, LR 0.000049 Loss 5.690690, Accuracy 84.877%\n",
      "Epoch 21, Batch 342, LR 0.000049 Loss 5.692779, Accuracy 84.852%\n",
      "Epoch 21, Batch 343, LR 0.000049 Loss 5.693280, Accuracy 84.837%\n",
      "Epoch 21, Batch 344, LR 0.000049 Loss 5.694349, Accuracy 84.831%\n",
      "Epoch 21, Batch 345, LR 0.000049 Loss 5.695560, Accuracy 84.821%\n",
      "Epoch 21, Batch 346, LR 0.000049 Loss 5.694216, Accuracy 84.811%\n",
      "Epoch 21, Batch 347, LR 0.000049 Loss 5.695383, Accuracy 84.807%\n",
      "Epoch 21, Batch 348, LR 0.000049 Loss 5.694446, Accuracy 84.799%\n",
      "Epoch 21, Batch 349, LR 0.000049 Loss 5.694060, Accuracy 84.809%\n",
      "Epoch 21, Batch 350, LR 0.000049 Loss 5.695002, Accuracy 84.804%\n",
      "Epoch 21, Batch 351, LR 0.000049 Loss 5.696806, Accuracy 84.800%\n",
      "Epoch 21, Batch 352, LR 0.000049 Loss 5.695771, Accuracy 84.803%\n",
      "Epoch 21, Batch 353, LR 0.000049 Loss 5.695340, Accuracy 84.807%\n",
      "Epoch 21, Batch 354, LR 0.000049 Loss 5.692797, Accuracy 84.810%\n",
      "Epoch 21, Batch 355, LR 0.000049 Loss 5.691767, Accuracy 84.815%\n",
      "Epoch 21, Batch 356, LR 0.000049 Loss 5.692364, Accuracy 84.814%\n",
      "Epoch 21, Batch 357, LR 0.000049 Loss 5.691763, Accuracy 84.810%\n",
      "Epoch 21, Batch 358, LR 0.000049 Loss 5.693641, Accuracy 84.803%\n",
      "Epoch 21, Batch 359, LR 0.000049 Loss 5.693216, Accuracy 84.797%\n",
      "Epoch 21, Batch 360, LR 0.000049 Loss 5.694321, Accuracy 84.796%\n",
      "Epoch 21, Batch 361, LR 0.000049 Loss 5.694779, Accuracy 84.788%\n",
      "Epoch 21, Batch 362, LR 0.000049 Loss 5.694415, Accuracy 84.785%\n",
      "Epoch 21, Batch 363, LR 0.000049 Loss 5.697133, Accuracy 84.777%\n",
      "Epoch 21, Batch 364, LR 0.000049 Loss 5.694983, Accuracy 84.789%\n",
      "Epoch 21, Batch 365, LR 0.000049 Loss 5.694537, Accuracy 84.799%\n",
      "Epoch 21, Batch 366, LR 0.000049 Loss 5.692876, Accuracy 84.810%\n",
      "Epoch 21, Batch 367, LR 0.000049 Loss 5.690869, Accuracy 84.822%\n",
      "Epoch 21, Batch 368, LR 0.000049 Loss 5.691596, Accuracy 84.821%\n",
      "Epoch 21, Batch 369, LR 0.000049 Loss 5.689170, Accuracy 84.847%\n",
      "Epoch 21, Batch 370, LR 0.000049 Loss 5.689358, Accuracy 84.861%\n",
      "Epoch 21, Batch 371, LR 0.000049 Loss 5.688598, Accuracy 84.876%\n",
      "Epoch 21, Batch 372, LR 0.000049 Loss 5.689522, Accuracy 84.875%\n",
      "Epoch 21, Batch 373, LR 0.000049 Loss 5.690111, Accuracy 84.863%\n",
      "Epoch 21, Batch 374, LR 0.000049 Loss 5.690498, Accuracy 84.858%\n",
      "Epoch 21, Batch 375, LR 0.000049 Loss 5.688941, Accuracy 84.854%\n",
      "Epoch 21, Batch 376, LR 0.000049 Loss 5.689567, Accuracy 84.840%\n",
      "Epoch 21, Batch 377, LR 0.000049 Loss 5.690164, Accuracy 84.835%\n",
      "Epoch 21, Batch 378, LR 0.000049 Loss 5.690463, Accuracy 84.832%\n",
      "Epoch 21, Batch 379, LR 0.000049 Loss 5.688271, Accuracy 84.837%\n",
      "Epoch 21, Batch 380, LR 0.000049 Loss 5.688040, Accuracy 84.840%\n",
      "Epoch 21, Batch 381, LR 0.000048 Loss 5.686234, Accuracy 84.849%\n",
      "Epoch 21, Batch 382, LR 0.000048 Loss 5.687840, Accuracy 84.839%\n",
      "Epoch 21, Batch 383, LR 0.000048 Loss 5.687474, Accuracy 84.834%\n",
      "Epoch 21, Batch 384, LR 0.000048 Loss 5.687038, Accuracy 84.837%\n",
      "Epoch 21, Batch 385, LR 0.000048 Loss 5.687456, Accuracy 84.840%\n",
      "Epoch 21, Batch 386, LR 0.000048 Loss 5.689170, Accuracy 84.824%\n",
      "Epoch 21, Batch 387, LR 0.000048 Loss 5.690435, Accuracy 84.817%\n",
      "Epoch 21, Batch 388, LR 0.000048 Loss 5.691507, Accuracy 84.810%\n",
      "Epoch 21, Batch 389, LR 0.000048 Loss 5.691378, Accuracy 84.825%\n",
      "Epoch 21, Batch 390, LR 0.000048 Loss 5.690309, Accuracy 84.834%\n",
      "Epoch 21, Batch 391, LR 0.000048 Loss 5.690317, Accuracy 84.835%\n",
      "Epoch 21, Batch 392, LR 0.000048 Loss 5.692772, Accuracy 84.821%\n",
      "Epoch 21, Batch 393, LR 0.000048 Loss 5.692856, Accuracy 84.816%\n",
      "Epoch 21, Batch 394, LR 0.000048 Loss 5.691374, Accuracy 84.835%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 395, LR 0.000048 Loss 5.692841, Accuracy 84.818%\n",
      "Epoch 21, Batch 396, LR 0.000048 Loss 5.691761, Accuracy 84.835%\n",
      "Epoch 21, Batch 397, LR 0.000048 Loss 5.691687, Accuracy 84.822%\n",
      "Epoch 21, Batch 398, LR 0.000048 Loss 5.690233, Accuracy 84.832%\n",
      "Epoch 21, Batch 399, LR 0.000048 Loss 5.690831, Accuracy 84.823%\n",
      "Epoch 21, Batch 400, LR 0.000048 Loss 5.690814, Accuracy 84.824%\n",
      "Epoch 21, Batch 401, LR 0.000048 Loss 5.691859, Accuracy 84.813%\n",
      "Epoch 21, Batch 402, LR 0.000048 Loss 5.691997, Accuracy 84.812%\n",
      "Epoch 21, Batch 403, LR 0.000048 Loss 5.692964, Accuracy 84.811%\n",
      "Epoch 21, Batch 404, LR 0.000048 Loss 5.693741, Accuracy 84.800%\n",
      "Epoch 21, Batch 405, LR 0.000048 Loss 5.691882, Accuracy 84.811%\n",
      "Epoch 21, Batch 406, LR 0.000048 Loss 5.692561, Accuracy 84.806%\n",
      "Epoch 21, Batch 407, LR 0.000048 Loss 5.690460, Accuracy 84.807%\n",
      "Epoch 21, Batch 408, LR 0.000048 Loss 5.689975, Accuracy 84.808%\n",
      "Epoch 21, Batch 409, LR 0.000048 Loss 5.689969, Accuracy 84.805%\n",
      "Epoch 21, Batch 410, LR 0.000048 Loss 5.689560, Accuracy 84.804%\n",
      "Epoch 21, Batch 411, LR 0.000048 Loss 5.688314, Accuracy 84.812%\n",
      "Epoch 21, Batch 412, LR 0.000048 Loss 5.687461, Accuracy 84.811%\n",
      "Epoch 21, Batch 413, LR 0.000048 Loss 5.686747, Accuracy 84.808%\n",
      "Epoch 21, Batch 414, LR 0.000048 Loss 5.686789, Accuracy 84.803%\n",
      "Epoch 21, Batch 415, LR 0.000048 Loss 5.686471, Accuracy 84.797%\n",
      "Epoch 21, Batch 416, LR 0.000048 Loss 5.686690, Accuracy 84.790%\n",
      "Epoch 21, Batch 417, LR 0.000048 Loss 5.684932, Accuracy 84.797%\n",
      "Epoch 21, Batch 418, LR 0.000048 Loss 5.683687, Accuracy 84.805%\n",
      "Epoch 21, Batch 419, LR 0.000048 Loss 5.684295, Accuracy 84.800%\n",
      "Epoch 21, Batch 420, LR 0.000048 Loss 5.683643, Accuracy 84.803%\n",
      "Epoch 21, Batch 421, LR 0.000048 Loss 5.681227, Accuracy 84.809%\n",
      "Epoch 21, Batch 422, LR 0.000048 Loss 5.681622, Accuracy 84.808%\n",
      "Epoch 21, Batch 423, LR 0.000048 Loss 5.682362, Accuracy 84.805%\n",
      "Epoch 21, Batch 424, LR 0.000048 Loss 5.683546, Accuracy 84.804%\n",
      "Epoch 21, Batch 425, LR 0.000048 Loss 5.684447, Accuracy 84.801%\n",
      "Epoch 21, Batch 426, LR 0.000048 Loss 5.683394, Accuracy 84.806%\n",
      "Epoch 21, Batch 427, LR 0.000048 Loss 5.682819, Accuracy 84.807%\n",
      "Epoch 21, Batch 428, LR 0.000048 Loss 5.681173, Accuracy 84.815%\n",
      "Epoch 21, Batch 429, LR 0.000048 Loss 5.679299, Accuracy 84.827%\n",
      "Epoch 21, Batch 430, LR 0.000048 Loss 5.678857, Accuracy 84.833%\n",
      "Epoch 21, Batch 431, LR 0.000048 Loss 5.679430, Accuracy 84.828%\n",
      "Epoch 21, Batch 432, LR 0.000048 Loss 5.680528, Accuracy 84.822%\n",
      "Epoch 21, Batch 433, LR 0.000048 Loss 5.678281, Accuracy 84.831%\n",
      "Epoch 21, Batch 434, LR 0.000048 Loss 5.678945, Accuracy 84.834%\n",
      "Epoch 21, Batch 435, LR 0.000048 Loss 5.678210, Accuracy 84.829%\n",
      "Epoch 21, Batch 436, LR 0.000048 Loss 5.679605, Accuracy 84.828%\n",
      "Epoch 21, Batch 437, LR 0.000048 Loss 5.679996, Accuracy 84.829%\n",
      "Epoch 21, Batch 438, LR 0.000048 Loss 5.679000, Accuracy 84.826%\n",
      "Epoch 21, Batch 439, LR 0.000048 Loss 5.677837, Accuracy 84.834%\n",
      "Epoch 21, Batch 440, LR 0.000048 Loss 5.678378, Accuracy 84.837%\n",
      "Epoch 21, Batch 441, LR 0.000048 Loss 5.677899, Accuracy 84.837%\n",
      "Epoch 21, Batch 442, LR 0.000048 Loss 5.679652, Accuracy 84.831%\n",
      "Epoch 21, Batch 443, LR 0.000048 Loss 5.680130, Accuracy 84.825%\n",
      "Epoch 21, Batch 444, LR 0.000048 Loss 5.680014, Accuracy 84.829%\n",
      "Epoch 21, Batch 445, LR 0.000048 Loss 5.679456, Accuracy 84.830%\n",
      "Epoch 21, Batch 446, LR 0.000048 Loss 5.678671, Accuracy 84.834%\n",
      "Epoch 21, Batch 447, LR 0.000048 Loss 5.679724, Accuracy 84.822%\n",
      "Epoch 21, Batch 448, LR 0.000048 Loss 5.678383, Accuracy 84.828%\n",
      "Epoch 21, Batch 449, LR 0.000048 Loss 5.676772, Accuracy 84.836%\n",
      "Epoch 21, Batch 450, LR 0.000048 Loss 5.676430, Accuracy 84.835%\n",
      "Epoch 21, Batch 451, LR 0.000048 Loss 5.677041, Accuracy 84.829%\n",
      "Epoch 21, Batch 452, LR 0.000048 Loss 5.677092, Accuracy 84.828%\n",
      "Epoch 21, Batch 453, LR 0.000048 Loss 5.678187, Accuracy 84.822%\n",
      "Epoch 21, Batch 454, LR 0.000048 Loss 5.678484, Accuracy 84.829%\n",
      "Epoch 21, Batch 455, LR 0.000048 Loss 5.678959, Accuracy 84.825%\n",
      "Epoch 21, Batch 456, LR 0.000048 Loss 5.678087, Accuracy 84.831%\n",
      "Epoch 21, Batch 457, LR 0.000048 Loss 5.677746, Accuracy 84.837%\n",
      "Epoch 21, Batch 458, LR 0.000048 Loss 5.678735, Accuracy 84.830%\n",
      "Epoch 21, Batch 459, LR 0.000048 Loss 5.677236, Accuracy 84.841%\n",
      "Epoch 21, Batch 460, LR 0.000048 Loss 5.676972, Accuracy 84.851%\n",
      "Epoch 21, Batch 461, LR 0.000048 Loss 5.677632, Accuracy 84.846%\n",
      "Epoch 21, Batch 462, LR 0.000048 Loss 5.675723, Accuracy 84.862%\n",
      "Epoch 21, Batch 463, LR 0.000048 Loss 5.675195, Accuracy 84.868%\n",
      "Epoch 21, Batch 464, LR 0.000048 Loss 5.672942, Accuracy 84.885%\n",
      "Epoch 21, Batch 465, LR 0.000048 Loss 5.674850, Accuracy 84.884%\n",
      "Epoch 21, Batch 466, LR 0.000048 Loss 5.674015, Accuracy 84.886%\n",
      "Epoch 21, Batch 467, LR 0.000048 Loss 5.673713, Accuracy 84.887%\n",
      "Epoch 21, Batch 468, LR 0.000048 Loss 5.673033, Accuracy 84.889%\n",
      "Epoch 21, Batch 469, LR 0.000048 Loss 5.673674, Accuracy 84.883%\n",
      "Epoch 21, Batch 470, LR 0.000048 Loss 5.674664, Accuracy 84.885%\n",
      "Epoch 21, Batch 471, LR 0.000048 Loss 5.673639, Accuracy 84.888%\n",
      "Epoch 21, Batch 472, LR 0.000048 Loss 5.673796, Accuracy 84.893%\n",
      "Epoch 21, Batch 473, LR 0.000048 Loss 5.674057, Accuracy 84.890%\n",
      "Epoch 21, Batch 474, LR 0.000048 Loss 5.674508, Accuracy 84.886%\n",
      "Epoch 21, Batch 475, LR 0.000048 Loss 5.674573, Accuracy 84.887%\n",
      "Epoch 21, Batch 476, LR 0.000048 Loss 5.672965, Accuracy 84.899%\n",
      "Epoch 21, Batch 477, LR 0.000048 Loss 5.670536, Accuracy 84.917%\n",
      "Epoch 21, Batch 478, LR 0.000048 Loss 5.668931, Accuracy 84.924%\n",
      "Epoch 21, Batch 479, LR 0.000048 Loss 5.669847, Accuracy 84.913%\n",
      "Epoch 21, Batch 480, LR 0.000048 Loss 5.670507, Accuracy 84.914%\n",
      "Epoch 21, Batch 481, LR 0.000048 Loss 5.671055, Accuracy 84.906%\n",
      "Epoch 21, Batch 482, LR 0.000048 Loss 5.671405, Accuracy 84.900%\n",
      "Epoch 21, Batch 483, LR 0.000048 Loss 5.669927, Accuracy 84.902%\n",
      "Epoch 21, Batch 484, LR 0.000048 Loss 5.668881, Accuracy 84.909%\n",
      "Epoch 21, Batch 485, LR 0.000048 Loss 5.668774, Accuracy 84.911%\n",
      "Epoch 21, Batch 486, LR 0.000048 Loss 5.669471, Accuracy 84.904%\n",
      "Epoch 21, Batch 487, LR 0.000048 Loss 5.668602, Accuracy 84.906%\n",
      "Epoch 21, Batch 488, LR 0.000048 Loss 5.669075, Accuracy 84.908%\n",
      "Epoch 21, Batch 489, LR 0.000048 Loss 5.667837, Accuracy 84.910%\n",
      "Epoch 21, Batch 490, LR 0.000048 Loss 5.668875, Accuracy 84.906%\n",
      "Epoch 21, Batch 491, LR 0.000048 Loss 5.668592, Accuracy 84.905%\n",
      "Epoch 21, Batch 492, LR 0.000048 Loss 5.668397, Accuracy 84.907%\n",
      "Epoch 21, Batch 493, LR 0.000048 Loss 5.668769, Accuracy 84.898%\n",
      "Epoch 21, Batch 494, LR 0.000048 Loss 5.668604, Accuracy 84.897%\n",
      "Epoch 21, Batch 495, LR 0.000048 Loss 5.668924, Accuracy 84.882%\n",
      "Epoch 21, Batch 496, LR 0.000048 Loss 5.668644, Accuracy 84.877%\n",
      "Epoch 21, Batch 497, LR 0.000048 Loss 5.668164, Accuracy 84.880%\n",
      "Epoch 21, Batch 498, LR 0.000048 Loss 5.667845, Accuracy 84.880%\n",
      "Epoch 21, Batch 499, LR 0.000048 Loss 5.667027, Accuracy 84.887%\n",
      "Epoch 21, Batch 500, LR 0.000048 Loss 5.665445, Accuracy 84.894%\n",
      "Epoch 21, Batch 501, LR 0.000048 Loss 5.664662, Accuracy 84.902%\n",
      "Epoch 21, Batch 502, LR 0.000048 Loss 5.663877, Accuracy 84.896%\n",
      "Epoch 21, Batch 503, LR 0.000048 Loss 5.663781, Accuracy 84.897%\n",
      "Epoch 21, Batch 504, LR 0.000048 Loss 5.662736, Accuracy 84.910%\n",
      "Epoch 21, Batch 505, LR 0.000048 Loss 5.662597, Accuracy 84.916%\n",
      "Epoch 21, Batch 506, LR 0.000048 Loss 5.661635, Accuracy 84.917%\n",
      "Epoch 21, Batch 507, LR 0.000048 Loss 5.663018, Accuracy 84.911%\n",
      "Epoch 21, Batch 508, LR 0.000048 Loss 5.662877, Accuracy 84.916%\n",
      "Epoch 21, Batch 509, LR 0.000048 Loss 5.663227, Accuracy 84.909%\n",
      "Epoch 21, Batch 510, LR 0.000048 Loss 5.663650, Accuracy 84.907%\n",
      "Epoch 21, Batch 511, LR 0.000048 Loss 5.662849, Accuracy 84.913%\n",
      "Epoch 21, Batch 512, LR 0.000048 Loss 5.662063, Accuracy 84.923%\n",
      "Epoch 21, Batch 513, LR 0.000048 Loss 5.661498, Accuracy 84.920%\n",
      "Epoch 21, Batch 514, LR 0.000048 Loss 5.660490, Accuracy 84.924%\n",
      "Epoch 21, Batch 515, LR 0.000048 Loss 5.659612, Accuracy 84.927%\n",
      "Epoch 21, Batch 516, LR 0.000048 Loss 5.660348, Accuracy 84.923%\n",
      "Epoch 21, Batch 517, LR 0.000048 Loss 5.659495, Accuracy 84.927%\n",
      "Epoch 21, Batch 518, LR 0.000048 Loss 5.659049, Accuracy 84.930%\n",
      "Epoch 21, Batch 519, LR 0.000048 Loss 5.658703, Accuracy 84.924%\n",
      "Epoch 21, Batch 520, LR 0.000048 Loss 5.659384, Accuracy 84.920%\n",
      "Epoch 21, Batch 521, LR 0.000048 Loss 5.658388, Accuracy 84.925%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 522, LR 0.000048 Loss 5.657694, Accuracy 84.930%\n",
      "Epoch 21, Batch 523, LR 0.000048 Loss 5.657722, Accuracy 84.935%\n",
      "Epoch 21, Batch 524, LR 0.000048 Loss 5.656367, Accuracy 84.936%\n",
      "Epoch 21, Batch 525, LR 0.000048 Loss 5.655745, Accuracy 84.936%\n",
      "Epoch 21, Batch 526, LR 0.000048 Loss 5.656570, Accuracy 84.930%\n",
      "Epoch 21, Batch 527, LR 0.000048 Loss 5.655639, Accuracy 84.938%\n",
      "Epoch 21, Batch 528, LR 0.000048 Loss 5.654527, Accuracy 84.937%\n",
      "Epoch 21, Batch 529, LR 0.000048 Loss 5.654565, Accuracy 84.938%\n",
      "Epoch 21, Batch 530, LR 0.000048 Loss 5.655853, Accuracy 84.938%\n",
      "Epoch 21, Batch 531, LR 0.000048 Loss 5.655639, Accuracy 84.936%\n",
      "Epoch 21, Batch 532, LR 0.000048 Loss 5.655652, Accuracy 84.935%\n",
      "Epoch 21, Batch 533, LR 0.000048 Loss 5.655293, Accuracy 84.941%\n",
      "Epoch 21, Batch 534, LR 0.000048 Loss 5.655352, Accuracy 84.941%\n",
      "Epoch 21, Batch 535, LR 0.000048 Loss 5.655958, Accuracy 84.945%\n",
      "Epoch 21, Batch 536, LR 0.000048 Loss 5.654762, Accuracy 84.951%\n",
      "Epoch 21, Batch 537, LR 0.000048 Loss 5.655617, Accuracy 84.951%\n",
      "Epoch 21, Batch 538, LR 0.000048 Loss 5.655507, Accuracy 84.951%\n",
      "Epoch 21, Batch 539, LR 0.000048 Loss 5.655921, Accuracy 84.952%\n",
      "Epoch 21, Batch 540, LR 0.000048 Loss 5.656418, Accuracy 84.951%\n",
      "Epoch 21, Batch 541, LR 0.000048 Loss 5.656252, Accuracy 84.957%\n",
      "Epoch 21, Batch 542, LR 0.000048 Loss 5.656442, Accuracy 84.962%\n",
      "Epoch 21, Batch 543, LR 0.000048 Loss 5.656285, Accuracy 84.963%\n",
      "Epoch 21, Batch 544, LR 0.000048 Loss 5.656321, Accuracy 84.964%\n",
      "Epoch 21, Batch 545, LR 0.000048 Loss 5.656058, Accuracy 84.970%\n",
      "Epoch 21, Batch 546, LR 0.000048 Loss 5.656736, Accuracy 84.967%\n",
      "Epoch 21, Batch 547, LR 0.000048 Loss 5.655621, Accuracy 84.973%\n",
      "Epoch 21, Batch 548, LR 0.000048 Loss 5.655598, Accuracy 84.971%\n",
      "Epoch 21, Batch 549, LR 0.000048 Loss 5.655555, Accuracy 84.973%\n",
      "Epoch 21, Batch 550, LR 0.000048 Loss 5.655615, Accuracy 84.966%\n",
      "Epoch 21, Batch 551, LR 0.000048 Loss 5.655547, Accuracy 84.971%\n",
      "Epoch 21, Batch 552, LR 0.000048 Loss 5.654627, Accuracy 84.972%\n",
      "Epoch 21, Batch 553, LR 0.000048 Loss 5.653219, Accuracy 84.978%\n",
      "Epoch 21, Batch 554, LR 0.000048 Loss 5.654434, Accuracy 84.970%\n",
      "Epoch 21, Batch 555, LR 0.000048 Loss 5.654807, Accuracy 84.970%\n",
      "Epoch 21, Batch 556, LR 0.000048 Loss 5.654564, Accuracy 84.968%\n",
      "Epoch 21, Batch 557, LR 0.000048 Loss 5.653147, Accuracy 84.978%\n",
      "Epoch 21, Batch 558, LR 0.000048 Loss 5.653231, Accuracy 84.981%\n",
      "Epoch 21, Batch 559, LR 0.000048 Loss 5.653827, Accuracy 84.976%\n",
      "Epoch 21, Batch 560, LR 0.000048 Loss 5.655785, Accuracy 84.961%\n",
      "Epoch 21, Batch 561, LR 0.000048 Loss 5.657725, Accuracy 84.950%\n",
      "Epoch 21, Batch 562, LR 0.000048 Loss 5.657959, Accuracy 84.951%\n",
      "Epoch 21, Batch 563, LR 0.000048 Loss 5.656944, Accuracy 84.959%\n",
      "Epoch 21, Batch 564, LR 0.000048 Loss 5.657124, Accuracy 84.951%\n",
      "Epoch 21, Batch 565, LR 0.000048 Loss 5.656836, Accuracy 84.950%\n",
      "Epoch 21, Batch 566, LR 0.000048 Loss 5.657420, Accuracy 84.940%\n",
      "Epoch 21, Batch 567, LR 0.000048 Loss 5.657320, Accuracy 84.939%\n",
      "Epoch 21, Batch 568, LR 0.000048 Loss 5.657711, Accuracy 84.943%\n",
      "Epoch 21, Batch 569, LR 0.000048 Loss 5.658914, Accuracy 84.938%\n",
      "Epoch 21, Batch 570, LR 0.000048 Loss 5.659013, Accuracy 84.934%\n",
      "Epoch 21, Batch 571, LR 0.000048 Loss 5.659855, Accuracy 84.932%\n",
      "Epoch 21, Batch 572, LR 0.000048 Loss 5.660446, Accuracy 84.927%\n",
      "Epoch 21, Batch 573, LR 0.000048 Loss 5.659471, Accuracy 84.935%\n",
      "Epoch 21, Batch 574, LR 0.000048 Loss 5.659297, Accuracy 84.945%\n",
      "Epoch 21, Batch 575, LR 0.000048 Loss 5.658561, Accuracy 84.950%\n",
      "Epoch 21, Batch 576, LR 0.000048 Loss 5.660364, Accuracy 84.939%\n",
      "Epoch 21, Batch 577, LR 0.000048 Loss 5.659578, Accuracy 84.941%\n",
      "Epoch 21, Batch 578, LR 0.000048 Loss 5.660734, Accuracy 84.933%\n",
      "Epoch 21, Batch 579, LR 0.000048 Loss 5.660717, Accuracy 84.923%\n",
      "Epoch 21, Batch 580, LR 0.000048 Loss 5.661000, Accuracy 84.926%\n",
      "Epoch 21, Batch 581, LR 0.000048 Loss 5.661725, Accuracy 84.914%\n",
      "Epoch 21, Batch 582, LR 0.000048 Loss 5.663285, Accuracy 84.909%\n",
      "Epoch 21, Batch 583, LR 0.000048 Loss 5.662177, Accuracy 84.918%\n",
      "Epoch 21, Batch 584, LR 0.000048 Loss 5.661151, Accuracy 84.921%\n",
      "Epoch 21, Batch 585, LR 0.000048 Loss 5.660380, Accuracy 84.925%\n",
      "Epoch 21, Batch 586, LR 0.000048 Loss 5.660477, Accuracy 84.927%\n",
      "Epoch 21, Batch 587, LR 0.000048 Loss 5.660224, Accuracy 84.930%\n",
      "Epoch 21, Batch 588, LR 0.000048 Loss 5.662699, Accuracy 84.916%\n",
      "Epoch 21, Batch 589, LR 0.000048 Loss 5.662668, Accuracy 84.915%\n",
      "Epoch 21, Batch 590, LR 0.000048 Loss 5.663205, Accuracy 84.915%\n",
      "Epoch 21, Batch 591, LR 0.000048 Loss 5.662969, Accuracy 84.913%\n",
      "Epoch 21, Batch 592, LR 0.000048 Loss 5.663341, Accuracy 84.911%\n",
      "Epoch 21, Batch 593, LR 0.000048 Loss 5.662824, Accuracy 84.911%\n",
      "Epoch 21, Batch 594, LR 0.000048 Loss 5.663564, Accuracy 84.914%\n",
      "Epoch 21, Batch 595, LR 0.000048 Loss 5.663439, Accuracy 84.915%\n",
      "Epoch 21, Batch 596, LR 0.000048 Loss 5.662845, Accuracy 84.914%\n",
      "Epoch 21, Batch 597, LR 0.000048 Loss 5.664063, Accuracy 84.912%\n",
      "Epoch 21, Batch 598, LR 0.000048 Loss 5.664417, Accuracy 84.904%\n",
      "Epoch 21, Batch 599, LR 0.000048 Loss 5.664938, Accuracy 84.903%\n",
      "Epoch 21, Batch 600, LR 0.000048 Loss 5.665418, Accuracy 84.902%\n",
      "Epoch 21, Batch 601, LR 0.000048 Loss 5.663925, Accuracy 84.907%\n",
      "Epoch 21, Batch 602, LR 0.000048 Loss 5.663450, Accuracy 84.912%\n",
      "Epoch 21, Batch 603, LR 0.000048 Loss 5.663635, Accuracy 84.913%\n",
      "Epoch 21, Batch 604, LR 0.000048 Loss 5.662666, Accuracy 84.913%\n",
      "Epoch 21, Batch 605, LR 0.000048 Loss 5.662574, Accuracy 84.912%\n",
      "Epoch 21, Batch 606, LR 0.000048 Loss 5.663969, Accuracy 84.911%\n",
      "Epoch 21, Batch 607, LR 0.000048 Loss 5.662032, Accuracy 84.921%\n",
      "Epoch 21, Batch 608, LR 0.000048 Loss 5.661381, Accuracy 84.919%\n",
      "Epoch 21, Batch 609, LR 0.000048 Loss 5.660133, Accuracy 84.923%\n",
      "Epoch 21, Batch 610, LR 0.000048 Loss 5.659530, Accuracy 84.927%\n",
      "Epoch 21, Batch 611, LR 0.000048 Loss 5.659151, Accuracy 84.921%\n",
      "Epoch 21, Batch 612, LR 0.000048 Loss 5.658736, Accuracy 84.925%\n",
      "Epoch 21, Batch 613, LR 0.000048 Loss 5.657748, Accuracy 84.931%\n",
      "Epoch 21, Batch 614, LR 0.000048 Loss 5.657060, Accuracy 84.936%\n",
      "Epoch 21, Batch 615, LR 0.000048 Loss 5.656239, Accuracy 84.942%\n",
      "Epoch 21, Batch 616, LR 0.000048 Loss 5.655901, Accuracy 84.938%\n",
      "Epoch 21, Batch 617, LR 0.000048 Loss 5.655991, Accuracy 84.936%\n",
      "Epoch 21, Batch 618, LR 0.000048 Loss 5.656385, Accuracy 84.936%\n",
      "Epoch 21, Batch 619, LR 0.000048 Loss 5.655926, Accuracy 84.937%\n",
      "Epoch 21, Batch 620, LR 0.000048 Loss 5.655970, Accuracy 84.932%\n",
      "Epoch 21, Batch 621, LR 0.000048 Loss 5.655767, Accuracy 84.931%\n",
      "Epoch 21, Batch 622, LR 0.000048 Loss 5.656056, Accuracy 84.928%\n",
      "Epoch 21, Batch 623, LR 0.000048 Loss 5.655683, Accuracy 84.929%\n",
      "Epoch 21, Batch 624, LR 0.000048 Loss 5.655945, Accuracy 84.928%\n",
      "Epoch 21, Batch 625, LR 0.000048 Loss 5.655164, Accuracy 84.934%\n",
      "Epoch 21, Batch 626, LR 0.000048 Loss 5.655038, Accuracy 84.937%\n",
      "Epoch 21, Batch 627, LR 0.000048 Loss 5.655243, Accuracy 84.938%\n",
      "Epoch 21, Batch 628, LR 0.000048 Loss 5.655310, Accuracy 84.941%\n",
      "Epoch 21, Batch 629, LR 0.000048 Loss 5.656449, Accuracy 84.936%\n",
      "Epoch 21, Batch 630, LR 0.000048 Loss 5.655121, Accuracy 84.949%\n",
      "Epoch 21, Batch 631, LR 0.000048 Loss 5.656018, Accuracy 84.943%\n",
      "Epoch 21, Batch 632, LR 0.000048 Loss 5.656503, Accuracy 84.935%\n",
      "Epoch 21, Batch 633, LR 0.000048 Loss 5.655724, Accuracy 84.937%\n",
      "Epoch 21, Batch 634, LR 0.000047 Loss 5.654934, Accuracy 84.942%\n",
      "Epoch 21, Batch 635, LR 0.000047 Loss 5.654049, Accuracy 84.945%\n",
      "Epoch 21, Batch 636, LR 0.000047 Loss 5.653699, Accuracy 84.938%\n",
      "Epoch 21, Batch 637, LR 0.000047 Loss 5.653971, Accuracy 84.935%\n",
      "Epoch 21, Batch 638, LR 0.000047 Loss 5.654339, Accuracy 84.926%\n",
      "Epoch 21, Batch 639, LR 0.000047 Loss 5.655444, Accuracy 84.928%\n",
      "Epoch 21, Batch 640, LR 0.000047 Loss 5.655344, Accuracy 84.929%\n",
      "Epoch 21, Batch 641, LR 0.000047 Loss 5.655765, Accuracy 84.928%\n",
      "Epoch 21, Batch 642, LR 0.000047 Loss 5.655505, Accuracy 84.923%\n",
      "Epoch 21, Batch 643, LR 0.000047 Loss 5.654171, Accuracy 84.929%\n",
      "Epoch 21, Batch 644, LR 0.000047 Loss 5.653697, Accuracy 84.926%\n",
      "Epoch 21, Batch 645, LR 0.000047 Loss 5.653769, Accuracy 84.926%\n",
      "Epoch 21, Batch 646, LR 0.000047 Loss 5.652549, Accuracy 84.934%\n",
      "Epoch 21, Batch 647, LR 0.000047 Loss 5.653004, Accuracy 84.929%\n",
      "Epoch 21, Batch 648, LR 0.000047 Loss 5.651050, Accuracy 84.938%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 649, LR 0.000047 Loss 5.651967, Accuracy 84.931%\n",
      "Epoch 21, Batch 650, LR 0.000047 Loss 5.651965, Accuracy 84.933%\n",
      "Epoch 21, Batch 651, LR 0.000047 Loss 5.651895, Accuracy 84.937%\n",
      "Epoch 21, Batch 652, LR 0.000047 Loss 5.652317, Accuracy 84.937%\n",
      "Epoch 21, Batch 653, LR 0.000047 Loss 5.652818, Accuracy 84.936%\n",
      "Epoch 21, Batch 654, LR 0.000047 Loss 5.652133, Accuracy 84.941%\n",
      "Epoch 21, Batch 655, LR 0.000047 Loss 5.651864, Accuracy 84.944%\n",
      "Epoch 21, Batch 656, LR 0.000047 Loss 5.651919, Accuracy 84.949%\n",
      "Epoch 21, Batch 657, LR 0.000047 Loss 5.649874, Accuracy 84.958%\n",
      "Epoch 21, Batch 658, LR 0.000047 Loss 5.649782, Accuracy 84.958%\n",
      "Epoch 21, Batch 659, LR 0.000047 Loss 5.649711, Accuracy 84.957%\n",
      "Epoch 21, Batch 660, LR 0.000047 Loss 5.649240, Accuracy 84.962%\n",
      "Epoch 21, Batch 661, LR 0.000047 Loss 5.649934, Accuracy 84.962%\n",
      "Epoch 21, Batch 662, LR 0.000047 Loss 5.650715, Accuracy 84.958%\n",
      "Epoch 21, Batch 663, LR 0.000047 Loss 5.650170, Accuracy 84.967%\n",
      "Epoch 21, Batch 664, LR 0.000047 Loss 5.650519, Accuracy 84.972%\n",
      "Epoch 21, Batch 665, LR 0.000047 Loss 5.650700, Accuracy 84.977%\n",
      "Epoch 21, Batch 666, LR 0.000047 Loss 5.650787, Accuracy 84.976%\n",
      "Epoch 21, Batch 667, LR 0.000047 Loss 5.650997, Accuracy 84.970%\n",
      "Epoch 21, Batch 668, LR 0.000047 Loss 5.651582, Accuracy 84.968%\n",
      "Epoch 21, Batch 669, LR 0.000047 Loss 5.651474, Accuracy 84.967%\n",
      "Epoch 21, Batch 670, LR 0.000047 Loss 5.651039, Accuracy 84.971%\n",
      "Epoch 21, Batch 671, LR 0.000047 Loss 5.650448, Accuracy 84.972%\n",
      "Epoch 21, Batch 672, LR 0.000047 Loss 5.650900, Accuracy 84.967%\n",
      "Epoch 21, Batch 673, LR 0.000047 Loss 5.651059, Accuracy 84.961%\n",
      "Epoch 21, Batch 674, LR 0.000047 Loss 5.652078, Accuracy 84.949%\n",
      "Epoch 21, Batch 675, LR 0.000047 Loss 5.653148, Accuracy 84.935%\n",
      "Epoch 21, Batch 676, LR 0.000047 Loss 5.652153, Accuracy 84.942%\n",
      "Epoch 21, Batch 677, LR 0.000047 Loss 5.651962, Accuracy 84.946%\n",
      "Epoch 21, Batch 678, LR 0.000047 Loss 5.651593, Accuracy 84.951%\n",
      "Epoch 21, Batch 679, LR 0.000047 Loss 5.651565, Accuracy 84.956%\n",
      "Epoch 21, Batch 680, LR 0.000047 Loss 5.652704, Accuracy 84.948%\n",
      "Epoch 21, Batch 681, LR 0.000047 Loss 5.652938, Accuracy 84.954%\n",
      "Epoch 21, Batch 682, LR 0.000047 Loss 5.652696, Accuracy 84.960%\n",
      "Epoch 21, Batch 683, LR 0.000047 Loss 5.651592, Accuracy 84.965%\n",
      "Epoch 21, Batch 684, LR 0.000047 Loss 5.651352, Accuracy 84.967%\n",
      "Epoch 21, Batch 685, LR 0.000047 Loss 5.652349, Accuracy 84.969%\n",
      "Epoch 21, Batch 686, LR 0.000047 Loss 5.652383, Accuracy 84.969%\n",
      "Epoch 21, Batch 687, LR 0.000047 Loss 5.652680, Accuracy 84.967%\n",
      "Epoch 21, Batch 688, LR 0.000047 Loss 5.653256, Accuracy 84.968%\n",
      "Epoch 21, Batch 689, LR 0.000047 Loss 5.653885, Accuracy 84.971%\n",
      "Epoch 21, Batch 690, LR 0.000047 Loss 5.652686, Accuracy 84.977%\n",
      "Epoch 21, Batch 691, LR 0.000047 Loss 5.653931, Accuracy 84.972%\n",
      "Epoch 21, Batch 692, LR 0.000047 Loss 5.653644, Accuracy 84.971%\n",
      "Epoch 21, Batch 693, LR 0.000047 Loss 5.654856, Accuracy 84.966%\n",
      "Epoch 21, Batch 694, LR 0.000047 Loss 5.654386, Accuracy 84.975%\n",
      "Epoch 21, Batch 695, LR 0.000047 Loss 5.654655, Accuracy 84.978%\n",
      "Epoch 21, Batch 696, LR 0.000047 Loss 5.654540, Accuracy 84.981%\n",
      "Epoch 21, Batch 697, LR 0.000047 Loss 5.653705, Accuracy 84.981%\n",
      "Epoch 21, Batch 698, LR 0.000047 Loss 5.654600, Accuracy 84.976%\n",
      "Epoch 21, Batch 699, LR 0.000047 Loss 5.654118, Accuracy 84.979%\n",
      "Epoch 21, Batch 700, LR 0.000047 Loss 5.653631, Accuracy 84.975%\n",
      "Epoch 21, Batch 701, LR 0.000047 Loss 5.653003, Accuracy 84.980%\n",
      "Epoch 21, Batch 702, LR 0.000047 Loss 5.653082, Accuracy 84.974%\n",
      "Epoch 21, Batch 703, LR 0.000047 Loss 5.652990, Accuracy 84.971%\n",
      "Epoch 21, Batch 704, LR 0.000047 Loss 5.651385, Accuracy 84.979%\n",
      "Epoch 21, Batch 705, LR 0.000047 Loss 5.650722, Accuracy 84.982%\n",
      "Epoch 21, Batch 706, LR 0.000047 Loss 5.650481, Accuracy 84.983%\n",
      "Epoch 21, Batch 707, LR 0.000047 Loss 5.650122, Accuracy 84.985%\n",
      "Epoch 21, Batch 708, LR 0.000047 Loss 5.650354, Accuracy 84.984%\n",
      "Epoch 21, Batch 709, LR 0.000047 Loss 5.650152, Accuracy 84.979%\n",
      "Epoch 21, Batch 710, LR 0.000047 Loss 5.650331, Accuracy 84.979%\n",
      "Epoch 21, Batch 711, LR 0.000047 Loss 5.650095, Accuracy 84.982%\n",
      "Epoch 21, Batch 712, LR 0.000047 Loss 5.649586, Accuracy 84.989%\n",
      "Epoch 21, Batch 713, LR 0.000047 Loss 5.650319, Accuracy 84.990%\n",
      "Epoch 21, Batch 714, LR 0.000047 Loss 5.651060, Accuracy 84.976%\n",
      "Epoch 21, Batch 715, LR 0.000047 Loss 5.650715, Accuracy 84.979%\n",
      "Epoch 21, Batch 716, LR 0.000047 Loss 5.651016, Accuracy 84.972%\n",
      "Epoch 21, Batch 717, LR 0.000047 Loss 5.652837, Accuracy 84.958%\n",
      "Epoch 21, Batch 718, LR 0.000047 Loss 5.653202, Accuracy 84.956%\n",
      "Epoch 21, Batch 719, LR 0.000047 Loss 5.653702, Accuracy 84.955%\n",
      "Epoch 21, Batch 720, LR 0.000047 Loss 5.653440, Accuracy 84.957%\n",
      "Epoch 21, Batch 721, LR 0.000047 Loss 5.653411, Accuracy 84.950%\n",
      "Epoch 21, Batch 722, LR 0.000047 Loss 5.653771, Accuracy 84.948%\n",
      "Epoch 21, Batch 723, LR 0.000047 Loss 5.652668, Accuracy 84.953%\n",
      "Epoch 21, Batch 724, LR 0.000047 Loss 5.650888, Accuracy 84.962%\n",
      "Epoch 21, Batch 725, LR 0.000047 Loss 5.650541, Accuracy 84.963%\n",
      "Epoch 21, Batch 726, LR 0.000047 Loss 5.651171, Accuracy 84.960%\n",
      "Epoch 21, Batch 727, LR 0.000047 Loss 5.650544, Accuracy 84.964%\n",
      "Epoch 21, Batch 728, LR 0.000047 Loss 5.650836, Accuracy 84.964%\n",
      "Epoch 21, Batch 729, LR 0.000047 Loss 5.651156, Accuracy 84.956%\n",
      "Epoch 21, Batch 730, LR 0.000047 Loss 5.652290, Accuracy 84.948%\n",
      "Epoch 21, Batch 731, LR 0.000047 Loss 5.651860, Accuracy 84.948%\n",
      "Epoch 21, Batch 732, LR 0.000047 Loss 5.652564, Accuracy 84.945%\n",
      "Epoch 21, Batch 733, LR 0.000047 Loss 5.651726, Accuracy 84.949%\n",
      "Epoch 21, Batch 734, LR 0.000047 Loss 5.651882, Accuracy 84.944%\n",
      "Epoch 21, Batch 735, LR 0.000047 Loss 5.652361, Accuracy 84.945%\n",
      "Epoch 21, Batch 736, LR 0.000047 Loss 5.652491, Accuracy 84.940%\n",
      "Epoch 21, Batch 737, LR 0.000047 Loss 5.652522, Accuracy 84.936%\n",
      "Epoch 21, Batch 738, LR 0.000047 Loss 5.650759, Accuracy 84.942%\n",
      "Epoch 21, Batch 739, LR 0.000047 Loss 5.651413, Accuracy 84.943%\n",
      "Epoch 21, Batch 740, LR 0.000047 Loss 5.651586, Accuracy 84.941%\n",
      "Epoch 21, Batch 741, LR 0.000047 Loss 5.651884, Accuracy 84.937%\n",
      "Epoch 21, Batch 742, LR 0.000047 Loss 5.652251, Accuracy 84.944%\n",
      "Epoch 21, Batch 743, LR 0.000047 Loss 5.652841, Accuracy 84.939%\n",
      "Epoch 21, Batch 744, LR 0.000047 Loss 5.652446, Accuracy 84.937%\n",
      "Epoch 21, Batch 745, LR 0.000047 Loss 5.652426, Accuracy 84.935%\n",
      "Epoch 21, Batch 746, LR 0.000047 Loss 5.652487, Accuracy 84.937%\n",
      "Epoch 21, Batch 747, LR 0.000047 Loss 5.651865, Accuracy 84.935%\n",
      "Epoch 21, Batch 748, LR 0.000047 Loss 5.651749, Accuracy 84.934%\n",
      "Epoch 21, Batch 749, LR 0.000047 Loss 5.651767, Accuracy 84.934%\n",
      "Epoch 21, Batch 750, LR 0.000047 Loss 5.651491, Accuracy 84.938%\n",
      "Epoch 21, Batch 751, LR 0.000047 Loss 5.650769, Accuracy 84.940%\n",
      "Epoch 21, Batch 752, LR 0.000047 Loss 5.650826, Accuracy 84.935%\n",
      "Epoch 21, Batch 753, LR 0.000047 Loss 5.650707, Accuracy 84.935%\n",
      "Epoch 21, Batch 754, LR 0.000047 Loss 5.650901, Accuracy 84.933%\n",
      "Epoch 21, Batch 755, LR 0.000047 Loss 5.650795, Accuracy 84.932%\n",
      "Epoch 21, Batch 756, LR 0.000047 Loss 5.651259, Accuracy 84.928%\n",
      "Epoch 21, Batch 757, LR 0.000047 Loss 5.651849, Accuracy 84.923%\n",
      "Epoch 21, Batch 758, LR 0.000047 Loss 5.651473, Accuracy 84.926%\n",
      "Epoch 21, Batch 759, LR 0.000047 Loss 5.652036, Accuracy 84.922%\n",
      "Epoch 21, Batch 760, LR 0.000047 Loss 5.651592, Accuracy 84.924%\n",
      "Epoch 21, Batch 761, LR 0.000047 Loss 5.651800, Accuracy 84.927%\n",
      "Epoch 21, Batch 762, LR 0.000047 Loss 5.651483, Accuracy 84.929%\n",
      "Epoch 21, Batch 763, LR 0.000047 Loss 5.651659, Accuracy 84.925%\n",
      "Epoch 21, Batch 764, LR 0.000047 Loss 5.650863, Accuracy 84.928%\n",
      "Epoch 21, Batch 765, LR 0.000047 Loss 5.650233, Accuracy 84.933%\n",
      "Epoch 21, Batch 766, LR 0.000047 Loss 5.649729, Accuracy 84.932%\n",
      "Epoch 21, Batch 767, LR 0.000047 Loss 5.650073, Accuracy 84.927%\n",
      "Epoch 21, Batch 768, LR 0.000047 Loss 5.650989, Accuracy 84.926%\n",
      "Epoch 21, Batch 769, LR 0.000047 Loss 5.650758, Accuracy 84.931%\n",
      "Epoch 21, Batch 770, LR 0.000047 Loss 5.651051, Accuracy 84.934%\n",
      "Epoch 21, Batch 771, LR 0.000047 Loss 5.650567, Accuracy 84.934%\n",
      "Epoch 21, Batch 772, LR 0.000047 Loss 5.651111, Accuracy 84.934%\n",
      "Epoch 21, Batch 773, LR 0.000047 Loss 5.650805, Accuracy 84.933%\n",
      "Epoch 21, Batch 774, LR 0.000047 Loss 5.650195, Accuracy 84.936%\n",
      "Epoch 21, Batch 775, LR 0.000047 Loss 5.651858, Accuracy 84.929%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 776, LR 0.000047 Loss 5.651568, Accuracy 84.929%\n",
      "Epoch 21, Batch 777, LR 0.000047 Loss 5.650484, Accuracy 84.936%\n",
      "Epoch 21, Batch 778, LR 0.000047 Loss 5.652031, Accuracy 84.928%\n",
      "Epoch 21, Batch 779, LR 0.000047 Loss 5.651445, Accuracy 84.937%\n",
      "Epoch 21, Batch 780, LR 0.000047 Loss 5.651228, Accuracy 84.939%\n",
      "Epoch 21, Batch 781, LR 0.000047 Loss 5.652454, Accuracy 84.926%\n",
      "Epoch 21, Batch 782, LR 0.000047 Loss 5.652642, Accuracy 84.927%\n",
      "Epoch 21, Batch 783, LR 0.000047 Loss 5.651434, Accuracy 84.935%\n",
      "Epoch 21, Batch 784, LR 0.000047 Loss 5.650966, Accuracy 84.937%\n",
      "Epoch 21, Batch 785, LR 0.000047 Loss 5.650150, Accuracy 84.941%\n",
      "Epoch 21, Batch 786, LR 0.000047 Loss 5.650959, Accuracy 84.938%\n",
      "Epoch 21, Batch 787, LR 0.000047 Loss 5.651179, Accuracy 84.939%\n",
      "Epoch 21, Batch 788, LR 0.000047 Loss 5.650406, Accuracy 84.943%\n",
      "Epoch 21, Batch 789, LR 0.000047 Loss 5.651035, Accuracy 84.939%\n",
      "Epoch 21, Batch 790, LR 0.000047 Loss 5.651319, Accuracy 84.943%\n",
      "Epoch 21, Batch 791, LR 0.000047 Loss 5.650887, Accuracy 84.938%\n",
      "Epoch 21, Batch 792, LR 0.000047 Loss 5.650649, Accuracy 84.937%\n",
      "Epoch 21, Batch 793, LR 0.000047 Loss 5.650477, Accuracy 84.940%\n",
      "Epoch 21, Batch 794, LR 0.000047 Loss 5.649726, Accuracy 84.941%\n",
      "Epoch 21, Batch 795, LR 0.000047 Loss 5.649997, Accuracy 84.940%\n",
      "Epoch 21, Batch 796, LR 0.000047 Loss 5.650366, Accuracy 84.939%\n",
      "Epoch 21, Batch 797, LR 0.000047 Loss 5.650751, Accuracy 84.937%\n",
      "Epoch 21, Batch 798, LR 0.000047 Loss 5.650389, Accuracy 84.938%\n",
      "Epoch 21, Batch 799, LR 0.000047 Loss 5.649792, Accuracy 84.940%\n",
      "Epoch 21, Batch 800, LR 0.000047 Loss 5.649684, Accuracy 84.939%\n",
      "Epoch 21, Batch 801, LR 0.000047 Loss 5.650140, Accuracy 84.937%\n",
      "Epoch 21, Batch 802, LR 0.000047 Loss 5.649990, Accuracy 84.940%\n",
      "Epoch 21, Batch 803, LR 0.000047 Loss 5.649541, Accuracy 84.940%\n",
      "Epoch 21, Batch 804, LR 0.000047 Loss 5.650477, Accuracy 84.936%\n",
      "Epoch 21, Batch 805, LR 0.000047 Loss 5.650720, Accuracy 84.932%\n",
      "Epoch 21, Batch 806, LR 0.000047 Loss 5.649804, Accuracy 84.939%\n",
      "Epoch 21, Batch 807, LR 0.000047 Loss 5.649071, Accuracy 84.942%\n",
      "Epoch 21, Batch 808, LR 0.000047 Loss 5.649440, Accuracy 84.940%\n",
      "Epoch 21, Batch 809, LR 0.000047 Loss 5.649085, Accuracy 84.936%\n",
      "Epoch 21, Batch 810, LR 0.000047 Loss 5.649696, Accuracy 84.937%\n",
      "Epoch 21, Batch 811, LR 0.000047 Loss 5.650244, Accuracy 84.937%\n",
      "Epoch 21, Batch 812, LR 0.000047 Loss 5.651014, Accuracy 84.935%\n",
      "Epoch 21, Batch 813, LR 0.000047 Loss 5.650754, Accuracy 84.939%\n",
      "Epoch 21, Batch 814, LR 0.000047 Loss 5.650690, Accuracy 84.941%\n",
      "Epoch 21, Batch 815, LR 0.000047 Loss 5.651419, Accuracy 84.938%\n",
      "Epoch 21, Batch 816, LR 0.000047 Loss 5.652167, Accuracy 84.934%\n",
      "Epoch 21, Batch 817, LR 0.000047 Loss 5.652306, Accuracy 84.932%\n",
      "Epoch 21, Batch 818, LR 0.000047 Loss 5.652284, Accuracy 84.933%\n",
      "Epoch 21, Batch 819, LR 0.000047 Loss 5.652528, Accuracy 84.930%\n",
      "Epoch 21, Batch 820, LR 0.000047 Loss 5.652571, Accuracy 84.923%\n",
      "Epoch 21, Batch 821, LR 0.000047 Loss 5.652653, Accuracy 84.921%\n",
      "Epoch 21, Batch 822, LR 0.000047 Loss 5.651712, Accuracy 84.924%\n",
      "Epoch 21, Batch 823, LR 0.000047 Loss 5.651829, Accuracy 84.924%\n",
      "Epoch 21, Batch 824, LR 0.000047 Loss 5.652268, Accuracy 84.922%\n",
      "Epoch 21, Batch 825, LR 0.000047 Loss 5.651934, Accuracy 84.926%\n",
      "Epoch 21, Batch 826, LR 0.000047 Loss 5.652113, Accuracy 84.926%\n",
      "Epoch 21, Batch 827, LR 0.000047 Loss 5.652668, Accuracy 84.922%\n",
      "Epoch 21, Batch 828, LR 0.000047 Loss 5.653151, Accuracy 84.921%\n",
      "Epoch 21, Batch 829, LR 0.000047 Loss 5.653436, Accuracy 84.921%\n",
      "Epoch 21, Batch 830, LR 0.000047 Loss 5.653758, Accuracy 84.922%\n",
      "Epoch 21, Batch 831, LR 0.000047 Loss 5.653135, Accuracy 84.927%\n",
      "Epoch 21, Batch 832, LR 0.000047 Loss 5.651736, Accuracy 84.935%\n",
      "Epoch 21, Batch 833, LR 0.000047 Loss 5.652367, Accuracy 84.931%\n",
      "Epoch 21, Batch 834, LR 0.000047 Loss 5.653055, Accuracy 84.927%\n",
      "Epoch 21, Batch 835, LR 0.000047 Loss 5.652351, Accuracy 84.930%\n",
      "Epoch 21, Batch 836, LR 0.000047 Loss 5.651923, Accuracy 84.931%\n",
      "Epoch 21, Batch 837, LR 0.000047 Loss 5.652524, Accuracy 84.927%\n",
      "Epoch 21, Batch 838, LR 0.000047 Loss 5.652998, Accuracy 84.926%\n",
      "Epoch 21, Batch 839, LR 0.000047 Loss 5.653156, Accuracy 84.927%\n",
      "Epoch 21, Batch 840, LR 0.000047 Loss 5.653440, Accuracy 84.927%\n",
      "Epoch 21, Batch 841, LR 0.000047 Loss 5.653125, Accuracy 84.926%\n",
      "Epoch 21, Batch 842, LR 0.000047 Loss 5.654199, Accuracy 84.920%\n",
      "Epoch 21, Batch 843, LR 0.000047 Loss 5.653712, Accuracy 84.923%\n",
      "Epoch 21, Batch 844, LR 0.000047 Loss 5.652993, Accuracy 84.927%\n",
      "Epoch 21, Batch 845, LR 0.000047 Loss 5.654444, Accuracy 84.924%\n",
      "Epoch 21, Batch 846, LR 0.000047 Loss 5.654461, Accuracy 84.923%\n",
      "Epoch 21, Batch 847, LR 0.000047 Loss 5.655104, Accuracy 84.921%\n",
      "Epoch 21, Batch 848, LR 0.000047 Loss 5.655326, Accuracy 84.916%\n",
      "Epoch 21, Batch 849, LR 0.000047 Loss 5.655970, Accuracy 84.911%\n",
      "Epoch 21, Batch 850, LR 0.000047 Loss 5.656525, Accuracy 84.909%\n",
      "Epoch 21, Batch 851, LR 0.000047 Loss 5.656747, Accuracy 84.908%\n",
      "Epoch 21, Batch 852, LR 0.000047 Loss 5.657745, Accuracy 84.900%\n",
      "Epoch 21, Batch 853, LR 0.000047 Loss 5.656273, Accuracy 84.905%\n",
      "Epoch 21, Batch 854, LR 0.000047 Loss 5.656023, Accuracy 84.908%\n",
      "Epoch 21, Batch 855, LR 0.000047 Loss 5.656246, Accuracy 84.907%\n",
      "Epoch 21, Batch 856, LR 0.000047 Loss 5.656082, Accuracy 84.905%\n",
      "Epoch 21, Batch 857, LR 0.000047 Loss 5.655183, Accuracy 84.911%\n",
      "Epoch 21, Batch 858, LR 0.000047 Loss 5.655536, Accuracy 84.912%\n",
      "Epoch 21, Batch 859, LR 0.000047 Loss 5.654913, Accuracy 84.916%\n",
      "Epoch 21, Batch 860, LR 0.000047 Loss 5.654133, Accuracy 84.921%\n",
      "Epoch 21, Batch 861, LR 0.000047 Loss 5.653808, Accuracy 84.926%\n",
      "Epoch 21, Batch 862, LR 0.000047 Loss 5.653417, Accuracy 84.926%\n",
      "Epoch 21, Batch 863, LR 0.000047 Loss 5.654312, Accuracy 84.919%\n",
      "Epoch 21, Batch 864, LR 0.000047 Loss 5.654171, Accuracy 84.921%\n",
      "Epoch 21, Batch 865, LR 0.000047 Loss 5.654523, Accuracy 84.915%\n",
      "Epoch 21, Batch 866, LR 0.000047 Loss 5.655751, Accuracy 84.907%\n",
      "Epoch 21, Batch 867, LR 0.000047 Loss 5.655647, Accuracy 84.907%\n",
      "Epoch 21, Batch 868, LR 0.000047 Loss 5.656655, Accuracy 84.904%\n",
      "Epoch 21, Batch 869, LR 0.000047 Loss 5.656579, Accuracy 84.906%\n",
      "Epoch 21, Batch 870, LR 0.000047 Loss 5.657489, Accuracy 84.902%\n",
      "Epoch 21, Batch 871, LR 0.000047 Loss 5.659070, Accuracy 84.892%\n",
      "Epoch 21, Batch 872, LR 0.000047 Loss 5.658007, Accuracy 84.900%\n",
      "Epoch 21, Batch 873, LR 0.000047 Loss 5.658292, Accuracy 84.901%\n",
      "Epoch 21, Batch 874, LR 0.000047 Loss 5.659165, Accuracy 84.898%\n",
      "Epoch 21, Batch 875, LR 0.000047 Loss 5.658922, Accuracy 84.901%\n",
      "Epoch 21, Batch 876, LR 0.000047 Loss 5.659372, Accuracy 84.897%\n",
      "Epoch 21, Batch 877, LR 0.000047 Loss 5.659335, Accuracy 84.898%\n",
      "Epoch 21, Batch 878, LR 0.000047 Loss 5.659168, Accuracy 84.896%\n",
      "Epoch 21, Batch 879, LR 0.000047 Loss 5.657961, Accuracy 84.901%\n",
      "Epoch 21, Batch 880, LR 0.000047 Loss 5.658768, Accuracy 84.897%\n",
      "Epoch 21, Batch 881, LR 0.000047 Loss 5.658020, Accuracy 84.899%\n",
      "Epoch 21, Batch 882, LR 0.000047 Loss 5.657732, Accuracy 84.898%\n",
      "Epoch 21, Batch 883, LR 0.000047 Loss 5.658208, Accuracy 84.898%\n",
      "Epoch 21, Batch 884, LR 0.000047 Loss 5.657526, Accuracy 84.900%\n",
      "Epoch 21, Batch 885, LR 0.000047 Loss 5.658092, Accuracy 84.901%\n",
      "Epoch 21, Batch 886, LR 0.000047 Loss 5.657658, Accuracy 84.904%\n",
      "Epoch 21, Batch 887, LR 0.000047 Loss 5.658072, Accuracy 84.904%\n",
      "Epoch 21, Batch 888, LR 0.000046 Loss 5.658292, Accuracy 84.901%\n",
      "Epoch 21, Batch 889, LR 0.000046 Loss 5.658407, Accuracy 84.898%\n",
      "Epoch 21, Batch 890, LR 0.000046 Loss 5.658404, Accuracy 84.897%\n",
      "Epoch 21, Batch 891, LR 0.000046 Loss 5.658766, Accuracy 84.893%\n",
      "Epoch 21, Batch 892, LR 0.000046 Loss 5.659129, Accuracy 84.892%\n",
      "Epoch 21, Batch 893, LR 0.000046 Loss 5.658154, Accuracy 84.896%\n",
      "Epoch 21, Batch 894, LR 0.000046 Loss 5.657870, Accuracy 84.892%\n",
      "Epoch 21, Batch 895, LR 0.000046 Loss 5.658219, Accuracy 84.893%\n",
      "Epoch 21, Batch 896, LR 0.000046 Loss 5.658745, Accuracy 84.889%\n",
      "Epoch 21, Batch 897, LR 0.000046 Loss 5.658183, Accuracy 84.890%\n",
      "Epoch 21, Batch 898, LR 0.000046 Loss 5.657411, Accuracy 84.894%\n",
      "Epoch 21, Batch 899, LR 0.000046 Loss 5.657672, Accuracy 84.893%\n",
      "Epoch 21, Batch 900, LR 0.000046 Loss 5.658470, Accuracy 84.891%\n",
      "Epoch 21, Batch 901, LR 0.000046 Loss 5.659227, Accuracy 84.888%\n",
      "Epoch 21, Batch 902, LR 0.000046 Loss 5.660143, Accuracy 84.883%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 903, LR 0.000046 Loss 5.659342, Accuracy 84.884%\n",
      "Epoch 21, Batch 904, LR 0.000046 Loss 5.659843, Accuracy 84.881%\n",
      "Epoch 21, Batch 905, LR 0.000046 Loss 5.659794, Accuracy 84.882%\n",
      "Epoch 21, Batch 906, LR 0.000046 Loss 5.659999, Accuracy 84.884%\n",
      "Epoch 21, Batch 907, LR 0.000046 Loss 5.659760, Accuracy 84.888%\n",
      "Epoch 21, Batch 908, LR 0.000046 Loss 5.659585, Accuracy 84.890%\n",
      "Epoch 21, Batch 909, LR 0.000046 Loss 5.660092, Accuracy 84.885%\n",
      "Epoch 21, Batch 910, LR 0.000046 Loss 5.660362, Accuracy 84.882%\n",
      "Epoch 21, Batch 911, LR 0.000046 Loss 5.660581, Accuracy 84.885%\n",
      "Epoch 21, Batch 912, LR 0.000046 Loss 5.660130, Accuracy 84.889%\n",
      "Epoch 21, Batch 913, LR 0.000046 Loss 5.660062, Accuracy 84.890%\n",
      "Epoch 21, Batch 914, LR 0.000046 Loss 5.660298, Accuracy 84.887%\n",
      "Epoch 21, Batch 915, LR 0.000046 Loss 5.660419, Accuracy 84.892%\n",
      "Epoch 21, Batch 916, LR 0.000046 Loss 5.659612, Accuracy 84.894%\n",
      "Epoch 21, Batch 917, LR 0.000046 Loss 5.659383, Accuracy 84.895%\n",
      "Epoch 21, Batch 918, LR 0.000046 Loss 5.659674, Accuracy 84.893%\n",
      "Epoch 21, Batch 919, LR 0.000046 Loss 5.659400, Accuracy 84.895%\n",
      "Epoch 21, Batch 920, LR 0.000046 Loss 5.659736, Accuracy 84.893%\n",
      "Epoch 21, Batch 921, LR 0.000046 Loss 5.659719, Accuracy 84.891%\n",
      "Epoch 21, Batch 922, LR 0.000046 Loss 5.659466, Accuracy 84.894%\n",
      "Epoch 21, Batch 923, LR 0.000046 Loss 5.659498, Accuracy 84.892%\n",
      "Epoch 21, Batch 924, LR 0.000046 Loss 5.659434, Accuracy 84.896%\n",
      "Epoch 21, Batch 925, LR 0.000046 Loss 5.659271, Accuracy 84.899%\n",
      "Epoch 21, Batch 926, LR 0.000046 Loss 5.659322, Accuracy 84.899%\n",
      "Epoch 21, Batch 927, LR 0.000046 Loss 5.659445, Accuracy 84.903%\n",
      "Epoch 21, Batch 928, LR 0.000046 Loss 5.658803, Accuracy 84.904%\n",
      "Epoch 21, Batch 929, LR 0.000046 Loss 5.658515, Accuracy 84.907%\n",
      "Epoch 21, Batch 930, LR 0.000046 Loss 5.658279, Accuracy 84.907%\n",
      "Epoch 21, Batch 931, LR 0.000046 Loss 5.658321, Accuracy 84.909%\n",
      "Epoch 21, Batch 932, LR 0.000046 Loss 5.658090, Accuracy 84.909%\n",
      "Epoch 21, Batch 933, LR 0.000046 Loss 5.658718, Accuracy 84.907%\n",
      "Epoch 21, Batch 934, LR 0.000046 Loss 5.658937, Accuracy 84.904%\n",
      "Epoch 21, Batch 935, LR 0.000046 Loss 5.658602, Accuracy 84.907%\n",
      "Epoch 21, Batch 936, LR 0.000046 Loss 5.658288, Accuracy 84.911%\n",
      "Epoch 21, Batch 937, LR 0.000046 Loss 5.657908, Accuracy 84.916%\n",
      "Epoch 21, Batch 938, LR 0.000046 Loss 5.658181, Accuracy 84.916%\n",
      "Epoch 21, Batch 939, LR 0.000046 Loss 5.658705, Accuracy 84.912%\n",
      "Epoch 21, Batch 940, LR 0.000046 Loss 5.658195, Accuracy 84.914%\n",
      "Epoch 21, Batch 941, LR 0.000046 Loss 5.658649, Accuracy 84.911%\n",
      "Epoch 21, Batch 942, LR 0.000046 Loss 5.659434, Accuracy 84.912%\n",
      "Epoch 21, Batch 943, LR 0.000046 Loss 5.659907, Accuracy 84.913%\n",
      "Epoch 21, Batch 944, LR 0.000046 Loss 5.660109, Accuracy 84.907%\n",
      "Epoch 21, Batch 945, LR 0.000046 Loss 5.660842, Accuracy 84.908%\n",
      "Epoch 21, Batch 946, LR 0.000046 Loss 5.661391, Accuracy 84.905%\n",
      "Epoch 21, Batch 947, LR 0.000046 Loss 5.661038, Accuracy 84.905%\n",
      "Epoch 21, Batch 948, LR 0.000046 Loss 5.661099, Accuracy 84.908%\n",
      "Epoch 21, Batch 949, LR 0.000046 Loss 5.661037, Accuracy 84.907%\n",
      "Epoch 21, Batch 950, LR 0.000046 Loss 5.661078, Accuracy 84.903%\n",
      "Epoch 21, Batch 951, LR 0.000046 Loss 5.661524, Accuracy 84.898%\n",
      "Epoch 21, Batch 952, LR 0.000046 Loss 5.661352, Accuracy 84.896%\n",
      "Epoch 21, Batch 953, LR 0.000046 Loss 5.660656, Accuracy 84.897%\n",
      "Epoch 21, Batch 954, LR 0.000046 Loss 5.660837, Accuracy 84.897%\n",
      "Epoch 21, Batch 955, LR 0.000046 Loss 5.660536, Accuracy 84.894%\n",
      "Epoch 21, Batch 956, LR 0.000046 Loss 5.659928, Accuracy 84.898%\n",
      "Epoch 21, Batch 957, LR 0.000046 Loss 5.659765, Accuracy 84.902%\n",
      "Epoch 21, Batch 958, LR 0.000046 Loss 5.659717, Accuracy 84.903%\n",
      "Epoch 21, Batch 959, LR 0.000046 Loss 5.659905, Accuracy 84.900%\n",
      "Epoch 21, Batch 960, LR 0.000046 Loss 5.660290, Accuracy 84.894%\n",
      "Epoch 21, Batch 961, LR 0.000046 Loss 5.660874, Accuracy 84.889%\n",
      "Epoch 21, Batch 962, LR 0.000046 Loss 5.660739, Accuracy 84.892%\n",
      "Epoch 21, Batch 963, LR 0.000046 Loss 5.659864, Accuracy 84.895%\n",
      "Epoch 21, Batch 964, LR 0.000046 Loss 5.660164, Accuracy 84.891%\n",
      "Epoch 21, Batch 965, LR 0.000046 Loss 5.659427, Accuracy 84.894%\n",
      "Epoch 21, Batch 966, LR 0.000046 Loss 5.659797, Accuracy 84.889%\n",
      "Epoch 21, Batch 967, LR 0.000046 Loss 5.659654, Accuracy 84.887%\n",
      "Epoch 21, Batch 968, LR 0.000046 Loss 5.659700, Accuracy 84.882%\n",
      "Epoch 21, Batch 969, LR 0.000046 Loss 5.659183, Accuracy 84.884%\n",
      "Epoch 21, Batch 970, LR 0.000046 Loss 5.660280, Accuracy 84.881%\n",
      "Epoch 21, Batch 971, LR 0.000046 Loss 5.660820, Accuracy 84.878%\n",
      "Epoch 21, Batch 972, LR 0.000046 Loss 5.660239, Accuracy 84.879%\n",
      "Epoch 21, Batch 973, LR 0.000046 Loss 5.660583, Accuracy 84.876%\n",
      "Epoch 21, Batch 974, LR 0.000046 Loss 5.660747, Accuracy 84.874%\n",
      "Epoch 21, Batch 975, LR 0.000046 Loss 5.660147, Accuracy 84.876%\n",
      "Epoch 21, Batch 976, LR 0.000046 Loss 5.659667, Accuracy 84.882%\n",
      "Epoch 21, Batch 977, LR 0.000046 Loss 5.659625, Accuracy 84.881%\n",
      "Epoch 21, Batch 978, LR 0.000046 Loss 5.660078, Accuracy 84.877%\n",
      "Epoch 21, Batch 979, LR 0.000046 Loss 5.660369, Accuracy 84.875%\n",
      "Epoch 21, Batch 980, LR 0.000046 Loss 5.660242, Accuracy 84.878%\n",
      "Epoch 21, Batch 981, LR 0.000046 Loss 5.660498, Accuracy 84.877%\n",
      "Epoch 21, Batch 982, LR 0.000046 Loss 5.660437, Accuracy 84.878%\n",
      "Epoch 21, Batch 983, LR 0.000046 Loss 5.660314, Accuracy 84.879%\n",
      "Epoch 21, Batch 984, LR 0.000046 Loss 5.659933, Accuracy 84.875%\n",
      "Epoch 21, Batch 985, LR 0.000046 Loss 5.660337, Accuracy 84.872%\n",
      "Epoch 21, Batch 986, LR 0.000046 Loss 5.659535, Accuracy 84.873%\n",
      "Epoch 21, Batch 987, LR 0.000046 Loss 5.659468, Accuracy 84.873%\n",
      "Epoch 21, Batch 988, LR 0.000046 Loss 5.659077, Accuracy 84.876%\n",
      "Epoch 21, Batch 989, LR 0.000046 Loss 5.659545, Accuracy 84.874%\n",
      "Epoch 21, Batch 990, LR 0.000046 Loss 5.659351, Accuracy 84.877%\n",
      "Epoch 21, Batch 991, LR 0.000046 Loss 5.659067, Accuracy 84.883%\n",
      "Epoch 21, Batch 992, LR 0.000046 Loss 5.658469, Accuracy 84.881%\n",
      "Epoch 21, Batch 993, LR 0.000046 Loss 5.658587, Accuracy 84.883%\n",
      "Epoch 21, Batch 994, LR 0.000046 Loss 5.658478, Accuracy 84.884%\n",
      "Epoch 21, Batch 995, LR 0.000046 Loss 5.659284, Accuracy 84.877%\n",
      "Epoch 21, Batch 996, LR 0.000046 Loss 5.658899, Accuracy 84.875%\n",
      "Epoch 21, Batch 997, LR 0.000046 Loss 5.659489, Accuracy 84.869%\n",
      "Epoch 21, Batch 998, LR 0.000046 Loss 5.660029, Accuracy 84.867%\n",
      "Epoch 21, Batch 999, LR 0.000046 Loss 5.660429, Accuracy 84.861%\n",
      "Epoch 21, Batch 1000, LR 0.000046 Loss 5.660872, Accuracy 84.860%\n",
      "Epoch 21, Batch 1001, LR 0.000046 Loss 5.660650, Accuracy 84.858%\n",
      "Epoch 21, Batch 1002, LR 0.000046 Loss 5.660555, Accuracy 84.858%\n",
      "Epoch 21, Batch 1003, LR 0.000046 Loss 5.661140, Accuracy 84.857%\n",
      "Epoch 21, Batch 1004, LR 0.000046 Loss 5.661487, Accuracy 84.854%\n",
      "Epoch 21, Batch 1005, LR 0.000046 Loss 5.661375, Accuracy 84.855%\n",
      "Epoch 21, Batch 1006, LR 0.000046 Loss 5.660480, Accuracy 84.859%\n",
      "Epoch 21, Batch 1007, LR 0.000046 Loss 5.660280, Accuracy 84.861%\n",
      "Epoch 21, Batch 1008, LR 0.000046 Loss 5.659799, Accuracy 84.862%\n",
      "Epoch 21, Batch 1009, LR 0.000046 Loss 5.659625, Accuracy 84.864%\n",
      "Epoch 21, Batch 1010, LR 0.000046 Loss 5.659136, Accuracy 84.866%\n",
      "Epoch 21, Batch 1011, LR 0.000046 Loss 5.659126, Accuracy 84.866%\n",
      "Epoch 21, Batch 1012, LR 0.000046 Loss 5.659033, Accuracy 84.864%\n",
      "Epoch 21, Batch 1013, LR 0.000046 Loss 5.659244, Accuracy 84.864%\n",
      "Epoch 21, Batch 1014, LR 0.000046 Loss 5.658471, Accuracy 84.868%\n",
      "Epoch 21, Batch 1015, LR 0.000046 Loss 5.658241, Accuracy 84.868%\n",
      "Epoch 21, Batch 1016, LR 0.000046 Loss 5.657693, Accuracy 84.871%\n",
      "Epoch 21, Batch 1017, LR 0.000046 Loss 5.658261, Accuracy 84.867%\n",
      "Epoch 21, Batch 1018, LR 0.000046 Loss 5.658196, Accuracy 84.864%\n",
      "Epoch 21, Batch 1019, LR 0.000046 Loss 5.658852, Accuracy 84.861%\n",
      "Epoch 21, Batch 1020, LR 0.000046 Loss 5.659477, Accuracy 84.861%\n",
      "Epoch 21, Batch 1021, LR 0.000046 Loss 5.658810, Accuracy 84.860%\n",
      "Epoch 21, Batch 1022, LR 0.000046 Loss 5.658792, Accuracy 84.865%\n",
      "Epoch 21, Batch 1023, LR 0.000046 Loss 5.658491, Accuracy 84.867%\n",
      "Epoch 21, Batch 1024, LR 0.000046 Loss 5.658684, Accuracy 84.866%\n",
      "Epoch 21, Batch 1025, LR 0.000046 Loss 5.658330, Accuracy 84.867%\n",
      "Epoch 21, Batch 1026, LR 0.000046 Loss 5.657692, Accuracy 84.870%\n",
      "Epoch 21, Batch 1027, LR 0.000046 Loss 5.657734, Accuracy 84.872%\n",
      "Epoch 21, Batch 1028, LR 0.000046 Loss 5.657836, Accuracy 84.869%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 1029, LR 0.000046 Loss 5.657613, Accuracy 84.872%\n",
      "Epoch 21, Batch 1030, LR 0.000046 Loss 5.657868, Accuracy 84.869%\n",
      "Epoch 21, Batch 1031, LR 0.000046 Loss 5.657601, Accuracy 84.870%\n",
      "Epoch 21, Batch 1032, LR 0.000046 Loss 5.657203, Accuracy 84.873%\n",
      "Epoch 21, Batch 1033, LR 0.000046 Loss 5.656938, Accuracy 84.873%\n",
      "Epoch 21, Batch 1034, LR 0.000046 Loss 5.656758, Accuracy 84.873%\n",
      "Epoch 21, Batch 1035, LR 0.000046 Loss 5.656686, Accuracy 84.873%\n",
      "Epoch 21, Batch 1036, LR 0.000046 Loss 5.656675, Accuracy 84.873%\n",
      "Epoch 21, Batch 1037, LR 0.000046 Loss 5.656661, Accuracy 84.875%\n",
      "Epoch 21, Batch 1038, LR 0.000046 Loss 5.656001, Accuracy 84.879%\n",
      "Epoch 21, Batch 1039, LR 0.000046 Loss 5.656004, Accuracy 84.877%\n",
      "Epoch 21, Batch 1040, LR 0.000046 Loss 5.655706, Accuracy 84.876%\n",
      "Epoch 21, Batch 1041, LR 0.000046 Loss 5.655853, Accuracy 84.875%\n",
      "Epoch 21, Batch 1042, LR 0.000046 Loss 5.655688, Accuracy 84.876%\n",
      "Epoch 21, Batch 1043, LR 0.000046 Loss 5.656272, Accuracy 84.870%\n",
      "Epoch 21, Batch 1044, LR 0.000046 Loss 5.655492, Accuracy 84.878%\n",
      "Epoch 21, Batch 1045, LR 0.000046 Loss 5.655404, Accuracy 84.879%\n",
      "Epoch 21, Batch 1046, LR 0.000046 Loss 5.655559, Accuracy 84.878%\n",
      "Epoch 21, Batch 1047, LR 0.000046 Loss 5.654462, Accuracy 84.880%\n",
      "Epoch 21, Loss (train set) 5.654462, Accuracy (train set) 84.880%\n",
      "Epoch 22, Batch 1, LR 0.000046 Loss 5.530691, Accuracy 86.719%\n",
      "Epoch 22, Batch 2, LR 0.000046 Loss 5.824777, Accuracy 83.984%\n",
      "Epoch 22, Batch 3, LR 0.000046 Loss 6.111403, Accuracy 82.812%\n",
      "Epoch 22, Batch 4, LR 0.000046 Loss 5.936479, Accuracy 83.398%\n",
      "Epoch 22, Batch 5, LR 0.000046 Loss 5.914551, Accuracy 83.281%\n",
      "Epoch 22, Batch 6, LR 0.000046 Loss 5.773967, Accuracy 84.375%\n",
      "Epoch 22, Batch 7, LR 0.000046 Loss 5.624771, Accuracy 85.268%\n",
      "Epoch 22, Batch 8, LR 0.000046 Loss 5.634251, Accuracy 85.547%\n",
      "Epoch 22, Batch 9, LR 0.000046 Loss 5.669652, Accuracy 85.243%\n",
      "Epoch 22, Batch 10, LR 0.000046 Loss 5.654194, Accuracy 85.234%\n",
      "Epoch 22, Batch 11, LR 0.000046 Loss 5.663147, Accuracy 85.156%\n",
      "Epoch 22, Batch 12, LR 0.000046 Loss 5.594806, Accuracy 85.352%\n",
      "Epoch 22, Batch 13, LR 0.000046 Loss 5.590038, Accuracy 85.096%\n",
      "Epoch 22, Batch 14, LR 0.000046 Loss 5.647741, Accuracy 84.766%\n",
      "Epoch 22, Batch 15, LR 0.000046 Loss 5.677247, Accuracy 84.688%\n",
      "Epoch 22, Batch 16, LR 0.000046 Loss 5.685639, Accuracy 84.668%\n",
      "Epoch 22, Batch 17, LR 0.000046 Loss 5.677091, Accuracy 84.926%\n",
      "Epoch 22, Batch 18, LR 0.000046 Loss 5.649081, Accuracy 84.939%\n",
      "Epoch 22, Batch 19, LR 0.000046 Loss 5.589995, Accuracy 85.280%\n",
      "Epoch 22, Batch 20, LR 0.000046 Loss 5.578264, Accuracy 85.430%\n",
      "Epoch 22, Batch 21, LR 0.000046 Loss 5.593902, Accuracy 85.417%\n",
      "Epoch 22, Batch 22, LR 0.000046 Loss 5.560349, Accuracy 85.582%\n",
      "Epoch 22, Batch 23, LR 0.000046 Loss 5.545876, Accuracy 85.836%\n",
      "Epoch 22, Batch 24, LR 0.000046 Loss 5.554440, Accuracy 85.742%\n",
      "Epoch 22, Batch 25, LR 0.000046 Loss 5.562734, Accuracy 85.562%\n",
      "Epoch 22, Batch 26, LR 0.000046 Loss 5.543244, Accuracy 85.637%\n",
      "Epoch 22, Batch 27, LR 0.000046 Loss 5.522750, Accuracy 85.764%\n",
      "Epoch 22, Batch 28, LR 0.000046 Loss 5.514121, Accuracy 85.798%\n",
      "Epoch 22, Batch 29, LR 0.000046 Loss 5.523114, Accuracy 85.614%\n",
      "Epoch 22, Batch 30, LR 0.000046 Loss 5.524316, Accuracy 85.469%\n",
      "Epoch 22, Batch 31, LR 0.000046 Loss 5.534634, Accuracy 85.358%\n",
      "Epoch 22, Batch 32, LR 0.000046 Loss 5.504294, Accuracy 85.498%\n",
      "Epoch 22, Batch 33, LR 0.000046 Loss 5.507665, Accuracy 85.630%\n",
      "Epoch 22, Batch 34, LR 0.000046 Loss 5.518180, Accuracy 85.386%\n",
      "Epoch 22, Batch 35, LR 0.000046 Loss 5.548587, Accuracy 85.223%\n",
      "Epoch 22, Batch 36, LR 0.000046 Loss 5.555859, Accuracy 85.178%\n",
      "Epoch 22, Batch 37, LR 0.000046 Loss 5.544215, Accuracy 85.220%\n",
      "Epoch 22, Batch 38, LR 0.000046 Loss 5.527387, Accuracy 85.280%\n",
      "Epoch 22, Batch 39, LR 0.000046 Loss 5.519018, Accuracy 85.417%\n",
      "Epoch 22, Batch 40, LR 0.000046 Loss 5.525599, Accuracy 85.312%\n",
      "Epoch 22, Batch 41, LR 0.000046 Loss 5.542315, Accuracy 85.232%\n",
      "Epoch 22, Batch 42, LR 0.000046 Loss 5.527651, Accuracy 85.324%\n",
      "Epoch 22, Batch 43, LR 0.000046 Loss 5.541545, Accuracy 85.247%\n",
      "Epoch 22, Batch 44, LR 0.000046 Loss 5.548589, Accuracy 85.227%\n",
      "Epoch 22, Batch 45, LR 0.000046 Loss 5.541341, Accuracy 85.260%\n",
      "Epoch 22, Batch 46, LR 0.000046 Loss 5.540592, Accuracy 85.258%\n",
      "Epoch 22, Batch 47, LR 0.000046 Loss 5.532979, Accuracy 85.289%\n",
      "Epoch 22, Batch 48, LR 0.000046 Loss 5.530214, Accuracy 85.254%\n",
      "Epoch 22, Batch 49, LR 0.000046 Loss 5.533284, Accuracy 85.252%\n",
      "Epoch 22, Batch 50, LR 0.000046 Loss 5.517517, Accuracy 85.375%\n",
      "Epoch 22, Batch 51, LR 0.000046 Loss 5.524877, Accuracy 85.340%\n",
      "Epoch 22, Batch 52, LR 0.000046 Loss 5.512770, Accuracy 85.472%\n",
      "Epoch 22, Batch 53, LR 0.000046 Loss 5.514891, Accuracy 85.510%\n",
      "Epoch 22, Batch 54, LR 0.000046 Loss 5.519992, Accuracy 85.489%\n",
      "Epoch 22, Batch 55, LR 0.000046 Loss 5.513847, Accuracy 85.497%\n",
      "Epoch 22, Batch 56, LR 0.000046 Loss 5.513560, Accuracy 85.477%\n",
      "Epoch 22, Batch 57, LR 0.000046 Loss 5.519660, Accuracy 85.444%\n",
      "Epoch 22, Batch 58, LR 0.000046 Loss 5.527628, Accuracy 85.439%\n",
      "Epoch 22, Batch 59, LR 0.000046 Loss 5.534165, Accuracy 85.474%\n",
      "Epoch 22, Batch 60, LR 0.000046 Loss 5.535168, Accuracy 85.443%\n",
      "Epoch 22, Batch 61, LR 0.000046 Loss 5.541213, Accuracy 85.412%\n",
      "Epoch 22, Batch 62, LR 0.000046 Loss 5.551482, Accuracy 85.345%\n",
      "Epoch 22, Batch 63, LR 0.000046 Loss 5.555003, Accuracy 85.317%\n",
      "Epoch 22, Batch 64, LR 0.000046 Loss 5.546859, Accuracy 85.352%\n",
      "Epoch 22, Batch 65, LR 0.000046 Loss 5.550456, Accuracy 85.312%\n",
      "Epoch 22, Batch 66, LR 0.000046 Loss 5.557520, Accuracy 85.322%\n",
      "Epoch 22, Batch 67, LR 0.000046 Loss 5.551814, Accuracy 85.378%\n",
      "Epoch 22, Batch 68, LR 0.000046 Loss 5.549954, Accuracy 85.409%\n",
      "Epoch 22, Batch 69, LR 0.000046 Loss 5.549262, Accuracy 85.439%\n",
      "Epoch 22, Batch 70, LR 0.000046 Loss 5.537360, Accuracy 85.469%\n",
      "Epoch 22, Batch 71, LR 0.000046 Loss 5.542355, Accuracy 85.530%\n",
      "Epoch 22, Batch 72, LR 0.000046 Loss 5.541725, Accuracy 85.547%\n",
      "Epoch 22, Batch 73, LR 0.000046 Loss 5.533557, Accuracy 85.552%\n",
      "Epoch 22, Batch 74, LR 0.000046 Loss 5.526412, Accuracy 85.600%\n",
      "Epoch 22, Batch 75, LR 0.000046 Loss 5.530107, Accuracy 85.604%\n",
      "Epoch 22, Batch 76, LR 0.000046 Loss 5.533234, Accuracy 85.588%\n",
      "Epoch 22, Batch 77, LR 0.000046 Loss 5.541874, Accuracy 85.562%\n",
      "Epoch 22, Batch 78, LR 0.000046 Loss 5.548374, Accuracy 85.587%\n",
      "Epoch 22, Batch 79, LR 0.000046 Loss 5.545039, Accuracy 85.601%\n",
      "Epoch 22, Batch 80, LR 0.000046 Loss 5.540368, Accuracy 85.576%\n",
      "Epoch 22, Batch 81, LR 0.000046 Loss 5.541599, Accuracy 85.581%\n",
      "Epoch 22, Batch 82, LR 0.000046 Loss 5.541418, Accuracy 85.614%\n",
      "Epoch 22, Batch 83, LR 0.000046 Loss 5.532077, Accuracy 85.655%\n",
      "Epoch 22, Batch 84, LR 0.000046 Loss 5.535047, Accuracy 85.668%\n",
      "Epoch 22, Batch 85, LR 0.000046 Loss 5.547397, Accuracy 85.588%\n",
      "Epoch 22, Batch 86, LR 0.000046 Loss 5.543407, Accuracy 85.638%\n",
      "Epoch 22, Batch 87, LR 0.000046 Loss 5.547992, Accuracy 85.623%\n",
      "Epoch 22, Batch 88, LR 0.000046 Loss 5.554255, Accuracy 85.609%\n",
      "Epoch 22, Batch 89, LR 0.000046 Loss 5.547326, Accuracy 85.683%\n",
      "Epoch 22, Batch 90, LR 0.000046 Loss 5.551959, Accuracy 85.651%\n",
      "Epoch 22, Batch 91, LR 0.000046 Loss 5.546431, Accuracy 85.671%\n",
      "Epoch 22, Batch 92, LR 0.000046 Loss 5.549356, Accuracy 85.674%\n",
      "Epoch 22, Batch 93, LR 0.000046 Loss 5.545093, Accuracy 85.677%\n",
      "Epoch 22, Batch 94, LR 0.000046 Loss 5.540773, Accuracy 85.688%\n",
      "Epoch 22, Batch 95, LR 0.000045 Loss 5.534744, Accuracy 85.707%\n",
      "Epoch 22, Batch 96, LR 0.000045 Loss 5.536381, Accuracy 85.701%\n",
      "Epoch 22, Batch 97, LR 0.000045 Loss 5.544250, Accuracy 85.704%\n",
      "Epoch 22, Batch 98, LR 0.000045 Loss 5.542722, Accuracy 85.722%\n",
      "Epoch 22, Batch 99, LR 0.000045 Loss 5.536347, Accuracy 85.740%\n",
      "Epoch 22, Batch 100, LR 0.000045 Loss 5.528717, Accuracy 85.797%\n",
      "Epoch 22, Batch 101, LR 0.000045 Loss 5.519164, Accuracy 85.860%\n",
      "Epoch 22, Batch 102, LR 0.000045 Loss 5.527856, Accuracy 85.769%\n",
      "Epoch 22, Batch 103, LR 0.000045 Loss 5.523668, Accuracy 85.793%\n",
      "Epoch 22, Batch 104, LR 0.000045 Loss 5.523011, Accuracy 85.817%\n",
      "Epoch 22, Batch 105, LR 0.000045 Loss 5.519982, Accuracy 85.848%\n",
      "Epoch 22, Batch 106, LR 0.000045 Loss 5.519579, Accuracy 85.856%\n",
      "Epoch 22, Batch 107, LR 0.000045 Loss 5.519699, Accuracy 85.828%\n",
      "Epoch 22, Batch 108, LR 0.000045 Loss 5.516159, Accuracy 85.836%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 109, LR 0.000045 Loss 5.522017, Accuracy 85.844%\n",
      "Epoch 22, Batch 110, LR 0.000045 Loss 5.529586, Accuracy 85.817%\n",
      "Epoch 22, Batch 111, LR 0.000045 Loss 5.529291, Accuracy 85.818%\n",
      "Epoch 22, Batch 112, LR 0.000045 Loss 5.530701, Accuracy 85.784%\n",
      "Epoch 22, Batch 113, LR 0.000045 Loss 5.531389, Accuracy 85.785%\n",
      "Epoch 22, Batch 114, LR 0.000045 Loss 5.537818, Accuracy 85.759%\n",
      "Epoch 22, Batch 115, LR 0.000045 Loss 5.537087, Accuracy 85.754%\n",
      "Epoch 22, Batch 116, LR 0.000045 Loss 5.539838, Accuracy 85.729%\n",
      "Epoch 22, Batch 117, LR 0.000045 Loss 5.536735, Accuracy 85.697%\n",
      "Epoch 22, Batch 118, LR 0.000045 Loss 5.534603, Accuracy 85.726%\n",
      "Epoch 22, Batch 119, LR 0.000045 Loss 5.534135, Accuracy 85.714%\n",
      "Epoch 22, Batch 120, LR 0.000045 Loss 5.532857, Accuracy 85.736%\n",
      "Epoch 22, Batch 121, LR 0.000045 Loss 5.530583, Accuracy 85.731%\n",
      "Epoch 22, Batch 122, LR 0.000045 Loss 5.532739, Accuracy 85.707%\n",
      "Epoch 22, Batch 123, LR 0.000045 Loss 5.534882, Accuracy 85.683%\n",
      "Epoch 22, Batch 124, LR 0.000045 Loss 5.534657, Accuracy 85.641%\n",
      "Epoch 22, Batch 125, LR 0.000045 Loss 5.539503, Accuracy 85.594%\n",
      "Epoch 22, Batch 126, LR 0.000045 Loss 5.541934, Accuracy 85.578%\n",
      "Epoch 22, Batch 127, LR 0.000045 Loss 5.547209, Accuracy 85.544%\n",
      "Epoch 22, Batch 128, LR 0.000045 Loss 5.543824, Accuracy 85.596%\n",
      "Epoch 22, Batch 129, LR 0.000045 Loss 5.546475, Accuracy 85.562%\n",
      "Epoch 22, Batch 130, LR 0.000045 Loss 5.544013, Accuracy 85.589%\n",
      "Epoch 22, Batch 131, LR 0.000045 Loss 5.544875, Accuracy 85.574%\n",
      "Epoch 22, Batch 132, LR 0.000045 Loss 5.543406, Accuracy 85.588%\n",
      "Epoch 22, Batch 133, LR 0.000045 Loss 5.542575, Accuracy 85.609%\n",
      "Epoch 22, Batch 134, LR 0.000045 Loss 5.534588, Accuracy 85.617%\n",
      "Epoch 22, Batch 135, LR 0.000045 Loss 5.531388, Accuracy 85.637%\n",
      "Epoch 22, Batch 136, LR 0.000045 Loss 5.537910, Accuracy 85.633%\n",
      "Epoch 22, Batch 137, LR 0.000045 Loss 5.538244, Accuracy 85.607%\n",
      "Epoch 22, Batch 138, LR 0.000045 Loss 5.538759, Accuracy 85.581%\n",
      "Epoch 22, Batch 139, LR 0.000045 Loss 5.534011, Accuracy 85.606%\n",
      "Epoch 22, Batch 140, LR 0.000045 Loss 5.530141, Accuracy 85.619%\n",
      "Epoch 22, Batch 141, LR 0.000045 Loss 5.524200, Accuracy 85.644%\n",
      "Epoch 22, Batch 142, LR 0.000045 Loss 5.525348, Accuracy 85.629%\n",
      "Epoch 22, Batch 143, LR 0.000045 Loss 5.525069, Accuracy 85.626%\n",
      "Epoch 22, Batch 144, LR 0.000045 Loss 5.526874, Accuracy 85.650%\n",
      "Epoch 22, Batch 145, LR 0.000045 Loss 5.521131, Accuracy 85.663%\n",
      "Epoch 22, Batch 146, LR 0.000045 Loss 5.520855, Accuracy 85.649%\n",
      "Epoch 22, Batch 147, LR 0.000045 Loss 5.522627, Accuracy 85.624%\n",
      "Epoch 22, Batch 148, LR 0.000045 Loss 5.523527, Accuracy 85.631%\n",
      "Epoch 22, Batch 149, LR 0.000045 Loss 5.521978, Accuracy 85.623%\n",
      "Epoch 22, Batch 150, LR 0.000045 Loss 5.530523, Accuracy 85.573%\n",
      "Epoch 22, Batch 151, LR 0.000045 Loss 5.530766, Accuracy 85.575%\n",
      "Epoch 22, Batch 152, LR 0.000045 Loss 5.530708, Accuracy 85.567%\n",
      "Epoch 22, Batch 153, LR 0.000045 Loss 5.529093, Accuracy 85.570%\n",
      "Epoch 22, Batch 154, LR 0.000045 Loss 5.532354, Accuracy 85.557%\n",
      "Epoch 22, Batch 155, LR 0.000045 Loss 5.539432, Accuracy 85.534%\n",
      "Epoch 22, Batch 156, LR 0.000045 Loss 5.540153, Accuracy 85.527%\n",
      "Epoch 22, Batch 157, LR 0.000045 Loss 5.539498, Accuracy 85.549%\n",
      "Epoch 22, Batch 158, LR 0.000045 Loss 5.537969, Accuracy 85.537%\n",
      "Epoch 22, Batch 159, LR 0.000045 Loss 5.540813, Accuracy 85.520%\n",
      "Epoch 22, Batch 160, LR 0.000045 Loss 5.539802, Accuracy 85.527%\n",
      "Epoch 22, Batch 161, LR 0.000045 Loss 5.534939, Accuracy 85.549%\n",
      "Epoch 22, Batch 162, LR 0.000045 Loss 5.531357, Accuracy 85.552%\n",
      "Epoch 22, Batch 163, LR 0.000045 Loss 5.537885, Accuracy 85.530%\n",
      "Epoch 22, Batch 164, LR 0.000045 Loss 5.539073, Accuracy 85.537%\n",
      "Epoch 22, Batch 165, LR 0.000045 Loss 5.538836, Accuracy 85.540%\n",
      "Epoch 22, Batch 166, LR 0.000045 Loss 5.535211, Accuracy 85.542%\n",
      "Epoch 22, Batch 167, LR 0.000045 Loss 5.535221, Accuracy 85.545%\n",
      "Epoch 22, Batch 168, LR 0.000045 Loss 5.538774, Accuracy 85.542%\n",
      "Epoch 22, Batch 169, LR 0.000045 Loss 5.537737, Accuracy 85.549%\n",
      "Epoch 22, Batch 170, LR 0.000045 Loss 5.537862, Accuracy 85.561%\n",
      "Epoch 22, Batch 171, LR 0.000045 Loss 5.542216, Accuracy 85.522%\n",
      "Epoch 22, Batch 172, LR 0.000045 Loss 5.540884, Accuracy 85.529%\n",
      "Epoch 22, Batch 173, LR 0.000045 Loss 5.544189, Accuracy 85.527%\n",
      "Epoch 22, Batch 174, LR 0.000045 Loss 5.544602, Accuracy 85.502%\n",
      "Epoch 22, Batch 175, LR 0.000045 Loss 5.546945, Accuracy 85.500%\n",
      "Epoch 22, Batch 176, LR 0.000045 Loss 5.543262, Accuracy 85.529%\n",
      "Epoch 22, Batch 177, LR 0.000045 Loss 5.541357, Accuracy 85.531%\n",
      "Epoch 22, Batch 178, LR 0.000045 Loss 5.543694, Accuracy 85.481%\n",
      "Epoch 22, Batch 179, LR 0.000045 Loss 5.542726, Accuracy 85.497%\n",
      "Epoch 22, Batch 180, LR 0.000045 Loss 5.542242, Accuracy 85.477%\n",
      "Epoch 22, Batch 181, LR 0.000045 Loss 5.539759, Accuracy 85.480%\n",
      "Epoch 22, Batch 182, LR 0.000045 Loss 5.535009, Accuracy 85.491%\n",
      "Epoch 22, Batch 183, LR 0.000045 Loss 5.538389, Accuracy 85.476%\n",
      "Epoch 22, Batch 184, LR 0.000045 Loss 5.535980, Accuracy 85.492%\n",
      "Epoch 22, Batch 185, LR 0.000045 Loss 5.529383, Accuracy 85.532%\n",
      "Epoch 22, Batch 186, LR 0.000045 Loss 5.531800, Accuracy 85.517%\n",
      "Epoch 22, Batch 187, LR 0.000045 Loss 5.533085, Accuracy 85.528%\n",
      "Epoch 22, Batch 188, LR 0.000045 Loss 5.532028, Accuracy 85.534%\n",
      "Epoch 22, Batch 189, LR 0.000045 Loss 5.531282, Accuracy 85.528%\n",
      "Epoch 22, Batch 190, LR 0.000045 Loss 5.537477, Accuracy 85.493%\n",
      "Epoch 22, Batch 191, LR 0.000045 Loss 5.531857, Accuracy 85.533%\n",
      "Epoch 22, Batch 192, LR 0.000045 Loss 5.535106, Accuracy 85.522%\n",
      "Epoch 22, Batch 193, LR 0.000045 Loss 5.533027, Accuracy 85.537%\n",
      "Epoch 22, Batch 194, LR 0.000045 Loss 5.528834, Accuracy 85.559%\n",
      "Epoch 22, Batch 195, LR 0.000045 Loss 5.530497, Accuracy 85.565%\n",
      "Epoch 22, Batch 196, LR 0.000045 Loss 5.529537, Accuracy 85.555%\n",
      "Epoch 22, Batch 197, LR 0.000045 Loss 5.527777, Accuracy 85.561%\n",
      "Epoch 22, Batch 198, LR 0.000045 Loss 5.529617, Accuracy 85.547%\n",
      "Epoch 22, Batch 199, LR 0.000045 Loss 5.525587, Accuracy 85.565%\n",
      "Epoch 22, Batch 200, LR 0.000045 Loss 5.524156, Accuracy 85.578%\n",
      "Epoch 22, Batch 201, LR 0.000045 Loss 5.520712, Accuracy 85.603%\n",
      "Epoch 22, Batch 202, LR 0.000045 Loss 5.521334, Accuracy 85.586%\n",
      "Epoch 22, Batch 203, LR 0.000045 Loss 5.522907, Accuracy 85.591%\n",
      "Epoch 22, Batch 204, LR 0.000045 Loss 5.520919, Accuracy 85.581%\n",
      "Epoch 22, Batch 205, LR 0.000045 Loss 5.521798, Accuracy 85.572%\n",
      "Epoch 22, Batch 206, LR 0.000045 Loss 5.523042, Accuracy 85.573%\n",
      "Epoch 22, Batch 207, LR 0.000045 Loss 5.524606, Accuracy 85.564%\n",
      "Epoch 22, Batch 208, LR 0.000045 Loss 5.528680, Accuracy 85.524%\n",
      "Epoch 22, Batch 209, LR 0.000045 Loss 5.526461, Accuracy 85.556%\n",
      "Epoch 22, Batch 210, LR 0.000045 Loss 5.525760, Accuracy 85.565%\n",
      "Epoch 22, Batch 211, LR 0.000045 Loss 5.528288, Accuracy 85.527%\n",
      "Epoch 22, Batch 212, LR 0.000045 Loss 5.531789, Accuracy 85.510%\n",
      "Epoch 22, Batch 213, LR 0.000045 Loss 5.530635, Accuracy 85.516%\n",
      "Epoch 22, Batch 214, LR 0.000045 Loss 5.529327, Accuracy 85.529%\n",
      "Epoch 22, Batch 215, LR 0.000045 Loss 5.530455, Accuracy 85.520%\n",
      "Epoch 22, Batch 216, LR 0.000045 Loss 5.527017, Accuracy 85.518%\n",
      "Epoch 22, Batch 217, LR 0.000045 Loss 5.524843, Accuracy 85.541%\n",
      "Epoch 22, Batch 218, LR 0.000045 Loss 5.522409, Accuracy 85.550%\n",
      "Epoch 22, Batch 219, LR 0.000045 Loss 5.520406, Accuracy 85.566%\n",
      "Epoch 22, Batch 220, LR 0.000045 Loss 5.521198, Accuracy 85.558%\n",
      "Epoch 22, Batch 221, LR 0.000045 Loss 5.524230, Accuracy 85.542%\n",
      "Epoch 22, Batch 222, LR 0.000045 Loss 5.524157, Accuracy 85.540%\n",
      "Epoch 22, Batch 223, LR 0.000045 Loss 5.525357, Accuracy 85.535%\n",
      "Epoch 22, Batch 224, LR 0.000045 Loss 5.528961, Accuracy 85.515%\n",
      "Epoch 22, Batch 225, LR 0.000045 Loss 5.527669, Accuracy 85.521%\n",
      "Epoch 22, Batch 226, LR 0.000045 Loss 5.528258, Accuracy 85.502%\n",
      "Epoch 22, Batch 227, LR 0.000045 Loss 5.528551, Accuracy 85.504%\n",
      "Epoch 22, Batch 228, LR 0.000045 Loss 5.529045, Accuracy 85.489%\n",
      "Epoch 22, Batch 229, LR 0.000045 Loss 5.526432, Accuracy 85.508%\n",
      "Epoch 22, Batch 230, LR 0.000045 Loss 5.523724, Accuracy 85.516%\n",
      "Epoch 22, Batch 231, LR 0.000045 Loss 5.521380, Accuracy 85.535%\n",
      "Epoch 22, Batch 232, LR 0.000045 Loss 5.520320, Accuracy 85.550%\n",
      "Epoch 22, Batch 233, LR 0.000045 Loss 5.519243, Accuracy 85.559%\n",
      "Epoch 22, Batch 234, LR 0.000045 Loss 5.521474, Accuracy 85.564%\n",
      "Epoch 22, Batch 235, LR 0.000045 Loss 5.518432, Accuracy 85.565%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 236, LR 0.000045 Loss 5.519389, Accuracy 85.557%\n",
      "Epoch 22, Batch 237, LR 0.000045 Loss 5.520281, Accuracy 85.572%\n",
      "Epoch 22, Batch 238, LR 0.000045 Loss 5.523330, Accuracy 85.527%\n",
      "Epoch 22, Batch 239, LR 0.000045 Loss 5.523981, Accuracy 85.516%\n",
      "Epoch 22, Batch 240, LR 0.000045 Loss 5.520795, Accuracy 85.540%\n",
      "Epoch 22, Batch 241, LR 0.000045 Loss 5.518757, Accuracy 85.542%\n",
      "Epoch 22, Batch 242, LR 0.000045 Loss 5.518334, Accuracy 85.544%\n",
      "Epoch 22, Batch 243, LR 0.000045 Loss 5.515712, Accuracy 85.568%\n",
      "Epoch 22, Batch 244, LR 0.000045 Loss 5.516972, Accuracy 85.550%\n",
      "Epoch 22, Batch 245, LR 0.000045 Loss 5.518367, Accuracy 85.555%\n",
      "Epoch 22, Batch 246, LR 0.000045 Loss 5.520022, Accuracy 85.563%\n",
      "Epoch 22, Batch 247, LR 0.000045 Loss 5.518937, Accuracy 85.564%\n",
      "Epoch 22, Batch 248, LR 0.000045 Loss 5.519122, Accuracy 85.550%\n",
      "Epoch 22, Batch 249, LR 0.000045 Loss 5.521515, Accuracy 85.542%\n",
      "Epoch 22, Batch 250, LR 0.000045 Loss 5.522083, Accuracy 85.541%\n",
      "Epoch 22, Batch 251, LR 0.000045 Loss 5.520994, Accuracy 85.542%\n",
      "Epoch 22, Batch 252, LR 0.000045 Loss 5.519218, Accuracy 85.556%\n",
      "Epoch 22, Batch 253, LR 0.000045 Loss 5.520144, Accuracy 85.567%\n",
      "Epoch 22, Batch 254, LR 0.000045 Loss 5.519720, Accuracy 85.571%\n",
      "Epoch 22, Batch 255, LR 0.000045 Loss 5.517720, Accuracy 85.588%\n",
      "Epoch 22, Batch 256, LR 0.000045 Loss 5.519160, Accuracy 85.580%\n",
      "Epoch 22, Batch 257, LR 0.000045 Loss 5.518825, Accuracy 85.570%\n",
      "Epoch 22, Batch 258, LR 0.000045 Loss 5.518556, Accuracy 85.559%\n",
      "Epoch 22, Batch 259, LR 0.000045 Loss 5.515552, Accuracy 85.579%\n",
      "Epoch 22, Batch 260, LR 0.000045 Loss 5.517435, Accuracy 85.571%\n",
      "Epoch 22, Batch 261, LR 0.000045 Loss 5.515524, Accuracy 85.554%\n",
      "Epoch 22, Batch 262, LR 0.000045 Loss 5.516933, Accuracy 85.550%\n",
      "Epoch 22, Batch 263, LR 0.000045 Loss 5.516137, Accuracy 85.563%\n",
      "Epoch 22, Batch 264, LR 0.000045 Loss 5.515763, Accuracy 85.562%\n",
      "Epoch 22, Batch 265, LR 0.000045 Loss 5.518074, Accuracy 85.557%\n",
      "Epoch 22, Batch 266, LR 0.000045 Loss 5.516863, Accuracy 85.556%\n",
      "Epoch 22, Batch 267, LR 0.000045 Loss 5.515590, Accuracy 85.569%\n",
      "Epoch 22, Batch 268, LR 0.000045 Loss 5.515854, Accuracy 85.582%\n",
      "Epoch 22, Batch 269, LR 0.000045 Loss 5.517817, Accuracy 85.563%\n",
      "Epoch 22, Batch 270, LR 0.000045 Loss 5.519863, Accuracy 85.532%\n",
      "Epoch 22, Batch 271, LR 0.000045 Loss 5.518576, Accuracy 85.525%\n",
      "Epoch 22, Batch 272, LR 0.000045 Loss 5.516805, Accuracy 85.550%\n",
      "Epoch 22, Batch 273, LR 0.000045 Loss 5.515228, Accuracy 85.554%\n",
      "Epoch 22, Batch 274, LR 0.000045 Loss 5.511634, Accuracy 85.564%\n",
      "Epoch 22, Batch 275, LR 0.000045 Loss 5.508268, Accuracy 85.571%\n",
      "Epoch 22, Batch 276, LR 0.000045 Loss 5.508630, Accuracy 85.575%\n",
      "Epoch 22, Batch 277, LR 0.000045 Loss 5.506902, Accuracy 85.585%\n",
      "Epoch 22, Batch 278, LR 0.000045 Loss 5.504521, Accuracy 85.586%\n",
      "Epoch 22, Batch 279, LR 0.000045 Loss 5.508501, Accuracy 85.587%\n",
      "Epoch 22, Batch 280, LR 0.000045 Loss 5.505537, Accuracy 85.600%\n",
      "Epoch 22, Batch 281, LR 0.000045 Loss 5.504292, Accuracy 85.609%\n",
      "Epoch 22, Batch 282, LR 0.000045 Loss 5.504818, Accuracy 85.602%\n",
      "Epoch 22, Batch 283, LR 0.000045 Loss 5.503362, Accuracy 85.617%\n",
      "Epoch 22, Batch 284, LR 0.000045 Loss 5.501931, Accuracy 85.621%\n",
      "Epoch 22, Batch 285, LR 0.000045 Loss 5.502093, Accuracy 85.614%\n",
      "Epoch 22, Batch 286, LR 0.000045 Loss 5.501130, Accuracy 85.612%\n",
      "Epoch 22, Batch 287, LR 0.000045 Loss 5.503287, Accuracy 85.581%\n",
      "Epoch 22, Batch 288, LR 0.000045 Loss 5.504354, Accuracy 85.569%\n",
      "Epoch 22, Batch 289, LR 0.000045 Loss 5.504253, Accuracy 85.567%\n",
      "Epoch 22, Batch 290, LR 0.000045 Loss 5.501956, Accuracy 85.574%\n",
      "Epoch 22, Batch 291, LR 0.000045 Loss 5.502683, Accuracy 85.559%\n",
      "Epoch 22, Batch 292, LR 0.000045 Loss 5.502612, Accuracy 85.555%\n",
      "Epoch 22, Batch 293, LR 0.000045 Loss 5.501364, Accuracy 85.572%\n",
      "Epoch 22, Batch 294, LR 0.000045 Loss 5.500595, Accuracy 85.581%\n",
      "Epoch 22, Batch 295, LR 0.000045 Loss 5.498946, Accuracy 85.593%\n",
      "Epoch 22, Batch 296, LR 0.000045 Loss 5.497983, Accuracy 85.608%\n",
      "Epoch 22, Batch 297, LR 0.000045 Loss 5.497051, Accuracy 85.617%\n",
      "Epoch 22, Batch 298, LR 0.000045 Loss 5.493667, Accuracy 85.631%\n",
      "Epoch 22, Batch 299, LR 0.000045 Loss 5.494015, Accuracy 85.629%\n",
      "Epoch 22, Batch 300, LR 0.000045 Loss 5.493213, Accuracy 85.641%\n",
      "Epoch 22, Batch 301, LR 0.000045 Loss 5.491228, Accuracy 85.639%\n",
      "Epoch 22, Batch 302, LR 0.000045 Loss 5.491171, Accuracy 85.648%\n",
      "Epoch 22, Batch 303, LR 0.000045 Loss 5.492324, Accuracy 85.646%\n",
      "Epoch 22, Batch 304, LR 0.000045 Loss 5.491348, Accuracy 85.639%\n",
      "Epoch 22, Batch 305, LR 0.000045 Loss 5.493068, Accuracy 85.640%\n",
      "Epoch 22, Batch 306, LR 0.000045 Loss 5.491995, Accuracy 85.646%\n",
      "Epoch 22, Batch 307, LR 0.000045 Loss 5.491330, Accuracy 85.652%\n",
      "Epoch 22, Batch 308, LR 0.000045 Loss 5.492727, Accuracy 85.653%\n",
      "Epoch 22, Batch 309, LR 0.000045 Loss 5.492839, Accuracy 85.644%\n",
      "Epoch 22, Batch 310, LR 0.000045 Loss 5.491385, Accuracy 85.660%\n",
      "Epoch 22, Batch 311, LR 0.000045 Loss 5.489864, Accuracy 85.659%\n",
      "Epoch 22, Batch 312, LR 0.000045 Loss 5.492415, Accuracy 85.650%\n",
      "Epoch 22, Batch 313, LR 0.000045 Loss 5.492590, Accuracy 85.645%\n",
      "Epoch 22, Batch 314, LR 0.000045 Loss 5.491533, Accuracy 85.656%\n",
      "Epoch 22, Batch 315, LR 0.000045 Loss 5.491161, Accuracy 85.667%\n",
      "Epoch 22, Batch 316, LR 0.000045 Loss 5.491953, Accuracy 85.666%\n",
      "Epoch 22, Batch 317, LR 0.000045 Loss 5.491622, Accuracy 85.674%\n",
      "Epoch 22, Batch 318, LR 0.000045 Loss 5.489893, Accuracy 85.682%\n",
      "Epoch 22, Batch 319, LR 0.000045 Loss 5.490672, Accuracy 85.678%\n",
      "Epoch 22, Batch 320, LR 0.000045 Loss 5.490075, Accuracy 85.688%\n",
      "Epoch 22, Batch 321, LR 0.000045 Loss 5.489580, Accuracy 85.694%\n",
      "Epoch 22, Batch 322, LR 0.000045 Loss 5.492626, Accuracy 85.680%\n",
      "Epoch 22, Batch 323, LR 0.000045 Loss 5.493028, Accuracy 85.681%\n",
      "Epoch 22, Batch 324, LR 0.000045 Loss 5.491555, Accuracy 85.699%\n",
      "Epoch 22, Batch 325, LR 0.000045 Loss 5.491715, Accuracy 85.680%\n",
      "Epoch 22, Batch 326, LR 0.000045 Loss 5.491875, Accuracy 85.688%\n",
      "Epoch 22, Batch 327, LR 0.000045 Loss 5.492118, Accuracy 85.694%\n",
      "Epoch 22, Batch 328, LR 0.000045 Loss 5.490149, Accuracy 85.695%\n",
      "Epoch 22, Batch 329, LR 0.000045 Loss 5.490558, Accuracy 85.695%\n",
      "Epoch 22, Batch 330, LR 0.000045 Loss 5.493898, Accuracy 85.672%\n",
      "Epoch 22, Batch 331, LR 0.000045 Loss 5.494524, Accuracy 85.676%\n",
      "Epoch 22, Batch 332, LR 0.000045 Loss 5.493549, Accuracy 85.676%\n",
      "Epoch 22, Batch 333, LR 0.000045 Loss 5.494256, Accuracy 85.663%\n",
      "Epoch 22, Batch 334, LR 0.000045 Loss 5.493722, Accuracy 85.676%\n",
      "Epoch 22, Batch 335, LR 0.000045 Loss 5.493264, Accuracy 85.688%\n",
      "Epoch 22, Batch 336, LR 0.000045 Loss 5.492445, Accuracy 85.686%\n",
      "Epoch 22, Batch 337, LR 0.000045 Loss 5.490054, Accuracy 85.689%\n",
      "Epoch 22, Batch 338, LR 0.000045 Loss 5.492268, Accuracy 85.665%\n",
      "Epoch 22, Batch 339, LR 0.000045 Loss 5.490526, Accuracy 85.677%\n",
      "Epoch 22, Batch 340, LR 0.000045 Loss 5.493011, Accuracy 85.671%\n",
      "Epoch 22, Batch 341, LR 0.000045 Loss 5.492817, Accuracy 85.660%\n",
      "Epoch 22, Batch 342, LR 0.000045 Loss 5.494232, Accuracy 85.657%\n",
      "Epoch 22, Batch 343, LR 0.000045 Loss 5.493780, Accuracy 85.655%\n",
      "Epoch 22, Batch 344, LR 0.000045 Loss 5.490529, Accuracy 85.676%\n",
      "Epoch 22, Batch 345, LR 0.000045 Loss 5.491109, Accuracy 85.663%\n",
      "Epoch 22, Batch 346, LR 0.000045 Loss 5.490196, Accuracy 85.671%\n",
      "Epoch 22, Batch 347, LR 0.000045 Loss 5.491399, Accuracy 85.667%\n",
      "Epoch 22, Batch 348, LR 0.000045 Loss 5.491983, Accuracy 85.668%\n",
      "Epoch 22, Batch 349, LR 0.000045 Loss 5.494187, Accuracy 85.660%\n",
      "Epoch 22, Batch 350, LR 0.000044 Loss 5.495299, Accuracy 85.652%\n",
      "Epoch 22, Batch 351, LR 0.000044 Loss 5.496440, Accuracy 85.641%\n",
      "Epoch 22, Batch 352, LR 0.000044 Loss 5.497212, Accuracy 85.647%\n",
      "Epoch 22, Batch 353, LR 0.000044 Loss 5.497144, Accuracy 85.652%\n",
      "Epoch 22, Batch 354, LR 0.000044 Loss 5.496849, Accuracy 85.659%\n",
      "Epoch 22, Batch 355, LR 0.000044 Loss 5.497915, Accuracy 85.658%\n",
      "Epoch 22, Batch 356, LR 0.000044 Loss 5.499701, Accuracy 85.652%\n",
      "Epoch 22, Batch 357, LR 0.000044 Loss 5.499709, Accuracy 85.655%\n",
      "Epoch 22, Batch 358, LR 0.000044 Loss 5.500267, Accuracy 85.663%\n",
      "Epoch 22, Batch 359, LR 0.000044 Loss 5.498785, Accuracy 85.665%\n",
      "Epoch 22, Batch 360, LR 0.000044 Loss 5.499940, Accuracy 85.655%\n",
      "Epoch 22, Batch 361, LR 0.000044 Loss 5.498880, Accuracy 85.652%\n",
      "Epoch 22, Batch 362, LR 0.000044 Loss 5.497713, Accuracy 85.661%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 363, LR 0.000044 Loss 5.499704, Accuracy 85.649%\n",
      "Epoch 22, Batch 364, LR 0.000044 Loss 5.501096, Accuracy 85.633%\n",
      "Epoch 22, Batch 365, LR 0.000044 Loss 5.501747, Accuracy 85.627%\n",
      "Epoch 22, Batch 366, LR 0.000044 Loss 5.503477, Accuracy 85.624%\n",
      "Epoch 22, Batch 367, LR 0.000044 Loss 5.502770, Accuracy 85.627%\n",
      "Epoch 22, Batch 368, LR 0.000044 Loss 5.503071, Accuracy 85.623%\n",
      "Epoch 22, Batch 369, LR 0.000044 Loss 5.505297, Accuracy 85.601%\n",
      "Epoch 22, Batch 370, LR 0.000044 Loss 5.504590, Accuracy 85.602%\n",
      "Epoch 22, Batch 371, LR 0.000044 Loss 5.505610, Accuracy 85.598%\n",
      "Epoch 22, Batch 372, LR 0.000044 Loss 5.507023, Accuracy 85.585%\n",
      "Epoch 22, Batch 373, LR 0.000044 Loss 5.507720, Accuracy 85.588%\n",
      "Epoch 22, Batch 374, LR 0.000044 Loss 5.507511, Accuracy 85.578%\n",
      "Epoch 22, Batch 375, LR 0.000044 Loss 5.510533, Accuracy 85.565%\n",
      "Epoch 22, Batch 376, LR 0.000044 Loss 5.509476, Accuracy 85.574%\n",
      "Epoch 22, Batch 377, LR 0.000044 Loss 5.508845, Accuracy 85.577%\n",
      "Epoch 22, Batch 378, LR 0.000044 Loss 5.510754, Accuracy 85.561%\n",
      "Epoch 22, Batch 379, LR 0.000044 Loss 5.510756, Accuracy 85.573%\n",
      "Epoch 22, Batch 380, LR 0.000044 Loss 5.510402, Accuracy 85.576%\n",
      "Epoch 22, Batch 381, LR 0.000044 Loss 5.510219, Accuracy 85.589%\n",
      "Epoch 22, Batch 382, LR 0.000044 Loss 5.509160, Accuracy 85.592%\n",
      "Epoch 22, Batch 383, LR 0.000044 Loss 5.509046, Accuracy 85.595%\n",
      "Epoch 22, Batch 384, LR 0.000044 Loss 5.509331, Accuracy 85.592%\n",
      "Epoch 22, Batch 385, LR 0.000044 Loss 5.510032, Accuracy 85.584%\n",
      "Epoch 22, Batch 386, LR 0.000044 Loss 5.510065, Accuracy 85.581%\n",
      "Epoch 22, Batch 387, LR 0.000044 Loss 5.510022, Accuracy 85.580%\n",
      "Epoch 22, Batch 388, LR 0.000044 Loss 5.511395, Accuracy 85.569%\n",
      "Epoch 22, Batch 389, LR 0.000044 Loss 5.510639, Accuracy 85.566%\n",
      "Epoch 22, Batch 390, LR 0.000044 Loss 5.511414, Accuracy 85.555%\n",
      "Epoch 22, Batch 391, LR 0.000044 Loss 5.512172, Accuracy 85.548%\n",
      "Epoch 22, Batch 392, LR 0.000044 Loss 5.510969, Accuracy 85.557%\n",
      "Epoch 22, Batch 393, LR 0.000044 Loss 5.510172, Accuracy 85.568%\n",
      "Epoch 22, Batch 394, LR 0.000044 Loss 5.510399, Accuracy 85.567%\n",
      "Epoch 22, Batch 395, LR 0.000044 Loss 5.511064, Accuracy 85.556%\n",
      "Epoch 22, Batch 396, LR 0.000044 Loss 5.510747, Accuracy 85.559%\n",
      "Epoch 22, Batch 397, LR 0.000044 Loss 5.511739, Accuracy 85.548%\n",
      "Epoch 22, Batch 398, LR 0.000044 Loss 5.511344, Accuracy 85.549%\n",
      "Epoch 22, Batch 399, LR 0.000044 Loss 5.510361, Accuracy 85.562%\n",
      "Epoch 22, Batch 400, LR 0.000044 Loss 5.512077, Accuracy 85.547%\n",
      "Epoch 22, Batch 401, LR 0.000044 Loss 5.511647, Accuracy 85.554%\n",
      "Epoch 22, Batch 402, LR 0.000044 Loss 5.512844, Accuracy 85.547%\n",
      "Epoch 22, Batch 403, LR 0.000044 Loss 5.514096, Accuracy 85.540%\n",
      "Epoch 22, Batch 404, LR 0.000044 Loss 5.513677, Accuracy 85.539%\n",
      "Epoch 22, Batch 405, LR 0.000044 Loss 5.512181, Accuracy 85.548%\n",
      "Epoch 22, Batch 406, LR 0.000044 Loss 5.511834, Accuracy 85.556%\n",
      "Epoch 22, Batch 407, LR 0.000044 Loss 5.510937, Accuracy 85.561%\n",
      "Epoch 22, Batch 408, LR 0.000044 Loss 5.509290, Accuracy 85.568%\n",
      "Epoch 22, Batch 409, LR 0.000044 Loss 5.507985, Accuracy 85.571%\n",
      "Epoch 22, Batch 410, LR 0.000044 Loss 5.505861, Accuracy 85.579%\n",
      "Epoch 22, Batch 411, LR 0.000044 Loss 5.506784, Accuracy 85.576%\n",
      "Epoch 22, Batch 412, LR 0.000044 Loss 5.507797, Accuracy 85.568%\n",
      "Epoch 22, Batch 413, LR 0.000044 Loss 5.508395, Accuracy 85.559%\n",
      "Epoch 22, Batch 414, LR 0.000044 Loss 5.509155, Accuracy 85.564%\n",
      "Epoch 22, Batch 415, LR 0.000044 Loss 5.507093, Accuracy 85.576%\n",
      "Epoch 22, Batch 416, LR 0.000044 Loss 5.504194, Accuracy 85.592%\n",
      "Epoch 22, Batch 417, LR 0.000044 Loss 5.504992, Accuracy 85.589%\n",
      "Epoch 22, Batch 418, LR 0.000044 Loss 5.504814, Accuracy 85.582%\n",
      "Epoch 22, Batch 419, LR 0.000044 Loss 5.505171, Accuracy 85.578%\n",
      "Epoch 22, Batch 420, LR 0.000044 Loss 5.504455, Accuracy 85.584%\n",
      "Epoch 22, Batch 421, LR 0.000044 Loss 5.503195, Accuracy 85.585%\n",
      "Epoch 22, Batch 422, LR 0.000044 Loss 5.502991, Accuracy 85.584%\n",
      "Epoch 22, Batch 423, LR 0.000044 Loss 5.503506, Accuracy 85.572%\n",
      "Epoch 22, Batch 424, LR 0.000044 Loss 5.503617, Accuracy 85.563%\n",
      "Epoch 22, Batch 425, LR 0.000044 Loss 5.503212, Accuracy 85.561%\n",
      "Epoch 22, Batch 426, LR 0.000044 Loss 5.503403, Accuracy 85.560%\n",
      "Epoch 22, Batch 427, LR 0.000044 Loss 5.501757, Accuracy 85.562%\n",
      "Epoch 22, Batch 428, LR 0.000044 Loss 5.502608, Accuracy 85.561%\n",
      "Epoch 22, Batch 429, LR 0.000044 Loss 5.503420, Accuracy 85.557%\n",
      "Epoch 22, Batch 430, LR 0.000044 Loss 5.502630, Accuracy 85.569%\n",
      "Epoch 22, Batch 431, LR 0.000044 Loss 5.501721, Accuracy 85.571%\n",
      "Epoch 22, Batch 432, LR 0.000044 Loss 5.499211, Accuracy 85.583%\n",
      "Epoch 22, Batch 433, LR 0.000044 Loss 5.500548, Accuracy 85.575%\n",
      "Epoch 22, Batch 434, LR 0.000044 Loss 5.501315, Accuracy 85.559%\n",
      "Epoch 22, Batch 435, LR 0.000044 Loss 5.499545, Accuracy 85.562%\n",
      "Epoch 22, Batch 436, LR 0.000044 Loss 5.499586, Accuracy 85.556%\n",
      "Epoch 22, Batch 437, LR 0.000044 Loss 5.499954, Accuracy 85.555%\n",
      "Epoch 22, Batch 438, LR 0.000044 Loss 5.502130, Accuracy 85.545%\n",
      "Epoch 22, Batch 439, LR 0.000044 Loss 5.501050, Accuracy 85.557%\n",
      "Epoch 22, Batch 440, LR 0.000044 Loss 5.503861, Accuracy 85.549%\n",
      "Epoch 22, Batch 441, LR 0.000044 Loss 5.504050, Accuracy 85.542%\n",
      "Epoch 22, Batch 442, LR 0.000044 Loss 5.504530, Accuracy 85.547%\n",
      "Epoch 22, Batch 443, LR 0.000044 Loss 5.504343, Accuracy 85.546%\n",
      "Epoch 22, Batch 444, LR 0.000044 Loss 5.504901, Accuracy 85.538%\n",
      "Epoch 22, Batch 445, LR 0.000044 Loss 5.506652, Accuracy 85.537%\n",
      "Epoch 22, Batch 446, LR 0.000044 Loss 5.506164, Accuracy 85.543%\n",
      "Epoch 22, Batch 447, LR 0.000044 Loss 5.505593, Accuracy 85.543%\n",
      "Epoch 22, Batch 448, LR 0.000044 Loss 5.505655, Accuracy 85.547%\n",
      "Epoch 22, Batch 449, LR 0.000044 Loss 5.506387, Accuracy 85.549%\n",
      "Epoch 22, Batch 450, LR 0.000044 Loss 5.506038, Accuracy 85.545%\n",
      "Epoch 22, Batch 451, LR 0.000044 Loss 5.505252, Accuracy 85.548%\n",
      "Epoch 22, Batch 452, LR 0.000044 Loss 5.504921, Accuracy 85.542%\n",
      "Epoch 22, Batch 453, LR 0.000044 Loss 5.505477, Accuracy 85.537%\n",
      "Epoch 22, Batch 454, LR 0.000044 Loss 5.505096, Accuracy 85.537%\n",
      "Epoch 22, Batch 455, LR 0.000044 Loss 5.504753, Accuracy 85.543%\n",
      "Epoch 22, Batch 456, LR 0.000044 Loss 5.504770, Accuracy 85.538%\n",
      "Epoch 22, Batch 457, LR 0.000044 Loss 5.503768, Accuracy 85.543%\n",
      "Epoch 22, Batch 458, LR 0.000044 Loss 5.502724, Accuracy 85.550%\n",
      "Epoch 22, Batch 459, LR 0.000044 Loss 5.501998, Accuracy 85.549%\n",
      "Epoch 22, Batch 460, LR 0.000044 Loss 5.501448, Accuracy 85.557%\n",
      "Epoch 22, Batch 461, LR 0.000044 Loss 5.500477, Accuracy 85.551%\n",
      "Epoch 22, Batch 462, LR 0.000044 Loss 5.502347, Accuracy 85.532%\n",
      "Epoch 22, Batch 463, LR 0.000044 Loss 5.502656, Accuracy 85.522%\n",
      "Epoch 22, Batch 464, LR 0.000044 Loss 5.503821, Accuracy 85.515%\n",
      "Epoch 22, Batch 465, LR 0.000044 Loss 5.503453, Accuracy 85.514%\n",
      "Epoch 22, Batch 466, LR 0.000044 Loss 5.503361, Accuracy 85.510%\n",
      "Epoch 22, Batch 467, LR 0.000044 Loss 5.502153, Accuracy 85.513%\n",
      "Epoch 22, Batch 468, LR 0.000044 Loss 5.499954, Accuracy 85.522%\n",
      "Epoch 22, Batch 469, LR 0.000044 Loss 5.499138, Accuracy 85.521%\n",
      "Epoch 22, Batch 470, LR 0.000044 Loss 5.499390, Accuracy 85.517%\n",
      "Epoch 22, Batch 471, LR 0.000044 Loss 5.500115, Accuracy 85.515%\n",
      "Epoch 22, Batch 472, LR 0.000044 Loss 5.499860, Accuracy 85.520%\n",
      "Epoch 22, Batch 473, LR 0.000044 Loss 5.499724, Accuracy 85.526%\n",
      "Epoch 22, Batch 474, LR 0.000044 Loss 5.501010, Accuracy 85.519%\n",
      "Epoch 22, Batch 475, LR 0.000044 Loss 5.501940, Accuracy 85.520%\n",
      "Epoch 22, Batch 476, LR 0.000044 Loss 5.502638, Accuracy 85.521%\n",
      "Epoch 22, Batch 477, LR 0.000044 Loss 5.502955, Accuracy 85.513%\n",
      "Epoch 22, Batch 478, LR 0.000044 Loss 5.503745, Accuracy 85.511%\n",
      "Epoch 22, Batch 479, LR 0.000044 Loss 5.503202, Accuracy 85.517%\n",
      "Epoch 22, Batch 480, LR 0.000044 Loss 5.505175, Accuracy 85.511%\n",
      "Epoch 22, Batch 481, LR 0.000044 Loss 5.503337, Accuracy 85.522%\n",
      "Epoch 22, Batch 482, LR 0.000044 Loss 5.503453, Accuracy 85.521%\n",
      "Epoch 22, Batch 483, LR 0.000044 Loss 5.505836, Accuracy 85.510%\n",
      "Epoch 22, Batch 484, LR 0.000044 Loss 5.505500, Accuracy 85.515%\n",
      "Epoch 22, Batch 485, LR 0.000044 Loss 5.506507, Accuracy 85.506%\n",
      "Epoch 22, Batch 486, LR 0.000044 Loss 5.506746, Accuracy 85.495%\n",
      "Epoch 22, Batch 487, LR 0.000044 Loss 5.505154, Accuracy 85.498%\n",
      "Epoch 22, Batch 488, LR 0.000044 Loss 5.506882, Accuracy 85.489%\n",
      "Epoch 22, Batch 489, LR 0.000044 Loss 5.507903, Accuracy 85.479%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 490, LR 0.000044 Loss 5.507346, Accuracy 85.480%\n",
      "Epoch 22, Batch 491, LR 0.000044 Loss 5.508299, Accuracy 85.471%\n",
      "Epoch 22, Batch 492, LR 0.000044 Loss 5.509233, Accuracy 85.467%\n",
      "Epoch 22, Batch 493, LR 0.000044 Loss 5.509291, Accuracy 85.462%\n",
      "Epoch 22, Batch 494, LR 0.000044 Loss 5.508857, Accuracy 85.461%\n",
      "Epoch 22, Batch 495, LR 0.000044 Loss 5.507801, Accuracy 85.459%\n",
      "Epoch 22, Batch 496, LR 0.000044 Loss 5.507683, Accuracy 85.463%\n",
      "Epoch 22, Batch 497, LR 0.000044 Loss 5.508498, Accuracy 85.458%\n",
      "Epoch 22, Batch 498, LR 0.000044 Loss 5.509977, Accuracy 85.454%\n",
      "Epoch 22, Batch 499, LR 0.000044 Loss 5.510270, Accuracy 85.460%\n",
      "Epoch 22, Batch 500, LR 0.000044 Loss 5.511100, Accuracy 85.452%\n",
      "Epoch 22, Batch 501, LR 0.000044 Loss 5.510982, Accuracy 85.449%\n",
      "Epoch 22, Batch 502, LR 0.000044 Loss 5.511103, Accuracy 85.452%\n",
      "Epoch 22, Batch 503, LR 0.000044 Loss 5.510836, Accuracy 85.459%\n",
      "Epoch 22, Batch 504, LR 0.000044 Loss 5.509343, Accuracy 85.472%\n",
      "Epoch 22, Batch 505, LR 0.000044 Loss 5.510484, Accuracy 85.467%\n",
      "Epoch 22, Batch 506, LR 0.000044 Loss 5.511240, Accuracy 85.454%\n",
      "Epoch 22, Batch 507, LR 0.000044 Loss 5.510788, Accuracy 85.452%\n",
      "Epoch 22, Batch 508, LR 0.000044 Loss 5.509192, Accuracy 85.461%\n",
      "Epoch 22, Batch 509, LR 0.000044 Loss 5.510118, Accuracy 85.456%\n",
      "Epoch 22, Batch 510, LR 0.000044 Loss 5.510395, Accuracy 85.452%\n",
      "Epoch 22, Batch 511, LR 0.000044 Loss 5.511314, Accuracy 85.436%\n",
      "Epoch 22, Batch 512, LR 0.000044 Loss 5.510364, Accuracy 85.439%\n",
      "Epoch 22, Batch 513, LR 0.000044 Loss 5.510133, Accuracy 85.440%\n",
      "Epoch 22, Batch 514, LR 0.000044 Loss 5.511140, Accuracy 85.431%\n",
      "Epoch 22, Batch 515, LR 0.000044 Loss 5.511540, Accuracy 85.432%\n",
      "Epoch 22, Batch 516, LR 0.000044 Loss 5.510686, Accuracy 85.432%\n",
      "Epoch 22, Batch 517, LR 0.000044 Loss 5.509925, Accuracy 85.428%\n",
      "Epoch 22, Batch 518, LR 0.000044 Loss 5.510903, Accuracy 85.419%\n",
      "Epoch 22, Batch 519, LR 0.000044 Loss 5.512488, Accuracy 85.414%\n",
      "Epoch 22, Batch 520, LR 0.000044 Loss 5.512816, Accuracy 85.416%\n",
      "Epoch 22, Batch 521, LR 0.000044 Loss 5.512029, Accuracy 85.425%\n",
      "Epoch 22, Batch 522, LR 0.000044 Loss 5.512756, Accuracy 85.420%\n",
      "Epoch 22, Batch 523, LR 0.000044 Loss 5.512991, Accuracy 85.422%\n",
      "Epoch 22, Batch 524, LR 0.000044 Loss 5.512857, Accuracy 85.417%\n",
      "Epoch 22, Batch 525, LR 0.000044 Loss 5.512116, Accuracy 85.424%\n",
      "Epoch 22, Batch 526, LR 0.000044 Loss 5.512133, Accuracy 85.427%\n",
      "Epoch 22, Batch 527, LR 0.000044 Loss 5.510957, Accuracy 85.435%\n",
      "Epoch 22, Batch 528, LR 0.000044 Loss 5.511065, Accuracy 85.430%\n",
      "Epoch 22, Batch 529, LR 0.000044 Loss 5.511432, Accuracy 85.429%\n",
      "Epoch 22, Batch 530, LR 0.000044 Loss 5.512008, Accuracy 85.430%\n",
      "Epoch 22, Batch 531, LR 0.000044 Loss 5.510685, Accuracy 85.437%\n",
      "Epoch 22, Batch 532, LR 0.000044 Loss 5.510929, Accuracy 85.434%\n",
      "Epoch 22, Batch 533, LR 0.000044 Loss 5.510768, Accuracy 85.433%\n",
      "Epoch 22, Batch 534, LR 0.000044 Loss 5.510946, Accuracy 85.433%\n",
      "Epoch 22, Batch 535, LR 0.000044 Loss 5.510522, Accuracy 85.435%\n",
      "Epoch 22, Batch 536, LR 0.000044 Loss 5.510892, Accuracy 85.436%\n",
      "Epoch 22, Batch 537, LR 0.000044 Loss 5.513008, Accuracy 85.430%\n",
      "Epoch 22, Batch 538, LR 0.000044 Loss 5.512619, Accuracy 85.425%\n",
      "Epoch 22, Batch 539, LR 0.000044 Loss 5.512623, Accuracy 85.426%\n",
      "Epoch 22, Batch 540, LR 0.000044 Loss 5.512760, Accuracy 85.425%\n",
      "Epoch 22, Batch 541, LR 0.000044 Loss 5.512560, Accuracy 85.422%\n",
      "Epoch 22, Batch 542, LR 0.000044 Loss 5.512425, Accuracy 85.424%\n",
      "Epoch 22, Batch 543, LR 0.000044 Loss 5.513361, Accuracy 85.418%\n",
      "Epoch 22, Batch 544, LR 0.000044 Loss 5.514009, Accuracy 85.408%\n",
      "Epoch 22, Batch 545, LR 0.000044 Loss 5.515607, Accuracy 85.397%\n",
      "Epoch 22, Batch 546, LR 0.000044 Loss 5.515588, Accuracy 85.395%\n",
      "Epoch 22, Batch 547, LR 0.000044 Loss 5.516804, Accuracy 85.392%\n",
      "Epoch 22, Batch 548, LR 0.000044 Loss 5.515320, Accuracy 85.399%\n",
      "Epoch 22, Batch 549, LR 0.000044 Loss 5.515205, Accuracy 85.400%\n",
      "Epoch 22, Batch 550, LR 0.000044 Loss 5.516536, Accuracy 85.395%\n",
      "Epoch 22, Batch 551, LR 0.000044 Loss 5.516850, Accuracy 85.393%\n",
      "Epoch 22, Batch 552, LR 0.000044 Loss 5.517110, Accuracy 85.387%\n",
      "Epoch 22, Batch 553, LR 0.000044 Loss 5.517105, Accuracy 85.384%\n",
      "Epoch 22, Batch 554, LR 0.000044 Loss 5.517893, Accuracy 85.378%\n",
      "Epoch 22, Batch 555, LR 0.000044 Loss 5.516197, Accuracy 85.386%\n",
      "Epoch 22, Batch 556, LR 0.000044 Loss 5.515507, Accuracy 85.388%\n",
      "Epoch 22, Batch 557, LR 0.000044 Loss 5.515491, Accuracy 85.396%\n",
      "Epoch 22, Batch 558, LR 0.000044 Loss 5.514321, Accuracy 85.396%\n",
      "Epoch 22, Batch 559, LR 0.000044 Loss 5.513501, Accuracy 85.395%\n",
      "Epoch 22, Batch 560, LR 0.000044 Loss 5.512731, Accuracy 85.402%\n",
      "Epoch 22, Batch 561, LR 0.000044 Loss 5.512527, Accuracy 85.404%\n",
      "Epoch 22, Batch 562, LR 0.000044 Loss 5.512672, Accuracy 85.404%\n",
      "Epoch 22, Batch 563, LR 0.000044 Loss 5.511603, Accuracy 85.410%\n",
      "Epoch 22, Batch 564, LR 0.000044 Loss 5.511088, Accuracy 85.414%\n",
      "Epoch 22, Batch 565, LR 0.000044 Loss 5.510531, Accuracy 85.418%\n",
      "Epoch 22, Batch 566, LR 0.000044 Loss 5.509675, Accuracy 85.434%\n",
      "Epoch 22, Batch 567, LR 0.000044 Loss 5.509765, Accuracy 85.432%\n",
      "Epoch 22, Batch 568, LR 0.000044 Loss 5.508889, Accuracy 85.440%\n",
      "Epoch 22, Batch 569, LR 0.000044 Loss 5.509542, Accuracy 85.432%\n",
      "Epoch 22, Batch 570, LR 0.000044 Loss 5.511226, Accuracy 85.425%\n",
      "Epoch 22, Batch 571, LR 0.000044 Loss 5.511279, Accuracy 85.426%\n",
      "Epoch 22, Batch 572, LR 0.000044 Loss 5.511091, Accuracy 85.431%\n",
      "Epoch 22, Batch 573, LR 0.000044 Loss 5.511663, Accuracy 85.425%\n",
      "Epoch 22, Batch 574, LR 0.000044 Loss 5.512043, Accuracy 85.418%\n",
      "Epoch 22, Batch 575, LR 0.000044 Loss 5.511426, Accuracy 85.421%\n",
      "Epoch 22, Batch 576, LR 0.000044 Loss 5.510758, Accuracy 85.426%\n",
      "Epoch 22, Batch 577, LR 0.000044 Loss 5.510419, Accuracy 85.426%\n",
      "Epoch 22, Batch 578, LR 0.000044 Loss 5.509343, Accuracy 85.424%\n",
      "Epoch 22, Batch 579, LR 0.000044 Loss 5.509612, Accuracy 85.425%\n",
      "Epoch 22, Batch 580, LR 0.000044 Loss 5.510377, Accuracy 85.415%\n",
      "Epoch 22, Batch 581, LR 0.000044 Loss 5.511453, Accuracy 85.412%\n",
      "Epoch 22, Batch 582, LR 0.000044 Loss 5.510510, Accuracy 85.418%\n",
      "Epoch 22, Batch 583, LR 0.000044 Loss 5.510378, Accuracy 85.426%\n",
      "Epoch 22, Batch 584, LR 0.000044 Loss 5.510147, Accuracy 85.422%\n",
      "Epoch 22, Batch 585, LR 0.000044 Loss 5.510737, Accuracy 85.418%\n",
      "Epoch 22, Batch 586, LR 0.000044 Loss 5.511211, Accuracy 85.416%\n",
      "Epoch 22, Batch 587, LR 0.000044 Loss 5.510789, Accuracy 85.420%\n",
      "Epoch 22, Batch 588, LR 0.000044 Loss 5.511411, Accuracy 85.417%\n",
      "Epoch 22, Batch 589, LR 0.000044 Loss 5.511859, Accuracy 85.420%\n",
      "Epoch 22, Batch 590, LR 0.000044 Loss 5.512283, Accuracy 85.410%\n",
      "Epoch 22, Batch 591, LR 0.000044 Loss 5.512478, Accuracy 85.414%\n",
      "Epoch 22, Batch 592, LR 0.000044 Loss 5.513528, Accuracy 85.408%\n",
      "Epoch 22, Batch 593, LR 0.000044 Loss 5.513510, Accuracy 85.407%\n",
      "Epoch 22, Batch 594, LR 0.000044 Loss 5.512288, Accuracy 85.417%\n",
      "Epoch 22, Batch 595, LR 0.000044 Loss 5.512159, Accuracy 85.415%\n",
      "Epoch 22, Batch 596, LR 0.000044 Loss 5.512724, Accuracy 85.413%\n",
      "Epoch 22, Batch 597, LR 0.000044 Loss 5.512195, Accuracy 85.417%\n",
      "Epoch 22, Batch 598, LR 0.000044 Loss 5.511979, Accuracy 85.420%\n",
      "Epoch 22, Batch 599, LR 0.000044 Loss 5.510908, Accuracy 85.428%\n",
      "Epoch 22, Batch 600, LR 0.000044 Loss 5.510608, Accuracy 85.430%\n",
      "Epoch 22, Batch 601, LR 0.000044 Loss 5.509975, Accuracy 85.432%\n",
      "Epoch 22, Batch 602, LR 0.000044 Loss 5.509225, Accuracy 85.438%\n",
      "Epoch 22, Batch 603, LR 0.000044 Loss 5.507750, Accuracy 85.444%\n",
      "Epoch 22, Batch 604, LR 0.000044 Loss 5.508127, Accuracy 85.445%\n",
      "Epoch 22, Batch 605, LR 0.000043 Loss 5.509095, Accuracy 85.436%\n",
      "Epoch 22, Batch 606, LR 0.000043 Loss 5.509527, Accuracy 85.439%\n",
      "Epoch 22, Batch 607, LR 0.000043 Loss 5.510109, Accuracy 85.437%\n",
      "Epoch 22, Batch 608, LR 0.000043 Loss 5.509964, Accuracy 85.442%\n",
      "Epoch 22, Batch 609, LR 0.000043 Loss 5.509524, Accuracy 85.447%\n",
      "Epoch 22, Batch 610, LR 0.000043 Loss 5.509267, Accuracy 85.451%\n",
      "Epoch 22, Batch 611, LR 0.000043 Loss 5.509280, Accuracy 85.449%\n",
      "Epoch 22, Batch 612, LR 0.000043 Loss 5.509538, Accuracy 85.442%\n",
      "Epoch 22, Batch 613, LR 0.000043 Loss 5.508369, Accuracy 85.449%\n",
      "Epoch 22, Batch 614, LR 0.000043 Loss 5.507531, Accuracy 85.457%\n",
      "Epoch 22, Batch 615, LR 0.000043 Loss 5.507872, Accuracy 85.452%\n",
      "Epoch 22, Batch 616, LR 0.000043 Loss 5.508360, Accuracy 85.453%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 617, LR 0.000043 Loss 5.508911, Accuracy 85.450%\n",
      "Epoch 22, Batch 618, LR 0.000043 Loss 5.507659, Accuracy 85.457%\n",
      "Epoch 22, Batch 619, LR 0.000043 Loss 5.509861, Accuracy 85.449%\n",
      "Epoch 22, Batch 620, LR 0.000043 Loss 5.509284, Accuracy 85.455%\n",
      "Epoch 22, Batch 621, LR 0.000043 Loss 5.508561, Accuracy 85.459%\n",
      "Epoch 22, Batch 622, LR 0.000043 Loss 5.508693, Accuracy 85.461%\n",
      "Epoch 22, Batch 623, LR 0.000043 Loss 5.509469, Accuracy 85.461%\n",
      "Epoch 22, Batch 624, LR 0.000043 Loss 5.509050, Accuracy 85.469%\n",
      "Epoch 22, Batch 625, LR 0.000043 Loss 5.507590, Accuracy 85.475%\n",
      "Epoch 22, Batch 626, LR 0.000043 Loss 5.507539, Accuracy 85.477%\n",
      "Epoch 22, Batch 627, LR 0.000043 Loss 5.506653, Accuracy 85.485%\n",
      "Epoch 22, Batch 628, LR 0.000043 Loss 5.505757, Accuracy 85.485%\n",
      "Epoch 22, Batch 629, LR 0.000043 Loss 5.505957, Accuracy 85.480%\n",
      "Epoch 22, Batch 630, LR 0.000043 Loss 5.506870, Accuracy 85.470%\n",
      "Epoch 22, Batch 631, LR 0.000043 Loss 5.505678, Accuracy 85.476%\n",
      "Epoch 22, Batch 632, LR 0.000043 Loss 5.505704, Accuracy 85.479%\n",
      "Epoch 22, Batch 633, LR 0.000043 Loss 5.506601, Accuracy 85.467%\n",
      "Epoch 22, Batch 634, LR 0.000043 Loss 5.507099, Accuracy 85.464%\n",
      "Epoch 22, Batch 635, LR 0.000043 Loss 5.507878, Accuracy 85.460%\n",
      "Epoch 22, Batch 636, LR 0.000043 Loss 5.508324, Accuracy 85.457%\n",
      "Epoch 22, Batch 637, LR 0.000043 Loss 5.508060, Accuracy 85.460%\n",
      "Epoch 22, Batch 638, LR 0.000043 Loss 5.507283, Accuracy 85.464%\n",
      "Epoch 22, Batch 639, LR 0.000043 Loss 5.506984, Accuracy 85.466%\n",
      "Epoch 22, Batch 640, LR 0.000043 Loss 5.506464, Accuracy 85.465%\n",
      "Epoch 22, Batch 641, LR 0.000043 Loss 5.507411, Accuracy 85.460%\n",
      "Epoch 22, Batch 642, LR 0.000043 Loss 5.507866, Accuracy 85.459%\n",
      "Epoch 22, Batch 643, LR 0.000043 Loss 5.506981, Accuracy 85.466%\n",
      "Epoch 22, Batch 644, LR 0.000043 Loss 5.506153, Accuracy 85.464%\n",
      "Epoch 22, Batch 645, LR 0.000043 Loss 5.506739, Accuracy 85.466%\n",
      "Epoch 22, Batch 646, LR 0.000043 Loss 5.506849, Accuracy 85.472%\n",
      "Epoch 22, Batch 647, LR 0.000043 Loss 5.506139, Accuracy 85.476%\n",
      "Epoch 22, Batch 648, LR 0.000043 Loss 5.505009, Accuracy 85.485%\n",
      "Epoch 22, Batch 649, LR 0.000043 Loss 5.503934, Accuracy 85.493%\n",
      "Epoch 22, Batch 650, LR 0.000043 Loss 5.504636, Accuracy 85.493%\n",
      "Epoch 22, Batch 651, LR 0.000043 Loss 5.504800, Accuracy 85.483%\n",
      "Epoch 22, Batch 652, LR 0.000043 Loss 5.504048, Accuracy 85.487%\n",
      "Epoch 22, Batch 653, LR 0.000043 Loss 5.504270, Accuracy 85.489%\n",
      "Epoch 22, Batch 654, LR 0.000043 Loss 5.506314, Accuracy 85.474%\n",
      "Epoch 22, Batch 655, LR 0.000043 Loss 5.507126, Accuracy 85.470%\n",
      "Epoch 22, Batch 656, LR 0.000043 Loss 5.506936, Accuracy 85.473%\n",
      "Epoch 22, Batch 657, LR 0.000043 Loss 5.507814, Accuracy 85.471%\n",
      "Epoch 22, Batch 658, LR 0.000043 Loss 5.508111, Accuracy 85.472%\n",
      "Epoch 22, Batch 659, LR 0.000043 Loss 5.508170, Accuracy 85.473%\n",
      "Epoch 22, Batch 660, LR 0.000043 Loss 5.508185, Accuracy 85.472%\n",
      "Epoch 22, Batch 661, LR 0.000043 Loss 5.509112, Accuracy 85.468%\n",
      "Epoch 22, Batch 662, LR 0.000043 Loss 5.508252, Accuracy 85.475%\n",
      "Epoch 22, Batch 663, LR 0.000043 Loss 5.507940, Accuracy 85.479%\n",
      "Epoch 22, Batch 664, LR 0.000043 Loss 5.507377, Accuracy 85.480%\n",
      "Epoch 22, Batch 665, LR 0.000043 Loss 5.506791, Accuracy 85.483%\n",
      "Epoch 22, Batch 666, LR 0.000043 Loss 5.507071, Accuracy 85.482%\n",
      "Epoch 22, Batch 667, LR 0.000043 Loss 5.507728, Accuracy 85.474%\n",
      "Epoch 22, Batch 668, LR 0.000043 Loss 5.508413, Accuracy 85.471%\n",
      "Epoch 22, Batch 669, LR 0.000043 Loss 5.507746, Accuracy 85.468%\n",
      "Epoch 22, Batch 670, LR 0.000043 Loss 5.506830, Accuracy 85.472%\n",
      "Epoch 22, Batch 671, LR 0.000043 Loss 5.506682, Accuracy 85.473%\n",
      "Epoch 22, Batch 672, LR 0.000043 Loss 5.506433, Accuracy 85.474%\n",
      "Epoch 22, Batch 673, LR 0.000043 Loss 5.505507, Accuracy 85.474%\n",
      "Epoch 22, Batch 674, LR 0.000043 Loss 5.506172, Accuracy 85.467%\n",
      "Epoch 22, Batch 675, LR 0.000043 Loss 5.506085, Accuracy 85.459%\n",
      "Epoch 22, Batch 676, LR 0.000043 Loss 5.505939, Accuracy 85.465%\n",
      "Epoch 22, Batch 677, LR 0.000043 Loss 5.506769, Accuracy 85.456%\n",
      "Epoch 22, Batch 678, LR 0.000043 Loss 5.507126, Accuracy 85.459%\n",
      "Epoch 22, Batch 679, LR 0.000043 Loss 5.506811, Accuracy 85.463%\n",
      "Epoch 22, Batch 680, LR 0.000043 Loss 5.506511, Accuracy 85.466%\n",
      "Epoch 22, Batch 681, LR 0.000043 Loss 5.506426, Accuracy 85.472%\n",
      "Epoch 22, Batch 682, LR 0.000043 Loss 5.507464, Accuracy 85.467%\n",
      "Epoch 22, Batch 683, LR 0.000043 Loss 5.506023, Accuracy 85.472%\n",
      "Epoch 22, Batch 684, LR 0.000043 Loss 5.505915, Accuracy 85.476%\n",
      "Epoch 22, Batch 685, LR 0.000043 Loss 5.505827, Accuracy 85.473%\n",
      "Epoch 22, Batch 686, LR 0.000043 Loss 5.506295, Accuracy 85.476%\n",
      "Epoch 22, Batch 687, LR 0.000043 Loss 5.506426, Accuracy 85.478%\n",
      "Epoch 22, Batch 688, LR 0.000043 Loss 5.506852, Accuracy 85.476%\n",
      "Epoch 22, Batch 689, LR 0.000043 Loss 5.506777, Accuracy 85.481%\n",
      "Epoch 22, Batch 690, LR 0.000043 Loss 5.507214, Accuracy 85.474%\n",
      "Epoch 22, Batch 691, LR 0.000043 Loss 5.507474, Accuracy 85.473%\n",
      "Epoch 22, Batch 692, LR 0.000043 Loss 5.508441, Accuracy 85.462%\n",
      "Epoch 22, Batch 693, LR 0.000043 Loss 5.508345, Accuracy 85.467%\n",
      "Epoch 22, Batch 694, LR 0.000043 Loss 5.509214, Accuracy 85.460%\n",
      "Epoch 22, Batch 695, LR 0.000043 Loss 5.509799, Accuracy 85.455%\n",
      "Epoch 22, Batch 696, LR 0.000043 Loss 5.510585, Accuracy 85.458%\n",
      "Epoch 22, Batch 697, LR 0.000043 Loss 5.511500, Accuracy 85.461%\n",
      "Epoch 22, Batch 698, LR 0.000043 Loss 5.511627, Accuracy 85.462%\n",
      "Epoch 22, Batch 699, LR 0.000043 Loss 5.511767, Accuracy 85.460%\n",
      "Epoch 22, Batch 700, LR 0.000043 Loss 5.512600, Accuracy 85.453%\n",
      "Epoch 22, Batch 701, LR 0.000043 Loss 5.513033, Accuracy 85.450%\n",
      "Epoch 22, Batch 702, LR 0.000043 Loss 5.512691, Accuracy 85.450%\n",
      "Epoch 22, Batch 703, LR 0.000043 Loss 5.511705, Accuracy 85.454%\n",
      "Epoch 22, Batch 704, LR 0.000043 Loss 5.511505, Accuracy 85.460%\n",
      "Epoch 22, Batch 705, LR 0.000043 Loss 5.510881, Accuracy 85.460%\n",
      "Epoch 22, Batch 706, LR 0.000043 Loss 5.510394, Accuracy 85.462%\n",
      "Epoch 22, Batch 707, LR 0.000043 Loss 5.509704, Accuracy 85.467%\n",
      "Epoch 22, Batch 708, LR 0.000043 Loss 5.510719, Accuracy 85.459%\n",
      "Epoch 22, Batch 709, LR 0.000043 Loss 5.511636, Accuracy 85.460%\n",
      "Epoch 22, Batch 710, LR 0.000043 Loss 5.511172, Accuracy 85.460%\n",
      "Epoch 22, Batch 711, LR 0.000043 Loss 5.512036, Accuracy 85.457%\n",
      "Epoch 22, Batch 712, LR 0.000043 Loss 5.511769, Accuracy 85.459%\n",
      "Epoch 22, Batch 713, LR 0.000043 Loss 5.511619, Accuracy 85.461%\n",
      "Epoch 22, Batch 714, LR 0.000043 Loss 5.511551, Accuracy 85.467%\n",
      "Epoch 22, Batch 715, LR 0.000043 Loss 5.511715, Accuracy 85.470%\n",
      "Epoch 22, Batch 716, LR 0.000043 Loss 5.511002, Accuracy 85.475%\n",
      "Epoch 22, Batch 717, LR 0.000043 Loss 5.511136, Accuracy 85.474%\n",
      "Epoch 22, Batch 718, LR 0.000043 Loss 5.511551, Accuracy 85.475%\n",
      "Epoch 22, Batch 719, LR 0.000043 Loss 5.512026, Accuracy 85.477%\n",
      "Epoch 22, Batch 720, LR 0.000043 Loss 5.511783, Accuracy 85.481%\n",
      "Epoch 22, Batch 721, LR 0.000043 Loss 5.511914, Accuracy 85.477%\n",
      "Epoch 22, Batch 722, LR 0.000043 Loss 5.512731, Accuracy 85.473%\n",
      "Epoch 22, Batch 723, LR 0.000043 Loss 5.512656, Accuracy 85.471%\n",
      "Epoch 22, Batch 724, LR 0.000043 Loss 5.511870, Accuracy 85.472%\n",
      "Epoch 22, Batch 725, LR 0.000043 Loss 5.511112, Accuracy 85.478%\n",
      "Epoch 22, Batch 726, LR 0.000043 Loss 5.511203, Accuracy 85.478%\n",
      "Epoch 22, Batch 727, LR 0.000043 Loss 5.509506, Accuracy 85.489%\n",
      "Epoch 22, Batch 728, LR 0.000043 Loss 5.509508, Accuracy 85.489%\n",
      "Epoch 22, Batch 729, LR 0.000043 Loss 5.509461, Accuracy 85.493%\n",
      "Epoch 22, Batch 730, LR 0.000043 Loss 5.509863, Accuracy 85.491%\n",
      "Epoch 22, Batch 731, LR 0.000043 Loss 5.508956, Accuracy 85.497%\n",
      "Epoch 22, Batch 732, LR 0.000043 Loss 5.507383, Accuracy 85.501%\n",
      "Epoch 22, Batch 733, LR 0.000043 Loss 5.507241, Accuracy 85.494%\n",
      "Epoch 22, Batch 734, LR 0.000043 Loss 5.507513, Accuracy 85.486%\n",
      "Epoch 22, Batch 735, LR 0.000043 Loss 5.506696, Accuracy 85.486%\n",
      "Epoch 22, Batch 736, LR 0.000043 Loss 5.506800, Accuracy 85.488%\n",
      "Epoch 22, Batch 737, LR 0.000043 Loss 5.505976, Accuracy 85.492%\n",
      "Epoch 22, Batch 738, LR 0.000043 Loss 5.506746, Accuracy 85.481%\n",
      "Epoch 22, Batch 739, LR 0.000043 Loss 5.506906, Accuracy 85.481%\n",
      "Epoch 22, Batch 740, LR 0.000043 Loss 5.507121, Accuracy 85.478%\n",
      "Epoch 22, Batch 741, LR 0.000043 Loss 5.506604, Accuracy 85.480%\n",
      "Epoch 22, Batch 742, LR 0.000043 Loss 5.507219, Accuracy 85.476%\n",
      "Epoch 22, Batch 743, LR 0.000043 Loss 5.507915, Accuracy 85.476%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 744, LR 0.000043 Loss 5.507917, Accuracy 85.480%\n",
      "Epoch 22, Batch 745, LR 0.000043 Loss 5.507935, Accuracy 85.477%\n",
      "Epoch 22, Batch 746, LR 0.000043 Loss 5.508546, Accuracy 85.479%\n",
      "Epoch 22, Batch 747, LR 0.000043 Loss 5.508378, Accuracy 85.482%\n",
      "Epoch 22, Batch 748, LR 0.000043 Loss 5.507278, Accuracy 85.483%\n",
      "Epoch 22, Batch 749, LR 0.000043 Loss 5.507519, Accuracy 85.481%\n",
      "Epoch 22, Batch 750, LR 0.000043 Loss 5.506572, Accuracy 85.483%\n",
      "Epoch 22, Batch 751, LR 0.000043 Loss 5.506161, Accuracy 85.485%\n",
      "Epoch 22, Batch 752, LR 0.000043 Loss 5.507553, Accuracy 85.474%\n",
      "Epoch 22, Batch 753, LR 0.000043 Loss 5.507844, Accuracy 85.472%\n",
      "Epoch 22, Batch 754, LR 0.000043 Loss 5.506595, Accuracy 85.482%\n",
      "Epoch 22, Batch 755, LR 0.000043 Loss 5.506090, Accuracy 85.488%\n",
      "Epoch 22, Batch 756, LR 0.000043 Loss 5.505567, Accuracy 85.488%\n",
      "Epoch 22, Batch 757, LR 0.000043 Loss 5.505478, Accuracy 85.488%\n",
      "Epoch 22, Batch 758, LR 0.000043 Loss 5.505250, Accuracy 85.487%\n",
      "Epoch 22, Batch 759, LR 0.000043 Loss 5.505635, Accuracy 85.487%\n",
      "Epoch 22, Batch 760, LR 0.000043 Loss 5.506131, Accuracy 85.491%\n",
      "Epoch 22, Batch 761, LR 0.000043 Loss 5.506714, Accuracy 85.492%\n",
      "Epoch 22, Batch 762, LR 0.000043 Loss 5.507055, Accuracy 85.493%\n",
      "Epoch 22, Batch 763, LR 0.000043 Loss 5.506604, Accuracy 85.489%\n",
      "Epoch 22, Batch 764, LR 0.000043 Loss 5.506910, Accuracy 85.487%\n",
      "Epoch 22, Batch 765, LR 0.000043 Loss 5.507085, Accuracy 85.488%\n",
      "Epoch 22, Batch 766, LR 0.000043 Loss 5.506254, Accuracy 85.487%\n",
      "Epoch 22, Batch 767, LR 0.000043 Loss 5.505905, Accuracy 85.490%\n",
      "Epoch 22, Batch 768, LR 0.000043 Loss 5.505622, Accuracy 85.492%\n",
      "Epoch 22, Batch 769, LR 0.000043 Loss 5.505946, Accuracy 85.489%\n",
      "Epoch 22, Batch 770, LR 0.000043 Loss 5.505623, Accuracy 85.490%\n",
      "Epoch 22, Batch 771, LR 0.000043 Loss 5.505361, Accuracy 85.492%\n",
      "Epoch 22, Batch 772, LR 0.000043 Loss 5.506137, Accuracy 85.489%\n",
      "Epoch 22, Batch 773, LR 0.000043 Loss 5.506174, Accuracy 85.490%\n",
      "Epoch 22, Batch 774, LR 0.000043 Loss 5.506340, Accuracy 85.490%\n",
      "Epoch 22, Batch 775, LR 0.000043 Loss 5.506035, Accuracy 85.490%\n",
      "Epoch 22, Batch 776, LR 0.000043 Loss 5.506484, Accuracy 85.490%\n",
      "Epoch 22, Batch 777, LR 0.000043 Loss 5.506958, Accuracy 85.491%\n",
      "Epoch 22, Batch 778, LR 0.000043 Loss 5.507020, Accuracy 85.491%\n",
      "Epoch 22, Batch 779, LR 0.000043 Loss 5.507105, Accuracy 85.491%\n",
      "Epoch 22, Batch 780, LR 0.000043 Loss 5.507396, Accuracy 85.490%\n",
      "Epoch 22, Batch 781, LR 0.000043 Loss 5.508335, Accuracy 85.490%\n",
      "Epoch 22, Batch 782, LR 0.000043 Loss 5.508605, Accuracy 85.486%\n",
      "Epoch 22, Batch 783, LR 0.000043 Loss 5.508550, Accuracy 85.491%\n",
      "Epoch 22, Batch 784, LR 0.000043 Loss 5.507599, Accuracy 85.495%\n",
      "Epoch 22, Batch 785, LR 0.000043 Loss 5.507793, Accuracy 85.498%\n",
      "Epoch 22, Batch 786, LR 0.000043 Loss 5.507700, Accuracy 85.494%\n",
      "Epoch 22, Batch 787, LR 0.000043 Loss 5.507669, Accuracy 85.490%\n",
      "Epoch 22, Batch 788, LR 0.000043 Loss 5.507636, Accuracy 85.494%\n",
      "Epoch 22, Batch 789, LR 0.000043 Loss 5.507592, Accuracy 85.501%\n",
      "Epoch 22, Batch 790, LR 0.000043 Loss 5.508610, Accuracy 85.495%\n",
      "Epoch 22, Batch 791, LR 0.000043 Loss 5.508751, Accuracy 85.499%\n",
      "Epoch 22, Batch 792, LR 0.000043 Loss 5.508947, Accuracy 85.495%\n",
      "Epoch 22, Batch 793, LR 0.000043 Loss 5.507963, Accuracy 85.498%\n",
      "Epoch 22, Batch 794, LR 0.000043 Loss 5.508484, Accuracy 85.499%\n",
      "Epoch 22, Batch 795, LR 0.000043 Loss 5.508833, Accuracy 85.499%\n",
      "Epoch 22, Batch 796, LR 0.000043 Loss 5.509672, Accuracy 85.491%\n",
      "Epoch 22, Batch 797, LR 0.000043 Loss 5.509995, Accuracy 85.487%\n",
      "Epoch 22, Batch 798, LR 0.000043 Loss 5.508355, Accuracy 85.497%\n",
      "Epoch 22, Batch 799, LR 0.000043 Loss 5.508312, Accuracy 85.494%\n",
      "Epoch 22, Batch 800, LR 0.000043 Loss 5.508678, Accuracy 85.497%\n",
      "Epoch 22, Batch 801, LR 0.000043 Loss 5.508940, Accuracy 85.494%\n",
      "Epoch 22, Batch 802, LR 0.000043 Loss 5.508495, Accuracy 85.497%\n",
      "Epoch 22, Batch 803, LR 0.000043 Loss 5.507315, Accuracy 85.506%\n",
      "Epoch 22, Batch 804, LR 0.000043 Loss 5.507657, Accuracy 85.502%\n",
      "Epoch 22, Batch 805, LR 0.000043 Loss 5.507963, Accuracy 85.499%\n",
      "Epoch 22, Batch 806, LR 0.000043 Loss 5.507504, Accuracy 85.500%\n",
      "Epoch 22, Batch 807, LR 0.000043 Loss 5.507176, Accuracy 85.496%\n",
      "Epoch 22, Batch 808, LR 0.000043 Loss 5.508595, Accuracy 85.490%\n",
      "Epoch 22, Batch 809, LR 0.000043 Loss 5.508741, Accuracy 85.487%\n",
      "Epoch 22, Batch 810, LR 0.000043 Loss 5.508781, Accuracy 85.483%\n",
      "Epoch 22, Batch 811, LR 0.000043 Loss 5.507777, Accuracy 85.488%\n",
      "Epoch 22, Batch 812, LR 0.000043 Loss 5.507471, Accuracy 85.486%\n",
      "Epoch 22, Batch 813, LR 0.000043 Loss 5.507173, Accuracy 85.487%\n",
      "Epoch 22, Batch 814, LR 0.000043 Loss 5.508027, Accuracy 85.485%\n",
      "Epoch 22, Batch 815, LR 0.000043 Loss 5.508252, Accuracy 85.485%\n",
      "Epoch 22, Batch 816, LR 0.000043 Loss 5.509314, Accuracy 85.480%\n",
      "Epoch 22, Batch 817, LR 0.000043 Loss 5.509393, Accuracy 85.484%\n",
      "Epoch 22, Batch 818, LR 0.000043 Loss 5.509998, Accuracy 85.481%\n",
      "Epoch 22, Batch 819, LR 0.000043 Loss 5.509974, Accuracy 85.482%\n",
      "Epoch 22, Batch 820, LR 0.000043 Loss 5.509596, Accuracy 85.485%\n",
      "Epoch 22, Batch 821, LR 0.000043 Loss 5.510205, Accuracy 85.484%\n",
      "Epoch 22, Batch 822, LR 0.000043 Loss 5.510697, Accuracy 85.482%\n",
      "Epoch 22, Batch 823, LR 0.000043 Loss 5.510446, Accuracy 85.486%\n",
      "Epoch 22, Batch 824, LR 0.000043 Loss 5.509581, Accuracy 85.492%\n",
      "Epoch 22, Batch 825, LR 0.000043 Loss 5.510112, Accuracy 85.491%\n",
      "Epoch 22, Batch 826, LR 0.000043 Loss 5.510271, Accuracy 85.489%\n",
      "Epoch 22, Batch 827, LR 0.000043 Loss 5.509296, Accuracy 85.494%\n",
      "Epoch 22, Batch 828, LR 0.000043 Loss 5.509674, Accuracy 85.490%\n",
      "Epoch 22, Batch 829, LR 0.000043 Loss 5.509987, Accuracy 85.486%\n",
      "Epoch 22, Batch 830, LR 0.000043 Loss 5.509945, Accuracy 85.488%\n",
      "Epoch 22, Batch 831, LR 0.000043 Loss 5.509241, Accuracy 85.488%\n",
      "Epoch 22, Batch 832, LR 0.000043 Loss 5.509165, Accuracy 85.483%\n",
      "Epoch 22, Batch 833, LR 0.000043 Loss 5.508431, Accuracy 85.490%\n",
      "Epoch 22, Batch 834, LR 0.000043 Loss 5.508748, Accuracy 85.488%\n",
      "Epoch 22, Batch 835, LR 0.000043 Loss 5.508430, Accuracy 85.482%\n",
      "Epoch 22, Batch 836, LR 0.000043 Loss 5.508188, Accuracy 85.482%\n",
      "Epoch 22, Batch 837, LR 0.000043 Loss 5.508742, Accuracy 85.482%\n",
      "Epoch 22, Batch 838, LR 0.000043 Loss 5.509006, Accuracy 85.478%\n",
      "Epoch 22, Batch 839, LR 0.000043 Loss 5.509860, Accuracy 85.473%\n",
      "Epoch 22, Batch 840, LR 0.000043 Loss 5.508574, Accuracy 85.479%\n",
      "Epoch 22, Batch 841, LR 0.000043 Loss 5.508389, Accuracy 85.478%\n",
      "Epoch 22, Batch 842, LR 0.000043 Loss 5.507995, Accuracy 85.481%\n",
      "Epoch 22, Batch 843, LR 0.000043 Loss 5.507920, Accuracy 85.481%\n",
      "Epoch 22, Batch 844, LR 0.000043 Loss 5.507854, Accuracy 85.480%\n",
      "Epoch 22, Batch 845, LR 0.000043 Loss 5.507560, Accuracy 85.481%\n",
      "Epoch 22, Batch 846, LR 0.000043 Loss 5.507998, Accuracy 85.476%\n",
      "Epoch 22, Batch 847, LR 0.000043 Loss 5.507874, Accuracy 85.477%\n",
      "Epoch 22, Batch 848, LR 0.000043 Loss 5.507740, Accuracy 85.476%\n",
      "Epoch 22, Batch 849, LR 0.000043 Loss 5.507584, Accuracy 85.476%\n",
      "Epoch 22, Batch 850, LR 0.000043 Loss 5.506793, Accuracy 85.479%\n",
      "Epoch 22, Batch 851, LR 0.000043 Loss 5.506310, Accuracy 85.484%\n",
      "Epoch 22, Batch 852, LR 0.000043 Loss 5.506181, Accuracy 85.485%\n",
      "Epoch 22, Batch 853, LR 0.000043 Loss 5.506589, Accuracy 85.481%\n",
      "Epoch 22, Batch 854, LR 0.000043 Loss 5.506833, Accuracy 85.476%\n",
      "Epoch 22, Batch 855, LR 0.000043 Loss 5.506900, Accuracy 85.476%\n",
      "Epoch 22, Batch 856, LR 0.000043 Loss 5.507355, Accuracy 85.474%\n",
      "Epoch 22, Batch 857, LR 0.000043 Loss 5.507254, Accuracy 85.473%\n",
      "Epoch 22, Batch 858, LR 0.000043 Loss 5.506361, Accuracy 85.480%\n",
      "Epoch 22, Batch 859, LR 0.000043 Loss 5.506603, Accuracy 85.474%\n",
      "Epoch 22, Batch 860, LR 0.000042 Loss 5.506631, Accuracy 85.471%\n",
      "Epoch 22, Batch 861, LR 0.000042 Loss 5.506250, Accuracy 85.471%\n",
      "Epoch 22, Batch 862, LR 0.000042 Loss 5.506678, Accuracy 85.465%\n",
      "Epoch 22, Batch 863, LR 0.000042 Loss 5.506250, Accuracy 85.468%\n",
      "Epoch 22, Batch 864, LR 0.000042 Loss 5.505716, Accuracy 85.468%\n",
      "Epoch 22, Batch 865, LR 0.000042 Loss 5.505353, Accuracy 85.471%\n",
      "Epoch 22, Batch 866, LR 0.000042 Loss 5.505080, Accuracy 85.471%\n",
      "Epoch 22, Batch 867, LR 0.000042 Loss 5.504803, Accuracy 85.470%\n",
      "Epoch 22, Batch 868, LR 0.000042 Loss 5.505014, Accuracy 85.465%\n",
      "Epoch 22, Batch 869, LR 0.000042 Loss 5.504763, Accuracy 85.466%\n",
      "Epoch 22, Batch 870, LR 0.000042 Loss 5.504257, Accuracy 85.467%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 871, LR 0.000042 Loss 5.503404, Accuracy 85.471%\n",
      "Epoch 22, Batch 872, LR 0.000042 Loss 5.503133, Accuracy 85.474%\n",
      "Epoch 22, Batch 873, LR 0.000042 Loss 5.502372, Accuracy 85.478%\n",
      "Epoch 22, Batch 874, LR 0.000042 Loss 5.502933, Accuracy 85.474%\n",
      "Epoch 22, Batch 875, LR 0.000042 Loss 5.502720, Accuracy 85.471%\n",
      "Epoch 22, Batch 876, LR 0.000042 Loss 5.502443, Accuracy 85.471%\n",
      "Epoch 22, Batch 877, LR 0.000042 Loss 5.502481, Accuracy 85.469%\n",
      "Epoch 22, Batch 878, LR 0.000042 Loss 5.502621, Accuracy 85.466%\n",
      "Epoch 22, Batch 879, LR 0.000042 Loss 5.502959, Accuracy 85.466%\n",
      "Epoch 22, Batch 880, LR 0.000042 Loss 5.502917, Accuracy 85.463%\n",
      "Epoch 22, Batch 881, LR 0.000042 Loss 5.503241, Accuracy 85.463%\n",
      "Epoch 22, Batch 882, LR 0.000042 Loss 5.503330, Accuracy 85.461%\n",
      "Epoch 22, Batch 883, LR 0.000042 Loss 5.503581, Accuracy 85.459%\n",
      "Epoch 22, Batch 884, LR 0.000042 Loss 5.503586, Accuracy 85.460%\n",
      "Epoch 22, Batch 885, LR 0.000042 Loss 5.502412, Accuracy 85.462%\n",
      "Epoch 22, Batch 886, LR 0.000042 Loss 5.502477, Accuracy 85.465%\n",
      "Epoch 22, Batch 887, LR 0.000042 Loss 5.501434, Accuracy 85.466%\n",
      "Epoch 22, Batch 888, LR 0.000042 Loss 5.501864, Accuracy 85.466%\n",
      "Epoch 22, Batch 889, LR 0.000042 Loss 5.502163, Accuracy 85.465%\n",
      "Epoch 22, Batch 890, LR 0.000042 Loss 5.501299, Accuracy 85.470%\n",
      "Epoch 22, Batch 891, LR 0.000042 Loss 5.502140, Accuracy 85.468%\n",
      "Epoch 22, Batch 892, LR 0.000042 Loss 5.502376, Accuracy 85.465%\n",
      "Epoch 22, Batch 893, LR 0.000042 Loss 5.502525, Accuracy 85.464%\n",
      "Epoch 22, Batch 894, LR 0.000042 Loss 5.502437, Accuracy 85.465%\n",
      "Epoch 22, Batch 895, LR 0.000042 Loss 5.501511, Accuracy 85.470%\n",
      "Epoch 22, Batch 896, LR 0.000042 Loss 5.502263, Accuracy 85.468%\n",
      "Epoch 22, Batch 897, LR 0.000042 Loss 5.502243, Accuracy 85.472%\n",
      "Epoch 22, Batch 898, LR 0.000042 Loss 5.501794, Accuracy 85.475%\n",
      "Epoch 22, Batch 899, LR 0.000042 Loss 5.502112, Accuracy 85.471%\n",
      "Epoch 22, Batch 900, LR 0.000042 Loss 5.501701, Accuracy 85.472%\n",
      "Epoch 22, Batch 901, LR 0.000042 Loss 5.501308, Accuracy 85.472%\n",
      "Epoch 22, Batch 902, LR 0.000042 Loss 5.501681, Accuracy 85.475%\n",
      "Epoch 22, Batch 903, LR 0.000042 Loss 5.500982, Accuracy 85.480%\n",
      "Epoch 22, Batch 904, LR 0.000042 Loss 5.501081, Accuracy 85.479%\n",
      "Epoch 22, Batch 905, LR 0.000042 Loss 5.501053, Accuracy 85.477%\n",
      "Epoch 22, Batch 906, LR 0.000042 Loss 5.500354, Accuracy 85.480%\n",
      "Epoch 22, Batch 907, LR 0.000042 Loss 5.500366, Accuracy 85.477%\n",
      "Epoch 22, Batch 908, LR 0.000042 Loss 5.500255, Accuracy 85.479%\n",
      "Epoch 22, Batch 909, LR 0.000042 Loss 5.499554, Accuracy 85.479%\n",
      "Epoch 22, Batch 910, LR 0.000042 Loss 5.499193, Accuracy 85.479%\n",
      "Epoch 22, Batch 911, LR 0.000042 Loss 5.498354, Accuracy 85.482%\n",
      "Epoch 22, Batch 912, LR 0.000042 Loss 5.498539, Accuracy 85.481%\n",
      "Epoch 22, Batch 913, LR 0.000042 Loss 5.498669, Accuracy 85.476%\n",
      "Epoch 22, Batch 914, LR 0.000042 Loss 5.498669, Accuracy 85.473%\n",
      "Epoch 22, Batch 915, LR 0.000042 Loss 5.499300, Accuracy 85.469%\n",
      "Epoch 22, Batch 916, LR 0.000042 Loss 5.499020, Accuracy 85.466%\n",
      "Epoch 22, Batch 917, LR 0.000042 Loss 5.498818, Accuracy 85.467%\n",
      "Epoch 22, Batch 918, LR 0.000042 Loss 5.498125, Accuracy 85.470%\n",
      "Epoch 22, Batch 919, LR 0.000042 Loss 5.498799, Accuracy 85.468%\n",
      "Epoch 22, Batch 920, LR 0.000042 Loss 5.498869, Accuracy 85.465%\n",
      "Epoch 22, Batch 921, LR 0.000042 Loss 5.499547, Accuracy 85.460%\n",
      "Epoch 22, Batch 922, LR 0.000042 Loss 5.499924, Accuracy 85.460%\n",
      "Epoch 22, Batch 923, LR 0.000042 Loss 5.499413, Accuracy 85.463%\n",
      "Epoch 22, Batch 924, LR 0.000042 Loss 5.498659, Accuracy 85.467%\n",
      "Epoch 22, Batch 925, LR 0.000042 Loss 5.498635, Accuracy 85.465%\n",
      "Epoch 22, Batch 926, LR 0.000042 Loss 5.498491, Accuracy 85.466%\n",
      "Epoch 22, Batch 927, LR 0.000042 Loss 5.500117, Accuracy 85.454%\n",
      "Epoch 22, Batch 928, LR 0.000042 Loss 5.499458, Accuracy 85.460%\n",
      "Epoch 22, Batch 929, LR 0.000042 Loss 5.498827, Accuracy 85.463%\n",
      "Epoch 22, Batch 930, LR 0.000042 Loss 5.500147, Accuracy 85.462%\n",
      "Epoch 22, Batch 931, LR 0.000042 Loss 5.499817, Accuracy 85.463%\n",
      "Epoch 22, Batch 932, LR 0.000042 Loss 5.499304, Accuracy 85.461%\n",
      "Epoch 22, Batch 933, LR 0.000042 Loss 5.499053, Accuracy 85.465%\n",
      "Epoch 22, Batch 934, LR 0.000042 Loss 5.498082, Accuracy 85.467%\n",
      "Epoch 22, Batch 935, LR 0.000042 Loss 5.498212, Accuracy 85.464%\n",
      "Epoch 22, Batch 936, LR 0.000042 Loss 5.497541, Accuracy 85.467%\n",
      "Epoch 22, Batch 937, LR 0.000042 Loss 5.497671, Accuracy 85.469%\n",
      "Epoch 22, Batch 938, LR 0.000042 Loss 5.497369, Accuracy 85.469%\n",
      "Epoch 22, Batch 939, LR 0.000042 Loss 5.497452, Accuracy 85.467%\n",
      "Epoch 22, Batch 940, LR 0.000042 Loss 5.497339, Accuracy 85.466%\n",
      "Epoch 22, Batch 941, LR 0.000042 Loss 5.497010, Accuracy 85.471%\n",
      "Epoch 22, Batch 942, LR 0.000042 Loss 5.496669, Accuracy 85.470%\n",
      "Epoch 22, Batch 943, LR 0.000042 Loss 5.497388, Accuracy 85.465%\n",
      "Epoch 22, Batch 944, LR 0.000042 Loss 5.496850, Accuracy 85.469%\n",
      "Epoch 22, Batch 945, LR 0.000042 Loss 5.496482, Accuracy 85.470%\n",
      "Epoch 22, Batch 946, LR 0.000042 Loss 5.495999, Accuracy 85.473%\n",
      "Epoch 22, Batch 947, LR 0.000042 Loss 5.495693, Accuracy 85.474%\n",
      "Epoch 22, Batch 948, LR 0.000042 Loss 5.497049, Accuracy 85.469%\n",
      "Epoch 22, Batch 949, LR 0.000042 Loss 5.497169, Accuracy 85.469%\n",
      "Epoch 22, Batch 950, LR 0.000042 Loss 5.497355, Accuracy 85.466%\n",
      "Epoch 22, Batch 951, LR 0.000042 Loss 5.496718, Accuracy 85.471%\n",
      "Epoch 22, Batch 952, LR 0.000042 Loss 5.498010, Accuracy 85.464%\n",
      "Epoch 22, Batch 953, LR 0.000042 Loss 5.497421, Accuracy 85.464%\n",
      "Epoch 22, Batch 954, LR 0.000042 Loss 5.497065, Accuracy 85.469%\n",
      "Epoch 22, Batch 955, LR 0.000042 Loss 5.497740, Accuracy 85.465%\n",
      "Epoch 22, Batch 956, LR 0.000042 Loss 5.497982, Accuracy 85.460%\n",
      "Epoch 22, Batch 957, LR 0.000042 Loss 5.498318, Accuracy 85.457%\n",
      "Epoch 22, Batch 958, LR 0.000042 Loss 5.498594, Accuracy 85.457%\n",
      "Epoch 22, Batch 959, LR 0.000042 Loss 5.498644, Accuracy 85.458%\n",
      "Epoch 22, Batch 960, LR 0.000042 Loss 5.498835, Accuracy 85.458%\n",
      "Epoch 22, Batch 961, LR 0.000042 Loss 5.498787, Accuracy 85.455%\n",
      "Epoch 22, Batch 962, LR 0.000042 Loss 5.499441, Accuracy 85.455%\n",
      "Epoch 22, Batch 963, LR 0.000042 Loss 5.499235, Accuracy 85.458%\n",
      "Epoch 22, Batch 964, LR 0.000042 Loss 5.499713, Accuracy 85.456%\n",
      "Epoch 22, Batch 965, LR 0.000042 Loss 5.499640, Accuracy 85.458%\n",
      "Epoch 22, Batch 966, LR 0.000042 Loss 5.498784, Accuracy 85.462%\n",
      "Epoch 22, Batch 967, LR 0.000042 Loss 5.498992, Accuracy 85.459%\n",
      "Epoch 22, Batch 968, LR 0.000042 Loss 5.499090, Accuracy 85.461%\n",
      "Epoch 22, Batch 969, LR 0.000042 Loss 5.499723, Accuracy 85.456%\n",
      "Epoch 22, Batch 970, LR 0.000042 Loss 5.499711, Accuracy 85.453%\n",
      "Epoch 22, Batch 971, LR 0.000042 Loss 5.499117, Accuracy 85.456%\n",
      "Epoch 22, Batch 972, LR 0.000042 Loss 5.499411, Accuracy 85.456%\n",
      "Epoch 22, Batch 973, LR 0.000042 Loss 5.498685, Accuracy 85.457%\n",
      "Epoch 22, Batch 974, LR 0.000042 Loss 5.498609, Accuracy 85.459%\n",
      "Epoch 22, Batch 975, LR 0.000042 Loss 5.498440, Accuracy 85.460%\n",
      "Epoch 22, Batch 976, LR 0.000042 Loss 5.497078, Accuracy 85.464%\n",
      "Epoch 22, Batch 977, LR 0.000042 Loss 5.497088, Accuracy 85.463%\n",
      "Epoch 22, Batch 978, LR 0.000042 Loss 5.497320, Accuracy 85.460%\n",
      "Epoch 22, Batch 979, LR 0.000042 Loss 5.496699, Accuracy 85.462%\n",
      "Epoch 22, Batch 980, LR 0.000042 Loss 5.496392, Accuracy 85.464%\n",
      "Epoch 22, Batch 981, LR 0.000042 Loss 5.497275, Accuracy 85.463%\n",
      "Epoch 22, Batch 982, LR 0.000042 Loss 5.497526, Accuracy 85.460%\n",
      "Epoch 22, Batch 983, LR 0.000042 Loss 5.497100, Accuracy 85.457%\n",
      "Epoch 22, Batch 984, LR 0.000042 Loss 5.496720, Accuracy 85.458%\n",
      "Epoch 22, Batch 985, LR 0.000042 Loss 5.496463, Accuracy 85.460%\n",
      "Epoch 22, Batch 986, LR 0.000042 Loss 5.496801, Accuracy 85.457%\n",
      "Epoch 22, Batch 987, LR 0.000042 Loss 5.496966, Accuracy 85.452%\n",
      "Epoch 22, Batch 988, LR 0.000042 Loss 5.496754, Accuracy 85.457%\n",
      "Epoch 22, Batch 989, LR 0.000042 Loss 5.496992, Accuracy 85.459%\n",
      "Epoch 22, Batch 990, LR 0.000042 Loss 5.496951, Accuracy 85.460%\n",
      "Epoch 22, Batch 991, LR 0.000042 Loss 5.496766, Accuracy 85.457%\n",
      "Epoch 22, Batch 992, LR 0.000042 Loss 5.496731, Accuracy 85.459%\n",
      "Epoch 22, Batch 993, LR 0.000042 Loss 5.497086, Accuracy 85.459%\n",
      "Epoch 22, Batch 994, LR 0.000042 Loss 5.496730, Accuracy 85.460%\n",
      "Epoch 22, Batch 995, LR 0.000042 Loss 5.497020, Accuracy 85.462%\n",
      "Epoch 22, Batch 996, LR 0.000042 Loss 5.497114, Accuracy 85.463%\n",
      "Epoch 22, Batch 997, LR 0.000042 Loss 5.498074, Accuracy 85.456%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 998, LR 0.000042 Loss 5.498389, Accuracy 85.454%\n",
      "Epoch 22, Batch 999, LR 0.000042 Loss 5.498299, Accuracy 85.452%\n",
      "Epoch 22, Batch 1000, LR 0.000042 Loss 5.497519, Accuracy 85.457%\n",
      "Epoch 22, Batch 1001, LR 0.000042 Loss 5.498074, Accuracy 85.455%\n",
      "Epoch 22, Batch 1002, LR 0.000042 Loss 5.497066, Accuracy 85.456%\n",
      "Epoch 22, Batch 1003, LR 0.000042 Loss 5.497294, Accuracy 85.459%\n",
      "Epoch 22, Batch 1004, LR 0.000042 Loss 5.496291, Accuracy 85.464%\n",
      "Epoch 22, Batch 1005, LR 0.000042 Loss 5.496990, Accuracy 85.459%\n",
      "Epoch 22, Batch 1006, LR 0.000042 Loss 5.495870, Accuracy 85.463%\n",
      "Epoch 22, Batch 1007, LR 0.000042 Loss 5.495821, Accuracy 85.461%\n",
      "Epoch 22, Batch 1008, LR 0.000042 Loss 5.495748, Accuracy 85.465%\n",
      "Epoch 22, Batch 1009, LR 0.000042 Loss 5.495085, Accuracy 85.471%\n",
      "Epoch 22, Batch 1010, LR 0.000042 Loss 5.494852, Accuracy 85.472%\n",
      "Epoch 22, Batch 1011, LR 0.000042 Loss 5.494632, Accuracy 85.474%\n",
      "Epoch 22, Batch 1012, LR 0.000042 Loss 5.493926, Accuracy 85.474%\n",
      "Epoch 22, Batch 1013, LR 0.000042 Loss 5.492948, Accuracy 85.479%\n",
      "Epoch 22, Batch 1014, LR 0.000042 Loss 5.493134, Accuracy 85.476%\n",
      "Epoch 22, Batch 1015, LR 0.000042 Loss 5.493100, Accuracy 85.477%\n",
      "Epoch 22, Batch 1016, LR 0.000042 Loss 5.493004, Accuracy 85.482%\n",
      "Epoch 22, Batch 1017, LR 0.000042 Loss 5.492064, Accuracy 85.485%\n",
      "Epoch 22, Batch 1018, LR 0.000042 Loss 5.492244, Accuracy 85.483%\n",
      "Epoch 22, Batch 1019, LR 0.000042 Loss 5.492807, Accuracy 85.480%\n",
      "Epoch 22, Batch 1020, LR 0.000042 Loss 5.492346, Accuracy 85.483%\n",
      "Epoch 22, Batch 1021, LR 0.000042 Loss 5.492486, Accuracy 85.482%\n",
      "Epoch 22, Batch 1022, LR 0.000042 Loss 5.492702, Accuracy 85.482%\n",
      "Epoch 22, Batch 1023, LR 0.000042 Loss 5.493364, Accuracy 85.478%\n",
      "Epoch 22, Batch 1024, LR 0.000042 Loss 5.493247, Accuracy 85.477%\n",
      "Epoch 22, Batch 1025, LR 0.000042 Loss 5.493973, Accuracy 85.474%\n",
      "Epoch 22, Batch 1026, LR 0.000042 Loss 5.495185, Accuracy 85.470%\n",
      "Epoch 22, Batch 1027, LR 0.000042 Loss 5.495618, Accuracy 85.470%\n",
      "Epoch 22, Batch 1028, LR 0.000042 Loss 5.495207, Accuracy 85.472%\n",
      "Epoch 22, Batch 1029, LR 0.000042 Loss 5.494253, Accuracy 85.478%\n",
      "Epoch 22, Batch 1030, LR 0.000042 Loss 5.494286, Accuracy 85.473%\n",
      "Epoch 22, Batch 1031, LR 0.000042 Loss 5.494267, Accuracy 85.471%\n",
      "Epoch 22, Batch 1032, LR 0.000042 Loss 5.494196, Accuracy 85.469%\n",
      "Epoch 22, Batch 1033, LR 0.000042 Loss 5.495096, Accuracy 85.463%\n",
      "Epoch 22, Batch 1034, LR 0.000042 Loss 5.494963, Accuracy 85.465%\n",
      "Epoch 22, Batch 1035, LR 0.000042 Loss 5.495213, Accuracy 85.464%\n",
      "Epoch 22, Batch 1036, LR 0.000042 Loss 5.495259, Accuracy 85.466%\n",
      "Epoch 22, Batch 1037, LR 0.000042 Loss 5.496010, Accuracy 85.462%\n",
      "Epoch 22, Batch 1038, LR 0.000042 Loss 5.496498, Accuracy 85.460%\n",
      "Epoch 22, Batch 1039, LR 0.000042 Loss 5.496019, Accuracy 85.462%\n",
      "Epoch 22, Batch 1040, LR 0.000042 Loss 5.495937, Accuracy 85.461%\n",
      "Epoch 22, Batch 1041, LR 0.000042 Loss 5.495977, Accuracy 85.464%\n",
      "Epoch 22, Batch 1042, LR 0.000042 Loss 5.495730, Accuracy 85.465%\n",
      "Epoch 22, Batch 1043, LR 0.000042 Loss 5.494836, Accuracy 85.471%\n",
      "Epoch 22, Batch 1044, LR 0.000042 Loss 5.493799, Accuracy 85.475%\n",
      "Epoch 22, Batch 1045, LR 0.000042 Loss 5.493590, Accuracy 85.478%\n",
      "Epoch 22, Batch 1046, LR 0.000042 Loss 5.493695, Accuracy 85.479%\n",
      "Epoch 22, Batch 1047, LR 0.000042 Loss 5.494213, Accuracy 85.477%\n",
      "Epoch 22, Loss (train set) 5.494213, Accuracy (train set) 85.477%\n",
      "Epoch 23, Batch 1, LR 0.000042 Loss 5.533973, Accuracy 85.156%\n",
      "Epoch 23, Batch 2, LR 0.000042 Loss 5.409784, Accuracy 87.109%\n",
      "Epoch 23, Batch 3, LR 0.000042 Loss 5.163951, Accuracy 87.760%\n",
      "Epoch 23, Batch 4, LR 0.000042 Loss 4.967192, Accuracy 88.867%\n",
      "Epoch 23, Batch 5, LR 0.000042 Loss 5.207569, Accuracy 86.719%\n",
      "Epoch 23, Batch 6, LR 0.000042 Loss 5.274278, Accuracy 85.807%\n",
      "Epoch 23, Batch 7, LR 0.000042 Loss 5.262029, Accuracy 85.826%\n",
      "Epoch 23, Batch 8, LR 0.000042 Loss 5.238386, Accuracy 86.133%\n",
      "Epoch 23, Batch 9, LR 0.000042 Loss 5.185063, Accuracy 86.458%\n",
      "Epoch 23, Batch 10, LR 0.000042 Loss 5.201788, Accuracy 86.484%\n",
      "Epoch 23, Batch 11, LR 0.000042 Loss 5.231104, Accuracy 86.577%\n",
      "Epoch 23, Batch 12, LR 0.000042 Loss 5.273243, Accuracy 86.198%\n",
      "Epoch 23, Batch 13, LR 0.000042 Loss 5.241430, Accuracy 86.478%\n",
      "Epoch 23, Batch 14, LR 0.000042 Loss 5.268596, Accuracy 86.607%\n",
      "Epoch 23, Batch 15, LR 0.000042 Loss 5.273390, Accuracy 86.667%\n",
      "Epoch 23, Batch 16, LR 0.000042 Loss 5.268309, Accuracy 86.768%\n",
      "Epoch 23, Batch 17, LR 0.000042 Loss 5.280875, Accuracy 86.765%\n",
      "Epoch 23, Batch 18, LR 0.000042 Loss 5.313659, Accuracy 86.458%\n",
      "Epoch 23, Batch 19, LR 0.000042 Loss 5.297769, Accuracy 86.554%\n",
      "Epoch 23, Batch 20, LR 0.000042 Loss 5.316641, Accuracy 86.523%\n",
      "Epoch 23, Batch 21, LR 0.000042 Loss 5.298038, Accuracy 86.533%\n",
      "Epoch 23, Batch 22, LR 0.000042 Loss 5.297982, Accuracy 86.435%\n",
      "Epoch 23, Batch 23, LR 0.000042 Loss 5.298418, Accuracy 86.345%\n",
      "Epoch 23, Batch 24, LR 0.000042 Loss 5.312572, Accuracy 86.230%\n",
      "Epoch 23, Batch 25, LR 0.000042 Loss 5.326375, Accuracy 86.219%\n",
      "Epoch 23, Batch 26, LR 0.000042 Loss 5.322284, Accuracy 86.268%\n",
      "Epoch 23, Batch 27, LR 0.000042 Loss 5.350498, Accuracy 86.111%\n",
      "Epoch 23, Batch 28, LR 0.000042 Loss 5.335307, Accuracy 86.217%\n",
      "Epoch 23, Batch 29, LR 0.000042 Loss 5.341328, Accuracy 86.342%\n",
      "Epoch 23, Batch 30, LR 0.000042 Loss 5.309189, Accuracy 86.510%\n",
      "Epoch 23, Batch 31, LR 0.000042 Loss 5.300292, Accuracy 86.542%\n",
      "Epoch 23, Batch 32, LR 0.000042 Loss 5.311078, Accuracy 86.377%\n",
      "Epoch 23, Batch 33, LR 0.000042 Loss 5.323796, Accuracy 86.269%\n",
      "Epoch 23, Batch 34, LR 0.000042 Loss 5.322975, Accuracy 86.144%\n",
      "Epoch 23, Batch 35, LR 0.000042 Loss 5.309772, Accuracy 86.362%\n",
      "Epoch 23, Batch 36, LR 0.000042 Loss 5.316688, Accuracy 86.437%\n",
      "Epoch 23, Batch 37, LR 0.000042 Loss 5.301178, Accuracy 86.592%\n",
      "Epoch 23, Batch 38, LR 0.000042 Loss 5.284772, Accuracy 86.719%\n",
      "Epoch 23, Batch 39, LR 0.000042 Loss 5.309709, Accuracy 86.558%\n",
      "Epoch 23, Batch 40, LR 0.000042 Loss 5.332778, Accuracy 86.484%\n",
      "Epoch 23, Batch 41, LR 0.000042 Loss 5.338093, Accuracy 86.414%\n",
      "Epoch 23, Batch 42, LR 0.000042 Loss 5.345921, Accuracy 86.347%\n",
      "Epoch 23, Batch 43, LR 0.000042 Loss 5.349114, Accuracy 86.337%\n",
      "Epoch 23, Batch 44, LR 0.000042 Loss 5.355472, Accuracy 86.328%\n",
      "Epoch 23, Batch 45, LR 0.000042 Loss 5.347274, Accuracy 86.354%\n",
      "Epoch 23, Batch 46, LR 0.000042 Loss 5.347751, Accuracy 86.396%\n",
      "Epoch 23, Batch 47, LR 0.000042 Loss 5.354648, Accuracy 86.370%\n",
      "Epoch 23, Batch 48, LR 0.000042 Loss 5.357116, Accuracy 86.312%\n",
      "Epoch 23, Batch 49, LR 0.000042 Loss 5.344477, Accuracy 86.352%\n",
      "Epoch 23, Batch 50, LR 0.000042 Loss 5.343399, Accuracy 86.344%\n",
      "Epoch 23, Batch 51, LR 0.000042 Loss 5.345550, Accuracy 86.305%\n",
      "Epoch 23, Batch 52, LR 0.000042 Loss 5.347819, Accuracy 86.283%\n",
      "Epoch 23, Batch 53, LR 0.000042 Loss 5.365324, Accuracy 86.159%\n",
      "Epoch 23, Batch 54, LR 0.000042 Loss 5.355794, Accuracy 86.256%\n",
      "Epoch 23, Batch 55, LR 0.000042 Loss 5.344749, Accuracy 86.307%\n",
      "Epoch 23, Batch 56, LR 0.000042 Loss 5.333924, Accuracy 86.370%\n",
      "Epoch 23, Batch 57, LR 0.000042 Loss 5.338844, Accuracy 86.335%\n",
      "Epoch 23, Batch 58, LR 0.000042 Loss 5.333737, Accuracy 86.355%\n",
      "Epoch 23, Batch 59, LR 0.000042 Loss 5.348131, Accuracy 86.282%\n",
      "Epoch 23, Batch 60, LR 0.000042 Loss 5.338962, Accuracy 86.289%\n",
      "Epoch 23, Batch 61, LR 0.000042 Loss 5.339422, Accuracy 86.219%\n",
      "Epoch 23, Batch 62, LR 0.000042 Loss 5.354287, Accuracy 86.177%\n",
      "Epoch 23, Batch 63, LR 0.000042 Loss 5.356929, Accuracy 86.210%\n",
      "Epoch 23, Batch 64, LR 0.000042 Loss 5.367131, Accuracy 86.157%\n",
      "Epoch 23, Batch 65, LR 0.000042 Loss 5.356407, Accuracy 86.154%\n",
      "Epoch 23, Batch 66, LR 0.000042 Loss 5.354639, Accuracy 86.186%\n",
      "Epoch 23, Batch 67, LR 0.000042 Loss 5.359981, Accuracy 86.171%\n",
      "Epoch 23, Batch 68, LR 0.000042 Loss 5.368208, Accuracy 86.133%\n",
      "Epoch 23, Batch 69, LR 0.000042 Loss 5.361367, Accuracy 86.153%\n",
      "Epoch 23, Batch 70, LR 0.000041 Loss 5.373640, Accuracy 86.083%\n",
      "Epoch 23, Batch 71, LR 0.000041 Loss 5.364964, Accuracy 86.070%\n",
      "Epoch 23, Batch 72, LR 0.000041 Loss 5.361865, Accuracy 86.122%\n",
      "Epoch 23, Batch 73, LR 0.000041 Loss 5.354346, Accuracy 86.184%\n",
      "Epoch 23, Batch 74, LR 0.000041 Loss 5.354146, Accuracy 86.201%\n",
      "Epoch 23, Batch 75, LR 0.000041 Loss 5.355360, Accuracy 86.229%\n",
      "Epoch 23, Batch 76, LR 0.000041 Loss 5.357996, Accuracy 86.184%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 77, LR 0.000041 Loss 5.352690, Accuracy 86.222%\n",
      "Epoch 23, Batch 78, LR 0.000041 Loss 5.351902, Accuracy 86.168%\n",
      "Epoch 23, Batch 79, LR 0.000041 Loss 5.345169, Accuracy 86.234%\n",
      "Epoch 23, Batch 80, LR 0.000041 Loss 5.351060, Accuracy 86.221%\n",
      "Epoch 23, Batch 81, LR 0.000041 Loss 5.348533, Accuracy 86.246%\n",
      "Epoch 23, Batch 82, LR 0.000041 Loss 5.347015, Accuracy 86.233%\n",
      "Epoch 23, Batch 83, LR 0.000041 Loss 5.354210, Accuracy 86.182%\n",
      "Epoch 23, Batch 84, LR 0.000041 Loss 5.354988, Accuracy 86.161%\n",
      "Epoch 23, Batch 85, LR 0.000041 Loss 5.361823, Accuracy 86.094%\n",
      "Epoch 23, Batch 86, LR 0.000041 Loss 5.361076, Accuracy 86.128%\n",
      "Epoch 23, Batch 87, LR 0.000041 Loss 5.358907, Accuracy 86.099%\n",
      "Epoch 23, Batch 88, LR 0.000041 Loss 5.363156, Accuracy 86.080%\n",
      "Epoch 23, Batch 89, LR 0.000041 Loss 5.371299, Accuracy 86.017%\n",
      "Epoch 23, Batch 90, LR 0.000041 Loss 5.372692, Accuracy 86.007%\n",
      "Epoch 23, Batch 91, LR 0.000041 Loss 5.373452, Accuracy 86.023%\n",
      "Epoch 23, Batch 92, LR 0.000041 Loss 5.374396, Accuracy 86.031%\n",
      "Epoch 23, Batch 93, LR 0.000041 Loss 5.374332, Accuracy 86.005%\n",
      "Epoch 23, Batch 94, LR 0.000041 Loss 5.382512, Accuracy 85.987%\n",
      "Epoch 23, Batch 95, LR 0.000041 Loss 5.386963, Accuracy 85.979%\n",
      "Epoch 23, Batch 96, LR 0.000041 Loss 5.386893, Accuracy 85.986%\n",
      "Epoch 23, Batch 97, LR 0.000041 Loss 5.381478, Accuracy 86.042%\n",
      "Epoch 23, Batch 98, LR 0.000041 Loss 5.374316, Accuracy 86.057%\n",
      "Epoch 23, Batch 99, LR 0.000041 Loss 5.375143, Accuracy 86.064%\n",
      "Epoch 23, Batch 100, LR 0.000041 Loss 5.387336, Accuracy 86.000%\n",
      "Epoch 23, Batch 101, LR 0.000041 Loss 5.390627, Accuracy 85.999%\n",
      "Epoch 23, Batch 102, LR 0.000041 Loss 5.395484, Accuracy 85.960%\n",
      "Epoch 23, Batch 103, LR 0.000041 Loss 5.386978, Accuracy 85.983%\n",
      "Epoch 23, Batch 104, LR 0.000041 Loss 5.381833, Accuracy 86.035%\n",
      "Epoch 23, Batch 105, LR 0.000041 Loss 5.379860, Accuracy 86.019%\n",
      "Epoch 23, Batch 106, LR 0.000041 Loss 5.377214, Accuracy 86.026%\n",
      "Epoch 23, Batch 107, LR 0.000041 Loss 5.376774, Accuracy 86.018%\n",
      "Epoch 23, Batch 108, LR 0.000041 Loss 5.379230, Accuracy 85.966%\n",
      "Epoch 23, Batch 109, LR 0.000041 Loss 5.389795, Accuracy 85.894%\n",
      "Epoch 23, Batch 110, LR 0.000041 Loss 5.393658, Accuracy 85.895%\n",
      "Epoch 23, Batch 111, LR 0.000041 Loss 5.384363, Accuracy 85.938%\n",
      "Epoch 23, Batch 112, LR 0.000041 Loss 5.387892, Accuracy 85.951%\n",
      "Epoch 23, Batch 113, LR 0.000041 Loss 5.387507, Accuracy 85.944%\n",
      "Epoch 23, Batch 114, LR 0.000041 Loss 5.390253, Accuracy 85.951%\n",
      "Epoch 23, Batch 115, LR 0.000041 Loss 5.394520, Accuracy 85.910%\n",
      "Epoch 23, Batch 116, LR 0.000041 Loss 5.395098, Accuracy 85.897%\n",
      "Epoch 23, Batch 117, LR 0.000041 Loss 5.393884, Accuracy 85.911%\n",
      "Epoch 23, Batch 118, LR 0.000041 Loss 5.392495, Accuracy 85.871%\n",
      "Epoch 23, Batch 119, LR 0.000041 Loss 5.393193, Accuracy 85.819%\n",
      "Epoch 23, Batch 120, LR 0.000041 Loss 5.391056, Accuracy 85.794%\n",
      "Epoch 23, Batch 121, LR 0.000041 Loss 5.395461, Accuracy 85.789%\n",
      "Epoch 23, Batch 122, LR 0.000041 Loss 5.395929, Accuracy 85.816%\n",
      "Epoch 23, Batch 123, LR 0.000041 Loss 5.397572, Accuracy 85.804%\n",
      "Epoch 23, Batch 124, LR 0.000041 Loss 5.396278, Accuracy 85.824%\n",
      "Epoch 23, Batch 125, LR 0.000041 Loss 5.391112, Accuracy 85.862%\n",
      "Epoch 23, Batch 126, LR 0.000041 Loss 5.389047, Accuracy 85.838%\n",
      "Epoch 23, Batch 127, LR 0.000041 Loss 5.392719, Accuracy 85.821%\n",
      "Epoch 23, Batch 128, LR 0.000041 Loss 5.391246, Accuracy 85.828%\n",
      "Epoch 23, Batch 129, LR 0.000041 Loss 5.390888, Accuracy 85.841%\n",
      "Epoch 23, Batch 130, LR 0.000041 Loss 5.393028, Accuracy 85.859%\n",
      "Epoch 23, Batch 131, LR 0.000041 Loss 5.394501, Accuracy 85.878%\n",
      "Epoch 23, Batch 132, LR 0.000041 Loss 5.395416, Accuracy 85.896%\n",
      "Epoch 23, Batch 133, LR 0.000041 Loss 5.397297, Accuracy 85.855%\n",
      "Epoch 23, Batch 134, LR 0.000041 Loss 5.402328, Accuracy 85.803%\n",
      "Epoch 23, Batch 135, LR 0.000041 Loss 5.405447, Accuracy 85.804%\n",
      "Epoch 23, Batch 136, LR 0.000041 Loss 5.414436, Accuracy 85.754%\n",
      "Epoch 23, Batch 137, LR 0.000041 Loss 5.413509, Accuracy 85.732%\n",
      "Epoch 23, Batch 138, LR 0.000041 Loss 5.416952, Accuracy 85.739%\n",
      "Epoch 23, Batch 139, LR 0.000041 Loss 5.415193, Accuracy 85.763%\n",
      "Epoch 23, Batch 140, LR 0.000041 Loss 5.416564, Accuracy 85.731%\n",
      "Epoch 23, Batch 141, LR 0.000041 Loss 5.417883, Accuracy 85.749%\n",
      "Epoch 23, Batch 142, LR 0.000041 Loss 5.417438, Accuracy 85.745%\n",
      "Epoch 23, Batch 143, LR 0.000041 Loss 5.414029, Accuracy 85.763%\n",
      "Epoch 23, Batch 144, LR 0.000041 Loss 5.414359, Accuracy 85.748%\n",
      "Epoch 23, Batch 145, LR 0.000041 Loss 5.416752, Accuracy 85.717%\n",
      "Epoch 23, Batch 146, LR 0.000041 Loss 5.413290, Accuracy 85.734%\n",
      "Epoch 23, Batch 147, LR 0.000041 Loss 5.411731, Accuracy 85.762%\n",
      "Epoch 23, Batch 148, LR 0.000041 Loss 5.418591, Accuracy 85.732%\n",
      "Epoch 23, Batch 149, LR 0.000041 Loss 5.419793, Accuracy 85.717%\n",
      "Epoch 23, Batch 150, LR 0.000041 Loss 5.423787, Accuracy 85.672%\n",
      "Epoch 23, Batch 151, LR 0.000041 Loss 5.419625, Accuracy 85.705%\n",
      "Epoch 23, Batch 152, LR 0.000041 Loss 5.421166, Accuracy 85.686%\n",
      "Epoch 23, Batch 153, LR 0.000041 Loss 5.426283, Accuracy 85.631%\n",
      "Epoch 23, Batch 154, LR 0.000041 Loss 5.419042, Accuracy 85.669%\n",
      "Epoch 23, Batch 155, LR 0.000041 Loss 5.415514, Accuracy 85.640%\n",
      "Epoch 23, Batch 156, LR 0.000041 Loss 5.414403, Accuracy 85.667%\n",
      "Epoch 23, Batch 157, LR 0.000041 Loss 5.410885, Accuracy 85.709%\n",
      "Epoch 23, Batch 158, LR 0.000041 Loss 5.408065, Accuracy 85.725%\n",
      "Epoch 23, Batch 159, LR 0.000041 Loss 5.408307, Accuracy 85.746%\n",
      "Epoch 23, Batch 160, LR 0.000041 Loss 5.406540, Accuracy 85.747%\n",
      "Epoch 23, Batch 161, LR 0.000041 Loss 5.409799, Accuracy 85.734%\n",
      "Epoch 23, Batch 162, LR 0.000041 Loss 5.410326, Accuracy 85.730%\n",
      "Epoch 23, Batch 163, LR 0.000041 Loss 5.405824, Accuracy 85.755%\n",
      "Epoch 23, Batch 164, LR 0.000041 Loss 5.402237, Accuracy 85.795%\n",
      "Epoch 23, Batch 165, LR 0.000041 Loss 5.403101, Accuracy 85.777%\n",
      "Epoch 23, Batch 166, LR 0.000041 Loss 5.406256, Accuracy 85.754%\n",
      "Epoch 23, Batch 167, LR 0.000041 Loss 5.405777, Accuracy 85.760%\n",
      "Epoch 23, Batch 168, LR 0.000041 Loss 5.411354, Accuracy 85.714%\n",
      "Epoch 23, Batch 169, LR 0.000041 Loss 5.408701, Accuracy 85.729%\n",
      "Epoch 23, Batch 170, LR 0.000041 Loss 5.405363, Accuracy 85.735%\n",
      "Epoch 23, Batch 171, LR 0.000041 Loss 5.410904, Accuracy 85.718%\n",
      "Epoch 23, Batch 172, LR 0.000041 Loss 5.407824, Accuracy 85.733%\n",
      "Epoch 23, Batch 173, LR 0.000041 Loss 5.404730, Accuracy 85.748%\n",
      "Epoch 23, Batch 174, LR 0.000041 Loss 5.410352, Accuracy 85.726%\n",
      "Epoch 23, Batch 175, LR 0.000041 Loss 5.412255, Accuracy 85.719%\n",
      "Epoch 23, Batch 176, LR 0.000041 Loss 5.411509, Accuracy 85.738%\n",
      "Epoch 23, Batch 177, LR 0.000041 Loss 5.413119, Accuracy 85.717%\n",
      "Epoch 23, Batch 178, LR 0.000041 Loss 5.411311, Accuracy 85.731%\n",
      "Epoch 23, Batch 179, LR 0.000041 Loss 5.406829, Accuracy 85.759%\n",
      "Epoch 23, Batch 180, LR 0.000041 Loss 5.407133, Accuracy 85.755%\n",
      "Epoch 23, Batch 181, LR 0.000041 Loss 5.404703, Accuracy 85.769%\n",
      "Epoch 23, Batch 182, LR 0.000041 Loss 5.403374, Accuracy 85.774%\n",
      "Epoch 23, Batch 183, LR 0.000041 Loss 5.404623, Accuracy 85.771%\n",
      "Epoch 23, Batch 184, LR 0.000041 Loss 5.403772, Accuracy 85.793%\n",
      "Epoch 23, Batch 185, LR 0.000041 Loss 5.404547, Accuracy 85.819%\n",
      "Epoch 23, Batch 186, LR 0.000041 Loss 5.407591, Accuracy 85.795%\n",
      "Epoch 23, Batch 187, LR 0.000041 Loss 5.413022, Accuracy 85.770%\n",
      "Epoch 23, Batch 188, LR 0.000041 Loss 5.414976, Accuracy 85.759%\n",
      "Epoch 23, Batch 189, LR 0.000041 Loss 5.417254, Accuracy 85.747%\n",
      "Epoch 23, Batch 190, LR 0.000041 Loss 5.417381, Accuracy 85.740%\n",
      "Epoch 23, Batch 191, LR 0.000041 Loss 5.415526, Accuracy 85.753%\n",
      "Epoch 23, Batch 192, LR 0.000041 Loss 5.412255, Accuracy 85.758%\n",
      "Epoch 23, Batch 193, LR 0.000041 Loss 5.411733, Accuracy 85.763%\n",
      "Epoch 23, Batch 194, LR 0.000041 Loss 5.414902, Accuracy 85.748%\n",
      "Epoch 23, Batch 195, LR 0.000041 Loss 5.416934, Accuracy 85.769%\n",
      "Epoch 23, Batch 196, LR 0.000041 Loss 5.420378, Accuracy 85.758%\n",
      "Epoch 23, Batch 197, LR 0.000041 Loss 5.419769, Accuracy 85.755%\n",
      "Epoch 23, Batch 198, LR 0.000041 Loss 5.417947, Accuracy 85.780%\n",
      "Epoch 23, Batch 199, LR 0.000041 Loss 5.416609, Accuracy 85.765%\n",
      "Epoch 23, Batch 200, LR 0.000041 Loss 5.416051, Accuracy 85.773%\n",
      "Epoch 23, Batch 201, LR 0.000041 Loss 5.415122, Accuracy 85.766%\n",
      "Epoch 23, Batch 202, LR 0.000041 Loss 5.416714, Accuracy 85.783%\n",
      "Epoch 23, Batch 203, LR 0.000041 Loss 5.414763, Accuracy 85.791%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 204, LR 0.000041 Loss 5.409784, Accuracy 85.811%\n",
      "Epoch 23, Batch 205, LR 0.000041 Loss 5.405289, Accuracy 85.823%\n",
      "Epoch 23, Batch 206, LR 0.000041 Loss 5.403319, Accuracy 85.839%\n",
      "Epoch 23, Batch 207, LR 0.000041 Loss 5.402034, Accuracy 85.854%\n",
      "Epoch 23, Batch 208, LR 0.000041 Loss 5.402453, Accuracy 85.859%\n",
      "Epoch 23, Batch 209, LR 0.000041 Loss 5.400067, Accuracy 85.874%\n",
      "Epoch 23, Batch 210, LR 0.000041 Loss 5.397608, Accuracy 85.878%\n",
      "Epoch 23, Batch 211, LR 0.000041 Loss 5.395033, Accuracy 85.886%\n",
      "Epoch 23, Batch 212, LR 0.000041 Loss 5.392612, Accuracy 85.923%\n",
      "Epoch 23, Batch 213, LR 0.000041 Loss 5.392632, Accuracy 85.934%\n",
      "Epoch 23, Batch 214, LR 0.000041 Loss 5.388722, Accuracy 85.956%\n",
      "Epoch 23, Batch 215, LR 0.000041 Loss 5.390644, Accuracy 85.948%\n",
      "Epoch 23, Batch 216, LR 0.000041 Loss 5.388286, Accuracy 85.956%\n",
      "Epoch 23, Batch 217, LR 0.000041 Loss 5.385332, Accuracy 85.974%\n",
      "Epoch 23, Batch 218, LR 0.000041 Loss 5.387829, Accuracy 85.966%\n",
      "Epoch 23, Batch 219, LR 0.000041 Loss 5.387320, Accuracy 85.984%\n",
      "Epoch 23, Batch 220, LR 0.000041 Loss 5.386228, Accuracy 85.994%\n",
      "Epoch 23, Batch 221, LR 0.000041 Loss 5.386208, Accuracy 85.987%\n",
      "Epoch 23, Batch 222, LR 0.000041 Loss 5.388210, Accuracy 85.973%\n",
      "Epoch 23, Batch 223, LR 0.000041 Loss 5.385450, Accuracy 85.990%\n",
      "Epoch 23, Batch 224, LR 0.000041 Loss 5.385286, Accuracy 85.990%\n",
      "Epoch 23, Batch 225, LR 0.000041 Loss 5.386776, Accuracy 85.969%\n",
      "Epoch 23, Batch 226, LR 0.000041 Loss 5.382455, Accuracy 85.989%\n",
      "Epoch 23, Batch 227, LR 0.000041 Loss 5.386188, Accuracy 85.986%\n",
      "Epoch 23, Batch 228, LR 0.000041 Loss 5.384167, Accuracy 85.996%\n",
      "Epoch 23, Batch 229, LR 0.000041 Loss 5.384761, Accuracy 86.009%\n",
      "Epoch 23, Batch 230, LR 0.000041 Loss 5.383570, Accuracy 86.016%\n",
      "Epoch 23, Batch 231, LR 0.000041 Loss 5.382282, Accuracy 86.015%\n",
      "Epoch 23, Batch 232, LR 0.000041 Loss 5.385062, Accuracy 86.012%\n",
      "Epoch 23, Batch 233, LR 0.000041 Loss 5.380998, Accuracy 86.035%\n",
      "Epoch 23, Batch 234, LR 0.000041 Loss 5.383566, Accuracy 86.024%\n",
      "Epoch 23, Batch 235, LR 0.000041 Loss 5.382364, Accuracy 86.024%\n",
      "Epoch 23, Batch 236, LR 0.000041 Loss 5.384680, Accuracy 85.994%\n",
      "Epoch 23, Batch 237, LR 0.000041 Loss 5.387926, Accuracy 85.967%\n",
      "Epoch 23, Batch 238, LR 0.000041 Loss 5.386024, Accuracy 85.983%\n",
      "Epoch 23, Batch 239, LR 0.000041 Loss 5.385984, Accuracy 85.983%\n",
      "Epoch 23, Batch 240, LR 0.000041 Loss 5.386746, Accuracy 85.977%\n",
      "Epoch 23, Batch 241, LR 0.000041 Loss 5.386674, Accuracy 85.980%\n",
      "Epoch 23, Batch 242, LR 0.000041 Loss 5.386128, Accuracy 85.983%\n",
      "Epoch 23, Batch 243, LR 0.000041 Loss 5.384018, Accuracy 85.989%\n",
      "Epoch 23, Batch 244, LR 0.000041 Loss 5.381584, Accuracy 86.002%\n",
      "Epoch 23, Batch 245, LR 0.000041 Loss 5.380749, Accuracy 85.995%\n",
      "Epoch 23, Batch 246, LR 0.000041 Loss 5.381846, Accuracy 85.985%\n",
      "Epoch 23, Batch 247, LR 0.000041 Loss 5.382440, Accuracy 86.001%\n",
      "Epoch 23, Batch 248, LR 0.000041 Loss 5.385715, Accuracy 85.985%\n",
      "Epoch 23, Batch 249, LR 0.000041 Loss 5.386084, Accuracy 85.985%\n",
      "Epoch 23, Batch 250, LR 0.000041 Loss 5.386905, Accuracy 85.978%\n",
      "Epoch 23, Batch 251, LR 0.000041 Loss 5.388715, Accuracy 85.969%\n",
      "Epoch 23, Batch 252, LR 0.000041 Loss 5.388826, Accuracy 85.959%\n",
      "Epoch 23, Batch 253, LR 0.000041 Loss 5.391829, Accuracy 85.953%\n",
      "Epoch 23, Batch 254, LR 0.000041 Loss 5.391937, Accuracy 85.962%\n",
      "Epoch 23, Batch 255, LR 0.000041 Loss 5.391701, Accuracy 85.974%\n",
      "Epoch 23, Batch 256, LR 0.000041 Loss 5.394719, Accuracy 85.950%\n",
      "Epoch 23, Batch 257, LR 0.000041 Loss 5.395837, Accuracy 85.968%\n",
      "Epoch 23, Batch 258, LR 0.000041 Loss 5.395741, Accuracy 85.968%\n",
      "Epoch 23, Batch 259, LR 0.000041 Loss 5.395846, Accuracy 85.971%\n",
      "Epoch 23, Batch 260, LR 0.000041 Loss 5.393837, Accuracy 85.983%\n",
      "Epoch 23, Batch 261, LR 0.000041 Loss 5.393249, Accuracy 85.982%\n",
      "Epoch 23, Batch 262, LR 0.000041 Loss 5.391134, Accuracy 85.985%\n",
      "Epoch 23, Batch 263, LR 0.000041 Loss 5.390709, Accuracy 85.982%\n",
      "Epoch 23, Batch 264, LR 0.000041 Loss 5.387626, Accuracy 85.991%\n",
      "Epoch 23, Batch 265, LR 0.000041 Loss 5.388753, Accuracy 85.979%\n",
      "Epoch 23, Batch 266, LR 0.000041 Loss 5.388680, Accuracy 85.973%\n",
      "Epoch 23, Batch 267, LR 0.000041 Loss 5.392064, Accuracy 85.946%\n",
      "Epoch 23, Batch 268, LR 0.000041 Loss 5.389921, Accuracy 85.952%\n",
      "Epoch 23, Batch 269, LR 0.000041 Loss 5.387578, Accuracy 85.955%\n",
      "Epoch 23, Batch 270, LR 0.000041 Loss 5.389489, Accuracy 85.952%\n",
      "Epoch 23, Batch 271, LR 0.000041 Loss 5.391308, Accuracy 85.940%\n",
      "Epoch 23, Batch 272, LR 0.000041 Loss 5.387846, Accuracy 85.943%\n",
      "Epoch 23, Batch 273, LR 0.000041 Loss 5.388667, Accuracy 85.940%\n",
      "Epoch 23, Batch 274, LR 0.000041 Loss 5.388401, Accuracy 85.952%\n",
      "Epoch 23, Batch 275, LR 0.000041 Loss 5.387091, Accuracy 85.957%\n",
      "Epoch 23, Batch 276, LR 0.000041 Loss 5.388826, Accuracy 85.952%\n",
      "Epoch 23, Batch 277, LR 0.000041 Loss 5.391736, Accuracy 85.935%\n",
      "Epoch 23, Batch 278, LR 0.000041 Loss 5.393505, Accuracy 85.932%\n",
      "Epoch 23, Batch 279, LR 0.000041 Loss 5.392938, Accuracy 85.940%\n",
      "Epoch 23, Batch 280, LR 0.000041 Loss 5.392956, Accuracy 85.943%\n",
      "Epoch 23, Batch 281, LR 0.000041 Loss 5.392479, Accuracy 85.957%\n",
      "Epoch 23, Batch 282, LR 0.000041 Loss 5.394951, Accuracy 85.943%\n",
      "Epoch 23, Batch 283, LR 0.000041 Loss 5.394896, Accuracy 85.932%\n",
      "Epoch 23, Batch 284, LR 0.000041 Loss 5.396099, Accuracy 85.921%\n",
      "Epoch 23, Batch 285, LR 0.000041 Loss 5.394084, Accuracy 85.929%\n",
      "Epoch 23, Batch 286, LR 0.000041 Loss 5.393608, Accuracy 85.935%\n",
      "Epoch 23, Batch 287, LR 0.000041 Loss 5.394494, Accuracy 85.924%\n",
      "Epoch 23, Batch 288, LR 0.000041 Loss 5.392669, Accuracy 85.927%\n",
      "Epoch 23, Batch 289, LR 0.000041 Loss 5.390410, Accuracy 85.946%\n",
      "Epoch 23, Batch 290, LR 0.000041 Loss 5.390837, Accuracy 85.943%\n",
      "Epoch 23, Batch 291, LR 0.000041 Loss 5.390548, Accuracy 85.951%\n",
      "Epoch 23, Batch 292, LR 0.000041 Loss 5.390275, Accuracy 85.951%\n",
      "Epoch 23, Batch 293, LR 0.000041 Loss 5.390180, Accuracy 85.948%\n",
      "Epoch 23, Batch 294, LR 0.000041 Loss 5.391527, Accuracy 85.938%\n",
      "Epoch 23, Batch 295, LR 0.000041 Loss 5.389224, Accuracy 85.943%\n",
      "Epoch 23, Batch 296, LR 0.000041 Loss 5.388888, Accuracy 85.943%\n",
      "Epoch 23, Batch 297, LR 0.000041 Loss 5.389784, Accuracy 85.943%\n",
      "Epoch 23, Batch 298, LR 0.000041 Loss 5.391309, Accuracy 85.940%\n",
      "Epoch 23, Batch 299, LR 0.000041 Loss 5.391485, Accuracy 85.945%\n",
      "Epoch 23, Batch 300, LR 0.000041 Loss 5.393059, Accuracy 85.932%\n",
      "Epoch 23, Batch 301, LR 0.000041 Loss 5.393817, Accuracy 85.938%\n",
      "Epoch 23, Batch 302, LR 0.000041 Loss 5.393791, Accuracy 85.930%\n",
      "Epoch 23, Batch 303, LR 0.000041 Loss 5.394555, Accuracy 85.930%\n",
      "Epoch 23, Batch 304, LR 0.000041 Loss 5.394075, Accuracy 85.932%\n",
      "Epoch 23, Batch 305, LR 0.000041 Loss 5.396308, Accuracy 85.932%\n",
      "Epoch 23, Batch 306, LR 0.000041 Loss 5.398843, Accuracy 85.930%\n",
      "Epoch 23, Batch 307, LR 0.000041 Loss 5.401269, Accuracy 85.917%\n",
      "Epoch 23, Batch 308, LR 0.000041 Loss 5.400589, Accuracy 85.930%\n",
      "Epoch 23, Batch 309, LR 0.000041 Loss 5.400175, Accuracy 85.938%\n",
      "Epoch 23, Batch 310, LR 0.000041 Loss 5.400574, Accuracy 85.940%\n",
      "Epoch 23, Batch 311, LR 0.000041 Loss 5.400077, Accuracy 85.945%\n",
      "Epoch 23, Batch 312, LR 0.000041 Loss 5.401639, Accuracy 85.938%\n",
      "Epoch 23, Batch 313, LR 0.000041 Loss 5.401464, Accuracy 85.935%\n",
      "Epoch 23, Batch 314, LR 0.000041 Loss 5.401511, Accuracy 85.935%\n",
      "Epoch 23, Batch 315, LR 0.000041 Loss 5.400619, Accuracy 85.928%\n",
      "Epoch 23, Batch 316, LR 0.000041 Loss 5.400077, Accuracy 85.923%\n",
      "Epoch 23, Batch 317, LR 0.000041 Loss 5.397652, Accuracy 85.930%\n",
      "Epoch 23, Batch 318, LR 0.000041 Loss 5.396813, Accuracy 85.940%\n",
      "Epoch 23, Batch 319, LR 0.000041 Loss 5.398711, Accuracy 85.920%\n",
      "Epoch 23, Batch 320, LR 0.000041 Loss 5.397616, Accuracy 85.923%\n",
      "Epoch 23, Batch 321, LR 0.000041 Loss 5.396833, Accuracy 85.923%\n",
      "Epoch 23, Batch 322, LR 0.000041 Loss 5.397303, Accuracy 85.916%\n",
      "Epoch 23, Batch 323, LR 0.000041 Loss 5.397637, Accuracy 85.911%\n",
      "Epoch 23, Batch 324, LR 0.000041 Loss 5.398303, Accuracy 85.913%\n",
      "Epoch 23, Batch 325, LR 0.000041 Loss 5.400512, Accuracy 85.892%\n",
      "Epoch 23, Batch 326, LR 0.000041 Loss 5.400183, Accuracy 85.890%\n",
      "Epoch 23, Batch 327, LR 0.000041 Loss 5.401080, Accuracy 85.899%\n",
      "Epoch 23, Batch 328, LR 0.000040 Loss 5.401041, Accuracy 85.899%\n",
      "Epoch 23, Batch 329, LR 0.000040 Loss 5.400275, Accuracy 85.907%\n",
      "Epoch 23, Batch 330, LR 0.000040 Loss 5.399828, Accuracy 85.909%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 331, LR 0.000040 Loss 5.398577, Accuracy 85.919%\n",
      "Epoch 23, Batch 332, LR 0.000040 Loss 5.397009, Accuracy 85.923%\n",
      "Epoch 23, Batch 333, LR 0.000040 Loss 5.398215, Accuracy 85.909%\n",
      "Epoch 23, Batch 334, LR 0.000040 Loss 5.397072, Accuracy 85.919%\n",
      "Epoch 23, Batch 335, LR 0.000040 Loss 5.397661, Accuracy 85.919%\n",
      "Epoch 23, Batch 336, LR 0.000040 Loss 5.397427, Accuracy 85.931%\n",
      "Epoch 23, Batch 337, LR 0.000040 Loss 5.398595, Accuracy 85.926%\n",
      "Epoch 23, Batch 338, LR 0.000040 Loss 5.402228, Accuracy 85.894%\n",
      "Epoch 23, Batch 339, LR 0.000040 Loss 5.401935, Accuracy 85.896%\n",
      "Epoch 23, Batch 340, LR 0.000040 Loss 5.401287, Accuracy 85.898%\n",
      "Epoch 23, Batch 341, LR 0.000040 Loss 5.402138, Accuracy 85.908%\n",
      "Epoch 23, Batch 342, LR 0.000040 Loss 5.400599, Accuracy 85.910%\n",
      "Epoch 23, Batch 343, LR 0.000040 Loss 5.401208, Accuracy 85.910%\n",
      "Epoch 23, Batch 344, LR 0.000040 Loss 5.399498, Accuracy 85.919%\n",
      "Epoch 23, Batch 345, LR 0.000040 Loss 5.399332, Accuracy 85.919%\n",
      "Epoch 23, Batch 346, LR 0.000040 Loss 5.396937, Accuracy 85.935%\n",
      "Epoch 23, Batch 347, LR 0.000040 Loss 5.397424, Accuracy 85.935%\n",
      "Epoch 23, Batch 348, LR 0.000040 Loss 5.398096, Accuracy 85.935%\n",
      "Epoch 23, Batch 349, LR 0.000040 Loss 5.398578, Accuracy 85.926%\n",
      "Epoch 23, Batch 350, LR 0.000040 Loss 5.398646, Accuracy 85.933%\n",
      "Epoch 23, Batch 351, LR 0.000040 Loss 5.397162, Accuracy 85.940%\n",
      "Epoch 23, Batch 352, LR 0.000040 Loss 5.396370, Accuracy 85.933%\n",
      "Epoch 23, Batch 353, LR 0.000040 Loss 5.395895, Accuracy 85.933%\n",
      "Epoch 23, Batch 354, LR 0.000040 Loss 5.395365, Accuracy 85.938%\n",
      "Epoch 23, Batch 355, LR 0.000040 Loss 5.395287, Accuracy 85.935%\n",
      "Epoch 23, Batch 356, LR 0.000040 Loss 5.395888, Accuracy 85.938%\n",
      "Epoch 23, Batch 357, LR 0.000040 Loss 5.396859, Accuracy 85.929%\n",
      "Epoch 23, Batch 358, LR 0.000040 Loss 5.397042, Accuracy 85.933%\n",
      "Epoch 23, Batch 359, LR 0.000040 Loss 5.397390, Accuracy 85.929%\n",
      "Epoch 23, Batch 360, LR 0.000040 Loss 5.397756, Accuracy 85.935%\n",
      "Epoch 23, Batch 361, LR 0.000040 Loss 5.396389, Accuracy 85.953%\n",
      "Epoch 23, Batch 362, LR 0.000040 Loss 5.395314, Accuracy 85.966%\n",
      "Epoch 23, Batch 363, LR 0.000040 Loss 5.393085, Accuracy 85.970%\n",
      "Epoch 23, Batch 364, LR 0.000040 Loss 5.393072, Accuracy 85.968%\n",
      "Epoch 23, Batch 365, LR 0.000040 Loss 5.394866, Accuracy 85.961%\n",
      "Epoch 23, Batch 366, LR 0.000040 Loss 5.394294, Accuracy 85.959%\n",
      "Epoch 23, Batch 367, LR 0.000040 Loss 5.393008, Accuracy 85.972%\n",
      "Epoch 23, Batch 368, LR 0.000040 Loss 5.392947, Accuracy 85.969%\n",
      "Epoch 23, Batch 369, LR 0.000040 Loss 5.390112, Accuracy 85.980%\n",
      "Epoch 23, Batch 370, LR 0.000040 Loss 5.390982, Accuracy 85.967%\n",
      "Epoch 23, Batch 371, LR 0.000040 Loss 5.392030, Accuracy 85.950%\n",
      "Epoch 23, Batch 372, LR 0.000040 Loss 5.392424, Accuracy 85.944%\n",
      "Epoch 23, Batch 373, LR 0.000040 Loss 5.391295, Accuracy 85.944%\n",
      "Epoch 23, Batch 374, LR 0.000040 Loss 5.391849, Accuracy 85.946%\n",
      "Epoch 23, Batch 375, LR 0.000040 Loss 5.392559, Accuracy 85.933%\n",
      "Epoch 23, Batch 376, LR 0.000040 Loss 5.393737, Accuracy 85.931%\n",
      "Epoch 23, Batch 377, LR 0.000040 Loss 5.392862, Accuracy 85.938%\n",
      "Epoch 23, Batch 378, LR 0.000040 Loss 5.393994, Accuracy 85.938%\n",
      "Epoch 23, Batch 379, LR 0.000040 Loss 5.392600, Accuracy 85.940%\n",
      "Epoch 23, Batch 380, LR 0.000040 Loss 5.393091, Accuracy 85.935%\n",
      "Epoch 23, Batch 381, LR 0.000040 Loss 5.392270, Accuracy 85.940%\n",
      "Epoch 23, Batch 382, LR 0.000040 Loss 5.389682, Accuracy 85.942%\n",
      "Epoch 23, Batch 383, LR 0.000040 Loss 5.389231, Accuracy 85.946%\n",
      "Epoch 23, Batch 384, LR 0.000040 Loss 5.388481, Accuracy 85.942%\n",
      "Epoch 23, Batch 385, LR 0.000040 Loss 5.387125, Accuracy 85.958%\n",
      "Epoch 23, Batch 386, LR 0.000040 Loss 5.385922, Accuracy 85.976%\n",
      "Epoch 23, Batch 387, LR 0.000040 Loss 5.386210, Accuracy 85.986%\n",
      "Epoch 23, Batch 388, LR 0.000040 Loss 5.385583, Accuracy 85.986%\n",
      "Epoch 23, Batch 389, LR 0.000040 Loss 5.385217, Accuracy 85.980%\n",
      "Epoch 23, Batch 390, LR 0.000040 Loss 5.385135, Accuracy 85.972%\n",
      "Epoch 23, Batch 391, LR 0.000040 Loss 5.383745, Accuracy 85.973%\n",
      "Epoch 23, Batch 392, LR 0.000040 Loss 5.383043, Accuracy 85.979%\n",
      "Epoch 23, Batch 393, LR 0.000040 Loss 5.382134, Accuracy 85.989%\n",
      "Epoch 23, Batch 394, LR 0.000040 Loss 5.381662, Accuracy 85.995%\n",
      "Epoch 23, Batch 395, LR 0.000040 Loss 5.381068, Accuracy 85.987%\n",
      "Epoch 23, Batch 396, LR 0.000040 Loss 5.381848, Accuracy 85.977%\n",
      "Epoch 23, Batch 397, LR 0.000040 Loss 5.379784, Accuracy 85.987%\n",
      "Epoch 23, Batch 398, LR 0.000040 Loss 5.379857, Accuracy 85.983%\n",
      "Epoch 23, Batch 399, LR 0.000040 Loss 5.378849, Accuracy 85.992%\n",
      "Epoch 23, Batch 400, LR 0.000040 Loss 5.378248, Accuracy 86.006%\n",
      "Epoch 23, Batch 401, LR 0.000040 Loss 5.378077, Accuracy 86.002%\n",
      "Epoch 23, Batch 402, LR 0.000040 Loss 5.376659, Accuracy 86.013%\n",
      "Epoch 23, Batch 403, LR 0.000040 Loss 5.377103, Accuracy 86.009%\n",
      "Epoch 23, Batch 404, LR 0.000040 Loss 5.376141, Accuracy 86.011%\n",
      "Epoch 23, Batch 405, LR 0.000040 Loss 5.376791, Accuracy 86.007%\n",
      "Epoch 23, Batch 406, LR 0.000040 Loss 5.377883, Accuracy 85.993%\n",
      "Epoch 23, Batch 407, LR 0.000040 Loss 5.377814, Accuracy 85.995%\n",
      "Epoch 23, Batch 408, LR 0.000040 Loss 5.379559, Accuracy 85.991%\n",
      "Epoch 23, Batch 409, LR 0.000040 Loss 5.379311, Accuracy 85.983%\n",
      "Epoch 23, Batch 410, LR 0.000040 Loss 5.378492, Accuracy 85.985%\n",
      "Epoch 23, Batch 411, LR 0.000040 Loss 5.376154, Accuracy 86.000%\n",
      "Epoch 23, Batch 412, LR 0.000040 Loss 5.378033, Accuracy 85.989%\n",
      "Epoch 23, Batch 413, LR 0.000040 Loss 5.377255, Accuracy 85.989%\n",
      "Epoch 23, Batch 414, LR 0.000040 Loss 5.376730, Accuracy 85.998%\n",
      "Epoch 23, Batch 415, LR 0.000040 Loss 5.376737, Accuracy 85.998%\n",
      "Epoch 23, Batch 416, LR 0.000040 Loss 5.376535, Accuracy 85.994%\n",
      "Epoch 23, Batch 417, LR 0.000040 Loss 5.375794, Accuracy 86.001%\n",
      "Epoch 23, Batch 418, LR 0.000040 Loss 5.375710, Accuracy 85.999%\n",
      "Epoch 23, Batch 419, LR 0.000040 Loss 5.374231, Accuracy 86.003%\n",
      "Epoch 23, Batch 420, LR 0.000040 Loss 5.373966, Accuracy 86.004%\n",
      "Epoch 23, Batch 421, LR 0.000040 Loss 5.372996, Accuracy 86.002%\n",
      "Epoch 23, Batch 422, LR 0.000040 Loss 5.373521, Accuracy 85.991%\n",
      "Epoch 23, Batch 423, LR 0.000040 Loss 5.374605, Accuracy 85.991%\n",
      "Epoch 23, Batch 424, LR 0.000040 Loss 5.374146, Accuracy 85.993%\n",
      "Epoch 23, Batch 425, LR 0.000040 Loss 5.373954, Accuracy 85.985%\n",
      "Epoch 23, Batch 426, LR 0.000040 Loss 5.373420, Accuracy 85.985%\n",
      "Epoch 23, Batch 427, LR 0.000040 Loss 5.372969, Accuracy 85.991%\n",
      "Epoch 23, Batch 428, LR 0.000040 Loss 5.373437, Accuracy 85.990%\n",
      "Epoch 23, Batch 429, LR 0.000040 Loss 5.373982, Accuracy 85.992%\n",
      "Epoch 23, Batch 430, LR 0.000040 Loss 5.373960, Accuracy 85.985%\n",
      "Epoch 23, Batch 431, LR 0.000040 Loss 5.374385, Accuracy 85.977%\n",
      "Epoch 23, Batch 432, LR 0.000040 Loss 5.373877, Accuracy 85.979%\n",
      "Epoch 23, Batch 433, LR 0.000040 Loss 5.373750, Accuracy 85.977%\n",
      "Epoch 23, Batch 434, LR 0.000040 Loss 5.373212, Accuracy 85.986%\n",
      "Epoch 23, Batch 435, LR 0.000040 Loss 5.374582, Accuracy 85.977%\n",
      "Epoch 23, Batch 436, LR 0.000040 Loss 5.375664, Accuracy 85.963%\n",
      "Epoch 23, Batch 437, LR 0.000040 Loss 5.375928, Accuracy 85.959%\n",
      "Epoch 23, Batch 438, LR 0.000040 Loss 5.375620, Accuracy 85.957%\n",
      "Epoch 23, Batch 439, LR 0.000040 Loss 5.374974, Accuracy 85.955%\n",
      "Epoch 23, Batch 440, LR 0.000040 Loss 5.375498, Accuracy 85.950%\n",
      "Epoch 23, Batch 441, LR 0.000040 Loss 5.376345, Accuracy 85.946%\n",
      "Epoch 23, Batch 442, LR 0.000040 Loss 5.375970, Accuracy 85.948%\n",
      "Epoch 23, Batch 443, LR 0.000040 Loss 5.377376, Accuracy 85.941%\n",
      "Epoch 23, Batch 444, LR 0.000040 Loss 5.375256, Accuracy 85.952%\n",
      "Epoch 23, Batch 445, LR 0.000040 Loss 5.374696, Accuracy 85.953%\n",
      "Epoch 23, Batch 446, LR 0.000040 Loss 5.374854, Accuracy 85.952%\n",
      "Epoch 23, Batch 447, LR 0.000040 Loss 5.375068, Accuracy 85.953%\n",
      "Epoch 23, Batch 448, LR 0.000040 Loss 5.375040, Accuracy 85.951%\n",
      "Epoch 23, Batch 449, LR 0.000040 Loss 5.374792, Accuracy 85.948%\n",
      "Epoch 23, Batch 450, LR 0.000040 Loss 5.375189, Accuracy 85.941%\n",
      "Epoch 23, Batch 451, LR 0.000040 Loss 5.374040, Accuracy 85.948%\n",
      "Epoch 23, Batch 452, LR 0.000040 Loss 5.373131, Accuracy 85.950%\n",
      "Epoch 23, Batch 453, LR 0.000040 Loss 5.370849, Accuracy 85.958%\n",
      "Epoch 23, Batch 454, LR 0.000040 Loss 5.370855, Accuracy 85.950%\n",
      "Epoch 23, Batch 455, LR 0.000040 Loss 5.370836, Accuracy 85.943%\n",
      "Epoch 23, Batch 456, LR 0.000040 Loss 5.370102, Accuracy 85.946%\n",
      "Epoch 23, Batch 457, LR 0.000040 Loss 5.370315, Accuracy 85.946%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 458, LR 0.000040 Loss 5.369744, Accuracy 85.949%\n",
      "Epoch 23, Batch 459, LR 0.000040 Loss 5.369607, Accuracy 85.961%\n",
      "Epoch 23, Batch 460, LR 0.000040 Loss 5.370334, Accuracy 85.948%\n",
      "Epoch 23, Batch 461, LR 0.000040 Loss 5.369996, Accuracy 85.958%\n",
      "Epoch 23, Batch 462, LR 0.000040 Loss 5.371111, Accuracy 85.951%\n",
      "Epoch 23, Batch 463, LR 0.000040 Loss 5.371807, Accuracy 85.944%\n",
      "Epoch 23, Batch 464, LR 0.000040 Loss 5.371351, Accuracy 85.949%\n",
      "Epoch 23, Batch 465, LR 0.000040 Loss 5.371686, Accuracy 85.948%\n",
      "Epoch 23, Batch 466, LR 0.000040 Loss 5.372209, Accuracy 85.948%\n",
      "Epoch 23, Batch 467, LR 0.000040 Loss 5.370905, Accuracy 85.951%\n",
      "Epoch 23, Batch 468, LR 0.000040 Loss 5.371039, Accuracy 85.946%\n",
      "Epoch 23, Batch 469, LR 0.000040 Loss 5.370278, Accuracy 85.952%\n",
      "Epoch 23, Batch 470, LR 0.000040 Loss 5.371948, Accuracy 85.946%\n",
      "Epoch 23, Batch 471, LR 0.000040 Loss 5.372225, Accuracy 85.951%\n",
      "Epoch 23, Batch 472, LR 0.000040 Loss 5.372995, Accuracy 85.956%\n",
      "Epoch 23, Batch 473, LR 0.000040 Loss 5.372949, Accuracy 85.959%\n",
      "Epoch 23, Batch 474, LR 0.000040 Loss 5.373475, Accuracy 85.954%\n",
      "Epoch 23, Batch 475, LR 0.000040 Loss 5.373723, Accuracy 85.944%\n",
      "Epoch 23, Batch 476, LR 0.000040 Loss 5.374855, Accuracy 85.938%\n",
      "Epoch 23, Batch 477, LR 0.000040 Loss 5.374303, Accuracy 85.946%\n",
      "Epoch 23, Batch 478, LR 0.000040 Loss 5.375670, Accuracy 85.939%\n",
      "Epoch 23, Batch 479, LR 0.000040 Loss 5.375983, Accuracy 85.938%\n",
      "Epoch 23, Batch 480, LR 0.000040 Loss 5.374715, Accuracy 85.946%\n",
      "Epoch 23, Batch 481, LR 0.000040 Loss 5.373921, Accuracy 85.947%\n",
      "Epoch 23, Batch 482, LR 0.000040 Loss 5.374286, Accuracy 85.949%\n",
      "Epoch 23, Batch 483, LR 0.000040 Loss 5.375433, Accuracy 85.942%\n",
      "Epoch 23, Batch 484, LR 0.000040 Loss 5.375242, Accuracy 85.941%\n",
      "Epoch 23, Batch 485, LR 0.000040 Loss 5.375276, Accuracy 85.938%\n",
      "Epoch 23, Batch 486, LR 0.000040 Loss 5.375211, Accuracy 85.939%\n",
      "Epoch 23, Batch 487, LR 0.000040 Loss 5.375434, Accuracy 85.934%\n",
      "Epoch 23, Batch 488, LR 0.000040 Loss 5.374978, Accuracy 85.929%\n",
      "Epoch 23, Batch 489, LR 0.000040 Loss 5.374351, Accuracy 85.926%\n",
      "Epoch 23, Batch 490, LR 0.000040 Loss 5.373463, Accuracy 85.928%\n",
      "Epoch 23, Batch 491, LR 0.000040 Loss 5.373079, Accuracy 85.923%\n",
      "Epoch 23, Batch 492, LR 0.000040 Loss 5.372463, Accuracy 85.928%\n",
      "Epoch 23, Batch 493, LR 0.000040 Loss 5.373100, Accuracy 85.931%\n",
      "Epoch 23, Batch 494, LR 0.000040 Loss 5.373708, Accuracy 85.931%\n",
      "Epoch 23, Batch 495, LR 0.000040 Loss 5.373794, Accuracy 85.933%\n",
      "Epoch 23, Batch 496, LR 0.000040 Loss 5.374054, Accuracy 85.928%\n",
      "Epoch 23, Batch 497, LR 0.000040 Loss 5.371995, Accuracy 85.938%\n",
      "Epoch 23, Batch 498, LR 0.000040 Loss 5.371372, Accuracy 85.945%\n",
      "Epoch 23, Batch 499, LR 0.000040 Loss 5.375083, Accuracy 85.925%\n",
      "Epoch 23, Batch 500, LR 0.000040 Loss 5.375025, Accuracy 85.927%\n",
      "Epoch 23, Batch 501, LR 0.000040 Loss 5.375084, Accuracy 85.927%\n",
      "Epoch 23, Batch 502, LR 0.000040 Loss 5.375100, Accuracy 85.925%\n",
      "Epoch 23, Batch 503, LR 0.000040 Loss 5.375327, Accuracy 85.925%\n",
      "Epoch 23, Batch 504, LR 0.000040 Loss 5.375721, Accuracy 85.919%\n",
      "Epoch 23, Batch 505, LR 0.000040 Loss 5.375896, Accuracy 85.919%\n",
      "Epoch 23, Batch 506, LR 0.000040 Loss 5.375760, Accuracy 85.924%\n",
      "Epoch 23, Batch 507, LR 0.000040 Loss 5.374434, Accuracy 85.931%\n",
      "Epoch 23, Batch 508, LR 0.000040 Loss 5.373258, Accuracy 85.939%\n",
      "Epoch 23, Batch 509, LR 0.000040 Loss 5.374053, Accuracy 85.933%\n",
      "Epoch 23, Batch 510, LR 0.000040 Loss 5.374404, Accuracy 85.930%\n",
      "Epoch 23, Batch 511, LR 0.000040 Loss 5.373287, Accuracy 85.938%\n",
      "Epoch 23, Batch 512, LR 0.000040 Loss 5.372775, Accuracy 85.942%\n",
      "Epoch 23, Batch 513, LR 0.000040 Loss 5.372083, Accuracy 85.944%\n",
      "Epoch 23, Batch 514, LR 0.000040 Loss 5.371008, Accuracy 85.950%\n",
      "Epoch 23, Batch 515, LR 0.000040 Loss 5.370151, Accuracy 85.951%\n",
      "Epoch 23, Batch 516, LR 0.000040 Loss 5.369583, Accuracy 85.954%\n",
      "Epoch 23, Batch 517, LR 0.000040 Loss 5.368169, Accuracy 85.954%\n",
      "Epoch 23, Batch 518, LR 0.000040 Loss 5.368149, Accuracy 85.954%\n",
      "Epoch 23, Batch 519, LR 0.000040 Loss 5.367491, Accuracy 85.953%\n",
      "Epoch 23, Batch 520, LR 0.000040 Loss 5.369462, Accuracy 85.942%\n",
      "Epoch 23, Batch 521, LR 0.000040 Loss 5.371803, Accuracy 85.932%\n",
      "Epoch 23, Batch 522, LR 0.000040 Loss 5.371709, Accuracy 85.933%\n",
      "Epoch 23, Batch 523, LR 0.000040 Loss 5.372612, Accuracy 85.924%\n",
      "Epoch 23, Batch 524, LR 0.000040 Loss 5.372470, Accuracy 85.929%\n",
      "Epoch 23, Batch 525, LR 0.000040 Loss 5.373239, Accuracy 85.926%\n",
      "Epoch 23, Batch 526, LR 0.000040 Loss 5.374464, Accuracy 85.917%\n",
      "Epoch 23, Batch 527, LR 0.000040 Loss 5.373090, Accuracy 85.923%\n",
      "Epoch 23, Batch 528, LR 0.000040 Loss 5.373477, Accuracy 85.915%\n",
      "Epoch 23, Batch 529, LR 0.000040 Loss 5.372651, Accuracy 85.915%\n",
      "Epoch 23, Batch 530, LR 0.000040 Loss 5.373614, Accuracy 85.914%\n",
      "Epoch 23, Batch 531, LR 0.000040 Loss 5.373581, Accuracy 85.912%\n",
      "Epoch 23, Batch 532, LR 0.000040 Loss 5.372779, Accuracy 85.915%\n",
      "Epoch 23, Batch 533, LR 0.000040 Loss 5.371653, Accuracy 85.920%\n",
      "Epoch 23, Batch 534, LR 0.000040 Loss 5.372550, Accuracy 85.920%\n",
      "Epoch 23, Batch 535, LR 0.000040 Loss 5.373770, Accuracy 85.916%\n",
      "Epoch 23, Batch 536, LR 0.000040 Loss 5.372461, Accuracy 85.921%\n",
      "Epoch 23, Batch 537, LR 0.000040 Loss 5.372856, Accuracy 85.923%\n",
      "Epoch 23, Batch 538, LR 0.000040 Loss 5.372103, Accuracy 85.926%\n",
      "Epoch 23, Batch 539, LR 0.000040 Loss 5.372707, Accuracy 85.919%\n",
      "Epoch 23, Batch 540, LR 0.000040 Loss 5.372925, Accuracy 85.917%\n",
      "Epoch 23, Batch 541, LR 0.000040 Loss 5.373490, Accuracy 85.920%\n",
      "Epoch 23, Batch 542, LR 0.000040 Loss 5.373408, Accuracy 85.919%\n",
      "Epoch 23, Batch 543, LR 0.000040 Loss 5.373784, Accuracy 85.923%\n",
      "Epoch 23, Batch 544, LR 0.000040 Loss 5.372620, Accuracy 85.929%\n",
      "Epoch 23, Batch 545, LR 0.000040 Loss 5.372098, Accuracy 85.932%\n",
      "Epoch 23, Batch 546, LR 0.000040 Loss 5.372493, Accuracy 85.929%\n",
      "Epoch 23, Batch 547, LR 0.000040 Loss 5.371888, Accuracy 85.936%\n",
      "Epoch 23, Batch 548, LR 0.000040 Loss 5.371585, Accuracy 85.942%\n",
      "Epoch 23, Batch 549, LR 0.000040 Loss 5.371885, Accuracy 85.943%\n",
      "Epoch 23, Batch 550, LR 0.000040 Loss 5.371762, Accuracy 85.939%\n",
      "Epoch 23, Batch 551, LR 0.000040 Loss 5.371758, Accuracy 85.938%\n",
      "Epoch 23, Batch 552, LR 0.000040 Loss 5.371405, Accuracy 85.939%\n",
      "Epoch 23, Batch 553, LR 0.000040 Loss 5.371259, Accuracy 85.947%\n",
      "Epoch 23, Batch 554, LR 0.000040 Loss 5.369691, Accuracy 85.960%\n",
      "Epoch 23, Batch 555, LR 0.000040 Loss 5.369563, Accuracy 85.950%\n",
      "Epoch 23, Batch 556, LR 0.000040 Loss 5.369245, Accuracy 85.953%\n",
      "Epoch 23, Batch 557, LR 0.000040 Loss 5.369362, Accuracy 85.954%\n",
      "Epoch 23, Batch 558, LR 0.000040 Loss 5.369633, Accuracy 85.954%\n",
      "Epoch 23, Batch 559, LR 0.000040 Loss 5.369350, Accuracy 85.963%\n",
      "Epoch 23, Batch 560, LR 0.000040 Loss 5.368855, Accuracy 85.964%\n",
      "Epoch 23, Batch 561, LR 0.000040 Loss 5.370838, Accuracy 85.956%\n",
      "Epoch 23, Batch 562, LR 0.000040 Loss 5.371358, Accuracy 85.954%\n",
      "Epoch 23, Batch 563, LR 0.000040 Loss 5.370960, Accuracy 85.958%\n",
      "Epoch 23, Batch 564, LR 0.000040 Loss 5.370630, Accuracy 85.954%\n",
      "Epoch 23, Batch 565, LR 0.000040 Loss 5.371070, Accuracy 85.950%\n",
      "Epoch 23, Batch 566, LR 0.000040 Loss 5.370050, Accuracy 85.950%\n",
      "Epoch 23, Batch 567, LR 0.000040 Loss 5.369418, Accuracy 85.950%\n",
      "Epoch 23, Batch 568, LR 0.000040 Loss 5.370339, Accuracy 85.946%\n",
      "Epoch 23, Batch 569, LR 0.000040 Loss 5.370944, Accuracy 85.944%\n",
      "Epoch 23, Batch 570, LR 0.000040 Loss 5.370668, Accuracy 85.947%\n",
      "Epoch 23, Batch 571, LR 0.000040 Loss 5.369934, Accuracy 85.951%\n",
      "Epoch 23, Batch 572, LR 0.000040 Loss 5.368664, Accuracy 85.957%\n",
      "Epoch 23, Batch 573, LR 0.000040 Loss 5.368119, Accuracy 85.952%\n",
      "Epoch 23, Batch 574, LR 0.000040 Loss 5.368413, Accuracy 85.948%\n",
      "Epoch 23, Batch 575, LR 0.000040 Loss 5.368709, Accuracy 85.951%\n",
      "Epoch 23, Batch 576, LR 0.000040 Loss 5.367787, Accuracy 85.951%\n",
      "Epoch 23, Batch 577, LR 0.000040 Loss 5.368231, Accuracy 85.942%\n",
      "Epoch 23, Batch 578, LR 0.000040 Loss 5.367234, Accuracy 85.940%\n",
      "Epoch 23, Batch 579, LR 0.000040 Loss 5.366723, Accuracy 85.946%\n",
      "Epoch 23, Batch 580, LR 0.000040 Loss 5.366363, Accuracy 85.951%\n",
      "Epoch 23, Batch 581, LR 0.000040 Loss 5.366402, Accuracy 85.956%\n",
      "Epoch 23, Batch 582, LR 0.000040 Loss 5.365630, Accuracy 85.960%\n",
      "Epoch 23, Batch 583, LR 0.000040 Loss 5.365473, Accuracy 85.966%\n",
      "Epoch 23, Batch 584, LR 0.000040 Loss 5.366191, Accuracy 85.964%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 585, LR 0.000040 Loss 5.365956, Accuracy 85.967%\n",
      "Epoch 23, Batch 586, LR 0.000039 Loss 5.365369, Accuracy 85.977%\n",
      "Epoch 23, Batch 587, LR 0.000039 Loss 5.364308, Accuracy 85.980%\n",
      "Epoch 23, Batch 588, LR 0.000039 Loss 5.364327, Accuracy 85.988%\n",
      "Epoch 23, Batch 589, LR 0.000039 Loss 5.364471, Accuracy 85.987%\n",
      "Epoch 23, Batch 590, LR 0.000039 Loss 5.364997, Accuracy 85.981%\n",
      "Epoch 23, Batch 591, LR 0.000039 Loss 5.363900, Accuracy 85.986%\n",
      "Epoch 23, Batch 592, LR 0.000039 Loss 5.363524, Accuracy 85.984%\n",
      "Epoch 23, Batch 593, LR 0.000039 Loss 5.365075, Accuracy 85.978%\n",
      "Epoch 23, Batch 594, LR 0.000039 Loss 5.364991, Accuracy 85.970%\n",
      "Epoch 23, Batch 595, LR 0.000039 Loss 5.365988, Accuracy 85.965%\n",
      "Epoch 23, Batch 596, LR 0.000039 Loss 5.364669, Accuracy 85.970%\n",
      "Epoch 23, Batch 597, LR 0.000039 Loss 5.363568, Accuracy 85.982%\n",
      "Epoch 23, Batch 598, LR 0.000039 Loss 5.364021, Accuracy 85.982%\n",
      "Epoch 23, Batch 599, LR 0.000039 Loss 5.362235, Accuracy 85.996%\n",
      "Epoch 23, Batch 600, LR 0.000039 Loss 5.362069, Accuracy 86.001%\n",
      "Epoch 23, Batch 601, LR 0.000039 Loss 5.361164, Accuracy 86.004%\n",
      "Epoch 23, Batch 602, LR 0.000039 Loss 5.361308, Accuracy 86.002%\n",
      "Epoch 23, Batch 603, LR 0.000039 Loss 5.361230, Accuracy 86.002%\n",
      "Epoch 23, Batch 604, LR 0.000039 Loss 5.361219, Accuracy 86.005%\n",
      "Epoch 23, Batch 605, LR 0.000039 Loss 5.361686, Accuracy 86.003%\n",
      "Epoch 23, Batch 606, LR 0.000039 Loss 5.362033, Accuracy 86.011%\n",
      "Epoch 23, Batch 607, LR 0.000039 Loss 5.362862, Accuracy 86.004%\n",
      "Epoch 23, Batch 608, LR 0.000039 Loss 5.362133, Accuracy 86.004%\n",
      "Epoch 23, Batch 609, LR 0.000039 Loss 5.361275, Accuracy 86.008%\n",
      "Epoch 23, Batch 610, LR 0.000039 Loss 5.361840, Accuracy 86.003%\n",
      "Epoch 23, Batch 611, LR 0.000039 Loss 5.361657, Accuracy 86.001%\n",
      "Epoch 23, Batch 612, LR 0.000039 Loss 5.361574, Accuracy 85.999%\n",
      "Epoch 23, Batch 613, LR 0.000039 Loss 5.361599, Accuracy 85.997%\n",
      "Epoch 23, Batch 614, LR 0.000039 Loss 5.362236, Accuracy 85.999%\n",
      "Epoch 23, Batch 615, LR 0.000039 Loss 5.363306, Accuracy 85.993%\n",
      "Epoch 23, Batch 616, LR 0.000039 Loss 5.362905, Accuracy 85.995%\n",
      "Epoch 23, Batch 617, LR 0.000039 Loss 5.364043, Accuracy 85.994%\n",
      "Epoch 23, Batch 618, LR 0.000039 Loss 5.364918, Accuracy 85.989%\n",
      "Epoch 23, Batch 619, LR 0.000039 Loss 5.364898, Accuracy 85.991%\n",
      "Epoch 23, Batch 620, LR 0.000039 Loss 5.364565, Accuracy 85.989%\n",
      "Epoch 23, Batch 621, LR 0.000039 Loss 5.364200, Accuracy 85.989%\n",
      "Epoch 23, Batch 622, LR 0.000039 Loss 5.364855, Accuracy 85.986%\n",
      "Epoch 23, Batch 623, LR 0.000039 Loss 5.365880, Accuracy 85.984%\n",
      "Epoch 23, Batch 624, LR 0.000039 Loss 5.366806, Accuracy 85.981%\n",
      "Epoch 23, Batch 625, LR 0.000039 Loss 5.366572, Accuracy 85.984%\n",
      "Epoch 23, Batch 626, LR 0.000039 Loss 5.365286, Accuracy 85.989%\n",
      "Epoch 23, Batch 627, LR 0.000039 Loss 5.366127, Accuracy 85.986%\n",
      "Epoch 23, Batch 628, LR 0.000039 Loss 5.366772, Accuracy 85.990%\n",
      "Epoch 23, Batch 629, LR 0.000039 Loss 5.365982, Accuracy 85.988%\n",
      "Epoch 23, Batch 630, LR 0.000039 Loss 5.367218, Accuracy 85.981%\n",
      "Epoch 23, Batch 631, LR 0.000039 Loss 5.366438, Accuracy 85.986%\n",
      "Epoch 23, Batch 632, LR 0.000039 Loss 5.366637, Accuracy 85.981%\n",
      "Epoch 23, Batch 633, LR 0.000039 Loss 5.366673, Accuracy 85.979%\n",
      "Epoch 23, Batch 634, LR 0.000039 Loss 5.366125, Accuracy 85.981%\n",
      "Epoch 23, Batch 635, LR 0.000039 Loss 5.366020, Accuracy 85.976%\n",
      "Epoch 23, Batch 636, LR 0.000039 Loss 5.367012, Accuracy 85.971%\n",
      "Epoch 23, Batch 637, LR 0.000039 Loss 5.368258, Accuracy 85.967%\n",
      "Epoch 23, Batch 638, LR 0.000039 Loss 5.368607, Accuracy 85.971%\n",
      "Epoch 23, Batch 639, LR 0.000039 Loss 5.368247, Accuracy 85.977%\n",
      "Epoch 23, Batch 640, LR 0.000039 Loss 5.368225, Accuracy 85.972%\n",
      "Epoch 23, Batch 641, LR 0.000039 Loss 5.369379, Accuracy 85.961%\n",
      "Epoch 23, Batch 642, LR 0.000039 Loss 5.369974, Accuracy 85.959%\n",
      "Epoch 23, Batch 643, LR 0.000039 Loss 5.369894, Accuracy 85.965%\n",
      "Epoch 23, Batch 644, LR 0.000039 Loss 5.369684, Accuracy 85.964%\n",
      "Epoch 23, Batch 645, LR 0.000039 Loss 5.369331, Accuracy 85.964%\n",
      "Epoch 23, Batch 646, LR 0.000039 Loss 5.369348, Accuracy 85.964%\n",
      "Epoch 23, Batch 647, LR 0.000039 Loss 5.369712, Accuracy 85.958%\n",
      "Epoch 23, Batch 648, LR 0.000039 Loss 5.370457, Accuracy 85.958%\n",
      "Epoch 23, Batch 649, LR 0.000039 Loss 5.370463, Accuracy 85.957%\n",
      "Epoch 23, Batch 650, LR 0.000039 Loss 5.370031, Accuracy 85.954%\n",
      "Epoch 23, Batch 651, LR 0.000039 Loss 5.370897, Accuracy 85.948%\n",
      "Epoch 23, Batch 652, LR 0.000039 Loss 5.369995, Accuracy 85.953%\n",
      "Epoch 23, Batch 653, LR 0.000039 Loss 5.369818, Accuracy 85.958%\n",
      "Epoch 23, Batch 654, LR 0.000039 Loss 5.368965, Accuracy 85.964%\n",
      "Epoch 23, Batch 655, LR 0.000039 Loss 5.369598, Accuracy 85.958%\n",
      "Epoch 23, Batch 656, LR 0.000039 Loss 5.369811, Accuracy 85.955%\n",
      "Epoch 23, Batch 657, LR 0.000039 Loss 5.368363, Accuracy 85.958%\n",
      "Epoch 23, Batch 658, LR 0.000039 Loss 5.367842, Accuracy 85.961%\n",
      "Epoch 23, Batch 659, LR 0.000039 Loss 5.367070, Accuracy 85.961%\n",
      "Epoch 23, Batch 660, LR 0.000039 Loss 5.367552, Accuracy 85.955%\n",
      "Epoch 23, Batch 661, LR 0.000039 Loss 5.367305, Accuracy 85.953%\n",
      "Epoch 23, Batch 662, LR 0.000039 Loss 5.367708, Accuracy 85.949%\n",
      "Epoch 23, Batch 663, LR 0.000039 Loss 5.367962, Accuracy 85.946%\n",
      "Epoch 23, Batch 664, LR 0.000039 Loss 5.366216, Accuracy 85.959%\n",
      "Epoch 23, Batch 665, LR 0.000039 Loss 5.365106, Accuracy 85.969%\n",
      "Epoch 23, Batch 666, LR 0.000039 Loss 5.365354, Accuracy 85.969%\n",
      "Epoch 23, Batch 667, LR 0.000039 Loss 5.364425, Accuracy 85.976%\n",
      "Epoch 23, Batch 668, LR 0.000039 Loss 5.364647, Accuracy 85.971%\n",
      "Epoch 23, Batch 669, LR 0.000039 Loss 5.364841, Accuracy 85.970%\n",
      "Epoch 23, Batch 670, LR 0.000039 Loss 5.366593, Accuracy 85.956%\n",
      "Epoch 23, Batch 671, LR 0.000039 Loss 5.366098, Accuracy 85.963%\n",
      "Epoch 23, Batch 672, LR 0.000039 Loss 5.365625, Accuracy 85.965%\n",
      "Epoch 23, Batch 673, LR 0.000039 Loss 5.365494, Accuracy 85.970%\n",
      "Epoch 23, Batch 674, LR 0.000039 Loss 5.365239, Accuracy 85.972%\n",
      "Epoch 23, Batch 675, LR 0.000039 Loss 5.365948, Accuracy 85.976%\n",
      "Epoch 23, Batch 676, LR 0.000039 Loss 5.366391, Accuracy 85.971%\n",
      "Epoch 23, Batch 677, LR 0.000039 Loss 5.366284, Accuracy 85.972%\n",
      "Epoch 23, Batch 678, LR 0.000039 Loss 5.366260, Accuracy 85.972%\n",
      "Epoch 23, Batch 679, LR 0.000039 Loss 5.366153, Accuracy 85.969%\n",
      "Epoch 23, Batch 680, LR 0.000039 Loss 5.366604, Accuracy 85.969%\n",
      "Epoch 23, Batch 681, LR 0.000039 Loss 5.365801, Accuracy 85.971%\n",
      "Epoch 23, Batch 682, LR 0.000039 Loss 5.366470, Accuracy 85.964%\n",
      "Epoch 23, Batch 683, LR 0.000039 Loss 5.365859, Accuracy 85.966%\n",
      "Epoch 23, Batch 684, LR 0.000039 Loss 5.365754, Accuracy 85.967%\n",
      "Epoch 23, Batch 685, LR 0.000039 Loss 5.366031, Accuracy 85.963%\n",
      "Epoch 23, Batch 686, LR 0.000039 Loss 5.365817, Accuracy 85.960%\n",
      "Epoch 23, Batch 687, LR 0.000039 Loss 5.367085, Accuracy 85.959%\n",
      "Epoch 23, Batch 688, LR 0.000039 Loss 5.366723, Accuracy 85.962%\n",
      "Epoch 23, Batch 689, LR 0.000039 Loss 5.365315, Accuracy 85.970%\n",
      "Epoch 23, Batch 690, LR 0.000039 Loss 5.365310, Accuracy 85.971%\n",
      "Epoch 23, Batch 691, LR 0.000039 Loss 5.364092, Accuracy 85.977%\n",
      "Epoch 23, Batch 692, LR 0.000039 Loss 5.364477, Accuracy 85.976%\n",
      "Epoch 23, Batch 693, LR 0.000039 Loss 5.364787, Accuracy 85.969%\n",
      "Epoch 23, Batch 694, LR 0.000039 Loss 5.365660, Accuracy 85.968%\n",
      "Epoch 23, Batch 695, LR 0.000039 Loss 5.365747, Accuracy 85.970%\n",
      "Epoch 23, Batch 696, LR 0.000039 Loss 5.366379, Accuracy 85.966%\n",
      "Epoch 23, Batch 697, LR 0.000039 Loss 5.366965, Accuracy 85.967%\n",
      "Epoch 23, Batch 698, LR 0.000039 Loss 5.366865, Accuracy 85.963%\n",
      "Epoch 23, Batch 699, LR 0.000039 Loss 5.367557, Accuracy 85.955%\n",
      "Epoch 23, Batch 700, LR 0.000039 Loss 5.367375, Accuracy 85.958%\n",
      "Epoch 23, Batch 701, LR 0.000039 Loss 5.368614, Accuracy 85.955%\n",
      "Epoch 23, Batch 702, LR 0.000039 Loss 5.368528, Accuracy 85.952%\n",
      "Epoch 23, Batch 703, LR 0.000039 Loss 5.368063, Accuracy 85.959%\n",
      "Epoch 23, Batch 704, LR 0.000039 Loss 5.367740, Accuracy 85.963%\n",
      "Epoch 23, Batch 705, LR 0.000039 Loss 5.366538, Accuracy 85.971%\n",
      "Epoch 23, Batch 706, LR 0.000039 Loss 5.365040, Accuracy 85.980%\n",
      "Epoch 23, Batch 707, LR 0.000039 Loss 5.365241, Accuracy 85.983%\n",
      "Epoch 23, Batch 708, LR 0.000039 Loss 5.364984, Accuracy 85.983%\n",
      "Epoch 23, Batch 709, LR 0.000039 Loss 5.363965, Accuracy 85.987%\n",
      "Epoch 23, Batch 710, LR 0.000039 Loss 5.363535, Accuracy 85.987%\n",
      "Epoch 23, Batch 711, LR 0.000039 Loss 5.363199, Accuracy 85.986%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 712, LR 0.000039 Loss 5.363233, Accuracy 85.984%\n",
      "Epoch 23, Batch 713, LR 0.000039 Loss 5.363473, Accuracy 85.984%\n",
      "Epoch 23, Batch 714, LR 0.000039 Loss 5.363580, Accuracy 85.978%\n",
      "Epoch 23, Batch 715, LR 0.000039 Loss 5.362382, Accuracy 85.979%\n",
      "Epoch 23, Batch 716, LR 0.000039 Loss 5.360817, Accuracy 85.986%\n",
      "Epoch 23, Batch 717, LR 0.000039 Loss 5.359639, Accuracy 85.993%\n",
      "Epoch 23, Batch 718, LR 0.000039 Loss 5.359566, Accuracy 85.995%\n",
      "Epoch 23, Batch 719, LR 0.000039 Loss 5.360009, Accuracy 85.990%\n",
      "Epoch 23, Batch 720, LR 0.000039 Loss 5.360418, Accuracy 85.991%\n",
      "Epoch 23, Batch 721, LR 0.000039 Loss 5.360709, Accuracy 85.991%\n",
      "Epoch 23, Batch 722, LR 0.000039 Loss 5.360823, Accuracy 85.991%\n",
      "Epoch 23, Batch 723, LR 0.000039 Loss 5.361085, Accuracy 85.989%\n",
      "Epoch 23, Batch 724, LR 0.000039 Loss 5.361129, Accuracy 85.988%\n",
      "Epoch 23, Batch 725, LR 0.000039 Loss 5.361336, Accuracy 85.989%\n",
      "Epoch 23, Batch 726, LR 0.000039 Loss 5.361332, Accuracy 85.982%\n",
      "Epoch 23, Batch 727, LR 0.000039 Loss 5.360827, Accuracy 85.985%\n",
      "Epoch 23, Batch 728, LR 0.000039 Loss 5.360513, Accuracy 85.987%\n",
      "Epoch 23, Batch 729, LR 0.000039 Loss 5.360970, Accuracy 85.978%\n",
      "Epoch 23, Batch 730, LR 0.000039 Loss 5.361685, Accuracy 85.974%\n",
      "Epoch 23, Batch 731, LR 0.000039 Loss 5.362114, Accuracy 85.974%\n",
      "Epoch 23, Batch 732, LR 0.000039 Loss 5.361847, Accuracy 85.974%\n",
      "Epoch 23, Batch 733, LR 0.000039 Loss 5.362756, Accuracy 85.969%\n",
      "Epoch 23, Batch 734, LR 0.000039 Loss 5.362680, Accuracy 85.969%\n",
      "Epoch 23, Batch 735, LR 0.000039 Loss 5.362470, Accuracy 85.970%\n",
      "Epoch 23, Batch 736, LR 0.000039 Loss 5.362476, Accuracy 85.964%\n",
      "Epoch 23, Batch 737, LR 0.000039 Loss 5.362339, Accuracy 85.968%\n",
      "Epoch 23, Batch 738, LR 0.000039 Loss 5.362960, Accuracy 85.964%\n",
      "Epoch 23, Batch 739, LR 0.000039 Loss 5.362201, Accuracy 85.965%\n",
      "Epoch 23, Batch 740, LR 0.000039 Loss 5.361440, Accuracy 85.971%\n",
      "Epoch 23, Batch 741, LR 0.000039 Loss 5.361622, Accuracy 85.973%\n",
      "Epoch 23, Batch 742, LR 0.000039 Loss 5.362134, Accuracy 85.972%\n",
      "Epoch 23, Batch 743, LR 0.000039 Loss 5.362143, Accuracy 85.971%\n",
      "Epoch 23, Batch 744, LR 0.000039 Loss 5.362416, Accuracy 85.970%\n",
      "Epoch 23, Batch 745, LR 0.000039 Loss 5.362224, Accuracy 85.964%\n",
      "Epoch 23, Batch 746, LR 0.000039 Loss 5.362841, Accuracy 85.959%\n",
      "Epoch 23, Batch 747, LR 0.000039 Loss 5.361874, Accuracy 85.968%\n",
      "Epoch 23, Batch 748, LR 0.000039 Loss 5.361788, Accuracy 85.966%\n",
      "Epoch 23, Batch 749, LR 0.000039 Loss 5.360799, Accuracy 85.968%\n",
      "Epoch 23, Batch 750, LR 0.000039 Loss 5.360521, Accuracy 85.973%\n",
      "Epoch 23, Batch 751, LR 0.000039 Loss 5.360236, Accuracy 85.970%\n",
      "Epoch 23, Batch 752, LR 0.000039 Loss 5.361358, Accuracy 85.961%\n",
      "Epoch 23, Batch 753, LR 0.000039 Loss 5.361300, Accuracy 85.963%\n",
      "Epoch 23, Batch 754, LR 0.000039 Loss 5.362081, Accuracy 85.954%\n",
      "Epoch 23, Batch 755, LR 0.000039 Loss 5.361255, Accuracy 85.957%\n",
      "Epoch 23, Batch 756, LR 0.000039 Loss 5.361335, Accuracy 85.958%\n",
      "Epoch 23, Batch 757, LR 0.000039 Loss 5.360951, Accuracy 85.960%\n",
      "Epoch 23, Batch 758, LR 0.000039 Loss 5.360666, Accuracy 85.962%\n",
      "Epoch 23, Batch 759, LR 0.000039 Loss 5.360774, Accuracy 85.957%\n",
      "Epoch 23, Batch 760, LR 0.000039 Loss 5.360942, Accuracy 85.959%\n",
      "Epoch 23, Batch 761, LR 0.000039 Loss 5.360560, Accuracy 85.960%\n",
      "Epoch 23, Batch 762, LR 0.000039 Loss 5.359994, Accuracy 85.962%\n",
      "Epoch 23, Batch 763, LR 0.000039 Loss 5.359138, Accuracy 85.966%\n",
      "Epoch 23, Batch 764, LR 0.000039 Loss 5.358491, Accuracy 85.970%\n",
      "Epoch 23, Batch 765, LR 0.000039 Loss 5.358925, Accuracy 85.961%\n",
      "Epoch 23, Batch 766, LR 0.000039 Loss 5.358476, Accuracy 85.959%\n",
      "Epoch 23, Batch 767, LR 0.000039 Loss 5.358791, Accuracy 85.957%\n",
      "Epoch 23, Batch 768, LR 0.000039 Loss 5.359091, Accuracy 85.955%\n",
      "Epoch 23, Batch 769, LR 0.000039 Loss 5.359894, Accuracy 85.953%\n",
      "Epoch 23, Batch 770, LR 0.000039 Loss 5.359473, Accuracy 85.954%\n",
      "Epoch 23, Batch 771, LR 0.000039 Loss 5.361237, Accuracy 85.945%\n",
      "Epoch 23, Batch 772, LR 0.000039 Loss 5.361283, Accuracy 85.947%\n",
      "Epoch 23, Batch 773, LR 0.000039 Loss 5.361699, Accuracy 85.949%\n",
      "Epoch 23, Batch 774, LR 0.000039 Loss 5.361849, Accuracy 85.951%\n",
      "Epoch 23, Batch 775, LR 0.000039 Loss 5.361715, Accuracy 85.953%\n",
      "Epoch 23, Batch 776, LR 0.000039 Loss 5.361200, Accuracy 85.954%\n",
      "Epoch 23, Batch 777, LR 0.000039 Loss 5.360995, Accuracy 85.957%\n",
      "Epoch 23, Batch 778, LR 0.000039 Loss 5.360481, Accuracy 85.964%\n",
      "Epoch 23, Batch 779, LR 0.000039 Loss 5.360044, Accuracy 85.967%\n",
      "Epoch 23, Batch 780, LR 0.000039 Loss 5.360398, Accuracy 85.968%\n",
      "Epoch 23, Batch 781, LR 0.000039 Loss 5.359581, Accuracy 85.969%\n",
      "Epoch 23, Batch 782, LR 0.000039 Loss 5.360267, Accuracy 85.966%\n",
      "Epoch 23, Batch 783, LR 0.000039 Loss 5.359928, Accuracy 85.968%\n",
      "Epoch 23, Batch 784, LR 0.000039 Loss 5.359674, Accuracy 85.971%\n",
      "Epoch 23, Batch 785, LR 0.000039 Loss 5.359914, Accuracy 85.971%\n",
      "Epoch 23, Batch 786, LR 0.000039 Loss 5.360461, Accuracy 85.967%\n",
      "Epoch 23, Batch 787, LR 0.000039 Loss 5.360774, Accuracy 85.969%\n",
      "Epoch 23, Batch 788, LR 0.000039 Loss 5.359728, Accuracy 85.974%\n",
      "Epoch 23, Batch 789, LR 0.000039 Loss 5.359205, Accuracy 85.975%\n",
      "Epoch 23, Batch 790, LR 0.000039 Loss 5.359607, Accuracy 85.973%\n",
      "Epoch 23, Batch 791, LR 0.000039 Loss 5.359388, Accuracy 85.974%\n",
      "Epoch 23, Batch 792, LR 0.000039 Loss 5.359790, Accuracy 85.969%\n",
      "Epoch 23, Batch 793, LR 0.000039 Loss 5.361460, Accuracy 85.963%\n",
      "Epoch 23, Batch 794, LR 0.000039 Loss 5.361997, Accuracy 85.961%\n",
      "Epoch 23, Batch 795, LR 0.000039 Loss 5.361805, Accuracy 85.962%\n",
      "Epoch 23, Batch 796, LR 0.000039 Loss 5.362384, Accuracy 85.964%\n",
      "Epoch 23, Batch 797, LR 0.000039 Loss 5.362183, Accuracy 85.969%\n",
      "Epoch 23, Batch 798, LR 0.000039 Loss 5.362186, Accuracy 85.973%\n",
      "Epoch 23, Batch 799, LR 0.000039 Loss 5.361454, Accuracy 85.973%\n",
      "Epoch 23, Batch 800, LR 0.000039 Loss 5.360770, Accuracy 85.974%\n",
      "Epoch 23, Batch 801, LR 0.000039 Loss 5.360854, Accuracy 85.973%\n",
      "Epoch 23, Batch 802, LR 0.000039 Loss 5.361358, Accuracy 85.964%\n",
      "Epoch 23, Batch 803, LR 0.000039 Loss 5.360964, Accuracy 85.966%\n",
      "Epoch 23, Batch 804, LR 0.000039 Loss 5.361148, Accuracy 85.969%\n",
      "Epoch 23, Batch 805, LR 0.000039 Loss 5.360531, Accuracy 85.966%\n",
      "Epoch 23, Batch 806, LR 0.000039 Loss 5.360485, Accuracy 85.966%\n",
      "Epoch 23, Batch 807, LR 0.000039 Loss 5.361176, Accuracy 85.963%\n",
      "Epoch 23, Batch 808, LR 0.000039 Loss 5.360759, Accuracy 85.962%\n",
      "Epoch 23, Batch 809, LR 0.000039 Loss 5.360187, Accuracy 85.963%\n",
      "Epoch 23, Batch 810, LR 0.000039 Loss 5.359161, Accuracy 85.965%\n",
      "Epoch 23, Batch 811, LR 0.000039 Loss 5.359051, Accuracy 85.968%\n",
      "Epoch 23, Batch 812, LR 0.000039 Loss 5.358994, Accuracy 85.969%\n",
      "Epoch 23, Batch 813, LR 0.000039 Loss 5.359197, Accuracy 85.973%\n",
      "Epoch 23, Batch 814, LR 0.000039 Loss 5.358550, Accuracy 85.976%\n",
      "Epoch 23, Batch 815, LR 0.000039 Loss 5.358215, Accuracy 85.976%\n",
      "Epoch 23, Batch 816, LR 0.000039 Loss 5.359282, Accuracy 85.969%\n",
      "Epoch 23, Batch 817, LR 0.000039 Loss 5.359908, Accuracy 85.963%\n",
      "Epoch 23, Batch 818, LR 0.000039 Loss 5.360093, Accuracy 85.959%\n",
      "Epoch 23, Batch 819, LR 0.000039 Loss 5.360325, Accuracy 85.961%\n",
      "Epoch 23, Batch 820, LR 0.000039 Loss 5.361033, Accuracy 85.960%\n",
      "Epoch 23, Batch 821, LR 0.000039 Loss 5.361501, Accuracy 85.956%\n",
      "Epoch 23, Batch 822, LR 0.000039 Loss 5.361764, Accuracy 85.953%\n",
      "Epoch 23, Batch 823, LR 0.000039 Loss 5.361814, Accuracy 85.952%\n",
      "Epoch 23, Batch 824, LR 0.000039 Loss 5.361871, Accuracy 85.957%\n",
      "Epoch 23, Batch 825, LR 0.000039 Loss 5.362573, Accuracy 85.949%\n",
      "Epoch 23, Batch 826, LR 0.000039 Loss 5.362149, Accuracy 85.951%\n",
      "Epoch 23, Batch 827, LR 0.000039 Loss 5.361604, Accuracy 85.950%\n",
      "Epoch 23, Batch 828, LR 0.000039 Loss 5.362298, Accuracy 85.945%\n",
      "Epoch 23, Batch 829, LR 0.000039 Loss 5.361930, Accuracy 85.945%\n",
      "Epoch 23, Batch 830, LR 0.000039 Loss 5.361346, Accuracy 85.948%\n",
      "Epoch 23, Batch 831, LR 0.000039 Loss 5.361694, Accuracy 85.948%\n",
      "Epoch 23, Batch 832, LR 0.000039 Loss 5.362075, Accuracy 85.947%\n",
      "Epoch 23, Batch 833, LR 0.000039 Loss 5.361843, Accuracy 85.947%\n",
      "Epoch 23, Batch 834, LR 0.000039 Loss 5.361513, Accuracy 85.948%\n",
      "Epoch 23, Batch 835, LR 0.000039 Loss 5.361713, Accuracy 85.950%\n",
      "Epoch 23, Batch 836, LR 0.000039 Loss 5.361634, Accuracy 85.945%\n",
      "Epoch 23, Batch 837, LR 0.000039 Loss 5.360359, Accuracy 85.952%\n",
      "Epoch 23, Batch 838, LR 0.000039 Loss 5.359527, Accuracy 85.957%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 839, LR 0.000039 Loss 5.360555, Accuracy 85.950%\n",
      "Epoch 23, Batch 840, LR 0.000039 Loss 5.360762, Accuracy 85.949%\n",
      "Epoch 23, Batch 841, LR 0.000039 Loss 5.360455, Accuracy 85.948%\n",
      "Epoch 23, Batch 842, LR 0.000039 Loss 5.360419, Accuracy 85.949%\n",
      "Epoch 23, Batch 843, LR 0.000039 Loss 5.360599, Accuracy 85.944%\n",
      "Epoch 23, Batch 844, LR 0.000039 Loss 5.360482, Accuracy 85.945%\n",
      "Epoch 23, Batch 845, LR 0.000039 Loss 5.360743, Accuracy 85.947%\n",
      "Epoch 23, Batch 846, LR 0.000038 Loss 5.360174, Accuracy 85.949%\n",
      "Epoch 23, Batch 847, LR 0.000038 Loss 5.360143, Accuracy 85.952%\n",
      "Epoch 23, Batch 848, LR 0.000038 Loss 5.359758, Accuracy 85.958%\n",
      "Epoch 23, Batch 849, LR 0.000038 Loss 5.359237, Accuracy 85.960%\n",
      "Epoch 23, Batch 850, LR 0.000038 Loss 5.359118, Accuracy 85.960%\n",
      "Epoch 23, Batch 851, LR 0.000038 Loss 5.359070, Accuracy 85.957%\n",
      "Epoch 23, Batch 852, LR 0.000038 Loss 5.359028, Accuracy 85.955%\n",
      "Epoch 23, Batch 853, LR 0.000038 Loss 5.359444, Accuracy 85.951%\n",
      "Epoch 23, Batch 854, LR 0.000038 Loss 5.358979, Accuracy 85.951%\n",
      "Epoch 23, Batch 855, LR 0.000038 Loss 5.359165, Accuracy 85.949%\n",
      "Epoch 23, Batch 856, LR 0.000038 Loss 5.358559, Accuracy 85.953%\n",
      "Epoch 23, Batch 857, LR 0.000038 Loss 5.358625, Accuracy 85.951%\n",
      "Epoch 23, Batch 858, LR 0.000038 Loss 5.358063, Accuracy 85.958%\n",
      "Epoch 23, Batch 859, LR 0.000038 Loss 5.358078, Accuracy 85.960%\n",
      "Epoch 23, Batch 860, LR 0.000038 Loss 5.357953, Accuracy 85.966%\n",
      "Epoch 23, Batch 861, LR 0.000038 Loss 5.357890, Accuracy 85.967%\n",
      "Epoch 23, Batch 862, LR 0.000038 Loss 5.358095, Accuracy 85.964%\n",
      "Epoch 23, Batch 863, LR 0.000038 Loss 5.358325, Accuracy 85.963%\n",
      "Epoch 23, Batch 864, LR 0.000038 Loss 5.359025, Accuracy 85.961%\n",
      "Epoch 23, Batch 865, LR 0.000038 Loss 5.359203, Accuracy 85.965%\n",
      "Epoch 23, Batch 866, LR 0.000038 Loss 5.359523, Accuracy 85.965%\n",
      "Epoch 23, Batch 867, LR 0.000038 Loss 5.361049, Accuracy 85.955%\n",
      "Epoch 23, Batch 868, LR 0.000038 Loss 5.361845, Accuracy 85.948%\n",
      "Epoch 23, Batch 869, LR 0.000038 Loss 5.362057, Accuracy 85.950%\n",
      "Epoch 23, Batch 870, LR 0.000038 Loss 5.361272, Accuracy 85.948%\n",
      "Epoch 23, Batch 871, LR 0.000038 Loss 5.360359, Accuracy 85.949%\n",
      "Epoch 23, Batch 872, LR 0.000038 Loss 5.360715, Accuracy 85.947%\n",
      "Epoch 23, Batch 873, LR 0.000038 Loss 5.361456, Accuracy 85.943%\n",
      "Epoch 23, Batch 874, LR 0.000038 Loss 5.362318, Accuracy 85.938%\n",
      "Epoch 23, Batch 875, LR 0.000038 Loss 5.363034, Accuracy 85.936%\n",
      "Epoch 23, Batch 876, LR 0.000038 Loss 5.362555, Accuracy 85.941%\n",
      "Epoch 23, Batch 877, LR 0.000038 Loss 5.362377, Accuracy 85.943%\n",
      "Epoch 23, Batch 878, LR 0.000038 Loss 5.362048, Accuracy 85.946%\n",
      "Epoch 23, Batch 879, LR 0.000038 Loss 5.362575, Accuracy 85.945%\n",
      "Epoch 23, Batch 880, LR 0.000038 Loss 5.362624, Accuracy 85.943%\n",
      "Epoch 23, Batch 881, LR 0.000038 Loss 5.362582, Accuracy 85.944%\n",
      "Epoch 23, Batch 882, LR 0.000038 Loss 5.362194, Accuracy 85.946%\n",
      "Epoch 23, Batch 883, LR 0.000038 Loss 5.361952, Accuracy 85.946%\n",
      "Epoch 23, Batch 884, LR 0.000038 Loss 5.360744, Accuracy 85.953%\n",
      "Epoch 23, Batch 885, LR 0.000038 Loss 5.360072, Accuracy 85.954%\n",
      "Epoch 23, Batch 886, LR 0.000038 Loss 5.359860, Accuracy 85.954%\n",
      "Epoch 23, Batch 887, LR 0.000038 Loss 5.359396, Accuracy 85.959%\n",
      "Epoch 23, Batch 888, LR 0.000038 Loss 5.360087, Accuracy 85.951%\n",
      "Epoch 23, Batch 889, LR 0.000038 Loss 5.361079, Accuracy 85.947%\n",
      "Epoch 23, Batch 890, LR 0.000038 Loss 5.361323, Accuracy 85.948%\n",
      "Epoch 23, Batch 891, LR 0.000038 Loss 5.361046, Accuracy 85.952%\n",
      "Epoch 23, Batch 892, LR 0.000038 Loss 5.361044, Accuracy 85.952%\n",
      "Epoch 23, Batch 893, LR 0.000038 Loss 5.361167, Accuracy 85.950%\n",
      "Epoch 23, Batch 894, LR 0.000038 Loss 5.360593, Accuracy 85.955%\n",
      "Epoch 23, Batch 895, LR 0.000038 Loss 5.360807, Accuracy 85.952%\n",
      "Epoch 23, Batch 896, LR 0.000038 Loss 5.360069, Accuracy 85.958%\n",
      "Epoch 23, Batch 897, LR 0.000038 Loss 5.360242, Accuracy 85.958%\n",
      "Epoch 23, Batch 898, LR 0.000038 Loss 5.361007, Accuracy 85.958%\n",
      "Epoch 23, Batch 899, LR 0.000038 Loss 5.360721, Accuracy 85.957%\n",
      "Epoch 23, Batch 900, LR 0.000038 Loss 5.359941, Accuracy 85.960%\n",
      "Epoch 23, Batch 901, LR 0.000038 Loss 5.359614, Accuracy 85.958%\n",
      "Epoch 23, Batch 902, LR 0.000038 Loss 5.359334, Accuracy 85.962%\n",
      "Epoch 23, Batch 903, LR 0.000038 Loss 5.360188, Accuracy 85.961%\n",
      "Epoch 23, Batch 904, LR 0.000038 Loss 5.360692, Accuracy 85.956%\n",
      "Epoch 23, Batch 905, LR 0.000038 Loss 5.359867, Accuracy 85.959%\n",
      "Epoch 23, Batch 906, LR 0.000038 Loss 5.359537, Accuracy 85.964%\n",
      "Epoch 23, Batch 907, LR 0.000038 Loss 5.359676, Accuracy 85.966%\n",
      "Epoch 23, Batch 908, LR 0.000038 Loss 5.359695, Accuracy 85.965%\n",
      "Epoch 23, Batch 909, LR 0.000038 Loss 5.359535, Accuracy 85.965%\n",
      "Epoch 23, Batch 910, LR 0.000038 Loss 5.360243, Accuracy 85.961%\n",
      "Epoch 23, Batch 911, LR 0.000038 Loss 5.360388, Accuracy 85.958%\n",
      "Epoch 23, Batch 912, LR 0.000038 Loss 5.360093, Accuracy 85.958%\n",
      "Epoch 23, Batch 913, LR 0.000038 Loss 5.360332, Accuracy 85.956%\n",
      "Epoch 23, Batch 914, LR 0.000038 Loss 5.360688, Accuracy 85.956%\n",
      "Epoch 23, Batch 915, LR 0.000038 Loss 5.360697, Accuracy 85.953%\n",
      "Epoch 23, Batch 916, LR 0.000038 Loss 5.360777, Accuracy 85.952%\n",
      "Epoch 23, Batch 917, LR 0.000038 Loss 5.360257, Accuracy 85.955%\n",
      "Epoch 23, Batch 918, LR 0.000038 Loss 5.360996, Accuracy 85.955%\n",
      "Epoch 23, Batch 919, LR 0.000038 Loss 5.360896, Accuracy 85.953%\n",
      "Epoch 23, Batch 920, LR 0.000038 Loss 5.360519, Accuracy 85.954%\n",
      "Epoch 23, Batch 921, LR 0.000038 Loss 5.361391, Accuracy 85.949%\n",
      "Epoch 23, Batch 922, LR 0.000038 Loss 5.361764, Accuracy 85.948%\n",
      "Epoch 23, Batch 923, LR 0.000038 Loss 5.361855, Accuracy 85.944%\n",
      "Epoch 23, Batch 924, LR 0.000038 Loss 5.361800, Accuracy 85.944%\n",
      "Epoch 23, Batch 925, LR 0.000038 Loss 5.361472, Accuracy 85.944%\n",
      "Epoch 23, Batch 926, LR 0.000038 Loss 5.361404, Accuracy 85.945%\n",
      "Epoch 23, Batch 927, LR 0.000038 Loss 5.361002, Accuracy 85.946%\n",
      "Epoch 23, Batch 928, LR 0.000038 Loss 5.361700, Accuracy 85.944%\n",
      "Epoch 23, Batch 929, LR 0.000038 Loss 5.361283, Accuracy 85.948%\n",
      "Epoch 23, Batch 930, LR 0.000038 Loss 5.360674, Accuracy 85.953%\n",
      "Epoch 23, Batch 931, LR 0.000038 Loss 5.361055, Accuracy 85.947%\n",
      "Epoch 23, Batch 932, LR 0.000038 Loss 5.360321, Accuracy 85.948%\n",
      "Epoch 23, Batch 933, LR 0.000038 Loss 5.360344, Accuracy 85.948%\n",
      "Epoch 23, Batch 934, LR 0.000038 Loss 5.360390, Accuracy 85.947%\n",
      "Epoch 23, Batch 935, LR 0.000038 Loss 5.361156, Accuracy 85.946%\n",
      "Epoch 23, Batch 936, LR 0.000038 Loss 5.361663, Accuracy 85.947%\n",
      "Epoch 23, Batch 937, LR 0.000038 Loss 5.361685, Accuracy 85.948%\n",
      "Epoch 23, Batch 938, LR 0.000038 Loss 5.361857, Accuracy 85.945%\n",
      "Epoch 23, Batch 939, LR 0.000038 Loss 5.361526, Accuracy 85.947%\n",
      "Epoch 23, Batch 940, LR 0.000038 Loss 5.360923, Accuracy 85.952%\n",
      "Epoch 23, Batch 941, LR 0.000038 Loss 5.361154, Accuracy 85.952%\n",
      "Epoch 23, Batch 942, LR 0.000038 Loss 5.360898, Accuracy 85.950%\n",
      "Epoch 23, Batch 943, LR 0.000038 Loss 5.359908, Accuracy 85.952%\n",
      "Epoch 23, Batch 944, LR 0.000038 Loss 5.360608, Accuracy 85.947%\n",
      "Epoch 23, Batch 945, LR 0.000038 Loss 5.360731, Accuracy 85.949%\n",
      "Epoch 23, Batch 946, LR 0.000038 Loss 5.360321, Accuracy 85.952%\n",
      "Epoch 23, Batch 947, LR 0.000038 Loss 5.359350, Accuracy 85.959%\n",
      "Epoch 23, Batch 948, LR 0.000038 Loss 5.359798, Accuracy 85.959%\n",
      "Epoch 23, Batch 949, LR 0.000038 Loss 5.359938, Accuracy 85.954%\n",
      "Epoch 23, Batch 950, LR 0.000038 Loss 5.359710, Accuracy 85.954%\n",
      "Epoch 23, Batch 951, LR 0.000038 Loss 5.359622, Accuracy 85.952%\n",
      "Epoch 23, Batch 952, LR 0.000038 Loss 5.359490, Accuracy 85.955%\n",
      "Epoch 23, Batch 953, LR 0.000038 Loss 5.359466, Accuracy 85.954%\n",
      "Epoch 23, Batch 954, LR 0.000038 Loss 5.359000, Accuracy 85.956%\n",
      "Epoch 23, Batch 955, LR 0.000038 Loss 5.358406, Accuracy 85.957%\n",
      "Epoch 23, Batch 956, LR 0.000038 Loss 5.359151, Accuracy 85.955%\n",
      "Epoch 23, Batch 957, LR 0.000038 Loss 5.359853, Accuracy 85.955%\n",
      "Epoch 23, Batch 958, LR 0.000038 Loss 5.359745, Accuracy 85.955%\n",
      "Epoch 23, Batch 959, LR 0.000038 Loss 5.360162, Accuracy 85.950%\n",
      "Epoch 23, Batch 960, LR 0.000038 Loss 5.360362, Accuracy 85.947%\n",
      "Epoch 23, Batch 961, LR 0.000038 Loss 5.360671, Accuracy 85.945%\n",
      "Epoch 23, Batch 962, LR 0.000038 Loss 5.361058, Accuracy 85.938%\n",
      "Epoch 23, Batch 963, LR 0.000038 Loss 5.360184, Accuracy 85.945%\n",
      "Epoch 23, Batch 964, LR 0.000038 Loss 5.360135, Accuracy 85.950%\n",
      "Epoch 23, Batch 965, LR 0.000038 Loss 5.360817, Accuracy 85.945%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 966, LR 0.000038 Loss 5.362101, Accuracy 85.938%\n",
      "Epoch 23, Batch 967, LR 0.000038 Loss 5.361450, Accuracy 85.942%\n",
      "Epoch 23, Batch 968, LR 0.000038 Loss 5.361430, Accuracy 85.943%\n",
      "Epoch 23, Batch 969, LR 0.000038 Loss 5.360639, Accuracy 85.951%\n",
      "Epoch 23, Batch 970, LR 0.000038 Loss 5.360686, Accuracy 85.950%\n",
      "Epoch 23, Batch 971, LR 0.000038 Loss 5.360928, Accuracy 85.948%\n",
      "Epoch 23, Batch 972, LR 0.000038 Loss 5.360591, Accuracy 85.952%\n",
      "Epoch 23, Batch 973, LR 0.000038 Loss 5.360865, Accuracy 85.953%\n",
      "Epoch 23, Batch 974, LR 0.000038 Loss 5.360185, Accuracy 85.955%\n",
      "Epoch 23, Batch 975, LR 0.000038 Loss 5.360196, Accuracy 85.958%\n",
      "Epoch 23, Batch 976, LR 0.000038 Loss 5.359186, Accuracy 85.964%\n",
      "Epoch 23, Batch 977, LR 0.000038 Loss 5.358767, Accuracy 85.968%\n",
      "Epoch 23, Batch 978, LR 0.000038 Loss 5.358512, Accuracy 85.972%\n",
      "Epoch 23, Batch 979, LR 0.000038 Loss 5.358542, Accuracy 85.971%\n",
      "Epoch 23, Batch 980, LR 0.000038 Loss 5.359442, Accuracy 85.962%\n",
      "Epoch 23, Batch 981, LR 0.000038 Loss 5.359539, Accuracy 85.961%\n",
      "Epoch 23, Batch 982, LR 0.000038 Loss 5.359931, Accuracy 85.961%\n",
      "Epoch 23, Batch 983, LR 0.000038 Loss 5.360567, Accuracy 85.960%\n",
      "Epoch 23, Batch 984, LR 0.000038 Loss 5.360654, Accuracy 85.959%\n",
      "Epoch 23, Batch 985, LR 0.000038 Loss 5.360347, Accuracy 85.961%\n",
      "Epoch 23, Batch 986, LR 0.000038 Loss 5.360171, Accuracy 85.962%\n",
      "Epoch 23, Batch 987, LR 0.000038 Loss 5.359465, Accuracy 85.968%\n",
      "Epoch 23, Batch 988, LR 0.000038 Loss 5.359410, Accuracy 85.968%\n",
      "Epoch 23, Batch 989, LR 0.000038 Loss 5.358936, Accuracy 85.965%\n",
      "Epoch 23, Batch 990, LR 0.000038 Loss 5.358990, Accuracy 85.967%\n",
      "Epoch 23, Batch 991, LR 0.000038 Loss 5.358126, Accuracy 85.970%\n",
      "Epoch 23, Batch 992, LR 0.000038 Loss 5.357659, Accuracy 85.970%\n",
      "Epoch 23, Batch 993, LR 0.000038 Loss 5.357230, Accuracy 85.971%\n",
      "Epoch 23, Batch 994, LR 0.000038 Loss 5.357499, Accuracy 85.970%\n",
      "Epoch 23, Batch 995, LR 0.000038 Loss 5.357912, Accuracy 85.969%\n",
      "Epoch 23, Batch 996, LR 0.000038 Loss 5.358236, Accuracy 85.967%\n",
      "Epoch 23, Batch 997, LR 0.000038 Loss 5.358152, Accuracy 85.966%\n",
      "Epoch 23, Batch 998, LR 0.000038 Loss 5.358208, Accuracy 85.965%\n",
      "Epoch 23, Batch 999, LR 0.000038 Loss 5.358386, Accuracy 85.966%\n",
      "Epoch 23, Batch 1000, LR 0.000038 Loss 5.358111, Accuracy 85.970%\n",
      "Epoch 23, Batch 1001, LR 0.000038 Loss 5.357443, Accuracy 85.976%\n",
      "Epoch 23, Batch 1002, LR 0.000038 Loss 5.357454, Accuracy 85.976%\n",
      "Epoch 23, Batch 1003, LR 0.000038 Loss 5.356521, Accuracy 85.982%\n",
      "Epoch 23, Batch 1004, LR 0.000038 Loss 5.356384, Accuracy 85.984%\n",
      "Epoch 23, Batch 1005, LR 0.000038 Loss 5.355564, Accuracy 85.986%\n",
      "Epoch 23, Batch 1006, LR 0.000038 Loss 5.355254, Accuracy 85.990%\n",
      "Epoch 23, Batch 1007, LR 0.000038 Loss 5.355464, Accuracy 85.992%\n",
      "Epoch 23, Batch 1008, LR 0.000038 Loss 5.355245, Accuracy 85.991%\n",
      "Epoch 23, Batch 1009, LR 0.000038 Loss 5.355173, Accuracy 85.989%\n",
      "Epoch 23, Batch 1010, LR 0.000038 Loss 5.355010, Accuracy 85.992%\n",
      "Epoch 23, Batch 1011, LR 0.000038 Loss 5.353926, Accuracy 85.999%\n",
      "Epoch 23, Batch 1012, LR 0.000038 Loss 5.354215, Accuracy 85.997%\n",
      "Epoch 23, Batch 1013, LR 0.000038 Loss 5.354539, Accuracy 85.999%\n",
      "Epoch 23, Batch 1014, LR 0.000038 Loss 5.354900, Accuracy 86.004%\n",
      "Epoch 23, Batch 1015, LR 0.000038 Loss 5.355386, Accuracy 86.004%\n",
      "Epoch 23, Batch 1016, LR 0.000038 Loss 5.355333, Accuracy 86.005%\n",
      "Epoch 23, Batch 1017, LR 0.000038 Loss 5.354239, Accuracy 86.009%\n",
      "Epoch 23, Batch 1018, LR 0.000038 Loss 5.353724, Accuracy 86.012%\n",
      "Epoch 23, Batch 1019, LR 0.000038 Loss 5.353698, Accuracy 86.016%\n",
      "Epoch 23, Batch 1020, LR 0.000038 Loss 5.354577, Accuracy 86.009%\n",
      "Epoch 23, Batch 1021, LR 0.000038 Loss 5.354317, Accuracy 86.009%\n",
      "Epoch 23, Batch 1022, LR 0.000038 Loss 5.353645, Accuracy 86.008%\n",
      "Epoch 23, Batch 1023, LR 0.000038 Loss 5.353205, Accuracy 86.010%\n",
      "Epoch 23, Batch 1024, LR 0.000038 Loss 5.353947, Accuracy 86.008%\n",
      "Epoch 23, Batch 1025, LR 0.000038 Loss 5.353988, Accuracy 86.005%\n",
      "Epoch 23, Batch 1026, LR 0.000038 Loss 5.354299, Accuracy 86.006%\n",
      "Epoch 23, Batch 1027, LR 0.000038 Loss 5.353692, Accuracy 86.004%\n",
      "Epoch 23, Batch 1028, LR 0.000038 Loss 5.353220, Accuracy 86.006%\n",
      "Epoch 23, Batch 1029, LR 0.000038 Loss 5.353336, Accuracy 86.008%\n",
      "Epoch 23, Batch 1030, LR 0.000038 Loss 5.353202, Accuracy 86.007%\n",
      "Epoch 23, Batch 1031, LR 0.000038 Loss 5.353136, Accuracy 86.007%\n",
      "Epoch 23, Batch 1032, LR 0.000038 Loss 5.353600, Accuracy 86.005%\n",
      "Epoch 23, Batch 1033, LR 0.000038 Loss 5.352399, Accuracy 86.012%\n",
      "Epoch 23, Batch 1034, LR 0.000038 Loss 5.352636, Accuracy 86.007%\n",
      "Epoch 23, Batch 1035, LR 0.000038 Loss 5.351936, Accuracy 86.009%\n",
      "Epoch 23, Batch 1036, LR 0.000038 Loss 5.351966, Accuracy 86.008%\n",
      "Epoch 23, Batch 1037, LR 0.000038 Loss 5.352314, Accuracy 86.008%\n",
      "Epoch 23, Batch 1038, LR 0.000038 Loss 5.352575, Accuracy 86.007%\n",
      "Epoch 23, Batch 1039, LR 0.000038 Loss 5.352570, Accuracy 86.004%\n",
      "Epoch 23, Batch 1040, LR 0.000038 Loss 5.353249, Accuracy 86.001%\n",
      "Epoch 23, Batch 1041, LR 0.000038 Loss 5.353596, Accuracy 85.999%\n",
      "Epoch 23, Batch 1042, LR 0.000038 Loss 5.354190, Accuracy 85.998%\n",
      "Epoch 23, Batch 1043, LR 0.000038 Loss 5.353054, Accuracy 86.003%\n",
      "Epoch 23, Batch 1044, LR 0.000038 Loss 5.353811, Accuracy 86.000%\n",
      "Epoch 23, Batch 1045, LR 0.000038 Loss 5.353736, Accuracy 86.000%\n",
      "Epoch 23, Batch 1046, LR 0.000038 Loss 5.354018, Accuracy 85.996%\n",
      "Epoch 23, Batch 1047, LR 0.000038 Loss 5.354343, Accuracy 85.995%\n",
      "Epoch 23, Loss (train set) 5.354343, Accuracy (train set) 85.995%\n",
      "Epoch 24, Batch 1, LR 0.000038 Loss 5.457109, Accuracy 82.812%\n",
      "Epoch 24, Batch 2, LR 0.000038 Loss 5.163182, Accuracy 85.547%\n",
      "Epoch 24, Batch 3, LR 0.000038 Loss 5.261527, Accuracy 86.198%\n",
      "Epoch 24, Batch 4, LR 0.000038 Loss 5.446549, Accuracy 85.156%\n",
      "Epoch 24, Batch 5, LR 0.000038 Loss 5.531455, Accuracy 85.000%\n",
      "Epoch 24, Batch 6, LR 0.000038 Loss 5.553552, Accuracy 84.766%\n",
      "Epoch 24, Batch 7, LR 0.000038 Loss 5.428504, Accuracy 85.491%\n",
      "Epoch 24, Batch 8, LR 0.000038 Loss 5.343271, Accuracy 85.352%\n",
      "Epoch 24, Batch 9, LR 0.000038 Loss 5.360203, Accuracy 85.243%\n",
      "Epoch 24, Batch 10, LR 0.000038 Loss 5.383759, Accuracy 85.312%\n",
      "Epoch 24, Batch 11, LR 0.000038 Loss 5.314626, Accuracy 85.795%\n",
      "Epoch 24, Batch 12, LR 0.000038 Loss 5.361587, Accuracy 85.352%\n",
      "Epoch 24, Batch 13, LR 0.000038 Loss 5.338165, Accuracy 85.577%\n",
      "Epoch 24, Batch 14, LR 0.000038 Loss 5.265135, Accuracy 85.938%\n",
      "Epoch 24, Batch 15, LR 0.000038 Loss 5.309026, Accuracy 85.833%\n",
      "Epoch 24, Batch 16, LR 0.000038 Loss 5.349360, Accuracy 85.400%\n",
      "Epoch 24, Batch 17, LR 0.000038 Loss 5.324217, Accuracy 85.478%\n",
      "Epoch 24, Batch 18, LR 0.000038 Loss 5.334717, Accuracy 85.634%\n",
      "Epoch 24, Batch 19, LR 0.000038 Loss 5.309721, Accuracy 85.691%\n",
      "Epoch 24, Batch 20, LR 0.000038 Loss 5.340261, Accuracy 85.547%\n",
      "Epoch 24, Batch 21, LR 0.000038 Loss 5.341716, Accuracy 85.491%\n",
      "Epoch 24, Batch 22, LR 0.000038 Loss 5.328406, Accuracy 85.547%\n",
      "Epoch 24, Batch 23, LR 0.000038 Loss 5.345413, Accuracy 85.394%\n",
      "Epoch 24, Batch 24, LR 0.000038 Loss 5.359087, Accuracy 85.417%\n",
      "Epoch 24, Batch 25, LR 0.000038 Loss 5.334822, Accuracy 85.562%\n",
      "Epoch 24, Batch 26, LR 0.000038 Loss 5.338029, Accuracy 85.457%\n",
      "Epoch 24, Batch 27, LR 0.000038 Loss 5.333943, Accuracy 85.417%\n",
      "Epoch 24, Batch 28, LR 0.000038 Loss 5.331201, Accuracy 85.352%\n",
      "Epoch 24, Batch 29, LR 0.000038 Loss 5.309816, Accuracy 85.533%\n",
      "Epoch 24, Batch 30, LR 0.000038 Loss 5.298705, Accuracy 85.755%\n",
      "Epoch 24, Batch 31, LR 0.000038 Loss 5.279521, Accuracy 85.887%\n",
      "Epoch 24, Batch 32, LR 0.000038 Loss 5.270510, Accuracy 85.913%\n",
      "Epoch 24, Batch 33, LR 0.000038 Loss 5.257625, Accuracy 85.914%\n",
      "Epoch 24, Batch 34, LR 0.000038 Loss 5.263960, Accuracy 85.960%\n",
      "Epoch 24, Batch 35, LR 0.000038 Loss 5.260579, Accuracy 85.893%\n",
      "Epoch 24, Batch 36, LR 0.000038 Loss 5.250879, Accuracy 86.024%\n",
      "Epoch 24, Batch 37, LR 0.000038 Loss 5.288319, Accuracy 85.916%\n",
      "Epoch 24, Batch 38, LR 0.000038 Loss 5.274342, Accuracy 85.999%\n",
      "Epoch 24, Batch 39, LR 0.000038 Loss 5.275731, Accuracy 86.018%\n",
      "Epoch 24, Batch 40, LR 0.000038 Loss 5.272610, Accuracy 86.016%\n",
      "Epoch 24, Batch 41, LR 0.000038 Loss 5.258815, Accuracy 86.147%\n",
      "Epoch 24, Batch 42, LR 0.000038 Loss 5.269087, Accuracy 86.068%\n",
      "Epoch 24, Batch 43, LR 0.000038 Loss 5.272624, Accuracy 86.028%\n",
      "Epoch 24, Batch 44, LR 0.000038 Loss 5.291613, Accuracy 85.831%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 45, LR 0.000038 Loss 5.294914, Accuracy 85.851%\n",
      "Epoch 24, Batch 46, LR 0.000038 Loss 5.297218, Accuracy 85.819%\n",
      "Epoch 24, Batch 47, LR 0.000038 Loss 5.287810, Accuracy 85.838%\n",
      "Epoch 24, Batch 48, LR 0.000038 Loss 5.287741, Accuracy 85.856%\n",
      "Epoch 24, Batch 49, LR 0.000038 Loss 5.286354, Accuracy 85.826%\n",
      "Epoch 24, Batch 50, LR 0.000038 Loss 5.275297, Accuracy 85.891%\n",
      "Epoch 24, Batch 51, LR 0.000038 Loss 5.274007, Accuracy 85.861%\n",
      "Epoch 24, Batch 52, LR 0.000038 Loss 5.273567, Accuracy 85.802%\n",
      "Epoch 24, Batch 53, LR 0.000038 Loss 5.273504, Accuracy 85.820%\n",
      "Epoch 24, Batch 54, LR 0.000038 Loss 5.275864, Accuracy 85.807%\n",
      "Epoch 24, Batch 55, LR 0.000038 Loss 5.277316, Accuracy 85.810%\n",
      "Epoch 24, Batch 56, LR 0.000038 Loss 5.278116, Accuracy 85.896%\n",
      "Epoch 24, Batch 57, LR 0.000038 Loss 5.275275, Accuracy 85.896%\n",
      "Epoch 24, Batch 58, LR 0.000038 Loss 5.268936, Accuracy 85.964%\n",
      "Epoch 24, Batch 59, LR 0.000038 Loss 5.267613, Accuracy 85.951%\n",
      "Epoch 24, Batch 60, LR 0.000037 Loss 5.262372, Accuracy 86.042%\n",
      "Epoch 24, Batch 61, LR 0.000037 Loss 5.266608, Accuracy 86.014%\n",
      "Epoch 24, Batch 62, LR 0.000037 Loss 5.265951, Accuracy 85.988%\n",
      "Epoch 24, Batch 63, LR 0.000037 Loss 5.264421, Accuracy 85.987%\n",
      "Epoch 24, Batch 64, LR 0.000037 Loss 5.284839, Accuracy 85.986%\n",
      "Epoch 24, Batch 65, LR 0.000037 Loss 5.275949, Accuracy 85.986%\n",
      "Epoch 24, Batch 66, LR 0.000037 Loss 5.269941, Accuracy 86.044%\n",
      "Epoch 24, Batch 67, LR 0.000037 Loss 5.277972, Accuracy 86.031%\n",
      "Epoch 24, Batch 68, LR 0.000037 Loss 5.276749, Accuracy 86.041%\n",
      "Epoch 24, Batch 69, LR 0.000037 Loss 5.276584, Accuracy 86.028%\n",
      "Epoch 24, Batch 70, LR 0.000037 Loss 5.280013, Accuracy 86.016%\n",
      "Epoch 24, Batch 71, LR 0.000037 Loss 5.277452, Accuracy 86.026%\n",
      "Epoch 24, Batch 72, LR 0.000037 Loss 5.277781, Accuracy 86.035%\n",
      "Epoch 24, Batch 73, LR 0.000037 Loss 5.275430, Accuracy 86.055%\n",
      "Epoch 24, Batch 74, LR 0.000037 Loss 5.273473, Accuracy 86.033%\n",
      "Epoch 24, Batch 75, LR 0.000037 Loss 5.277271, Accuracy 85.958%\n",
      "Epoch 24, Batch 76, LR 0.000037 Loss 5.279174, Accuracy 85.979%\n",
      "Epoch 24, Batch 77, LR 0.000037 Loss 5.294684, Accuracy 85.887%\n",
      "Epoch 24, Batch 78, LR 0.000037 Loss 5.289020, Accuracy 85.988%\n",
      "Epoch 24, Batch 79, LR 0.000037 Loss 5.291314, Accuracy 85.997%\n",
      "Epoch 24, Batch 80, LR 0.000037 Loss 5.286510, Accuracy 86.035%\n",
      "Epoch 24, Batch 81, LR 0.000037 Loss 5.291083, Accuracy 86.015%\n",
      "Epoch 24, Batch 82, LR 0.000037 Loss 5.291506, Accuracy 86.052%\n",
      "Epoch 24, Batch 83, LR 0.000037 Loss 5.287916, Accuracy 86.116%\n",
      "Epoch 24, Batch 84, LR 0.000037 Loss 5.291161, Accuracy 86.151%\n",
      "Epoch 24, Batch 85, LR 0.000037 Loss 5.291358, Accuracy 86.131%\n",
      "Epoch 24, Batch 86, LR 0.000037 Loss 5.292795, Accuracy 86.146%\n",
      "Epoch 24, Batch 87, LR 0.000037 Loss 5.282266, Accuracy 86.180%\n",
      "Epoch 24, Batch 88, LR 0.000037 Loss 5.285665, Accuracy 86.159%\n",
      "Epoch 24, Batch 89, LR 0.000037 Loss 5.283851, Accuracy 86.131%\n",
      "Epoch 24, Batch 90, LR 0.000037 Loss 5.278910, Accuracy 86.181%\n",
      "Epoch 24, Batch 91, LR 0.000037 Loss 5.277934, Accuracy 86.229%\n",
      "Epoch 24, Batch 92, LR 0.000037 Loss 5.272092, Accuracy 86.252%\n",
      "Epoch 24, Batch 93, LR 0.000037 Loss 5.272370, Accuracy 86.223%\n",
      "Epoch 24, Batch 94, LR 0.000037 Loss 5.272290, Accuracy 86.195%\n",
      "Epoch 24, Batch 95, LR 0.000037 Loss 5.275769, Accuracy 86.192%\n",
      "Epoch 24, Batch 96, LR 0.000037 Loss 5.277872, Accuracy 86.182%\n",
      "Epoch 24, Batch 97, LR 0.000037 Loss 5.272887, Accuracy 86.227%\n",
      "Epoch 24, Batch 98, LR 0.000037 Loss 5.274718, Accuracy 86.209%\n",
      "Epoch 24, Batch 99, LR 0.000037 Loss 5.282176, Accuracy 86.182%\n",
      "Epoch 24, Batch 100, LR 0.000037 Loss 5.278379, Accuracy 86.172%\n",
      "Epoch 24, Batch 101, LR 0.000037 Loss 5.269984, Accuracy 86.185%\n",
      "Epoch 24, Batch 102, LR 0.000037 Loss 5.274361, Accuracy 86.160%\n",
      "Epoch 24, Batch 103, LR 0.000037 Loss 5.277937, Accuracy 86.097%\n",
      "Epoch 24, Batch 104, LR 0.000037 Loss 5.279149, Accuracy 86.125%\n",
      "Epoch 24, Batch 105, LR 0.000037 Loss 5.277831, Accuracy 86.101%\n",
      "Epoch 24, Batch 106, LR 0.000037 Loss 5.278651, Accuracy 86.122%\n",
      "Epoch 24, Batch 107, LR 0.000037 Loss 5.280692, Accuracy 86.142%\n",
      "Epoch 24, Batch 108, LR 0.000037 Loss 5.288774, Accuracy 86.097%\n",
      "Epoch 24, Batch 109, LR 0.000037 Loss 5.285744, Accuracy 86.110%\n",
      "Epoch 24, Batch 110, LR 0.000037 Loss 5.281721, Accuracy 86.122%\n",
      "Epoch 24, Batch 111, LR 0.000037 Loss 5.277687, Accuracy 86.099%\n",
      "Epoch 24, Batch 112, LR 0.000037 Loss 5.273759, Accuracy 86.147%\n",
      "Epoch 24, Batch 113, LR 0.000037 Loss 5.276750, Accuracy 86.145%\n",
      "Epoch 24, Batch 114, LR 0.000037 Loss 5.276813, Accuracy 86.129%\n",
      "Epoch 24, Batch 115, LR 0.000037 Loss 5.287279, Accuracy 86.114%\n",
      "Epoch 24, Batch 116, LR 0.000037 Loss 5.285208, Accuracy 86.140%\n",
      "Epoch 24, Batch 117, LR 0.000037 Loss 5.281221, Accuracy 86.165%\n",
      "Epoch 24, Batch 118, LR 0.000037 Loss 5.276064, Accuracy 86.169%\n",
      "Epoch 24, Batch 119, LR 0.000037 Loss 5.273028, Accuracy 86.194%\n",
      "Epoch 24, Batch 120, LR 0.000037 Loss 5.271487, Accuracy 86.191%\n",
      "Epoch 24, Batch 121, LR 0.000037 Loss 5.269185, Accuracy 86.202%\n",
      "Epoch 24, Batch 122, LR 0.000037 Loss 5.274326, Accuracy 86.194%\n",
      "Epoch 24, Batch 123, LR 0.000037 Loss 5.275545, Accuracy 86.204%\n",
      "Epoch 24, Batch 124, LR 0.000037 Loss 5.279519, Accuracy 86.190%\n",
      "Epoch 24, Batch 125, LR 0.000037 Loss 5.273664, Accuracy 86.213%\n",
      "Epoch 24, Batch 126, LR 0.000037 Loss 5.275581, Accuracy 86.210%\n",
      "Epoch 24, Batch 127, LR 0.000037 Loss 5.273913, Accuracy 86.196%\n",
      "Epoch 24, Batch 128, LR 0.000037 Loss 5.275206, Accuracy 86.188%\n",
      "Epoch 24, Batch 129, LR 0.000037 Loss 5.275650, Accuracy 86.210%\n",
      "Epoch 24, Batch 130, LR 0.000037 Loss 5.270447, Accuracy 86.226%\n",
      "Epoch 24, Batch 131, LR 0.000037 Loss 5.268033, Accuracy 86.224%\n",
      "Epoch 24, Batch 132, LR 0.000037 Loss 5.262796, Accuracy 86.239%\n",
      "Epoch 24, Batch 133, LR 0.000037 Loss 5.264302, Accuracy 86.237%\n",
      "Epoch 24, Batch 134, LR 0.000037 Loss 5.264094, Accuracy 86.217%\n",
      "Epoch 24, Batch 135, LR 0.000037 Loss 5.257811, Accuracy 86.250%\n",
      "Epoch 24, Batch 136, LR 0.000037 Loss 5.262929, Accuracy 86.225%\n",
      "Epoch 24, Batch 137, LR 0.000037 Loss 5.265693, Accuracy 86.206%\n",
      "Epoch 24, Batch 138, LR 0.000037 Loss 5.267056, Accuracy 86.215%\n",
      "Epoch 24, Batch 139, LR 0.000037 Loss 5.268321, Accuracy 86.196%\n",
      "Epoch 24, Batch 140, LR 0.000037 Loss 5.271054, Accuracy 86.183%\n",
      "Epoch 24, Batch 141, LR 0.000037 Loss 5.268564, Accuracy 86.170%\n",
      "Epoch 24, Batch 142, LR 0.000037 Loss 5.275071, Accuracy 86.147%\n",
      "Epoch 24, Batch 143, LR 0.000037 Loss 5.274808, Accuracy 86.178%\n",
      "Epoch 24, Batch 144, LR 0.000037 Loss 5.275262, Accuracy 86.198%\n",
      "Epoch 24, Batch 145, LR 0.000037 Loss 5.277249, Accuracy 86.202%\n",
      "Epoch 24, Batch 146, LR 0.000037 Loss 5.278447, Accuracy 86.205%\n",
      "Epoch 24, Batch 147, LR 0.000037 Loss 5.274256, Accuracy 86.209%\n",
      "Epoch 24, Batch 148, LR 0.000037 Loss 5.275335, Accuracy 86.201%\n",
      "Epoch 24, Batch 149, LR 0.000037 Loss 5.276662, Accuracy 86.231%\n",
      "Epoch 24, Batch 150, LR 0.000037 Loss 5.278076, Accuracy 86.214%\n",
      "Epoch 24, Batch 151, LR 0.000037 Loss 5.280318, Accuracy 86.186%\n",
      "Epoch 24, Batch 152, LR 0.000037 Loss 5.276890, Accuracy 86.200%\n",
      "Epoch 24, Batch 153, LR 0.000037 Loss 5.279648, Accuracy 86.183%\n",
      "Epoch 24, Batch 154, LR 0.000037 Loss 5.277383, Accuracy 86.196%\n",
      "Epoch 24, Batch 155, LR 0.000037 Loss 5.281299, Accuracy 86.190%\n",
      "Epoch 24, Batch 156, LR 0.000037 Loss 5.282744, Accuracy 86.178%\n",
      "Epoch 24, Batch 157, LR 0.000037 Loss 5.284369, Accuracy 86.146%\n",
      "Epoch 24, Batch 158, LR 0.000037 Loss 5.288782, Accuracy 86.140%\n",
      "Epoch 24, Batch 159, LR 0.000037 Loss 5.284940, Accuracy 86.159%\n",
      "Epoch 24, Batch 160, LR 0.000037 Loss 5.286895, Accuracy 86.157%\n",
      "Epoch 24, Batch 161, LR 0.000037 Loss 5.281572, Accuracy 86.175%\n",
      "Epoch 24, Batch 162, LR 0.000037 Loss 5.281294, Accuracy 86.179%\n",
      "Epoch 24, Batch 163, LR 0.000037 Loss 5.288326, Accuracy 86.148%\n",
      "Epoch 24, Batch 164, LR 0.000037 Loss 5.290467, Accuracy 86.133%\n",
      "Epoch 24, Batch 165, LR 0.000037 Loss 5.291210, Accuracy 86.141%\n",
      "Epoch 24, Batch 166, LR 0.000037 Loss 5.290521, Accuracy 86.154%\n",
      "Epoch 24, Batch 167, LR 0.000037 Loss 5.291109, Accuracy 86.167%\n",
      "Epoch 24, Batch 168, LR 0.000037 Loss 5.287607, Accuracy 86.184%\n",
      "Epoch 24, Batch 169, LR 0.000037 Loss 5.288662, Accuracy 86.192%\n",
      "Epoch 24, Batch 170, LR 0.000037 Loss 5.286738, Accuracy 86.199%\n",
      "Epoch 24, Batch 171, LR 0.000037 Loss 5.284763, Accuracy 86.212%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 172, LR 0.000037 Loss 5.284282, Accuracy 86.205%\n",
      "Epoch 24, Batch 173, LR 0.000037 Loss 5.281357, Accuracy 86.204%\n",
      "Epoch 24, Batch 174, LR 0.000037 Loss 5.281539, Accuracy 86.198%\n",
      "Epoch 24, Batch 175, LR 0.000037 Loss 5.280663, Accuracy 86.232%\n",
      "Epoch 24, Batch 176, LR 0.000037 Loss 5.280310, Accuracy 86.244%\n",
      "Epoch 24, Batch 177, LR 0.000037 Loss 5.277971, Accuracy 86.255%\n",
      "Epoch 24, Batch 178, LR 0.000037 Loss 5.278548, Accuracy 86.267%\n",
      "Epoch 24, Batch 179, LR 0.000037 Loss 5.278576, Accuracy 86.274%\n",
      "Epoch 24, Batch 180, LR 0.000037 Loss 5.278179, Accuracy 86.276%\n",
      "Epoch 24, Batch 181, LR 0.000037 Loss 5.280541, Accuracy 86.261%\n",
      "Epoch 24, Batch 182, LR 0.000037 Loss 5.281332, Accuracy 86.272%\n",
      "Epoch 24, Batch 183, LR 0.000037 Loss 5.281391, Accuracy 86.262%\n",
      "Epoch 24, Batch 184, LR 0.000037 Loss 5.281229, Accuracy 86.256%\n",
      "Epoch 24, Batch 185, LR 0.000037 Loss 5.278994, Accuracy 86.250%\n",
      "Epoch 24, Batch 186, LR 0.000037 Loss 5.280008, Accuracy 86.240%\n",
      "Epoch 24, Batch 187, LR 0.000037 Loss 5.280046, Accuracy 86.230%\n",
      "Epoch 24, Batch 188, LR 0.000037 Loss 5.281365, Accuracy 86.237%\n",
      "Epoch 24, Batch 189, LR 0.000037 Loss 5.277537, Accuracy 86.264%\n",
      "Epoch 24, Batch 190, LR 0.000037 Loss 5.277437, Accuracy 86.283%\n",
      "Epoch 24, Batch 191, LR 0.000037 Loss 5.276523, Accuracy 86.273%\n",
      "Epoch 24, Batch 192, LR 0.000037 Loss 5.276370, Accuracy 86.279%\n",
      "Epoch 24, Batch 193, LR 0.000037 Loss 5.275526, Accuracy 86.294%\n",
      "Epoch 24, Batch 194, LR 0.000037 Loss 5.278156, Accuracy 86.288%\n",
      "Epoch 24, Batch 195, LR 0.000037 Loss 5.283468, Accuracy 86.274%\n",
      "Epoch 24, Batch 196, LR 0.000037 Loss 5.282874, Accuracy 86.272%\n",
      "Epoch 24, Batch 197, LR 0.000037 Loss 5.281411, Accuracy 86.279%\n",
      "Epoch 24, Batch 198, LR 0.000037 Loss 5.286082, Accuracy 86.245%\n",
      "Epoch 24, Batch 199, LR 0.000037 Loss 5.285067, Accuracy 86.267%\n",
      "Epoch 24, Batch 200, LR 0.000037 Loss 5.284392, Accuracy 86.285%\n",
      "Epoch 24, Batch 201, LR 0.000037 Loss 5.283852, Accuracy 86.276%\n",
      "Epoch 24, Batch 202, LR 0.000037 Loss 5.283547, Accuracy 86.278%\n",
      "Epoch 24, Batch 203, LR 0.000037 Loss 5.280550, Accuracy 86.288%\n",
      "Epoch 24, Batch 204, LR 0.000037 Loss 5.278893, Accuracy 86.301%\n",
      "Epoch 24, Batch 205, LR 0.000037 Loss 5.280531, Accuracy 86.277%\n",
      "Epoch 24, Batch 206, LR 0.000037 Loss 5.281120, Accuracy 86.279%\n",
      "Epoch 24, Batch 207, LR 0.000037 Loss 5.283013, Accuracy 86.285%\n",
      "Epoch 24, Batch 208, LR 0.000037 Loss 5.282975, Accuracy 86.291%\n",
      "Epoch 24, Batch 209, LR 0.000037 Loss 5.283519, Accuracy 86.300%\n",
      "Epoch 24, Batch 210, LR 0.000037 Loss 5.279411, Accuracy 86.321%\n",
      "Epoch 24, Batch 211, LR 0.000037 Loss 5.278654, Accuracy 86.311%\n",
      "Epoch 24, Batch 212, LR 0.000037 Loss 5.279758, Accuracy 86.310%\n",
      "Epoch 24, Batch 213, LR 0.000037 Loss 5.281706, Accuracy 86.301%\n",
      "Epoch 24, Batch 214, LR 0.000037 Loss 5.281472, Accuracy 86.299%\n",
      "Epoch 24, Batch 215, LR 0.000037 Loss 5.277015, Accuracy 86.315%\n",
      "Epoch 24, Batch 216, LR 0.000037 Loss 5.279536, Accuracy 86.310%\n",
      "Epoch 24, Batch 217, LR 0.000037 Loss 5.280307, Accuracy 86.298%\n",
      "Epoch 24, Batch 218, LR 0.000037 Loss 5.275757, Accuracy 86.332%\n",
      "Epoch 24, Batch 219, LR 0.000037 Loss 5.279920, Accuracy 86.294%\n",
      "Epoch 24, Batch 220, LR 0.000037 Loss 5.279890, Accuracy 86.271%\n",
      "Epoch 24, Batch 221, LR 0.000037 Loss 5.280301, Accuracy 86.259%\n",
      "Epoch 24, Batch 222, LR 0.000037 Loss 5.280597, Accuracy 86.258%\n",
      "Epoch 24, Batch 223, LR 0.000037 Loss 5.282389, Accuracy 86.249%\n",
      "Epoch 24, Batch 224, LR 0.000037 Loss 5.282714, Accuracy 86.262%\n",
      "Epoch 24, Batch 225, LR 0.000037 Loss 5.282090, Accuracy 86.253%\n",
      "Epoch 24, Batch 226, LR 0.000037 Loss 5.281341, Accuracy 86.259%\n",
      "Epoch 24, Batch 227, LR 0.000037 Loss 5.280618, Accuracy 86.268%\n",
      "Epoch 24, Batch 228, LR 0.000037 Loss 5.277961, Accuracy 86.287%\n",
      "Epoch 24, Batch 229, LR 0.000037 Loss 5.277447, Accuracy 86.313%\n",
      "Epoch 24, Batch 230, LR 0.000037 Loss 5.279007, Accuracy 86.304%\n",
      "Epoch 24, Batch 231, LR 0.000037 Loss 5.276976, Accuracy 86.313%\n",
      "Epoch 24, Batch 232, LR 0.000037 Loss 5.273289, Accuracy 86.338%\n",
      "Epoch 24, Batch 233, LR 0.000037 Loss 5.270960, Accuracy 86.360%\n",
      "Epoch 24, Batch 234, LR 0.000037 Loss 5.272570, Accuracy 86.351%\n",
      "Epoch 24, Batch 235, LR 0.000037 Loss 5.270601, Accuracy 86.356%\n",
      "Epoch 24, Batch 236, LR 0.000037 Loss 5.270449, Accuracy 86.365%\n",
      "Epoch 24, Batch 237, LR 0.000037 Loss 5.270812, Accuracy 86.369%\n",
      "Epoch 24, Batch 238, LR 0.000037 Loss 5.270612, Accuracy 86.371%\n",
      "Epoch 24, Batch 239, LR 0.000037 Loss 5.269575, Accuracy 86.379%\n",
      "Epoch 24, Batch 240, LR 0.000037 Loss 5.266814, Accuracy 86.390%\n",
      "Epoch 24, Batch 241, LR 0.000037 Loss 5.267692, Accuracy 86.388%\n",
      "Epoch 24, Batch 242, LR 0.000037 Loss 5.269916, Accuracy 86.383%\n",
      "Epoch 24, Batch 243, LR 0.000037 Loss 5.271822, Accuracy 86.378%\n",
      "Epoch 24, Batch 244, LR 0.000037 Loss 5.271697, Accuracy 86.389%\n",
      "Epoch 24, Batch 245, LR 0.000037 Loss 5.274021, Accuracy 86.384%\n",
      "Epoch 24, Batch 246, LR 0.000037 Loss 5.274767, Accuracy 86.376%\n",
      "Epoch 24, Batch 247, LR 0.000037 Loss 5.275040, Accuracy 86.380%\n",
      "Epoch 24, Batch 248, LR 0.000037 Loss 5.276727, Accuracy 86.347%\n",
      "Epoch 24, Batch 249, LR 0.000037 Loss 5.275618, Accuracy 86.336%\n",
      "Epoch 24, Batch 250, LR 0.000037 Loss 5.277377, Accuracy 86.328%\n",
      "Epoch 24, Batch 251, LR 0.000037 Loss 5.277730, Accuracy 86.336%\n",
      "Epoch 24, Batch 252, LR 0.000037 Loss 5.278181, Accuracy 86.344%\n",
      "Epoch 24, Batch 253, LR 0.000037 Loss 5.280054, Accuracy 86.323%\n",
      "Epoch 24, Batch 254, LR 0.000037 Loss 5.280182, Accuracy 86.316%\n",
      "Epoch 24, Batch 255, LR 0.000037 Loss 5.278943, Accuracy 86.320%\n",
      "Epoch 24, Batch 256, LR 0.000037 Loss 5.278989, Accuracy 86.337%\n",
      "Epoch 24, Batch 257, LR 0.000037 Loss 5.280911, Accuracy 86.339%\n",
      "Epoch 24, Batch 258, LR 0.000037 Loss 5.281741, Accuracy 86.331%\n",
      "Epoch 24, Batch 259, LR 0.000037 Loss 5.279822, Accuracy 86.336%\n",
      "Epoch 24, Batch 260, LR 0.000037 Loss 5.276003, Accuracy 86.358%\n",
      "Epoch 24, Batch 261, LR 0.000037 Loss 5.277338, Accuracy 86.366%\n",
      "Epoch 24, Batch 262, LR 0.000037 Loss 5.274907, Accuracy 86.385%\n",
      "Epoch 24, Batch 263, LR 0.000037 Loss 5.272789, Accuracy 86.392%\n",
      "Epoch 24, Batch 264, LR 0.000037 Loss 5.272135, Accuracy 86.390%\n",
      "Epoch 24, Batch 265, LR 0.000037 Loss 5.268597, Accuracy 86.418%\n",
      "Epoch 24, Batch 266, LR 0.000037 Loss 5.270394, Accuracy 86.393%\n",
      "Epoch 24, Batch 267, LR 0.000037 Loss 5.271508, Accuracy 86.397%\n",
      "Epoch 24, Batch 268, LR 0.000037 Loss 5.271534, Accuracy 86.386%\n",
      "Epoch 24, Batch 269, LR 0.000037 Loss 5.273680, Accuracy 86.376%\n",
      "Epoch 24, Batch 270, LR 0.000037 Loss 5.275364, Accuracy 86.374%\n",
      "Epoch 24, Batch 271, LR 0.000037 Loss 5.277074, Accuracy 86.367%\n",
      "Epoch 24, Batch 272, LR 0.000037 Loss 5.278481, Accuracy 86.365%\n",
      "Epoch 24, Batch 273, LR 0.000037 Loss 5.275808, Accuracy 86.375%\n",
      "Epoch 24, Batch 274, LR 0.000037 Loss 5.276652, Accuracy 86.368%\n",
      "Epoch 24, Batch 275, LR 0.000037 Loss 5.275717, Accuracy 86.372%\n",
      "Epoch 24, Batch 276, LR 0.000037 Loss 5.273779, Accuracy 86.388%\n",
      "Epoch 24, Batch 277, LR 0.000037 Loss 5.271695, Accuracy 86.394%\n",
      "Epoch 24, Batch 278, LR 0.000037 Loss 5.270553, Accuracy 86.396%\n",
      "Epoch 24, Batch 279, LR 0.000037 Loss 5.271244, Accuracy 86.391%\n",
      "Epoch 24, Batch 280, LR 0.000037 Loss 5.270543, Accuracy 86.395%\n",
      "Epoch 24, Batch 281, LR 0.000037 Loss 5.271679, Accuracy 86.391%\n",
      "Epoch 24, Batch 282, LR 0.000037 Loss 5.271670, Accuracy 86.384%\n",
      "Epoch 24, Batch 283, LR 0.000037 Loss 5.271866, Accuracy 86.385%\n",
      "Epoch 24, Batch 284, LR 0.000037 Loss 5.271822, Accuracy 86.380%\n",
      "Epoch 24, Batch 285, LR 0.000037 Loss 5.270039, Accuracy 86.398%\n",
      "Epoch 24, Batch 286, LR 0.000037 Loss 5.269347, Accuracy 86.396%\n",
      "Epoch 24, Batch 287, LR 0.000037 Loss 5.269589, Accuracy 86.398%\n",
      "Epoch 24, Batch 288, LR 0.000037 Loss 5.270773, Accuracy 86.396%\n",
      "Epoch 24, Batch 289, LR 0.000037 Loss 5.269375, Accuracy 86.408%\n",
      "Epoch 24, Batch 290, LR 0.000037 Loss 5.268683, Accuracy 86.417%\n",
      "Epoch 24, Batch 291, LR 0.000037 Loss 5.267059, Accuracy 86.429%\n",
      "Epoch 24, Batch 292, LR 0.000037 Loss 5.266817, Accuracy 86.430%\n",
      "Epoch 24, Batch 293, LR 0.000037 Loss 5.265742, Accuracy 86.420%\n",
      "Epoch 24, Batch 294, LR 0.000037 Loss 5.267110, Accuracy 86.418%\n",
      "Epoch 24, Batch 295, LR 0.000037 Loss 5.270981, Accuracy 86.398%\n",
      "Epoch 24, Batch 296, LR 0.000037 Loss 5.269222, Accuracy 86.402%\n",
      "Epoch 24, Batch 297, LR 0.000037 Loss 5.269230, Accuracy 86.393%\n",
      "Epoch 24, Batch 298, LR 0.000037 Loss 5.270628, Accuracy 86.394%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 299, LR 0.000037 Loss 5.272596, Accuracy 86.392%\n",
      "Epoch 24, Batch 300, LR 0.000037 Loss 5.274102, Accuracy 86.385%\n",
      "Epoch 24, Batch 301, LR 0.000037 Loss 5.272253, Accuracy 86.392%\n",
      "Epoch 24, Batch 302, LR 0.000037 Loss 5.271149, Accuracy 86.401%\n",
      "Epoch 24, Batch 303, LR 0.000037 Loss 5.273492, Accuracy 86.402%\n",
      "Epoch 24, Batch 304, LR 0.000037 Loss 5.274885, Accuracy 86.387%\n",
      "Epoch 24, Batch 305, LR 0.000037 Loss 5.275167, Accuracy 86.378%\n",
      "Epoch 24, Batch 306, LR 0.000037 Loss 5.275133, Accuracy 86.369%\n",
      "Epoch 24, Batch 307, LR 0.000037 Loss 5.277217, Accuracy 86.347%\n",
      "Epoch 24, Batch 308, LR 0.000037 Loss 5.281973, Accuracy 86.326%\n",
      "Epoch 24, Batch 309, LR 0.000037 Loss 5.283286, Accuracy 86.314%\n",
      "Epoch 24, Batch 310, LR 0.000037 Loss 5.283726, Accuracy 86.318%\n",
      "Epoch 24, Batch 311, LR 0.000037 Loss 5.284816, Accuracy 86.309%\n",
      "Epoch 24, Batch 312, LR 0.000037 Loss 5.283802, Accuracy 86.328%\n",
      "Epoch 24, Batch 313, LR 0.000037 Loss 5.284088, Accuracy 86.322%\n",
      "Epoch 24, Batch 314, LR 0.000037 Loss 5.283107, Accuracy 86.326%\n",
      "Epoch 24, Batch 315, LR 0.000037 Loss 5.282938, Accuracy 86.327%\n",
      "Epoch 24, Batch 316, LR 0.000037 Loss 5.282599, Accuracy 86.331%\n",
      "Epoch 24, Batch 317, LR 0.000037 Loss 5.278747, Accuracy 86.354%\n",
      "Epoch 24, Batch 318, LR 0.000037 Loss 5.278977, Accuracy 86.358%\n",
      "Epoch 24, Batch 319, LR 0.000037 Loss 5.278868, Accuracy 86.351%\n",
      "Epoch 24, Batch 320, LR 0.000037 Loss 5.278646, Accuracy 86.350%\n",
      "Epoch 24, Batch 321, LR 0.000037 Loss 5.279379, Accuracy 86.354%\n",
      "Epoch 24, Batch 322, LR 0.000036 Loss 5.278170, Accuracy 86.365%\n",
      "Epoch 24, Batch 323, LR 0.000036 Loss 5.277724, Accuracy 86.370%\n",
      "Epoch 24, Batch 324, LR 0.000036 Loss 5.276744, Accuracy 86.374%\n",
      "Epoch 24, Batch 325, LR 0.000036 Loss 5.277730, Accuracy 86.370%\n",
      "Epoch 24, Batch 326, LR 0.000036 Loss 5.275554, Accuracy 86.374%\n",
      "Epoch 24, Batch 327, LR 0.000036 Loss 5.277969, Accuracy 86.363%\n",
      "Epoch 24, Batch 328, LR 0.000036 Loss 5.276125, Accuracy 86.381%\n",
      "Epoch 24, Batch 329, LR 0.000036 Loss 5.274754, Accuracy 86.382%\n",
      "Epoch 24, Batch 330, LR 0.000036 Loss 5.276047, Accuracy 86.373%\n",
      "Epoch 24, Batch 331, LR 0.000036 Loss 5.275204, Accuracy 86.367%\n",
      "Epoch 24, Batch 332, LR 0.000036 Loss 5.272850, Accuracy 86.382%\n",
      "Epoch 24, Batch 333, LR 0.000036 Loss 5.272005, Accuracy 86.393%\n",
      "Epoch 24, Batch 334, LR 0.000036 Loss 5.274476, Accuracy 86.368%\n",
      "Epoch 24, Batch 335, LR 0.000036 Loss 5.273960, Accuracy 86.367%\n",
      "Epoch 24, Batch 336, LR 0.000036 Loss 5.272824, Accuracy 86.361%\n",
      "Epoch 24, Batch 337, LR 0.000036 Loss 5.272564, Accuracy 86.357%\n",
      "Epoch 24, Batch 338, LR 0.000036 Loss 5.271335, Accuracy 86.356%\n",
      "Epoch 24, Batch 339, LR 0.000036 Loss 5.275290, Accuracy 86.329%\n",
      "Epoch 24, Batch 340, LR 0.000036 Loss 5.275827, Accuracy 86.333%\n",
      "Epoch 24, Batch 341, LR 0.000036 Loss 5.277149, Accuracy 86.332%\n",
      "Epoch 24, Batch 342, LR 0.000036 Loss 5.277564, Accuracy 86.333%\n",
      "Epoch 24, Batch 343, LR 0.000036 Loss 5.277690, Accuracy 86.336%\n",
      "Epoch 24, Batch 344, LR 0.000036 Loss 5.278039, Accuracy 86.330%\n",
      "Epoch 24, Batch 345, LR 0.000036 Loss 5.279529, Accuracy 86.327%\n",
      "Epoch 24, Batch 346, LR 0.000036 Loss 5.279288, Accuracy 86.328%\n",
      "Epoch 24, Batch 347, LR 0.000036 Loss 5.278558, Accuracy 86.332%\n",
      "Epoch 24, Batch 348, LR 0.000036 Loss 5.277437, Accuracy 86.330%\n",
      "Epoch 24, Batch 349, LR 0.000036 Loss 5.277918, Accuracy 86.336%\n",
      "Epoch 24, Batch 350, LR 0.000036 Loss 5.278544, Accuracy 86.335%\n",
      "Epoch 24, Batch 351, LR 0.000036 Loss 5.276876, Accuracy 86.340%\n",
      "Epoch 24, Batch 352, LR 0.000036 Loss 5.277594, Accuracy 86.344%\n",
      "Epoch 24, Batch 353, LR 0.000036 Loss 5.276843, Accuracy 86.349%\n",
      "Epoch 24, Batch 354, LR 0.000036 Loss 5.276076, Accuracy 86.350%\n",
      "Epoch 24, Batch 355, LR 0.000036 Loss 5.275038, Accuracy 86.353%\n",
      "Epoch 24, Batch 356, LR 0.000036 Loss 5.275327, Accuracy 86.357%\n",
      "Epoch 24, Batch 357, LR 0.000036 Loss 5.274008, Accuracy 86.366%\n",
      "Epoch 24, Batch 358, LR 0.000036 Loss 5.274348, Accuracy 86.363%\n",
      "Epoch 24, Batch 359, LR 0.000036 Loss 5.274275, Accuracy 86.360%\n",
      "Epoch 24, Batch 360, LR 0.000036 Loss 5.275749, Accuracy 86.350%\n",
      "Epoch 24, Batch 361, LR 0.000036 Loss 5.275298, Accuracy 86.355%\n",
      "Epoch 24, Batch 362, LR 0.000036 Loss 5.273639, Accuracy 86.369%\n",
      "Epoch 24, Batch 363, LR 0.000036 Loss 5.273895, Accuracy 86.361%\n",
      "Epoch 24, Batch 364, LR 0.000036 Loss 5.273065, Accuracy 86.365%\n",
      "Epoch 24, Batch 365, LR 0.000036 Loss 5.273437, Accuracy 86.361%\n",
      "Epoch 24, Batch 366, LR 0.000036 Loss 5.274367, Accuracy 86.358%\n",
      "Epoch 24, Batch 367, LR 0.000036 Loss 5.273479, Accuracy 86.365%\n",
      "Epoch 24, Batch 368, LR 0.000036 Loss 5.272082, Accuracy 86.368%\n",
      "Epoch 24, Batch 369, LR 0.000036 Loss 5.270225, Accuracy 86.380%\n",
      "Epoch 24, Batch 370, LR 0.000036 Loss 5.270143, Accuracy 86.379%\n",
      "Epoch 24, Batch 371, LR 0.000036 Loss 5.269745, Accuracy 86.373%\n",
      "Epoch 24, Batch 372, LR 0.000036 Loss 5.268076, Accuracy 86.383%\n",
      "Epoch 24, Batch 373, LR 0.000036 Loss 5.268724, Accuracy 86.390%\n",
      "Epoch 24, Batch 374, LR 0.000036 Loss 5.264587, Accuracy 86.405%\n",
      "Epoch 24, Batch 375, LR 0.000036 Loss 5.263770, Accuracy 86.400%\n",
      "Epoch 24, Batch 376, LR 0.000036 Loss 5.262536, Accuracy 86.399%\n",
      "Epoch 24, Batch 377, LR 0.000036 Loss 5.260769, Accuracy 86.406%\n",
      "Epoch 24, Batch 378, LR 0.000036 Loss 5.260947, Accuracy 86.405%\n",
      "Epoch 24, Batch 379, LR 0.000036 Loss 5.261538, Accuracy 86.403%\n",
      "Epoch 24, Batch 380, LR 0.000036 Loss 5.263118, Accuracy 86.394%\n",
      "Epoch 24, Batch 381, LR 0.000036 Loss 5.260578, Accuracy 86.403%\n",
      "Epoch 24, Batch 382, LR 0.000036 Loss 5.262150, Accuracy 86.406%\n",
      "Epoch 24, Batch 383, LR 0.000036 Loss 5.262300, Accuracy 86.421%\n",
      "Epoch 24, Batch 384, LR 0.000036 Loss 5.264460, Accuracy 86.399%\n",
      "Epoch 24, Batch 385, LR 0.000036 Loss 5.267276, Accuracy 86.392%\n",
      "Epoch 24, Batch 386, LR 0.000036 Loss 5.268318, Accuracy 86.383%\n",
      "Epoch 24, Batch 387, LR 0.000036 Loss 5.267593, Accuracy 86.378%\n",
      "Epoch 24, Batch 388, LR 0.000036 Loss 5.268018, Accuracy 86.376%\n",
      "Epoch 24, Batch 389, LR 0.000036 Loss 5.267368, Accuracy 86.375%\n",
      "Epoch 24, Batch 390, LR 0.000036 Loss 5.265448, Accuracy 86.378%\n",
      "Epoch 24, Batch 391, LR 0.000036 Loss 5.264619, Accuracy 86.377%\n",
      "Epoch 24, Batch 392, LR 0.000036 Loss 5.264515, Accuracy 86.386%\n",
      "Epoch 24, Batch 393, LR 0.000036 Loss 5.264827, Accuracy 86.383%\n",
      "Epoch 24, Batch 394, LR 0.000036 Loss 5.265957, Accuracy 86.382%\n",
      "Epoch 24, Batch 395, LR 0.000036 Loss 5.264907, Accuracy 86.379%\n",
      "Epoch 24, Batch 396, LR 0.000036 Loss 5.263941, Accuracy 86.383%\n",
      "Epoch 24, Batch 397, LR 0.000036 Loss 5.263417, Accuracy 86.396%\n",
      "Epoch 24, Batch 398, LR 0.000036 Loss 5.263124, Accuracy 86.389%\n",
      "Epoch 24, Batch 399, LR 0.000036 Loss 5.261602, Accuracy 86.402%\n",
      "Epoch 24, Batch 400, LR 0.000036 Loss 5.259089, Accuracy 86.416%\n",
      "Epoch 24, Batch 401, LR 0.000036 Loss 5.257604, Accuracy 86.421%\n",
      "Epoch 24, Batch 402, LR 0.000036 Loss 5.257976, Accuracy 86.419%\n",
      "Epoch 24, Batch 403, LR 0.000036 Loss 5.260658, Accuracy 86.399%\n",
      "Epoch 24, Batch 404, LR 0.000036 Loss 5.258137, Accuracy 86.417%\n",
      "Epoch 24, Batch 405, LR 0.000036 Loss 5.257789, Accuracy 86.414%\n",
      "Epoch 24, Batch 406, LR 0.000036 Loss 5.258146, Accuracy 86.411%\n",
      "Epoch 24, Batch 407, LR 0.000036 Loss 5.258536, Accuracy 86.410%\n",
      "Epoch 24, Batch 408, LR 0.000036 Loss 5.257690, Accuracy 86.414%\n",
      "Epoch 24, Batch 409, LR 0.000036 Loss 5.258851, Accuracy 86.409%\n",
      "Epoch 24, Batch 410, LR 0.000036 Loss 5.258190, Accuracy 86.421%\n",
      "Epoch 24, Batch 411, LR 0.000036 Loss 5.258656, Accuracy 86.424%\n",
      "Epoch 24, Batch 412, LR 0.000036 Loss 5.257096, Accuracy 86.436%\n",
      "Epoch 24, Batch 413, LR 0.000036 Loss 5.257531, Accuracy 86.429%\n",
      "Epoch 24, Batch 414, LR 0.000036 Loss 5.258297, Accuracy 86.426%\n",
      "Epoch 24, Batch 415, LR 0.000036 Loss 5.259785, Accuracy 86.408%\n",
      "Epoch 24, Batch 416, LR 0.000036 Loss 5.262298, Accuracy 86.401%\n",
      "Epoch 24, Batch 417, LR 0.000036 Loss 5.261327, Accuracy 86.408%\n",
      "Epoch 24, Batch 418, LR 0.000036 Loss 5.260353, Accuracy 86.410%\n",
      "Epoch 24, Batch 419, LR 0.000036 Loss 5.261888, Accuracy 86.396%\n",
      "Epoch 24, Batch 420, LR 0.000036 Loss 5.261185, Accuracy 86.404%\n",
      "Epoch 24, Batch 421, LR 0.000036 Loss 5.259969, Accuracy 86.416%\n",
      "Epoch 24, Batch 422, LR 0.000036 Loss 5.259376, Accuracy 86.415%\n",
      "Epoch 24, Batch 423, LR 0.000036 Loss 5.257916, Accuracy 86.420%\n",
      "Epoch 24, Batch 424, LR 0.000036 Loss 5.259200, Accuracy 86.415%\n",
      "Epoch 24, Batch 425, LR 0.000036 Loss 5.260002, Accuracy 86.425%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 426, LR 0.000036 Loss 5.258529, Accuracy 86.433%\n",
      "Epoch 24, Batch 427, LR 0.000036 Loss 5.257715, Accuracy 86.430%\n",
      "Epoch 24, Batch 428, LR 0.000036 Loss 5.257793, Accuracy 86.429%\n",
      "Epoch 24, Batch 429, LR 0.000036 Loss 5.257806, Accuracy 86.424%\n",
      "Epoch 24, Batch 430, LR 0.000036 Loss 5.257178, Accuracy 86.423%\n",
      "Epoch 24, Batch 431, LR 0.000036 Loss 5.256331, Accuracy 86.420%\n",
      "Epoch 24, Batch 432, LR 0.000036 Loss 5.255866, Accuracy 86.419%\n",
      "Epoch 24, Batch 433, LR 0.000036 Loss 5.254410, Accuracy 86.421%\n",
      "Epoch 24, Batch 434, LR 0.000036 Loss 5.254494, Accuracy 86.416%\n",
      "Epoch 24, Batch 435, LR 0.000036 Loss 5.254900, Accuracy 86.419%\n",
      "Epoch 24, Batch 436, LR 0.000036 Loss 5.254672, Accuracy 86.412%\n",
      "Epoch 24, Batch 437, LR 0.000036 Loss 5.253989, Accuracy 86.413%\n",
      "Epoch 24, Batch 438, LR 0.000036 Loss 5.252994, Accuracy 86.416%\n",
      "Epoch 24, Batch 439, LR 0.000036 Loss 5.253328, Accuracy 86.404%\n",
      "Epoch 24, Batch 440, LR 0.000036 Loss 5.253542, Accuracy 86.399%\n",
      "Epoch 24, Batch 441, LR 0.000036 Loss 5.253361, Accuracy 86.393%\n",
      "Epoch 24, Batch 442, LR 0.000036 Loss 5.253246, Accuracy 86.397%\n",
      "Epoch 24, Batch 443, LR 0.000036 Loss 5.251730, Accuracy 86.398%\n",
      "Epoch 24, Batch 444, LR 0.000036 Loss 5.251088, Accuracy 86.409%\n",
      "Epoch 24, Batch 445, LR 0.000036 Loss 5.251016, Accuracy 86.408%\n",
      "Epoch 24, Batch 446, LR 0.000036 Loss 5.251830, Accuracy 86.410%\n",
      "Epoch 24, Batch 447, LR 0.000036 Loss 5.251248, Accuracy 86.415%\n",
      "Epoch 24, Batch 448, LR 0.000036 Loss 5.251184, Accuracy 86.414%\n",
      "Epoch 24, Batch 449, LR 0.000036 Loss 5.250733, Accuracy 86.418%\n",
      "Epoch 24, Batch 450, LR 0.000036 Loss 5.249220, Accuracy 86.429%\n",
      "Epoch 24, Batch 451, LR 0.000036 Loss 5.248625, Accuracy 86.436%\n",
      "Epoch 24, Batch 452, LR 0.000036 Loss 5.247698, Accuracy 86.440%\n",
      "Epoch 24, Batch 453, LR 0.000036 Loss 5.245190, Accuracy 86.450%\n",
      "Epoch 24, Batch 454, LR 0.000036 Loss 5.246685, Accuracy 86.438%\n",
      "Epoch 24, Batch 455, LR 0.000036 Loss 5.244987, Accuracy 86.446%\n",
      "Epoch 24, Batch 456, LR 0.000036 Loss 5.244913, Accuracy 86.453%\n",
      "Epoch 24, Batch 457, LR 0.000036 Loss 5.244121, Accuracy 86.449%\n",
      "Epoch 24, Batch 458, LR 0.000036 Loss 5.242349, Accuracy 86.454%\n",
      "Epoch 24, Batch 459, LR 0.000036 Loss 5.242943, Accuracy 86.458%\n",
      "Epoch 24, Batch 460, LR 0.000036 Loss 5.242861, Accuracy 86.457%\n",
      "Epoch 24, Batch 461, LR 0.000036 Loss 5.242277, Accuracy 86.459%\n",
      "Epoch 24, Batch 462, LR 0.000036 Loss 5.242933, Accuracy 86.458%\n",
      "Epoch 24, Batch 463, LR 0.000036 Loss 5.242814, Accuracy 86.457%\n",
      "Epoch 24, Batch 464, LR 0.000036 Loss 5.241826, Accuracy 86.465%\n",
      "Epoch 24, Batch 465, LR 0.000036 Loss 5.240736, Accuracy 86.465%\n",
      "Epoch 24, Batch 466, LR 0.000036 Loss 5.241885, Accuracy 86.464%\n",
      "Epoch 24, Batch 467, LR 0.000036 Loss 5.241104, Accuracy 86.466%\n",
      "Epoch 24, Batch 468, LR 0.000036 Loss 5.241581, Accuracy 86.473%\n",
      "Epoch 24, Batch 469, LR 0.000036 Loss 5.241919, Accuracy 86.469%\n",
      "Epoch 24, Batch 470, LR 0.000036 Loss 5.241675, Accuracy 86.474%\n",
      "Epoch 24, Batch 471, LR 0.000036 Loss 5.244408, Accuracy 86.463%\n",
      "Epoch 24, Batch 472, LR 0.000036 Loss 5.244492, Accuracy 86.459%\n",
      "Epoch 24, Batch 473, LR 0.000036 Loss 5.245185, Accuracy 86.456%\n",
      "Epoch 24, Batch 474, LR 0.000036 Loss 5.245567, Accuracy 86.452%\n",
      "Epoch 24, Batch 475, LR 0.000036 Loss 5.246151, Accuracy 86.456%\n",
      "Epoch 24, Batch 476, LR 0.000036 Loss 5.246692, Accuracy 86.455%\n",
      "Epoch 24, Batch 477, LR 0.000036 Loss 5.247731, Accuracy 86.447%\n",
      "Epoch 24, Batch 478, LR 0.000036 Loss 5.247696, Accuracy 86.447%\n",
      "Epoch 24, Batch 479, LR 0.000036 Loss 5.247928, Accuracy 86.437%\n",
      "Epoch 24, Batch 480, LR 0.000036 Loss 5.247738, Accuracy 86.436%\n",
      "Epoch 24, Batch 481, LR 0.000036 Loss 5.248385, Accuracy 86.444%\n",
      "Epoch 24, Batch 482, LR 0.000036 Loss 5.248070, Accuracy 86.442%\n",
      "Epoch 24, Batch 483, LR 0.000036 Loss 5.249023, Accuracy 86.436%\n",
      "Epoch 24, Batch 484, LR 0.000036 Loss 5.250276, Accuracy 86.431%\n",
      "Epoch 24, Batch 485, LR 0.000036 Loss 5.248945, Accuracy 86.440%\n",
      "Epoch 24, Batch 486, LR 0.000036 Loss 5.249047, Accuracy 86.437%\n",
      "Epoch 24, Batch 487, LR 0.000036 Loss 5.247459, Accuracy 86.444%\n",
      "Epoch 24, Batch 488, LR 0.000036 Loss 5.247245, Accuracy 86.443%\n",
      "Epoch 24, Batch 489, LR 0.000036 Loss 5.248607, Accuracy 86.439%\n",
      "Epoch 24, Batch 490, LR 0.000036 Loss 5.247632, Accuracy 86.440%\n",
      "Epoch 24, Batch 491, LR 0.000036 Loss 5.247273, Accuracy 86.447%\n",
      "Epoch 24, Batch 492, LR 0.000036 Loss 5.247016, Accuracy 86.454%\n",
      "Epoch 24, Batch 493, LR 0.000036 Loss 5.246834, Accuracy 86.454%\n",
      "Epoch 24, Batch 494, LR 0.000036 Loss 5.246652, Accuracy 86.451%\n",
      "Epoch 24, Batch 495, LR 0.000036 Loss 5.247203, Accuracy 86.449%\n",
      "Epoch 24, Batch 496, LR 0.000036 Loss 5.247753, Accuracy 86.456%\n",
      "Epoch 24, Batch 497, LR 0.000036 Loss 5.247588, Accuracy 86.450%\n",
      "Epoch 24, Batch 498, LR 0.000036 Loss 5.247154, Accuracy 86.455%\n",
      "Epoch 24, Batch 499, LR 0.000036 Loss 5.247436, Accuracy 86.451%\n",
      "Epoch 24, Batch 500, LR 0.000036 Loss 5.248201, Accuracy 86.448%\n",
      "Epoch 24, Batch 501, LR 0.000036 Loss 5.247891, Accuracy 86.444%\n",
      "Epoch 24, Batch 502, LR 0.000036 Loss 5.246375, Accuracy 86.446%\n",
      "Epoch 24, Batch 503, LR 0.000036 Loss 5.246309, Accuracy 86.453%\n",
      "Epoch 24, Batch 504, LR 0.000036 Loss 5.246075, Accuracy 86.449%\n",
      "Epoch 24, Batch 505, LR 0.000036 Loss 5.247930, Accuracy 86.439%\n",
      "Epoch 24, Batch 506, LR 0.000036 Loss 5.247440, Accuracy 86.442%\n",
      "Epoch 24, Batch 507, LR 0.000036 Loss 5.247051, Accuracy 86.435%\n",
      "Epoch 24, Batch 508, LR 0.000036 Loss 5.246062, Accuracy 86.443%\n",
      "Epoch 24, Batch 509, LR 0.000036 Loss 5.247004, Accuracy 86.438%\n",
      "Epoch 24, Batch 510, LR 0.000036 Loss 5.247033, Accuracy 86.445%\n",
      "Epoch 24, Batch 511, LR 0.000036 Loss 5.246492, Accuracy 86.445%\n",
      "Epoch 24, Batch 512, LR 0.000036 Loss 5.247287, Accuracy 86.440%\n",
      "Epoch 24, Batch 513, LR 0.000036 Loss 5.246320, Accuracy 86.452%\n",
      "Epoch 24, Batch 514, LR 0.000036 Loss 5.246919, Accuracy 86.453%\n",
      "Epoch 24, Batch 515, LR 0.000036 Loss 5.246329, Accuracy 86.453%\n",
      "Epoch 24, Batch 516, LR 0.000036 Loss 5.247672, Accuracy 86.454%\n",
      "Epoch 24, Batch 517, LR 0.000036 Loss 5.246681, Accuracy 86.456%\n",
      "Epoch 24, Batch 518, LR 0.000036 Loss 5.245611, Accuracy 86.456%\n",
      "Epoch 24, Batch 519, LR 0.000036 Loss 5.245453, Accuracy 86.458%\n",
      "Epoch 24, Batch 520, LR 0.000036 Loss 5.245679, Accuracy 86.459%\n",
      "Epoch 24, Batch 521, LR 0.000036 Loss 5.245267, Accuracy 86.465%\n",
      "Epoch 24, Batch 522, LR 0.000036 Loss 5.246678, Accuracy 86.460%\n",
      "Epoch 24, Batch 523, LR 0.000036 Loss 5.245367, Accuracy 86.465%\n",
      "Epoch 24, Batch 524, LR 0.000036 Loss 5.246452, Accuracy 86.456%\n",
      "Epoch 24, Batch 525, LR 0.000036 Loss 5.246898, Accuracy 86.458%\n",
      "Epoch 24, Batch 526, LR 0.000036 Loss 5.246705, Accuracy 86.457%\n",
      "Epoch 24, Batch 527, LR 0.000036 Loss 5.246277, Accuracy 86.458%\n",
      "Epoch 24, Batch 528, LR 0.000036 Loss 5.245780, Accuracy 86.457%\n",
      "Epoch 24, Batch 529, LR 0.000036 Loss 5.243419, Accuracy 86.468%\n",
      "Epoch 24, Batch 530, LR 0.000036 Loss 5.242423, Accuracy 86.471%\n",
      "Epoch 24, Batch 531, LR 0.000036 Loss 5.242173, Accuracy 86.476%\n",
      "Epoch 24, Batch 532, LR 0.000036 Loss 5.243447, Accuracy 86.460%\n",
      "Epoch 24, Batch 533, LR 0.000036 Loss 5.245071, Accuracy 86.455%\n",
      "Epoch 24, Batch 534, LR 0.000036 Loss 5.245287, Accuracy 86.454%\n",
      "Epoch 24, Batch 535, LR 0.000036 Loss 5.245597, Accuracy 86.454%\n",
      "Epoch 24, Batch 536, LR 0.000036 Loss 5.245264, Accuracy 86.456%\n",
      "Epoch 24, Batch 537, LR 0.000036 Loss 5.245369, Accuracy 86.463%\n",
      "Epoch 24, Batch 538, LR 0.000036 Loss 5.245851, Accuracy 86.459%\n",
      "Epoch 24, Batch 539, LR 0.000036 Loss 5.246485, Accuracy 86.459%\n",
      "Epoch 24, Batch 540, LR 0.000036 Loss 5.247063, Accuracy 86.457%\n",
      "Epoch 24, Batch 541, LR 0.000036 Loss 5.246903, Accuracy 86.457%\n",
      "Epoch 24, Batch 542, LR 0.000036 Loss 5.246137, Accuracy 86.458%\n",
      "Epoch 24, Batch 543, LR 0.000036 Loss 5.246433, Accuracy 86.454%\n",
      "Epoch 24, Batch 544, LR 0.000036 Loss 5.246008, Accuracy 86.455%\n",
      "Epoch 24, Batch 545, LR 0.000036 Loss 5.244917, Accuracy 86.454%\n",
      "Epoch 24, Batch 546, LR 0.000036 Loss 5.244415, Accuracy 86.455%\n",
      "Epoch 24, Batch 547, LR 0.000036 Loss 5.244862, Accuracy 86.453%\n",
      "Epoch 24, Batch 548, LR 0.000036 Loss 5.245422, Accuracy 86.458%\n",
      "Epoch 24, Batch 549, LR 0.000036 Loss 5.245402, Accuracy 86.454%\n",
      "Epoch 24, Batch 550, LR 0.000036 Loss 5.246708, Accuracy 86.446%\n",
      "Epoch 24, Batch 551, LR 0.000036 Loss 5.246404, Accuracy 86.445%\n",
      "Epoch 24, Batch 552, LR 0.000036 Loss 5.246240, Accuracy 86.447%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 553, LR 0.000036 Loss 5.247081, Accuracy 86.450%\n",
      "Epoch 24, Batch 554, LR 0.000036 Loss 5.247916, Accuracy 86.451%\n",
      "Epoch 24, Batch 555, LR 0.000036 Loss 5.247617, Accuracy 86.457%\n",
      "Epoch 24, Batch 556, LR 0.000036 Loss 5.247701, Accuracy 86.464%\n",
      "Epoch 24, Batch 557, LR 0.000036 Loss 5.246682, Accuracy 86.469%\n",
      "Epoch 24, Batch 558, LR 0.000036 Loss 5.245050, Accuracy 86.474%\n",
      "Epoch 24, Batch 559, LR 0.000036 Loss 5.244481, Accuracy 86.477%\n",
      "Epoch 24, Batch 560, LR 0.000036 Loss 5.245357, Accuracy 86.482%\n",
      "Epoch 24, Batch 561, LR 0.000036 Loss 5.245499, Accuracy 86.474%\n",
      "Epoch 24, Batch 562, LR 0.000036 Loss 5.245219, Accuracy 86.473%\n",
      "Epoch 24, Batch 563, LR 0.000036 Loss 5.246182, Accuracy 86.468%\n",
      "Epoch 24, Batch 564, LR 0.000036 Loss 5.245864, Accuracy 86.462%\n",
      "Epoch 24, Batch 565, LR 0.000036 Loss 5.245779, Accuracy 86.464%\n",
      "Epoch 24, Batch 566, LR 0.000036 Loss 5.245707, Accuracy 86.462%\n",
      "Epoch 24, Batch 567, LR 0.000036 Loss 5.245794, Accuracy 86.465%\n",
      "Epoch 24, Batch 568, LR 0.000036 Loss 5.245720, Accuracy 86.467%\n",
      "Epoch 24, Batch 569, LR 0.000036 Loss 5.245376, Accuracy 86.472%\n",
      "Epoch 24, Batch 570, LR 0.000036 Loss 5.245729, Accuracy 86.469%\n",
      "Epoch 24, Batch 571, LR 0.000036 Loss 5.244976, Accuracy 86.474%\n",
      "Epoch 24, Batch 572, LR 0.000036 Loss 5.245226, Accuracy 86.477%\n",
      "Epoch 24, Batch 573, LR 0.000036 Loss 5.244397, Accuracy 86.483%\n",
      "Epoch 24, Batch 574, LR 0.000036 Loss 5.245629, Accuracy 86.476%\n",
      "Epoch 24, Batch 575, LR 0.000036 Loss 5.244314, Accuracy 86.480%\n",
      "Epoch 24, Batch 576, LR 0.000036 Loss 5.244839, Accuracy 86.469%\n",
      "Epoch 24, Batch 577, LR 0.000036 Loss 5.244570, Accuracy 86.470%\n",
      "Epoch 24, Batch 578, LR 0.000036 Loss 5.243689, Accuracy 86.477%\n",
      "Epoch 24, Batch 579, LR 0.000036 Loss 5.242924, Accuracy 86.475%\n",
      "Epoch 24, Batch 580, LR 0.000036 Loss 5.242598, Accuracy 86.471%\n",
      "Epoch 24, Batch 581, LR 0.000036 Loss 5.242177, Accuracy 86.475%\n",
      "Epoch 24, Batch 582, LR 0.000036 Loss 5.241677, Accuracy 86.476%\n",
      "Epoch 24, Batch 583, LR 0.000036 Loss 5.241424, Accuracy 86.472%\n",
      "Epoch 24, Batch 584, LR 0.000036 Loss 5.241097, Accuracy 86.475%\n",
      "Epoch 24, Batch 585, LR 0.000036 Loss 5.242140, Accuracy 86.470%\n",
      "Epoch 24, Batch 586, LR 0.000035 Loss 5.241383, Accuracy 86.475%\n",
      "Epoch 24, Batch 587, LR 0.000035 Loss 5.239852, Accuracy 86.479%\n",
      "Epoch 24, Batch 588, LR 0.000035 Loss 5.240854, Accuracy 86.477%\n",
      "Epoch 24, Batch 589, LR 0.000035 Loss 5.241648, Accuracy 86.469%\n",
      "Epoch 24, Batch 590, LR 0.000035 Loss 5.241238, Accuracy 86.475%\n",
      "Epoch 24, Batch 591, LR 0.000035 Loss 5.239943, Accuracy 86.489%\n",
      "Epoch 24, Batch 592, LR 0.000035 Loss 5.239391, Accuracy 86.489%\n",
      "Epoch 24, Batch 593, LR 0.000035 Loss 5.239096, Accuracy 86.493%\n",
      "Epoch 24, Batch 594, LR 0.000035 Loss 5.239399, Accuracy 86.493%\n",
      "Epoch 24, Batch 595, LR 0.000035 Loss 5.240986, Accuracy 86.486%\n",
      "Epoch 24, Batch 596, LR 0.000035 Loss 5.241137, Accuracy 86.479%\n",
      "Epoch 24, Batch 597, LR 0.000035 Loss 5.240757, Accuracy 86.478%\n",
      "Epoch 24, Batch 598, LR 0.000035 Loss 5.241252, Accuracy 86.476%\n",
      "Epoch 24, Batch 599, LR 0.000035 Loss 5.242360, Accuracy 86.474%\n",
      "Epoch 24, Batch 600, LR 0.000035 Loss 5.242504, Accuracy 86.477%\n",
      "Epoch 24, Batch 601, LR 0.000035 Loss 5.242212, Accuracy 86.482%\n",
      "Epoch 24, Batch 602, LR 0.000035 Loss 5.240389, Accuracy 86.485%\n",
      "Epoch 24, Batch 603, LR 0.000035 Loss 5.239119, Accuracy 86.491%\n",
      "Epoch 24, Batch 604, LR 0.000035 Loss 5.237516, Accuracy 86.491%\n",
      "Epoch 24, Batch 605, LR 0.000035 Loss 5.237253, Accuracy 86.489%\n",
      "Epoch 24, Batch 606, LR 0.000035 Loss 5.236543, Accuracy 86.489%\n",
      "Epoch 24, Batch 607, LR 0.000035 Loss 5.237499, Accuracy 86.494%\n",
      "Epoch 24, Batch 608, LR 0.000035 Loss 5.236716, Accuracy 86.498%\n",
      "Epoch 24, Batch 609, LR 0.000035 Loss 5.236964, Accuracy 86.497%\n",
      "Epoch 24, Batch 610, LR 0.000035 Loss 5.237092, Accuracy 86.501%\n",
      "Epoch 24, Batch 611, LR 0.000035 Loss 5.237003, Accuracy 86.500%\n",
      "Epoch 24, Batch 612, LR 0.000035 Loss 5.237509, Accuracy 86.492%\n",
      "Epoch 24, Batch 613, LR 0.000035 Loss 5.236470, Accuracy 86.498%\n",
      "Epoch 24, Batch 614, LR 0.000035 Loss 5.235448, Accuracy 86.506%\n",
      "Epoch 24, Batch 615, LR 0.000035 Loss 5.235226, Accuracy 86.505%\n",
      "Epoch 24, Batch 616, LR 0.000035 Loss 5.235318, Accuracy 86.507%\n",
      "Epoch 24, Batch 617, LR 0.000035 Loss 5.234599, Accuracy 86.511%\n",
      "Epoch 24, Batch 618, LR 0.000035 Loss 5.234916, Accuracy 86.505%\n",
      "Epoch 24, Batch 619, LR 0.000035 Loss 5.235631, Accuracy 86.500%\n",
      "Epoch 24, Batch 620, LR 0.000035 Loss 5.237303, Accuracy 86.496%\n",
      "Epoch 24, Batch 621, LR 0.000035 Loss 5.236746, Accuracy 86.500%\n",
      "Epoch 24, Batch 622, LR 0.000035 Loss 5.236977, Accuracy 86.503%\n",
      "Epoch 24, Batch 623, LR 0.000035 Loss 5.236442, Accuracy 86.509%\n",
      "Epoch 24, Batch 624, LR 0.000035 Loss 5.236445, Accuracy 86.505%\n",
      "Epoch 24, Batch 625, LR 0.000035 Loss 5.236485, Accuracy 86.505%\n",
      "Epoch 24, Batch 626, LR 0.000035 Loss 5.236334, Accuracy 86.503%\n",
      "Epoch 24, Batch 627, LR 0.000035 Loss 5.236523, Accuracy 86.502%\n",
      "Epoch 24, Batch 628, LR 0.000035 Loss 5.235613, Accuracy 86.505%\n",
      "Epoch 24, Batch 629, LR 0.000035 Loss 5.235642, Accuracy 86.504%\n",
      "Epoch 24, Batch 630, LR 0.000035 Loss 5.235025, Accuracy 86.505%\n",
      "Epoch 24, Batch 631, LR 0.000035 Loss 5.234921, Accuracy 86.507%\n",
      "Epoch 24, Batch 632, LR 0.000035 Loss 5.235335, Accuracy 86.510%\n",
      "Epoch 24, Batch 633, LR 0.000035 Loss 5.235233, Accuracy 86.513%\n",
      "Epoch 24, Batch 634, LR 0.000035 Loss 5.234566, Accuracy 86.509%\n",
      "Epoch 24, Batch 635, LR 0.000035 Loss 5.235568, Accuracy 86.510%\n",
      "Epoch 24, Batch 636, LR 0.000035 Loss 5.235527, Accuracy 86.506%\n",
      "Epoch 24, Batch 637, LR 0.000035 Loss 5.235374, Accuracy 86.511%\n",
      "Epoch 24, Batch 638, LR 0.000035 Loss 5.235620, Accuracy 86.511%\n",
      "Epoch 24, Batch 639, LR 0.000035 Loss 5.235372, Accuracy 86.517%\n",
      "Epoch 24, Batch 640, LR 0.000035 Loss 5.234350, Accuracy 86.522%\n",
      "Epoch 24, Batch 641, LR 0.000035 Loss 5.233856, Accuracy 86.516%\n",
      "Epoch 24, Batch 642, LR 0.000035 Loss 5.234571, Accuracy 86.506%\n",
      "Epoch 24, Batch 643, LR 0.000035 Loss 5.235150, Accuracy 86.502%\n",
      "Epoch 24, Batch 644, LR 0.000035 Loss 5.235468, Accuracy 86.505%\n",
      "Epoch 24, Batch 645, LR 0.000035 Loss 5.234574, Accuracy 86.503%\n",
      "Epoch 24, Batch 646, LR 0.000035 Loss 5.235528, Accuracy 86.488%\n",
      "Epoch 24, Batch 647, LR 0.000035 Loss 5.235913, Accuracy 86.481%\n",
      "Epoch 24, Batch 648, LR 0.000035 Loss 5.236578, Accuracy 86.480%\n",
      "Epoch 24, Batch 649, LR 0.000035 Loss 5.237781, Accuracy 86.476%\n",
      "Epoch 24, Batch 650, LR 0.000035 Loss 5.237471, Accuracy 86.475%\n",
      "Epoch 24, Batch 651, LR 0.000035 Loss 5.238309, Accuracy 86.472%\n",
      "Epoch 24, Batch 652, LR 0.000035 Loss 5.239309, Accuracy 86.466%\n",
      "Epoch 24, Batch 653, LR 0.000035 Loss 5.238624, Accuracy 86.472%\n",
      "Epoch 24, Batch 654, LR 0.000035 Loss 5.239125, Accuracy 86.470%\n",
      "Epoch 24, Batch 655, LR 0.000035 Loss 5.238628, Accuracy 86.472%\n",
      "Epoch 24, Batch 656, LR 0.000035 Loss 5.238056, Accuracy 86.473%\n",
      "Epoch 24, Batch 657, LR 0.000035 Loss 5.237872, Accuracy 86.469%\n",
      "Epoch 24, Batch 658, LR 0.000035 Loss 5.236631, Accuracy 86.473%\n",
      "Epoch 24, Batch 659, LR 0.000035 Loss 5.236249, Accuracy 86.472%\n",
      "Epoch 24, Batch 660, LR 0.000035 Loss 5.235724, Accuracy 86.475%\n",
      "Epoch 24, Batch 661, LR 0.000035 Loss 5.234716, Accuracy 86.476%\n",
      "Epoch 24, Batch 662, LR 0.000035 Loss 5.235313, Accuracy 86.472%\n",
      "Epoch 24, Batch 663, LR 0.000035 Loss 5.235399, Accuracy 86.474%\n",
      "Epoch 24, Batch 664, LR 0.000035 Loss 5.235686, Accuracy 86.476%\n",
      "Epoch 24, Batch 665, LR 0.000035 Loss 5.235352, Accuracy 86.474%\n",
      "Epoch 24, Batch 666, LR 0.000035 Loss 5.235082, Accuracy 86.475%\n",
      "Epoch 24, Batch 667, LR 0.000035 Loss 5.235715, Accuracy 86.466%\n",
      "Epoch 24, Batch 668, LR 0.000035 Loss 5.236012, Accuracy 86.467%\n",
      "Epoch 24, Batch 669, LR 0.000035 Loss 5.235405, Accuracy 86.470%\n",
      "Epoch 24, Batch 670, LR 0.000035 Loss 5.234646, Accuracy 86.477%\n",
      "Epoch 24, Batch 671, LR 0.000035 Loss 5.235107, Accuracy 86.478%\n",
      "Epoch 24, Batch 672, LR 0.000035 Loss 5.233802, Accuracy 86.482%\n",
      "Epoch 24, Batch 673, LR 0.000035 Loss 5.233203, Accuracy 86.483%\n",
      "Epoch 24, Batch 674, LR 0.000035 Loss 5.232525, Accuracy 86.485%\n",
      "Epoch 24, Batch 675, LR 0.000035 Loss 5.232348, Accuracy 86.491%\n",
      "Epoch 24, Batch 676, LR 0.000035 Loss 5.231661, Accuracy 86.495%\n",
      "Epoch 24, Batch 677, LR 0.000035 Loss 5.231709, Accuracy 86.494%\n",
      "Epoch 24, Batch 678, LR 0.000035 Loss 5.231756, Accuracy 86.496%\n",
      "Epoch 24, Batch 679, LR 0.000035 Loss 5.231464, Accuracy 86.496%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 680, LR 0.000035 Loss 5.231537, Accuracy 86.495%\n",
      "Epoch 24, Batch 681, LR 0.000035 Loss 5.231878, Accuracy 86.492%\n",
      "Epoch 24, Batch 682, LR 0.000035 Loss 5.231303, Accuracy 86.494%\n",
      "Epoch 24, Batch 683, LR 0.000035 Loss 5.230686, Accuracy 86.496%\n",
      "Epoch 24, Batch 684, LR 0.000035 Loss 5.230533, Accuracy 86.491%\n",
      "Epoch 24, Batch 685, LR 0.000035 Loss 5.231737, Accuracy 86.491%\n",
      "Epoch 24, Batch 686, LR 0.000035 Loss 5.231222, Accuracy 86.497%\n",
      "Epoch 24, Batch 687, LR 0.000035 Loss 5.231961, Accuracy 86.494%\n",
      "Epoch 24, Batch 688, LR 0.000035 Loss 5.232356, Accuracy 86.497%\n",
      "Epoch 24, Batch 689, LR 0.000035 Loss 5.231699, Accuracy 86.500%\n",
      "Epoch 24, Batch 690, LR 0.000035 Loss 5.231637, Accuracy 86.496%\n",
      "Epoch 24, Batch 691, LR 0.000035 Loss 5.232075, Accuracy 86.487%\n",
      "Epoch 24, Batch 692, LR 0.000035 Loss 5.231452, Accuracy 86.492%\n",
      "Epoch 24, Batch 693, LR 0.000035 Loss 5.231738, Accuracy 86.492%\n",
      "Epoch 24, Batch 694, LR 0.000035 Loss 5.231672, Accuracy 86.490%\n",
      "Epoch 24, Batch 695, LR 0.000035 Loss 5.231575, Accuracy 86.492%\n",
      "Epoch 24, Batch 696, LR 0.000035 Loss 5.231929, Accuracy 86.494%\n",
      "Epoch 24, Batch 697, LR 0.000035 Loss 5.232666, Accuracy 86.489%\n",
      "Epoch 24, Batch 698, LR 0.000035 Loss 5.233627, Accuracy 86.481%\n",
      "Epoch 24, Batch 699, LR 0.000035 Loss 5.234128, Accuracy 86.483%\n",
      "Epoch 24, Batch 700, LR 0.000035 Loss 5.233839, Accuracy 86.482%\n",
      "Epoch 24, Batch 701, LR 0.000035 Loss 5.233667, Accuracy 86.482%\n",
      "Epoch 24, Batch 702, LR 0.000035 Loss 5.232625, Accuracy 86.485%\n",
      "Epoch 24, Batch 703, LR 0.000035 Loss 5.232253, Accuracy 86.488%\n",
      "Epoch 24, Batch 704, LR 0.000035 Loss 5.232808, Accuracy 86.485%\n",
      "Epoch 24, Batch 705, LR 0.000035 Loss 5.232707, Accuracy 86.484%\n",
      "Epoch 24, Batch 706, LR 0.000035 Loss 5.233845, Accuracy 86.478%\n",
      "Epoch 24, Batch 707, LR 0.000035 Loss 5.233512, Accuracy 86.480%\n",
      "Epoch 24, Batch 708, LR 0.000035 Loss 5.233879, Accuracy 86.478%\n",
      "Epoch 24, Batch 709, LR 0.000035 Loss 5.234359, Accuracy 86.481%\n",
      "Epoch 24, Batch 710, LR 0.000035 Loss 5.232969, Accuracy 86.489%\n",
      "Epoch 24, Batch 711, LR 0.000035 Loss 5.232626, Accuracy 86.485%\n",
      "Epoch 24, Batch 712, LR 0.000035 Loss 5.231703, Accuracy 86.491%\n",
      "Epoch 24, Batch 713, LR 0.000035 Loss 5.231335, Accuracy 86.493%\n",
      "Epoch 24, Batch 714, LR 0.000035 Loss 5.230883, Accuracy 86.491%\n",
      "Epoch 24, Batch 715, LR 0.000035 Loss 5.229491, Accuracy 86.498%\n",
      "Epoch 24, Batch 716, LR 0.000035 Loss 5.228563, Accuracy 86.502%\n",
      "Epoch 24, Batch 717, LR 0.000035 Loss 5.228614, Accuracy 86.501%\n",
      "Epoch 24, Batch 718, LR 0.000035 Loss 5.227438, Accuracy 86.502%\n",
      "Epoch 24, Batch 719, LR 0.000035 Loss 5.227385, Accuracy 86.501%\n",
      "Epoch 24, Batch 720, LR 0.000035 Loss 5.227336, Accuracy 86.505%\n",
      "Epoch 24, Batch 721, LR 0.000035 Loss 5.226644, Accuracy 86.505%\n",
      "Epoch 24, Batch 722, LR 0.000035 Loss 5.226703, Accuracy 86.510%\n",
      "Epoch 24, Batch 723, LR 0.000035 Loss 5.227647, Accuracy 86.504%\n",
      "Epoch 24, Batch 724, LR 0.000035 Loss 5.227542, Accuracy 86.499%\n",
      "Epoch 24, Batch 725, LR 0.000035 Loss 5.228110, Accuracy 86.497%\n",
      "Epoch 24, Batch 726, LR 0.000035 Loss 5.228840, Accuracy 86.496%\n",
      "Epoch 24, Batch 727, LR 0.000035 Loss 5.229275, Accuracy 86.497%\n",
      "Epoch 24, Batch 728, LR 0.000035 Loss 5.228587, Accuracy 86.501%\n",
      "Epoch 24, Batch 729, LR 0.000035 Loss 5.228936, Accuracy 86.501%\n",
      "Epoch 24, Batch 730, LR 0.000035 Loss 5.228481, Accuracy 86.504%\n",
      "Epoch 24, Batch 731, LR 0.000035 Loss 5.228355, Accuracy 86.506%\n",
      "Epoch 24, Batch 732, LR 0.000035 Loss 5.230453, Accuracy 86.492%\n",
      "Epoch 24, Batch 733, LR 0.000035 Loss 5.229645, Accuracy 86.494%\n",
      "Epoch 24, Batch 734, LR 0.000035 Loss 5.229632, Accuracy 86.492%\n",
      "Epoch 24, Batch 735, LR 0.000035 Loss 5.228751, Accuracy 86.497%\n",
      "Epoch 24, Batch 736, LR 0.000035 Loss 5.228598, Accuracy 86.495%\n",
      "Epoch 24, Batch 737, LR 0.000035 Loss 5.228343, Accuracy 86.497%\n",
      "Epoch 24, Batch 738, LR 0.000035 Loss 5.227287, Accuracy 86.507%\n",
      "Epoch 24, Batch 739, LR 0.000035 Loss 5.226910, Accuracy 86.509%\n",
      "Epoch 24, Batch 740, LR 0.000035 Loss 5.226698, Accuracy 86.509%\n",
      "Epoch 24, Batch 741, LR 0.000035 Loss 5.226066, Accuracy 86.509%\n",
      "Epoch 24, Batch 742, LR 0.000035 Loss 5.225969, Accuracy 86.510%\n",
      "Epoch 24, Batch 743, LR 0.000035 Loss 5.226005, Accuracy 86.513%\n",
      "Epoch 24, Batch 744, LR 0.000035 Loss 5.226218, Accuracy 86.506%\n",
      "Epoch 24, Batch 745, LR 0.000035 Loss 5.225056, Accuracy 86.508%\n",
      "Epoch 24, Batch 746, LR 0.000035 Loss 5.224395, Accuracy 86.506%\n",
      "Epoch 24, Batch 747, LR 0.000035 Loss 5.224603, Accuracy 86.505%\n",
      "Epoch 24, Batch 748, LR 0.000035 Loss 5.224005, Accuracy 86.509%\n",
      "Epoch 24, Batch 749, LR 0.000035 Loss 5.224026, Accuracy 86.508%\n",
      "Epoch 24, Batch 750, LR 0.000035 Loss 5.223330, Accuracy 86.509%\n",
      "Epoch 24, Batch 751, LR 0.000035 Loss 5.222506, Accuracy 86.515%\n",
      "Epoch 24, Batch 752, LR 0.000035 Loss 5.222800, Accuracy 86.514%\n",
      "Epoch 24, Batch 753, LR 0.000035 Loss 5.222727, Accuracy 86.516%\n",
      "Epoch 24, Batch 754, LR 0.000035 Loss 5.221547, Accuracy 86.518%\n",
      "Epoch 24, Batch 755, LR 0.000035 Loss 5.220697, Accuracy 86.523%\n",
      "Epoch 24, Batch 756, LR 0.000035 Loss 5.220811, Accuracy 86.523%\n",
      "Epoch 24, Batch 757, LR 0.000035 Loss 5.222801, Accuracy 86.509%\n",
      "Epoch 24, Batch 758, LR 0.000035 Loss 5.223635, Accuracy 86.505%\n",
      "Epoch 24, Batch 759, LR 0.000035 Loss 5.223902, Accuracy 86.508%\n",
      "Epoch 24, Batch 760, LR 0.000035 Loss 5.224383, Accuracy 86.504%\n",
      "Epoch 24, Batch 761, LR 0.000035 Loss 5.225488, Accuracy 86.500%\n",
      "Epoch 24, Batch 762, LR 0.000035 Loss 5.225310, Accuracy 86.501%\n",
      "Epoch 24, Batch 763, LR 0.000035 Loss 5.224909, Accuracy 86.503%\n",
      "Epoch 24, Batch 764, LR 0.000035 Loss 5.226128, Accuracy 86.496%\n",
      "Epoch 24, Batch 765, LR 0.000035 Loss 5.225727, Accuracy 86.496%\n",
      "Epoch 24, Batch 766, LR 0.000035 Loss 5.225301, Accuracy 86.495%\n",
      "Epoch 24, Batch 767, LR 0.000035 Loss 5.225163, Accuracy 86.494%\n",
      "Epoch 24, Batch 768, LR 0.000035 Loss 5.224400, Accuracy 86.497%\n",
      "Epoch 24, Batch 769, LR 0.000035 Loss 5.223403, Accuracy 86.503%\n",
      "Epoch 24, Batch 770, LR 0.000035 Loss 5.223896, Accuracy 86.501%\n",
      "Epoch 24, Batch 771, LR 0.000035 Loss 5.224105, Accuracy 86.501%\n",
      "Epoch 24, Batch 772, LR 0.000035 Loss 5.224324, Accuracy 86.496%\n",
      "Epoch 24, Batch 773, LR 0.000035 Loss 5.224211, Accuracy 86.501%\n",
      "Epoch 24, Batch 774, LR 0.000035 Loss 5.224141, Accuracy 86.499%\n",
      "Epoch 24, Batch 775, LR 0.000035 Loss 5.224403, Accuracy 86.497%\n",
      "Epoch 24, Batch 776, LR 0.000035 Loss 5.225113, Accuracy 86.492%\n",
      "Epoch 24, Batch 777, LR 0.000035 Loss 5.224610, Accuracy 86.499%\n",
      "Epoch 24, Batch 778, LR 0.000035 Loss 5.224298, Accuracy 86.500%\n",
      "Epoch 24, Batch 779, LR 0.000035 Loss 5.225262, Accuracy 86.493%\n",
      "Epoch 24, Batch 780, LR 0.000035 Loss 5.225289, Accuracy 86.489%\n",
      "Epoch 24, Batch 781, LR 0.000035 Loss 5.225591, Accuracy 86.481%\n",
      "Epoch 24, Batch 782, LR 0.000035 Loss 5.226258, Accuracy 86.474%\n",
      "Epoch 24, Batch 783, LR 0.000035 Loss 5.226364, Accuracy 86.474%\n",
      "Epoch 24, Batch 784, LR 0.000035 Loss 5.225425, Accuracy 86.479%\n",
      "Epoch 24, Batch 785, LR 0.000035 Loss 5.225838, Accuracy 86.477%\n",
      "Epoch 24, Batch 786, LR 0.000035 Loss 5.226124, Accuracy 86.477%\n",
      "Epoch 24, Batch 787, LR 0.000035 Loss 5.226498, Accuracy 86.472%\n",
      "Epoch 24, Batch 788, LR 0.000035 Loss 5.226162, Accuracy 86.476%\n",
      "Epoch 24, Batch 789, LR 0.000035 Loss 5.226297, Accuracy 86.471%\n",
      "Epoch 24, Batch 790, LR 0.000035 Loss 5.225566, Accuracy 86.473%\n",
      "Epoch 24, Batch 791, LR 0.000035 Loss 5.225580, Accuracy 86.473%\n",
      "Epoch 24, Batch 792, LR 0.000035 Loss 5.224697, Accuracy 86.482%\n",
      "Epoch 24, Batch 793, LR 0.000035 Loss 5.224217, Accuracy 86.481%\n",
      "Epoch 24, Batch 794, LR 0.000035 Loss 5.222894, Accuracy 86.482%\n",
      "Epoch 24, Batch 795, LR 0.000035 Loss 5.222100, Accuracy 86.481%\n",
      "Epoch 24, Batch 796, LR 0.000035 Loss 5.222599, Accuracy 86.480%\n",
      "Epoch 24, Batch 797, LR 0.000035 Loss 5.222217, Accuracy 86.481%\n",
      "Epoch 24, Batch 798, LR 0.000035 Loss 5.222392, Accuracy 86.480%\n",
      "Epoch 24, Batch 799, LR 0.000035 Loss 5.222046, Accuracy 86.485%\n",
      "Epoch 24, Batch 800, LR 0.000035 Loss 5.221789, Accuracy 86.482%\n",
      "Epoch 24, Batch 801, LR 0.000035 Loss 5.221468, Accuracy 86.484%\n",
      "Epoch 24, Batch 802, LR 0.000035 Loss 5.221309, Accuracy 86.487%\n",
      "Epoch 24, Batch 803, LR 0.000035 Loss 5.221130, Accuracy 86.488%\n",
      "Epoch 24, Batch 804, LR 0.000035 Loss 5.221212, Accuracy 86.488%\n",
      "Epoch 24, Batch 805, LR 0.000035 Loss 5.221045, Accuracy 86.487%\n",
      "Epoch 24, Batch 806, LR 0.000035 Loss 5.220722, Accuracy 86.487%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 807, LR 0.000035 Loss 5.220643, Accuracy 86.485%\n",
      "Epoch 24, Batch 808, LR 0.000035 Loss 5.220848, Accuracy 86.485%\n",
      "Epoch 24, Batch 809, LR 0.000035 Loss 5.221462, Accuracy 86.483%\n",
      "Epoch 24, Batch 810, LR 0.000035 Loss 5.221954, Accuracy 86.476%\n",
      "Epoch 24, Batch 811, LR 0.000035 Loss 5.221682, Accuracy 86.477%\n",
      "Epoch 24, Batch 812, LR 0.000035 Loss 5.221212, Accuracy 86.480%\n",
      "Epoch 24, Batch 813, LR 0.000035 Loss 5.221203, Accuracy 86.482%\n",
      "Epoch 24, Batch 814, LR 0.000035 Loss 5.221456, Accuracy 86.475%\n",
      "Epoch 24, Batch 815, LR 0.000035 Loss 5.220804, Accuracy 86.480%\n",
      "Epoch 24, Batch 816, LR 0.000035 Loss 5.220383, Accuracy 86.477%\n",
      "Epoch 24, Batch 817, LR 0.000035 Loss 5.220942, Accuracy 86.475%\n",
      "Epoch 24, Batch 818, LR 0.000035 Loss 5.222039, Accuracy 86.466%\n",
      "Epoch 24, Batch 819, LR 0.000035 Loss 5.220816, Accuracy 86.475%\n",
      "Epoch 24, Batch 820, LR 0.000035 Loss 5.221930, Accuracy 86.467%\n",
      "Epoch 24, Batch 821, LR 0.000035 Loss 5.221865, Accuracy 86.469%\n",
      "Epoch 24, Batch 822, LR 0.000035 Loss 5.222749, Accuracy 86.467%\n",
      "Epoch 24, Batch 823, LR 0.000035 Loss 5.221899, Accuracy 86.470%\n",
      "Epoch 24, Batch 824, LR 0.000035 Loss 5.221815, Accuracy 86.471%\n",
      "Epoch 24, Batch 825, LR 0.000035 Loss 5.221448, Accuracy 86.471%\n",
      "Epoch 24, Batch 826, LR 0.000035 Loss 5.221072, Accuracy 86.470%\n",
      "Epoch 24, Batch 827, LR 0.000035 Loss 5.222220, Accuracy 86.471%\n",
      "Epoch 24, Batch 828, LR 0.000035 Loss 5.221438, Accuracy 86.472%\n",
      "Epoch 24, Batch 829, LR 0.000035 Loss 5.222111, Accuracy 86.467%\n",
      "Epoch 24, Batch 830, LR 0.000035 Loss 5.222868, Accuracy 86.464%\n",
      "Epoch 24, Batch 831, LR 0.000035 Loss 5.223266, Accuracy 86.460%\n",
      "Epoch 24, Batch 832, LR 0.000035 Loss 5.223184, Accuracy 86.459%\n",
      "Epoch 24, Batch 833, LR 0.000035 Loss 5.222886, Accuracy 86.464%\n",
      "Epoch 24, Batch 834, LR 0.000035 Loss 5.223251, Accuracy 86.461%\n",
      "Epoch 24, Batch 835, LR 0.000035 Loss 5.224028, Accuracy 86.458%\n",
      "Epoch 24, Batch 836, LR 0.000035 Loss 5.223896, Accuracy 86.459%\n",
      "Epoch 24, Batch 837, LR 0.000035 Loss 5.224385, Accuracy 86.456%\n",
      "Epoch 24, Batch 838, LR 0.000035 Loss 5.224554, Accuracy 86.458%\n",
      "Epoch 24, Batch 839, LR 0.000035 Loss 5.223667, Accuracy 86.459%\n",
      "Epoch 24, Batch 840, LR 0.000035 Loss 5.223549, Accuracy 86.463%\n",
      "Epoch 24, Batch 841, LR 0.000035 Loss 5.223994, Accuracy 86.461%\n",
      "Epoch 24, Batch 842, LR 0.000035 Loss 5.224866, Accuracy 86.456%\n",
      "Epoch 24, Batch 843, LR 0.000035 Loss 5.224880, Accuracy 86.456%\n",
      "Epoch 24, Batch 844, LR 0.000035 Loss 5.225062, Accuracy 86.454%\n",
      "Epoch 24, Batch 845, LR 0.000035 Loss 5.224917, Accuracy 86.458%\n",
      "Epoch 24, Batch 846, LR 0.000035 Loss 5.224580, Accuracy 86.456%\n",
      "Epoch 24, Batch 847, LR 0.000035 Loss 5.224549, Accuracy 86.457%\n",
      "Epoch 24, Batch 848, LR 0.000035 Loss 5.224576, Accuracy 86.459%\n",
      "Epoch 24, Batch 849, LR 0.000035 Loss 5.225181, Accuracy 86.456%\n",
      "Epoch 24, Batch 850, LR 0.000035 Loss 5.225459, Accuracy 86.452%\n",
      "Epoch 24, Batch 851, LR 0.000034 Loss 5.225361, Accuracy 86.455%\n",
      "Epoch 24, Batch 852, LR 0.000034 Loss 5.225067, Accuracy 86.456%\n",
      "Epoch 24, Batch 853, LR 0.000034 Loss 5.224322, Accuracy 86.461%\n",
      "Epoch 24, Batch 854, LR 0.000034 Loss 5.224280, Accuracy 86.464%\n",
      "Epoch 24, Batch 855, LR 0.000034 Loss 5.223392, Accuracy 86.470%\n",
      "Epoch 24, Batch 856, LR 0.000034 Loss 5.222782, Accuracy 86.473%\n",
      "Epoch 24, Batch 857, LR 0.000034 Loss 5.221606, Accuracy 86.476%\n",
      "Epoch 24, Batch 858, LR 0.000034 Loss 5.221857, Accuracy 86.475%\n",
      "Epoch 24, Batch 859, LR 0.000034 Loss 5.222019, Accuracy 86.480%\n",
      "Epoch 24, Batch 860, LR 0.000034 Loss 5.221750, Accuracy 86.481%\n",
      "Epoch 24, Batch 861, LR 0.000034 Loss 5.221722, Accuracy 86.481%\n",
      "Epoch 24, Batch 862, LR 0.000034 Loss 5.221410, Accuracy 86.482%\n",
      "Epoch 24, Batch 863, LR 0.000034 Loss 5.221449, Accuracy 86.486%\n",
      "Epoch 24, Batch 864, LR 0.000034 Loss 5.221196, Accuracy 86.487%\n",
      "Epoch 24, Batch 865, LR 0.000034 Loss 5.221754, Accuracy 86.483%\n",
      "Epoch 24, Batch 866, LR 0.000034 Loss 5.221956, Accuracy 86.476%\n",
      "Epoch 24, Batch 867, LR 0.000034 Loss 5.222428, Accuracy 86.476%\n",
      "Epoch 24, Batch 868, LR 0.000034 Loss 5.223856, Accuracy 86.469%\n",
      "Epoch 24, Batch 869, LR 0.000034 Loss 5.223867, Accuracy 86.472%\n",
      "Epoch 24, Batch 870, LR 0.000034 Loss 5.224169, Accuracy 86.474%\n",
      "Epoch 24, Batch 871, LR 0.000034 Loss 5.224023, Accuracy 86.475%\n",
      "Epoch 24, Batch 872, LR 0.000034 Loss 5.224468, Accuracy 86.477%\n",
      "Epoch 24, Batch 873, LR 0.000034 Loss 5.224651, Accuracy 86.476%\n",
      "Epoch 24, Batch 874, LR 0.000034 Loss 5.224743, Accuracy 86.476%\n",
      "Epoch 24, Batch 875, LR 0.000034 Loss 5.224324, Accuracy 86.476%\n",
      "Epoch 24, Batch 876, LR 0.000034 Loss 5.224951, Accuracy 86.469%\n",
      "Epoch 24, Batch 877, LR 0.000034 Loss 5.225272, Accuracy 86.471%\n",
      "Epoch 24, Batch 878, LR 0.000034 Loss 5.225305, Accuracy 86.468%\n",
      "Epoch 24, Batch 879, LR 0.000034 Loss 5.225981, Accuracy 86.470%\n",
      "Epoch 24, Batch 880, LR 0.000034 Loss 5.226034, Accuracy 86.469%\n",
      "Epoch 24, Batch 881, LR 0.000034 Loss 5.226623, Accuracy 86.468%\n",
      "Epoch 24, Batch 882, LR 0.000034 Loss 5.226804, Accuracy 86.465%\n",
      "Epoch 24, Batch 883, LR 0.000034 Loss 5.226357, Accuracy 86.471%\n",
      "Epoch 24, Batch 884, LR 0.000034 Loss 5.226781, Accuracy 86.469%\n",
      "Epoch 24, Batch 885, LR 0.000034 Loss 5.227280, Accuracy 86.465%\n",
      "Epoch 24, Batch 886, LR 0.000034 Loss 5.226995, Accuracy 86.462%\n",
      "Epoch 24, Batch 887, LR 0.000034 Loss 5.226255, Accuracy 86.463%\n",
      "Epoch 24, Batch 888, LR 0.000034 Loss 5.225338, Accuracy 86.469%\n",
      "Epoch 24, Batch 889, LR 0.000034 Loss 5.224654, Accuracy 86.474%\n",
      "Epoch 24, Batch 890, LR 0.000034 Loss 5.224362, Accuracy 86.472%\n",
      "Epoch 24, Batch 891, LR 0.000034 Loss 5.223704, Accuracy 86.479%\n",
      "Epoch 24, Batch 892, LR 0.000034 Loss 5.223452, Accuracy 86.481%\n",
      "Epoch 24, Batch 893, LR 0.000034 Loss 5.223026, Accuracy 86.484%\n",
      "Epoch 24, Batch 894, LR 0.000034 Loss 5.222737, Accuracy 86.489%\n",
      "Epoch 24, Batch 895, LR 0.000034 Loss 5.222183, Accuracy 86.491%\n",
      "Epoch 24, Batch 896, LR 0.000034 Loss 5.222017, Accuracy 86.491%\n",
      "Epoch 24, Batch 897, LR 0.000034 Loss 5.221996, Accuracy 86.492%\n",
      "Epoch 24, Batch 898, LR 0.000034 Loss 5.221947, Accuracy 86.493%\n",
      "Epoch 24, Batch 899, LR 0.000034 Loss 5.221479, Accuracy 86.493%\n",
      "Epoch 24, Batch 900, LR 0.000034 Loss 5.221639, Accuracy 86.489%\n",
      "Epoch 24, Batch 901, LR 0.000034 Loss 5.221026, Accuracy 86.490%\n",
      "Epoch 24, Batch 902, LR 0.000034 Loss 5.221290, Accuracy 86.491%\n",
      "Epoch 24, Batch 903, LR 0.000034 Loss 5.221553, Accuracy 86.493%\n",
      "Epoch 24, Batch 904, LR 0.000034 Loss 5.221493, Accuracy 86.493%\n",
      "Epoch 24, Batch 905, LR 0.000034 Loss 5.221588, Accuracy 86.496%\n",
      "Epoch 24, Batch 906, LR 0.000034 Loss 5.220861, Accuracy 86.500%\n",
      "Epoch 24, Batch 907, LR 0.000034 Loss 5.220300, Accuracy 86.500%\n",
      "Epoch 24, Batch 908, LR 0.000034 Loss 5.220336, Accuracy 86.503%\n",
      "Epoch 24, Batch 909, LR 0.000034 Loss 5.220287, Accuracy 86.506%\n",
      "Epoch 24, Batch 910, LR 0.000034 Loss 5.220258, Accuracy 86.507%\n",
      "Epoch 24, Batch 911, LR 0.000034 Loss 5.220259, Accuracy 86.507%\n",
      "Epoch 24, Batch 912, LR 0.000034 Loss 5.220089, Accuracy 86.508%\n",
      "Epoch 24, Batch 913, LR 0.000034 Loss 5.219691, Accuracy 86.510%\n",
      "Epoch 24, Batch 914, LR 0.000034 Loss 5.219024, Accuracy 86.513%\n",
      "Epoch 24, Batch 915, LR 0.000034 Loss 5.218905, Accuracy 86.514%\n",
      "Epoch 24, Batch 916, LR 0.000034 Loss 5.218026, Accuracy 86.518%\n",
      "Epoch 24, Batch 917, LR 0.000034 Loss 5.217200, Accuracy 86.519%\n",
      "Epoch 24, Batch 918, LR 0.000034 Loss 5.217665, Accuracy 86.518%\n",
      "Epoch 24, Batch 919, LR 0.000034 Loss 5.217308, Accuracy 86.522%\n",
      "Epoch 24, Batch 920, LR 0.000034 Loss 5.217087, Accuracy 86.522%\n",
      "Epoch 24, Batch 921, LR 0.000034 Loss 5.217416, Accuracy 86.519%\n",
      "Epoch 24, Batch 922, LR 0.000034 Loss 5.217268, Accuracy 86.523%\n",
      "Epoch 24, Batch 923, LR 0.000034 Loss 5.217058, Accuracy 86.525%\n",
      "Epoch 24, Batch 924, LR 0.000034 Loss 5.216937, Accuracy 86.524%\n",
      "Epoch 24, Batch 925, LR 0.000034 Loss 5.216545, Accuracy 86.529%\n",
      "Epoch 24, Batch 926, LR 0.000034 Loss 5.216298, Accuracy 86.529%\n",
      "Epoch 24, Batch 927, LR 0.000034 Loss 5.215550, Accuracy 86.530%\n",
      "Epoch 24, Batch 928, LR 0.000034 Loss 5.215553, Accuracy 86.530%\n",
      "Epoch 24, Batch 929, LR 0.000034 Loss 5.215091, Accuracy 86.535%\n",
      "Epoch 24, Batch 930, LR 0.000034 Loss 5.216247, Accuracy 86.536%\n",
      "Epoch 24, Batch 931, LR 0.000034 Loss 5.216524, Accuracy 86.537%\n",
      "Epoch 24, Batch 932, LR 0.000034 Loss 5.217451, Accuracy 86.534%\n",
      "Epoch 24, Batch 933, LR 0.000034 Loss 5.217531, Accuracy 86.535%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 934, LR 0.000034 Loss 5.217573, Accuracy 86.531%\n",
      "Epoch 24, Batch 935, LR 0.000034 Loss 5.217158, Accuracy 86.534%\n",
      "Epoch 24, Batch 936, LR 0.000034 Loss 5.217870, Accuracy 86.528%\n",
      "Epoch 24, Batch 937, LR 0.000034 Loss 5.217227, Accuracy 86.529%\n",
      "Epoch 24, Batch 938, LR 0.000034 Loss 5.217706, Accuracy 86.526%\n",
      "Epoch 24, Batch 939, LR 0.000034 Loss 5.217562, Accuracy 86.527%\n",
      "Epoch 24, Batch 940, LR 0.000034 Loss 5.217766, Accuracy 86.525%\n",
      "Epoch 24, Batch 941, LR 0.000034 Loss 5.217582, Accuracy 86.526%\n",
      "Epoch 24, Batch 942, LR 0.000034 Loss 5.217543, Accuracy 86.527%\n",
      "Epoch 24, Batch 943, LR 0.000034 Loss 5.217700, Accuracy 86.527%\n",
      "Epoch 24, Batch 944, LR 0.000034 Loss 5.217739, Accuracy 86.526%\n",
      "Epoch 24, Batch 945, LR 0.000034 Loss 5.217866, Accuracy 86.527%\n",
      "Epoch 24, Batch 946, LR 0.000034 Loss 5.217200, Accuracy 86.530%\n",
      "Epoch 24, Batch 947, LR 0.000034 Loss 5.216919, Accuracy 86.532%\n",
      "Epoch 24, Batch 948, LR 0.000034 Loss 5.216816, Accuracy 86.532%\n",
      "Epoch 24, Batch 949, LR 0.000034 Loss 5.217285, Accuracy 86.530%\n",
      "Epoch 24, Batch 950, LR 0.000034 Loss 5.217670, Accuracy 86.530%\n",
      "Epoch 24, Batch 951, LR 0.000034 Loss 5.216449, Accuracy 86.535%\n",
      "Epoch 24, Batch 952, LR 0.000034 Loss 5.216501, Accuracy 86.536%\n",
      "Epoch 24, Batch 953, LR 0.000034 Loss 5.216180, Accuracy 86.538%\n",
      "Epoch 24, Batch 954, LR 0.000034 Loss 5.216180, Accuracy 86.540%\n",
      "Epoch 24, Batch 955, LR 0.000034 Loss 5.215969, Accuracy 86.536%\n",
      "Epoch 24, Batch 956, LR 0.000034 Loss 5.215295, Accuracy 86.538%\n",
      "Epoch 24, Batch 957, LR 0.000034 Loss 5.214450, Accuracy 86.544%\n",
      "Epoch 24, Batch 958, LR 0.000034 Loss 5.214509, Accuracy 86.543%\n",
      "Epoch 24, Batch 959, LR 0.000034 Loss 5.214852, Accuracy 86.540%\n",
      "Epoch 24, Batch 960, LR 0.000034 Loss 5.214804, Accuracy 86.546%\n",
      "Epoch 24, Batch 961, LR 0.000034 Loss 5.214391, Accuracy 86.550%\n",
      "Epoch 24, Batch 962, LR 0.000034 Loss 5.214046, Accuracy 86.553%\n",
      "Epoch 24, Batch 963, LR 0.000034 Loss 5.214374, Accuracy 86.552%\n",
      "Epoch 24, Batch 964, LR 0.000034 Loss 5.214849, Accuracy 86.551%\n",
      "Epoch 24, Batch 965, LR 0.000034 Loss 5.215691, Accuracy 86.546%\n",
      "Epoch 24, Batch 966, LR 0.000034 Loss 5.216020, Accuracy 86.544%\n",
      "Epoch 24, Batch 967, LR 0.000034 Loss 5.215590, Accuracy 86.548%\n",
      "Epoch 24, Batch 968, LR 0.000034 Loss 5.215340, Accuracy 86.548%\n",
      "Epoch 24, Batch 969, LR 0.000034 Loss 5.215517, Accuracy 86.549%\n",
      "Epoch 24, Batch 970, LR 0.000034 Loss 5.215332, Accuracy 86.549%\n",
      "Epoch 24, Batch 971, LR 0.000034 Loss 5.215238, Accuracy 86.548%\n",
      "Epoch 24, Batch 972, LR 0.000034 Loss 5.214748, Accuracy 86.549%\n",
      "Epoch 24, Batch 973, LR 0.000034 Loss 5.213979, Accuracy 86.555%\n",
      "Epoch 24, Batch 974, LR 0.000034 Loss 5.214236, Accuracy 86.554%\n",
      "Epoch 24, Batch 975, LR 0.000034 Loss 5.214232, Accuracy 86.554%\n",
      "Epoch 24, Batch 976, LR 0.000034 Loss 5.214605, Accuracy 86.554%\n",
      "Epoch 24, Batch 977, LR 0.000034 Loss 5.215215, Accuracy 86.554%\n",
      "Epoch 24, Batch 978, LR 0.000034 Loss 5.215498, Accuracy 86.554%\n",
      "Epoch 24, Batch 979, LR 0.000034 Loss 5.215428, Accuracy 86.554%\n",
      "Epoch 24, Batch 980, LR 0.000034 Loss 5.214916, Accuracy 86.558%\n",
      "Epoch 24, Batch 981, LR 0.000034 Loss 5.214436, Accuracy 86.560%\n",
      "Epoch 24, Batch 982, LR 0.000034 Loss 5.214793, Accuracy 86.556%\n",
      "Epoch 24, Batch 983, LR 0.000034 Loss 5.214915, Accuracy 86.554%\n",
      "Epoch 24, Batch 984, LR 0.000034 Loss 5.215131, Accuracy 86.553%\n",
      "Epoch 24, Batch 985, LR 0.000034 Loss 5.214864, Accuracy 86.553%\n",
      "Epoch 24, Batch 986, LR 0.000034 Loss 5.215096, Accuracy 86.552%\n",
      "Epoch 24, Batch 987, LR 0.000034 Loss 5.215235, Accuracy 86.551%\n",
      "Epoch 24, Batch 988, LR 0.000034 Loss 5.216037, Accuracy 86.550%\n",
      "Epoch 24, Batch 989, LR 0.000034 Loss 5.216621, Accuracy 86.548%\n",
      "Epoch 24, Batch 990, LR 0.000034 Loss 5.216394, Accuracy 86.550%\n",
      "Epoch 24, Batch 991, LR 0.000034 Loss 5.216369, Accuracy 86.552%\n",
      "Epoch 24, Batch 992, LR 0.000034 Loss 5.216910, Accuracy 86.549%\n",
      "Epoch 24, Batch 993, LR 0.000034 Loss 5.217270, Accuracy 86.552%\n",
      "Epoch 24, Batch 994, LR 0.000034 Loss 5.218545, Accuracy 86.543%\n",
      "Epoch 24, Batch 995, LR 0.000034 Loss 5.218530, Accuracy 86.544%\n",
      "Epoch 24, Batch 996, LR 0.000034 Loss 5.218591, Accuracy 86.545%\n",
      "Epoch 24, Batch 997, LR 0.000034 Loss 5.218910, Accuracy 86.546%\n",
      "Epoch 24, Batch 998, LR 0.000034 Loss 5.218827, Accuracy 86.549%\n",
      "Epoch 24, Batch 999, LR 0.000034 Loss 5.219373, Accuracy 86.553%\n",
      "Epoch 24, Batch 1000, LR 0.000034 Loss 5.220831, Accuracy 86.545%\n",
      "Epoch 24, Batch 1001, LR 0.000034 Loss 5.220975, Accuracy 86.543%\n",
      "Epoch 24, Batch 1002, LR 0.000034 Loss 5.221032, Accuracy 86.541%\n",
      "Epoch 24, Batch 1003, LR 0.000034 Loss 5.221234, Accuracy 86.540%\n",
      "Epoch 24, Batch 1004, LR 0.000034 Loss 5.221469, Accuracy 86.535%\n",
      "Epoch 24, Batch 1005, LR 0.000034 Loss 5.221921, Accuracy 86.534%\n",
      "Epoch 24, Batch 1006, LR 0.000034 Loss 5.221922, Accuracy 86.535%\n",
      "Epoch 24, Batch 1007, LR 0.000034 Loss 5.222248, Accuracy 86.533%\n",
      "Epoch 24, Batch 1008, LR 0.000034 Loss 5.221924, Accuracy 86.534%\n",
      "Epoch 24, Batch 1009, LR 0.000034 Loss 5.222075, Accuracy 86.535%\n",
      "Epoch 24, Batch 1010, LR 0.000034 Loss 5.222639, Accuracy 86.535%\n",
      "Epoch 24, Batch 1011, LR 0.000034 Loss 5.222711, Accuracy 86.531%\n",
      "Epoch 24, Batch 1012, LR 0.000034 Loss 5.222941, Accuracy 86.528%\n",
      "Epoch 24, Batch 1013, LR 0.000034 Loss 5.222703, Accuracy 86.527%\n",
      "Epoch 24, Batch 1014, LR 0.000034 Loss 5.223412, Accuracy 86.524%\n",
      "Epoch 24, Batch 1015, LR 0.000034 Loss 5.222793, Accuracy 86.529%\n",
      "Epoch 24, Batch 1016, LR 0.000034 Loss 5.222572, Accuracy 86.530%\n",
      "Epoch 24, Batch 1017, LR 0.000034 Loss 5.222845, Accuracy 86.530%\n",
      "Epoch 24, Batch 1018, LR 0.000034 Loss 5.222805, Accuracy 86.532%\n",
      "Epoch 24, Batch 1019, LR 0.000034 Loss 5.223424, Accuracy 86.530%\n",
      "Epoch 24, Batch 1020, LR 0.000034 Loss 5.223725, Accuracy 86.533%\n",
      "Epoch 24, Batch 1021, LR 0.000034 Loss 5.223769, Accuracy 86.535%\n",
      "Epoch 24, Batch 1022, LR 0.000034 Loss 5.223935, Accuracy 86.536%\n",
      "Epoch 24, Batch 1023, LR 0.000034 Loss 5.224357, Accuracy 86.533%\n",
      "Epoch 24, Batch 1024, LR 0.000034 Loss 5.224026, Accuracy 86.536%\n",
      "Epoch 24, Batch 1025, LR 0.000034 Loss 5.224825, Accuracy 86.536%\n",
      "Epoch 24, Batch 1026, LR 0.000034 Loss 5.225012, Accuracy 86.534%\n",
      "Epoch 24, Batch 1027, LR 0.000034 Loss 5.223879, Accuracy 86.540%\n",
      "Epoch 24, Batch 1028, LR 0.000034 Loss 5.223874, Accuracy 86.542%\n",
      "Epoch 24, Batch 1029, LR 0.000034 Loss 5.223547, Accuracy 86.545%\n",
      "Epoch 24, Batch 1030, LR 0.000034 Loss 5.224056, Accuracy 86.546%\n",
      "Epoch 24, Batch 1031, LR 0.000034 Loss 5.224503, Accuracy 86.542%\n",
      "Epoch 24, Batch 1032, LR 0.000034 Loss 5.225020, Accuracy 86.541%\n",
      "Epoch 24, Batch 1033, LR 0.000034 Loss 5.225404, Accuracy 86.540%\n",
      "Epoch 24, Batch 1034, LR 0.000034 Loss 5.225662, Accuracy 86.539%\n",
      "Epoch 24, Batch 1035, LR 0.000034 Loss 5.225465, Accuracy 86.541%\n",
      "Epoch 24, Batch 1036, LR 0.000034 Loss 5.225485, Accuracy 86.542%\n",
      "Epoch 24, Batch 1037, LR 0.000034 Loss 5.225717, Accuracy 86.543%\n",
      "Epoch 24, Batch 1038, LR 0.000034 Loss 5.225945, Accuracy 86.541%\n",
      "Epoch 24, Batch 1039, LR 0.000034 Loss 5.225690, Accuracy 86.541%\n",
      "Epoch 24, Batch 1040, LR 0.000034 Loss 5.225243, Accuracy 86.544%\n",
      "Epoch 24, Batch 1041, LR 0.000034 Loss 5.225280, Accuracy 86.546%\n",
      "Epoch 24, Batch 1042, LR 0.000034 Loss 5.225407, Accuracy 86.549%\n",
      "Epoch 24, Batch 1043, LR 0.000034 Loss 5.226041, Accuracy 86.545%\n",
      "Epoch 24, Batch 1044, LR 0.000034 Loss 5.226363, Accuracy 86.547%\n",
      "Epoch 24, Batch 1045, LR 0.000034 Loss 5.226193, Accuracy 86.551%\n",
      "Epoch 24, Batch 1046, LR 0.000034 Loss 5.226484, Accuracy 86.549%\n",
      "Epoch 24, Batch 1047, LR 0.000034 Loss 5.227402, Accuracy 86.547%\n",
      "Epoch 24, Loss (train set) 5.227402, Accuracy (train set) 86.547%\n",
      "Epoch 24, Accuracy (validation set) 68.192%\n",
      "Epoch 24, EER (test set) 5.425%\n",
      "Epoch 25, Batch 1, LR 0.000034 Loss 5.692909, Accuracy 82.031%\n",
      "Epoch 25, Batch 2, LR 0.000034 Loss 5.398133, Accuracy 83.984%\n",
      "Epoch 25, Batch 3, LR 0.000034 Loss 5.438466, Accuracy 84.115%\n",
      "Epoch 25, Batch 4, LR 0.000034 Loss 5.309299, Accuracy 85.547%\n",
      "Epoch 25, Batch 5, LR 0.000034 Loss 5.281586, Accuracy 86.719%\n",
      "Epoch 25, Batch 6, LR 0.000034 Loss 5.269328, Accuracy 86.979%\n",
      "Epoch 25, Batch 7, LR 0.000034 Loss 5.063518, Accuracy 87.835%\n",
      "Epoch 25, Batch 8, LR 0.000034 Loss 5.222175, Accuracy 86.816%\n",
      "Epoch 25, Batch 9, LR 0.000034 Loss 5.220041, Accuracy 87.066%\n",
      "Epoch 25, Batch 10, LR 0.000034 Loss 5.329978, Accuracy 86.328%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 11, LR 0.000034 Loss 5.308017, Accuracy 86.577%\n",
      "Epoch 25, Batch 12, LR 0.000034 Loss 5.258835, Accuracy 86.719%\n",
      "Epoch 25, Batch 13, LR 0.000034 Loss 5.227932, Accuracy 86.959%\n",
      "Epoch 25, Batch 14, LR 0.000034 Loss 5.213094, Accuracy 86.886%\n",
      "Epoch 25, Batch 15, LR 0.000034 Loss 5.194102, Accuracy 86.823%\n",
      "Epoch 25, Batch 16, LR 0.000034 Loss 5.179323, Accuracy 86.865%\n",
      "Epoch 25, Batch 17, LR 0.000034 Loss 5.201998, Accuracy 86.857%\n",
      "Epoch 25, Batch 18, LR 0.000034 Loss 5.231459, Accuracy 86.849%\n",
      "Epoch 25, Batch 19, LR 0.000034 Loss 5.264346, Accuracy 86.924%\n",
      "Epoch 25, Batch 20, LR 0.000034 Loss 5.264505, Accuracy 86.992%\n",
      "Epoch 25, Batch 21, LR 0.000034 Loss 5.234907, Accuracy 87.165%\n",
      "Epoch 25, Batch 22, LR 0.000034 Loss 5.236848, Accuracy 87.074%\n",
      "Epoch 25, Batch 23, LR 0.000034 Loss 5.250257, Accuracy 86.957%\n",
      "Epoch 25, Batch 24, LR 0.000034 Loss 5.251702, Accuracy 86.947%\n",
      "Epoch 25, Batch 25, LR 0.000034 Loss 5.267820, Accuracy 86.906%\n",
      "Epoch 25, Batch 26, LR 0.000034 Loss 5.222917, Accuracy 87.079%\n",
      "Epoch 25, Batch 27, LR 0.000034 Loss 5.209926, Accuracy 87.066%\n",
      "Epoch 25, Batch 28, LR 0.000034 Loss 5.199523, Accuracy 87.109%\n",
      "Epoch 25, Batch 29, LR 0.000034 Loss 5.192541, Accuracy 87.177%\n",
      "Epoch 25, Batch 30, LR 0.000034 Loss 5.203349, Accuracy 87.005%\n",
      "Epoch 25, Batch 31, LR 0.000034 Loss 5.195024, Accuracy 86.946%\n",
      "Epoch 25, Batch 32, LR 0.000034 Loss 5.194541, Accuracy 87.012%\n",
      "Epoch 25, Batch 33, LR 0.000034 Loss 5.215568, Accuracy 86.766%\n",
      "Epoch 25, Batch 34, LR 0.000034 Loss 5.242687, Accuracy 86.650%\n",
      "Epoch 25, Batch 35, LR 0.000034 Loss 5.231654, Accuracy 86.696%\n",
      "Epoch 25, Batch 36, LR 0.000034 Loss 5.225846, Accuracy 86.719%\n",
      "Epoch 25, Batch 37, LR 0.000034 Loss 5.212180, Accuracy 86.698%\n",
      "Epoch 25, Batch 38, LR 0.000034 Loss 5.219805, Accuracy 86.739%\n",
      "Epoch 25, Batch 39, LR 0.000034 Loss 5.233089, Accuracy 86.719%\n",
      "Epoch 25, Batch 40, LR 0.000034 Loss 5.223779, Accuracy 86.875%\n",
      "Epoch 25, Batch 41, LR 0.000034 Loss 5.213370, Accuracy 86.909%\n",
      "Epoch 25, Batch 42, LR 0.000034 Loss 5.210844, Accuracy 86.942%\n",
      "Epoch 25, Batch 43, LR 0.000034 Loss 5.220424, Accuracy 86.900%\n",
      "Epoch 25, Batch 44, LR 0.000034 Loss 5.228878, Accuracy 86.843%\n",
      "Epoch 25, Batch 45, LR 0.000034 Loss 5.234237, Accuracy 86.788%\n",
      "Epoch 25, Batch 46, LR 0.000034 Loss 5.229862, Accuracy 86.821%\n",
      "Epoch 25, Batch 47, LR 0.000034 Loss 5.227276, Accuracy 86.835%\n",
      "Epoch 25, Batch 48, LR 0.000034 Loss 5.219395, Accuracy 86.833%\n",
      "Epoch 25, Batch 49, LR 0.000034 Loss 5.206847, Accuracy 86.942%\n",
      "Epoch 25, Batch 50, LR 0.000034 Loss 5.193357, Accuracy 86.938%\n",
      "Epoch 25, Batch 51, LR 0.000034 Loss 5.177845, Accuracy 87.025%\n",
      "Epoch 25, Batch 52, LR 0.000034 Loss 5.174957, Accuracy 86.959%\n",
      "Epoch 25, Batch 53, LR 0.000034 Loss 5.182821, Accuracy 86.837%\n",
      "Epoch 25, Batch 54, LR 0.000034 Loss 5.193649, Accuracy 86.820%\n",
      "Epoch 25, Batch 55, LR 0.000034 Loss 5.192500, Accuracy 86.932%\n",
      "Epoch 25, Batch 56, LR 0.000034 Loss 5.197909, Accuracy 86.886%\n",
      "Epoch 25, Batch 57, LR 0.000034 Loss 5.183700, Accuracy 86.938%\n",
      "Epoch 25, Batch 58, LR 0.000034 Loss 5.178076, Accuracy 86.921%\n",
      "Epoch 25, Batch 59, LR 0.000034 Loss 5.177946, Accuracy 86.931%\n",
      "Epoch 25, Batch 60, LR 0.000034 Loss 5.176716, Accuracy 87.018%\n",
      "Epoch 25, Batch 61, LR 0.000034 Loss 5.170849, Accuracy 87.103%\n",
      "Epoch 25, Batch 62, LR 0.000034 Loss 5.167811, Accuracy 87.122%\n",
      "Epoch 25, Batch 63, LR 0.000034 Loss 5.171766, Accuracy 87.078%\n",
      "Epoch 25, Batch 64, LR 0.000034 Loss 5.170452, Accuracy 87.048%\n",
      "Epoch 25, Batch 65, LR 0.000034 Loss 5.161301, Accuracy 87.067%\n",
      "Epoch 25, Batch 66, LR 0.000034 Loss 5.157777, Accuracy 87.062%\n",
      "Epoch 25, Batch 67, LR 0.000034 Loss 5.158319, Accuracy 87.069%\n",
      "Epoch 25, Batch 68, LR 0.000034 Loss 5.162293, Accuracy 87.063%\n",
      "Epoch 25, Batch 69, LR 0.000034 Loss 5.160162, Accuracy 87.115%\n",
      "Epoch 25, Batch 70, LR 0.000034 Loss 5.163681, Accuracy 87.109%\n",
      "Epoch 25, Batch 71, LR 0.000034 Loss 5.162151, Accuracy 87.126%\n",
      "Epoch 25, Batch 72, LR 0.000033 Loss 5.159628, Accuracy 87.131%\n",
      "Epoch 25, Batch 73, LR 0.000033 Loss 5.158616, Accuracy 87.061%\n",
      "Epoch 25, Batch 74, LR 0.000033 Loss 5.149895, Accuracy 87.088%\n",
      "Epoch 25, Batch 75, LR 0.000033 Loss 5.146290, Accuracy 87.146%\n",
      "Epoch 25, Batch 76, LR 0.000033 Loss 5.151707, Accuracy 87.079%\n",
      "Epoch 25, Batch 77, LR 0.000033 Loss 5.148757, Accuracy 87.114%\n",
      "Epoch 25, Batch 78, LR 0.000033 Loss 5.152409, Accuracy 87.059%\n",
      "Epoch 25, Batch 79, LR 0.000033 Loss 5.149097, Accuracy 87.015%\n",
      "Epoch 25, Batch 80, LR 0.000033 Loss 5.153807, Accuracy 87.021%\n",
      "Epoch 25, Batch 81, LR 0.000033 Loss 5.144515, Accuracy 87.076%\n",
      "Epoch 25, Batch 82, LR 0.000033 Loss 5.149101, Accuracy 87.062%\n",
      "Epoch 25, Batch 83, LR 0.000033 Loss 5.146181, Accuracy 87.020%\n",
      "Epoch 25, Batch 84, LR 0.000033 Loss 5.150850, Accuracy 86.998%\n",
      "Epoch 25, Batch 85, LR 0.000033 Loss 5.148709, Accuracy 87.022%\n",
      "Epoch 25, Batch 86, LR 0.000033 Loss 5.148618, Accuracy 87.009%\n",
      "Epoch 25, Batch 87, LR 0.000033 Loss 5.151156, Accuracy 87.015%\n",
      "Epoch 25, Batch 88, LR 0.000033 Loss 5.144606, Accuracy 87.038%\n",
      "Epoch 25, Batch 89, LR 0.000033 Loss 5.139260, Accuracy 87.079%\n",
      "Epoch 25, Batch 90, LR 0.000033 Loss 5.146454, Accuracy 87.057%\n",
      "Epoch 25, Batch 91, LR 0.000033 Loss 5.141726, Accuracy 87.079%\n",
      "Epoch 25, Batch 92, LR 0.000033 Loss 5.147537, Accuracy 87.007%\n",
      "Epoch 25, Batch 93, LR 0.000033 Loss 5.148577, Accuracy 87.013%\n",
      "Epoch 25, Batch 94, LR 0.000033 Loss 5.143774, Accuracy 87.001%\n",
      "Epoch 25, Batch 95, LR 0.000033 Loss 5.143831, Accuracy 87.031%\n",
      "Epoch 25, Batch 96, LR 0.000033 Loss 5.146248, Accuracy 87.020%\n",
      "Epoch 25, Batch 97, LR 0.000033 Loss 5.146632, Accuracy 87.001%\n",
      "Epoch 25, Batch 98, LR 0.000033 Loss 5.142481, Accuracy 86.990%\n",
      "Epoch 25, Batch 99, LR 0.000033 Loss 5.141128, Accuracy 86.987%\n",
      "Epoch 25, Batch 100, LR 0.000033 Loss 5.142416, Accuracy 86.953%\n",
      "Epoch 25, Batch 101, LR 0.000033 Loss 5.139139, Accuracy 86.974%\n",
      "Epoch 25, Batch 102, LR 0.000033 Loss 5.135466, Accuracy 86.987%\n",
      "Epoch 25, Batch 103, LR 0.000033 Loss 5.128445, Accuracy 87.022%\n",
      "Epoch 25, Batch 104, LR 0.000033 Loss 5.126203, Accuracy 87.049%\n",
      "Epoch 25, Batch 105, LR 0.000033 Loss 5.125843, Accuracy 86.994%\n",
      "Epoch 25, Batch 106, LR 0.000033 Loss 5.128219, Accuracy 86.991%\n",
      "Epoch 25, Batch 107, LR 0.000033 Loss 5.126556, Accuracy 87.004%\n",
      "Epoch 25, Batch 108, LR 0.000033 Loss 5.131598, Accuracy 87.008%\n",
      "Epoch 25, Batch 109, LR 0.000033 Loss 5.137666, Accuracy 86.991%\n",
      "Epoch 25, Batch 110, LR 0.000033 Loss 5.140915, Accuracy 86.996%\n",
      "Epoch 25, Batch 111, LR 0.000033 Loss 5.140560, Accuracy 86.979%\n",
      "Epoch 25, Batch 112, LR 0.000033 Loss 5.140221, Accuracy 86.991%\n",
      "Epoch 25, Batch 113, LR 0.000033 Loss 5.141493, Accuracy 86.981%\n",
      "Epoch 25, Batch 114, LR 0.000033 Loss 5.144040, Accuracy 86.959%\n",
      "Epoch 25, Batch 115, LR 0.000033 Loss 5.141787, Accuracy 86.977%\n",
      "Epoch 25, Batch 116, LR 0.000033 Loss 5.138663, Accuracy 87.008%\n",
      "Epoch 25, Batch 117, LR 0.000033 Loss 5.142827, Accuracy 86.999%\n",
      "Epoch 25, Batch 118, LR 0.000033 Loss 5.139054, Accuracy 87.003%\n",
      "Epoch 25, Batch 119, LR 0.000033 Loss 5.136715, Accuracy 87.008%\n",
      "Epoch 25, Batch 120, LR 0.000033 Loss 5.143599, Accuracy 86.973%\n",
      "Epoch 25, Batch 121, LR 0.000033 Loss 5.143040, Accuracy 86.964%\n",
      "Epoch 25, Batch 122, LR 0.000033 Loss 5.141135, Accuracy 86.975%\n",
      "Epoch 25, Batch 123, LR 0.000033 Loss 5.137467, Accuracy 87.011%\n",
      "Epoch 25, Batch 124, LR 0.000033 Loss 5.140359, Accuracy 86.990%\n",
      "Epoch 25, Batch 125, LR 0.000033 Loss 5.141281, Accuracy 86.975%\n",
      "Epoch 25, Batch 126, LR 0.000033 Loss 5.144116, Accuracy 86.961%\n",
      "Epoch 25, Batch 127, LR 0.000033 Loss 5.143058, Accuracy 86.916%\n",
      "Epoch 25, Batch 128, LR 0.000033 Loss 5.143894, Accuracy 86.908%\n",
      "Epoch 25, Batch 129, LR 0.000033 Loss 5.141049, Accuracy 86.919%\n",
      "Epoch 25, Batch 130, LR 0.000033 Loss 5.144090, Accuracy 86.911%\n",
      "Epoch 25, Batch 131, LR 0.000033 Loss 5.148488, Accuracy 86.916%\n",
      "Epoch 25, Batch 132, LR 0.000033 Loss 5.147602, Accuracy 86.914%\n",
      "Epoch 25, Batch 133, LR 0.000033 Loss 5.150878, Accuracy 86.889%\n",
      "Epoch 25, Batch 134, LR 0.000033 Loss 5.154297, Accuracy 86.853%\n",
      "Epoch 25, Batch 135, LR 0.000033 Loss 5.155708, Accuracy 86.852%\n",
      "Epoch 25, Batch 136, LR 0.000033 Loss 5.158489, Accuracy 86.857%\n",
      "Epoch 25, Batch 137, LR 0.000033 Loss 5.155836, Accuracy 86.867%\n",
      "Epoch 25, Batch 138, LR 0.000033 Loss 5.157390, Accuracy 86.872%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 139, LR 0.000033 Loss 5.159282, Accuracy 86.859%\n",
      "Epoch 25, Batch 140, LR 0.000033 Loss 5.159039, Accuracy 86.881%\n",
      "Epoch 25, Batch 141, LR 0.000033 Loss 5.162835, Accuracy 86.885%\n",
      "Epoch 25, Batch 142, LR 0.000033 Loss 5.159764, Accuracy 86.900%\n",
      "Epoch 25, Batch 143, LR 0.000033 Loss 5.157815, Accuracy 86.905%\n",
      "Epoch 25, Batch 144, LR 0.000033 Loss 5.162097, Accuracy 86.876%\n",
      "Epoch 25, Batch 145, LR 0.000033 Loss 5.154920, Accuracy 86.891%\n",
      "Epoch 25, Batch 146, LR 0.000033 Loss 5.150732, Accuracy 86.885%\n",
      "Epoch 25, Batch 147, LR 0.000033 Loss 5.147111, Accuracy 86.905%\n",
      "Epoch 25, Batch 148, LR 0.000033 Loss 5.142658, Accuracy 86.909%\n",
      "Epoch 25, Batch 149, LR 0.000033 Loss 5.140106, Accuracy 86.928%\n",
      "Epoch 25, Batch 150, LR 0.000033 Loss 5.139685, Accuracy 86.932%\n",
      "Epoch 25, Batch 151, LR 0.000033 Loss 5.139051, Accuracy 86.936%\n",
      "Epoch 25, Batch 152, LR 0.000033 Loss 5.140914, Accuracy 86.914%\n",
      "Epoch 25, Batch 153, LR 0.000033 Loss 5.141786, Accuracy 86.913%\n",
      "Epoch 25, Batch 154, LR 0.000033 Loss 5.140955, Accuracy 86.937%\n",
      "Epoch 25, Batch 155, LR 0.000033 Loss 5.139960, Accuracy 86.941%\n",
      "Epoch 25, Batch 156, LR 0.000033 Loss 5.143912, Accuracy 86.934%\n",
      "Epoch 25, Batch 157, LR 0.000033 Loss 5.147939, Accuracy 86.933%\n",
      "Epoch 25, Batch 158, LR 0.000033 Loss 5.152516, Accuracy 86.921%\n",
      "Epoch 25, Batch 159, LR 0.000033 Loss 5.154698, Accuracy 86.930%\n",
      "Epoch 25, Batch 160, LR 0.000033 Loss 5.155032, Accuracy 86.924%\n",
      "Epoch 25, Batch 161, LR 0.000033 Loss 5.156149, Accuracy 86.932%\n",
      "Epoch 25, Batch 162, LR 0.000033 Loss 5.158265, Accuracy 86.931%\n",
      "Epoch 25, Batch 163, LR 0.000033 Loss 5.159614, Accuracy 86.920%\n",
      "Epoch 25, Batch 164, LR 0.000033 Loss 5.157901, Accuracy 86.933%\n",
      "Epoch 25, Batch 165, LR 0.000033 Loss 5.156643, Accuracy 86.932%\n",
      "Epoch 25, Batch 166, LR 0.000033 Loss 5.159308, Accuracy 86.921%\n",
      "Epoch 25, Batch 167, LR 0.000033 Loss 5.158977, Accuracy 86.934%\n",
      "Epoch 25, Batch 168, LR 0.000033 Loss 5.161898, Accuracy 86.909%\n",
      "Epoch 25, Batch 169, LR 0.000033 Loss 5.158653, Accuracy 86.931%\n",
      "Epoch 25, Batch 170, LR 0.000033 Loss 5.150221, Accuracy 86.958%\n",
      "Epoch 25, Batch 171, LR 0.000033 Loss 5.151720, Accuracy 86.956%\n",
      "Epoch 25, Batch 172, LR 0.000033 Loss 5.149951, Accuracy 86.973%\n",
      "Epoch 25, Batch 173, LR 0.000033 Loss 5.147349, Accuracy 86.990%\n",
      "Epoch 25, Batch 174, LR 0.000033 Loss 5.146595, Accuracy 86.984%\n",
      "Epoch 25, Batch 175, LR 0.000033 Loss 5.148461, Accuracy 86.964%\n",
      "Epoch 25, Batch 176, LR 0.000033 Loss 5.145509, Accuracy 86.985%\n",
      "Epoch 25, Batch 177, LR 0.000033 Loss 5.148007, Accuracy 86.966%\n",
      "Epoch 25, Batch 178, LR 0.000033 Loss 5.145597, Accuracy 86.991%\n",
      "Epoch 25, Batch 179, LR 0.000033 Loss 5.144443, Accuracy 87.002%\n",
      "Epoch 25, Batch 180, LR 0.000033 Loss 5.145124, Accuracy 86.992%\n",
      "Epoch 25, Batch 181, LR 0.000033 Loss 5.151016, Accuracy 86.973%\n",
      "Epoch 25, Batch 182, LR 0.000033 Loss 5.149997, Accuracy 86.985%\n",
      "Epoch 25, Batch 183, LR 0.000033 Loss 5.151190, Accuracy 86.979%\n",
      "Epoch 25, Batch 184, LR 0.000033 Loss 5.154496, Accuracy 86.969%\n",
      "Epoch 25, Batch 185, LR 0.000033 Loss 5.156452, Accuracy 86.968%\n",
      "Epoch 25, Batch 186, LR 0.000033 Loss 5.154778, Accuracy 86.967%\n",
      "Epoch 25, Batch 187, LR 0.000033 Loss 5.153815, Accuracy 86.965%\n",
      "Epoch 25, Batch 188, LR 0.000033 Loss 5.150999, Accuracy 86.972%\n",
      "Epoch 25, Batch 189, LR 0.000033 Loss 5.148643, Accuracy 86.983%\n",
      "Epoch 25, Batch 190, LR 0.000033 Loss 5.146935, Accuracy 86.994%\n",
      "Epoch 25, Batch 191, LR 0.000033 Loss 5.145227, Accuracy 86.997%\n",
      "Epoch 25, Batch 192, LR 0.000033 Loss 5.146578, Accuracy 86.979%\n",
      "Epoch 25, Batch 193, LR 0.000033 Loss 5.149530, Accuracy 86.949%\n",
      "Epoch 25, Batch 194, LR 0.000033 Loss 5.146731, Accuracy 86.964%\n",
      "Epoch 25, Batch 195, LR 0.000033 Loss 5.148416, Accuracy 86.959%\n",
      "Epoch 25, Batch 196, LR 0.000033 Loss 5.150716, Accuracy 86.922%\n",
      "Epoch 25, Batch 197, LR 0.000033 Loss 5.153143, Accuracy 86.917%\n",
      "Epoch 25, Batch 198, LR 0.000033 Loss 5.153087, Accuracy 86.920%\n",
      "Epoch 25, Batch 199, LR 0.000033 Loss 5.155593, Accuracy 86.911%\n",
      "Epoch 25, Batch 200, LR 0.000033 Loss 5.156720, Accuracy 86.910%\n",
      "Epoch 25, Batch 201, LR 0.000033 Loss 5.155895, Accuracy 86.913%\n",
      "Epoch 25, Batch 202, LR 0.000033 Loss 5.153371, Accuracy 86.931%\n",
      "Epoch 25, Batch 203, LR 0.000033 Loss 5.150348, Accuracy 86.950%\n",
      "Epoch 25, Batch 204, LR 0.000033 Loss 5.146668, Accuracy 86.972%\n",
      "Epoch 25, Batch 205, LR 0.000033 Loss 5.142495, Accuracy 86.993%\n",
      "Epoch 25, Batch 206, LR 0.000033 Loss 5.140332, Accuracy 86.999%\n",
      "Epoch 25, Batch 207, LR 0.000033 Loss 5.140247, Accuracy 87.013%\n",
      "Epoch 25, Batch 208, LR 0.000033 Loss 5.141322, Accuracy 87.000%\n",
      "Epoch 25, Batch 209, LR 0.000033 Loss 5.143004, Accuracy 86.995%\n",
      "Epoch 25, Batch 210, LR 0.000033 Loss 5.144543, Accuracy 86.994%\n",
      "Epoch 25, Batch 211, LR 0.000033 Loss 5.148026, Accuracy 86.974%\n",
      "Epoch 25, Batch 212, LR 0.000033 Loss 5.147868, Accuracy 86.969%\n",
      "Epoch 25, Batch 213, LR 0.000033 Loss 5.146845, Accuracy 86.964%\n",
      "Epoch 25, Batch 214, LR 0.000033 Loss 5.146361, Accuracy 86.974%\n",
      "Epoch 25, Batch 215, LR 0.000033 Loss 5.145557, Accuracy 86.977%\n",
      "Epoch 25, Batch 216, LR 0.000033 Loss 5.146144, Accuracy 86.983%\n",
      "Epoch 25, Batch 217, LR 0.000033 Loss 5.147269, Accuracy 86.971%\n",
      "Epoch 25, Batch 218, LR 0.000033 Loss 5.152525, Accuracy 86.923%\n",
      "Epoch 25, Batch 219, LR 0.000033 Loss 5.150224, Accuracy 86.936%\n",
      "Epoch 25, Batch 220, LR 0.000033 Loss 5.150672, Accuracy 86.925%\n",
      "Epoch 25, Batch 221, LR 0.000033 Loss 5.150541, Accuracy 86.917%\n",
      "Epoch 25, Batch 222, LR 0.000033 Loss 5.146646, Accuracy 86.926%\n",
      "Epoch 25, Batch 223, LR 0.000033 Loss 5.145981, Accuracy 86.929%\n",
      "Epoch 25, Batch 224, LR 0.000033 Loss 5.143644, Accuracy 86.935%\n",
      "Epoch 25, Batch 225, LR 0.000033 Loss 5.145301, Accuracy 86.920%\n",
      "Epoch 25, Batch 226, LR 0.000033 Loss 5.144457, Accuracy 86.919%\n",
      "Epoch 25, Batch 227, LR 0.000033 Loss 5.141703, Accuracy 86.925%\n",
      "Epoch 25, Batch 228, LR 0.000033 Loss 5.142246, Accuracy 86.928%\n",
      "Epoch 25, Batch 229, LR 0.000033 Loss 5.143909, Accuracy 86.913%\n",
      "Epoch 25, Batch 230, LR 0.000033 Loss 5.145780, Accuracy 86.892%\n",
      "Epoch 25, Batch 231, LR 0.000033 Loss 5.147021, Accuracy 86.898%\n",
      "Epoch 25, Batch 232, LR 0.000033 Loss 5.145926, Accuracy 86.904%\n",
      "Epoch 25, Batch 233, LR 0.000033 Loss 5.146012, Accuracy 86.907%\n",
      "Epoch 25, Batch 234, LR 0.000033 Loss 5.146772, Accuracy 86.912%\n",
      "Epoch 25, Batch 235, LR 0.000033 Loss 5.143437, Accuracy 86.932%\n",
      "Epoch 25, Batch 236, LR 0.000033 Loss 5.143992, Accuracy 86.927%\n",
      "Epoch 25, Batch 237, LR 0.000033 Loss 5.144163, Accuracy 86.920%\n",
      "Epoch 25, Batch 238, LR 0.000033 Loss 5.141912, Accuracy 86.926%\n",
      "Epoch 25, Batch 239, LR 0.000033 Loss 5.143409, Accuracy 86.908%\n",
      "Epoch 25, Batch 240, LR 0.000033 Loss 5.142620, Accuracy 86.917%\n",
      "Epoch 25, Batch 241, LR 0.000033 Loss 5.143235, Accuracy 86.910%\n",
      "Epoch 25, Batch 242, LR 0.000033 Loss 5.145998, Accuracy 86.900%\n",
      "Epoch 25, Batch 243, LR 0.000033 Loss 5.147291, Accuracy 86.896%\n",
      "Epoch 25, Batch 244, LR 0.000033 Loss 5.147422, Accuracy 86.895%\n",
      "Epoch 25, Batch 245, LR 0.000033 Loss 5.148379, Accuracy 86.888%\n",
      "Epoch 25, Batch 246, LR 0.000033 Loss 5.148071, Accuracy 86.881%\n",
      "Epoch 25, Batch 247, LR 0.000033 Loss 5.150295, Accuracy 86.871%\n",
      "Epoch 25, Batch 248, LR 0.000033 Loss 5.149153, Accuracy 86.879%\n",
      "Epoch 25, Batch 249, LR 0.000033 Loss 5.150581, Accuracy 86.860%\n",
      "Epoch 25, Batch 250, LR 0.000033 Loss 5.149206, Accuracy 86.853%\n",
      "Epoch 25, Batch 251, LR 0.000033 Loss 5.149030, Accuracy 86.859%\n",
      "Epoch 25, Batch 252, LR 0.000033 Loss 5.149024, Accuracy 86.864%\n",
      "Epoch 25, Batch 253, LR 0.000033 Loss 5.149634, Accuracy 86.861%\n",
      "Epoch 25, Batch 254, LR 0.000033 Loss 5.151700, Accuracy 86.857%\n",
      "Epoch 25, Batch 255, LR 0.000033 Loss 5.151622, Accuracy 86.872%\n",
      "Epoch 25, Batch 256, LR 0.000033 Loss 5.151030, Accuracy 86.880%\n",
      "Epoch 25, Batch 257, LR 0.000033 Loss 5.149324, Accuracy 86.886%\n",
      "Epoch 25, Batch 258, LR 0.000033 Loss 5.148574, Accuracy 86.876%\n",
      "Epoch 25, Batch 259, LR 0.000033 Loss 5.147962, Accuracy 86.879%\n",
      "Epoch 25, Batch 260, LR 0.000033 Loss 5.143826, Accuracy 86.893%\n",
      "Epoch 25, Batch 261, LR 0.000033 Loss 5.143653, Accuracy 86.904%\n",
      "Epoch 25, Batch 262, LR 0.000033 Loss 5.143739, Accuracy 86.907%\n",
      "Epoch 25, Batch 263, LR 0.000033 Loss 5.141817, Accuracy 86.909%\n",
      "Epoch 25, Batch 264, LR 0.000033 Loss 5.145765, Accuracy 86.896%\n",
      "Epoch 25, Batch 265, LR 0.000033 Loss 5.142786, Accuracy 86.916%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 266, LR 0.000033 Loss 5.144311, Accuracy 86.907%\n",
      "Epoch 25, Batch 267, LR 0.000033 Loss 5.144554, Accuracy 86.903%\n",
      "Epoch 25, Batch 268, LR 0.000033 Loss 5.144178, Accuracy 86.911%\n",
      "Epoch 25, Batch 269, LR 0.000033 Loss 5.145304, Accuracy 86.902%\n",
      "Epoch 25, Batch 270, LR 0.000033 Loss 5.146410, Accuracy 86.907%\n",
      "Epoch 25, Batch 271, LR 0.000033 Loss 5.144545, Accuracy 86.932%\n",
      "Epoch 25, Batch 272, LR 0.000033 Loss 5.146011, Accuracy 86.920%\n",
      "Epoch 25, Batch 273, LR 0.000033 Loss 5.146730, Accuracy 86.913%\n",
      "Epoch 25, Batch 274, LR 0.000033 Loss 5.146146, Accuracy 86.910%\n",
      "Epoch 25, Batch 275, LR 0.000033 Loss 5.144381, Accuracy 86.912%\n",
      "Epoch 25, Batch 276, LR 0.000033 Loss 5.146877, Accuracy 86.900%\n",
      "Epoch 25, Batch 277, LR 0.000033 Loss 5.147845, Accuracy 86.902%\n",
      "Epoch 25, Batch 278, LR 0.000033 Loss 5.147263, Accuracy 86.899%\n",
      "Epoch 25, Batch 279, LR 0.000033 Loss 5.147455, Accuracy 86.915%\n",
      "Epoch 25, Batch 280, LR 0.000033 Loss 5.147538, Accuracy 86.914%\n",
      "Epoch 25, Batch 281, LR 0.000033 Loss 5.144452, Accuracy 86.930%\n",
      "Epoch 25, Batch 282, LR 0.000033 Loss 5.147093, Accuracy 86.913%\n",
      "Epoch 25, Batch 283, LR 0.000033 Loss 5.151166, Accuracy 86.901%\n",
      "Epoch 25, Batch 284, LR 0.000033 Loss 5.150786, Accuracy 86.898%\n",
      "Epoch 25, Batch 285, LR 0.000033 Loss 5.150872, Accuracy 86.900%\n",
      "Epoch 25, Batch 286, LR 0.000033 Loss 5.151619, Accuracy 86.907%\n",
      "Epoch 25, Batch 287, LR 0.000033 Loss 5.152159, Accuracy 86.893%\n",
      "Epoch 25, Batch 288, LR 0.000033 Loss 5.152449, Accuracy 86.892%\n",
      "Epoch 25, Batch 289, LR 0.000033 Loss 5.151054, Accuracy 86.900%\n",
      "Epoch 25, Batch 290, LR 0.000033 Loss 5.148707, Accuracy 86.902%\n",
      "Epoch 25, Batch 291, LR 0.000033 Loss 5.147891, Accuracy 86.907%\n",
      "Epoch 25, Batch 292, LR 0.000033 Loss 5.146683, Accuracy 86.901%\n",
      "Epoch 25, Batch 293, LR 0.000033 Loss 5.144017, Accuracy 86.911%\n",
      "Epoch 25, Batch 294, LR 0.000033 Loss 5.143831, Accuracy 86.918%\n",
      "Epoch 25, Batch 295, LR 0.000033 Loss 5.143704, Accuracy 86.917%\n",
      "Epoch 25, Batch 296, LR 0.000033 Loss 5.144435, Accuracy 86.914%\n",
      "Epoch 25, Batch 297, LR 0.000033 Loss 5.145761, Accuracy 86.913%\n",
      "Epoch 25, Batch 298, LR 0.000033 Loss 5.144695, Accuracy 86.926%\n",
      "Epoch 25, Batch 299, LR 0.000033 Loss 5.144176, Accuracy 86.928%\n",
      "Epoch 25, Batch 300, LR 0.000033 Loss 5.144532, Accuracy 86.927%\n",
      "Epoch 25, Batch 301, LR 0.000033 Loss 5.143814, Accuracy 86.924%\n",
      "Epoch 25, Batch 302, LR 0.000033 Loss 5.141428, Accuracy 86.939%\n",
      "Epoch 25, Batch 303, LR 0.000033 Loss 5.141703, Accuracy 86.943%\n",
      "Epoch 25, Batch 304, LR 0.000033 Loss 5.141800, Accuracy 86.940%\n",
      "Epoch 25, Batch 305, LR 0.000033 Loss 5.140063, Accuracy 86.947%\n",
      "Epoch 25, Batch 306, LR 0.000033 Loss 5.139730, Accuracy 86.946%\n",
      "Epoch 25, Batch 307, LR 0.000033 Loss 5.141029, Accuracy 86.945%\n",
      "Epoch 25, Batch 308, LR 0.000033 Loss 5.141301, Accuracy 86.950%\n",
      "Epoch 25, Batch 309, LR 0.000033 Loss 5.142710, Accuracy 86.941%\n",
      "Epoch 25, Batch 310, LR 0.000033 Loss 5.141496, Accuracy 86.946%\n",
      "Epoch 25, Batch 311, LR 0.000033 Loss 5.142054, Accuracy 86.952%\n",
      "Epoch 25, Batch 312, LR 0.000033 Loss 5.139136, Accuracy 86.972%\n",
      "Epoch 25, Batch 313, LR 0.000033 Loss 5.140914, Accuracy 86.956%\n",
      "Epoch 25, Batch 314, LR 0.000033 Loss 5.141815, Accuracy 86.955%\n",
      "Epoch 25, Batch 315, LR 0.000033 Loss 5.143286, Accuracy 86.952%\n",
      "Epoch 25, Batch 316, LR 0.000033 Loss 5.141630, Accuracy 86.956%\n",
      "Epoch 25, Batch 317, LR 0.000033 Loss 5.141956, Accuracy 86.958%\n",
      "Epoch 25, Batch 318, LR 0.000033 Loss 5.141819, Accuracy 86.964%\n",
      "Epoch 25, Batch 319, LR 0.000033 Loss 5.142123, Accuracy 86.959%\n",
      "Epoch 25, Batch 320, LR 0.000033 Loss 5.145697, Accuracy 86.943%\n",
      "Epoch 25, Batch 321, LR 0.000033 Loss 5.147292, Accuracy 86.930%\n",
      "Epoch 25, Batch 322, LR 0.000033 Loss 5.147080, Accuracy 86.932%\n",
      "Epoch 25, Batch 323, LR 0.000033 Loss 5.146757, Accuracy 86.936%\n",
      "Epoch 25, Batch 324, LR 0.000033 Loss 5.148708, Accuracy 86.914%\n",
      "Epoch 25, Batch 325, LR 0.000033 Loss 5.147733, Accuracy 86.909%\n",
      "Epoch 25, Batch 326, LR 0.000033 Loss 5.147036, Accuracy 86.906%\n",
      "Epoch 25, Batch 327, LR 0.000033 Loss 5.146001, Accuracy 86.905%\n",
      "Epoch 25, Batch 328, LR 0.000033 Loss 5.145204, Accuracy 86.916%\n",
      "Epoch 25, Batch 329, LR 0.000033 Loss 5.146222, Accuracy 86.918%\n",
      "Epoch 25, Batch 330, LR 0.000033 Loss 5.147025, Accuracy 86.918%\n",
      "Epoch 25, Batch 331, LR 0.000033 Loss 5.146677, Accuracy 86.922%\n",
      "Epoch 25, Batch 332, LR 0.000033 Loss 5.147229, Accuracy 86.928%\n",
      "Epoch 25, Batch 333, LR 0.000033 Loss 5.146234, Accuracy 86.925%\n",
      "Epoch 25, Batch 334, LR 0.000033 Loss 5.144895, Accuracy 86.936%\n",
      "Epoch 25, Batch 335, LR 0.000033 Loss 5.147315, Accuracy 86.938%\n",
      "Epoch 25, Batch 336, LR 0.000033 Loss 5.145380, Accuracy 86.949%\n",
      "Epoch 25, Batch 337, LR 0.000033 Loss 5.145044, Accuracy 86.934%\n",
      "Epoch 25, Batch 338, LR 0.000033 Loss 5.145307, Accuracy 86.941%\n",
      "Epoch 25, Batch 339, LR 0.000033 Loss 5.146493, Accuracy 86.947%\n",
      "Epoch 25, Batch 340, LR 0.000033 Loss 5.144798, Accuracy 86.958%\n",
      "Epoch 25, Batch 341, LR 0.000032 Loss 5.145576, Accuracy 86.952%\n",
      "Epoch 25, Batch 342, LR 0.000032 Loss 5.145564, Accuracy 86.959%\n",
      "Epoch 25, Batch 343, LR 0.000032 Loss 5.146040, Accuracy 86.951%\n",
      "Epoch 25, Batch 344, LR 0.000032 Loss 5.143665, Accuracy 86.957%\n",
      "Epoch 25, Batch 345, LR 0.000032 Loss 5.145149, Accuracy 86.943%\n",
      "Epoch 25, Batch 346, LR 0.000032 Loss 5.144194, Accuracy 86.945%\n",
      "Epoch 25, Batch 347, LR 0.000032 Loss 5.145967, Accuracy 86.944%\n",
      "Epoch 25, Batch 348, LR 0.000032 Loss 5.144691, Accuracy 86.945%\n",
      "Epoch 25, Batch 349, LR 0.000032 Loss 5.143241, Accuracy 86.954%\n",
      "Epoch 25, Batch 350, LR 0.000032 Loss 5.141077, Accuracy 86.964%\n",
      "Epoch 25, Batch 351, LR 0.000032 Loss 5.139298, Accuracy 86.970%\n",
      "Epoch 25, Batch 352, LR 0.000032 Loss 5.139631, Accuracy 86.974%\n",
      "Epoch 25, Batch 353, LR 0.000032 Loss 5.138760, Accuracy 86.978%\n",
      "Epoch 25, Batch 354, LR 0.000032 Loss 5.138091, Accuracy 86.970%\n",
      "Epoch 25, Batch 355, LR 0.000032 Loss 5.138250, Accuracy 86.961%\n",
      "Epoch 25, Batch 356, LR 0.000032 Loss 5.138528, Accuracy 86.960%\n",
      "Epoch 25, Batch 357, LR 0.000032 Loss 5.137873, Accuracy 86.964%\n",
      "Epoch 25, Batch 358, LR 0.000032 Loss 5.138548, Accuracy 86.952%\n",
      "Epoch 25, Batch 359, LR 0.000032 Loss 5.138278, Accuracy 86.954%\n",
      "Epoch 25, Batch 360, LR 0.000032 Loss 5.138977, Accuracy 86.957%\n",
      "Epoch 25, Batch 361, LR 0.000032 Loss 5.137247, Accuracy 86.965%\n",
      "Epoch 25, Batch 362, LR 0.000032 Loss 5.135645, Accuracy 86.973%\n",
      "Epoch 25, Batch 363, LR 0.000032 Loss 5.133970, Accuracy 86.973%\n",
      "Epoch 25, Batch 364, LR 0.000032 Loss 5.133786, Accuracy 86.981%\n",
      "Epoch 25, Batch 365, LR 0.000032 Loss 5.132857, Accuracy 86.982%\n",
      "Epoch 25, Batch 366, LR 0.000032 Loss 5.133082, Accuracy 86.988%\n",
      "Epoch 25, Batch 367, LR 0.000032 Loss 5.133156, Accuracy 86.983%\n",
      "Epoch 25, Batch 368, LR 0.000032 Loss 5.132498, Accuracy 86.982%\n",
      "Epoch 25, Batch 369, LR 0.000032 Loss 5.131247, Accuracy 86.983%\n",
      "Epoch 25, Batch 370, LR 0.000032 Loss 5.130511, Accuracy 86.983%\n",
      "Epoch 25, Batch 371, LR 0.000032 Loss 5.130672, Accuracy 86.982%\n",
      "Epoch 25, Batch 372, LR 0.000032 Loss 5.130469, Accuracy 86.979%\n",
      "Epoch 25, Batch 373, LR 0.000032 Loss 5.133568, Accuracy 86.962%\n",
      "Epoch 25, Batch 374, LR 0.000032 Loss 5.132881, Accuracy 86.963%\n",
      "Epoch 25, Batch 375, LR 0.000032 Loss 5.132817, Accuracy 86.967%\n",
      "Epoch 25, Batch 376, LR 0.000032 Loss 5.134330, Accuracy 86.949%\n",
      "Epoch 25, Batch 377, LR 0.000032 Loss 5.133158, Accuracy 86.955%\n",
      "Epoch 25, Batch 378, LR 0.000032 Loss 5.131712, Accuracy 86.967%\n",
      "Epoch 25, Batch 379, LR 0.000032 Loss 5.133860, Accuracy 86.954%\n",
      "Epoch 25, Batch 380, LR 0.000032 Loss 5.136984, Accuracy 86.943%\n",
      "Epoch 25, Batch 381, LR 0.000032 Loss 5.135527, Accuracy 86.942%\n",
      "Epoch 25, Batch 382, LR 0.000032 Loss 5.135978, Accuracy 86.942%\n",
      "Epoch 25, Batch 383, LR 0.000032 Loss 5.135795, Accuracy 86.949%\n",
      "Epoch 25, Batch 384, LR 0.000032 Loss 5.134937, Accuracy 86.951%\n",
      "Epoch 25, Batch 385, LR 0.000032 Loss 5.134259, Accuracy 86.952%\n",
      "Epoch 25, Batch 386, LR 0.000032 Loss 5.135099, Accuracy 86.939%\n",
      "Epoch 25, Batch 387, LR 0.000032 Loss 5.134970, Accuracy 86.937%\n",
      "Epoch 25, Batch 388, LR 0.000032 Loss 5.135298, Accuracy 86.926%\n",
      "Epoch 25, Batch 389, LR 0.000032 Loss 5.134787, Accuracy 86.930%\n",
      "Epoch 25, Batch 390, LR 0.000032 Loss 5.135699, Accuracy 86.925%\n",
      "Epoch 25, Batch 391, LR 0.000032 Loss 5.134262, Accuracy 86.939%\n",
      "Epoch 25, Batch 392, LR 0.000032 Loss 5.132960, Accuracy 86.934%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 393, LR 0.000032 Loss 5.132610, Accuracy 86.941%\n",
      "Epoch 25, Batch 394, LR 0.000032 Loss 5.132518, Accuracy 86.939%\n",
      "Epoch 25, Batch 395, LR 0.000032 Loss 5.132148, Accuracy 86.942%\n",
      "Epoch 25, Batch 396, LR 0.000032 Loss 5.131399, Accuracy 86.944%\n",
      "Epoch 25, Batch 397, LR 0.000032 Loss 5.130287, Accuracy 86.945%\n",
      "Epoch 25, Batch 398, LR 0.000032 Loss 5.130187, Accuracy 86.956%\n",
      "Epoch 25, Batch 399, LR 0.000032 Loss 5.128695, Accuracy 86.977%\n",
      "Epoch 25, Batch 400, LR 0.000032 Loss 5.130760, Accuracy 86.971%\n",
      "Epoch 25, Batch 401, LR 0.000032 Loss 5.130958, Accuracy 86.960%\n",
      "Epoch 25, Batch 402, LR 0.000032 Loss 5.130078, Accuracy 86.952%\n",
      "Epoch 25, Batch 403, LR 0.000032 Loss 5.129715, Accuracy 86.957%\n",
      "Epoch 25, Batch 404, LR 0.000032 Loss 5.127922, Accuracy 86.960%\n",
      "Epoch 25, Batch 405, LR 0.000032 Loss 5.127702, Accuracy 86.960%\n",
      "Epoch 25, Batch 406, LR 0.000032 Loss 5.129933, Accuracy 86.948%\n",
      "Epoch 25, Batch 407, LR 0.000032 Loss 5.129064, Accuracy 86.953%\n",
      "Epoch 25, Batch 408, LR 0.000032 Loss 5.128214, Accuracy 86.954%\n",
      "Epoch 25, Batch 409, LR 0.000032 Loss 5.128198, Accuracy 86.956%\n",
      "Epoch 25, Batch 410, LR 0.000032 Loss 5.126562, Accuracy 86.953%\n",
      "Epoch 25, Batch 411, LR 0.000032 Loss 5.128104, Accuracy 86.949%\n",
      "Epoch 25, Batch 412, LR 0.000032 Loss 5.127143, Accuracy 86.944%\n",
      "Epoch 25, Batch 413, LR 0.000032 Loss 5.127207, Accuracy 86.950%\n",
      "Epoch 25, Batch 414, LR 0.000032 Loss 5.126039, Accuracy 86.953%\n",
      "Epoch 25, Batch 415, LR 0.000032 Loss 5.123692, Accuracy 86.960%\n",
      "Epoch 25, Batch 416, LR 0.000032 Loss 5.123351, Accuracy 86.965%\n",
      "Epoch 25, Batch 417, LR 0.000032 Loss 5.121743, Accuracy 86.962%\n",
      "Epoch 25, Batch 418, LR 0.000032 Loss 5.121024, Accuracy 86.965%\n",
      "Epoch 25, Batch 419, LR 0.000032 Loss 5.119129, Accuracy 86.978%\n",
      "Epoch 25, Batch 420, LR 0.000032 Loss 5.117860, Accuracy 86.983%\n",
      "Epoch 25, Batch 421, LR 0.000032 Loss 5.119300, Accuracy 86.975%\n",
      "Epoch 25, Batch 422, LR 0.000032 Loss 5.119719, Accuracy 86.971%\n",
      "Epoch 25, Batch 423, LR 0.000032 Loss 5.118006, Accuracy 86.975%\n",
      "Epoch 25, Batch 424, LR 0.000032 Loss 5.118035, Accuracy 86.979%\n",
      "Epoch 25, Batch 425, LR 0.000032 Loss 5.119286, Accuracy 86.982%\n",
      "Epoch 25, Batch 426, LR 0.000032 Loss 5.119891, Accuracy 86.979%\n",
      "Epoch 25, Batch 427, LR 0.000032 Loss 5.119169, Accuracy 86.990%\n",
      "Epoch 25, Batch 428, LR 0.000032 Loss 5.118005, Accuracy 86.998%\n",
      "Epoch 25, Batch 429, LR 0.000032 Loss 5.117646, Accuracy 86.999%\n",
      "Epoch 25, Batch 430, LR 0.000032 Loss 5.117417, Accuracy 87.000%\n",
      "Epoch 25, Batch 431, LR 0.000032 Loss 5.116187, Accuracy 87.005%\n",
      "Epoch 25, Batch 432, LR 0.000032 Loss 5.116331, Accuracy 87.004%\n",
      "Epoch 25, Batch 433, LR 0.000032 Loss 5.115983, Accuracy 87.016%\n",
      "Epoch 25, Batch 434, LR 0.000032 Loss 5.115359, Accuracy 87.014%\n",
      "Epoch 25, Batch 435, LR 0.000032 Loss 5.115229, Accuracy 87.015%\n",
      "Epoch 25, Batch 436, LR 0.000032 Loss 5.115698, Accuracy 87.002%\n",
      "Epoch 25, Batch 437, LR 0.000032 Loss 5.117448, Accuracy 86.998%\n",
      "Epoch 25, Batch 438, LR 0.000032 Loss 5.117709, Accuracy 87.006%\n",
      "Epoch 25, Batch 439, LR 0.000032 Loss 5.118284, Accuracy 87.000%\n",
      "Epoch 25, Batch 440, LR 0.000032 Loss 5.118503, Accuracy 87.010%\n",
      "Epoch 25, Batch 441, LR 0.000032 Loss 5.119909, Accuracy 87.002%\n",
      "Epoch 25, Batch 442, LR 0.000032 Loss 5.125276, Accuracy 86.984%\n",
      "Epoch 25, Batch 443, LR 0.000032 Loss 5.124725, Accuracy 86.990%\n",
      "Epoch 25, Batch 444, LR 0.000032 Loss 5.124227, Accuracy 86.991%\n",
      "Epoch 25, Batch 445, LR 0.000032 Loss 5.123315, Accuracy 86.991%\n",
      "Epoch 25, Batch 446, LR 0.000032 Loss 5.123395, Accuracy 86.989%\n",
      "Epoch 25, Batch 447, LR 0.000032 Loss 5.123301, Accuracy 86.993%\n",
      "Epoch 25, Batch 448, LR 0.000032 Loss 5.121995, Accuracy 87.006%\n",
      "Epoch 25, Batch 449, LR 0.000032 Loss 5.122476, Accuracy 87.008%\n",
      "Epoch 25, Batch 450, LR 0.000032 Loss 5.121549, Accuracy 87.016%\n",
      "Epoch 25, Batch 451, LR 0.000032 Loss 5.123412, Accuracy 87.005%\n",
      "Epoch 25, Batch 452, LR 0.000032 Loss 5.123323, Accuracy 87.002%\n",
      "Epoch 25, Batch 453, LR 0.000032 Loss 5.123400, Accuracy 87.008%\n",
      "Epoch 25, Batch 454, LR 0.000032 Loss 5.124488, Accuracy 87.003%\n",
      "Epoch 25, Batch 455, LR 0.000032 Loss 5.126364, Accuracy 86.999%\n",
      "Epoch 25, Batch 456, LR 0.000032 Loss 5.127027, Accuracy 87.003%\n",
      "Epoch 25, Batch 457, LR 0.000032 Loss 5.127460, Accuracy 86.999%\n",
      "Epoch 25, Batch 458, LR 0.000032 Loss 5.127111, Accuracy 87.004%\n",
      "Epoch 25, Batch 459, LR 0.000032 Loss 5.125005, Accuracy 87.008%\n",
      "Epoch 25, Batch 460, LR 0.000032 Loss 5.125633, Accuracy 87.011%\n",
      "Epoch 25, Batch 461, LR 0.000032 Loss 5.124382, Accuracy 87.020%\n",
      "Epoch 25, Batch 462, LR 0.000032 Loss 5.124556, Accuracy 87.020%\n",
      "Epoch 25, Batch 463, LR 0.000032 Loss 5.124878, Accuracy 87.016%\n",
      "Epoch 25, Batch 464, LR 0.000032 Loss 5.123687, Accuracy 87.020%\n",
      "Epoch 25, Batch 465, LR 0.000032 Loss 5.122440, Accuracy 87.019%\n",
      "Epoch 25, Batch 466, LR 0.000032 Loss 5.121563, Accuracy 87.024%\n",
      "Epoch 25, Batch 467, LR 0.000032 Loss 5.120048, Accuracy 87.032%\n",
      "Epoch 25, Batch 468, LR 0.000032 Loss 5.119992, Accuracy 87.029%\n",
      "Epoch 25, Batch 469, LR 0.000032 Loss 5.120931, Accuracy 87.027%\n",
      "Epoch 25, Batch 470, LR 0.000032 Loss 5.121167, Accuracy 87.031%\n",
      "Epoch 25, Batch 471, LR 0.000032 Loss 5.121404, Accuracy 87.026%\n",
      "Epoch 25, Batch 472, LR 0.000032 Loss 5.121371, Accuracy 87.025%\n",
      "Epoch 25, Batch 473, LR 0.000032 Loss 5.120723, Accuracy 87.019%\n",
      "Epoch 25, Batch 474, LR 0.000032 Loss 5.119811, Accuracy 87.029%\n",
      "Epoch 25, Batch 475, LR 0.000032 Loss 5.120252, Accuracy 87.026%\n",
      "Epoch 25, Batch 476, LR 0.000032 Loss 5.120249, Accuracy 87.027%\n",
      "Epoch 25, Batch 477, LR 0.000032 Loss 5.119713, Accuracy 87.032%\n",
      "Epoch 25, Batch 478, LR 0.000032 Loss 5.119502, Accuracy 87.036%\n",
      "Epoch 25, Batch 479, LR 0.000032 Loss 5.120326, Accuracy 87.029%\n",
      "Epoch 25, Batch 480, LR 0.000032 Loss 5.120601, Accuracy 87.033%\n",
      "Epoch 25, Batch 481, LR 0.000032 Loss 5.119915, Accuracy 87.045%\n",
      "Epoch 25, Batch 482, LR 0.000032 Loss 5.119745, Accuracy 87.054%\n",
      "Epoch 25, Batch 483, LR 0.000032 Loss 5.119062, Accuracy 87.058%\n",
      "Epoch 25, Batch 484, LR 0.000032 Loss 5.118130, Accuracy 87.064%\n",
      "Epoch 25, Batch 485, LR 0.000032 Loss 5.117985, Accuracy 87.068%\n",
      "Epoch 25, Batch 486, LR 0.000032 Loss 5.118544, Accuracy 87.061%\n",
      "Epoch 25, Batch 487, LR 0.000032 Loss 5.119037, Accuracy 87.057%\n",
      "Epoch 25, Batch 488, LR 0.000032 Loss 5.121679, Accuracy 87.045%\n",
      "Epoch 25, Batch 489, LR 0.000032 Loss 5.122019, Accuracy 87.038%\n",
      "Epoch 25, Batch 490, LR 0.000032 Loss 5.123035, Accuracy 87.034%\n",
      "Epoch 25, Batch 491, LR 0.000032 Loss 5.122778, Accuracy 87.034%\n",
      "Epoch 25, Batch 492, LR 0.000032 Loss 5.120554, Accuracy 87.043%\n",
      "Epoch 25, Batch 493, LR 0.000032 Loss 5.121212, Accuracy 87.039%\n",
      "Epoch 25, Batch 494, LR 0.000032 Loss 5.118944, Accuracy 87.057%\n",
      "Epoch 25, Batch 495, LR 0.000032 Loss 5.118075, Accuracy 87.060%\n",
      "Epoch 25, Batch 496, LR 0.000032 Loss 5.117251, Accuracy 87.072%\n",
      "Epoch 25, Batch 497, LR 0.000032 Loss 5.118114, Accuracy 87.077%\n",
      "Epoch 25, Batch 498, LR 0.000032 Loss 5.117634, Accuracy 87.084%\n",
      "Epoch 25, Batch 499, LR 0.000032 Loss 5.118467, Accuracy 87.082%\n",
      "Epoch 25, Batch 500, LR 0.000032 Loss 5.116877, Accuracy 87.089%\n",
      "Epoch 25, Batch 501, LR 0.000032 Loss 5.116404, Accuracy 87.077%\n",
      "Epoch 25, Batch 502, LR 0.000032 Loss 5.115341, Accuracy 87.083%\n",
      "Epoch 25, Batch 503, LR 0.000032 Loss 5.116276, Accuracy 87.078%\n",
      "Epoch 25, Batch 504, LR 0.000032 Loss 5.117176, Accuracy 87.069%\n",
      "Epoch 25, Batch 505, LR 0.000032 Loss 5.116256, Accuracy 87.073%\n",
      "Epoch 25, Batch 506, LR 0.000032 Loss 5.115072, Accuracy 87.075%\n",
      "Epoch 25, Batch 507, LR 0.000032 Loss 5.114763, Accuracy 87.070%\n",
      "Epoch 25, Batch 508, LR 0.000032 Loss 5.115209, Accuracy 87.066%\n",
      "Epoch 25, Batch 509, LR 0.000032 Loss 5.117098, Accuracy 87.056%\n",
      "Epoch 25, Batch 510, LR 0.000032 Loss 5.117032, Accuracy 87.060%\n",
      "Epoch 25, Batch 511, LR 0.000032 Loss 5.117290, Accuracy 87.061%\n",
      "Epoch 25, Batch 512, LR 0.000032 Loss 5.117432, Accuracy 87.059%\n",
      "Epoch 25, Batch 513, LR 0.000032 Loss 5.116031, Accuracy 87.071%\n",
      "Epoch 25, Batch 514, LR 0.000032 Loss 5.115661, Accuracy 87.076%\n",
      "Epoch 25, Batch 515, LR 0.000032 Loss 5.115968, Accuracy 87.074%\n",
      "Epoch 25, Batch 516, LR 0.000032 Loss 5.116846, Accuracy 87.075%\n",
      "Epoch 25, Batch 517, LR 0.000032 Loss 5.117966, Accuracy 87.074%\n",
      "Epoch 25, Batch 518, LR 0.000032 Loss 5.117947, Accuracy 87.070%\n",
      "Epoch 25, Batch 519, LR 0.000032 Loss 5.118234, Accuracy 87.066%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 520, LR 0.000032 Loss 5.119290, Accuracy 87.064%\n",
      "Epoch 25, Batch 521, LR 0.000032 Loss 5.118809, Accuracy 87.067%\n",
      "Epoch 25, Batch 522, LR 0.000032 Loss 5.117436, Accuracy 87.069%\n",
      "Epoch 25, Batch 523, LR 0.000032 Loss 5.116931, Accuracy 87.073%\n",
      "Epoch 25, Batch 524, LR 0.000032 Loss 5.117755, Accuracy 87.068%\n",
      "Epoch 25, Batch 525, LR 0.000032 Loss 5.118096, Accuracy 87.061%\n",
      "Epoch 25, Batch 526, LR 0.000032 Loss 5.116480, Accuracy 87.071%\n",
      "Epoch 25, Batch 527, LR 0.000032 Loss 5.116066, Accuracy 87.075%\n",
      "Epoch 25, Batch 528, LR 0.000032 Loss 5.115699, Accuracy 87.077%\n",
      "Epoch 25, Batch 529, LR 0.000032 Loss 5.116581, Accuracy 87.078%\n",
      "Epoch 25, Batch 530, LR 0.000032 Loss 5.115679, Accuracy 87.089%\n",
      "Epoch 25, Batch 531, LR 0.000032 Loss 5.116392, Accuracy 87.081%\n",
      "Epoch 25, Batch 532, LR 0.000032 Loss 5.116857, Accuracy 87.077%\n",
      "Epoch 25, Batch 533, LR 0.000032 Loss 5.115824, Accuracy 87.082%\n",
      "Epoch 25, Batch 534, LR 0.000032 Loss 5.116046, Accuracy 87.082%\n",
      "Epoch 25, Batch 535, LR 0.000032 Loss 5.115607, Accuracy 87.084%\n",
      "Epoch 25, Batch 536, LR 0.000032 Loss 5.113778, Accuracy 87.093%\n",
      "Epoch 25, Batch 537, LR 0.000032 Loss 5.115266, Accuracy 87.090%\n",
      "Epoch 25, Batch 538, LR 0.000032 Loss 5.114898, Accuracy 87.104%\n",
      "Epoch 25, Batch 539, LR 0.000032 Loss 5.113830, Accuracy 87.113%\n",
      "Epoch 25, Batch 540, LR 0.000032 Loss 5.113222, Accuracy 87.118%\n",
      "Epoch 25, Batch 541, LR 0.000032 Loss 5.113104, Accuracy 87.122%\n",
      "Epoch 25, Batch 542, LR 0.000032 Loss 5.113183, Accuracy 87.118%\n",
      "Epoch 25, Batch 543, LR 0.000032 Loss 5.112166, Accuracy 87.126%\n",
      "Epoch 25, Batch 544, LR 0.000032 Loss 5.111200, Accuracy 87.127%\n",
      "Epoch 25, Batch 545, LR 0.000032 Loss 5.112593, Accuracy 87.117%\n",
      "Epoch 25, Batch 546, LR 0.000032 Loss 5.112440, Accuracy 87.118%\n",
      "Epoch 25, Batch 547, LR 0.000032 Loss 5.112302, Accuracy 87.119%\n",
      "Epoch 25, Batch 548, LR 0.000032 Loss 5.113752, Accuracy 87.115%\n",
      "Epoch 25, Batch 549, LR 0.000032 Loss 5.114336, Accuracy 87.110%\n",
      "Epoch 25, Batch 550, LR 0.000032 Loss 5.111953, Accuracy 87.119%\n",
      "Epoch 25, Batch 551, LR 0.000032 Loss 5.111728, Accuracy 87.119%\n",
      "Epoch 25, Batch 552, LR 0.000032 Loss 5.112545, Accuracy 87.108%\n",
      "Epoch 25, Batch 553, LR 0.000032 Loss 5.111976, Accuracy 87.113%\n",
      "Epoch 25, Batch 554, LR 0.000032 Loss 5.110468, Accuracy 87.112%\n",
      "Epoch 25, Batch 555, LR 0.000032 Loss 5.109822, Accuracy 87.119%\n",
      "Epoch 25, Batch 556, LR 0.000032 Loss 5.108672, Accuracy 87.122%\n",
      "Epoch 25, Batch 557, LR 0.000032 Loss 5.108862, Accuracy 87.116%\n",
      "Epoch 25, Batch 558, LR 0.000032 Loss 5.108458, Accuracy 87.119%\n",
      "Epoch 25, Batch 559, LR 0.000032 Loss 5.110094, Accuracy 87.106%\n",
      "Epoch 25, Batch 560, LR 0.000032 Loss 5.109393, Accuracy 87.109%\n",
      "Epoch 25, Batch 561, LR 0.000032 Loss 5.111005, Accuracy 87.103%\n",
      "Epoch 25, Batch 562, LR 0.000032 Loss 5.111045, Accuracy 87.102%\n",
      "Epoch 25, Batch 563, LR 0.000032 Loss 5.110712, Accuracy 87.105%\n",
      "Epoch 25, Batch 564, LR 0.000032 Loss 5.109372, Accuracy 87.111%\n",
      "Epoch 25, Batch 565, LR 0.000032 Loss 5.110899, Accuracy 87.106%\n",
      "Epoch 25, Batch 566, LR 0.000032 Loss 5.111205, Accuracy 87.100%\n",
      "Epoch 25, Batch 567, LR 0.000032 Loss 5.111924, Accuracy 87.096%\n",
      "Epoch 25, Batch 568, LR 0.000032 Loss 5.111447, Accuracy 87.098%\n",
      "Epoch 25, Batch 569, LR 0.000032 Loss 5.111905, Accuracy 87.091%\n",
      "Epoch 25, Batch 570, LR 0.000032 Loss 5.112009, Accuracy 87.083%\n",
      "Epoch 25, Batch 571, LR 0.000032 Loss 5.111480, Accuracy 87.092%\n",
      "Epoch 25, Batch 572, LR 0.000032 Loss 5.111596, Accuracy 87.083%\n",
      "Epoch 25, Batch 573, LR 0.000032 Loss 5.111417, Accuracy 87.088%\n",
      "Epoch 25, Batch 574, LR 0.000032 Loss 5.110737, Accuracy 87.096%\n",
      "Epoch 25, Batch 575, LR 0.000032 Loss 5.109505, Accuracy 87.095%\n",
      "Epoch 25, Batch 576, LR 0.000032 Loss 5.109172, Accuracy 87.099%\n",
      "Epoch 25, Batch 577, LR 0.000032 Loss 5.109716, Accuracy 87.090%\n",
      "Epoch 25, Batch 578, LR 0.000032 Loss 5.109626, Accuracy 87.085%\n",
      "Epoch 25, Batch 579, LR 0.000032 Loss 5.109439, Accuracy 87.088%\n",
      "Epoch 25, Batch 580, LR 0.000032 Loss 5.109095, Accuracy 87.089%\n",
      "Epoch 25, Batch 581, LR 0.000032 Loss 5.108154, Accuracy 87.094%\n",
      "Epoch 25, Batch 582, LR 0.000032 Loss 5.107665, Accuracy 87.095%\n",
      "Epoch 25, Batch 583, LR 0.000032 Loss 5.107282, Accuracy 87.095%\n",
      "Epoch 25, Batch 584, LR 0.000032 Loss 5.108237, Accuracy 87.080%\n",
      "Epoch 25, Batch 585, LR 0.000032 Loss 5.107031, Accuracy 87.091%\n",
      "Epoch 25, Batch 586, LR 0.000032 Loss 5.108918, Accuracy 87.079%\n",
      "Epoch 25, Batch 587, LR 0.000032 Loss 5.108078, Accuracy 87.087%\n",
      "Epoch 25, Batch 588, LR 0.000032 Loss 5.108690, Accuracy 87.081%\n",
      "Epoch 25, Batch 589, LR 0.000032 Loss 5.107175, Accuracy 87.091%\n",
      "Epoch 25, Batch 590, LR 0.000032 Loss 5.107297, Accuracy 87.088%\n",
      "Epoch 25, Batch 591, LR 0.000032 Loss 5.107055, Accuracy 87.090%\n",
      "Epoch 25, Batch 592, LR 0.000032 Loss 5.107979, Accuracy 87.088%\n",
      "Epoch 25, Batch 593, LR 0.000032 Loss 5.108646, Accuracy 87.085%\n",
      "Epoch 25, Batch 594, LR 0.000032 Loss 5.110838, Accuracy 87.067%\n",
      "Epoch 25, Batch 595, LR 0.000032 Loss 5.110616, Accuracy 87.065%\n",
      "Epoch 25, Batch 596, LR 0.000032 Loss 5.110105, Accuracy 87.071%\n",
      "Epoch 25, Batch 597, LR 0.000032 Loss 5.108744, Accuracy 87.077%\n",
      "Epoch 25, Batch 598, LR 0.000032 Loss 5.110096, Accuracy 87.069%\n",
      "Epoch 25, Batch 599, LR 0.000032 Loss 5.111377, Accuracy 87.058%\n",
      "Epoch 25, Batch 600, LR 0.000032 Loss 5.111974, Accuracy 87.059%\n",
      "Epoch 25, Batch 601, LR 0.000032 Loss 5.111434, Accuracy 87.062%\n",
      "Epoch 25, Batch 602, LR 0.000032 Loss 5.112636, Accuracy 87.059%\n",
      "Epoch 25, Batch 603, LR 0.000032 Loss 5.112380, Accuracy 87.065%\n",
      "Epoch 25, Batch 604, LR 0.000032 Loss 5.112509, Accuracy 87.062%\n",
      "Epoch 25, Batch 605, LR 0.000032 Loss 5.112122, Accuracy 87.066%\n",
      "Epoch 25, Batch 606, LR 0.000032 Loss 5.112377, Accuracy 87.060%\n",
      "Epoch 25, Batch 607, LR 0.000032 Loss 5.112616, Accuracy 87.060%\n",
      "Epoch 25, Batch 608, LR 0.000032 Loss 5.113085, Accuracy 87.062%\n",
      "Epoch 25, Batch 609, LR 0.000032 Loss 5.112428, Accuracy 87.061%\n",
      "Epoch 25, Batch 610, LR 0.000032 Loss 5.112003, Accuracy 87.070%\n",
      "Epoch 25, Batch 611, LR 0.000032 Loss 5.112317, Accuracy 87.072%\n",
      "Epoch 25, Batch 612, LR 0.000032 Loss 5.112671, Accuracy 87.077%\n",
      "Epoch 25, Batch 613, LR 0.000031 Loss 5.111844, Accuracy 87.083%\n",
      "Epoch 25, Batch 614, LR 0.000031 Loss 5.112625, Accuracy 87.080%\n",
      "Epoch 25, Batch 615, LR 0.000031 Loss 5.112242, Accuracy 87.086%\n",
      "Epoch 25, Batch 616, LR 0.000031 Loss 5.113376, Accuracy 87.084%\n",
      "Epoch 25, Batch 617, LR 0.000031 Loss 5.114451, Accuracy 87.071%\n",
      "Epoch 25, Batch 618, LR 0.000031 Loss 5.114420, Accuracy 87.069%\n",
      "Epoch 25, Batch 619, LR 0.000031 Loss 5.113998, Accuracy 87.072%\n",
      "Epoch 25, Batch 620, LR 0.000031 Loss 5.114347, Accuracy 87.064%\n",
      "Epoch 25, Batch 621, LR 0.000031 Loss 5.114466, Accuracy 87.062%\n",
      "Epoch 25, Batch 622, LR 0.000031 Loss 5.114542, Accuracy 87.068%\n",
      "Epoch 25, Batch 623, LR 0.000031 Loss 5.114761, Accuracy 87.067%\n",
      "Epoch 25, Batch 624, LR 0.000031 Loss 5.115544, Accuracy 87.064%\n",
      "Epoch 25, Batch 625, LR 0.000031 Loss 5.115534, Accuracy 87.059%\n",
      "Epoch 25, Batch 626, LR 0.000031 Loss 5.115793, Accuracy 87.063%\n",
      "Epoch 25, Batch 627, LR 0.000031 Loss 5.114048, Accuracy 87.068%\n",
      "Epoch 25, Batch 628, LR 0.000031 Loss 5.114454, Accuracy 87.061%\n",
      "Epoch 25, Batch 629, LR 0.000031 Loss 5.114084, Accuracy 87.064%\n",
      "Epoch 25, Batch 630, LR 0.000031 Loss 5.113463, Accuracy 87.066%\n",
      "Epoch 25, Batch 631, LR 0.000031 Loss 5.112637, Accuracy 87.069%\n",
      "Epoch 25, Batch 632, LR 0.000031 Loss 5.111815, Accuracy 87.069%\n",
      "Epoch 25, Batch 633, LR 0.000031 Loss 5.112772, Accuracy 87.067%\n",
      "Epoch 25, Batch 634, LR 0.000031 Loss 5.111508, Accuracy 87.070%\n",
      "Epoch 25, Batch 635, LR 0.000031 Loss 5.111110, Accuracy 87.066%\n",
      "Epoch 25, Batch 636, LR 0.000031 Loss 5.109020, Accuracy 87.073%\n",
      "Epoch 25, Batch 637, LR 0.000031 Loss 5.107545, Accuracy 87.079%\n",
      "Epoch 25, Batch 638, LR 0.000031 Loss 5.107416, Accuracy 87.078%\n",
      "Epoch 25, Batch 639, LR 0.000031 Loss 5.107033, Accuracy 87.076%\n",
      "Epoch 25, Batch 640, LR 0.000031 Loss 5.106462, Accuracy 87.079%\n",
      "Epoch 25, Batch 641, LR 0.000031 Loss 5.105194, Accuracy 87.086%\n",
      "Epoch 25, Batch 642, LR 0.000031 Loss 5.105180, Accuracy 87.087%\n",
      "Epoch 25, Batch 643, LR 0.000031 Loss 5.106034, Accuracy 87.083%\n",
      "Epoch 25, Batch 644, LR 0.000031 Loss 5.105198, Accuracy 87.084%\n",
      "Epoch 25, Batch 645, LR 0.000031 Loss 5.105502, Accuracy 87.074%\n",
      "Epoch 25, Batch 646, LR 0.000031 Loss 5.106437, Accuracy 87.068%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 647, LR 0.000031 Loss 5.105420, Accuracy 87.069%\n",
      "Epoch 25, Batch 648, LR 0.000031 Loss 5.105031, Accuracy 87.074%\n",
      "Epoch 25, Batch 649, LR 0.000031 Loss 5.104398, Accuracy 87.079%\n",
      "Epoch 25, Batch 650, LR 0.000031 Loss 5.104838, Accuracy 87.075%\n",
      "Epoch 25, Batch 651, LR 0.000031 Loss 5.105066, Accuracy 87.072%\n",
      "Epoch 25, Batch 652, LR 0.000031 Loss 5.104932, Accuracy 87.070%\n",
      "Epoch 25, Batch 653, LR 0.000031 Loss 5.105314, Accuracy 87.074%\n",
      "Epoch 25, Batch 654, LR 0.000031 Loss 5.105927, Accuracy 87.069%\n",
      "Epoch 25, Batch 655, LR 0.000031 Loss 5.105967, Accuracy 87.067%\n",
      "Epoch 25, Batch 656, LR 0.000031 Loss 5.105970, Accuracy 87.065%\n",
      "Epoch 25, Batch 657, LR 0.000031 Loss 5.107230, Accuracy 87.060%\n",
      "Epoch 25, Batch 658, LR 0.000031 Loss 5.107043, Accuracy 87.063%\n",
      "Epoch 25, Batch 659, LR 0.000031 Loss 5.106105, Accuracy 87.065%\n",
      "Epoch 25, Batch 660, LR 0.000031 Loss 5.106029, Accuracy 87.067%\n",
      "Epoch 25, Batch 661, LR 0.000031 Loss 5.105731, Accuracy 87.067%\n",
      "Epoch 25, Batch 662, LR 0.000031 Loss 5.106044, Accuracy 87.073%\n",
      "Epoch 25, Batch 663, LR 0.000031 Loss 5.106585, Accuracy 87.066%\n",
      "Epoch 25, Batch 664, LR 0.000031 Loss 5.106552, Accuracy 87.065%\n",
      "Epoch 25, Batch 665, LR 0.000031 Loss 5.106608, Accuracy 87.062%\n",
      "Epoch 25, Batch 666, LR 0.000031 Loss 5.104894, Accuracy 87.067%\n",
      "Epoch 25, Batch 667, LR 0.000031 Loss 5.104900, Accuracy 87.063%\n",
      "Epoch 25, Batch 668, LR 0.000031 Loss 5.104531, Accuracy 87.064%\n",
      "Epoch 25, Batch 669, LR 0.000031 Loss 5.104035, Accuracy 87.063%\n",
      "Epoch 25, Batch 670, LR 0.000031 Loss 5.104674, Accuracy 87.059%\n",
      "Epoch 25, Batch 671, LR 0.000031 Loss 5.104596, Accuracy 87.060%\n",
      "Epoch 25, Batch 672, LR 0.000031 Loss 5.103846, Accuracy 87.062%\n",
      "Epoch 25, Batch 673, LR 0.000031 Loss 5.104719, Accuracy 87.052%\n",
      "Epoch 25, Batch 674, LR 0.000031 Loss 5.105914, Accuracy 87.050%\n",
      "Epoch 25, Batch 675, LR 0.000031 Loss 5.105634, Accuracy 87.050%\n",
      "Epoch 25, Batch 676, LR 0.000031 Loss 5.107734, Accuracy 87.034%\n",
      "Epoch 25, Batch 677, LR 0.000031 Loss 5.107895, Accuracy 87.036%\n",
      "Epoch 25, Batch 678, LR 0.000031 Loss 5.108243, Accuracy 87.033%\n",
      "Epoch 25, Batch 679, LR 0.000031 Loss 5.107447, Accuracy 87.039%\n",
      "Epoch 25, Batch 680, LR 0.000031 Loss 5.107613, Accuracy 87.035%\n",
      "Epoch 25, Batch 681, LR 0.000031 Loss 5.105920, Accuracy 87.045%\n",
      "Epoch 25, Batch 682, LR 0.000031 Loss 5.105707, Accuracy 87.046%\n",
      "Epoch 25, Batch 683, LR 0.000031 Loss 5.105771, Accuracy 87.042%\n",
      "Epoch 25, Batch 684, LR 0.000031 Loss 5.105979, Accuracy 87.035%\n",
      "Epoch 25, Batch 685, LR 0.000031 Loss 5.105846, Accuracy 87.032%\n",
      "Epoch 25, Batch 686, LR 0.000031 Loss 5.104825, Accuracy 87.036%\n",
      "Epoch 25, Batch 687, LR 0.000031 Loss 5.105017, Accuracy 87.039%\n",
      "Epoch 25, Batch 688, LR 0.000031 Loss 5.105390, Accuracy 87.041%\n",
      "Epoch 25, Batch 689, LR 0.000031 Loss 5.105218, Accuracy 87.042%\n",
      "Epoch 25, Batch 690, LR 0.000031 Loss 5.105911, Accuracy 87.036%\n",
      "Epoch 25, Batch 691, LR 0.000031 Loss 5.105816, Accuracy 87.035%\n",
      "Epoch 25, Batch 692, LR 0.000031 Loss 5.105915, Accuracy 87.033%\n",
      "Epoch 25, Batch 693, LR 0.000031 Loss 5.105763, Accuracy 87.032%\n",
      "Epoch 25, Batch 694, LR 0.000031 Loss 5.105804, Accuracy 87.033%\n",
      "Epoch 25, Batch 695, LR 0.000031 Loss 5.105285, Accuracy 87.040%\n",
      "Epoch 25, Batch 696, LR 0.000031 Loss 5.105575, Accuracy 87.036%\n",
      "Epoch 25, Batch 697, LR 0.000031 Loss 5.105439, Accuracy 87.037%\n",
      "Epoch 25, Batch 698, LR 0.000031 Loss 5.104835, Accuracy 87.038%\n",
      "Epoch 25, Batch 699, LR 0.000031 Loss 5.104130, Accuracy 87.042%\n",
      "Epoch 25, Batch 700, LR 0.000031 Loss 5.104301, Accuracy 87.044%\n",
      "Epoch 25, Batch 701, LR 0.000031 Loss 5.104726, Accuracy 87.041%\n",
      "Epoch 25, Batch 702, LR 0.000031 Loss 5.105567, Accuracy 87.031%\n",
      "Epoch 25, Batch 703, LR 0.000031 Loss 5.104967, Accuracy 87.034%\n",
      "Epoch 25, Batch 704, LR 0.000031 Loss 5.104465, Accuracy 87.038%\n",
      "Epoch 25, Batch 705, LR 0.000031 Loss 5.103877, Accuracy 87.043%\n",
      "Epoch 25, Batch 706, LR 0.000031 Loss 5.103039, Accuracy 87.046%\n",
      "Epoch 25, Batch 707, LR 0.000031 Loss 5.102815, Accuracy 87.046%\n",
      "Epoch 25, Batch 708, LR 0.000031 Loss 5.102668, Accuracy 87.053%\n",
      "Epoch 25, Batch 709, LR 0.000031 Loss 5.101811, Accuracy 87.060%\n",
      "Epoch 25, Batch 710, LR 0.000031 Loss 5.101141, Accuracy 87.066%\n",
      "Epoch 25, Batch 711, LR 0.000031 Loss 5.100398, Accuracy 87.069%\n",
      "Epoch 25, Batch 712, LR 0.000031 Loss 5.100631, Accuracy 87.068%\n",
      "Epoch 25, Batch 713, LR 0.000031 Loss 5.099391, Accuracy 87.067%\n",
      "Epoch 25, Batch 714, LR 0.000031 Loss 5.099567, Accuracy 87.071%\n",
      "Epoch 25, Batch 715, LR 0.000031 Loss 5.099757, Accuracy 87.064%\n",
      "Epoch 25, Batch 716, LR 0.000031 Loss 5.099924, Accuracy 87.061%\n",
      "Epoch 25, Batch 717, LR 0.000031 Loss 5.099363, Accuracy 87.059%\n",
      "Epoch 25, Batch 718, LR 0.000031 Loss 5.099392, Accuracy 87.061%\n",
      "Epoch 25, Batch 719, LR 0.000031 Loss 5.098988, Accuracy 87.060%\n",
      "Epoch 25, Batch 720, LR 0.000031 Loss 5.098680, Accuracy 87.064%\n",
      "Epoch 25, Batch 721, LR 0.000031 Loss 5.097976, Accuracy 87.069%\n",
      "Epoch 25, Batch 722, LR 0.000031 Loss 5.097342, Accuracy 87.068%\n",
      "Epoch 25, Batch 723, LR 0.000031 Loss 5.097425, Accuracy 87.071%\n",
      "Epoch 25, Batch 724, LR 0.000031 Loss 5.097207, Accuracy 87.072%\n",
      "Epoch 25, Batch 725, LR 0.000031 Loss 5.096181, Accuracy 87.075%\n",
      "Epoch 25, Batch 726, LR 0.000031 Loss 5.096240, Accuracy 87.078%\n",
      "Epoch 25, Batch 727, LR 0.000031 Loss 5.096061, Accuracy 87.081%\n",
      "Epoch 25, Batch 728, LR 0.000031 Loss 5.096319, Accuracy 87.075%\n",
      "Epoch 25, Batch 729, LR 0.000031 Loss 5.096192, Accuracy 87.076%\n",
      "Epoch 25, Batch 730, LR 0.000031 Loss 5.096042, Accuracy 87.079%\n",
      "Epoch 25, Batch 731, LR 0.000031 Loss 5.095299, Accuracy 87.079%\n",
      "Epoch 25, Batch 732, LR 0.000031 Loss 5.095756, Accuracy 87.075%\n",
      "Epoch 25, Batch 733, LR 0.000031 Loss 5.095919, Accuracy 87.073%\n",
      "Epoch 25, Batch 734, LR 0.000031 Loss 5.097086, Accuracy 87.061%\n",
      "Epoch 25, Batch 735, LR 0.000031 Loss 5.097845, Accuracy 87.059%\n",
      "Epoch 25, Batch 736, LR 0.000031 Loss 5.098010, Accuracy 87.059%\n",
      "Epoch 25, Batch 737, LR 0.000031 Loss 5.099268, Accuracy 87.055%\n",
      "Epoch 25, Batch 738, LR 0.000031 Loss 5.098783, Accuracy 87.061%\n",
      "Epoch 25, Batch 739, LR 0.000031 Loss 5.098585, Accuracy 87.063%\n",
      "Epoch 25, Batch 740, LR 0.000031 Loss 5.098739, Accuracy 87.064%\n",
      "Epoch 25, Batch 741, LR 0.000031 Loss 5.099069, Accuracy 87.067%\n",
      "Epoch 25, Batch 742, LR 0.000031 Loss 5.099711, Accuracy 87.067%\n",
      "Epoch 25, Batch 743, LR 0.000031 Loss 5.099475, Accuracy 87.074%\n",
      "Epoch 25, Batch 744, LR 0.000031 Loss 5.100246, Accuracy 87.071%\n",
      "Epoch 25, Batch 745, LR 0.000031 Loss 5.100370, Accuracy 87.070%\n",
      "Epoch 25, Batch 746, LR 0.000031 Loss 5.100937, Accuracy 87.067%\n",
      "Epoch 25, Batch 747, LR 0.000031 Loss 5.099799, Accuracy 87.072%\n",
      "Epoch 25, Batch 748, LR 0.000031 Loss 5.099399, Accuracy 87.072%\n",
      "Epoch 25, Batch 749, LR 0.000031 Loss 5.100029, Accuracy 87.068%\n",
      "Epoch 25, Batch 750, LR 0.000031 Loss 5.099692, Accuracy 87.069%\n",
      "Epoch 25, Batch 751, LR 0.000031 Loss 5.099914, Accuracy 87.067%\n",
      "Epoch 25, Batch 752, LR 0.000031 Loss 5.099965, Accuracy 87.067%\n",
      "Epoch 25, Batch 753, LR 0.000031 Loss 5.100498, Accuracy 87.066%\n",
      "Epoch 25, Batch 754, LR 0.000031 Loss 5.100439, Accuracy 87.068%\n",
      "Epoch 25, Batch 755, LR 0.000031 Loss 5.100685, Accuracy 87.067%\n",
      "Epoch 25, Batch 756, LR 0.000031 Loss 5.101797, Accuracy 87.067%\n",
      "Epoch 25, Batch 757, LR 0.000031 Loss 5.102424, Accuracy 87.064%\n",
      "Epoch 25, Batch 758, LR 0.000031 Loss 5.102500, Accuracy 87.063%\n",
      "Epoch 25, Batch 759, LR 0.000031 Loss 5.100830, Accuracy 87.067%\n",
      "Epoch 25, Batch 760, LR 0.000031 Loss 5.100259, Accuracy 87.073%\n",
      "Epoch 25, Batch 761, LR 0.000031 Loss 5.099713, Accuracy 87.076%\n",
      "Epoch 25, Batch 762, LR 0.000031 Loss 5.099266, Accuracy 87.084%\n",
      "Epoch 25, Batch 763, LR 0.000031 Loss 5.099450, Accuracy 87.085%\n",
      "Epoch 25, Batch 764, LR 0.000031 Loss 5.099928, Accuracy 87.080%\n",
      "Epoch 25, Batch 765, LR 0.000031 Loss 5.099717, Accuracy 87.082%\n",
      "Epoch 25, Batch 766, LR 0.000031 Loss 5.100423, Accuracy 87.079%\n",
      "Epoch 25, Batch 767, LR 0.000031 Loss 5.099416, Accuracy 87.084%\n",
      "Epoch 25, Batch 768, LR 0.000031 Loss 5.099492, Accuracy 87.085%\n",
      "Epoch 25, Batch 769, LR 0.000031 Loss 5.098846, Accuracy 87.088%\n",
      "Epoch 25, Batch 770, LR 0.000031 Loss 5.099119, Accuracy 87.085%\n",
      "Epoch 25, Batch 771, LR 0.000031 Loss 5.098841, Accuracy 87.087%\n",
      "Epoch 25, Batch 772, LR 0.000031 Loss 5.099801, Accuracy 87.078%\n",
      "Epoch 25, Batch 773, LR 0.000031 Loss 5.100020, Accuracy 87.073%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 774, LR 0.000031 Loss 5.099629, Accuracy 87.074%\n",
      "Epoch 25, Batch 775, LR 0.000031 Loss 5.100572, Accuracy 87.066%\n",
      "Epoch 25, Batch 776, LR 0.000031 Loss 5.101028, Accuracy 87.067%\n",
      "Epoch 25, Batch 777, LR 0.000031 Loss 5.101522, Accuracy 87.069%\n",
      "Epoch 25, Batch 778, LR 0.000031 Loss 5.101339, Accuracy 87.073%\n",
      "Epoch 25, Batch 779, LR 0.000031 Loss 5.100933, Accuracy 87.076%\n",
      "Epoch 25, Batch 780, LR 0.000031 Loss 5.101729, Accuracy 87.074%\n",
      "Epoch 25, Batch 781, LR 0.000031 Loss 5.102220, Accuracy 87.076%\n",
      "Epoch 25, Batch 782, LR 0.000031 Loss 5.101887, Accuracy 87.081%\n",
      "Epoch 25, Batch 783, LR 0.000031 Loss 5.101619, Accuracy 87.080%\n",
      "Epoch 25, Batch 784, LR 0.000031 Loss 5.101019, Accuracy 87.084%\n",
      "Epoch 25, Batch 785, LR 0.000031 Loss 5.101153, Accuracy 87.090%\n",
      "Epoch 25, Batch 786, LR 0.000031 Loss 5.101035, Accuracy 87.093%\n",
      "Epoch 25, Batch 787, LR 0.000031 Loss 5.099391, Accuracy 87.099%\n",
      "Epoch 25, Batch 788, LR 0.000031 Loss 5.099353, Accuracy 87.095%\n",
      "Epoch 25, Batch 789, LR 0.000031 Loss 5.098776, Accuracy 87.098%\n",
      "Epoch 25, Batch 790, LR 0.000031 Loss 5.099024, Accuracy 87.094%\n",
      "Epoch 25, Batch 791, LR 0.000031 Loss 5.100900, Accuracy 87.078%\n",
      "Epoch 25, Batch 792, LR 0.000031 Loss 5.101033, Accuracy 87.075%\n",
      "Epoch 25, Batch 793, LR 0.000031 Loss 5.101210, Accuracy 87.074%\n",
      "Epoch 25, Batch 794, LR 0.000031 Loss 5.100831, Accuracy 87.076%\n",
      "Epoch 25, Batch 795, LR 0.000031 Loss 5.101005, Accuracy 87.075%\n",
      "Epoch 25, Batch 796, LR 0.000031 Loss 5.100433, Accuracy 87.079%\n",
      "Epoch 25, Batch 797, LR 0.000031 Loss 5.099944, Accuracy 87.080%\n",
      "Epoch 25, Batch 798, LR 0.000031 Loss 5.099394, Accuracy 87.086%\n",
      "Epoch 25, Batch 799, LR 0.000031 Loss 5.099071, Accuracy 87.089%\n",
      "Epoch 25, Batch 800, LR 0.000031 Loss 5.099317, Accuracy 87.085%\n",
      "Epoch 25, Batch 801, LR 0.000031 Loss 5.099850, Accuracy 87.085%\n",
      "Epoch 25, Batch 802, LR 0.000031 Loss 5.100316, Accuracy 87.087%\n",
      "Epoch 25, Batch 803, LR 0.000031 Loss 5.100390, Accuracy 87.089%\n",
      "Epoch 25, Batch 804, LR 0.000031 Loss 5.100335, Accuracy 87.091%\n",
      "Epoch 25, Batch 805, LR 0.000031 Loss 5.101532, Accuracy 87.083%\n",
      "Epoch 25, Batch 806, LR 0.000031 Loss 5.101858, Accuracy 87.079%\n",
      "Epoch 25, Batch 807, LR 0.000031 Loss 5.102001, Accuracy 87.080%\n",
      "Epoch 25, Batch 808, LR 0.000031 Loss 5.102097, Accuracy 87.080%\n",
      "Epoch 25, Batch 809, LR 0.000031 Loss 5.101846, Accuracy 87.079%\n",
      "Epoch 25, Batch 810, LR 0.000031 Loss 5.102093, Accuracy 87.081%\n",
      "Epoch 25, Batch 811, LR 0.000031 Loss 5.102636, Accuracy 87.077%\n",
      "Epoch 25, Batch 812, LR 0.000031 Loss 5.103504, Accuracy 87.073%\n",
      "Epoch 25, Batch 813, LR 0.000031 Loss 5.103828, Accuracy 87.068%\n",
      "Epoch 25, Batch 814, LR 0.000031 Loss 5.103278, Accuracy 87.074%\n",
      "Epoch 25, Batch 815, LR 0.000031 Loss 5.103651, Accuracy 87.074%\n",
      "Epoch 25, Batch 816, LR 0.000031 Loss 5.104513, Accuracy 87.072%\n",
      "Epoch 25, Batch 817, LR 0.000031 Loss 5.104623, Accuracy 87.073%\n",
      "Epoch 25, Batch 818, LR 0.000031 Loss 5.105115, Accuracy 87.073%\n",
      "Epoch 25, Batch 819, LR 0.000031 Loss 5.105910, Accuracy 87.064%\n",
      "Epoch 25, Batch 820, LR 0.000031 Loss 5.105472, Accuracy 87.068%\n",
      "Epoch 25, Batch 821, LR 0.000031 Loss 5.104954, Accuracy 87.070%\n",
      "Epoch 25, Batch 822, LR 0.000031 Loss 5.105229, Accuracy 87.069%\n",
      "Epoch 25, Batch 823, LR 0.000031 Loss 5.106027, Accuracy 87.070%\n",
      "Epoch 25, Batch 824, LR 0.000031 Loss 5.106169, Accuracy 87.066%\n",
      "Epoch 25, Batch 825, LR 0.000031 Loss 5.106082, Accuracy 87.068%\n",
      "Epoch 25, Batch 826, LR 0.000031 Loss 5.106747, Accuracy 87.073%\n",
      "Epoch 25, Batch 827, LR 0.000031 Loss 5.106855, Accuracy 87.075%\n",
      "Epoch 25, Batch 828, LR 0.000031 Loss 5.108380, Accuracy 87.067%\n",
      "Epoch 25, Batch 829, LR 0.000031 Loss 5.109016, Accuracy 87.064%\n",
      "Epoch 25, Batch 830, LR 0.000031 Loss 5.108565, Accuracy 87.068%\n",
      "Epoch 25, Batch 831, LR 0.000031 Loss 5.109429, Accuracy 87.067%\n",
      "Epoch 25, Batch 832, LR 0.000031 Loss 5.109570, Accuracy 87.065%\n",
      "Epoch 25, Batch 833, LR 0.000031 Loss 5.109565, Accuracy 87.066%\n",
      "Epoch 25, Batch 834, LR 0.000031 Loss 5.110951, Accuracy 87.053%\n",
      "Epoch 25, Batch 835, LR 0.000031 Loss 5.111021, Accuracy 87.056%\n",
      "Epoch 25, Batch 836, LR 0.000031 Loss 5.111006, Accuracy 87.055%\n",
      "Epoch 25, Batch 837, LR 0.000031 Loss 5.111206, Accuracy 87.050%\n",
      "Epoch 25, Batch 838, LR 0.000031 Loss 5.112174, Accuracy 87.046%\n",
      "Epoch 25, Batch 839, LR 0.000031 Loss 5.111667, Accuracy 87.050%\n",
      "Epoch 25, Batch 840, LR 0.000031 Loss 5.111010, Accuracy 87.055%\n",
      "Epoch 25, Batch 841, LR 0.000031 Loss 5.109543, Accuracy 87.063%\n",
      "Epoch 25, Batch 842, LR 0.000031 Loss 5.108972, Accuracy 87.065%\n",
      "Epoch 25, Batch 843, LR 0.000031 Loss 5.109067, Accuracy 87.069%\n",
      "Epoch 25, Batch 844, LR 0.000031 Loss 5.108854, Accuracy 87.072%\n",
      "Epoch 25, Batch 845, LR 0.000031 Loss 5.108150, Accuracy 87.075%\n",
      "Epoch 25, Batch 846, LR 0.000031 Loss 5.107661, Accuracy 87.077%\n",
      "Epoch 25, Batch 847, LR 0.000031 Loss 5.107114, Accuracy 87.080%\n",
      "Epoch 25, Batch 848, LR 0.000031 Loss 5.106699, Accuracy 87.078%\n",
      "Epoch 25, Batch 849, LR 0.000031 Loss 5.106816, Accuracy 87.081%\n",
      "Epoch 25, Batch 850, LR 0.000031 Loss 5.107070, Accuracy 87.078%\n",
      "Epoch 25, Batch 851, LR 0.000031 Loss 5.107112, Accuracy 87.075%\n",
      "Epoch 25, Batch 852, LR 0.000031 Loss 5.106722, Accuracy 87.076%\n",
      "Epoch 25, Batch 853, LR 0.000031 Loss 5.107178, Accuracy 87.074%\n",
      "Epoch 25, Batch 854, LR 0.000031 Loss 5.106657, Accuracy 87.077%\n",
      "Epoch 25, Batch 855, LR 0.000031 Loss 5.107007, Accuracy 87.078%\n",
      "Epoch 25, Batch 856, LR 0.000031 Loss 5.107004, Accuracy 87.077%\n",
      "Epoch 25, Batch 857, LR 0.000031 Loss 5.106722, Accuracy 87.078%\n",
      "Epoch 25, Batch 858, LR 0.000031 Loss 5.105662, Accuracy 87.083%\n",
      "Epoch 25, Batch 859, LR 0.000031 Loss 5.104935, Accuracy 87.086%\n",
      "Epoch 25, Batch 860, LR 0.000031 Loss 5.105142, Accuracy 87.084%\n",
      "Epoch 25, Batch 861, LR 0.000031 Loss 5.105548, Accuracy 87.081%\n",
      "Epoch 25, Batch 862, LR 0.000031 Loss 5.104612, Accuracy 87.085%\n",
      "Epoch 25, Batch 863, LR 0.000031 Loss 5.104297, Accuracy 87.086%\n",
      "Epoch 25, Batch 864, LR 0.000031 Loss 5.104355, Accuracy 87.086%\n",
      "Epoch 25, Batch 865, LR 0.000031 Loss 5.103839, Accuracy 87.094%\n",
      "Epoch 25, Batch 866, LR 0.000031 Loss 5.102850, Accuracy 87.099%\n",
      "Epoch 25, Batch 867, LR 0.000031 Loss 5.103314, Accuracy 87.098%\n",
      "Epoch 25, Batch 868, LR 0.000031 Loss 5.103742, Accuracy 87.094%\n",
      "Epoch 25, Batch 869, LR 0.000031 Loss 5.104505, Accuracy 87.089%\n",
      "Epoch 25, Batch 870, LR 0.000031 Loss 5.104661, Accuracy 87.087%\n",
      "Epoch 25, Batch 871, LR 0.000031 Loss 5.105070, Accuracy 87.086%\n",
      "Epoch 25, Batch 872, LR 0.000031 Loss 5.106275, Accuracy 87.077%\n",
      "Epoch 25, Batch 873, LR 0.000031 Loss 5.105735, Accuracy 87.081%\n",
      "Epoch 25, Batch 874, LR 0.000031 Loss 5.105659, Accuracy 87.082%\n",
      "Epoch 25, Batch 875, LR 0.000031 Loss 5.106583, Accuracy 87.077%\n",
      "Epoch 25, Batch 876, LR 0.000031 Loss 5.106348, Accuracy 87.080%\n",
      "Epoch 25, Batch 877, LR 0.000031 Loss 5.106348, Accuracy 87.079%\n",
      "Epoch 25, Batch 878, LR 0.000031 Loss 5.106477, Accuracy 87.079%\n",
      "Epoch 25, Batch 879, LR 0.000031 Loss 5.106229, Accuracy 87.082%\n",
      "Epoch 25, Batch 880, LR 0.000031 Loss 5.106609, Accuracy 87.082%\n",
      "Epoch 25, Batch 881, LR 0.000031 Loss 5.106394, Accuracy 87.079%\n",
      "Epoch 25, Batch 882, LR 0.000031 Loss 5.106191, Accuracy 87.077%\n",
      "Epoch 25, Batch 883, LR 0.000031 Loss 5.106642, Accuracy 87.075%\n",
      "Epoch 25, Batch 884, LR 0.000031 Loss 5.106644, Accuracy 87.079%\n",
      "Epoch 25, Batch 885, LR 0.000031 Loss 5.107425, Accuracy 87.075%\n",
      "Epoch 25, Batch 886, LR 0.000030 Loss 5.107198, Accuracy 87.076%\n",
      "Epoch 25, Batch 887, LR 0.000030 Loss 5.107279, Accuracy 87.081%\n",
      "Epoch 25, Batch 888, LR 0.000030 Loss 5.106980, Accuracy 87.082%\n",
      "Epoch 25, Batch 889, LR 0.000030 Loss 5.107061, Accuracy 87.083%\n",
      "Epoch 25, Batch 890, LR 0.000030 Loss 5.106380, Accuracy 87.087%\n",
      "Epoch 25, Batch 891, LR 0.000030 Loss 5.106159, Accuracy 87.088%\n",
      "Epoch 25, Batch 892, LR 0.000030 Loss 5.106390, Accuracy 87.085%\n",
      "Epoch 25, Batch 893, LR 0.000030 Loss 5.106102, Accuracy 87.084%\n",
      "Epoch 25, Batch 894, LR 0.000030 Loss 5.106030, Accuracy 87.083%\n",
      "Epoch 25, Batch 895, LR 0.000030 Loss 5.105486, Accuracy 87.085%\n",
      "Epoch 25, Batch 896, LR 0.000030 Loss 5.106325, Accuracy 87.079%\n",
      "Epoch 25, Batch 897, LR 0.000030 Loss 5.107210, Accuracy 87.078%\n",
      "Epoch 25, Batch 898, LR 0.000030 Loss 5.107502, Accuracy 87.076%\n",
      "Epoch 25, Batch 899, LR 0.000030 Loss 5.107051, Accuracy 87.074%\n",
      "Epoch 25, Batch 900, LR 0.000030 Loss 5.107408, Accuracy 87.076%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 901, LR 0.000030 Loss 5.108022, Accuracy 87.074%\n",
      "Epoch 25, Batch 902, LR 0.000030 Loss 5.107765, Accuracy 87.073%\n",
      "Epoch 25, Batch 903, LR 0.000030 Loss 5.107965, Accuracy 87.071%\n",
      "Epoch 25, Batch 904, LR 0.000030 Loss 5.107780, Accuracy 87.070%\n",
      "Epoch 25, Batch 905, LR 0.000030 Loss 5.108031, Accuracy 87.069%\n",
      "Epoch 25, Batch 906, LR 0.000030 Loss 5.107986, Accuracy 87.069%\n",
      "Epoch 25, Batch 907, LR 0.000030 Loss 5.107749, Accuracy 87.069%\n",
      "Epoch 25, Batch 908, LR 0.000030 Loss 5.107866, Accuracy 87.069%\n",
      "Epoch 25, Batch 909, LR 0.000030 Loss 5.108399, Accuracy 87.065%\n",
      "Epoch 25, Batch 910, LR 0.000030 Loss 5.108569, Accuracy 87.069%\n",
      "Epoch 25, Batch 911, LR 0.000030 Loss 5.108100, Accuracy 87.069%\n",
      "Epoch 25, Batch 912, LR 0.000030 Loss 5.107259, Accuracy 87.072%\n",
      "Epoch 25, Batch 913, LR 0.000030 Loss 5.107152, Accuracy 87.076%\n",
      "Epoch 25, Batch 914, LR 0.000030 Loss 5.106869, Accuracy 87.079%\n",
      "Epoch 25, Batch 915, LR 0.000030 Loss 5.107872, Accuracy 87.075%\n",
      "Epoch 25, Batch 916, LR 0.000030 Loss 5.108348, Accuracy 87.077%\n",
      "Epoch 25, Batch 917, LR 0.000030 Loss 5.108518, Accuracy 87.075%\n",
      "Epoch 25, Batch 918, LR 0.000030 Loss 5.109121, Accuracy 87.073%\n",
      "Epoch 25, Batch 919, LR 0.000030 Loss 5.108943, Accuracy 87.080%\n",
      "Epoch 25, Batch 920, LR 0.000030 Loss 5.109224, Accuracy 87.077%\n",
      "Epoch 25, Batch 921, LR 0.000030 Loss 5.107711, Accuracy 87.082%\n",
      "Epoch 25, Batch 922, LR 0.000030 Loss 5.107290, Accuracy 87.085%\n",
      "Epoch 25, Batch 923, LR 0.000030 Loss 5.107737, Accuracy 87.081%\n",
      "Epoch 25, Batch 924, LR 0.000030 Loss 5.108151, Accuracy 87.080%\n",
      "Epoch 25, Batch 925, LR 0.000030 Loss 5.107812, Accuracy 87.078%\n",
      "Epoch 25, Batch 926, LR 0.000030 Loss 5.107797, Accuracy 87.082%\n",
      "Epoch 25, Batch 927, LR 0.000030 Loss 5.108595, Accuracy 87.076%\n",
      "Epoch 25, Batch 928, LR 0.000030 Loss 5.108138, Accuracy 87.079%\n",
      "Epoch 25, Batch 929, LR 0.000030 Loss 5.108727, Accuracy 87.075%\n",
      "Epoch 25, Batch 930, LR 0.000030 Loss 5.109242, Accuracy 87.073%\n",
      "Epoch 25, Batch 931, LR 0.000030 Loss 5.109101, Accuracy 87.075%\n",
      "Epoch 25, Batch 932, LR 0.000030 Loss 5.108654, Accuracy 87.074%\n",
      "Epoch 25, Batch 933, LR 0.000030 Loss 5.108835, Accuracy 87.070%\n",
      "Epoch 25, Batch 934, LR 0.000030 Loss 5.109038, Accuracy 87.071%\n",
      "Epoch 25, Batch 935, LR 0.000030 Loss 5.109456, Accuracy 87.065%\n",
      "Epoch 25, Batch 936, LR 0.000030 Loss 5.108873, Accuracy 87.071%\n",
      "Epoch 25, Batch 937, LR 0.000030 Loss 5.109659, Accuracy 87.065%\n",
      "Epoch 25, Batch 938, LR 0.000030 Loss 5.109557, Accuracy 87.065%\n",
      "Epoch 25, Batch 939, LR 0.000030 Loss 5.108796, Accuracy 87.070%\n",
      "Epoch 25, Batch 940, LR 0.000030 Loss 5.108815, Accuracy 87.073%\n",
      "Epoch 25, Batch 941, LR 0.000030 Loss 5.108559, Accuracy 87.073%\n",
      "Epoch 25, Batch 942, LR 0.000030 Loss 5.108626, Accuracy 87.072%\n",
      "Epoch 25, Batch 943, LR 0.000030 Loss 5.109180, Accuracy 87.072%\n",
      "Epoch 25, Batch 944, LR 0.000030 Loss 5.108894, Accuracy 87.075%\n",
      "Epoch 25, Batch 945, LR 0.000030 Loss 5.109278, Accuracy 87.072%\n",
      "Epoch 25, Batch 946, LR 0.000030 Loss 5.109225, Accuracy 87.074%\n",
      "Epoch 25, Batch 947, LR 0.000030 Loss 5.108584, Accuracy 87.076%\n",
      "Epoch 25, Batch 948, LR 0.000030 Loss 5.108882, Accuracy 87.072%\n",
      "Epoch 25, Batch 949, LR 0.000030 Loss 5.108833, Accuracy 87.071%\n",
      "Epoch 25, Batch 950, LR 0.000030 Loss 5.108440, Accuracy 87.070%\n",
      "Epoch 25, Batch 951, LR 0.000030 Loss 5.109756, Accuracy 87.065%\n",
      "Epoch 25, Batch 952, LR 0.000030 Loss 5.109212, Accuracy 87.066%\n",
      "Epoch 25, Batch 953, LR 0.000030 Loss 5.109017, Accuracy 87.066%\n",
      "Epoch 25, Batch 954, LR 0.000030 Loss 5.108875, Accuracy 87.064%\n",
      "Epoch 25, Batch 955, LR 0.000030 Loss 5.108607, Accuracy 87.064%\n",
      "Epoch 25, Batch 956, LR 0.000030 Loss 5.108208, Accuracy 87.070%\n",
      "Epoch 25, Batch 957, LR 0.000030 Loss 5.108514, Accuracy 87.071%\n",
      "Epoch 25, Batch 958, LR 0.000030 Loss 5.108564, Accuracy 87.073%\n",
      "Epoch 25, Batch 959, LR 0.000030 Loss 5.109382, Accuracy 87.070%\n",
      "Epoch 25, Batch 960, LR 0.000030 Loss 5.108753, Accuracy 87.074%\n",
      "Epoch 25, Batch 961, LR 0.000030 Loss 5.109764, Accuracy 87.068%\n",
      "Epoch 25, Batch 962, LR 0.000030 Loss 5.109252, Accuracy 87.069%\n",
      "Epoch 25, Batch 963, LR 0.000030 Loss 5.109013, Accuracy 87.068%\n",
      "Epoch 25, Batch 964, LR 0.000030 Loss 5.109640, Accuracy 87.066%\n",
      "Epoch 25, Batch 965, LR 0.000030 Loss 5.109716, Accuracy 87.065%\n",
      "Epoch 25, Batch 966, LR 0.000030 Loss 5.109881, Accuracy 87.067%\n",
      "Epoch 25, Batch 967, LR 0.000030 Loss 5.109259, Accuracy 87.071%\n",
      "Epoch 25, Batch 968, LR 0.000030 Loss 5.109012, Accuracy 87.071%\n",
      "Epoch 25, Batch 969, LR 0.000030 Loss 5.108686, Accuracy 87.069%\n",
      "Epoch 25, Batch 970, LR 0.000030 Loss 5.109639, Accuracy 87.063%\n",
      "Epoch 25, Batch 971, LR 0.000030 Loss 5.110292, Accuracy 87.061%\n",
      "Epoch 25, Batch 972, LR 0.000030 Loss 5.110825, Accuracy 87.063%\n",
      "Epoch 25, Batch 973, LR 0.000030 Loss 5.111057, Accuracy 87.060%\n",
      "Epoch 25, Batch 974, LR 0.000030 Loss 5.110235, Accuracy 87.062%\n",
      "Epoch 25, Batch 975, LR 0.000030 Loss 5.110729, Accuracy 87.057%\n",
      "Epoch 25, Batch 976, LR 0.000030 Loss 5.111331, Accuracy 87.051%\n",
      "Epoch 25, Batch 977, LR 0.000030 Loss 5.111104, Accuracy 87.051%\n",
      "Epoch 25, Batch 978, LR 0.000030 Loss 5.111454, Accuracy 87.047%\n",
      "Epoch 25, Batch 979, LR 0.000030 Loss 5.112228, Accuracy 87.040%\n",
      "Epoch 25, Batch 980, LR 0.000030 Loss 5.112170, Accuracy 87.038%\n",
      "Epoch 25, Batch 981, LR 0.000030 Loss 5.111728, Accuracy 87.040%\n",
      "Epoch 25, Batch 982, LR 0.000030 Loss 5.111100, Accuracy 87.043%\n",
      "Epoch 25, Batch 983, LR 0.000030 Loss 5.110635, Accuracy 87.046%\n",
      "Epoch 25, Batch 984, LR 0.000030 Loss 5.110321, Accuracy 87.048%\n",
      "Epoch 25, Batch 985, LR 0.000030 Loss 5.109693, Accuracy 87.050%\n",
      "Epoch 25, Batch 986, LR 0.000030 Loss 5.109064, Accuracy 87.053%\n",
      "Epoch 25, Batch 987, LR 0.000030 Loss 5.108601, Accuracy 87.054%\n",
      "Epoch 25, Batch 988, LR 0.000030 Loss 5.108903, Accuracy 87.052%\n",
      "Epoch 25, Batch 989, LR 0.000030 Loss 5.108813, Accuracy 87.052%\n",
      "Epoch 25, Batch 990, LR 0.000030 Loss 5.109534, Accuracy 87.052%\n",
      "Epoch 25, Batch 991, LR 0.000030 Loss 5.109958, Accuracy 87.047%\n",
      "Epoch 25, Batch 992, LR 0.000030 Loss 5.110360, Accuracy 87.046%\n",
      "Epoch 25, Batch 993, LR 0.000030 Loss 5.110998, Accuracy 87.044%\n",
      "Epoch 25, Batch 994, LR 0.000030 Loss 5.110870, Accuracy 87.043%\n",
      "Epoch 25, Batch 995, LR 0.000030 Loss 5.110867, Accuracy 87.042%\n",
      "Epoch 25, Batch 996, LR 0.000030 Loss 5.111515, Accuracy 87.042%\n",
      "Epoch 25, Batch 997, LR 0.000030 Loss 5.111431, Accuracy 87.046%\n",
      "Epoch 25, Batch 998, LR 0.000030 Loss 5.111073, Accuracy 87.051%\n",
      "Epoch 25, Batch 999, LR 0.000030 Loss 5.111251, Accuracy 87.053%\n",
      "Epoch 25, Batch 1000, LR 0.000030 Loss 5.111570, Accuracy 87.050%\n",
      "Epoch 25, Batch 1001, LR 0.000030 Loss 5.111338, Accuracy 87.050%\n",
      "Epoch 25, Batch 1002, LR 0.000030 Loss 5.110847, Accuracy 87.054%\n",
      "Epoch 25, Batch 1003, LR 0.000030 Loss 5.110171, Accuracy 87.059%\n",
      "Epoch 25, Batch 1004, LR 0.000030 Loss 5.109879, Accuracy 87.059%\n",
      "Epoch 25, Batch 1005, LR 0.000030 Loss 5.110009, Accuracy 87.056%\n",
      "Epoch 25, Batch 1006, LR 0.000030 Loss 5.110338, Accuracy 87.055%\n",
      "Epoch 25, Batch 1007, LR 0.000030 Loss 5.110719, Accuracy 87.054%\n",
      "Epoch 25, Batch 1008, LR 0.000030 Loss 5.110998, Accuracy 87.053%\n",
      "Epoch 25, Batch 1009, LR 0.000030 Loss 5.110622, Accuracy 87.052%\n",
      "Epoch 25, Batch 1010, LR 0.000030 Loss 5.110432, Accuracy 87.054%\n",
      "Epoch 25, Batch 1011, LR 0.000030 Loss 5.110100, Accuracy 87.056%\n",
      "Epoch 25, Batch 1012, LR 0.000030 Loss 5.110267, Accuracy 87.051%\n",
      "Epoch 25, Batch 1013, LR 0.000030 Loss 5.109617, Accuracy 87.053%\n",
      "Epoch 25, Batch 1014, LR 0.000030 Loss 5.108678, Accuracy 87.059%\n",
      "Epoch 25, Batch 1015, LR 0.000030 Loss 5.109350, Accuracy 87.050%\n",
      "Epoch 25, Batch 1016, LR 0.000030 Loss 5.109152, Accuracy 87.050%\n",
      "Epoch 25, Batch 1017, LR 0.000030 Loss 5.108256, Accuracy 87.055%\n",
      "Epoch 25, Batch 1018, LR 0.000030 Loss 5.108368, Accuracy 87.056%\n",
      "Epoch 25, Batch 1019, LR 0.000030 Loss 5.108238, Accuracy 87.055%\n",
      "Epoch 25, Batch 1020, LR 0.000030 Loss 5.107659, Accuracy 87.060%\n",
      "Epoch 25, Batch 1021, LR 0.000030 Loss 5.106975, Accuracy 87.064%\n",
      "Epoch 25, Batch 1022, LR 0.000030 Loss 5.107017, Accuracy 87.067%\n",
      "Epoch 25, Batch 1023, LR 0.000030 Loss 5.107315, Accuracy 87.069%\n",
      "Epoch 25, Batch 1024, LR 0.000030 Loss 5.106565, Accuracy 87.074%\n",
      "Epoch 25, Batch 1025, LR 0.000030 Loss 5.106747, Accuracy 87.069%\n",
      "Epoch 25, Batch 1026, LR 0.000030 Loss 5.107152, Accuracy 87.069%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 1027, LR 0.000030 Loss 5.107326, Accuracy 87.069%\n",
      "Epoch 25, Batch 1028, LR 0.000030 Loss 5.106713, Accuracy 87.071%\n",
      "Epoch 25, Batch 1029, LR 0.000030 Loss 5.106680, Accuracy 87.071%\n",
      "Epoch 25, Batch 1030, LR 0.000030 Loss 5.107444, Accuracy 87.071%\n",
      "Epoch 25, Batch 1031, LR 0.000030 Loss 5.106967, Accuracy 87.074%\n",
      "Epoch 25, Batch 1032, LR 0.000030 Loss 5.106843, Accuracy 87.075%\n",
      "Epoch 25, Batch 1033, LR 0.000030 Loss 5.106554, Accuracy 87.079%\n",
      "Epoch 25, Batch 1034, LR 0.000030 Loss 5.106265, Accuracy 87.078%\n",
      "Epoch 25, Batch 1035, LR 0.000030 Loss 5.105210, Accuracy 87.083%\n",
      "Epoch 25, Batch 1036, LR 0.000030 Loss 5.105068, Accuracy 87.084%\n",
      "Epoch 25, Batch 1037, LR 0.000030 Loss 5.104371, Accuracy 87.088%\n",
      "Epoch 25, Batch 1038, LR 0.000030 Loss 5.103982, Accuracy 87.087%\n",
      "Epoch 25, Batch 1039, LR 0.000030 Loss 5.104625, Accuracy 87.085%\n",
      "Epoch 25, Batch 1040, LR 0.000030 Loss 5.105197, Accuracy 87.087%\n",
      "Epoch 25, Batch 1041, LR 0.000030 Loss 5.105629, Accuracy 87.085%\n",
      "Epoch 25, Batch 1042, LR 0.000030 Loss 5.104948, Accuracy 87.087%\n",
      "Epoch 25, Batch 1043, LR 0.000030 Loss 5.104286, Accuracy 87.089%\n",
      "Epoch 25, Batch 1044, LR 0.000030 Loss 5.103890, Accuracy 87.089%\n",
      "Epoch 25, Batch 1045, LR 0.000030 Loss 5.104210, Accuracy 87.089%\n",
      "Epoch 25, Batch 1046, LR 0.000030 Loss 5.104603, Accuracy 87.088%\n",
      "Epoch 25, Batch 1047, LR 0.000030 Loss 5.103842, Accuracy 87.090%\n",
      "Epoch 25, Loss (train set) 5.103842, Accuracy (train set) 87.090%\n",
      "Epoch 26, Batch 1, LR 0.000030 Loss 4.973279, Accuracy 88.281%\n",
      "Epoch 26, Batch 2, LR 0.000030 Loss 5.142497, Accuracy 88.281%\n",
      "Epoch 26, Batch 3, LR 0.000030 Loss 5.086296, Accuracy 88.021%\n",
      "Epoch 26, Batch 4, LR 0.000030 Loss 5.096874, Accuracy 86.914%\n",
      "Epoch 26, Batch 5, LR 0.000030 Loss 5.145812, Accuracy 86.406%\n",
      "Epoch 26, Batch 6, LR 0.000030 Loss 5.086054, Accuracy 86.589%\n",
      "Epoch 26, Batch 7, LR 0.000030 Loss 5.107528, Accuracy 86.607%\n",
      "Epoch 26, Batch 8, LR 0.000030 Loss 5.062260, Accuracy 86.816%\n",
      "Epoch 26, Batch 9, LR 0.000030 Loss 5.054738, Accuracy 86.979%\n",
      "Epoch 26, Batch 10, LR 0.000030 Loss 5.061812, Accuracy 86.797%\n",
      "Epoch 26, Batch 11, LR 0.000030 Loss 5.031165, Accuracy 86.790%\n",
      "Epoch 26, Batch 12, LR 0.000030 Loss 5.044268, Accuracy 86.849%\n",
      "Epoch 26, Batch 13, LR 0.000030 Loss 5.027692, Accuracy 87.500%\n",
      "Epoch 26, Batch 14, LR 0.000030 Loss 5.003246, Accuracy 87.835%\n",
      "Epoch 26, Batch 15, LR 0.000030 Loss 5.018674, Accuracy 87.708%\n",
      "Epoch 26, Batch 16, LR 0.000030 Loss 5.015538, Accuracy 87.695%\n",
      "Epoch 26, Batch 17, LR 0.000030 Loss 5.021583, Accuracy 87.592%\n",
      "Epoch 26, Batch 18, LR 0.000030 Loss 4.994152, Accuracy 87.457%\n",
      "Epoch 26, Batch 19, LR 0.000030 Loss 5.013759, Accuracy 87.418%\n",
      "Epoch 26, Batch 20, LR 0.000030 Loss 4.980581, Accuracy 87.656%\n",
      "Epoch 26, Batch 21, LR 0.000030 Loss 4.965327, Accuracy 87.649%\n",
      "Epoch 26, Batch 22, LR 0.000030 Loss 4.985934, Accuracy 87.464%\n",
      "Epoch 26, Batch 23, LR 0.000030 Loss 5.009710, Accuracy 87.092%\n",
      "Epoch 26, Batch 24, LR 0.000030 Loss 5.034156, Accuracy 86.947%\n",
      "Epoch 26, Batch 25, LR 0.000030 Loss 5.017368, Accuracy 87.031%\n",
      "Epoch 26, Batch 26, LR 0.000030 Loss 5.024012, Accuracy 87.169%\n",
      "Epoch 26, Batch 27, LR 0.000030 Loss 5.009153, Accuracy 87.153%\n",
      "Epoch 26, Batch 28, LR 0.000030 Loss 5.023933, Accuracy 87.221%\n",
      "Epoch 26, Batch 29, LR 0.000030 Loss 5.015537, Accuracy 87.338%\n",
      "Epoch 26, Batch 30, LR 0.000030 Loss 5.035529, Accuracy 87.370%\n",
      "Epoch 26, Batch 31, LR 0.000030 Loss 5.016292, Accuracy 87.525%\n",
      "Epoch 26, Batch 32, LR 0.000030 Loss 5.010040, Accuracy 87.646%\n",
      "Epoch 26, Batch 33, LR 0.000030 Loss 5.025652, Accuracy 87.571%\n",
      "Epoch 26, Batch 34, LR 0.000030 Loss 5.007996, Accuracy 87.638%\n",
      "Epoch 26, Batch 35, LR 0.000030 Loss 4.997207, Accuracy 87.768%\n",
      "Epoch 26, Batch 36, LR 0.000030 Loss 4.990313, Accuracy 87.739%\n",
      "Epoch 26, Batch 37, LR 0.000030 Loss 4.990053, Accuracy 87.669%\n",
      "Epoch 26, Batch 38, LR 0.000030 Loss 4.988464, Accuracy 87.664%\n",
      "Epoch 26, Batch 39, LR 0.000030 Loss 5.011493, Accuracy 87.520%\n",
      "Epoch 26, Batch 40, LR 0.000030 Loss 5.021834, Accuracy 87.500%\n",
      "Epoch 26, Batch 41, LR 0.000030 Loss 5.020630, Accuracy 87.557%\n",
      "Epoch 26, Batch 42, LR 0.000030 Loss 5.028266, Accuracy 87.519%\n",
      "Epoch 26, Batch 43, LR 0.000030 Loss 5.040595, Accuracy 87.336%\n",
      "Epoch 26, Batch 44, LR 0.000030 Loss 5.041044, Accuracy 87.340%\n",
      "Epoch 26, Batch 45, LR 0.000030 Loss 5.048379, Accuracy 87.309%\n",
      "Epoch 26, Batch 46, LR 0.000030 Loss 5.041509, Accuracy 87.364%\n",
      "Epoch 26, Batch 47, LR 0.000030 Loss 5.046581, Accuracy 87.434%\n",
      "Epoch 26, Batch 48, LR 0.000030 Loss 5.041047, Accuracy 87.435%\n",
      "Epoch 26, Batch 49, LR 0.000030 Loss 5.038104, Accuracy 87.452%\n",
      "Epoch 26, Batch 50, LR 0.000030 Loss 5.026495, Accuracy 87.531%\n",
      "Epoch 26, Batch 51, LR 0.000030 Loss 5.033313, Accuracy 87.531%\n",
      "Epoch 26, Batch 52, LR 0.000030 Loss 5.037585, Accuracy 87.560%\n",
      "Epoch 26, Batch 53, LR 0.000030 Loss 5.048638, Accuracy 87.500%\n",
      "Epoch 26, Batch 54, LR 0.000030 Loss 5.058279, Accuracy 87.457%\n",
      "Epoch 26, Batch 55, LR 0.000030 Loss 5.057026, Accuracy 87.486%\n",
      "Epoch 26, Batch 56, LR 0.000030 Loss 5.045753, Accuracy 87.514%\n",
      "Epoch 26, Batch 57, LR 0.000030 Loss 5.053452, Accuracy 87.404%\n",
      "Epoch 26, Batch 58, LR 0.000030 Loss 5.047796, Accuracy 87.460%\n",
      "Epoch 26, Batch 59, LR 0.000030 Loss 5.052052, Accuracy 87.447%\n",
      "Epoch 26, Batch 60, LR 0.000030 Loss 5.057246, Accuracy 87.435%\n",
      "Epoch 26, Batch 61, LR 0.000030 Loss 5.049072, Accuracy 87.462%\n",
      "Epoch 26, Batch 62, LR 0.000030 Loss 5.037216, Accuracy 87.513%\n",
      "Epoch 26, Batch 63, LR 0.000030 Loss 5.038525, Accuracy 87.488%\n",
      "Epoch 26, Batch 64, LR 0.000030 Loss 5.030804, Accuracy 87.512%\n",
      "Epoch 26, Batch 65, LR 0.000030 Loss 5.028124, Accuracy 87.536%\n",
      "Epoch 26, Batch 66, LR 0.000030 Loss 5.031272, Accuracy 87.595%\n",
      "Epoch 26, Batch 67, LR 0.000030 Loss 5.020989, Accuracy 87.675%\n",
      "Epoch 26, Batch 68, LR 0.000030 Loss 5.012369, Accuracy 87.707%\n",
      "Epoch 26, Batch 69, LR 0.000030 Loss 5.011232, Accuracy 87.783%\n",
      "Epoch 26, Batch 70, LR 0.000030 Loss 5.013645, Accuracy 87.790%\n",
      "Epoch 26, Batch 71, LR 0.000030 Loss 5.009527, Accuracy 87.830%\n",
      "Epoch 26, Batch 72, LR 0.000030 Loss 5.013560, Accuracy 87.760%\n",
      "Epoch 26, Batch 73, LR 0.000030 Loss 5.016420, Accuracy 87.768%\n",
      "Epoch 26, Batch 74, LR 0.000030 Loss 5.009810, Accuracy 87.796%\n",
      "Epoch 26, Batch 75, LR 0.000030 Loss 5.010109, Accuracy 87.781%\n",
      "Epoch 26, Batch 76, LR 0.000030 Loss 5.008119, Accuracy 87.850%\n",
      "Epoch 26, Batch 77, LR 0.000030 Loss 5.009795, Accuracy 87.794%\n",
      "Epoch 26, Batch 78, LR 0.000030 Loss 5.013664, Accuracy 87.750%\n",
      "Epoch 26, Batch 79, LR 0.000030 Loss 5.030014, Accuracy 87.629%\n",
      "Epoch 26, Batch 80, LR 0.000030 Loss 5.026645, Accuracy 87.637%\n",
      "Epoch 26, Batch 81, LR 0.000030 Loss 5.031538, Accuracy 87.616%\n",
      "Epoch 26, Batch 82, LR 0.000030 Loss 5.029670, Accuracy 87.662%\n",
      "Epoch 26, Batch 83, LR 0.000030 Loss 5.033877, Accuracy 87.669%\n",
      "Epoch 26, Batch 84, LR 0.000030 Loss 5.035041, Accuracy 87.686%\n",
      "Epoch 26, Batch 85, LR 0.000030 Loss 5.040016, Accuracy 87.665%\n",
      "Epoch 26, Batch 86, LR 0.000030 Loss 5.040382, Accuracy 87.636%\n",
      "Epoch 26, Batch 87, LR 0.000030 Loss 5.034455, Accuracy 87.626%\n",
      "Epoch 26, Batch 88, LR 0.000030 Loss 5.029279, Accuracy 87.633%\n",
      "Epoch 26, Batch 89, LR 0.000030 Loss 5.024509, Accuracy 87.632%\n",
      "Epoch 26, Batch 90, LR 0.000030 Loss 5.018788, Accuracy 87.622%\n",
      "Epoch 26, Batch 91, LR 0.000030 Loss 5.013466, Accuracy 87.663%\n",
      "Epoch 26, Batch 92, LR 0.000030 Loss 5.004006, Accuracy 87.712%\n",
      "Epoch 26, Batch 93, LR 0.000030 Loss 5.008770, Accuracy 87.660%\n",
      "Epoch 26, Batch 94, LR 0.000030 Loss 5.009326, Accuracy 87.650%\n",
      "Epoch 26, Batch 95, LR 0.000030 Loss 5.011228, Accuracy 87.648%\n",
      "Epoch 26, Batch 96, LR 0.000030 Loss 5.009261, Accuracy 87.663%\n",
      "Epoch 26, Batch 97, LR 0.000030 Loss 5.016845, Accuracy 87.637%\n",
      "Epoch 26, Batch 98, LR 0.000030 Loss 5.016094, Accuracy 87.636%\n",
      "Epoch 26, Batch 99, LR 0.000030 Loss 5.014444, Accuracy 87.634%\n",
      "Epoch 26, Batch 100, LR 0.000030 Loss 5.013401, Accuracy 87.625%\n",
      "Epoch 26, Batch 101, LR 0.000030 Loss 5.009136, Accuracy 87.647%\n",
      "Epoch 26, Batch 102, LR 0.000030 Loss 5.015777, Accuracy 87.561%\n",
      "Epoch 26, Batch 103, LR 0.000030 Loss 5.017035, Accuracy 87.568%\n",
      "Epoch 26, Batch 104, LR 0.000030 Loss 5.018194, Accuracy 87.538%\n",
      "Epoch 26, Batch 105, LR 0.000030 Loss 5.018288, Accuracy 87.537%\n",
      "Epoch 26, Batch 106, LR 0.000030 Loss 5.013481, Accuracy 87.559%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 107, LR 0.000030 Loss 5.016354, Accuracy 87.522%\n",
      "Epoch 26, Batch 108, LR 0.000030 Loss 5.016271, Accuracy 87.522%\n",
      "Epoch 26, Batch 109, LR 0.000030 Loss 5.020158, Accuracy 87.500%\n",
      "Epoch 26, Batch 110, LR 0.000030 Loss 5.017689, Accuracy 87.521%\n",
      "Epoch 26, Batch 111, LR 0.000030 Loss 5.015037, Accuracy 87.507%\n",
      "Epoch 26, Batch 112, LR 0.000030 Loss 5.014468, Accuracy 87.500%\n",
      "Epoch 26, Batch 113, LR 0.000030 Loss 5.021271, Accuracy 87.459%\n",
      "Epoch 26, Batch 114, LR 0.000030 Loss 5.017128, Accuracy 87.500%\n",
      "Epoch 26, Batch 115, LR 0.000030 Loss 5.023840, Accuracy 87.446%\n",
      "Epoch 26, Batch 116, LR 0.000029 Loss 5.023650, Accuracy 87.460%\n",
      "Epoch 26, Batch 117, LR 0.000029 Loss 5.030609, Accuracy 87.427%\n",
      "Epoch 26, Batch 118, LR 0.000029 Loss 5.035035, Accuracy 87.407%\n",
      "Epoch 26, Batch 119, LR 0.000029 Loss 5.031423, Accuracy 87.408%\n",
      "Epoch 26, Batch 120, LR 0.000029 Loss 5.030840, Accuracy 87.428%\n",
      "Epoch 26, Batch 121, LR 0.000029 Loss 5.029881, Accuracy 87.474%\n",
      "Epoch 26, Batch 122, LR 0.000029 Loss 5.032420, Accuracy 87.468%\n",
      "Epoch 26, Batch 123, LR 0.000029 Loss 5.030738, Accuracy 87.468%\n",
      "Epoch 26, Batch 124, LR 0.000029 Loss 5.028177, Accuracy 87.443%\n",
      "Epoch 26, Batch 125, LR 0.000029 Loss 5.030091, Accuracy 87.444%\n",
      "Epoch 26, Batch 126, LR 0.000029 Loss 5.030865, Accuracy 87.426%\n",
      "Epoch 26, Batch 127, LR 0.000029 Loss 5.034133, Accuracy 87.432%\n",
      "Epoch 26, Batch 128, LR 0.000029 Loss 5.034457, Accuracy 87.427%\n",
      "Epoch 26, Batch 129, LR 0.000029 Loss 5.034144, Accuracy 87.415%\n",
      "Epoch 26, Batch 130, LR 0.000029 Loss 5.036797, Accuracy 87.398%\n",
      "Epoch 26, Batch 131, LR 0.000029 Loss 5.036852, Accuracy 87.381%\n",
      "Epoch 26, Batch 132, LR 0.000029 Loss 5.032979, Accuracy 87.405%\n",
      "Epoch 26, Batch 133, LR 0.000029 Loss 5.035289, Accuracy 87.406%\n",
      "Epoch 26, Batch 134, LR 0.000029 Loss 5.038095, Accuracy 87.407%\n",
      "Epoch 26, Batch 135, LR 0.000029 Loss 5.033593, Accuracy 87.419%\n",
      "Epoch 26, Batch 136, LR 0.000029 Loss 5.033304, Accuracy 87.420%\n",
      "Epoch 26, Batch 137, LR 0.000029 Loss 5.028088, Accuracy 87.449%\n",
      "Epoch 26, Batch 138, LR 0.000029 Loss 5.023166, Accuracy 87.466%\n",
      "Epoch 26, Batch 139, LR 0.000029 Loss 5.027525, Accuracy 87.461%\n",
      "Epoch 26, Batch 140, LR 0.000029 Loss 5.032815, Accuracy 87.450%\n",
      "Epoch 26, Batch 141, LR 0.000029 Loss 5.034931, Accuracy 87.472%\n",
      "Epoch 26, Batch 142, LR 0.000029 Loss 5.029554, Accuracy 87.489%\n",
      "Epoch 26, Batch 143, LR 0.000029 Loss 5.028912, Accuracy 87.489%\n",
      "Epoch 26, Batch 144, LR 0.000029 Loss 5.032832, Accuracy 87.484%\n",
      "Epoch 26, Batch 145, LR 0.000029 Loss 5.031789, Accuracy 87.495%\n",
      "Epoch 26, Batch 146, LR 0.000029 Loss 5.032666, Accuracy 87.505%\n",
      "Epoch 26, Batch 147, LR 0.000029 Loss 5.027631, Accuracy 87.521%\n",
      "Epoch 26, Batch 148, LR 0.000029 Loss 5.030173, Accuracy 87.511%\n",
      "Epoch 26, Batch 149, LR 0.000029 Loss 5.028903, Accuracy 87.521%\n",
      "Epoch 26, Batch 150, LR 0.000029 Loss 5.033847, Accuracy 87.510%\n",
      "Epoch 26, Batch 151, LR 0.000029 Loss 5.035586, Accuracy 87.479%\n",
      "Epoch 26, Batch 152, LR 0.000029 Loss 5.036916, Accuracy 87.464%\n",
      "Epoch 26, Batch 153, LR 0.000029 Loss 5.037696, Accuracy 87.480%\n",
      "Epoch 26, Batch 154, LR 0.000029 Loss 5.035422, Accuracy 87.500%\n",
      "Epoch 26, Batch 155, LR 0.000029 Loss 5.037204, Accuracy 87.475%\n",
      "Epoch 26, Batch 156, LR 0.000029 Loss 5.038046, Accuracy 87.460%\n",
      "Epoch 26, Batch 157, LR 0.000029 Loss 5.040236, Accuracy 87.440%\n",
      "Epoch 26, Batch 158, LR 0.000029 Loss 5.039722, Accuracy 87.446%\n",
      "Epoch 26, Batch 159, LR 0.000029 Loss 5.040544, Accuracy 87.436%\n",
      "Epoch 26, Batch 160, LR 0.000029 Loss 5.045057, Accuracy 87.422%\n",
      "Epoch 26, Batch 161, LR 0.000029 Loss 5.045948, Accuracy 87.418%\n",
      "Epoch 26, Batch 162, LR 0.000029 Loss 5.051144, Accuracy 87.408%\n",
      "Epoch 26, Batch 163, LR 0.000029 Loss 5.051102, Accuracy 87.395%\n",
      "Epoch 26, Batch 164, LR 0.000029 Loss 5.055755, Accuracy 87.348%\n",
      "Epoch 26, Batch 165, LR 0.000029 Loss 5.056189, Accuracy 87.358%\n",
      "Epoch 26, Batch 166, LR 0.000029 Loss 5.055832, Accuracy 87.354%\n",
      "Epoch 26, Batch 167, LR 0.000029 Loss 5.055719, Accuracy 87.360%\n",
      "Epoch 26, Batch 168, LR 0.000029 Loss 5.049738, Accuracy 87.398%\n",
      "Epoch 26, Batch 169, LR 0.000029 Loss 5.051854, Accuracy 87.384%\n",
      "Epoch 26, Batch 170, LR 0.000029 Loss 5.050809, Accuracy 87.385%\n",
      "Epoch 26, Batch 171, LR 0.000029 Loss 5.048529, Accuracy 87.395%\n",
      "Epoch 26, Batch 172, LR 0.000029 Loss 5.051129, Accuracy 87.396%\n",
      "Epoch 26, Batch 173, LR 0.000029 Loss 5.053417, Accuracy 87.383%\n",
      "Epoch 26, Batch 174, LR 0.000029 Loss 5.055709, Accuracy 87.388%\n",
      "Epoch 26, Batch 175, LR 0.000029 Loss 5.052010, Accuracy 87.415%\n",
      "Epoch 26, Batch 176, LR 0.000029 Loss 5.052358, Accuracy 87.407%\n",
      "Epoch 26, Batch 177, LR 0.000029 Loss 5.050826, Accuracy 87.403%\n",
      "Epoch 26, Batch 178, LR 0.000029 Loss 5.051936, Accuracy 87.403%\n",
      "Epoch 26, Batch 179, LR 0.000029 Loss 5.049105, Accuracy 87.408%\n",
      "Epoch 26, Batch 180, LR 0.000029 Loss 5.045722, Accuracy 87.426%\n",
      "Epoch 26, Batch 181, LR 0.000029 Loss 5.040480, Accuracy 87.465%\n",
      "Epoch 26, Batch 182, LR 0.000029 Loss 5.040774, Accuracy 87.461%\n",
      "Epoch 26, Batch 183, LR 0.000029 Loss 5.042175, Accuracy 87.462%\n",
      "Epoch 26, Batch 184, LR 0.000029 Loss 5.040135, Accuracy 87.479%\n",
      "Epoch 26, Batch 185, LR 0.000029 Loss 5.039645, Accuracy 87.479%\n",
      "Epoch 26, Batch 186, LR 0.000029 Loss 5.039201, Accuracy 87.492%\n",
      "Epoch 26, Batch 187, LR 0.000029 Loss 5.043600, Accuracy 87.467%\n",
      "Epoch 26, Batch 188, LR 0.000029 Loss 5.042830, Accuracy 87.438%\n",
      "Epoch 26, Batch 189, LR 0.000029 Loss 5.043170, Accuracy 87.434%\n",
      "Epoch 26, Batch 190, LR 0.000029 Loss 5.038619, Accuracy 87.451%\n",
      "Epoch 26, Batch 191, LR 0.000029 Loss 5.032264, Accuracy 87.488%\n",
      "Epoch 26, Batch 192, LR 0.000029 Loss 5.031279, Accuracy 87.480%\n",
      "Epoch 26, Batch 193, LR 0.000029 Loss 5.028250, Accuracy 87.496%\n",
      "Epoch 26, Batch 194, LR 0.000029 Loss 5.025744, Accuracy 87.520%\n",
      "Epoch 26, Batch 195, LR 0.000029 Loss 5.022646, Accuracy 87.540%\n",
      "Epoch 26, Batch 196, LR 0.000029 Loss 5.021757, Accuracy 87.544%\n",
      "Epoch 26, Batch 197, LR 0.000029 Loss 5.021618, Accuracy 87.559%\n",
      "Epoch 26, Batch 198, LR 0.000029 Loss 5.018166, Accuracy 87.591%\n",
      "Epoch 26, Batch 199, LR 0.000029 Loss 5.018742, Accuracy 87.582%\n",
      "Epoch 26, Batch 200, LR 0.000029 Loss 5.015223, Accuracy 87.605%\n",
      "Epoch 26, Batch 201, LR 0.000029 Loss 5.015408, Accuracy 87.597%\n",
      "Epoch 26, Batch 202, LR 0.000029 Loss 5.016357, Accuracy 87.585%\n",
      "Epoch 26, Batch 203, LR 0.000029 Loss 5.019123, Accuracy 87.577%\n",
      "Epoch 26, Batch 204, LR 0.000029 Loss 5.016310, Accuracy 87.592%\n",
      "Epoch 26, Batch 205, LR 0.000029 Loss 5.015050, Accuracy 87.584%\n",
      "Epoch 26, Batch 206, LR 0.000029 Loss 5.013450, Accuracy 87.587%\n",
      "Epoch 26, Batch 207, LR 0.000029 Loss 5.016525, Accuracy 87.572%\n",
      "Epoch 26, Batch 208, LR 0.000029 Loss 5.021361, Accuracy 87.538%\n",
      "Epoch 26, Batch 209, LR 0.000029 Loss 5.022283, Accuracy 87.515%\n",
      "Epoch 26, Batch 210, LR 0.000029 Loss 5.023129, Accuracy 87.504%\n",
      "Epoch 26, Batch 211, LR 0.000029 Loss 5.021812, Accuracy 87.515%\n",
      "Epoch 26, Batch 212, LR 0.000029 Loss 5.026850, Accuracy 87.478%\n",
      "Epoch 26, Batch 213, LR 0.000029 Loss 5.026378, Accuracy 87.482%\n",
      "Epoch 26, Batch 214, LR 0.000029 Loss 5.026368, Accuracy 87.485%\n",
      "Epoch 26, Batch 215, LR 0.000029 Loss 5.026723, Accuracy 87.467%\n",
      "Epoch 26, Batch 216, LR 0.000029 Loss 5.023938, Accuracy 87.478%\n",
      "Epoch 26, Batch 217, LR 0.000029 Loss 5.023142, Accuracy 87.471%\n",
      "Epoch 26, Batch 218, LR 0.000029 Loss 5.023220, Accuracy 87.471%\n",
      "Epoch 26, Batch 219, LR 0.000029 Loss 5.023412, Accuracy 87.468%\n",
      "Epoch 26, Batch 220, LR 0.000029 Loss 5.021943, Accuracy 87.468%\n",
      "Epoch 26, Batch 221, LR 0.000029 Loss 5.021600, Accuracy 87.479%\n",
      "Epoch 26, Batch 222, LR 0.000029 Loss 5.022745, Accuracy 87.489%\n",
      "Epoch 26, Batch 223, LR 0.000029 Loss 5.023129, Accuracy 87.482%\n",
      "Epoch 26, Batch 224, LR 0.000029 Loss 5.025035, Accuracy 87.479%\n",
      "Epoch 26, Batch 225, LR 0.000029 Loss 5.028168, Accuracy 87.458%\n",
      "Epoch 26, Batch 226, LR 0.000029 Loss 5.029852, Accuracy 87.455%\n",
      "Epoch 26, Batch 227, LR 0.000029 Loss 5.030644, Accuracy 87.441%\n",
      "Epoch 26, Batch 228, LR 0.000029 Loss 5.032841, Accuracy 87.445%\n",
      "Epoch 26, Batch 229, LR 0.000029 Loss 5.033942, Accuracy 87.428%\n",
      "Epoch 26, Batch 230, LR 0.000029 Loss 5.033560, Accuracy 87.439%\n",
      "Epoch 26, Batch 231, LR 0.000029 Loss 5.032825, Accuracy 87.443%\n",
      "Epoch 26, Batch 232, LR 0.000029 Loss 5.033414, Accuracy 87.439%\n",
      "Epoch 26, Batch 233, LR 0.000029 Loss 5.034023, Accuracy 87.443%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 234, LR 0.000029 Loss 5.036606, Accuracy 87.430%\n",
      "Epoch 26, Batch 235, LR 0.000029 Loss 5.038513, Accuracy 87.424%\n",
      "Epoch 26, Batch 236, LR 0.000029 Loss 5.037233, Accuracy 87.437%\n",
      "Epoch 26, Batch 237, LR 0.000029 Loss 5.035373, Accuracy 87.437%\n",
      "Epoch 26, Batch 238, LR 0.000029 Loss 5.034110, Accuracy 87.447%\n",
      "Epoch 26, Batch 239, LR 0.000029 Loss 5.033151, Accuracy 87.458%\n",
      "Epoch 26, Batch 240, LR 0.000029 Loss 5.034141, Accuracy 87.454%\n",
      "Epoch 26, Batch 241, LR 0.000029 Loss 5.035200, Accuracy 87.451%\n",
      "Epoch 26, Batch 242, LR 0.000029 Loss 5.033460, Accuracy 87.455%\n",
      "Epoch 26, Batch 243, LR 0.000029 Loss 5.034421, Accuracy 87.445%\n",
      "Epoch 26, Batch 244, LR 0.000029 Loss 5.033139, Accuracy 87.458%\n",
      "Epoch 26, Batch 245, LR 0.000029 Loss 5.037187, Accuracy 87.443%\n",
      "Epoch 26, Batch 246, LR 0.000029 Loss 5.037060, Accuracy 87.446%\n",
      "Epoch 26, Batch 247, LR 0.000029 Loss 5.035146, Accuracy 87.453%\n",
      "Epoch 26, Batch 248, LR 0.000029 Loss 5.036476, Accuracy 87.446%\n",
      "Epoch 26, Batch 249, LR 0.000029 Loss 5.036857, Accuracy 87.431%\n",
      "Epoch 26, Batch 250, LR 0.000029 Loss 5.037351, Accuracy 87.434%\n",
      "Epoch 26, Batch 251, LR 0.000029 Loss 5.034752, Accuracy 87.447%\n",
      "Epoch 26, Batch 252, LR 0.000029 Loss 5.035384, Accuracy 87.435%\n",
      "Epoch 26, Batch 253, LR 0.000029 Loss 5.037591, Accuracy 87.429%\n",
      "Epoch 26, Batch 254, LR 0.000029 Loss 5.035623, Accuracy 87.438%\n",
      "Epoch 26, Batch 255, LR 0.000029 Loss 5.034593, Accuracy 87.454%\n",
      "Epoch 26, Batch 256, LR 0.000029 Loss 5.034392, Accuracy 87.451%\n",
      "Epoch 26, Batch 257, LR 0.000029 Loss 5.034222, Accuracy 87.445%\n",
      "Epoch 26, Batch 258, LR 0.000029 Loss 5.031946, Accuracy 87.455%\n",
      "Epoch 26, Batch 259, LR 0.000029 Loss 5.029982, Accuracy 87.455%\n",
      "Epoch 26, Batch 260, LR 0.000029 Loss 5.030928, Accuracy 87.446%\n",
      "Epoch 26, Batch 261, LR 0.000029 Loss 5.029581, Accuracy 87.455%\n",
      "Epoch 26, Batch 262, LR 0.000029 Loss 5.029958, Accuracy 87.461%\n",
      "Epoch 26, Batch 263, LR 0.000029 Loss 5.027923, Accuracy 87.461%\n",
      "Epoch 26, Batch 264, LR 0.000029 Loss 5.026394, Accuracy 87.470%\n",
      "Epoch 26, Batch 265, LR 0.000029 Loss 5.028013, Accuracy 87.473%\n",
      "Epoch 26, Batch 266, LR 0.000029 Loss 5.027113, Accuracy 87.477%\n",
      "Epoch 26, Batch 267, LR 0.000029 Loss 5.026461, Accuracy 87.482%\n",
      "Epoch 26, Batch 268, LR 0.000029 Loss 5.024726, Accuracy 87.491%\n",
      "Epoch 26, Batch 269, LR 0.000029 Loss 5.027016, Accuracy 87.480%\n",
      "Epoch 26, Batch 270, LR 0.000029 Loss 5.024701, Accuracy 87.486%\n",
      "Epoch 26, Batch 271, LR 0.000029 Loss 5.024253, Accuracy 87.494%\n",
      "Epoch 26, Batch 272, LR 0.000029 Loss 5.026394, Accuracy 87.483%\n",
      "Epoch 26, Batch 273, LR 0.000029 Loss 5.026524, Accuracy 87.477%\n",
      "Epoch 26, Batch 274, LR 0.000029 Loss 5.025442, Accuracy 87.474%\n",
      "Epoch 26, Batch 275, LR 0.000029 Loss 5.025822, Accuracy 87.474%\n",
      "Epoch 26, Batch 276, LR 0.000029 Loss 5.028932, Accuracy 87.469%\n",
      "Epoch 26, Batch 277, LR 0.000029 Loss 5.032669, Accuracy 87.455%\n",
      "Epoch 26, Batch 278, LR 0.000029 Loss 5.036656, Accuracy 87.430%\n",
      "Epoch 26, Batch 279, LR 0.000029 Loss 5.038675, Accuracy 87.419%\n",
      "Epoch 26, Batch 280, LR 0.000029 Loss 5.040068, Accuracy 87.414%\n",
      "Epoch 26, Batch 281, LR 0.000029 Loss 5.038753, Accuracy 87.414%\n",
      "Epoch 26, Batch 282, LR 0.000029 Loss 5.039630, Accuracy 87.414%\n",
      "Epoch 26, Batch 283, LR 0.000029 Loss 5.041769, Accuracy 87.398%\n",
      "Epoch 26, Batch 284, LR 0.000029 Loss 5.042057, Accuracy 87.390%\n",
      "Epoch 26, Batch 285, LR 0.000029 Loss 5.040843, Accuracy 87.393%\n",
      "Epoch 26, Batch 286, LR 0.000029 Loss 5.041277, Accuracy 87.391%\n",
      "Epoch 26, Batch 287, LR 0.000029 Loss 5.044062, Accuracy 87.372%\n",
      "Epoch 26, Batch 288, LR 0.000029 Loss 5.043639, Accuracy 87.367%\n",
      "Epoch 26, Batch 289, LR 0.000029 Loss 5.045704, Accuracy 87.357%\n",
      "Epoch 26, Batch 290, LR 0.000029 Loss 5.045315, Accuracy 87.363%\n",
      "Epoch 26, Batch 291, LR 0.000029 Loss 5.045439, Accuracy 87.368%\n",
      "Epoch 26, Batch 292, LR 0.000029 Loss 5.046438, Accuracy 87.374%\n",
      "Epoch 26, Batch 293, LR 0.000029 Loss 5.049612, Accuracy 87.361%\n",
      "Epoch 26, Batch 294, LR 0.000029 Loss 5.053276, Accuracy 87.349%\n",
      "Epoch 26, Batch 295, LR 0.000029 Loss 5.054830, Accuracy 87.357%\n",
      "Epoch 26, Batch 296, LR 0.000029 Loss 5.055448, Accuracy 87.360%\n",
      "Epoch 26, Batch 297, LR 0.000029 Loss 5.052712, Accuracy 87.379%\n",
      "Epoch 26, Batch 298, LR 0.000029 Loss 5.053210, Accuracy 87.366%\n",
      "Epoch 26, Batch 299, LR 0.000029 Loss 5.054155, Accuracy 87.362%\n",
      "Epoch 26, Batch 300, LR 0.000029 Loss 5.052236, Accuracy 87.378%\n",
      "Epoch 26, Batch 301, LR 0.000029 Loss 5.052143, Accuracy 87.373%\n",
      "Epoch 26, Batch 302, LR 0.000029 Loss 5.050750, Accuracy 87.381%\n",
      "Epoch 26, Batch 303, LR 0.000029 Loss 5.050861, Accuracy 87.381%\n",
      "Epoch 26, Batch 304, LR 0.000029 Loss 5.052492, Accuracy 87.377%\n",
      "Epoch 26, Batch 305, LR 0.000029 Loss 5.051607, Accuracy 87.382%\n",
      "Epoch 26, Batch 306, LR 0.000029 Loss 5.049469, Accuracy 87.406%\n",
      "Epoch 26, Batch 307, LR 0.000029 Loss 5.050443, Accuracy 87.403%\n",
      "Epoch 26, Batch 308, LR 0.000029 Loss 5.050887, Accuracy 87.399%\n",
      "Epoch 26, Batch 309, LR 0.000029 Loss 5.049676, Accuracy 87.404%\n",
      "Epoch 26, Batch 310, LR 0.000029 Loss 5.048844, Accuracy 87.417%\n",
      "Epoch 26, Batch 311, LR 0.000029 Loss 5.049416, Accuracy 87.417%\n",
      "Epoch 26, Batch 312, LR 0.000029 Loss 5.047225, Accuracy 87.430%\n",
      "Epoch 26, Batch 313, LR 0.000029 Loss 5.046613, Accuracy 87.433%\n",
      "Epoch 26, Batch 314, LR 0.000029 Loss 5.046699, Accuracy 87.428%\n",
      "Epoch 26, Batch 315, LR 0.000029 Loss 5.046408, Accuracy 87.428%\n",
      "Epoch 26, Batch 316, LR 0.000029 Loss 5.046039, Accuracy 87.436%\n",
      "Epoch 26, Batch 317, LR 0.000029 Loss 5.049849, Accuracy 87.416%\n",
      "Epoch 26, Batch 318, LR 0.000029 Loss 5.048689, Accuracy 87.416%\n",
      "Epoch 26, Batch 319, LR 0.000029 Loss 5.047975, Accuracy 87.409%\n",
      "Epoch 26, Batch 320, LR 0.000029 Loss 5.048232, Accuracy 87.410%\n",
      "Epoch 26, Batch 321, LR 0.000029 Loss 5.047528, Accuracy 87.410%\n",
      "Epoch 26, Batch 322, LR 0.000029 Loss 5.047608, Accuracy 87.410%\n",
      "Epoch 26, Batch 323, LR 0.000029 Loss 5.046612, Accuracy 87.420%\n",
      "Epoch 26, Batch 324, LR 0.000029 Loss 5.046886, Accuracy 87.413%\n",
      "Epoch 26, Batch 325, LR 0.000029 Loss 5.049782, Accuracy 87.401%\n",
      "Epoch 26, Batch 326, LR 0.000029 Loss 5.048422, Accuracy 87.407%\n",
      "Epoch 26, Batch 327, LR 0.000029 Loss 5.046043, Accuracy 87.416%\n",
      "Epoch 26, Batch 328, LR 0.000029 Loss 5.044401, Accuracy 87.412%\n",
      "Epoch 26, Batch 329, LR 0.000029 Loss 5.043972, Accuracy 87.419%\n",
      "Epoch 26, Batch 330, LR 0.000029 Loss 5.041993, Accuracy 87.415%\n",
      "Epoch 26, Batch 331, LR 0.000029 Loss 5.040993, Accuracy 87.429%\n",
      "Epoch 26, Batch 332, LR 0.000029 Loss 5.041310, Accuracy 87.429%\n",
      "Epoch 26, Batch 333, LR 0.000029 Loss 5.041546, Accuracy 87.420%\n",
      "Epoch 26, Batch 334, LR 0.000029 Loss 5.043537, Accuracy 87.418%\n",
      "Epoch 26, Batch 335, LR 0.000029 Loss 5.041951, Accuracy 87.423%\n",
      "Epoch 26, Batch 336, LR 0.000029 Loss 5.039479, Accuracy 87.428%\n",
      "Epoch 26, Batch 337, LR 0.000029 Loss 5.038460, Accuracy 87.430%\n",
      "Epoch 26, Batch 338, LR 0.000029 Loss 5.036986, Accuracy 87.440%\n",
      "Epoch 26, Batch 339, LR 0.000029 Loss 5.035790, Accuracy 87.447%\n",
      "Epoch 26, Batch 340, LR 0.000029 Loss 5.035356, Accuracy 87.440%\n",
      "Epoch 26, Batch 341, LR 0.000029 Loss 5.033519, Accuracy 87.454%\n",
      "Epoch 26, Batch 342, LR 0.000029 Loss 5.032227, Accuracy 87.459%\n",
      "Epoch 26, Batch 343, LR 0.000029 Loss 5.030298, Accuracy 87.468%\n",
      "Epoch 26, Batch 344, LR 0.000029 Loss 5.033421, Accuracy 87.461%\n",
      "Epoch 26, Batch 345, LR 0.000029 Loss 5.032079, Accuracy 87.471%\n",
      "Epoch 26, Batch 346, LR 0.000029 Loss 5.031605, Accuracy 87.468%\n",
      "Epoch 26, Batch 347, LR 0.000029 Loss 5.030959, Accuracy 87.462%\n",
      "Epoch 26, Batch 348, LR 0.000029 Loss 5.033229, Accuracy 87.444%\n",
      "Epoch 26, Batch 349, LR 0.000029 Loss 5.033758, Accuracy 87.442%\n",
      "Epoch 26, Batch 350, LR 0.000029 Loss 5.034543, Accuracy 87.435%\n",
      "Epoch 26, Batch 351, LR 0.000029 Loss 5.032814, Accuracy 87.444%\n",
      "Epoch 26, Batch 352, LR 0.000029 Loss 5.031005, Accuracy 87.451%\n",
      "Epoch 26, Batch 353, LR 0.000029 Loss 5.031670, Accuracy 87.458%\n",
      "Epoch 26, Batch 354, LR 0.000029 Loss 5.031557, Accuracy 87.469%\n",
      "Epoch 26, Batch 355, LR 0.000029 Loss 5.034350, Accuracy 87.452%\n",
      "Epoch 26, Batch 356, LR 0.000029 Loss 5.033458, Accuracy 87.454%\n",
      "Epoch 26, Batch 357, LR 0.000029 Loss 5.033292, Accuracy 87.456%\n",
      "Epoch 26, Batch 358, LR 0.000029 Loss 5.032563, Accuracy 87.459%\n",
      "Epoch 26, Batch 359, LR 0.000029 Loss 5.034280, Accuracy 87.435%\n",
      "Epoch 26, Batch 360, LR 0.000029 Loss 5.036290, Accuracy 87.422%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 361, LR 0.000029 Loss 5.037192, Accuracy 87.411%\n",
      "Epoch 26, Batch 362, LR 0.000029 Loss 5.036618, Accuracy 87.407%\n",
      "Epoch 26, Batch 363, LR 0.000029 Loss 5.038117, Accuracy 87.416%\n",
      "Epoch 26, Batch 364, LR 0.000029 Loss 5.038302, Accuracy 87.412%\n",
      "Epoch 26, Batch 365, LR 0.000029 Loss 5.038544, Accuracy 87.408%\n",
      "Epoch 26, Batch 366, LR 0.000029 Loss 5.039874, Accuracy 87.404%\n",
      "Epoch 26, Batch 367, LR 0.000029 Loss 5.037098, Accuracy 87.417%\n",
      "Epoch 26, Batch 368, LR 0.000029 Loss 5.036556, Accuracy 87.417%\n",
      "Epoch 26, Batch 369, LR 0.000029 Loss 5.037719, Accuracy 87.420%\n",
      "Epoch 26, Batch 370, LR 0.000029 Loss 5.037267, Accuracy 87.422%\n",
      "Epoch 26, Batch 371, LR 0.000029 Loss 5.036840, Accuracy 87.414%\n",
      "Epoch 26, Batch 372, LR 0.000029 Loss 5.034632, Accuracy 87.433%\n",
      "Epoch 26, Batch 373, LR 0.000029 Loss 5.034160, Accuracy 87.433%\n",
      "Epoch 26, Batch 374, LR 0.000029 Loss 5.034329, Accuracy 87.427%\n",
      "Epoch 26, Batch 375, LR 0.000029 Loss 5.034174, Accuracy 87.435%\n",
      "Epoch 26, Batch 376, LR 0.000029 Loss 5.033299, Accuracy 87.444%\n",
      "Epoch 26, Batch 377, LR 0.000029 Loss 5.030240, Accuracy 87.448%\n",
      "Epoch 26, Batch 378, LR 0.000029 Loss 5.031238, Accuracy 87.448%\n",
      "Epoch 26, Batch 379, LR 0.000029 Loss 5.032733, Accuracy 87.436%\n",
      "Epoch 26, Batch 380, LR 0.000029 Loss 5.032338, Accuracy 87.438%\n",
      "Epoch 26, Batch 381, LR 0.000029 Loss 5.032987, Accuracy 87.436%\n",
      "Epoch 26, Batch 382, LR 0.000029 Loss 5.032029, Accuracy 87.445%\n",
      "Epoch 26, Batch 383, LR 0.000029 Loss 5.033098, Accuracy 87.445%\n",
      "Epoch 26, Batch 384, LR 0.000029 Loss 5.034777, Accuracy 87.425%\n",
      "Epoch 26, Batch 385, LR 0.000029 Loss 5.036176, Accuracy 87.417%\n",
      "Epoch 26, Batch 386, LR 0.000029 Loss 5.038899, Accuracy 87.405%\n",
      "Epoch 26, Batch 387, LR 0.000029 Loss 5.039252, Accuracy 87.403%\n",
      "Epoch 26, Batch 388, LR 0.000029 Loss 5.041048, Accuracy 87.391%\n",
      "Epoch 26, Batch 389, LR 0.000029 Loss 5.041547, Accuracy 87.386%\n",
      "Epoch 26, Batch 390, LR 0.000029 Loss 5.042811, Accuracy 87.374%\n",
      "Epoch 26, Batch 391, LR 0.000029 Loss 5.042286, Accuracy 87.374%\n",
      "Epoch 26, Batch 392, LR 0.000029 Loss 5.039723, Accuracy 87.388%\n",
      "Epoch 26, Batch 393, LR 0.000029 Loss 5.039580, Accuracy 87.389%\n",
      "Epoch 26, Batch 394, LR 0.000029 Loss 5.039004, Accuracy 87.383%\n",
      "Epoch 26, Batch 395, LR 0.000028 Loss 5.038004, Accuracy 87.387%\n",
      "Epoch 26, Batch 396, LR 0.000028 Loss 5.037376, Accuracy 87.390%\n",
      "Epoch 26, Batch 397, LR 0.000028 Loss 5.037131, Accuracy 87.392%\n",
      "Epoch 26, Batch 398, LR 0.000028 Loss 5.037130, Accuracy 87.392%\n",
      "Epoch 26, Batch 399, LR 0.000028 Loss 5.036998, Accuracy 87.384%\n",
      "Epoch 26, Batch 400, LR 0.000028 Loss 5.037194, Accuracy 87.389%\n",
      "Epoch 26, Batch 401, LR 0.000028 Loss 5.036119, Accuracy 87.387%\n",
      "Epoch 26, Batch 402, LR 0.000028 Loss 5.034985, Accuracy 87.391%\n",
      "Epoch 26, Batch 403, LR 0.000028 Loss 5.035407, Accuracy 87.386%\n",
      "Epoch 26, Batch 404, LR 0.000028 Loss 5.035645, Accuracy 87.380%\n",
      "Epoch 26, Batch 405, LR 0.000028 Loss 5.034645, Accuracy 87.384%\n",
      "Epoch 26, Batch 406, LR 0.000028 Loss 5.036026, Accuracy 87.373%\n",
      "Epoch 26, Batch 407, LR 0.000028 Loss 5.035023, Accuracy 87.377%\n",
      "Epoch 26, Batch 408, LR 0.000028 Loss 5.034663, Accuracy 87.383%\n",
      "Epoch 26, Batch 409, LR 0.000028 Loss 5.036136, Accuracy 87.378%\n",
      "Epoch 26, Batch 410, LR 0.000028 Loss 5.034490, Accuracy 87.378%\n",
      "Epoch 26, Batch 411, LR 0.000028 Loss 5.033367, Accuracy 87.384%\n",
      "Epoch 26, Batch 412, LR 0.000028 Loss 5.035490, Accuracy 87.375%\n",
      "Epoch 26, Batch 413, LR 0.000028 Loss 5.035397, Accuracy 87.373%\n",
      "Epoch 26, Batch 414, LR 0.000028 Loss 5.035564, Accuracy 87.364%\n",
      "Epoch 26, Batch 415, LR 0.000028 Loss 5.036342, Accuracy 87.359%\n",
      "Epoch 26, Batch 416, LR 0.000028 Loss 5.034681, Accuracy 87.367%\n",
      "Epoch 26, Batch 417, LR 0.000028 Loss 5.035317, Accuracy 87.371%\n",
      "Epoch 26, Batch 418, LR 0.000028 Loss 5.034059, Accuracy 87.384%\n",
      "Epoch 26, Batch 419, LR 0.000028 Loss 5.034751, Accuracy 87.377%\n",
      "Epoch 26, Batch 420, LR 0.000028 Loss 5.033983, Accuracy 87.377%\n",
      "Epoch 26, Batch 421, LR 0.000028 Loss 5.031466, Accuracy 87.391%\n",
      "Epoch 26, Batch 422, LR 0.000028 Loss 5.032071, Accuracy 87.382%\n",
      "Epoch 26, Batch 423, LR 0.000028 Loss 5.031584, Accuracy 87.387%\n",
      "Epoch 26, Batch 424, LR 0.000028 Loss 5.032219, Accuracy 87.388%\n",
      "Epoch 26, Batch 425, LR 0.000028 Loss 5.031284, Accuracy 87.395%\n",
      "Epoch 26, Batch 426, LR 0.000028 Loss 5.030920, Accuracy 87.397%\n",
      "Epoch 26, Batch 427, LR 0.000028 Loss 5.030397, Accuracy 87.403%\n",
      "Epoch 26, Batch 428, LR 0.000028 Loss 5.029819, Accuracy 87.398%\n",
      "Epoch 26, Batch 429, LR 0.000028 Loss 5.029599, Accuracy 87.394%\n",
      "Epoch 26, Batch 430, LR 0.000028 Loss 5.031121, Accuracy 87.389%\n",
      "Epoch 26, Batch 431, LR 0.000028 Loss 5.032402, Accuracy 87.380%\n",
      "Epoch 26, Batch 432, LR 0.000028 Loss 5.033835, Accuracy 87.377%\n",
      "Epoch 26, Batch 433, LR 0.000028 Loss 5.033070, Accuracy 87.377%\n",
      "Epoch 26, Batch 434, LR 0.000028 Loss 5.034073, Accuracy 87.367%\n",
      "Epoch 26, Batch 435, LR 0.000028 Loss 5.032599, Accuracy 87.376%\n",
      "Epoch 26, Batch 436, LR 0.000028 Loss 5.032334, Accuracy 87.369%\n",
      "Epoch 26, Batch 437, LR 0.000028 Loss 5.031460, Accuracy 87.371%\n",
      "Epoch 26, Batch 438, LR 0.000028 Loss 5.032866, Accuracy 87.375%\n",
      "Epoch 26, Batch 439, LR 0.000028 Loss 5.032978, Accuracy 87.379%\n",
      "Epoch 26, Batch 440, LR 0.000028 Loss 5.030613, Accuracy 87.386%\n",
      "Epoch 26, Batch 441, LR 0.000028 Loss 5.029666, Accuracy 87.399%\n",
      "Epoch 26, Batch 442, LR 0.000028 Loss 5.030480, Accuracy 87.396%\n",
      "Epoch 26, Batch 443, LR 0.000028 Loss 5.029120, Accuracy 87.401%\n",
      "Epoch 26, Batch 444, LR 0.000028 Loss 5.030730, Accuracy 87.394%\n",
      "Epoch 26, Batch 445, LR 0.000028 Loss 5.029762, Accuracy 87.405%\n",
      "Epoch 26, Batch 446, LR 0.000028 Loss 5.028294, Accuracy 87.412%\n",
      "Epoch 26, Batch 447, LR 0.000028 Loss 5.029492, Accuracy 87.407%\n",
      "Epoch 26, Batch 448, LR 0.000028 Loss 5.031689, Accuracy 87.392%\n",
      "Epoch 26, Batch 449, LR 0.000028 Loss 5.031940, Accuracy 87.394%\n",
      "Epoch 26, Batch 450, LR 0.000028 Loss 5.031050, Accuracy 87.401%\n",
      "Epoch 26, Batch 451, LR 0.000028 Loss 5.029512, Accuracy 87.405%\n",
      "Epoch 26, Batch 452, LR 0.000028 Loss 5.029810, Accuracy 87.407%\n",
      "Epoch 26, Batch 453, LR 0.000028 Loss 5.030220, Accuracy 87.400%\n",
      "Epoch 26, Batch 454, LR 0.000028 Loss 5.030746, Accuracy 87.386%\n",
      "Epoch 26, Batch 455, LR 0.000028 Loss 5.030237, Accuracy 87.395%\n",
      "Epoch 26, Batch 456, LR 0.000028 Loss 5.031260, Accuracy 87.390%\n",
      "Epoch 26, Batch 457, LR 0.000028 Loss 5.031227, Accuracy 87.394%\n",
      "Epoch 26, Batch 458, LR 0.000028 Loss 5.030238, Accuracy 87.401%\n",
      "Epoch 26, Batch 459, LR 0.000028 Loss 5.029307, Accuracy 87.417%\n",
      "Epoch 26, Batch 460, LR 0.000028 Loss 5.027498, Accuracy 87.430%\n",
      "Epoch 26, Batch 461, LR 0.000028 Loss 5.027716, Accuracy 87.429%\n",
      "Epoch 26, Batch 462, LR 0.000028 Loss 5.028146, Accuracy 87.424%\n",
      "Epoch 26, Batch 463, LR 0.000028 Loss 5.028398, Accuracy 87.419%\n",
      "Epoch 26, Batch 464, LR 0.000028 Loss 5.028680, Accuracy 87.419%\n",
      "Epoch 26, Batch 465, LR 0.000028 Loss 5.028109, Accuracy 87.423%\n",
      "Epoch 26, Batch 466, LR 0.000028 Loss 5.026577, Accuracy 87.433%\n",
      "Epoch 26, Batch 467, LR 0.000028 Loss 5.025580, Accuracy 87.436%\n",
      "Epoch 26, Batch 468, LR 0.000028 Loss 5.025188, Accuracy 87.437%\n",
      "Epoch 26, Batch 469, LR 0.000028 Loss 5.024943, Accuracy 87.433%\n",
      "Epoch 26, Batch 470, LR 0.000028 Loss 5.023542, Accuracy 87.437%\n",
      "Epoch 26, Batch 471, LR 0.000028 Loss 5.025275, Accuracy 87.424%\n",
      "Epoch 26, Batch 472, LR 0.000028 Loss 5.025874, Accuracy 87.424%\n",
      "Epoch 26, Batch 473, LR 0.000028 Loss 5.026880, Accuracy 87.417%\n",
      "Epoch 26, Batch 474, LR 0.000028 Loss 5.025286, Accuracy 87.426%\n",
      "Epoch 26, Batch 475, LR 0.000028 Loss 5.025423, Accuracy 87.424%\n",
      "Epoch 26, Batch 476, LR 0.000028 Loss 5.025936, Accuracy 87.425%\n",
      "Epoch 26, Batch 477, LR 0.000028 Loss 5.024983, Accuracy 87.430%\n",
      "Epoch 26, Batch 478, LR 0.000028 Loss 5.022945, Accuracy 87.438%\n",
      "Epoch 26, Batch 479, LR 0.000028 Loss 5.021160, Accuracy 87.449%\n",
      "Epoch 26, Batch 480, LR 0.000028 Loss 5.020560, Accuracy 87.454%\n",
      "Epoch 26, Batch 481, LR 0.000028 Loss 5.020319, Accuracy 87.455%\n",
      "Epoch 26, Batch 482, LR 0.000028 Loss 5.020506, Accuracy 87.451%\n",
      "Epoch 26, Batch 483, LR 0.000028 Loss 5.019715, Accuracy 87.450%\n",
      "Epoch 26, Batch 484, LR 0.000028 Loss 5.019357, Accuracy 87.444%\n",
      "Epoch 26, Batch 485, LR 0.000028 Loss 5.019230, Accuracy 87.450%\n",
      "Epoch 26, Batch 486, LR 0.000028 Loss 5.019584, Accuracy 87.447%\n",
      "Epoch 26, Batch 487, LR 0.000028 Loss 5.017596, Accuracy 87.457%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 488, LR 0.000028 Loss 5.019376, Accuracy 87.455%\n",
      "Epoch 26, Batch 489, LR 0.000028 Loss 5.019213, Accuracy 87.458%\n",
      "Epoch 26, Batch 490, LR 0.000028 Loss 5.018498, Accuracy 87.467%\n",
      "Epoch 26, Batch 491, LR 0.000028 Loss 5.017708, Accuracy 87.473%\n",
      "Epoch 26, Batch 492, LR 0.000028 Loss 5.017775, Accuracy 87.470%\n",
      "Epoch 26, Batch 493, LR 0.000028 Loss 5.016891, Accuracy 87.478%\n",
      "Epoch 26, Batch 494, LR 0.000028 Loss 5.015709, Accuracy 87.479%\n",
      "Epoch 26, Batch 495, LR 0.000028 Loss 5.016394, Accuracy 87.478%\n",
      "Epoch 26, Batch 496, LR 0.000028 Loss 5.014620, Accuracy 87.481%\n",
      "Epoch 26, Batch 497, LR 0.000028 Loss 5.014811, Accuracy 87.480%\n",
      "Epoch 26, Batch 498, LR 0.000028 Loss 5.014194, Accuracy 87.481%\n",
      "Epoch 26, Batch 499, LR 0.000028 Loss 5.013679, Accuracy 87.486%\n",
      "Epoch 26, Batch 500, LR 0.000028 Loss 5.012974, Accuracy 87.486%\n",
      "Epoch 26, Batch 501, LR 0.000028 Loss 5.013757, Accuracy 87.483%\n",
      "Epoch 26, Batch 502, LR 0.000028 Loss 5.013293, Accuracy 87.475%\n",
      "Epoch 26, Batch 503, LR 0.000028 Loss 5.011962, Accuracy 87.489%\n",
      "Epoch 26, Batch 504, LR 0.000028 Loss 5.012348, Accuracy 87.483%\n",
      "Epoch 26, Batch 505, LR 0.000028 Loss 5.013110, Accuracy 87.480%\n",
      "Epoch 26, Batch 506, LR 0.000028 Loss 5.012628, Accuracy 87.483%\n",
      "Epoch 26, Batch 507, LR 0.000028 Loss 5.013013, Accuracy 87.483%\n",
      "Epoch 26, Batch 508, LR 0.000028 Loss 5.012131, Accuracy 87.488%\n",
      "Epoch 26, Batch 509, LR 0.000028 Loss 5.012089, Accuracy 87.491%\n",
      "Epoch 26, Batch 510, LR 0.000028 Loss 5.010336, Accuracy 87.503%\n",
      "Epoch 26, Batch 511, LR 0.000028 Loss 5.011532, Accuracy 87.495%\n",
      "Epoch 26, Batch 512, LR 0.000028 Loss 5.008640, Accuracy 87.511%\n",
      "Epoch 26, Batch 513, LR 0.000028 Loss 5.007832, Accuracy 87.514%\n",
      "Epoch 26, Batch 514, LR 0.000028 Loss 5.007470, Accuracy 87.515%\n",
      "Epoch 26, Batch 515, LR 0.000028 Loss 5.006603, Accuracy 87.523%\n",
      "Epoch 26, Batch 516, LR 0.000028 Loss 5.007311, Accuracy 87.520%\n",
      "Epoch 26, Batch 517, LR 0.000028 Loss 5.006018, Accuracy 87.523%\n",
      "Epoch 26, Batch 518, LR 0.000028 Loss 5.006159, Accuracy 87.521%\n",
      "Epoch 26, Batch 519, LR 0.000028 Loss 5.004614, Accuracy 87.532%\n",
      "Epoch 26, Batch 520, LR 0.000028 Loss 5.004404, Accuracy 87.539%\n",
      "Epoch 26, Batch 521, LR 0.000028 Loss 5.004783, Accuracy 87.540%\n",
      "Epoch 26, Batch 522, LR 0.000028 Loss 5.004347, Accuracy 87.543%\n",
      "Epoch 26, Batch 523, LR 0.000028 Loss 5.004086, Accuracy 87.551%\n",
      "Epoch 26, Batch 524, LR 0.000028 Loss 5.003119, Accuracy 87.563%\n",
      "Epoch 26, Batch 525, LR 0.000028 Loss 5.003140, Accuracy 87.558%\n",
      "Epoch 26, Batch 526, LR 0.000028 Loss 5.001936, Accuracy 87.565%\n",
      "Epoch 26, Batch 527, LR 0.000028 Loss 5.002076, Accuracy 87.568%\n",
      "Epoch 26, Batch 528, LR 0.000028 Loss 5.002842, Accuracy 87.561%\n",
      "Epoch 26, Batch 529, LR 0.000028 Loss 5.001937, Accuracy 87.568%\n",
      "Epoch 26, Batch 530, LR 0.000028 Loss 5.001069, Accuracy 87.568%\n",
      "Epoch 26, Batch 531, LR 0.000028 Loss 5.000177, Accuracy 87.574%\n",
      "Epoch 26, Batch 532, LR 0.000028 Loss 5.000924, Accuracy 87.568%\n",
      "Epoch 26, Batch 533, LR 0.000028 Loss 5.001867, Accuracy 87.564%\n",
      "Epoch 26, Batch 534, LR 0.000028 Loss 5.003106, Accuracy 87.556%\n",
      "Epoch 26, Batch 535, LR 0.000028 Loss 5.003920, Accuracy 87.554%\n",
      "Epoch 26, Batch 536, LR 0.000028 Loss 5.005292, Accuracy 87.551%\n",
      "Epoch 26, Batch 537, LR 0.000028 Loss 5.004594, Accuracy 87.555%\n",
      "Epoch 26, Batch 538, LR 0.000028 Loss 5.002595, Accuracy 87.564%\n",
      "Epoch 26, Batch 539, LR 0.000028 Loss 5.002033, Accuracy 87.565%\n",
      "Epoch 26, Batch 540, LR 0.000028 Loss 5.002525, Accuracy 87.555%\n",
      "Epoch 26, Batch 541, LR 0.000028 Loss 5.002546, Accuracy 87.556%\n",
      "Epoch 26, Batch 542, LR 0.000028 Loss 5.001104, Accuracy 87.571%\n",
      "Epoch 26, Batch 543, LR 0.000028 Loss 5.002848, Accuracy 87.572%\n",
      "Epoch 26, Batch 544, LR 0.000028 Loss 5.001537, Accuracy 87.572%\n",
      "Epoch 26, Batch 545, LR 0.000028 Loss 5.000637, Accuracy 87.570%\n",
      "Epoch 26, Batch 546, LR 0.000028 Loss 4.999656, Accuracy 87.576%\n",
      "Epoch 26, Batch 547, LR 0.000028 Loss 5.000562, Accuracy 87.583%\n",
      "Epoch 26, Batch 548, LR 0.000028 Loss 5.000858, Accuracy 87.578%\n",
      "Epoch 26, Batch 549, LR 0.000028 Loss 5.001394, Accuracy 87.584%\n",
      "Epoch 26, Batch 550, LR 0.000028 Loss 5.002163, Accuracy 87.585%\n",
      "Epoch 26, Batch 551, LR 0.000028 Loss 5.002735, Accuracy 87.581%\n",
      "Epoch 26, Batch 552, LR 0.000028 Loss 5.003269, Accuracy 87.576%\n",
      "Epoch 26, Batch 553, LR 0.000028 Loss 5.002713, Accuracy 87.579%\n",
      "Epoch 26, Batch 554, LR 0.000028 Loss 5.000954, Accuracy 87.582%\n",
      "Epoch 26, Batch 555, LR 0.000028 Loss 5.001233, Accuracy 87.577%\n",
      "Epoch 26, Batch 556, LR 0.000028 Loss 5.002830, Accuracy 87.570%\n",
      "Epoch 26, Batch 557, LR 0.000028 Loss 5.002820, Accuracy 87.574%\n",
      "Epoch 26, Batch 558, LR 0.000028 Loss 5.002161, Accuracy 87.580%\n",
      "Epoch 26, Batch 559, LR 0.000028 Loss 5.001749, Accuracy 87.575%\n",
      "Epoch 26, Batch 560, LR 0.000028 Loss 5.001744, Accuracy 87.581%\n",
      "Epoch 26, Batch 561, LR 0.000028 Loss 5.001429, Accuracy 87.579%\n",
      "Epoch 26, Batch 562, LR 0.000028 Loss 5.001374, Accuracy 87.571%\n",
      "Epoch 26, Batch 563, LR 0.000028 Loss 5.000197, Accuracy 87.575%\n",
      "Epoch 26, Batch 564, LR 0.000028 Loss 4.999941, Accuracy 87.573%\n",
      "Epoch 26, Batch 565, LR 0.000028 Loss 4.999981, Accuracy 87.573%\n",
      "Epoch 26, Batch 566, LR 0.000028 Loss 5.000385, Accuracy 87.568%\n",
      "Epoch 26, Batch 567, LR 0.000028 Loss 5.000820, Accuracy 87.565%\n",
      "Epoch 26, Batch 568, LR 0.000028 Loss 5.001150, Accuracy 87.562%\n",
      "Epoch 26, Batch 569, LR 0.000028 Loss 5.001293, Accuracy 87.560%\n",
      "Epoch 26, Batch 570, LR 0.000028 Loss 5.002732, Accuracy 87.555%\n",
      "Epoch 26, Batch 571, LR 0.000028 Loss 5.002615, Accuracy 87.556%\n",
      "Epoch 26, Batch 572, LR 0.000028 Loss 5.002111, Accuracy 87.559%\n",
      "Epoch 26, Batch 573, LR 0.000028 Loss 5.001920, Accuracy 87.559%\n",
      "Epoch 26, Batch 574, LR 0.000028 Loss 5.001084, Accuracy 87.568%\n",
      "Epoch 26, Batch 575, LR 0.000028 Loss 4.999802, Accuracy 87.573%\n",
      "Epoch 26, Batch 576, LR 0.000028 Loss 4.999505, Accuracy 87.577%\n",
      "Epoch 26, Batch 577, LR 0.000028 Loss 4.999523, Accuracy 87.579%\n",
      "Epoch 26, Batch 578, LR 0.000028 Loss 4.999242, Accuracy 87.581%\n",
      "Epoch 26, Batch 579, LR 0.000028 Loss 5.000380, Accuracy 87.577%\n",
      "Epoch 26, Batch 580, LR 0.000028 Loss 5.000143, Accuracy 87.577%\n",
      "Epoch 26, Batch 581, LR 0.000028 Loss 5.001257, Accuracy 87.571%\n",
      "Epoch 26, Batch 582, LR 0.000028 Loss 5.000745, Accuracy 87.571%\n",
      "Epoch 26, Batch 583, LR 0.000028 Loss 5.000939, Accuracy 87.570%\n",
      "Epoch 26, Batch 584, LR 0.000028 Loss 5.000268, Accuracy 87.574%\n",
      "Epoch 26, Batch 585, LR 0.000028 Loss 5.001387, Accuracy 87.563%\n",
      "Epoch 26, Batch 586, LR 0.000028 Loss 5.002751, Accuracy 87.567%\n",
      "Epoch 26, Batch 587, LR 0.000028 Loss 5.002297, Accuracy 87.567%\n",
      "Epoch 26, Batch 588, LR 0.000028 Loss 5.001111, Accuracy 87.572%\n",
      "Epoch 26, Batch 589, LR 0.000028 Loss 5.001903, Accuracy 87.568%\n",
      "Epoch 26, Batch 590, LR 0.000028 Loss 5.002403, Accuracy 87.565%\n",
      "Epoch 26, Batch 591, LR 0.000028 Loss 5.001345, Accuracy 87.570%\n",
      "Epoch 26, Batch 592, LR 0.000028 Loss 5.000100, Accuracy 87.574%\n",
      "Epoch 26, Batch 593, LR 0.000028 Loss 4.999381, Accuracy 87.574%\n",
      "Epoch 26, Batch 594, LR 0.000028 Loss 5.000944, Accuracy 87.564%\n",
      "Epoch 26, Batch 595, LR 0.000028 Loss 5.000650, Accuracy 87.562%\n",
      "Epoch 26, Batch 596, LR 0.000028 Loss 5.001258, Accuracy 87.558%\n",
      "Epoch 26, Batch 597, LR 0.000028 Loss 5.001360, Accuracy 87.560%\n",
      "Epoch 26, Batch 598, LR 0.000028 Loss 5.001400, Accuracy 87.561%\n",
      "Epoch 26, Batch 599, LR 0.000028 Loss 5.000884, Accuracy 87.564%\n",
      "Epoch 26, Batch 600, LR 0.000028 Loss 5.000310, Accuracy 87.568%\n",
      "Epoch 26, Batch 601, LR 0.000028 Loss 5.002296, Accuracy 87.558%\n",
      "Epoch 26, Batch 602, LR 0.000028 Loss 5.002537, Accuracy 87.553%\n",
      "Epoch 26, Batch 603, LR 0.000028 Loss 5.001447, Accuracy 87.556%\n",
      "Epoch 26, Batch 604, LR 0.000028 Loss 5.000850, Accuracy 87.556%\n",
      "Epoch 26, Batch 605, LR 0.000028 Loss 5.001359, Accuracy 87.559%\n",
      "Epoch 26, Batch 606, LR 0.000028 Loss 5.001289, Accuracy 87.566%\n",
      "Epoch 26, Batch 607, LR 0.000028 Loss 5.000290, Accuracy 87.570%\n",
      "Epoch 26, Batch 608, LR 0.000028 Loss 5.001362, Accuracy 87.563%\n",
      "Epoch 26, Batch 609, LR 0.000028 Loss 5.000802, Accuracy 87.568%\n",
      "Epoch 26, Batch 610, LR 0.000028 Loss 4.999777, Accuracy 87.568%\n",
      "Epoch 26, Batch 611, LR 0.000028 Loss 4.999171, Accuracy 87.572%\n",
      "Epoch 26, Batch 612, LR 0.000028 Loss 4.999059, Accuracy 87.570%\n",
      "Epoch 26, Batch 613, LR 0.000028 Loss 5.000410, Accuracy 87.565%\n",
      "Epoch 26, Batch 614, LR 0.000028 Loss 5.000880, Accuracy 87.566%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 615, LR 0.000028 Loss 5.001453, Accuracy 87.567%\n",
      "Epoch 26, Batch 616, LR 0.000028 Loss 5.002125, Accuracy 87.566%\n",
      "Epoch 26, Batch 617, LR 0.000028 Loss 5.000940, Accuracy 87.571%\n",
      "Epoch 26, Batch 618, LR 0.000028 Loss 5.001433, Accuracy 87.568%\n",
      "Epoch 26, Batch 619, LR 0.000028 Loss 5.001439, Accuracy 87.561%\n",
      "Epoch 26, Batch 620, LR 0.000028 Loss 5.001710, Accuracy 87.559%\n",
      "Epoch 26, Batch 621, LR 0.000028 Loss 5.001896, Accuracy 87.555%\n",
      "Epoch 26, Batch 622, LR 0.000028 Loss 5.001623, Accuracy 87.567%\n",
      "Epoch 26, Batch 623, LR 0.000028 Loss 5.000268, Accuracy 87.570%\n",
      "Epoch 26, Batch 624, LR 0.000028 Loss 4.999893, Accuracy 87.570%\n",
      "Epoch 26, Batch 625, LR 0.000028 Loss 5.000546, Accuracy 87.564%\n",
      "Epoch 26, Batch 626, LR 0.000028 Loss 5.000289, Accuracy 87.562%\n",
      "Epoch 26, Batch 627, LR 0.000028 Loss 5.000092, Accuracy 87.566%\n",
      "Epoch 26, Batch 628, LR 0.000028 Loss 5.000229, Accuracy 87.567%\n",
      "Epoch 26, Batch 629, LR 0.000028 Loss 5.002056, Accuracy 87.561%\n",
      "Epoch 26, Batch 630, LR 0.000028 Loss 5.001276, Accuracy 87.567%\n",
      "Epoch 26, Batch 631, LR 0.000028 Loss 5.001621, Accuracy 87.557%\n",
      "Epoch 26, Batch 632, LR 0.000028 Loss 5.000799, Accuracy 87.562%\n",
      "Epoch 26, Batch 633, LR 0.000028 Loss 5.000223, Accuracy 87.559%\n",
      "Epoch 26, Batch 634, LR 0.000028 Loss 5.000523, Accuracy 87.554%\n",
      "Epoch 26, Batch 635, LR 0.000028 Loss 5.001295, Accuracy 87.546%\n",
      "Epoch 26, Batch 636, LR 0.000028 Loss 5.001300, Accuracy 87.545%\n",
      "Epoch 26, Batch 637, LR 0.000028 Loss 5.000709, Accuracy 87.550%\n",
      "Epoch 26, Batch 638, LR 0.000028 Loss 5.000364, Accuracy 87.549%\n",
      "Epoch 26, Batch 639, LR 0.000028 Loss 5.001075, Accuracy 87.535%\n",
      "Epoch 26, Batch 640, LR 0.000028 Loss 5.000956, Accuracy 87.533%\n",
      "Epoch 26, Batch 641, LR 0.000028 Loss 5.000013, Accuracy 87.534%\n",
      "Epoch 26, Batch 642, LR 0.000028 Loss 5.001471, Accuracy 87.523%\n",
      "Epoch 26, Batch 643, LR 0.000028 Loss 5.002136, Accuracy 87.519%\n",
      "Epoch 26, Batch 644, LR 0.000028 Loss 5.001069, Accuracy 87.525%\n",
      "Epoch 26, Batch 645, LR 0.000028 Loss 5.000967, Accuracy 87.523%\n",
      "Epoch 26, Batch 646, LR 0.000028 Loss 5.000438, Accuracy 87.527%\n",
      "Epoch 26, Batch 647, LR 0.000028 Loss 5.000855, Accuracy 87.523%\n",
      "Epoch 26, Batch 648, LR 0.000028 Loss 5.000976, Accuracy 87.520%\n",
      "Epoch 26, Batch 649, LR 0.000028 Loss 5.001140, Accuracy 87.519%\n",
      "Epoch 26, Batch 650, LR 0.000028 Loss 5.000653, Accuracy 87.530%\n",
      "Epoch 26, Batch 651, LR 0.000028 Loss 5.000902, Accuracy 87.525%\n",
      "Epoch 26, Batch 652, LR 0.000028 Loss 5.001370, Accuracy 87.522%\n",
      "Epoch 26, Batch 653, LR 0.000028 Loss 5.001347, Accuracy 87.522%\n",
      "Epoch 26, Batch 654, LR 0.000028 Loss 5.002003, Accuracy 87.522%\n",
      "Epoch 26, Batch 655, LR 0.000028 Loss 5.002331, Accuracy 87.516%\n",
      "Epoch 26, Batch 656, LR 0.000028 Loss 5.001318, Accuracy 87.521%\n",
      "Epoch 26, Batch 657, LR 0.000028 Loss 5.001859, Accuracy 87.518%\n",
      "Epoch 26, Batch 658, LR 0.000028 Loss 5.002612, Accuracy 87.515%\n",
      "Epoch 26, Batch 659, LR 0.000028 Loss 5.003438, Accuracy 87.511%\n",
      "Epoch 26, Batch 660, LR 0.000028 Loss 5.003240, Accuracy 87.508%\n",
      "Epoch 26, Batch 661, LR 0.000028 Loss 5.002684, Accuracy 87.508%\n",
      "Epoch 26, Batch 662, LR 0.000028 Loss 5.002127, Accuracy 87.513%\n",
      "Epoch 26, Batch 663, LR 0.000028 Loss 5.001751, Accuracy 87.516%\n",
      "Epoch 26, Batch 664, LR 0.000028 Loss 5.001099, Accuracy 87.520%\n",
      "Epoch 26, Batch 665, LR 0.000028 Loss 5.000906, Accuracy 87.526%\n",
      "Epoch 26, Batch 666, LR 0.000028 Loss 5.000628, Accuracy 87.527%\n",
      "Epoch 26, Batch 667, LR 0.000028 Loss 5.001052, Accuracy 87.528%\n",
      "Epoch 26, Batch 668, LR 0.000028 Loss 4.999135, Accuracy 87.535%\n",
      "Epoch 26, Batch 669, LR 0.000028 Loss 4.998955, Accuracy 87.539%\n",
      "Epoch 26, Batch 670, LR 0.000028 Loss 4.999525, Accuracy 87.534%\n",
      "Epoch 26, Batch 671, LR 0.000028 Loss 4.998365, Accuracy 87.542%\n",
      "Epoch 26, Batch 672, LR 0.000028 Loss 4.998425, Accuracy 87.544%\n",
      "Epoch 26, Batch 673, LR 0.000028 Loss 4.998650, Accuracy 87.543%\n",
      "Epoch 26, Batch 674, LR 0.000028 Loss 4.997616, Accuracy 87.550%\n",
      "Epoch 26, Batch 675, LR 0.000028 Loss 4.996934, Accuracy 87.556%\n",
      "Epoch 26, Batch 676, LR 0.000028 Loss 4.996772, Accuracy 87.558%\n",
      "Epoch 26, Batch 677, LR 0.000027 Loss 4.996211, Accuracy 87.558%\n",
      "Epoch 26, Batch 678, LR 0.000027 Loss 4.996957, Accuracy 87.550%\n",
      "Epoch 26, Batch 679, LR 0.000027 Loss 4.996209, Accuracy 87.554%\n",
      "Epoch 26, Batch 680, LR 0.000027 Loss 4.995705, Accuracy 87.553%\n",
      "Epoch 26, Batch 681, LR 0.000027 Loss 4.995763, Accuracy 87.557%\n",
      "Epoch 26, Batch 682, LR 0.000027 Loss 4.995783, Accuracy 87.558%\n",
      "Epoch 26, Batch 683, LR 0.000027 Loss 4.995389, Accuracy 87.558%\n",
      "Epoch 26, Batch 684, LR 0.000027 Loss 4.995943, Accuracy 87.554%\n",
      "Epoch 26, Batch 685, LR 0.000027 Loss 4.996270, Accuracy 87.551%\n",
      "Epoch 26, Batch 686, LR 0.000027 Loss 4.995493, Accuracy 87.551%\n",
      "Epoch 26, Batch 687, LR 0.000027 Loss 4.995719, Accuracy 87.547%\n",
      "Epoch 26, Batch 688, LR 0.000027 Loss 4.995451, Accuracy 87.545%\n",
      "Epoch 26, Batch 689, LR 0.000027 Loss 4.996903, Accuracy 87.532%\n",
      "Epoch 26, Batch 690, LR 0.000027 Loss 4.996777, Accuracy 87.529%\n",
      "Epoch 26, Batch 691, LR 0.000027 Loss 4.997044, Accuracy 87.527%\n",
      "Epoch 26, Batch 692, LR 0.000027 Loss 4.997098, Accuracy 87.523%\n",
      "Epoch 26, Batch 693, LR 0.000027 Loss 4.997410, Accuracy 87.524%\n",
      "Epoch 26, Batch 694, LR 0.000027 Loss 4.997110, Accuracy 87.520%\n",
      "Epoch 26, Batch 695, LR 0.000027 Loss 4.997310, Accuracy 87.516%\n",
      "Epoch 26, Batch 696, LR 0.000027 Loss 4.997778, Accuracy 87.517%\n",
      "Epoch 26, Batch 697, LR 0.000027 Loss 4.997881, Accuracy 87.519%\n",
      "Epoch 26, Batch 698, LR 0.000027 Loss 4.997270, Accuracy 87.524%\n",
      "Epoch 26, Batch 699, LR 0.000027 Loss 4.996227, Accuracy 87.531%\n",
      "Epoch 26, Batch 700, LR 0.000027 Loss 4.997402, Accuracy 87.526%\n",
      "Epoch 26, Batch 701, LR 0.000027 Loss 4.995696, Accuracy 87.533%\n",
      "Epoch 26, Batch 702, LR 0.000027 Loss 4.994267, Accuracy 87.540%\n",
      "Epoch 26, Batch 703, LR 0.000027 Loss 4.995072, Accuracy 87.530%\n",
      "Epoch 26, Batch 704, LR 0.000027 Loss 4.995707, Accuracy 87.524%\n",
      "Epoch 26, Batch 705, LR 0.000027 Loss 4.995106, Accuracy 87.527%\n",
      "Epoch 26, Batch 706, LR 0.000027 Loss 4.994980, Accuracy 87.525%\n",
      "Epoch 26, Batch 707, LR 0.000027 Loss 4.994638, Accuracy 87.530%\n",
      "Epoch 26, Batch 708, LR 0.000027 Loss 4.993896, Accuracy 87.530%\n",
      "Epoch 26, Batch 709, LR 0.000027 Loss 4.993433, Accuracy 87.533%\n",
      "Epoch 26, Batch 710, LR 0.000027 Loss 4.993905, Accuracy 87.532%\n",
      "Epoch 26, Batch 711, LR 0.000027 Loss 4.994306, Accuracy 87.529%\n",
      "Epoch 26, Batch 712, LR 0.000027 Loss 4.994471, Accuracy 87.529%\n",
      "Epoch 26, Batch 713, LR 0.000027 Loss 4.994305, Accuracy 87.532%\n",
      "Epoch 26, Batch 714, LR 0.000027 Loss 4.993581, Accuracy 87.538%\n",
      "Epoch 26, Batch 715, LR 0.000027 Loss 4.993790, Accuracy 87.536%\n",
      "Epoch 26, Batch 716, LR 0.000027 Loss 4.992844, Accuracy 87.539%\n",
      "Epoch 26, Batch 717, LR 0.000027 Loss 4.992463, Accuracy 87.541%\n",
      "Epoch 26, Batch 718, LR 0.000027 Loss 4.992274, Accuracy 87.542%\n",
      "Epoch 26, Batch 719, LR 0.000027 Loss 4.992224, Accuracy 87.541%\n",
      "Epoch 26, Batch 720, LR 0.000027 Loss 4.993990, Accuracy 87.530%\n",
      "Epoch 26, Batch 721, LR 0.000027 Loss 4.994589, Accuracy 87.527%\n",
      "Epoch 26, Batch 722, LR 0.000027 Loss 4.995032, Accuracy 87.527%\n",
      "Epoch 26, Batch 723, LR 0.000027 Loss 4.994890, Accuracy 87.530%\n",
      "Epoch 26, Batch 724, LR 0.000027 Loss 4.995589, Accuracy 87.529%\n",
      "Epoch 26, Batch 725, LR 0.000027 Loss 4.995833, Accuracy 87.529%\n",
      "Epoch 26, Batch 726, LR 0.000027 Loss 4.995511, Accuracy 87.531%\n",
      "Epoch 26, Batch 727, LR 0.000027 Loss 4.994375, Accuracy 87.539%\n",
      "Epoch 26, Batch 728, LR 0.000027 Loss 4.993482, Accuracy 87.540%\n",
      "Epoch 26, Batch 729, LR 0.000027 Loss 4.993086, Accuracy 87.545%\n",
      "Epoch 26, Batch 730, LR 0.000027 Loss 4.992484, Accuracy 87.540%\n",
      "Epoch 26, Batch 731, LR 0.000027 Loss 4.991879, Accuracy 87.541%\n",
      "Epoch 26, Batch 732, LR 0.000027 Loss 4.991996, Accuracy 87.539%\n",
      "Epoch 26, Batch 733, LR 0.000027 Loss 4.992265, Accuracy 87.543%\n",
      "Epoch 26, Batch 734, LR 0.000027 Loss 4.992345, Accuracy 87.539%\n",
      "Epoch 26, Batch 735, LR 0.000027 Loss 4.991136, Accuracy 87.546%\n",
      "Epoch 26, Batch 736, LR 0.000027 Loss 4.989759, Accuracy 87.555%\n",
      "Epoch 26, Batch 737, LR 0.000027 Loss 4.990053, Accuracy 87.548%\n",
      "Epoch 26, Batch 738, LR 0.000027 Loss 4.990295, Accuracy 87.547%\n",
      "Epoch 26, Batch 739, LR 0.000027 Loss 4.990342, Accuracy 87.544%\n",
      "Epoch 26, Batch 740, LR 0.000027 Loss 4.990690, Accuracy 87.546%\n",
      "Epoch 26, Batch 741, LR 0.000027 Loss 4.989712, Accuracy 87.552%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 742, LR 0.000027 Loss 4.989340, Accuracy 87.553%\n",
      "Epoch 26, Batch 743, LR 0.000027 Loss 4.989331, Accuracy 87.556%\n",
      "Epoch 26, Batch 744, LR 0.000027 Loss 4.989102, Accuracy 87.562%\n",
      "Epoch 26, Batch 745, LR 0.000027 Loss 4.988799, Accuracy 87.560%\n",
      "Epoch 26, Batch 746, LR 0.000027 Loss 4.989103, Accuracy 87.563%\n",
      "Epoch 26, Batch 747, LR 0.000027 Loss 4.989321, Accuracy 87.560%\n",
      "Epoch 26, Batch 748, LR 0.000027 Loss 4.988999, Accuracy 87.558%\n",
      "Epoch 26, Batch 749, LR 0.000027 Loss 4.989043, Accuracy 87.553%\n",
      "Epoch 26, Batch 750, LR 0.000027 Loss 4.988866, Accuracy 87.553%\n",
      "Epoch 26, Batch 751, LR 0.000027 Loss 4.987946, Accuracy 87.554%\n",
      "Epoch 26, Batch 752, LR 0.000027 Loss 4.987627, Accuracy 87.559%\n",
      "Epoch 26, Batch 753, LR 0.000027 Loss 4.987564, Accuracy 87.553%\n",
      "Epoch 26, Batch 754, LR 0.000027 Loss 4.986619, Accuracy 87.556%\n",
      "Epoch 26, Batch 755, LR 0.000027 Loss 4.987889, Accuracy 87.553%\n",
      "Epoch 26, Batch 756, LR 0.000027 Loss 4.987468, Accuracy 87.551%\n",
      "Epoch 26, Batch 757, LR 0.000027 Loss 4.987478, Accuracy 87.546%\n",
      "Epoch 26, Batch 758, LR 0.000027 Loss 4.987610, Accuracy 87.547%\n",
      "Epoch 26, Batch 759, LR 0.000027 Loss 4.987062, Accuracy 87.547%\n",
      "Epoch 26, Batch 760, LR 0.000027 Loss 4.987427, Accuracy 87.547%\n",
      "Epoch 26, Batch 761, LR 0.000027 Loss 4.987532, Accuracy 87.551%\n",
      "Epoch 26, Batch 762, LR 0.000027 Loss 4.988598, Accuracy 87.544%\n",
      "Epoch 26, Batch 763, LR 0.000027 Loss 4.988120, Accuracy 87.546%\n",
      "Epoch 26, Batch 764, LR 0.000027 Loss 4.987765, Accuracy 87.549%\n",
      "Epoch 26, Batch 765, LR 0.000027 Loss 4.987617, Accuracy 87.551%\n",
      "Epoch 26, Batch 766, LR 0.000027 Loss 4.987788, Accuracy 87.547%\n",
      "Epoch 26, Batch 767, LR 0.000027 Loss 4.987023, Accuracy 87.550%\n",
      "Epoch 26, Batch 768, LR 0.000027 Loss 4.987251, Accuracy 87.552%\n",
      "Epoch 26, Batch 769, LR 0.000027 Loss 4.986946, Accuracy 87.554%\n",
      "Epoch 26, Batch 770, LR 0.000027 Loss 4.987220, Accuracy 87.555%\n",
      "Epoch 26, Batch 771, LR 0.000027 Loss 4.987493, Accuracy 87.556%\n",
      "Epoch 26, Batch 772, LR 0.000027 Loss 4.987122, Accuracy 87.558%\n",
      "Epoch 26, Batch 773, LR 0.000027 Loss 4.986355, Accuracy 87.558%\n",
      "Epoch 26, Batch 774, LR 0.000027 Loss 4.987086, Accuracy 87.555%\n",
      "Epoch 26, Batch 775, LR 0.000027 Loss 4.987011, Accuracy 87.554%\n",
      "Epoch 26, Batch 776, LR 0.000027 Loss 4.987113, Accuracy 87.548%\n",
      "Epoch 26, Batch 777, LR 0.000027 Loss 4.987007, Accuracy 87.548%\n",
      "Epoch 26, Batch 778, LR 0.000027 Loss 4.987845, Accuracy 87.549%\n",
      "Epoch 26, Batch 779, LR 0.000027 Loss 4.988551, Accuracy 87.548%\n",
      "Epoch 26, Batch 780, LR 0.000027 Loss 4.988445, Accuracy 87.545%\n",
      "Epoch 26, Batch 781, LR 0.000027 Loss 4.989539, Accuracy 87.539%\n",
      "Epoch 26, Batch 782, LR 0.000027 Loss 4.989641, Accuracy 87.538%\n",
      "Epoch 26, Batch 783, LR 0.000027 Loss 4.989274, Accuracy 87.539%\n",
      "Epoch 26, Batch 784, LR 0.000027 Loss 4.989476, Accuracy 87.540%\n",
      "Epoch 26, Batch 785, LR 0.000027 Loss 4.990272, Accuracy 87.538%\n",
      "Epoch 26, Batch 786, LR 0.000027 Loss 4.991035, Accuracy 87.537%\n",
      "Epoch 26, Batch 787, LR 0.000027 Loss 4.990098, Accuracy 87.539%\n",
      "Epoch 26, Batch 788, LR 0.000027 Loss 4.989736, Accuracy 87.543%\n",
      "Epoch 26, Batch 789, LR 0.000027 Loss 4.990094, Accuracy 87.540%\n",
      "Epoch 26, Batch 790, LR 0.000027 Loss 4.990502, Accuracy 87.541%\n",
      "Epoch 26, Batch 791, LR 0.000027 Loss 4.990129, Accuracy 87.539%\n",
      "Epoch 26, Batch 792, LR 0.000027 Loss 4.990455, Accuracy 87.537%\n",
      "Epoch 26, Batch 793, LR 0.000027 Loss 4.992026, Accuracy 87.531%\n",
      "Epoch 26, Batch 794, LR 0.000027 Loss 4.991631, Accuracy 87.534%\n",
      "Epoch 26, Batch 795, LR 0.000027 Loss 4.992205, Accuracy 87.534%\n",
      "Epoch 26, Batch 796, LR 0.000027 Loss 4.992249, Accuracy 87.535%\n",
      "Epoch 26, Batch 797, LR 0.000027 Loss 4.991936, Accuracy 87.538%\n",
      "Epoch 26, Batch 798, LR 0.000027 Loss 4.992036, Accuracy 87.539%\n",
      "Epoch 26, Batch 799, LR 0.000027 Loss 4.991832, Accuracy 87.539%\n",
      "Epoch 26, Batch 800, LR 0.000027 Loss 4.991780, Accuracy 87.541%\n",
      "Epoch 26, Batch 801, LR 0.000027 Loss 4.991387, Accuracy 87.547%\n",
      "Epoch 26, Batch 802, LR 0.000027 Loss 4.990918, Accuracy 87.550%\n",
      "Epoch 26, Batch 803, LR 0.000027 Loss 4.991672, Accuracy 87.549%\n",
      "Epoch 26, Batch 804, LR 0.000027 Loss 4.992496, Accuracy 87.543%\n",
      "Epoch 26, Batch 805, LR 0.000027 Loss 4.992000, Accuracy 87.549%\n",
      "Epoch 26, Batch 806, LR 0.000027 Loss 4.992235, Accuracy 87.547%\n",
      "Epoch 26, Batch 807, LR 0.000027 Loss 4.992714, Accuracy 87.545%\n",
      "Epoch 26, Batch 808, LR 0.000027 Loss 4.992586, Accuracy 87.548%\n",
      "Epoch 26, Batch 809, LR 0.000027 Loss 4.992031, Accuracy 87.548%\n",
      "Epoch 26, Batch 810, LR 0.000027 Loss 4.991950, Accuracy 87.545%\n",
      "Epoch 26, Batch 811, LR 0.000027 Loss 4.991459, Accuracy 87.549%\n",
      "Epoch 26, Batch 812, LR 0.000027 Loss 4.991697, Accuracy 87.545%\n",
      "Epoch 26, Batch 813, LR 0.000027 Loss 4.991072, Accuracy 87.546%\n",
      "Epoch 26, Batch 814, LR 0.000027 Loss 4.990866, Accuracy 87.541%\n",
      "Epoch 26, Batch 815, LR 0.000027 Loss 4.990479, Accuracy 87.543%\n",
      "Epoch 26, Batch 816, LR 0.000027 Loss 4.990368, Accuracy 87.542%\n",
      "Epoch 26, Batch 817, LR 0.000027 Loss 4.991496, Accuracy 87.536%\n",
      "Epoch 26, Batch 818, LR 0.000027 Loss 4.991568, Accuracy 87.538%\n",
      "Epoch 26, Batch 819, LR 0.000027 Loss 4.991267, Accuracy 87.543%\n",
      "Epoch 26, Batch 820, LR 0.000027 Loss 4.991705, Accuracy 87.538%\n",
      "Epoch 26, Batch 821, LR 0.000027 Loss 4.991460, Accuracy 87.535%\n",
      "Epoch 26, Batch 822, LR 0.000027 Loss 4.991332, Accuracy 87.535%\n",
      "Epoch 26, Batch 823, LR 0.000027 Loss 4.991305, Accuracy 87.539%\n",
      "Epoch 26, Batch 824, LR 0.000027 Loss 4.991705, Accuracy 87.537%\n",
      "Epoch 26, Batch 825, LR 0.000027 Loss 4.991144, Accuracy 87.540%\n",
      "Epoch 26, Batch 826, LR 0.000027 Loss 4.991103, Accuracy 87.540%\n",
      "Epoch 26, Batch 827, LR 0.000027 Loss 4.991066, Accuracy 87.542%\n",
      "Epoch 26, Batch 828, LR 0.000027 Loss 4.990815, Accuracy 87.542%\n",
      "Epoch 26, Batch 829, LR 0.000027 Loss 4.990992, Accuracy 87.543%\n",
      "Epoch 26, Batch 830, LR 0.000027 Loss 4.990926, Accuracy 87.551%\n",
      "Epoch 26, Batch 831, LR 0.000027 Loss 4.991369, Accuracy 87.547%\n",
      "Epoch 26, Batch 832, LR 0.000027 Loss 4.991438, Accuracy 87.550%\n",
      "Epoch 26, Batch 833, LR 0.000027 Loss 4.991845, Accuracy 87.552%\n",
      "Epoch 26, Batch 834, LR 0.000027 Loss 4.991473, Accuracy 87.550%\n",
      "Epoch 26, Batch 835, LR 0.000027 Loss 4.990529, Accuracy 87.551%\n",
      "Epoch 26, Batch 836, LR 0.000027 Loss 4.989986, Accuracy 87.553%\n",
      "Epoch 26, Batch 837, LR 0.000027 Loss 4.989497, Accuracy 87.553%\n",
      "Epoch 26, Batch 838, LR 0.000027 Loss 4.988854, Accuracy 87.558%\n",
      "Epoch 26, Batch 839, LR 0.000027 Loss 4.987998, Accuracy 87.562%\n",
      "Epoch 26, Batch 840, LR 0.000027 Loss 4.987742, Accuracy 87.564%\n",
      "Epoch 26, Batch 841, LR 0.000027 Loss 4.988975, Accuracy 87.556%\n",
      "Epoch 26, Batch 842, LR 0.000027 Loss 4.988764, Accuracy 87.554%\n",
      "Epoch 26, Batch 843, LR 0.000027 Loss 4.988403, Accuracy 87.556%\n",
      "Epoch 26, Batch 844, LR 0.000027 Loss 4.988337, Accuracy 87.555%\n",
      "Epoch 26, Batch 845, LR 0.000027 Loss 4.987951, Accuracy 87.553%\n",
      "Epoch 26, Batch 846, LR 0.000027 Loss 4.987005, Accuracy 87.558%\n",
      "Epoch 26, Batch 847, LR 0.000027 Loss 4.986766, Accuracy 87.560%\n",
      "Epoch 26, Batch 848, LR 0.000027 Loss 4.986751, Accuracy 87.564%\n",
      "Epoch 26, Batch 849, LR 0.000027 Loss 4.987000, Accuracy 87.565%\n",
      "Epoch 26, Batch 850, LR 0.000027 Loss 4.986850, Accuracy 87.567%\n",
      "Epoch 26, Batch 851, LR 0.000027 Loss 4.987254, Accuracy 87.561%\n",
      "Epoch 26, Batch 852, LR 0.000027 Loss 4.987942, Accuracy 87.553%\n",
      "Epoch 26, Batch 853, LR 0.000027 Loss 4.987701, Accuracy 87.551%\n",
      "Epoch 26, Batch 854, LR 0.000027 Loss 4.987341, Accuracy 87.553%\n",
      "Epoch 26, Batch 855, LR 0.000027 Loss 4.987412, Accuracy 87.555%\n",
      "Epoch 26, Batch 856, LR 0.000027 Loss 4.987303, Accuracy 87.556%\n",
      "Epoch 26, Batch 857, LR 0.000027 Loss 4.986831, Accuracy 87.563%\n",
      "Epoch 26, Batch 858, LR 0.000027 Loss 4.987072, Accuracy 87.564%\n",
      "Epoch 26, Batch 859, LR 0.000027 Loss 4.987129, Accuracy 87.561%\n",
      "Epoch 26, Batch 860, LR 0.000027 Loss 4.987365, Accuracy 87.566%\n",
      "Epoch 26, Batch 861, LR 0.000027 Loss 4.986153, Accuracy 87.571%\n",
      "Epoch 26, Batch 862, LR 0.000027 Loss 4.986060, Accuracy 87.572%\n",
      "Epoch 26, Batch 863, LR 0.000027 Loss 4.986334, Accuracy 87.570%\n",
      "Epoch 26, Batch 864, LR 0.000027 Loss 4.987857, Accuracy 87.563%\n",
      "Epoch 26, Batch 865, LR 0.000027 Loss 4.988086, Accuracy 87.560%\n",
      "Epoch 26, Batch 866, LR 0.000027 Loss 4.987982, Accuracy 87.559%\n",
      "Epoch 26, Batch 867, LR 0.000027 Loss 4.988214, Accuracy 87.557%\n",
      "Epoch 26, Batch 868, LR 0.000027 Loss 4.988308, Accuracy 87.554%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 869, LR 0.000027 Loss 4.988120, Accuracy 87.554%\n",
      "Epoch 26, Batch 870, LR 0.000027 Loss 4.988635, Accuracy 87.550%\n",
      "Epoch 26, Batch 871, LR 0.000027 Loss 4.988114, Accuracy 87.549%\n",
      "Epoch 26, Batch 872, LR 0.000027 Loss 4.987197, Accuracy 87.553%\n",
      "Epoch 26, Batch 873, LR 0.000027 Loss 4.986748, Accuracy 87.555%\n",
      "Epoch 26, Batch 874, LR 0.000027 Loss 4.985920, Accuracy 87.558%\n",
      "Epoch 26, Batch 875, LR 0.000027 Loss 4.985621, Accuracy 87.558%\n",
      "Epoch 26, Batch 876, LR 0.000027 Loss 4.985975, Accuracy 87.557%\n",
      "Epoch 26, Batch 877, LR 0.000027 Loss 4.986140, Accuracy 87.554%\n",
      "Epoch 26, Batch 878, LR 0.000027 Loss 4.986254, Accuracy 87.556%\n",
      "Epoch 26, Batch 879, LR 0.000027 Loss 4.985864, Accuracy 87.562%\n",
      "Epoch 26, Batch 880, LR 0.000027 Loss 4.984921, Accuracy 87.569%\n",
      "Epoch 26, Batch 881, LR 0.000027 Loss 4.985112, Accuracy 87.567%\n",
      "Epoch 26, Batch 882, LR 0.000027 Loss 4.984706, Accuracy 87.568%\n",
      "Epoch 26, Batch 883, LR 0.000027 Loss 4.984663, Accuracy 87.571%\n",
      "Epoch 26, Batch 884, LR 0.000027 Loss 4.984264, Accuracy 87.579%\n",
      "Epoch 26, Batch 885, LR 0.000027 Loss 4.985075, Accuracy 87.572%\n",
      "Epoch 26, Batch 886, LR 0.000027 Loss 4.985006, Accuracy 87.575%\n",
      "Epoch 26, Batch 887, LR 0.000027 Loss 4.985863, Accuracy 87.571%\n",
      "Epoch 26, Batch 888, LR 0.000027 Loss 4.985758, Accuracy 87.576%\n",
      "Epoch 26, Batch 889, LR 0.000027 Loss 4.985248, Accuracy 87.579%\n",
      "Epoch 26, Batch 890, LR 0.000027 Loss 4.984748, Accuracy 87.582%\n",
      "Epoch 26, Batch 891, LR 0.000027 Loss 4.984374, Accuracy 87.583%\n",
      "Epoch 26, Batch 892, LR 0.000027 Loss 4.984116, Accuracy 87.585%\n",
      "Epoch 26, Batch 893, LR 0.000027 Loss 4.984688, Accuracy 87.581%\n",
      "Epoch 26, Batch 894, LR 0.000027 Loss 4.984807, Accuracy 87.579%\n",
      "Epoch 26, Batch 895, LR 0.000027 Loss 4.984525, Accuracy 87.582%\n",
      "Epoch 26, Batch 896, LR 0.000027 Loss 4.984526, Accuracy 87.581%\n",
      "Epoch 26, Batch 897, LR 0.000027 Loss 4.984160, Accuracy 87.583%\n",
      "Epoch 26, Batch 898, LR 0.000027 Loss 4.983778, Accuracy 87.586%\n",
      "Epoch 26, Batch 899, LR 0.000027 Loss 4.983087, Accuracy 87.587%\n",
      "Epoch 26, Batch 900, LR 0.000027 Loss 4.983170, Accuracy 87.588%\n",
      "Epoch 26, Batch 901, LR 0.000027 Loss 4.983336, Accuracy 87.588%\n",
      "Epoch 26, Batch 902, LR 0.000027 Loss 4.983314, Accuracy 87.593%\n",
      "Epoch 26, Batch 903, LR 0.000027 Loss 4.984321, Accuracy 87.584%\n",
      "Epoch 26, Batch 904, LR 0.000027 Loss 4.984459, Accuracy 87.585%\n",
      "Epoch 26, Batch 905, LR 0.000027 Loss 4.984311, Accuracy 87.588%\n",
      "Epoch 26, Batch 906, LR 0.000027 Loss 4.984951, Accuracy 87.581%\n",
      "Epoch 26, Batch 907, LR 0.000027 Loss 4.985135, Accuracy 87.583%\n",
      "Epoch 26, Batch 908, LR 0.000027 Loss 4.984434, Accuracy 87.588%\n",
      "Epoch 26, Batch 909, LR 0.000027 Loss 4.984280, Accuracy 87.588%\n",
      "Epoch 26, Batch 910, LR 0.000027 Loss 4.983273, Accuracy 87.593%\n",
      "Epoch 26, Batch 911, LR 0.000027 Loss 4.983192, Accuracy 87.595%\n",
      "Epoch 26, Batch 912, LR 0.000027 Loss 4.982550, Accuracy 87.599%\n",
      "Epoch 26, Batch 913, LR 0.000027 Loss 4.982548, Accuracy 87.598%\n",
      "Epoch 26, Batch 914, LR 0.000027 Loss 4.982008, Accuracy 87.596%\n",
      "Epoch 26, Batch 915, LR 0.000027 Loss 4.982270, Accuracy 87.595%\n",
      "Epoch 26, Batch 916, LR 0.000027 Loss 4.982355, Accuracy 87.592%\n",
      "Epoch 26, Batch 917, LR 0.000027 Loss 4.982187, Accuracy 87.591%\n",
      "Epoch 26, Batch 918, LR 0.000027 Loss 4.982810, Accuracy 87.589%\n",
      "Epoch 26, Batch 919, LR 0.000027 Loss 4.982577, Accuracy 87.592%\n",
      "Epoch 26, Batch 920, LR 0.000027 Loss 4.983553, Accuracy 87.589%\n",
      "Epoch 26, Batch 921, LR 0.000027 Loss 4.983735, Accuracy 87.587%\n",
      "Epoch 26, Batch 922, LR 0.000027 Loss 4.984498, Accuracy 87.587%\n",
      "Epoch 26, Batch 923, LR 0.000027 Loss 4.983942, Accuracy 87.587%\n",
      "Epoch 26, Batch 924, LR 0.000027 Loss 4.984180, Accuracy 87.586%\n",
      "Epoch 26, Batch 925, LR 0.000027 Loss 4.985036, Accuracy 87.587%\n",
      "Epoch 26, Batch 926, LR 0.000027 Loss 4.985364, Accuracy 87.584%\n",
      "Epoch 26, Batch 927, LR 0.000027 Loss 4.985337, Accuracy 87.585%\n",
      "Epoch 26, Batch 928, LR 0.000027 Loss 4.986242, Accuracy 87.578%\n",
      "Epoch 26, Batch 929, LR 0.000027 Loss 4.986120, Accuracy 87.579%\n",
      "Epoch 26, Batch 930, LR 0.000027 Loss 4.987022, Accuracy 87.576%\n",
      "Epoch 26, Batch 931, LR 0.000027 Loss 4.987177, Accuracy 87.574%\n",
      "Epoch 26, Batch 932, LR 0.000027 Loss 4.987152, Accuracy 87.572%\n",
      "Epoch 26, Batch 933, LR 0.000027 Loss 4.987351, Accuracy 87.571%\n",
      "Epoch 26, Batch 934, LR 0.000027 Loss 4.986774, Accuracy 87.568%\n",
      "Epoch 26, Batch 935, LR 0.000027 Loss 4.986256, Accuracy 87.574%\n",
      "Epoch 26, Batch 936, LR 0.000027 Loss 4.987132, Accuracy 87.570%\n",
      "Epoch 26, Batch 937, LR 0.000027 Loss 4.986333, Accuracy 87.573%\n",
      "Epoch 26, Batch 938, LR 0.000027 Loss 4.986449, Accuracy 87.574%\n",
      "Epoch 26, Batch 939, LR 0.000027 Loss 4.987131, Accuracy 87.573%\n",
      "Epoch 26, Batch 940, LR 0.000027 Loss 4.987135, Accuracy 87.570%\n",
      "Epoch 26, Batch 941, LR 0.000027 Loss 4.988018, Accuracy 87.569%\n",
      "Epoch 26, Batch 942, LR 0.000027 Loss 4.987523, Accuracy 87.571%\n",
      "Epoch 26, Batch 943, LR 0.000027 Loss 4.986683, Accuracy 87.573%\n",
      "Epoch 26, Batch 944, LR 0.000027 Loss 4.986035, Accuracy 87.577%\n",
      "Epoch 26, Batch 945, LR 0.000027 Loss 4.985687, Accuracy 87.577%\n",
      "Epoch 26, Batch 946, LR 0.000027 Loss 4.986579, Accuracy 87.572%\n",
      "Epoch 26, Batch 947, LR 0.000027 Loss 4.986850, Accuracy 87.569%\n",
      "Epoch 26, Batch 948, LR 0.000027 Loss 4.986248, Accuracy 87.570%\n",
      "Epoch 26, Batch 949, LR 0.000027 Loss 4.986573, Accuracy 87.568%\n",
      "Epoch 26, Batch 950, LR 0.000027 Loss 4.986983, Accuracy 87.563%\n",
      "Epoch 26, Batch 951, LR 0.000027 Loss 4.986873, Accuracy 87.567%\n",
      "Epoch 26, Batch 952, LR 0.000027 Loss 4.986803, Accuracy 87.566%\n",
      "Epoch 26, Batch 953, LR 0.000027 Loss 4.987192, Accuracy 87.568%\n",
      "Epoch 26, Batch 954, LR 0.000027 Loss 4.987107, Accuracy 87.570%\n",
      "Epoch 26, Batch 955, LR 0.000027 Loss 4.987185, Accuracy 87.570%\n",
      "Epoch 26, Batch 956, LR 0.000027 Loss 4.986847, Accuracy 87.571%\n",
      "Epoch 26, Batch 957, LR 0.000027 Loss 4.987076, Accuracy 87.568%\n",
      "Epoch 26, Batch 958, LR 0.000027 Loss 4.986963, Accuracy 87.564%\n",
      "Epoch 26, Batch 959, LR 0.000027 Loss 4.986804, Accuracy 87.563%\n",
      "Epoch 26, Batch 960, LR 0.000027 Loss 4.986554, Accuracy 87.566%\n",
      "Epoch 26, Batch 961, LR 0.000027 Loss 4.986641, Accuracy 87.565%\n",
      "Epoch 26, Batch 962, LR 0.000026 Loss 4.986497, Accuracy 87.566%\n",
      "Epoch 26, Batch 963, LR 0.000026 Loss 4.985965, Accuracy 87.567%\n",
      "Epoch 26, Batch 964, LR 0.000026 Loss 4.986198, Accuracy 87.566%\n",
      "Epoch 26, Batch 965, LR 0.000026 Loss 4.986294, Accuracy 87.564%\n",
      "Epoch 26, Batch 966, LR 0.000026 Loss 4.985653, Accuracy 87.568%\n",
      "Epoch 26, Batch 967, LR 0.000026 Loss 4.986657, Accuracy 87.563%\n",
      "Epoch 26, Batch 968, LR 0.000026 Loss 4.986451, Accuracy 87.564%\n",
      "Epoch 26, Batch 969, LR 0.000026 Loss 4.986398, Accuracy 87.559%\n",
      "Epoch 26, Batch 970, LR 0.000026 Loss 4.985872, Accuracy 87.558%\n",
      "Epoch 26, Batch 971, LR 0.000026 Loss 4.985607, Accuracy 87.560%\n",
      "Epoch 26, Batch 972, LR 0.000026 Loss 4.985979, Accuracy 87.557%\n",
      "Epoch 26, Batch 973, LR 0.000026 Loss 4.985759, Accuracy 87.559%\n",
      "Epoch 26, Batch 974, LR 0.000026 Loss 4.986182, Accuracy 87.557%\n",
      "Epoch 26, Batch 975, LR 0.000026 Loss 4.985320, Accuracy 87.560%\n",
      "Epoch 26, Batch 976, LR 0.000026 Loss 4.985345, Accuracy 87.558%\n",
      "Epoch 26, Batch 977, LR 0.000026 Loss 4.984918, Accuracy 87.558%\n",
      "Epoch 26, Batch 978, LR 0.000026 Loss 4.984689, Accuracy 87.561%\n",
      "Epoch 26, Batch 979, LR 0.000026 Loss 4.983771, Accuracy 87.564%\n",
      "Epoch 26, Batch 980, LR 0.000026 Loss 4.983488, Accuracy 87.567%\n",
      "Epoch 26, Batch 981, LR 0.000026 Loss 4.983868, Accuracy 87.572%\n",
      "Epoch 26, Batch 982, LR 0.000026 Loss 4.983010, Accuracy 87.576%\n",
      "Epoch 26, Batch 983, LR 0.000026 Loss 4.982167, Accuracy 87.576%\n",
      "Epoch 26, Batch 984, LR 0.000026 Loss 4.981580, Accuracy 87.583%\n",
      "Epoch 26, Batch 985, LR 0.000026 Loss 4.981452, Accuracy 87.583%\n",
      "Epoch 26, Batch 986, LR 0.000026 Loss 4.981255, Accuracy 87.585%\n",
      "Epoch 26, Batch 987, LR 0.000026 Loss 4.982132, Accuracy 87.581%\n",
      "Epoch 26, Batch 988, LR 0.000026 Loss 4.982914, Accuracy 87.574%\n",
      "Epoch 26, Batch 989, LR 0.000026 Loss 4.982758, Accuracy 87.576%\n",
      "Epoch 26, Batch 990, LR 0.000026 Loss 4.982438, Accuracy 87.580%\n",
      "Epoch 26, Batch 991, LR 0.000026 Loss 4.982109, Accuracy 87.582%\n",
      "Epoch 26, Batch 992, LR 0.000026 Loss 4.982732, Accuracy 87.580%\n",
      "Epoch 26, Batch 993, LR 0.000026 Loss 4.982254, Accuracy 87.584%\n",
      "Epoch 26, Batch 994, LR 0.000026 Loss 4.982616, Accuracy 87.583%\n",
      "Epoch 26, Batch 995, LR 0.000026 Loss 4.982468, Accuracy 87.585%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 996, LR 0.000026 Loss 4.982980, Accuracy 87.582%\n",
      "Epoch 26, Batch 997, LR 0.000026 Loss 4.983585, Accuracy 87.577%\n",
      "Epoch 26, Batch 998, LR 0.000026 Loss 4.983558, Accuracy 87.577%\n",
      "Epoch 26, Batch 999, LR 0.000026 Loss 4.984400, Accuracy 87.570%\n",
      "Epoch 26, Batch 1000, LR 0.000026 Loss 4.984547, Accuracy 87.567%\n",
      "Epoch 26, Batch 1001, LR 0.000026 Loss 4.984785, Accuracy 87.566%\n",
      "Epoch 26, Batch 1002, LR 0.000026 Loss 4.985724, Accuracy 87.565%\n",
      "Epoch 26, Batch 1003, LR 0.000026 Loss 4.985434, Accuracy 87.562%\n",
      "Epoch 26, Batch 1004, LR 0.000026 Loss 4.985641, Accuracy 87.561%\n",
      "Epoch 26, Batch 1005, LR 0.000026 Loss 4.986378, Accuracy 87.559%\n",
      "Epoch 26, Batch 1006, LR 0.000026 Loss 4.986530, Accuracy 87.557%\n",
      "Epoch 26, Batch 1007, LR 0.000026 Loss 4.986274, Accuracy 87.561%\n",
      "Epoch 26, Batch 1008, LR 0.000026 Loss 4.986779, Accuracy 87.556%\n",
      "Epoch 26, Batch 1009, LR 0.000026 Loss 4.986542, Accuracy 87.560%\n",
      "Epoch 26, Batch 1010, LR 0.000026 Loss 4.986803, Accuracy 87.558%\n",
      "Epoch 26, Batch 1011, LR 0.000026 Loss 4.986435, Accuracy 87.558%\n",
      "Epoch 26, Batch 1012, LR 0.000026 Loss 4.986716, Accuracy 87.559%\n",
      "Epoch 26, Batch 1013, LR 0.000026 Loss 4.987764, Accuracy 87.550%\n",
      "Epoch 26, Batch 1014, LR 0.000026 Loss 4.988089, Accuracy 87.552%\n",
      "Epoch 26, Batch 1015, LR 0.000026 Loss 4.988266, Accuracy 87.548%\n",
      "Epoch 26, Batch 1016, LR 0.000026 Loss 4.988270, Accuracy 87.552%\n",
      "Epoch 26, Batch 1017, LR 0.000026 Loss 4.988170, Accuracy 87.552%\n",
      "Epoch 26, Batch 1018, LR 0.000026 Loss 4.988281, Accuracy 87.555%\n",
      "Epoch 26, Batch 1019, LR 0.000026 Loss 4.988672, Accuracy 87.554%\n",
      "Epoch 26, Batch 1020, LR 0.000026 Loss 4.988536, Accuracy 87.555%\n",
      "Epoch 26, Batch 1021, LR 0.000026 Loss 4.989313, Accuracy 87.549%\n",
      "Epoch 26, Batch 1022, LR 0.000026 Loss 4.989011, Accuracy 87.550%\n",
      "Epoch 26, Batch 1023, LR 0.000026 Loss 4.988647, Accuracy 87.549%\n",
      "Epoch 26, Batch 1024, LR 0.000026 Loss 4.989020, Accuracy 87.548%\n",
      "Epoch 26, Batch 1025, LR 0.000026 Loss 4.988231, Accuracy 87.552%\n",
      "Epoch 26, Batch 1026, LR 0.000026 Loss 4.988466, Accuracy 87.551%\n",
      "Epoch 26, Batch 1027, LR 0.000026 Loss 4.988599, Accuracy 87.551%\n",
      "Epoch 26, Batch 1028, LR 0.000026 Loss 4.989532, Accuracy 87.547%\n",
      "Epoch 26, Batch 1029, LR 0.000026 Loss 4.988657, Accuracy 87.552%\n",
      "Epoch 26, Batch 1030, LR 0.000026 Loss 4.988635, Accuracy 87.551%\n",
      "Epoch 26, Batch 1031, LR 0.000026 Loss 4.988369, Accuracy 87.552%\n",
      "Epoch 26, Batch 1032, LR 0.000026 Loss 4.988637, Accuracy 87.552%\n",
      "Epoch 26, Batch 1033, LR 0.000026 Loss 4.988894, Accuracy 87.556%\n",
      "Epoch 26, Batch 1034, LR 0.000026 Loss 4.988535, Accuracy 87.555%\n",
      "Epoch 26, Batch 1035, LR 0.000026 Loss 4.988374, Accuracy 87.557%\n",
      "Epoch 26, Batch 1036, LR 0.000026 Loss 4.988287, Accuracy 87.560%\n",
      "Epoch 26, Batch 1037, LR 0.000026 Loss 4.988219, Accuracy 87.559%\n",
      "Epoch 26, Batch 1038, LR 0.000026 Loss 4.987775, Accuracy 87.559%\n",
      "Epoch 26, Batch 1039, LR 0.000026 Loss 4.987269, Accuracy 87.559%\n",
      "Epoch 26, Batch 1040, LR 0.000026 Loss 4.986185, Accuracy 87.563%\n",
      "Epoch 26, Batch 1041, LR 0.000026 Loss 4.986087, Accuracy 87.562%\n",
      "Epoch 26, Batch 1042, LR 0.000026 Loss 4.985713, Accuracy 87.564%\n",
      "Epoch 26, Batch 1043, LR 0.000026 Loss 4.985595, Accuracy 87.563%\n",
      "Epoch 26, Batch 1044, LR 0.000026 Loss 4.986271, Accuracy 87.558%\n",
      "Epoch 26, Batch 1045, LR 0.000026 Loss 4.986405, Accuracy 87.556%\n",
      "Epoch 26, Batch 1046, LR 0.000026 Loss 4.986276, Accuracy 87.556%\n",
      "Epoch 26, Batch 1047, LR 0.000026 Loss 4.986396, Accuracy 87.556%\n",
      "Epoch 26, Loss (train set) 4.986396, Accuracy (train set) 87.556%\n",
      "Epoch 27, Batch 1, LR 0.000026 Loss 5.393124, Accuracy 88.281%\n",
      "Epoch 27, Batch 2, LR 0.000026 Loss 4.936090, Accuracy 89.844%\n",
      "Epoch 27, Batch 3, LR 0.000026 Loss 4.804592, Accuracy 88.021%\n",
      "Epoch 27, Batch 4, LR 0.000026 Loss 4.782151, Accuracy 88.867%\n",
      "Epoch 27, Batch 5, LR 0.000026 Loss 4.747702, Accuracy 88.906%\n",
      "Epoch 27, Batch 6, LR 0.000026 Loss 5.042858, Accuracy 87.240%\n",
      "Epoch 27, Batch 7, LR 0.000026 Loss 5.012930, Accuracy 87.388%\n",
      "Epoch 27, Batch 8, LR 0.000026 Loss 4.953369, Accuracy 87.598%\n",
      "Epoch 27, Batch 9, LR 0.000026 Loss 4.876165, Accuracy 88.194%\n",
      "Epoch 27, Batch 10, LR 0.000026 Loss 4.874155, Accuracy 88.125%\n",
      "Epoch 27, Batch 11, LR 0.000026 Loss 4.906128, Accuracy 87.997%\n",
      "Epoch 27, Batch 12, LR 0.000026 Loss 4.929118, Accuracy 87.956%\n",
      "Epoch 27, Batch 13, LR 0.000026 Loss 4.920735, Accuracy 88.281%\n",
      "Epoch 27, Batch 14, LR 0.000026 Loss 4.950987, Accuracy 88.002%\n",
      "Epoch 27, Batch 15, LR 0.000026 Loss 4.927970, Accuracy 88.125%\n",
      "Epoch 27, Batch 16, LR 0.000026 Loss 4.868020, Accuracy 88.379%\n",
      "Epoch 27, Batch 17, LR 0.000026 Loss 4.857083, Accuracy 88.465%\n",
      "Epoch 27, Batch 18, LR 0.000026 Loss 4.881095, Accuracy 88.411%\n",
      "Epoch 27, Batch 19, LR 0.000026 Loss 4.928683, Accuracy 87.911%\n",
      "Epoch 27, Batch 20, LR 0.000026 Loss 4.903030, Accuracy 88.047%\n",
      "Epoch 27, Batch 21, LR 0.000026 Loss 4.884974, Accuracy 88.132%\n",
      "Epoch 27, Batch 22, LR 0.000026 Loss 4.881472, Accuracy 88.033%\n",
      "Epoch 27, Batch 23, LR 0.000026 Loss 4.884176, Accuracy 88.077%\n",
      "Epoch 27, Batch 24, LR 0.000026 Loss 4.875658, Accuracy 88.053%\n",
      "Epoch 27, Batch 25, LR 0.000026 Loss 4.841491, Accuracy 88.250%\n",
      "Epoch 27, Batch 26, LR 0.000026 Loss 4.821137, Accuracy 88.281%\n",
      "Epoch 27, Batch 27, LR 0.000026 Loss 4.828220, Accuracy 88.166%\n",
      "Epoch 27, Batch 28, LR 0.000026 Loss 4.841440, Accuracy 88.198%\n",
      "Epoch 27, Batch 29, LR 0.000026 Loss 4.860145, Accuracy 88.147%\n",
      "Epoch 27, Batch 30, LR 0.000026 Loss 4.856235, Accuracy 88.151%\n",
      "Epoch 27, Batch 31, LR 0.000026 Loss 4.834534, Accuracy 88.332%\n",
      "Epoch 27, Batch 32, LR 0.000026 Loss 4.830352, Accuracy 88.208%\n",
      "Epoch 27, Batch 33, LR 0.000026 Loss 4.812499, Accuracy 88.281%\n",
      "Epoch 27, Batch 34, LR 0.000026 Loss 4.808438, Accuracy 88.212%\n",
      "Epoch 27, Batch 35, LR 0.000026 Loss 4.802852, Accuracy 88.304%\n",
      "Epoch 27, Batch 36, LR 0.000026 Loss 4.780816, Accuracy 88.411%\n",
      "Epoch 27, Batch 37, LR 0.000026 Loss 4.787519, Accuracy 88.323%\n",
      "Epoch 27, Batch 38, LR 0.000026 Loss 4.792922, Accuracy 88.322%\n",
      "Epoch 27, Batch 39, LR 0.000026 Loss 4.780485, Accuracy 88.401%\n",
      "Epoch 27, Batch 40, LR 0.000026 Loss 4.771604, Accuracy 88.457%\n",
      "Epoch 27, Batch 41, LR 0.000026 Loss 4.753190, Accuracy 88.548%\n",
      "Epoch 27, Batch 42, LR 0.000026 Loss 4.756965, Accuracy 88.523%\n",
      "Epoch 27, Batch 43, LR 0.000026 Loss 4.769299, Accuracy 88.445%\n",
      "Epoch 27, Batch 44, LR 0.000026 Loss 4.785829, Accuracy 88.406%\n",
      "Epoch 27, Batch 45, LR 0.000026 Loss 4.784870, Accuracy 88.455%\n",
      "Epoch 27, Batch 46, LR 0.000026 Loss 4.773524, Accuracy 88.502%\n",
      "Epoch 27, Batch 47, LR 0.000026 Loss 4.789162, Accuracy 88.431%\n",
      "Epoch 27, Batch 48, LR 0.000026 Loss 4.803938, Accuracy 88.411%\n",
      "Epoch 27, Batch 49, LR 0.000026 Loss 4.789755, Accuracy 88.425%\n",
      "Epoch 27, Batch 50, LR 0.000026 Loss 4.765750, Accuracy 88.484%\n",
      "Epoch 27, Batch 51, LR 0.000026 Loss 4.763766, Accuracy 88.404%\n",
      "Epoch 27, Batch 52, LR 0.000026 Loss 4.767656, Accuracy 88.326%\n",
      "Epoch 27, Batch 53, LR 0.000026 Loss 4.772551, Accuracy 88.325%\n",
      "Epoch 27, Batch 54, LR 0.000026 Loss 4.775143, Accuracy 88.310%\n",
      "Epoch 27, Batch 55, LR 0.000026 Loss 4.781946, Accuracy 88.196%\n",
      "Epoch 27, Batch 56, LR 0.000026 Loss 4.768176, Accuracy 88.267%\n",
      "Epoch 27, Batch 57, LR 0.000026 Loss 4.774376, Accuracy 88.226%\n",
      "Epoch 27, Batch 58, LR 0.000026 Loss 4.764566, Accuracy 88.335%\n",
      "Epoch 27, Batch 59, LR 0.000026 Loss 4.759379, Accuracy 88.400%\n",
      "Epoch 27, Batch 60, LR 0.000026 Loss 4.744256, Accuracy 88.464%\n",
      "Epoch 27, Batch 61, LR 0.000026 Loss 4.762415, Accuracy 88.435%\n",
      "Epoch 27, Batch 62, LR 0.000026 Loss 4.766886, Accuracy 88.420%\n",
      "Epoch 27, Batch 63, LR 0.000026 Loss 4.767722, Accuracy 88.418%\n",
      "Epoch 27, Batch 64, LR 0.000026 Loss 4.774439, Accuracy 88.416%\n",
      "Epoch 27, Batch 65, LR 0.000026 Loss 4.767779, Accuracy 88.438%\n",
      "Epoch 27, Batch 66, LR 0.000026 Loss 4.757396, Accuracy 88.471%\n",
      "Epoch 27, Batch 67, LR 0.000026 Loss 4.755933, Accuracy 88.514%\n",
      "Epoch 27, Batch 68, LR 0.000026 Loss 4.745342, Accuracy 88.545%\n",
      "Epoch 27, Batch 69, LR 0.000026 Loss 4.759366, Accuracy 88.440%\n",
      "Epoch 27, Batch 70, LR 0.000026 Loss 4.762341, Accuracy 88.426%\n",
      "Epoch 27, Batch 71, LR 0.000026 Loss 4.758891, Accuracy 88.446%\n",
      "Epoch 27, Batch 72, LR 0.000026 Loss 4.749250, Accuracy 88.455%\n",
      "Epoch 27, Batch 73, LR 0.000026 Loss 4.760410, Accuracy 88.420%\n",
      "Epoch 27, Batch 74, LR 0.000026 Loss 4.760453, Accuracy 88.429%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 75, LR 0.000026 Loss 4.770996, Accuracy 88.385%\n",
      "Epoch 27, Batch 76, LR 0.000026 Loss 4.771232, Accuracy 88.425%\n",
      "Epoch 27, Batch 77, LR 0.000026 Loss 4.774187, Accuracy 88.383%\n",
      "Epoch 27, Batch 78, LR 0.000026 Loss 4.779481, Accuracy 88.371%\n",
      "Epoch 27, Batch 79, LR 0.000026 Loss 4.777397, Accuracy 88.370%\n",
      "Epoch 27, Batch 80, LR 0.000026 Loss 4.781214, Accuracy 88.389%\n",
      "Epoch 27, Batch 81, LR 0.000026 Loss 4.787138, Accuracy 88.320%\n",
      "Epoch 27, Batch 82, LR 0.000026 Loss 4.788126, Accuracy 88.310%\n",
      "Epoch 27, Batch 83, LR 0.000026 Loss 4.793896, Accuracy 88.253%\n",
      "Epoch 27, Batch 84, LR 0.000026 Loss 4.797579, Accuracy 88.225%\n",
      "Epoch 27, Batch 85, LR 0.000026 Loss 4.794827, Accuracy 88.226%\n",
      "Epoch 27, Batch 86, LR 0.000026 Loss 4.797557, Accuracy 88.227%\n",
      "Epoch 27, Batch 87, LR 0.000026 Loss 4.798062, Accuracy 88.227%\n",
      "Epoch 27, Batch 88, LR 0.000026 Loss 4.790535, Accuracy 88.299%\n",
      "Epoch 27, Batch 89, LR 0.000026 Loss 4.791219, Accuracy 88.325%\n",
      "Epoch 27, Batch 90, LR 0.000026 Loss 4.787875, Accuracy 88.333%\n",
      "Epoch 27, Batch 91, LR 0.000026 Loss 4.791201, Accuracy 88.341%\n",
      "Epoch 27, Batch 92, LR 0.000026 Loss 4.791556, Accuracy 88.341%\n",
      "Epoch 27, Batch 93, LR 0.000026 Loss 4.791751, Accuracy 88.340%\n",
      "Epoch 27, Batch 94, LR 0.000026 Loss 4.788500, Accuracy 88.323%\n",
      "Epoch 27, Batch 95, LR 0.000026 Loss 4.789974, Accuracy 88.322%\n",
      "Epoch 27, Batch 96, LR 0.000026 Loss 4.791685, Accuracy 88.322%\n",
      "Epoch 27, Batch 97, LR 0.000026 Loss 4.800706, Accuracy 88.289%\n",
      "Epoch 27, Batch 98, LR 0.000026 Loss 4.802595, Accuracy 88.257%\n",
      "Epoch 27, Batch 99, LR 0.000026 Loss 4.807672, Accuracy 88.258%\n",
      "Epoch 27, Batch 100, LR 0.000026 Loss 4.812423, Accuracy 88.219%\n",
      "Epoch 27, Batch 101, LR 0.000026 Loss 4.811358, Accuracy 88.227%\n",
      "Epoch 27, Batch 102, LR 0.000026 Loss 4.808850, Accuracy 88.274%\n",
      "Epoch 27, Batch 103, LR 0.000026 Loss 4.807258, Accuracy 88.327%\n",
      "Epoch 27, Batch 104, LR 0.000026 Loss 4.805559, Accuracy 88.296%\n",
      "Epoch 27, Batch 105, LR 0.000026 Loss 4.806506, Accuracy 88.333%\n",
      "Epoch 27, Batch 106, LR 0.000026 Loss 4.806511, Accuracy 88.333%\n",
      "Epoch 27, Batch 107, LR 0.000026 Loss 4.808347, Accuracy 88.347%\n",
      "Epoch 27, Batch 108, LR 0.000026 Loss 4.814868, Accuracy 88.310%\n",
      "Epoch 27, Batch 109, LR 0.000026 Loss 4.818195, Accuracy 88.281%\n",
      "Epoch 27, Batch 110, LR 0.000026 Loss 4.820650, Accuracy 88.260%\n",
      "Epoch 27, Batch 111, LR 0.000026 Loss 4.821149, Accuracy 88.253%\n",
      "Epoch 27, Batch 112, LR 0.000026 Loss 4.822377, Accuracy 88.232%\n",
      "Epoch 27, Batch 113, LR 0.000026 Loss 4.827740, Accuracy 88.240%\n",
      "Epoch 27, Batch 114, LR 0.000026 Loss 4.829939, Accuracy 88.247%\n",
      "Epoch 27, Batch 115, LR 0.000026 Loss 4.827240, Accuracy 88.247%\n",
      "Epoch 27, Batch 116, LR 0.000026 Loss 4.827138, Accuracy 88.254%\n",
      "Epoch 27, Batch 117, LR 0.000026 Loss 4.821588, Accuracy 88.261%\n",
      "Epoch 27, Batch 118, LR 0.000026 Loss 4.819543, Accuracy 88.275%\n",
      "Epoch 27, Batch 119, LR 0.000026 Loss 4.821259, Accuracy 88.262%\n",
      "Epoch 27, Batch 120, LR 0.000026 Loss 4.820345, Accuracy 88.249%\n",
      "Epoch 27, Batch 121, LR 0.000026 Loss 4.826076, Accuracy 88.243%\n",
      "Epoch 27, Batch 122, LR 0.000026 Loss 4.823725, Accuracy 88.262%\n",
      "Epoch 27, Batch 123, LR 0.000026 Loss 4.826371, Accuracy 88.262%\n",
      "Epoch 27, Batch 124, LR 0.000026 Loss 4.828774, Accuracy 88.256%\n",
      "Epoch 27, Batch 125, LR 0.000026 Loss 4.821248, Accuracy 88.275%\n",
      "Epoch 27, Batch 126, LR 0.000026 Loss 4.820622, Accuracy 88.287%\n",
      "Epoch 27, Batch 127, LR 0.000026 Loss 4.825579, Accuracy 88.275%\n",
      "Epoch 27, Batch 128, LR 0.000026 Loss 4.821643, Accuracy 88.293%\n",
      "Epoch 27, Batch 129, LR 0.000026 Loss 4.823676, Accuracy 88.281%\n",
      "Epoch 27, Batch 130, LR 0.000026 Loss 4.823360, Accuracy 88.287%\n",
      "Epoch 27, Batch 131, LR 0.000026 Loss 4.819712, Accuracy 88.293%\n",
      "Epoch 27, Batch 132, LR 0.000026 Loss 4.824670, Accuracy 88.258%\n",
      "Epoch 27, Batch 133, LR 0.000026 Loss 4.823812, Accuracy 88.287%\n",
      "Epoch 27, Batch 134, LR 0.000026 Loss 4.830102, Accuracy 88.235%\n",
      "Epoch 27, Batch 135, LR 0.000026 Loss 4.836306, Accuracy 88.212%\n",
      "Epoch 27, Batch 136, LR 0.000026 Loss 4.837410, Accuracy 88.212%\n",
      "Epoch 27, Batch 137, LR 0.000026 Loss 4.840205, Accuracy 88.213%\n",
      "Epoch 27, Batch 138, LR 0.000026 Loss 4.841528, Accuracy 88.225%\n",
      "Epoch 27, Batch 139, LR 0.000026 Loss 4.840056, Accuracy 88.203%\n",
      "Epoch 27, Batch 140, LR 0.000026 Loss 4.841728, Accuracy 88.203%\n",
      "Epoch 27, Batch 141, LR 0.000026 Loss 4.845336, Accuracy 88.204%\n",
      "Epoch 27, Batch 142, LR 0.000026 Loss 4.850666, Accuracy 88.188%\n",
      "Epoch 27, Batch 143, LR 0.000026 Loss 4.855019, Accuracy 88.150%\n",
      "Epoch 27, Batch 144, LR 0.000026 Loss 4.848768, Accuracy 88.200%\n",
      "Epoch 27, Batch 145, LR 0.000026 Loss 4.846060, Accuracy 88.211%\n",
      "Epoch 27, Batch 146, LR 0.000026 Loss 4.844983, Accuracy 88.217%\n",
      "Epoch 27, Batch 147, LR 0.000026 Loss 4.845174, Accuracy 88.212%\n",
      "Epoch 27, Batch 148, LR 0.000026 Loss 4.848537, Accuracy 88.213%\n",
      "Epoch 27, Batch 149, LR 0.000026 Loss 4.846054, Accuracy 88.234%\n",
      "Epoch 27, Batch 150, LR 0.000026 Loss 4.848177, Accuracy 88.219%\n",
      "Epoch 27, Batch 151, LR 0.000026 Loss 4.845911, Accuracy 88.230%\n",
      "Epoch 27, Batch 152, LR 0.000026 Loss 4.848741, Accuracy 88.220%\n",
      "Epoch 27, Batch 153, LR 0.000026 Loss 4.846110, Accuracy 88.225%\n",
      "Epoch 27, Batch 154, LR 0.000026 Loss 4.849930, Accuracy 88.185%\n",
      "Epoch 27, Batch 155, LR 0.000026 Loss 4.848985, Accuracy 88.196%\n",
      "Epoch 27, Batch 156, LR 0.000026 Loss 4.848426, Accuracy 88.206%\n",
      "Epoch 27, Batch 157, LR 0.000026 Loss 4.849840, Accuracy 88.202%\n",
      "Epoch 27, Batch 158, LR 0.000026 Loss 4.847117, Accuracy 88.212%\n",
      "Epoch 27, Batch 159, LR 0.000026 Loss 4.843818, Accuracy 88.232%\n",
      "Epoch 27, Batch 160, LR 0.000026 Loss 4.846236, Accuracy 88.213%\n",
      "Epoch 27, Batch 161, LR 0.000026 Loss 4.848064, Accuracy 88.204%\n",
      "Epoch 27, Batch 162, LR 0.000026 Loss 4.841469, Accuracy 88.209%\n",
      "Epoch 27, Batch 163, LR 0.000026 Loss 4.840375, Accuracy 88.219%\n",
      "Epoch 27, Batch 164, LR 0.000026 Loss 4.839311, Accuracy 88.224%\n",
      "Epoch 27, Batch 165, LR 0.000026 Loss 4.832695, Accuracy 88.248%\n",
      "Epoch 27, Batch 166, LR 0.000026 Loss 4.835854, Accuracy 88.215%\n",
      "Epoch 27, Batch 167, LR 0.000026 Loss 4.834332, Accuracy 88.225%\n",
      "Epoch 27, Batch 168, LR 0.000026 Loss 4.835658, Accuracy 88.225%\n",
      "Epoch 27, Batch 169, LR 0.000026 Loss 4.833666, Accuracy 88.244%\n",
      "Epoch 27, Batch 170, LR 0.000026 Loss 4.831880, Accuracy 88.235%\n",
      "Epoch 27, Batch 171, LR 0.000026 Loss 4.831614, Accuracy 88.226%\n",
      "Epoch 27, Batch 172, LR 0.000026 Loss 4.833734, Accuracy 88.199%\n",
      "Epoch 27, Batch 173, LR 0.000026 Loss 4.835712, Accuracy 88.200%\n",
      "Epoch 27, Batch 174, LR 0.000026 Loss 4.835142, Accuracy 88.191%\n",
      "Epoch 27, Batch 175, LR 0.000026 Loss 4.837800, Accuracy 88.174%\n",
      "Epoch 27, Batch 176, LR 0.000026 Loss 4.837771, Accuracy 88.179%\n",
      "Epoch 27, Batch 177, LR 0.000026 Loss 4.836537, Accuracy 88.184%\n",
      "Epoch 27, Batch 178, LR 0.000026 Loss 4.839702, Accuracy 88.167%\n",
      "Epoch 27, Batch 179, LR 0.000026 Loss 4.837306, Accuracy 88.190%\n",
      "Epoch 27, Batch 180, LR 0.000026 Loss 4.834628, Accuracy 88.203%\n",
      "Epoch 27, Batch 181, LR 0.000026 Loss 4.832013, Accuracy 88.221%\n",
      "Epoch 27, Batch 182, LR 0.000026 Loss 4.835679, Accuracy 88.208%\n",
      "Epoch 27, Batch 183, LR 0.000026 Loss 4.835621, Accuracy 88.187%\n",
      "Epoch 27, Batch 184, LR 0.000026 Loss 4.834309, Accuracy 88.192%\n",
      "Epoch 27, Batch 185, LR 0.000026 Loss 4.840247, Accuracy 88.167%\n",
      "Epoch 27, Batch 186, LR 0.000026 Loss 4.842163, Accuracy 88.172%\n",
      "Epoch 27, Batch 187, LR 0.000026 Loss 4.841939, Accuracy 88.181%\n",
      "Epoch 27, Batch 188, LR 0.000026 Loss 4.838264, Accuracy 88.198%\n",
      "Epoch 27, Batch 189, LR 0.000026 Loss 4.834197, Accuracy 88.194%\n",
      "Epoch 27, Batch 190, LR 0.000026 Loss 4.833624, Accuracy 88.203%\n",
      "Epoch 27, Batch 191, LR 0.000026 Loss 4.837138, Accuracy 88.167%\n",
      "Epoch 27, Batch 192, LR 0.000026 Loss 4.835430, Accuracy 88.184%\n",
      "Epoch 27, Batch 193, LR 0.000026 Loss 4.834316, Accuracy 88.168%\n",
      "Epoch 27, Batch 194, LR 0.000026 Loss 4.835615, Accuracy 88.181%\n",
      "Epoch 27, Batch 195, LR 0.000026 Loss 4.833557, Accuracy 88.185%\n",
      "Epoch 27, Batch 196, LR 0.000026 Loss 4.833358, Accuracy 88.202%\n",
      "Epoch 27, Batch 197, LR 0.000026 Loss 4.832293, Accuracy 88.202%\n",
      "Epoch 27, Batch 198, LR 0.000026 Loss 4.834565, Accuracy 88.202%\n",
      "Epoch 27, Batch 199, LR 0.000026 Loss 4.835267, Accuracy 88.183%\n",
      "Epoch 27, Batch 200, LR 0.000026 Loss 4.834160, Accuracy 88.195%\n",
      "Epoch 27, Batch 201, LR 0.000026 Loss 4.836310, Accuracy 88.169%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 202, LR 0.000026 Loss 4.836098, Accuracy 88.154%\n",
      "Epoch 27, Batch 203, LR 0.000026 Loss 4.839584, Accuracy 88.127%\n",
      "Epoch 27, Batch 204, LR 0.000025 Loss 4.839257, Accuracy 88.124%\n",
      "Epoch 27, Batch 205, LR 0.000025 Loss 4.838474, Accuracy 88.144%\n",
      "Epoch 27, Batch 206, LR 0.000025 Loss 4.837605, Accuracy 88.137%\n",
      "Epoch 27, Batch 207, LR 0.000025 Loss 4.840373, Accuracy 88.127%\n",
      "Epoch 27, Batch 208, LR 0.000025 Loss 4.843618, Accuracy 88.101%\n",
      "Epoch 27, Batch 209, LR 0.000025 Loss 4.843373, Accuracy 88.098%\n",
      "Epoch 27, Batch 210, LR 0.000025 Loss 4.844573, Accuracy 88.103%\n",
      "Epoch 27, Batch 211, LR 0.000025 Loss 4.846491, Accuracy 88.085%\n",
      "Epoch 27, Batch 212, LR 0.000025 Loss 4.848251, Accuracy 88.064%\n",
      "Epoch 27, Batch 213, LR 0.000025 Loss 4.849324, Accuracy 88.065%\n",
      "Epoch 27, Batch 214, LR 0.000025 Loss 4.849002, Accuracy 88.066%\n",
      "Epoch 27, Batch 215, LR 0.000025 Loss 4.850249, Accuracy 88.070%\n",
      "Epoch 27, Batch 216, LR 0.000025 Loss 4.850531, Accuracy 88.075%\n",
      "Epoch 27, Batch 217, LR 0.000025 Loss 4.850785, Accuracy 88.080%\n",
      "Epoch 27, Batch 218, LR 0.000025 Loss 4.848106, Accuracy 88.091%\n",
      "Epoch 27, Batch 219, LR 0.000025 Loss 4.849792, Accuracy 88.078%\n",
      "Epoch 27, Batch 220, LR 0.000025 Loss 4.849257, Accuracy 88.075%\n",
      "Epoch 27, Batch 221, LR 0.000025 Loss 4.852279, Accuracy 88.076%\n",
      "Epoch 27, Batch 222, LR 0.000025 Loss 4.855463, Accuracy 88.074%\n",
      "Epoch 27, Batch 223, LR 0.000025 Loss 4.857402, Accuracy 88.043%\n",
      "Epoch 27, Batch 224, LR 0.000025 Loss 4.859853, Accuracy 88.037%\n",
      "Epoch 27, Batch 225, LR 0.000025 Loss 4.858651, Accuracy 88.024%\n",
      "Epoch 27, Batch 226, LR 0.000025 Loss 4.859248, Accuracy 88.015%\n",
      "Epoch 27, Batch 227, LR 0.000025 Loss 4.860407, Accuracy 88.023%\n",
      "Epoch 27, Batch 228, LR 0.000025 Loss 4.859729, Accuracy 88.035%\n",
      "Epoch 27, Batch 229, LR 0.000025 Loss 4.857218, Accuracy 88.056%\n",
      "Epoch 27, Batch 230, LR 0.000025 Loss 4.858961, Accuracy 88.067%\n",
      "Epoch 27, Batch 231, LR 0.000025 Loss 4.857479, Accuracy 88.095%\n",
      "Epoch 27, Batch 232, LR 0.000025 Loss 4.856930, Accuracy 88.106%\n",
      "Epoch 27, Batch 233, LR 0.000025 Loss 4.857561, Accuracy 88.107%\n",
      "Epoch 27, Batch 234, LR 0.000025 Loss 4.858117, Accuracy 88.084%\n",
      "Epoch 27, Batch 235, LR 0.000025 Loss 4.854793, Accuracy 88.102%\n",
      "Epoch 27, Batch 236, LR 0.000025 Loss 4.855125, Accuracy 88.086%\n",
      "Epoch 27, Batch 237, LR 0.000025 Loss 4.853442, Accuracy 88.090%\n",
      "Epoch 27, Batch 238, LR 0.000025 Loss 4.853231, Accuracy 88.094%\n",
      "Epoch 27, Batch 239, LR 0.000025 Loss 4.854768, Accuracy 88.088%\n",
      "Epoch 27, Batch 240, LR 0.000025 Loss 4.852700, Accuracy 88.099%\n",
      "Epoch 27, Batch 241, LR 0.000025 Loss 4.851955, Accuracy 88.090%\n",
      "Epoch 27, Batch 242, LR 0.000025 Loss 4.851912, Accuracy 88.097%\n",
      "Epoch 27, Batch 243, LR 0.000025 Loss 4.848742, Accuracy 88.124%\n",
      "Epoch 27, Batch 244, LR 0.000025 Loss 4.849951, Accuracy 88.124%\n",
      "Epoch 27, Batch 245, LR 0.000025 Loss 4.849162, Accuracy 88.119%\n",
      "Epoch 27, Batch 246, LR 0.000025 Loss 4.846199, Accuracy 88.116%\n",
      "Epoch 27, Batch 247, LR 0.000025 Loss 4.848886, Accuracy 88.110%\n",
      "Epoch 27, Batch 248, LR 0.000025 Loss 4.848655, Accuracy 88.099%\n",
      "Epoch 27, Batch 249, LR 0.000025 Loss 4.849495, Accuracy 88.099%\n",
      "Epoch 27, Batch 250, LR 0.000025 Loss 4.849835, Accuracy 88.100%\n",
      "Epoch 27, Batch 251, LR 0.000025 Loss 4.851649, Accuracy 88.073%\n",
      "Epoch 27, Batch 252, LR 0.000025 Loss 4.851597, Accuracy 88.080%\n",
      "Epoch 27, Batch 253, LR 0.000025 Loss 4.855152, Accuracy 88.074%\n",
      "Epoch 27, Batch 254, LR 0.000025 Loss 4.853539, Accuracy 88.069%\n",
      "Epoch 27, Batch 255, LR 0.000025 Loss 4.855227, Accuracy 88.076%\n",
      "Epoch 27, Batch 256, LR 0.000025 Loss 4.857273, Accuracy 88.068%\n",
      "Epoch 27, Batch 257, LR 0.000025 Loss 4.858341, Accuracy 88.059%\n",
      "Epoch 27, Batch 258, LR 0.000025 Loss 4.859904, Accuracy 88.057%\n",
      "Epoch 27, Batch 259, LR 0.000025 Loss 4.857909, Accuracy 88.046%\n",
      "Epoch 27, Batch 260, LR 0.000025 Loss 4.860830, Accuracy 88.038%\n",
      "Epoch 27, Batch 261, LR 0.000025 Loss 4.862515, Accuracy 88.027%\n",
      "Epoch 27, Batch 262, LR 0.000025 Loss 4.861490, Accuracy 88.043%\n",
      "Epoch 27, Batch 263, LR 0.000025 Loss 4.864787, Accuracy 88.038%\n",
      "Epoch 27, Batch 264, LR 0.000025 Loss 4.865589, Accuracy 88.039%\n",
      "Epoch 27, Batch 265, LR 0.000025 Loss 4.868740, Accuracy 88.019%\n",
      "Epoch 27, Batch 266, LR 0.000025 Loss 4.867075, Accuracy 88.029%\n",
      "Epoch 27, Batch 267, LR 0.000025 Loss 4.866682, Accuracy 88.030%\n",
      "Epoch 27, Batch 268, LR 0.000025 Loss 4.866419, Accuracy 88.039%\n",
      "Epoch 27, Batch 269, LR 0.000025 Loss 4.867302, Accuracy 88.034%\n",
      "Epoch 27, Batch 270, LR 0.000025 Loss 4.868294, Accuracy 88.032%\n",
      "Epoch 27, Batch 271, LR 0.000025 Loss 4.866806, Accuracy 88.054%\n",
      "Epoch 27, Batch 272, LR 0.000025 Loss 4.865164, Accuracy 88.060%\n",
      "Epoch 27, Batch 273, LR 0.000025 Loss 4.864158, Accuracy 88.078%\n",
      "Epoch 27, Batch 274, LR 0.000025 Loss 4.864273, Accuracy 88.079%\n",
      "Epoch 27, Batch 275, LR 0.000025 Loss 4.865791, Accuracy 88.082%\n",
      "Epoch 27, Batch 276, LR 0.000025 Loss 4.867730, Accuracy 88.072%\n",
      "Epoch 27, Batch 277, LR 0.000025 Loss 4.865159, Accuracy 88.081%\n",
      "Epoch 27, Batch 278, LR 0.000025 Loss 4.865471, Accuracy 88.059%\n",
      "Epoch 27, Batch 279, LR 0.000025 Loss 4.863770, Accuracy 88.077%\n",
      "Epoch 27, Batch 280, LR 0.000025 Loss 4.865073, Accuracy 88.069%\n",
      "Epoch 27, Batch 281, LR 0.000025 Loss 4.865020, Accuracy 88.084%\n",
      "Epoch 27, Batch 282, LR 0.000025 Loss 4.864034, Accuracy 88.098%\n",
      "Epoch 27, Batch 283, LR 0.000025 Loss 4.864300, Accuracy 88.085%\n",
      "Epoch 27, Batch 284, LR 0.000025 Loss 4.863497, Accuracy 88.091%\n",
      "Epoch 27, Batch 285, LR 0.000025 Loss 4.861838, Accuracy 88.095%\n",
      "Epoch 27, Batch 286, LR 0.000025 Loss 4.863689, Accuracy 88.087%\n",
      "Epoch 27, Batch 287, LR 0.000025 Loss 4.865496, Accuracy 88.093%\n",
      "Epoch 27, Batch 288, LR 0.000025 Loss 4.867654, Accuracy 88.094%\n",
      "Epoch 27, Batch 289, LR 0.000025 Loss 4.865511, Accuracy 88.106%\n",
      "Epoch 27, Batch 290, LR 0.000025 Loss 4.866250, Accuracy 88.101%\n",
      "Epoch 27, Batch 291, LR 0.000025 Loss 4.865656, Accuracy 88.107%\n",
      "Epoch 27, Batch 292, LR 0.000025 Loss 4.864945, Accuracy 88.113%\n",
      "Epoch 27, Batch 293, LR 0.000025 Loss 4.865604, Accuracy 88.116%\n",
      "Epoch 27, Batch 294, LR 0.000025 Loss 4.865698, Accuracy 88.114%\n",
      "Epoch 27, Batch 295, LR 0.000025 Loss 4.865245, Accuracy 88.099%\n",
      "Epoch 27, Batch 296, LR 0.000025 Loss 4.866171, Accuracy 88.102%\n",
      "Epoch 27, Batch 297, LR 0.000025 Loss 4.866620, Accuracy 88.094%\n",
      "Epoch 27, Batch 298, LR 0.000025 Loss 4.865715, Accuracy 88.103%\n",
      "Epoch 27, Batch 299, LR 0.000025 Loss 4.864290, Accuracy 88.109%\n",
      "Epoch 27, Batch 300, LR 0.000025 Loss 4.863632, Accuracy 88.109%\n",
      "Epoch 27, Batch 301, LR 0.000025 Loss 4.862947, Accuracy 88.115%\n",
      "Epoch 27, Batch 302, LR 0.000025 Loss 4.866213, Accuracy 88.103%\n",
      "Epoch 27, Batch 303, LR 0.000025 Loss 4.866863, Accuracy 88.096%\n",
      "Epoch 27, Batch 304, LR 0.000025 Loss 4.869366, Accuracy 88.081%\n",
      "Epoch 27, Batch 305, LR 0.000025 Loss 4.870448, Accuracy 88.076%\n",
      "Epoch 27, Batch 306, LR 0.000025 Loss 4.870583, Accuracy 88.064%\n",
      "Epoch 27, Batch 307, LR 0.000025 Loss 4.869979, Accuracy 88.070%\n",
      "Epoch 27, Batch 308, LR 0.000025 Loss 4.868694, Accuracy 88.076%\n",
      "Epoch 27, Batch 309, LR 0.000025 Loss 4.868086, Accuracy 88.071%\n",
      "Epoch 27, Batch 310, LR 0.000025 Loss 4.868144, Accuracy 88.072%\n",
      "Epoch 27, Batch 311, LR 0.000025 Loss 4.867548, Accuracy 88.078%\n",
      "Epoch 27, Batch 312, LR 0.000025 Loss 4.865763, Accuracy 88.088%\n",
      "Epoch 27, Batch 313, LR 0.000025 Loss 4.865342, Accuracy 88.089%\n",
      "Epoch 27, Batch 314, LR 0.000025 Loss 4.867378, Accuracy 88.072%\n",
      "Epoch 27, Batch 315, LR 0.000025 Loss 4.866349, Accuracy 88.085%\n",
      "Epoch 27, Batch 316, LR 0.000025 Loss 4.865855, Accuracy 88.091%\n",
      "Epoch 27, Batch 317, LR 0.000025 Loss 4.866019, Accuracy 88.101%\n",
      "Epoch 27, Batch 318, LR 0.000025 Loss 4.868438, Accuracy 88.080%\n",
      "Epoch 27, Batch 319, LR 0.000025 Loss 4.871700, Accuracy 88.053%\n",
      "Epoch 27, Batch 320, LR 0.000025 Loss 4.871026, Accuracy 88.071%\n",
      "Epoch 27, Batch 321, LR 0.000025 Loss 4.872675, Accuracy 88.067%\n",
      "Epoch 27, Batch 322, LR 0.000025 Loss 4.873083, Accuracy 88.058%\n",
      "Epoch 27, Batch 323, LR 0.000025 Loss 4.870482, Accuracy 88.064%\n",
      "Epoch 27, Batch 324, LR 0.000025 Loss 4.870120, Accuracy 88.069%\n",
      "Epoch 27, Batch 325, LR 0.000025 Loss 4.871535, Accuracy 88.055%\n",
      "Epoch 27, Batch 326, LR 0.000025 Loss 4.870226, Accuracy 88.070%\n",
      "Epoch 27, Batch 327, LR 0.000025 Loss 4.872095, Accuracy 88.050%\n",
      "Epoch 27, Batch 328, LR 0.000025 Loss 4.874460, Accuracy 88.036%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 329, LR 0.000025 Loss 4.874983, Accuracy 88.037%\n",
      "Epoch 27, Batch 330, LR 0.000025 Loss 4.877135, Accuracy 88.016%\n",
      "Epoch 27, Batch 331, LR 0.000025 Loss 4.878468, Accuracy 88.007%\n",
      "Epoch 27, Batch 332, LR 0.000025 Loss 4.877524, Accuracy 88.020%\n",
      "Epoch 27, Batch 333, LR 0.000025 Loss 4.877827, Accuracy 88.021%\n",
      "Epoch 27, Batch 334, LR 0.000025 Loss 4.878200, Accuracy 88.022%\n",
      "Epoch 27, Batch 335, LR 0.000025 Loss 4.879307, Accuracy 88.020%\n",
      "Epoch 27, Batch 336, LR 0.000025 Loss 4.881184, Accuracy 88.012%\n",
      "Epoch 27, Batch 337, LR 0.000025 Loss 4.879838, Accuracy 88.017%\n",
      "Epoch 27, Batch 338, LR 0.000025 Loss 4.880083, Accuracy 88.015%\n",
      "Epoch 27, Batch 339, LR 0.000025 Loss 4.881080, Accuracy 88.016%\n",
      "Epoch 27, Batch 340, LR 0.000025 Loss 4.880186, Accuracy 88.024%\n",
      "Epoch 27, Batch 341, LR 0.000025 Loss 4.880245, Accuracy 88.020%\n",
      "Epoch 27, Batch 342, LR 0.000025 Loss 4.880125, Accuracy 88.025%\n",
      "Epoch 27, Batch 343, LR 0.000025 Loss 4.880499, Accuracy 88.019%\n",
      "Epoch 27, Batch 344, LR 0.000025 Loss 4.882282, Accuracy 88.009%\n",
      "Epoch 27, Batch 345, LR 0.000025 Loss 4.886224, Accuracy 87.985%\n",
      "Epoch 27, Batch 346, LR 0.000025 Loss 4.887458, Accuracy 87.985%\n",
      "Epoch 27, Batch 347, LR 0.000025 Loss 4.886590, Accuracy 87.980%\n",
      "Epoch 27, Batch 348, LR 0.000025 Loss 4.886056, Accuracy 87.983%\n",
      "Epoch 27, Batch 349, LR 0.000025 Loss 4.887091, Accuracy 87.970%\n",
      "Epoch 27, Batch 350, LR 0.000025 Loss 4.886124, Accuracy 87.973%\n",
      "Epoch 27, Batch 351, LR 0.000025 Loss 4.885586, Accuracy 87.985%\n",
      "Epoch 27, Batch 352, LR 0.000025 Loss 4.885335, Accuracy 87.991%\n",
      "Epoch 27, Batch 353, LR 0.000025 Loss 4.885316, Accuracy 88.002%\n",
      "Epoch 27, Batch 354, LR 0.000025 Loss 4.883180, Accuracy 88.021%\n",
      "Epoch 27, Batch 355, LR 0.000025 Loss 4.882685, Accuracy 88.028%\n",
      "Epoch 27, Batch 356, LR 0.000025 Loss 4.882913, Accuracy 88.029%\n",
      "Epoch 27, Batch 357, LR 0.000025 Loss 4.883411, Accuracy 88.032%\n",
      "Epoch 27, Batch 358, LR 0.000025 Loss 4.883766, Accuracy 88.028%\n",
      "Epoch 27, Batch 359, LR 0.000025 Loss 4.885811, Accuracy 88.018%\n",
      "Epoch 27, Batch 360, LR 0.000025 Loss 4.888060, Accuracy 88.003%\n",
      "Epoch 27, Batch 361, LR 0.000025 Loss 4.888599, Accuracy 88.002%\n",
      "Epoch 27, Batch 362, LR 0.000025 Loss 4.887821, Accuracy 88.003%\n",
      "Epoch 27, Batch 363, LR 0.000025 Loss 4.887822, Accuracy 88.006%\n",
      "Epoch 27, Batch 364, LR 0.000025 Loss 4.889385, Accuracy 87.992%\n",
      "Epoch 27, Batch 365, LR 0.000025 Loss 4.889690, Accuracy 87.994%\n",
      "Epoch 27, Batch 366, LR 0.000025 Loss 4.889760, Accuracy 87.995%\n",
      "Epoch 27, Batch 367, LR 0.000025 Loss 4.888559, Accuracy 88.005%\n",
      "Epoch 27, Batch 368, LR 0.000025 Loss 4.887962, Accuracy 88.024%\n",
      "Epoch 27, Batch 369, LR 0.000025 Loss 4.887581, Accuracy 88.027%\n",
      "Epoch 27, Batch 370, LR 0.000025 Loss 4.889696, Accuracy 88.017%\n",
      "Epoch 27, Batch 371, LR 0.000025 Loss 4.889231, Accuracy 88.016%\n",
      "Epoch 27, Batch 372, LR 0.000025 Loss 4.886680, Accuracy 88.027%\n",
      "Epoch 27, Batch 373, LR 0.000025 Loss 4.886311, Accuracy 88.030%\n",
      "Epoch 27, Batch 374, LR 0.000025 Loss 4.886077, Accuracy 88.037%\n",
      "Epoch 27, Batch 375, LR 0.000025 Loss 4.887343, Accuracy 88.033%\n",
      "Epoch 27, Batch 376, LR 0.000025 Loss 4.887190, Accuracy 88.034%\n",
      "Epoch 27, Batch 377, LR 0.000025 Loss 4.887389, Accuracy 88.043%\n",
      "Epoch 27, Batch 378, LR 0.000025 Loss 4.884780, Accuracy 88.060%\n",
      "Epoch 27, Batch 379, LR 0.000025 Loss 4.885253, Accuracy 88.061%\n",
      "Epoch 27, Batch 380, LR 0.000025 Loss 4.884516, Accuracy 88.061%\n",
      "Epoch 27, Batch 381, LR 0.000025 Loss 4.885412, Accuracy 88.056%\n",
      "Epoch 27, Batch 382, LR 0.000025 Loss 4.885435, Accuracy 88.052%\n",
      "Epoch 27, Batch 383, LR 0.000025 Loss 4.885091, Accuracy 88.053%\n",
      "Epoch 27, Batch 384, LR 0.000025 Loss 4.883779, Accuracy 88.057%\n",
      "Epoch 27, Batch 385, LR 0.000025 Loss 4.882289, Accuracy 88.062%\n",
      "Epoch 27, Batch 386, LR 0.000025 Loss 4.883903, Accuracy 88.038%\n",
      "Epoch 27, Batch 387, LR 0.000025 Loss 4.883942, Accuracy 88.043%\n",
      "Epoch 27, Batch 388, LR 0.000025 Loss 4.883841, Accuracy 88.046%\n",
      "Epoch 27, Batch 389, LR 0.000025 Loss 4.883996, Accuracy 88.044%\n",
      "Epoch 27, Batch 390, LR 0.000025 Loss 4.883534, Accuracy 88.047%\n",
      "Epoch 27, Batch 391, LR 0.000025 Loss 4.885173, Accuracy 88.039%\n",
      "Epoch 27, Batch 392, LR 0.000025 Loss 4.885127, Accuracy 88.034%\n",
      "Epoch 27, Batch 393, LR 0.000025 Loss 4.885865, Accuracy 88.025%\n",
      "Epoch 27, Batch 394, LR 0.000025 Loss 4.884636, Accuracy 88.021%\n",
      "Epoch 27, Batch 395, LR 0.000025 Loss 4.885500, Accuracy 88.018%\n",
      "Epoch 27, Batch 396, LR 0.000025 Loss 4.884650, Accuracy 88.025%\n",
      "Epoch 27, Batch 397, LR 0.000025 Loss 4.882952, Accuracy 88.035%\n",
      "Epoch 27, Batch 398, LR 0.000025 Loss 4.882949, Accuracy 88.026%\n",
      "Epoch 27, Batch 399, LR 0.000025 Loss 4.883160, Accuracy 88.031%\n",
      "Epoch 27, Batch 400, LR 0.000025 Loss 4.881154, Accuracy 88.037%\n",
      "Epoch 27, Batch 401, LR 0.000025 Loss 4.880326, Accuracy 88.038%\n",
      "Epoch 27, Batch 402, LR 0.000025 Loss 4.879432, Accuracy 88.042%\n",
      "Epoch 27, Batch 403, LR 0.000025 Loss 4.880377, Accuracy 88.035%\n",
      "Epoch 27, Batch 404, LR 0.000025 Loss 4.881864, Accuracy 88.020%\n",
      "Epoch 27, Batch 405, LR 0.000025 Loss 4.883210, Accuracy 88.021%\n",
      "Epoch 27, Batch 406, LR 0.000025 Loss 4.881958, Accuracy 88.023%\n",
      "Epoch 27, Batch 407, LR 0.000025 Loss 4.883257, Accuracy 88.013%\n",
      "Epoch 27, Batch 408, LR 0.000025 Loss 4.883256, Accuracy 88.025%\n",
      "Epoch 27, Batch 409, LR 0.000025 Loss 4.882660, Accuracy 88.031%\n",
      "Epoch 27, Batch 410, LR 0.000025 Loss 4.882151, Accuracy 88.047%\n",
      "Epoch 27, Batch 411, LR 0.000025 Loss 4.882523, Accuracy 88.049%\n",
      "Epoch 27, Batch 412, LR 0.000025 Loss 4.883325, Accuracy 88.048%\n",
      "Epoch 27, Batch 413, LR 0.000025 Loss 4.885200, Accuracy 88.041%\n",
      "Epoch 27, Batch 414, LR 0.000025 Loss 4.885954, Accuracy 88.028%\n",
      "Epoch 27, Batch 415, LR 0.000025 Loss 4.884712, Accuracy 88.023%\n",
      "Epoch 27, Batch 416, LR 0.000025 Loss 4.886087, Accuracy 88.018%\n",
      "Epoch 27, Batch 417, LR 0.000025 Loss 4.888129, Accuracy 88.011%\n",
      "Epoch 27, Batch 418, LR 0.000025 Loss 4.887971, Accuracy 88.001%\n",
      "Epoch 27, Batch 419, LR 0.000025 Loss 4.887413, Accuracy 87.998%\n",
      "Epoch 27, Batch 420, LR 0.000025 Loss 4.886487, Accuracy 88.012%\n",
      "Epoch 27, Batch 421, LR 0.000025 Loss 4.885288, Accuracy 88.016%\n",
      "Epoch 27, Batch 422, LR 0.000025 Loss 4.886035, Accuracy 88.013%\n",
      "Epoch 27, Batch 423, LR 0.000025 Loss 4.886530, Accuracy 88.008%\n",
      "Epoch 27, Batch 424, LR 0.000025 Loss 4.886157, Accuracy 88.007%\n",
      "Epoch 27, Batch 425, LR 0.000025 Loss 4.886088, Accuracy 88.013%\n",
      "Epoch 27, Batch 426, LR 0.000025 Loss 4.885062, Accuracy 88.017%\n",
      "Epoch 27, Batch 427, LR 0.000025 Loss 4.885916, Accuracy 88.025%\n",
      "Epoch 27, Batch 428, LR 0.000025 Loss 4.883429, Accuracy 88.040%\n",
      "Epoch 27, Batch 429, LR 0.000025 Loss 4.881195, Accuracy 88.052%\n",
      "Epoch 27, Batch 430, LR 0.000025 Loss 4.880182, Accuracy 88.063%\n",
      "Epoch 27, Batch 431, LR 0.000025 Loss 4.879176, Accuracy 88.073%\n",
      "Epoch 27, Batch 432, LR 0.000025 Loss 4.879594, Accuracy 88.075%\n",
      "Epoch 27, Batch 433, LR 0.000025 Loss 4.878219, Accuracy 88.085%\n",
      "Epoch 27, Batch 434, LR 0.000025 Loss 4.876836, Accuracy 88.090%\n",
      "Epoch 27, Batch 435, LR 0.000025 Loss 4.877835, Accuracy 88.084%\n",
      "Epoch 27, Batch 436, LR 0.000025 Loss 4.878438, Accuracy 88.079%\n",
      "Epoch 27, Batch 437, LR 0.000025 Loss 4.879001, Accuracy 88.070%\n",
      "Epoch 27, Batch 438, LR 0.000025 Loss 4.879486, Accuracy 88.060%\n",
      "Epoch 27, Batch 439, LR 0.000025 Loss 4.881657, Accuracy 88.052%\n",
      "Epoch 27, Batch 440, LR 0.000025 Loss 4.882020, Accuracy 88.047%\n",
      "Epoch 27, Batch 441, LR 0.000025 Loss 4.881402, Accuracy 88.046%\n",
      "Epoch 27, Batch 442, LR 0.000025 Loss 4.881228, Accuracy 88.055%\n",
      "Epoch 27, Batch 443, LR 0.000025 Loss 4.881863, Accuracy 88.057%\n",
      "Epoch 27, Batch 444, LR 0.000025 Loss 4.881413, Accuracy 88.060%\n",
      "Epoch 27, Batch 445, LR 0.000025 Loss 4.882434, Accuracy 88.050%\n",
      "Epoch 27, Batch 446, LR 0.000025 Loss 4.882481, Accuracy 88.050%\n",
      "Epoch 27, Batch 447, LR 0.000025 Loss 4.883489, Accuracy 88.042%\n",
      "Epoch 27, Batch 448, LR 0.000025 Loss 4.883076, Accuracy 88.048%\n",
      "Epoch 27, Batch 449, LR 0.000025 Loss 4.884857, Accuracy 88.048%\n",
      "Epoch 27, Batch 450, LR 0.000025 Loss 4.884649, Accuracy 88.056%\n",
      "Epoch 27, Batch 451, LR 0.000025 Loss 4.885010, Accuracy 88.044%\n",
      "Epoch 27, Batch 452, LR 0.000025 Loss 4.884842, Accuracy 88.050%\n",
      "Epoch 27, Batch 453, LR 0.000025 Loss 4.884409, Accuracy 88.050%\n",
      "Epoch 27, Batch 454, LR 0.000025 Loss 4.883103, Accuracy 88.058%\n",
      "Epoch 27, Batch 455, LR 0.000025 Loss 4.883987, Accuracy 88.046%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 456, LR 0.000025 Loss 4.884174, Accuracy 88.052%\n",
      "Epoch 27, Batch 457, LR 0.000025 Loss 4.885333, Accuracy 88.042%\n",
      "Epoch 27, Batch 458, LR 0.000025 Loss 4.883796, Accuracy 88.051%\n",
      "Epoch 27, Batch 459, LR 0.000025 Loss 4.884365, Accuracy 88.048%\n",
      "Epoch 27, Batch 460, LR 0.000025 Loss 4.884371, Accuracy 88.043%\n",
      "Epoch 27, Batch 461, LR 0.000025 Loss 4.883072, Accuracy 88.047%\n",
      "Epoch 27, Batch 462, LR 0.000025 Loss 4.882932, Accuracy 88.046%\n",
      "Epoch 27, Batch 463, LR 0.000025 Loss 4.883625, Accuracy 88.035%\n",
      "Epoch 27, Batch 464, LR 0.000025 Loss 4.883314, Accuracy 88.039%\n",
      "Epoch 27, Batch 465, LR 0.000025 Loss 4.883360, Accuracy 88.038%\n",
      "Epoch 27, Batch 466, LR 0.000025 Loss 4.883051, Accuracy 88.036%\n",
      "Epoch 27, Batch 467, LR 0.000025 Loss 4.883995, Accuracy 88.030%\n",
      "Epoch 27, Batch 468, LR 0.000025 Loss 4.884420, Accuracy 88.029%\n",
      "Epoch 27, Batch 469, LR 0.000025 Loss 4.885363, Accuracy 88.026%\n",
      "Epoch 27, Batch 470, LR 0.000025 Loss 4.885241, Accuracy 88.025%\n",
      "Epoch 27, Batch 471, LR 0.000025 Loss 4.887252, Accuracy 88.021%\n",
      "Epoch 27, Batch 472, LR 0.000025 Loss 4.886960, Accuracy 88.020%\n",
      "Epoch 27, Batch 473, LR 0.000025 Loss 4.887610, Accuracy 88.017%\n",
      "Epoch 27, Batch 474, LR 0.000025 Loss 4.887969, Accuracy 88.016%\n",
      "Epoch 27, Batch 475, LR 0.000025 Loss 4.887582, Accuracy 88.026%\n",
      "Epoch 27, Batch 476, LR 0.000025 Loss 4.888138, Accuracy 88.024%\n",
      "Epoch 27, Batch 477, LR 0.000025 Loss 4.887880, Accuracy 88.022%\n",
      "Epoch 27, Batch 478, LR 0.000025 Loss 4.887596, Accuracy 88.028%\n",
      "Epoch 27, Batch 479, LR 0.000025 Loss 4.888143, Accuracy 88.030%\n",
      "Epoch 27, Batch 480, LR 0.000025 Loss 4.887881, Accuracy 88.032%\n",
      "Epoch 27, Batch 481, LR 0.000025 Loss 4.888286, Accuracy 88.026%\n",
      "Epoch 27, Batch 482, LR 0.000025 Loss 4.889329, Accuracy 88.022%\n",
      "Epoch 27, Batch 483, LR 0.000025 Loss 4.889778, Accuracy 88.024%\n",
      "Epoch 27, Batch 484, LR 0.000025 Loss 4.890576, Accuracy 88.018%\n",
      "Epoch 27, Batch 485, LR 0.000025 Loss 4.890455, Accuracy 88.019%\n",
      "Epoch 27, Batch 486, LR 0.000025 Loss 4.889117, Accuracy 88.019%\n",
      "Epoch 27, Batch 487, LR 0.000025 Loss 4.887969, Accuracy 88.025%\n",
      "Epoch 27, Batch 488, LR 0.000025 Loss 4.887763, Accuracy 88.028%\n",
      "Epoch 27, Batch 489, LR 0.000025 Loss 4.886911, Accuracy 88.035%\n",
      "Epoch 27, Batch 490, LR 0.000025 Loss 4.887247, Accuracy 88.033%\n",
      "Epoch 27, Batch 491, LR 0.000025 Loss 4.887434, Accuracy 88.033%\n",
      "Epoch 27, Batch 492, LR 0.000025 Loss 4.887794, Accuracy 88.032%\n",
      "Epoch 27, Batch 493, LR 0.000025 Loss 4.888395, Accuracy 88.028%\n",
      "Epoch 27, Batch 494, LR 0.000025 Loss 4.888560, Accuracy 88.027%\n",
      "Epoch 27, Batch 495, LR 0.000025 Loss 4.887840, Accuracy 88.029%\n",
      "Epoch 27, Batch 496, LR 0.000024 Loss 4.887407, Accuracy 88.032%\n",
      "Epoch 27, Batch 497, LR 0.000024 Loss 4.888269, Accuracy 88.034%\n",
      "Epoch 27, Batch 498, LR 0.000024 Loss 4.887782, Accuracy 88.030%\n",
      "Epoch 27, Batch 499, LR 0.000024 Loss 4.887425, Accuracy 88.037%\n",
      "Epoch 27, Batch 500, LR 0.000024 Loss 4.888566, Accuracy 88.030%\n",
      "Epoch 27, Batch 501, LR 0.000024 Loss 4.888429, Accuracy 88.024%\n",
      "Epoch 27, Batch 502, LR 0.000024 Loss 4.888705, Accuracy 88.026%\n",
      "Epoch 27, Batch 503, LR 0.000024 Loss 4.889897, Accuracy 88.017%\n",
      "Epoch 27, Batch 504, LR 0.000024 Loss 4.890064, Accuracy 88.019%\n",
      "Epoch 27, Batch 505, LR 0.000024 Loss 4.890645, Accuracy 88.018%\n",
      "Epoch 27, Batch 506, LR 0.000024 Loss 4.891997, Accuracy 88.006%\n",
      "Epoch 27, Batch 507, LR 0.000024 Loss 4.890971, Accuracy 88.013%\n",
      "Epoch 27, Batch 508, LR 0.000024 Loss 4.890047, Accuracy 88.021%\n",
      "Epoch 27, Batch 509, LR 0.000024 Loss 4.891213, Accuracy 88.017%\n",
      "Epoch 27, Batch 510, LR 0.000024 Loss 4.890992, Accuracy 88.012%\n",
      "Epoch 27, Batch 511, LR 0.000024 Loss 4.890986, Accuracy 88.006%\n",
      "Epoch 27, Batch 512, LR 0.000024 Loss 4.889768, Accuracy 88.017%\n",
      "Epoch 27, Batch 513, LR 0.000024 Loss 4.887202, Accuracy 88.030%\n",
      "Epoch 27, Batch 514, LR 0.000024 Loss 4.886651, Accuracy 88.035%\n",
      "Epoch 27, Batch 515, LR 0.000024 Loss 4.886723, Accuracy 88.034%\n",
      "Epoch 27, Batch 516, LR 0.000024 Loss 4.887700, Accuracy 88.025%\n",
      "Epoch 27, Batch 517, LR 0.000024 Loss 4.887417, Accuracy 88.029%\n",
      "Epoch 27, Batch 518, LR 0.000024 Loss 4.887166, Accuracy 88.028%\n",
      "Epoch 27, Batch 519, LR 0.000024 Loss 4.886580, Accuracy 88.031%\n",
      "Epoch 27, Batch 520, LR 0.000024 Loss 4.886220, Accuracy 88.035%\n",
      "Epoch 27, Batch 521, LR 0.000024 Loss 4.885705, Accuracy 88.037%\n",
      "Epoch 27, Batch 522, LR 0.000024 Loss 4.884765, Accuracy 88.042%\n",
      "Epoch 27, Batch 523, LR 0.000024 Loss 4.884778, Accuracy 88.039%\n",
      "Epoch 27, Batch 524, LR 0.000024 Loss 4.882969, Accuracy 88.050%\n",
      "Epoch 27, Batch 525, LR 0.000024 Loss 4.882638, Accuracy 88.052%\n",
      "Epoch 27, Batch 526, LR 0.000024 Loss 4.883411, Accuracy 88.045%\n",
      "Epoch 27, Batch 527, LR 0.000024 Loss 4.883799, Accuracy 88.043%\n",
      "Epoch 27, Batch 528, LR 0.000024 Loss 4.883741, Accuracy 88.043%\n",
      "Epoch 27, Batch 529, LR 0.000024 Loss 4.882853, Accuracy 88.051%\n",
      "Epoch 27, Batch 530, LR 0.000024 Loss 4.883661, Accuracy 88.044%\n",
      "Epoch 27, Batch 531, LR 0.000024 Loss 4.882777, Accuracy 88.041%\n",
      "Epoch 27, Batch 532, LR 0.000024 Loss 4.883448, Accuracy 88.039%\n",
      "Epoch 27, Batch 533, LR 0.000024 Loss 4.883488, Accuracy 88.039%\n",
      "Epoch 27, Batch 534, LR 0.000024 Loss 4.884229, Accuracy 88.031%\n",
      "Epoch 27, Batch 535, LR 0.000024 Loss 4.885303, Accuracy 88.023%\n",
      "Epoch 27, Batch 536, LR 0.000024 Loss 4.885730, Accuracy 88.020%\n",
      "Epoch 27, Batch 537, LR 0.000024 Loss 4.885711, Accuracy 88.021%\n",
      "Epoch 27, Batch 538, LR 0.000024 Loss 4.887216, Accuracy 88.014%\n",
      "Epoch 27, Batch 539, LR 0.000024 Loss 4.886902, Accuracy 88.013%\n",
      "Epoch 27, Batch 540, LR 0.000024 Loss 4.886757, Accuracy 88.016%\n",
      "Epoch 27, Batch 541, LR 0.000024 Loss 4.885544, Accuracy 88.017%\n",
      "Epoch 27, Batch 542, LR 0.000024 Loss 4.884099, Accuracy 88.025%\n",
      "Epoch 27, Batch 543, LR 0.000024 Loss 4.884757, Accuracy 88.025%\n",
      "Epoch 27, Batch 544, LR 0.000024 Loss 4.883833, Accuracy 88.020%\n",
      "Epoch 27, Batch 545, LR 0.000024 Loss 4.884157, Accuracy 88.022%\n",
      "Epoch 27, Batch 546, LR 0.000024 Loss 4.884069, Accuracy 88.019%\n",
      "Epoch 27, Batch 547, LR 0.000024 Loss 4.883985, Accuracy 88.018%\n",
      "Epoch 27, Batch 548, LR 0.000024 Loss 4.883839, Accuracy 88.018%\n",
      "Epoch 27, Batch 549, LR 0.000024 Loss 4.883239, Accuracy 88.021%\n",
      "Epoch 27, Batch 550, LR 0.000024 Loss 4.884019, Accuracy 88.017%\n",
      "Epoch 27, Batch 551, LR 0.000024 Loss 4.883968, Accuracy 88.020%\n",
      "Epoch 27, Batch 552, LR 0.000024 Loss 4.883899, Accuracy 88.022%\n",
      "Epoch 27, Batch 553, LR 0.000024 Loss 4.882621, Accuracy 88.031%\n",
      "Epoch 27, Batch 554, LR 0.000024 Loss 4.882414, Accuracy 88.027%\n",
      "Epoch 27, Batch 555, LR 0.000024 Loss 4.881957, Accuracy 88.034%\n",
      "Epoch 27, Batch 556, LR 0.000024 Loss 4.882203, Accuracy 88.035%\n",
      "Epoch 27, Batch 557, LR 0.000024 Loss 4.882623, Accuracy 88.030%\n",
      "Epoch 27, Batch 558, LR 0.000024 Loss 4.881910, Accuracy 88.029%\n",
      "Epoch 27, Batch 559, LR 0.000024 Loss 4.881562, Accuracy 88.030%\n",
      "Epoch 27, Batch 560, LR 0.000024 Loss 4.882874, Accuracy 88.025%\n",
      "Epoch 27, Batch 561, LR 0.000024 Loss 4.883181, Accuracy 88.024%\n",
      "Epoch 27, Batch 562, LR 0.000024 Loss 4.883677, Accuracy 88.023%\n",
      "Epoch 27, Batch 563, LR 0.000024 Loss 4.883140, Accuracy 88.026%\n",
      "Epoch 27, Batch 564, LR 0.000024 Loss 4.882701, Accuracy 88.028%\n",
      "Epoch 27, Batch 565, LR 0.000024 Loss 4.880894, Accuracy 88.032%\n",
      "Epoch 27, Batch 566, LR 0.000024 Loss 4.880163, Accuracy 88.031%\n",
      "Epoch 27, Batch 567, LR 0.000024 Loss 4.879609, Accuracy 88.032%\n",
      "Epoch 27, Batch 568, LR 0.000024 Loss 4.879740, Accuracy 88.027%\n",
      "Epoch 27, Batch 569, LR 0.000024 Loss 4.879512, Accuracy 88.027%\n",
      "Epoch 27, Batch 570, LR 0.000024 Loss 4.879069, Accuracy 88.026%\n",
      "Epoch 27, Batch 571, LR 0.000024 Loss 4.879400, Accuracy 88.028%\n",
      "Epoch 27, Batch 572, LR 0.000024 Loss 4.879822, Accuracy 88.020%\n",
      "Epoch 27, Batch 573, LR 0.000024 Loss 4.879955, Accuracy 88.024%\n",
      "Epoch 27, Batch 574, LR 0.000024 Loss 4.880127, Accuracy 88.021%\n",
      "Epoch 27, Batch 575, LR 0.000024 Loss 4.880355, Accuracy 88.018%\n",
      "Epoch 27, Batch 576, LR 0.000024 Loss 4.881328, Accuracy 88.007%\n",
      "Epoch 27, Batch 577, LR 0.000024 Loss 4.883388, Accuracy 88.000%\n",
      "Epoch 27, Batch 578, LR 0.000024 Loss 4.882997, Accuracy 87.997%\n",
      "Epoch 27, Batch 579, LR 0.000024 Loss 4.883111, Accuracy 87.992%\n",
      "Epoch 27, Batch 580, LR 0.000024 Loss 4.883038, Accuracy 87.990%\n",
      "Epoch 27, Batch 581, LR 0.000024 Loss 4.882769, Accuracy 87.992%\n",
      "Epoch 27, Batch 582, LR 0.000024 Loss 4.883966, Accuracy 87.983%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 583, LR 0.000024 Loss 4.885362, Accuracy 87.973%\n",
      "Epoch 27, Batch 584, LR 0.000024 Loss 4.886223, Accuracy 87.975%\n",
      "Epoch 27, Batch 585, LR 0.000024 Loss 4.885432, Accuracy 87.978%\n",
      "Epoch 27, Batch 586, LR 0.000024 Loss 4.885272, Accuracy 87.976%\n",
      "Epoch 27, Batch 587, LR 0.000024 Loss 4.884647, Accuracy 87.979%\n",
      "Epoch 27, Batch 588, LR 0.000024 Loss 4.884549, Accuracy 87.985%\n",
      "Epoch 27, Batch 589, LR 0.000024 Loss 4.884386, Accuracy 87.989%\n",
      "Epoch 27, Batch 590, LR 0.000024 Loss 4.886283, Accuracy 87.985%\n",
      "Epoch 27, Batch 591, LR 0.000024 Loss 4.885922, Accuracy 87.986%\n",
      "Epoch 27, Batch 592, LR 0.000024 Loss 4.886079, Accuracy 87.986%\n",
      "Epoch 27, Batch 593, LR 0.000024 Loss 4.887282, Accuracy 87.972%\n",
      "Epoch 27, Batch 594, LR 0.000024 Loss 4.887860, Accuracy 87.971%\n",
      "Epoch 27, Batch 595, LR 0.000024 Loss 4.887679, Accuracy 87.971%\n",
      "Epoch 27, Batch 596, LR 0.000024 Loss 4.886651, Accuracy 87.976%\n",
      "Epoch 27, Batch 597, LR 0.000024 Loss 4.885366, Accuracy 87.984%\n",
      "Epoch 27, Batch 598, LR 0.000024 Loss 4.884978, Accuracy 87.986%\n",
      "Epoch 27, Batch 599, LR 0.000024 Loss 4.883989, Accuracy 87.984%\n",
      "Epoch 27, Batch 600, LR 0.000024 Loss 4.884572, Accuracy 87.980%\n",
      "Epoch 27, Batch 601, LR 0.000024 Loss 4.885723, Accuracy 87.973%\n",
      "Epoch 27, Batch 602, LR 0.000024 Loss 4.886169, Accuracy 87.972%\n",
      "Epoch 27, Batch 603, LR 0.000024 Loss 4.886055, Accuracy 87.972%\n",
      "Epoch 27, Batch 604, LR 0.000024 Loss 4.885761, Accuracy 87.970%\n",
      "Epoch 27, Batch 605, LR 0.000024 Loss 4.885505, Accuracy 87.975%\n",
      "Epoch 27, Batch 606, LR 0.000024 Loss 4.884971, Accuracy 87.976%\n",
      "Epoch 27, Batch 607, LR 0.000024 Loss 4.884902, Accuracy 87.972%\n",
      "Epoch 27, Batch 608, LR 0.000024 Loss 4.885075, Accuracy 87.977%\n",
      "Epoch 27, Batch 609, LR 0.000024 Loss 4.885268, Accuracy 87.975%\n",
      "Epoch 27, Batch 610, LR 0.000024 Loss 4.885161, Accuracy 87.973%\n",
      "Epoch 27, Batch 611, LR 0.000024 Loss 4.884265, Accuracy 87.974%\n",
      "Epoch 27, Batch 612, LR 0.000024 Loss 4.885741, Accuracy 87.963%\n",
      "Epoch 27, Batch 613, LR 0.000024 Loss 4.886059, Accuracy 87.960%\n",
      "Epoch 27, Batch 614, LR 0.000024 Loss 4.887556, Accuracy 87.959%\n",
      "Epoch 27, Batch 615, LR 0.000024 Loss 4.885903, Accuracy 87.971%\n",
      "Epoch 27, Batch 616, LR 0.000024 Loss 4.885496, Accuracy 87.973%\n",
      "Epoch 27, Batch 617, LR 0.000024 Loss 4.884755, Accuracy 87.979%\n",
      "Epoch 27, Batch 618, LR 0.000024 Loss 4.885127, Accuracy 87.984%\n",
      "Epoch 27, Batch 619, LR 0.000024 Loss 4.885208, Accuracy 87.980%\n",
      "Epoch 27, Batch 620, LR 0.000024 Loss 4.885358, Accuracy 87.976%\n",
      "Epoch 27, Batch 621, LR 0.000024 Loss 4.883255, Accuracy 87.987%\n",
      "Epoch 27, Batch 622, LR 0.000024 Loss 4.882226, Accuracy 87.987%\n",
      "Epoch 27, Batch 623, LR 0.000024 Loss 4.882055, Accuracy 87.982%\n",
      "Epoch 27, Batch 624, LR 0.000024 Loss 4.881220, Accuracy 87.988%\n",
      "Epoch 27, Batch 625, LR 0.000024 Loss 4.881142, Accuracy 87.985%\n",
      "Epoch 27, Batch 626, LR 0.000024 Loss 4.880557, Accuracy 87.988%\n",
      "Epoch 27, Batch 627, LR 0.000024 Loss 4.880205, Accuracy 87.983%\n",
      "Epoch 27, Batch 628, LR 0.000024 Loss 4.880005, Accuracy 87.989%\n",
      "Epoch 27, Batch 629, LR 0.000024 Loss 4.880838, Accuracy 87.987%\n",
      "Epoch 27, Batch 630, LR 0.000024 Loss 4.880318, Accuracy 87.992%\n",
      "Epoch 27, Batch 631, LR 0.000024 Loss 4.880101, Accuracy 87.994%\n",
      "Epoch 27, Batch 632, LR 0.000024 Loss 4.880335, Accuracy 87.992%\n",
      "Epoch 27, Batch 633, LR 0.000024 Loss 4.880410, Accuracy 87.991%\n",
      "Epoch 27, Batch 634, LR 0.000024 Loss 4.879774, Accuracy 87.997%\n",
      "Epoch 27, Batch 635, LR 0.000024 Loss 4.879097, Accuracy 88.003%\n",
      "Epoch 27, Batch 636, LR 0.000024 Loss 4.879590, Accuracy 88.002%\n",
      "Epoch 27, Batch 637, LR 0.000024 Loss 4.878768, Accuracy 88.003%\n",
      "Epoch 27, Batch 638, LR 0.000024 Loss 4.879479, Accuracy 88.002%\n",
      "Epoch 27, Batch 639, LR 0.000024 Loss 4.879204, Accuracy 88.005%\n",
      "Epoch 27, Batch 640, LR 0.000024 Loss 4.879950, Accuracy 88.004%\n",
      "Epoch 27, Batch 641, LR 0.000024 Loss 4.879689, Accuracy 88.008%\n",
      "Epoch 27, Batch 642, LR 0.000024 Loss 4.879692, Accuracy 88.012%\n",
      "Epoch 27, Batch 643, LR 0.000024 Loss 4.878858, Accuracy 88.018%\n",
      "Epoch 27, Batch 644, LR 0.000024 Loss 4.878966, Accuracy 88.020%\n",
      "Epoch 27, Batch 645, LR 0.000024 Loss 4.879623, Accuracy 88.015%\n",
      "Epoch 27, Batch 646, LR 0.000024 Loss 4.880199, Accuracy 88.008%\n",
      "Epoch 27, Batch 647, LR 0.000024 Loss 4.880033, Accuracy 88.006%\n",
      "Epoch 27, Batch 648, LR 0.000024 Loss 4.880305, Accuracy 88.006%\n",
      "Epoch 27, Batch 649, LR 0.000024 Loss 4.879717, Accuracy 88.008%\n",
      "Epoch 27, Batch 650, LR 0.000024 Loss 4.878608, Accuracy 88.018%\n",
      "Epoch 27, Batch 651, LR 0.000024 Loss 4.879429, Accuracy 88.010%\n",
      "Epoch 27, Batch 652, LR 0.000024 Loss 4.879806, Accuracy 88.007%\n",
      "Epoch 27, Batch 653, LR 0.000024 Loss 4.880307, Accuracy 88.005%\n",
      "Epoch 27, Batch 654, LR 0.000024 Loss 4.880490, Accuracy 88.004%\n",
      "Epoch 27, Batch 655, LR 0.000024 Loss 4.879434, Accuracy 88.009%\n",
      "Epoch 27, Batch 656, LR 0.000024 Loss 4.880207, Accuracy 88.005%\n",
      "Epoch 27, Batch 657, LR 0.000024 Loss 4.879503, Accuracy 88.010%\n",
      "Epoch 27, Batch 658, LR 0.000024 Loss 4.879838, Accuracy 88.008%\n",
      "Epoch 27, Batch 659, LR 0.000024 Loss 4.881072, Accuracy 87.998%\n",
      "Epoch 27, Batch 660, LR 0.000024 Loss 4.880981, Accuracy 88.003%\n",
      "Epoch 27, Batch 661, LR 0.000024 Loss 4.880865, Accuracy 88.005%\n",
      "Epoch 27, Batch 662, LR 0.000024 Loss 4.881852, Accuracy 88.003%\n",
      "Epoch 27, Batch 663, LR 0.000024 Loss 4.881970, Accuracy 87.997%\n",
      "Epoch 27, Batch 664, LR 0.000024 Loss 4.882356, Accuracy 88.001%\n",
      "Epoch 27, Batch 665, LR 0.000024 Loss 4.882450, Accuracy 88.003%\n",
      "Epoch 27, Batch 666, LR 0.000024 Loss 4.882524, Accuracy 88.003%\n",
      "Epoch 27, Batch 667, LR 0.000024 Loss 4.882120, Accuracy 88.005%\n",
      "Epoch 27, Batch 668, LR 0.000024 Loss 4.882426, Accuracy 87.999%\n",
      "Epoch 27, Batch 669, LR 0.000024 Loss 4.882094, Accuracy 88.004%\n",
      "Epoch 27, Batch 670, LR 0.000024 Loss 4.880734, Accuracy 88.010%\n",
      "Epoch 27, Batch 671, LR 0.000024 Loss 4.878884, Accuracy 88.015%\n",
      "Epoch 27, Batch 672, LR 0.000024 Loss 4.877683, Accuracy 88.023%\n",
      "Epoch 27, Batch 673, LR 0.000024 Loss 4.877253, Accuracy 88.026%\n",
      "Epoch 27, Batch 674, LR 0.000024 Loss 4.876342, Accuracy 88.024%\n",
      "Epoch 27, Batch 675, LR 0.000024 Loss 4.875172, Accuracy 88.027%\n",
      "Epoch 27, Batch 676, LR 0.000024 Loss 4.876377, Accuracy 88.019%\n",
      "Epoch 27, Batch 677, LR 0.000024 Loss 4.876197, Accuracy 88.018%\n",
      "Epoch 27, Batch 678, LR 0.000024 Loss 4.876132, Accuracy 88.019%\n",
      "Epoch 27, Batch 679, LR 0.000024 Loss 4.876911, Accuracy 88.015%\n",
      "Epoch 27, Batch 680, LR 0.000024 Loss 4.876749, Accuracy 88.017%\n",
      "Epoch 27, Batch 681, LR 0.000024 Loss 4.878152, Accuracy 88.009%\n",
      "Epoch 27, Batch 682, LR 0.000024 Loss 4.878754, Accuracy 88.007%\n",
      "Epoch 27, Batch 683, LR 0.000024 Loss 4.878045, Accuracy 88.009%\n",
      "Epoch 27, Batch 684, LR 0.000024 Loss 4.878265, Accuracy 88.006%\n",
      "Epoch 27, Batch 685, LR 0.000024 Loss 4.878653, Accuracy 88.009%\n",
      "Epoch 27, Batch 686, LR 0.000024 Loss 4.877904, Accuracy 88.015%\n",
      "Epoch 27, Batch 687, LR 0.000024 Loss 4.879190, Accuracy 88.007%\n",
      "Epoch 27, Batch 688, LR 0.000024 Loss 4.878253, Accuracy 88.014%\n",
      "Epoch 27, Batch 689, LR 0.000024 Loss 4.878653, Accuracy 88.016%\n",
      "Epoch 27, Batch 690, LR 0.000024 Loss 4.879073, Accuracy 88.008%\n",
      "Epoch 27, Batch 691, LR 0.000024 Loss 4.879126, Accuracy 88.003%\n",
      "Epoch 27, Batch 692, LR 0.000024 Loss 4.879765, Accuracy 88.000%\n",
      "Epoch 27, Batch 693, LR 0.000024 Loss 4.880229, Accuracy 87.997%\n",
      "Epoch 27, Batch 694, LR 0.000024 Loss 4.879179, Accuracy 88.003%\n",
      "Epoch 27, Batch 695, LR 0.000024 Loss 4.878524, Accuracy 88.006%\n",
      "Epoch 27, Batch 696, LR 0.000024 Loss 4.878056, Accuracy 88.012%\n",
      "Epoch 27, Batch 697, LR 0.000024 Loss 4.878530, Accuracy 88.012%\n",
      "Epoch 27, Batch 698, LR 0.000024 Loss 4.879423, Accuracy 88.012%\n",
      "Epoch 27, Batch 699, LR 0.000024 Loss 4.879340, Accuracy 88.016%\n",
      "Epoch 27, Batch 700, LR 0.000024 Loss 4.878608, Accuracy 88.020%\n",
      "Epoch 27, Batch 701, LR 0.000024 Loss 4.878596, Accuracy 88.022%\n",
      "Epoch 27, Batch 702, LR 0.000024 Loss 4.877962, Accuracy 88.028%\n",
      "Epoch 27, Batch 703, LR 0.000024 Loss 4.878896, Accuracy 88.021%\n",
      "Epoch 27, Batch 704, LR 0.000024 Loss 4.878287, Accuracy 88.026%\n",
      "Epoch 27, Batch 705, LR 0.000024 Loss 4.877825, Accuracy 88.030%\n",
      "Epoch 27, Batch 706, LR 0.000024 Loss 4.878864, Accuracy 88.023%\n",
      "Epoch 27, Batch 707, LR 0.000024 Loss 4.878892, Accuracy 88.027%\n",
      "Epoch 27, Batch 708, LR 0.000024 Loss 4.878971, Accuracy 88.023%\n",
      "Epoch 27, Batch 709, LR 0.000024 Loss 4.879444, Accuracy 88.019%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 710, LR 0.000024 Loss 4.878982, Accuracy 88.020%\n",
      "Epoch 27, Batch 711, LR 0.000024 Loss 4.878877, Accuracy 88.022%\n",
      "Epoch 27, Batch 712, LR 0.000024 Loss 4.878396, Accuracy 88.023%\n",
      "Epoch 27, Batch 713, LR 0.000024 Loss 4.879203, Accuracy 88.022%\n",
      "Epoch 27, Batch 714, LR 0.000024 Loss 4.878708, Accuracy 88.021%\n",
      "Epoch 27, Batch 715, LR 0.000024 Loss 4.879410, Accuracy 88.018%\n",
      "Epoch 27, Batch 716, LR 0.000024 Loss 4.880273, Accuracy 88.014%\n",
      "Epoch 27, Batch 717, LR 0.000024 Loss 4.880938, Accuracy 88.018%\n",
      "Epoch 27, Batch 718, LR 0.000024 Loss 4.881176, Accuracy 88.018%\n",
      "Epoch 27, Batch 719, LR 0.000024 Loss 4.881299, Accuracy 88.020%\n",
      "Epoch 27, Batch 720, LR 0.000024 Loss 4.881708, Accuracy 88.014%\n",
      "Epoch 27, Batch 721, LR 0.000024 Loss 4.881396, Accuracy 88.015%\n",
      "Epoch 27, Batch 722, LR 0.000024 Loss 4.881574, Accuracy 88.007%\n",
      "Epoch 27, Batch 723, LR 0.000024 Loss 4.881793, Accuracy 88.008%\n",
      "Epoch 27, Batch 724, LR 0.000024 Loss 4.881243, Accuracy 88.014%\n",
      "Epoch 27, Batch 725, LR 0.000024 Loss 4.882203, Accuracy 88.011%\n",
      "Epoch 27, Batch 726, LR 0.000024 Loss 4.881668, Accuracy 88.014%\n",
      "Epoch 27, Batch 727, LR 0.000024 Loss 4.883202, Accuracy 88.013%\n",
      "Epoch 27, Batch 728, LR 0.000024 Loss 4.881992, Accuracy 88.020%\n",
      "Epoch 27, Batch 729, LR 0.000024 Loss 4.883305, Accuracy 88.017%\n",
      "Epoch 27, Batch 730, LR 0.000024 Loss 4.883114, Accuracy 88.020%\n",
      "Epoch 27, Batch 731, LR 0.000024 Loss 4.882468, Accuracy 88.022%\n",
      "Epoch 27, Batch 732, LR 0.000024 Loss 4.881951, Accuracy 88.026%\n",
      "Epoch 27, Batch 733, LR 0.000024 Loss 4.882703, Accuracy 88.020%\n",
      "Epoch 27, Batch 734, LR 0.000024 Loss 4.882964, Accuracy 88.017%\n",
      "Epoch 27, Batch 735, LR 0.000024 Loss 4.882840, Accuracy 88.017%\n",
      "Epoch 27, Batch 736, LR 0.000024 Loss 4.882761, Accuracy 88.014%\n",
      "Epoch 27, Batch 737, LR 0.000024 Loss 4.883037, Accuracy 88.011%\n",
      "Epoch 27, Batch 738, LR 0.000024 Loss 4.884625, Accuracy 88.004%\n",
      "Epoch 27, Batch 739, LR 0.000024 Loss 4.885534, Accuracy 88.004%\n",
      "Epoch 27, Batch 740, LR 0.000024 Loss 4.886131, Accuracy 88.004%\n",
      "Epoch 27, Batch 741, LR 0.000024 Loss 4.886206, Accuracy 88.006%\n",
      "Epoch 27, Batch 742, LR 0.000024 Loss 4.885325, Accuracy 88.009%\n",
      "Epoch 27, Batch 743, LR 0.000024 Loss 4.884634, Accuracy 88.008%\n",
      "Epoch 27, Batch 744, LR 0.000024 Loss 4.883860, Accuracy 88.013%\n",
      "Epoch 27, Batch 745, LR 0.000024 Loss 4.883664, Accuracy 88.014%\n",
      "Epoch 27, Batch 746, LR 0.000024 Loss 4.884391, Accuracy 88.012%\n",
      "Epoch 27, Batch 747, LR 0.000024 Loss 4.883849, Accuracy 88.011%\n",
      "Epoch 27, Batch 748, LR 0.000024 Loss 4.883773, Accuracy 88.006%\n",
      "Epoch 27, Batch 749, LR 0.000024 Loss 4.883828, Accuracy 88.004%\n",
      "Epoch 27, Batch 750, LR 0.000024 Loss 4.882376, Accuracy 88.011%\n",
      "Epoch 27, Batch 751, LR 0.000024 Loss 4.882888, Accuracy 88.009%\n",
      "Epoch 27, Batch 752, LR 0.000024 Loss 4.883255, Accuracy 88.002%\n",
      "Epoch 27, Batch 753, LR 0.000024 Loss 4.884182, Accuracy 88.001%\n",
      "Epoch 27, Batch 754, LR 0.000024 Loss 4.883522, Accuracy 88.003%\n",
      "Epoch 27, Batch 755, LR 0.000024 Loss 4.883067, Accuracy 88.004%\n",
      "Epoch 27, Batch 756, LR 0.000024 Loss 4.883388, Accuracy 88.002%\n",
      "Epoch 27, Batch 757, LR 0.000024 Loss 4.883143, Accuracy 88.004%\n",
      "Epoch 27, Batch 758, LR 0.000024 Loss 4.883419, Accuracy 88.004%\n",
      "Epoch 27, Batch 759, LR 0.000024 Loss 4.883476, Accuracy 88.000%\n",
      "Epoch 27, Batch 760, LR 0.000024 Loss 4.883717, Accuracy 88.003%\n",
      "Epoch 27, Batch 761, LR 0.000024 Loss 4.883640, Accuracy 88.003%\n",
      "Epoch 27, Batch 762, LR 0.000024 Loss 4.883640, Accuracy 88.000%\n",
      "Epoch 27, Batch 763, LR 0.000024 Loss 4.883717, Accuracy 87.999%\n",
      "Epoch 27, Batch 764, LR 0.000024 Loss 4.883399, Accuracy 87.998%\n",
      "Epoch 27, Batch 765, LR 0.000024 Loss 4.883654, Accuracy 87.998%\n",
      "Epoch 27, Batch 766, LR 0.000024 Loss 4.882616, Accuracy 88.004%\n",
      "Epoch 27, Batch 767, LR 0.000024 Loss 4.882977, Accuracy 87.993%\n",
      "Epoch 27, Batch 768, LR 0.000024 Loss 4.883072, Accuracy 87.998%\n",
      "Epoch 27, Batch 769, LR 0.000024 Loss 4.883558, Accuracy 88.003%\n",
      "Epoch 27, Batch 770, LR 0.000024 Loss 4.883734, Accuracy 88.001%\n",
      "Epoch 27, Batch 771, LR 0.000024 Loss 4.884518, Accuracy 88.002%\n",
      "Epoch 27, Batch 772, LR 0.000024 Loss 4.884237, Accuracy 88.005%\n",
      "Epoch 27, Batch 773, LR 0.000024 Loss 4.883893, Accuracy 88.007%\n",
      "Epoch 27, Batch 774, LR 0.000024 Loss 4.883420, Accuracy 88.013%\n",
      "Epoch 27, Batch 775, LR 0.000024 Loss 4.883466, Accuracy 88.016%\n",
      "Epoch 27, Batch 776, LR 0.000024 Loss 4.884183, Accuracy 88.005%\n",
      "Epoch 27, Batch 777, LR 0.000024 Loss 4.884388, Accuracy 88.006%\n",
      "Epoch 27, Batch 778, LR 0.000024 Loss 4.884078, Accuracy 88.015%\n",
      "Epoch 27, Batch 779, LR 0.000024 Loss 4.884605, Accuracy 88.013%\n",
      "Epoch 27, Batch 780, LR 0.000024 Loss 4.884864, Accuracy 88.014%\n",
      "Epoch 27, Batch 781, LR 0.000024 Loss 4.885199, Accuracy 88.016%\n",
      "Epoch 27, Batch 782, LR 0.000024 Loss 4.884681, Accuracy 88.021%\n",
      "Epoch 27, Batch 783, LR 0.000024 Loss 4.884952, Accuracy 88.017%\n",
      "Epoch 27, Batch 784, LR 0.000024 Loss 4.884261, Accuracy 88.024%\n",
      "Epoch 27, Batch 785, LR 0.000024 Loss 4.884295, Accuracy 88.025%\n",
      "Epoch 27, Batch 786, LR 0.000024 Loss 4.884233, Accuracy 88.025%\n",
      "Epoch 27, Batch 787, LR 0.000024 Loss 4.883419, Accuracy 88.026%\n",
      "Epoch 27, Batch 788, LR 0.000024 Loss 4.883468, Accuracy 88.022%\n",
      "Epoch 27, Batch 789, LR 0.000024 Loss 4.884775, Accuracy 88.017%\n",
      "Epoch 27, Batch 790, LR 0.000024 Loss 4.884048, Accuracy 88.024%\n",
      "Epoch 27, Batch 791, LR 0.000024 Loss 4.884827, Accuracy 88.020%\n",
      "Epoch 27, Batch 792, LR 0.000024 Loss 4.884911, Accuracy 88.015%\n",
      "Epoch 27, Batch 793, LR 0.000023 Loss 4.884444, Accuracy 88.016%\n",
      "Epoch 27, Batch 794, LR 0.000023 Loss 4.885485, Accuracy 88.008%\n",
      "Epoch 27, Batch 795, LR 0.000023 Loss 4.885624, Accuracy 88.006%\n",
      "Epoch 27, Batch 796, LR 0.000023 Loss 4.885965, Accuracy 88.000%\n",
      "Epoch 27, Batch 797, LR 0.000023 Loss 4.885798, Accuracy 87.997%\n",
      "Epoch 27, Batch 798, LR 0.000023 Loss 4.885660, Accuracy 87.994%\n",
      "Epoch 27, Batch 799, LR 0.000023 Loss 4.885448, Accuracy 87.997%\n",
      "Epoch 27, Batch 800, LR 0.000023 Loss 4.884015, Accuracy 88.004%\n",
      "Epoch 27, Batch 801, LR 0.000023 Loss 4.884667, Accuracy 88.002%\n",
      "Epoch 27, Batch 802, LR 0.000023 Loss 4.885321, Accuracy 88.004%\n",
      "Epoch 27, Batch 803, LR 0.000023 Loss 4.885905, Accuracy 88.001%\n",
      "Epoch 27, Batch 804, LR 0.000023 Loss 4.885520, Accuracy 88.002%\n",
      "Epoch 27, Batch 805, LR 0.000023 Loss 4.886040, Accuracy 88.001%\n",
      "Epoch 27, Batch 806, LR 0.000023 Loss 4.886625, Accuracy 87.998%\n",
      "Epoch 27, Batch 807, LR 0.000023 Loss 4.887260, Accuracy 87.992%\n",
      "Epoch 27, Batch 808, LR 0.000023 Loss 4.887425, Accuracy 87.994%\n",
      "Epoch 27, Batch 809, LR 0.000023 Loss 4.887354, Accuracy 87.994%\n",
      "Epoch 27, Batch 810, LR 0.000023 Loss 4.886571, Accuracy 88.000%\n",
      "Epoch 27, Batch 811, LR 0.000023 Loss 4.886959, Accuracy 87.995%\n",
      "Epoch 27, Batch 812, LR 0.000023 Loss 4.886034, Accuracy 88.000%\n",
      "Epoch 27, Batch 813, LR 0.000023 Loss 4.885976, Accuracy 88.000%\n",
      "Epoch 27, Batch 814, LR 0.000023 Loss 4.886236, Accuracy 87.994%\n",
      "Epoch 27, Batch 815, LR 0.000023 Loss 4.886010, Accuracy 87.998%\n",
      "Epoch 27, Batch 816, LR 0.000023 Loss 4.885790, Accuracy 87.999%\n",
      "Epoch 27, Batch 817, LR 0.000023 Loss 4.885532, Accuracy 88.003%\n",
      "Epoch 27, Batch 818, LR 0.000023 Loss 4.886403, Accuracy 87.995%\n",
      "Epoch 27, Batch 819, LR 0.000023 Loss 4.886304, Accuracy 87.992%\n",
      "Epoch 27, Batch 820, LR 0.000023 Loss 4.887071, Accuracy 87.987%\n",
      "Epoch 27, Batch 821, LR 0.000023 Loss 4.887794, Accuracy 87.987%\n",
      "Epoch 27, Batch 822, LR 0.000023 Loss 4.887705, Accuracy 87.988%\n",
      "Epoch 27, Batch 823, LR 0.000023 Loss 4.888474, Accuracy 87.977%\n",
      "Epoch 27, Batch 824, LR 0.000023 Loss 4.887998, Accuracy 87.979%\n",
      "Epoch 27, Batch 825, LR 0.000023 Loss 4.888195, Accuracy 87.980%\n",
      "Epoch 27, Batch 826, LR 0.000023 Loss 4.887680, Accuracy 87.985%\n",
      "Epoch 27, Batch 827, LR 0.000023 Loss 4.887273, Accuracy 87.989%\n",
      "Epoch 27, Batch 828, LR 0.000023 Loss 4.887618, Accuracy 87.990%\n",
      "Epoch 27, Batch 829, LR 0.000023 Loss 4.887828, Accuracy 87.987%\n",
      "Epoch 27, Batch 830, LR 0.000023 Loss 4.887320, Accuracy 87.990%\n",
      "Epoch 27, Batch 831, LR 0.000023 Loss 4.887208, Accuracy 87.997%\n",
      "Epoch 27, Batch 832, LR 0.000023 Loss 4.887147, Accuracy 88.000%\n",
      "Epoch 27, Batch 833, LR 0.000023 Loss 4.886788, Accuracy 88.002%\n",
      "Epoch 27, Batch 834, LR 0.000023 Loss 4.887264, Accuracy 88.002%\n",
      "Epoch 27, Batch 835, LR 0.000023 Loss 4.886665, Accuracy 88.010%\n",
      "Epoch 27, Batch 836, LR 0.000023 Loss 4.886601, Accuracy 88.013%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 837, LR 0.000023 Loss 4.886101, Accuracy 88.013%\n",
      "Epoch 27, Batch 838, LR 0.000023 Loss 4.886237, Accuracy 88.014%\n",
      "Epoch 27, Batch 839, LR 0.000023 Loss 4.886513, Accuracy 88.013%\n",
      "Epoch 27, Batch 840, LR 0.000023 Loss 4.886788, Accuracy 88.011%\n",
      "Epoch 27, Batch 841, LR 0.000023 Loss 4.887823, Accuracy 88.006%\n",
      "Epoch 27, Batch 842, LR 0.000023 Loss 4.888207, Accuracy 88.003%\n",
      "Epoch 27, Batch 843, LR 0.000023 Loss 4.887627, Accuracy 88.008%\n",
      "Epoch 27, Batch 844, LR 0.000023 Loss 4.887528, Accuracy 88.008%\n",
      "Epoch 27, Batch 845, LR 0.000023 Loss 4.887366, Accuracy 88.010%\n",
      "Epoch 27, Batch 846, LR 0.000023 Loss 4.886938, Accuracy 88.011%\n",
      "Epoch 27, Batch 847, LR 0.000023 Loss 4.887242, Accuracy 88.006%\n",
      "Epoch 27, Batch 848, LR 0.000023 Loss 4.887186, Accuracy 88.007%\n",
      "Epoch 27, Batch 849, LR 0.000023 Loss 4.887508, Accuracy 88.003%\n",
      "Epoch 27, Batch 850, LR 0.000023 Loss 4.887600, Accuracy 88.001%\n",
      "Epoch 27, Batch 851, LR 0.000023 Loss 4.886796, Accuracy 88.007%\n",
      "Epoch 27, Batch 852, LR 0.000023 Loss 4.887442, Accuracy 88.005%\n",
      "Epoch 27, Batch 853, LR 0.000023 Loss 4.886559, Accuracy 88.006%\n",
      "Epoch 27, Batch 854, LR 0.000023 Loss 4.887445, Accuracy 88.000%\n",
      "Epoch 27, Batch 855, LR 0.000023 Loss 4.887261, Accuracy 88.001%\n",
      "Epoch 27, Batch 856, LR 0.000023 Loss 4.886748, Accuracy 88.003%\n",
      "Epoch 27, Batch 857, LR 0.000023 Loss 4.887558, Accuracy 87.999%\n",
      "Epoch 27, Batch 858, LR 0.000023 Loss 4.887317, Accuracy 88.002%\n",
      "Epoch 27, Batch 859, LR 0.000023 Loss 4.887111, Accuracy 88.007%\n",
      "Epoch 27, Batch 860, LR 0.000023 Loss 4.887072, Accuracy 88.005%\n",
      "Epoch 27, Batch 861, LR 0.000023 Loss 4.887012, Accuracy 88.005%\n",
      "Epoch 27, Batch 862, LR 0.000023 Loss 4.886572, Accuracy 88.006%\n",
      "Epoch 27, Batch 863, LR 0.000023 Loss 4.886975, Accuracy 88.000%\n",
      "Epoch 27, Batch 864, LR 0.000023 Loss 4.886690, Accuracy 88.004%\n",
      "Epoch 27, Batch 865, LR 0.000023 Loss 4.886538, Accuracy 88.003%\n",
      "Epoch 27, Batch 866, LR 0.000023 Loss 4.886458, Accuracy 88.003%\n",
      "Epoch 27, Batch 867, LR 0.000023 Loss 4.886659, Accuracy 87.999%\n",
      "Epoch 27, Batch 868, LR 0.000023 Loss 4.886345, Accuracy 87.999%\n",
      "Epoch 27, Batch 869, LR 0.000023 Loss 4.886825, Accuracy 87.991%\n",
      "Epoch 27, Batch 870, LR 0.000023 Loss 4.885721, Accuracy 87.997%\n",
      "Epoch 27, Batch 871, LR 0.000023 Loss 4.886171, Accuracy 87.992%\n",
      "Epoch 27, Batch 872, LR 0.000023 Loss 4.885307, Accuracy 87.995%\n",
      "Epoch 27, Batch 873, LR 0.000023 Loss 4.885581, Accuracy 87.995%\n",
      "Epoch 27, Batch 874, LR 0.000023 Loss 4.885728, Accuracy 87.996%\n",
      "Epoch 27, Batch 875, LR 0.000023 Loss 4.886163, Accuracy 87.996%\n",
      "Epoch 27, Batch 876, LR 0.000023 Loss 4.885952, Accuracy 87.999%\n",
      "Epoch 27, Batch 877, LR 0.000023 Loss 4.885731, Accuracy 87.999%\n",
      "Epoch 27, Batch 878, LR 0.000023 Loss 4.885479, Accuracy 87.999%\n",
      "Epoch 27, Batch 879, LR 0.000023 Loss 4.885702, Accuracy 88.000%\n",
      "Epoch 27, Batch 880, LR 0.000023 Loss 4.885379, Accuracy 87.997%\n",
      "Epoch 27, Batch 881, LR 0.000023 Loss 4.884991, Accuracy 87.996%\n",
      "Epoch 27, Batch 882, LR 0.000023 Loss 4.885421, Accuracy 87.992%\n",
      "Epoch 27, Batch 883, LR 0.000023 Loss 4.884628, Accuracy 87.997%\n",
      "Epoch 27, Batch 884, LR 0.000023 Loss 4.884039, Accuracy 88.002%\n",
      "Epoch 27, Batch 885, LR 0.000023 Loss 4.884663, Accuracy 88.000%\n",
      "Epoch 27, Batch 886, LR 0.000023 Loss 4.884841, Accuracy 87.999%\n",
      "Epoch 27, Batch 887, LR 0.000023 Loss 4.885031, Accuracy 87.995%\n",
      "Epoch 27, Batch 888, LR 0.000023 Loss 4.884451, Accuracy 87.998%\n",
      "Epoch 27, Batch 889, LR 0.000023 Loss 4.884247, Accuracy 88.001%\n",
      "Epoch 27, Batch 890, LR 0.000023 Loss 4.884099, Accuracy 87.998%\n",
      "Epoch 27, Batch 891, LR 0.000023 Loss 4.884009, Accuracy 88.001%\n",
      "Epoch 27, Batch 892, LR 0.000023 Loss 4.883318, Accuracy 88.005%\n",
      "Epoch 27, Batch 893, LR 0.000023 Loss 4.883494, Accuracy 88.006%\n",
      "Epoch 27, Batch 894, LR 0.000023 Loss 4.883126, Accuracy 88.010%\n",
      "Epoch 27, Batch 895, LR 0.000023 Loss 4.883622, Accuracy 88.002%\n",
      "Epoch 27, Batch 896, LR 0.000023 Loss 4.883641, Accuracy 88.000%\n",
      "Epoch 27, Batch 897, LR 0.000023 Loss 4.884216, Accuracy 87.994%\n",
      "Epoch 27, Batch 898, LR 0.000023 Loss 4.884025, Accuracy 87.995%\n",
      "Epoch 27, Batch 899, LR 0.000023 Loss 4.883946, Accuracy 87.994%\n",
      "Epoch 27, Batch 900, LR 0.000023 Loss 4.883555, Accuracy 87.997%\n",
      "Epoch 27, Batch 901, LR 0.000023 Loss 4.883347, Accuracy 87.997%\n",
      "Epoch 27, Batch 902, LR 0.000023 Loss 4.883984, Accuracy 87.992%\n",
      "Epoch 27, Batch 903, LR 0.000023 Loss 4.883573, Accuracy 87.993%\n",
      "Epoch 27, Batch 904, LR 0.000023 Loss 4.884354, Accuracy 87.989%\n",
      "Epoch 27, Batch 905, LR 0.000023 Loss 4.883942, Accuracy 87.990%\n",
      "Epoch 27, Batch 906, LR 0.000023 Loss 4.883692, Accuracy 87.990%\n",
      "Epoch 27, Batch 907, LR 0.000023 Loss 4.883171, Accuracy 87.989%\n",
      "Epoch 27, Batch 908, LR 0.000023 Loss 4.883473, Accuracy 87.986%\n",
      "Epoch 27, Batch 909, LR 0.000023 Loss 4.884154, Accuracy 87.984%\n",
      "Epoch 27, Batch 910, LR 0.000023 Loss 4.884650, Accuracy 87.975%\n",
      "Epoch 27, Batch 911, LR 0.000023 Loss 4.883946, Accuracy 87.976%\n",
      "Epoch 27, Batch 912, LR 0.000023 Loss 4.883852, Accuracy 87.978%\n",
      "Epoch 27, Batch 913, LR 0.000023 Loss 4.883688, Accuracy 87.976%\n",
      "Epoch 27, Batch 914, LR 0.000023 Loss 4.882927, Accuracy 87.980%\n",
      "Epoch 27, Batch 915, LR 0.000023 Loss 4.882557, Accuracy 87.980%\n",
      "Epoch 27, Batch 916, LR 0.000023 Loss 4.882293, Accuracy 87.984%\n",
      "Epoch 27, Batch 917, LR 0.000023 Loss 4.881870, Accuracy 87.986%\n",
      "Epoch 27, Batch 918, LR 0.000023 Loss 4.881496, Accuracy 87.984%\n",
      "Epoch 27, Batch 919, LR 0.000023 Loss 4.880820, Accuracy 87.985%\n",
      "Epoch 27, Batch 920, LR 0.000023 Loss 4.880197, Accuracy 87.988%\n",
      "Epoch 27, Batch 921, LR 0.000023 Loss 4.879956, Accuracy 87.989%\n",
      "Epoch 27, Batch 922, LR 0.000023 Loss 4.880446, Accuracy 87.988%\n",
      "Epoch 27, Batch 923, LR 0.000023 Loss 4.881639, Accuracy 87.980%\n",
      "Epoch 27, Batch 924, LR 0.000023 Loss 4.881671, Accuracy 87.979%\n",
      "Epoch 27, Batch 925, LR 0.000023 Loss 4.881240, Accuracy 87.984%\n",
      "Epoch 27, Batch 926, LR 0.000023 Loss 4.881321, Accuracy 87.983%\n",
      "Epoch 27, Batch 927, LR 0.000023 Loss 4.881017, Accuracy 87.984%\n",
      "Epoch 27, Batch 928, LR 0.000023 Loss 4.881280, Accuracy 87.985%\n",
      "Epoch 27, Batch 929, LR 0.000023 Loss 4.881611, Accuracy 87.982%\n",
      "Epoch 27, Batch 930, LR 0.000023 Loss 4.882154, Accuracy 87.980%\n",
      "Epoch 27, Batch 931, LR 0.000023 Loss 4.882346, Accuracy 87.977%\n",
      "Epoch 27, Batch 932, LR 0.000023 Loss 4.882279, Accuracy 87.980%\n",
      "Epoch 27, Batch 933, LR 0.000023 Loss 4.882585, Accuracy 87.980%\n",
      "Epoch 27, Batch 934, LR 0.000023 Loss 4.883038, Accuracy 87.977%\n",
      "Epoch 27, Batch 935, LR 0.000023 Loss 4.882834, Accuracy 87.981%\n",
      "Epoch 27, Batch 936, LR 0.000023 Loss 4.883013, Accuracy 87.982%\n",
      "Epoch 27, Batch 937, LR 0.000023 Loss 4.883265, Accuracy 87.984%\n",
      "Epoch 27, Batch 938, LR 0.000023 Loss 4.883714, Accuracy 87.983%\n",
      "Epoch 27, Batch 939, LR 0.000023 Loss 4.883056, Accuracy 87.986%\n",
      "Epoch 27, Batch 940, LR 0.000023 Loss 4.883510, Accuracy 87.982%\n",
      "Epoch 27, Batch 941, LR 0.000023 Loss 4.883784, Accuracy 87.984%\n",
      "Epoch 27, Batch 942, LR 0.000023 Loss 4.883586, Accuracy 87.985%\n",
      "Epoch 27, Batch 943, LR 0.000023 Loss 4.883721, Accuracy 87.985%\n",
      "Epoch 27, Batch 944, LR 0.000023 Loss 4.883836, Accuracy 87.982%\n",
      "Epoch 27, Batch 945, LR 0.000023 Loss 4.884171, Accuracy 87.979%\n",
      "Epoch 27, Batch 946, LR 0.000023 Loss 4.884381, Accuracy 87.977%\n",
      "Epoch 27, Batch 947, LR 0.000023 Loss 4.883822, Accuracy 87.982%\n",
      "Epoch 27, Batch 948, LR 0.000023 Loss 4.884536, Accuracy 87.981%\n",
      "Epoch 27, Batch 949, LR 0.000023 Loss 4.885067, Accuracy 87.977%\n",
      "Epoch 27, Batch 950, LR 0.000023 Loss 4.885868, Accuracy 87.971%\n",
      "Epoch 27, Batch 951, LR 0.000023 Loss 4.886418, Accuracy 87.966%\n",
      "Epoch 27, Batch 952, LR 0.000023 Loss 4.886220, Accuracy 87.960%\n",
      "Epoch 27, Batch 953, LR 0.000023 Loss 4.886398, Accuracy 87.962%\n",
      "Epoch 27, Batch 954, LR 0.000023 Loss 4.886651, Accuracy 87.964%\n",
      "Epoch 27, Batch 955, LR 0.000023 Loss 4.886593, Accuracy 87.961%\n",
      "Epoch 27, Batch 956, LR 0.000023 Loss 4.886009, Accuracy 87.963%\n",
      "Epoch 27, Batch 957, LR 0.000023 Loss 4.886210, Accuracy 87.962%\n",
      "Epoch 27, Batch 958, LR 0.000023 Loss 4.886070, Accuracy 87.962%\n",
      "Epoch 27, Batch 959, LR 0.000023 Loss 4.886179, Accuracy 87.962%\n",
      "Epoch 27, Batch 960, LR 0.000023 Loss 4.886444, Accuracy 87.965%\n",
      "Epoch 27, Batch 961, LR 0.000023 Loss 4.886450, Accuracy 87.962%\n",
      "Epoch 27, Batch 962, LR 0.000023 Loss 4.886419, Accuracy 87.968%\n",
      "Epoch 27, Batch 963, LR 0.000023 Loss 4.886817, Accuracy 87.966%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 964, LR 0.000023 Loss 4.887257, Accuracy 87.962%\n",
      "Epoch 27, Batch 965, LR 0.000023 Loss 4.887675, Accuracy 87.958%\n",
      "Epoch 27, Batch 966, LR 0.000023 Loss 4.887986, Accuracy 87.956%\n",
      "Epoch 27, Batch 967, LR 0.000023 Loss 4.887400, Accuracy 87.956%\n",
      "Epoch 27, Batch 968, LR 0.000023 Loss 4.887387, Accuracy 87.957%\n",
      "Epoch 27, Batch 969, LR 0.000023 Loss 4.887746, Accuracy 87.955%\n",
      "Epoch 27, Batch 970, LR 0.000023 Loss 4.888472, Accuracy 87.950%\n",
      "Epoch 27, Batch 971, LR 0.000023 Loss 4.887765, Accuracy 87.951%\n",
      "Epoch 27, Batch 972, LR 0.000023 Loss 4.887485, Accuracy 87.950%\n",
      "Epoch 27, Batch 973, LR 0.000023 Loss 4.887582, Accuracy 87.952%\n",
      "Epoch 27, Batch 974, LR 0.000023 Loss 4.887073, Accuracy 87.952%\n",
      "Epoch 27, Batch 975, LR 0.000023 Loss 4.886348, Accuracy 87.959%\n",
      "Epoch 27, Batch 976, LR 0.000023 Loss 4.886726, Accuracy 87.958%\n",
      "Epoch 27, Batch 977, LR 0.000023 Loss 4.887447, Accuracy 87.951%\n",
      "Epoch 27, Batch 978, LR 0.000023 Loss 4.886846, Accuracy 87.958%\n",
      "Epoch 27, Batch 979, LR 0.000023 Loss 4.886545, Accuracy 87.962%\n",
      "Epoch 27, Batch 980, LR 0.000023 Loss 4.887344, Accuracy 87.959%\n",
      "Epoch 27, Batch 981, LR 0.000023 Loss 4.887126, Accuracy 87.960%\n",
      "Epoch 27, Batch 982, LR 0.000023 Loss 4.887226, Accuracy 87.955%\n",
      "Epoch 27, Batch 983, LR 0.000023 Loss 4.888317, Accuracy 87.951%\n",
      "Epoch 27, Batch 984, LR 0.000023 Loss 4.887845, Accuracy 87.953%\n",
      "Epoch 27, Batch 985, LR 0.000023 Loss 4.887839, Accuracy 87.954%\n",
      "Epoch 27, Batch 986, LR 0.000023 Loss 4.887987, Accuracy 87.952%\n",
      "Epoch 27, Batch 987, LR 0.000023 Loss 4.887414, Accuracy 87.955%\n",
      "Epoch 27, Batch 988, LR 0.000023 Loss 4.887378, Accuracy 87.955%\n",
      "Epoch 27, Batch 989, LR 0.000023 Loss 4.886913, Accuracy 87.956%\n",
      "Epoch 27, Batch 990, LR 0.000023 Loss 4.887192, Accuracy 87.956%\n",
      "Epoch 27, Batch 991, LR 0.000023 Loss 4.888082, Accuracy 87.952%\n",
      "Epoch 27, Batch 992, LR 0.000023 Loss 4.887944, Accuracy 87.950%\n",
      "Epoch 27, Batch 993, LR 0.000023 Loss 4.888095, Accuracy 87.950%\n",
      "Epoch 27, Batch 994, LR 0.000023 Loss 4.887605, Accuracy 87.956%\n",
      "Epoch 27, Batch 995, LR 0.000023 Loss 4.887249, Accuracy 87.956%\n",
      "Epoch 27, Batch 996, LR 0.000023 Loss 4.887707, Accuracy 87.958%\n",
      "Epoch 27, Batch 997, LR 0.000023 Loss 4.887596, Accuracy 87.959%\n",
      "Epoch 27, Batch 998, LR 0.000023 Loss 4.887089, Accuracy 87.960%\n",
      "Epoch 27, Batch 999, LR 0.000023 Loss 4.886872, Accuracy 87.960%\n",
      "Epoch 27, Batch 1000, LR 0.000023 Loss 4.886878, Accuracy 87.963%\n",
      "Epoch 27, Batch 1001, LR 0.000023 Loss 4.886923, Accuracy 87.964%\n",
      "Epoch 27, Batch 1002, LR 0.000023 Loss 4.887431, Accuracy 87.957%\n",
      "Epoch 27, Batch 1003, LR 0.000023 Loss 4.887660, Accuracy 87.959%\n",
      "Epoch 27, Batch 1004, LR 0.000023 Loss 4.887589, Accuracy 87.958%\n",
      "Epoch 27, Batch 1005, LR 0.000023 Loss 4.887404, Accuracy 87.956%\n",
      "Epoch 27, Batch 1006, LR 0.000023 Loss 4.886724, Accuracy 87.960%\n",
      "Epoch 27, Batch 1007, LR 0.000023 Loss 4.886924, Accuracy 87.959%\n",
      "Epoch 27, Batch 1008, LR 0.000023 Loss 4.887113, Accuracy 87.953%\n",
      "Epoch 27, Batch 1009, LR 0.000023 Loss 4.887319, Accuracy 87.951%\n",
      "Epoch 27, Batch 1010, LR 0.000023 Loss 4.887697, Accuracy 87.953%\n",
      "Epoch 27, Batch 1011, LR 0.000023 Loss 4.888066, Accuracy 87.949%\n",
      "Epoch 27, Batch 1012, LR 0.000023 Loss 4.887558, Accuracy 87.949%\n",
      "Epoch 27, Batch 1013, LR 0.000023 Loss 4.888012, Accuracy 87.947%\n",
      "Epoch 27, Batch 1014, LR 0.000023 Loss 4.888110, Accuracy 87.944%\n",
      "Epoch 27, Batch 1015, LR 0.000023 Loss 4.887692, Accuracy 87.946%\n",
      "Epoch 27, Batch 1016, LR 0.000023 Loss 4.887710, Accuracy 87.944%\n",
      "Epoch 27, Batch 1017, LR 0.000023 Loss 4.887435, Accuracy 87.946%\n",
      "Epoch 27, Batch 1018, LR 0.000023 Loss 4.887673, Accuracy 87.946%\n",
      "Epoch 27, Batch 1019, LR 0.000023 Loss 4.888346, Accuracy 87.944%\n",
      "Epoch 27, Batch 1020, LR 0.000023 Loss 4.888757, Accuracy 87.944%\n",
      "Epoch 27, Batch 1021, LR 0.000023 Loss 4.888485, Accuracy 87.944%\n",
      "Epoch 27, Batch 1022, LR 0.000023 Loss 4.888563, Accuracy 87.945%\n",
      "Epoch 27, Batch 1023, LR 0.000023 Loss 4.888504, Accuracy 87.945%\n",
      "Epoch 27, Batch 1024, LR 0.000023 Loss 4.889064, Accuracy 87.939%\n",
      "Epoch 27, Batch 1025, LR 0.000023 Loss 4.888845, Accuracy 87.940%\n",
      "Epoch 27, Batch 1026, LR 0.000023 Loss 4.888660, Accuracy 87.939%\n",
      "Epoch 27, Batch 1027, LR 0.000023 Loss 4.888430, Accuracy 87.940%\n",
      "Epoch 27, Batch 1028, LR 0.000023 Loss 4.888180, Accuracy 87.940%\n",
      "Epoch 27, Batch 1029, LR 0.000023 Loss 4.888256, Accuracy 87.937%\n",
      "Epoch 27, Batch 1030, LR 0.000023 Loss 4.887542, Accuracy 87.938%\n",
      "Epoch 27, Batch 1031, LR 0.000023 Loss 4.887621, Accuracy 87.939%\n",
      "Epoch 27, Batch 1032, LR 0.000023 Loss 4.888239, Accuracy 87.936%\n",
      "Epoch 27, Batch 1033, LR 0.000023 Loss 4.888336, Accuracy 87.937%\n",
      "Epoch 27, Batch 1034, LR 0.000023 Loss 4.887961, Accuracy 87.937%\n",
      "Epoch 27, Batch 1035, LR 0.000023 Loss 4.888965, Accuracy 87.933%\n",
      "Epoch 27, Batch 1036, LR 0.000023 Loss 4.889456, Accuracy 87.929%\n",
      "Epoch 27, Batch 1037, LR 0.000023 Loss 4.889262, Accuracy 87.929%\n",
      "Epoch 27, Batch 1038, LR 0.000023 Loss 4.889290, Accuracy 87.931%\n",
      "Epoch 27, Batch 1039, LR 0.000023 Loss 4.889921, Accuracy 87.926%\n",
      "Epoch 27, Batch 1040, LR 0.000023 Loss 4.890593, Accuracy 87.920%\n",
      "Epoch 27, Batch 1041, LR 0.000023 Loss 4.890364, Accuracy 87.920%\n",
      "Epoch 27, Batch 1042, LR 0.000023 Loss 4.890642, Accuracy 87.920%\n",
      "Epoch 27, Batch 1043, LR 0.000023 Loss 4.890835, Accuracy 87.919%\n",
      "Epoch 27, Batch 1044, LR 0.000023 Loss 4.890484, Accuracy 87.921%\n",
      "Epoch 27, Batch 1045, LR 0.000023 Loss 4.890688, Accuracy 87.919%\n",
      "Epoch 27, Batch 1046, LR 0.000023 Loss 4.890800, Accuracy 87.919%\n",
      "Epoch 27, Batch 1047, LR 0.000023 Loss 4.890528, Accuracy 87.921%\n",
      "Epoch 27, Loss (train set) 4.890528, Accuracy (train set) 87.921%\n",
      "Epoch 28, Batch 1, LR 0.000023 Loss 4.219755, Accuracy 90.625%\n",
      "Epoch 28, Batch 2, LR 0.000023 Loss 4.401561, Accuracy 90.234%\n",
      "Epoch 28, Batch 3, LR 0.000023 Loss 4.474546, Accuracy 89.844%\n",
      "Epoch 28, Batch 4, LR 0.000023 Loss 4.399440, Accuracy 90.234%\n",
      "Epoch 28, Batch 5, LR 0.000023 Loss 4.571220, Accuracy 89.688%\n",
      "Epoch 28, Batch 6, LR 0.000023 Loss 4.541566, Accuracy 89.844%\n",
      "Epoch 28, Batch 7, LR 0.000023 Loss 4.570105, Accuracy 89.844%\n",
      "Epoch 28, Batch 8, LR 0.000023 Loss 4.638325, Accuracy 88.965%\n",
      "Epoch 28, Batch 9, LR 0.000023 Loss 4.596684, Accuracy 88.976%\n",
      "Epoch 28, Batch 10, LR 0.000023 Loss 4.690417, Accuracy 89.062%\n",
      "Epoch 28, Batch 11, LR 0.000023 Loss 4.755565, Accuracy 88.920%\n",
      "Epoch 28, Batch 12, LR 0.000023 Loss 4.794183, Accuracy 88.477%\n",
      "Epoch 28, Batch 13, LR 0.000023 Loss 4.801598, Accuracy 88.341%\n",
      "Epoch 28, Batch 14, LR 0.000023 Loss 4.781282, Accuracy 88.225%\n",
      "Epoch 28, Batch 15, LR 0.000023 Loss 4.740628, Accuracy 88.125%\n",
      "Epoch 28, Batch 16, LR 0.000023 Loss 4.779064, Accuracy 87.988%\n",
      "Epoch 28, Batch 17, LR 0.000023 Loss 4.723043, Accuracy 88.189%\n",
      "Epoch 28, Batch 18, LR 0.000023 Loss 4.766358, Accuracy 87.760%\n",
      "Epoch 28, Batch 19, LR 0.000023 Loss 4.748658, Accuracy 87.993%\n",
      "Epoch 28, Batch 20, LR 0.000023 Loss 4.756887, Accuracy 87.852%\n",
      "Epoch 28, Batch 21, LR 0.000023 Loss 4.768333, Accuracy 87.946%\n",
      "Epoch 28, Batch 22, LR 0.000023 Loss 4.774056, Accuracy 88.033%\n",
      "Epoch 28, Batch 23, LR 0.000023 Loss 4.760223, Accuracy 88.247%\n",
      "Epoch 28, Batch 24, LR 0.000023 Loss 4.792354, Accuracy 88.151%\n",
      "Epoch 28, Batch 25, LR 0.000023 Loss 4.762381, Accuracy 88.281%\n",
      "Epoch 28, Batch 26, LR 0.000023 Loss 4.787357, Accuracy 88.191%\n",
      "Epoch 28, Batch 27, LR 0.000023 Loss 4.781208, Accuracy 88.252%\n",
      "Epoch 28, Batch 28, LR 0.000023 Loss 4.818583, Accuracy 88.170%\n",
      "Epoch 28, Batch 29, LR 0.000023 Loss 4.809955, Accuracy 88.173%\n",
      "Epoch 28, Batch 30, LR 0.000023 Loss 4.809202, Accuracy 88.281%\n",
      "Epoch 28, Batch 31, LR 0.000023 Loss 4.824505, Accuracy 88.206%\n",
      "Epoch 28, Batch 32, LR 0.000023 Loss 4.802106, Accuracy 88.257%\n",
      "Epoch 28, Batch 33, LR 0.000023 Loss 4.814310, Accuracy 88.139%\n",
      "Epoch 28, Batch 34, LR 0.000023 Loss 4.811493, Accuracy 88.143%\n",
      "Epoch 28, Batch 35, LR 0.000023 Loss 4.823923, Accuracy 88.103%\n",
      "Epoch 28, Batch 36, LR 0.000023 Loss 4.848994, Accuracy 88.086%\n",
      "Epoch 28, Batch 37, LR 0.000023 Loss 4.849632, Accuracy 88.112%\n",
      "Epoch 28, Batch 38, LR 0.000023 Loss 4.836974, Accuracy 88.158%\n",
      "Epoch 28, Batch 39, LR 0.000023 Loss 4.837289, Accuracy 88.141%\n",
      "Epoch 28, Batch 40, LR 0.000023 Loss 4.828783, Accuracy 88.184%\n",
      "Epoch 28, Batch 41, LR 0.000023 Loss 4.824183, Accuracy 88.205%\n",
      "Epoch 28, Batch 42, LR 0.000023 Loss 4.842542, Accuracy 88.132%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 43, LR 0.000023 Loss 4.844801, Accuracy 88.154%\n",
      "Epoch 28, Batch 44, LR 0.000023 Loss 4.847348, Accuracy 88.086%\n",
      "Epoch 28, Batch 45, LR 0.000023 Loss 4.855406, Accuracy 87.986%\n",
      "Epoch 28, Batch 46, LR 0.000023 Loss 4.852725, Accuracy 88.010%\n",
      "Epoch 28, Batch 47, LR 0.000022 Loss 4.871420, Accuracy 87.949%\n",
      "Epoch 28, Batch 48, LR 0.000022 Loss 4.879270, Accuracy 87.923%\n",
      "Epoch 28, Batch 49, LR 0.000022 Loss 4.873367, Accuracy 87.930%\n",
      "Epoch 28, Batch 50, LR 0.000022 Loss 4.872925, Accuracy 87.922%\n",
      "Epoch 28, Batch 51, LR 0.000022 Loss 4.873844, Accuracy 87.898%\n",
      "Epoch 28, Batch 52, LR 0.000022 Loss 4.881049, Accuracy 87.876%\n",
      "Epoch 28, Batch 53, LR 0.000022 Loss 4.882546, Accuracy 87.927%\n",
      "Epoch 28, Batch 54, LR 0.000022 Loss 4.873954, Accuracy 88.006%\n",
      "Epoch 28, Batch 55, LR 0.000022 Loss 4.891597, Accuracy 87.912%\n",
      "Epoch 28, Batch 56, LR 0.000022 Loss 4.894961, Accuracy 87.919%\n",
      "Epoch 28, Batch 57, LR 0.000022 Loss 4.906893, Accuracy 87.856%\n",
      "Epoch 28, Batch 58, LR 0.000022 Loss 4.901717, Accuracy 87.945%\n",
      "Epoch 28, Batch 59, LR 0.000022 Loss 4.904658, Accuracy 87.937%\n",
      "Epoch 28, Batch 60, LR 0.000022 Loss 4.898767, Accuracy 87.969%\n",
      "Epoch 28, Batch 61, LR 0.000022 Loss 4.902776, Accuracy 87.910%\n",
      "Epoch 28, Batch 62, LR 0.000022 Loss 4.913256, Accuracy 87.903%\n",
      "Epoch 28, Batch 63, LR 0.000022 Loss 4.925666, Accuracy 87.847%\n",
      "Epoch 28, Batch 64, LR 0.000022 Loss 4.937705, Accuracy 87.817%\n",
      "Epoch 28, Batch 65, LR 0.000022 Loss 4.939797, Accuracy 87.825%\n",
      "Epoch 28, Batch 66, LR 0.000022 Loss 4.936181, Accuracy 87.855%\n",
      "Epoch 28, Batch 67, LR 0.000022 Loss 4.946734, Accuracy 87.803%\n",
      "Epoch 28, Batch 68, LR 0.000022 Loss 4.948569, Accuracy 87.764%\n",
      "Epoch 28, Batch 69, LR 0.000022 Loss 4.942923, Accuracy 87.772%\n",
      "Epoch 28, Batch 70, LR 0.000022 Loss 4.947229, Accuracy 87.712%\n",
      "Epoch 28, Batch 71, LR 0.000022 Loss 4.941562, Accuracy 87.720%\n",
      "Epoch 28, Batch 72, LR 0.000022 Loss 4.950196, Accuracy 87.663%\n",
      "Epoch 28, Batch 73, LR 0.000022 Loss 4.944674, Accuracy 87.682%\n",
      "Epoch 28, Batch 74, LR 0.000022 Loss 4.950081, Accuracy 87.658%\n",
      "Epoch 28, Batch 75, LR 0.000022 Loss 4.948915, Accuracy 87.656%\n",
      "Epoch 28, Batch 76, LR 0.000022 Loss 4.941369, Accuracy 87.675%\n",
      "Epoch 28, Batch 77, LR 0.000022 Loss 4.943848, Accuracy 87.683%\n",
      "Epoch 28, Batch 78, LR 0.000022 Loss 4.939685, Accuracy 87.700%\n",
      "Epoch 28, Batch 79, LR 0.000022 Loss 4.939455, Accuracy 87.708%\n",
      "Epoch 28, Batch 80, LR 0.000022 Loss 4.936187, Accuracy 87.715%\n",
      "Epoch 28, Batch 81, LR 0.000022 Loss 4.922965, Accuracy 87.760%\n",
      "Epoch 28, Batch 82, LR 0.000022 Loss 4.928273, Accuracy 87.671%\n",
      "Epoch 28, Batch 83, LR 0.000022 Loss 4.934772, Accuracy 87.632%\n",
      "Epoch 28, Batch 84, LR 0.000022 Loss 4.936516, Accuracy 87.612%\n",
      "Epoch 28, Batch 85, LR 0.000022 Loss 4.925829, Accuracy 87.693%\n",
      "Epoch 28, Batch 86, LR 0.000022 Loss 4.940980, Accuracy 87.618%\n",
      "Epoch 28, Batch 87, LR 0.000022 Loss 4.939020, Accuracy 87.644%\n",
      "Epoch 28, Batch 88, LR 0.000022 Loss 4.943943, Accuracy 87.598%\n",
      "Epoch 28, Batch 89, LR 0.000022 Loss 4.944542, Accuracy 87.605%\n",
      "Epoch 28, Batch 90, LR 0.000022 Loss 4.948517, Accuracy 87.569%\n",
      "Epoch 28, Batch 91, LR 0.000022 Loss 4.950518, Accuracy 87.552%\n",
      "Epoch 28, Batch 92, LR 0.000022 Loss 4.954716, Accuracy 87.551%\n",
      "Epoch 28, Batch 93, LR 0.000022 Loss 4.954069, Accuracy 87.576%\n",
      "Epoch 28, Batch 94, LR 0.000022 Loss 4.950207, Accuracy 87.575%\n",
      "Epoch 28, Batch 95, LR 0.000022 Loss 4.954757, Accuracy 87.574%\n",
      "Epoch 28, Batch 96, LR 0.000022 Loss 4.941553, Accuracy 87.622%\n",
      "Epoch 28, Batch 97, LR 0.000022 Loss 4.941229, Accuracy 87.637%\n",
      "Epoch 28, Batch 98, LR 0.000022 Loss 4.938669, Accuracy 87.651%\n",
      "Epoch 28, Batch 99, LR 0.000022 Loss 4.941193, Accuracy 87.650%\n",
      "Epoch 28, Batch 100, LR 0.000022 Loss 4.940291, Accuracy 87.664%\n",
      "Epoch 28, Batch 101, LR 0.000022 Loss 4.943624, Accuracy 87.670%\n",
      "Epoch 28, Batch 102, LR 0.000022 Loss 4.943280, Accuracy 87.684%\n",
      "Epoch 28, Batch 103, LR 0.000022 Loss 4.942957, Accuracy 87.690%\n",
      "Epoch 28, Batch 104, LR 0.000022 Loss 4.946217, Accuracy 87.665%\n",
      "Epoch 28, Batch 105, LR 0.000022 Loss 4.949332, Accuracy 87.671%\n",
      "Epoch 28, Batch 106, LR 0.000022 Loss 4.950413, Accuracy 87.677%\n",
      "Epoch 28, Batch 107, LR 0.000022 Loss 4.943023, Accuracy 87.719%\n",
      "Epoch 28, Batch 108, LR 0.000022 Loss 4.941681, Accuracy 87.731%\n",
      "Epoch 28, Batch 109, LR 0.000022 Loss 4.937101, Accuracy 87.780%\n",
      "Epoch 28, Batch 110, LR 0.000022 Loss 4.927763, Accuracy 87.827%\n",
      "Epoch 28, Batch 111, LR 0.000022 Loss 4.925994, Accuracy 87.817%\n",
      "Epoch 28, Batch 112, LR 0.000022 Loss 4.921689, Accuracy 87.849%\n",
      "Epoch 28, Batch 113, LR 0.000022 Loss 4.921894, Accuracy 87.866%\n",
      "Epoch 28, Batch 114, LR 0.000022 Loss 4.922564, Accuracy 87.884%\n",
      "Epoch 28, Batch 115, LR 0.000022 Loss 4.923699, Accuracy 87.914%\n",
      "Epoch 28, Batch 116, LR 0.000022 Loss 4.926280, Accuracy 87.870%\n",
      "Epoch 28, Batch 117, LR 0.000022 Loss 4.929854, Accuracy 87.841%\n",
      "Epoch 28, Batch 118, LR 0.000022 Loss 4.925893, Accuracy 87.871%\n",
      "Epoch 28, Batch 119, LR 0.000022 Loss 4.934043, Accuracy 87.828%\n",
      "Epoch 28, Batch 120, LR 0.000022 Loss 4.932126, Accuracy 87.819%\n",
      "Epoch 28, Batch 121, LR 0.000022 Loss 4.920550, Accuracy 87.887%\n",
      "Epoch 28, Batch 122, LR 0.000022 Loss 4.916188, Accuracy 87.891%\n",
      "Epoch 28, Batch 123, LR 0.000022 Loss 4.911355, Accuracy 87.919%\n",
      "Epoch 28, Batch 124, LR 0.000022 Loss 4.907547, Accuracy 87.941%\n",
      "Epoch 28, Batch 125, LR 0.000022 Loss 4.902424, Accuracy 87.969%\n",
      "Epoch 28, Batch 126, LR 0.000022 Loss 4.904475, Accuracy 87.984%\n",
      "Epoch 28, Batch 127, LR 0.000022 Loss 4.904490, Accuracy 87.998%\n",
      "Epoch 28, Batch 128, LR 0.000022 Loss 4.902695, Accuracy 88.019%\n",
      "Epoch 28, Batch 129, LR 0.000022 Loss 4.902284, Accuracy 88.021%\n",
      "Epoch 28, Batch 130, LR 0.000022 Loss 4.904925, Accuracy 88.041%\n",
      "Epoch 28, Batch 131, LR 0.000022 Loss 4.900317, Accuracy 88.037%\n",
      "Epoch 28, Batch 132, LR 0.000022 Loss 4.902065, Accuracy 88.027%\n",
      "Epoch 28, Batch 133, LR 0.000022 Loss 4.897907, Accuracy 88.029%\n",
      "Epoch 28, Batch 134, LR 0.000022 Loss 4.893969, Accuracy 88.031%\n",
      "Epoch 28, Batch 135, LR 0.000022 Loss 4.899402, Accuracy 88.015%\n",
      "Epoch 28, Batch 136, LR 0.000022 Loss 4.898860, Accuracy 88.046%\n",
      "Epoch 28, Batch 137, LR 0.000022 Loss 4.897224, Accuracy 88.053%\n",
      "Epoch 28, Batch 138, LR 0.000022 Loss 4.897254, Accuracy 88.060%\n",
      "Epoch 28, Batch 139, LR 0.000022 Loss 4.897068, Accuracy 88.062%\n",
      "Epoch 28, Batch 140, LR 0.000022 Loss 4.898667, Accuracy 88.069%\n",
      "Epoch 28, Batch 141, LR 0.000022 Loss 4.897931, Accuracy 88.060%\n",
      "Epoch 28, Batch 142, LR 0.000022 Loss 4.895477, Accuracy 88.072%\n",
      "Epoch 28, Batch 143, LR 0.000022 Loss 4.895027, Accuracy 88.057%\n",
      "Epoch 28, Batch 144, LR 0.000022 Loss 4.895905, Accuracy 88.043%\n",
      "Epoch 28, Batch 145, LR 0.000022 Loss 4.893875, Accuracy 88.071%\n",
      "Epoch 28, Batch 146, LR 0.000022 Loss 4.892660, Accuracy 88.073%\n",
      "Epoch 28, Batch 147, LR 0.000022 Loss 4.893862, Accuracy 88.042%\n",
      "Epoch 28, Batch 148, LR 0.000022 Loss 4.895886, Accuracy 88.044%\n",
      "Epoch 28, Batch 149, LR 0.000022 Loss 4.897819, Accuracy 88.051%\n",
      "Epoch 28, Batch 150, LR 0.000022 Loss 4.895308, Accuracy 88.057%\n",
      "Epoch 28, Batch 151, LR 0.000022 Loss 4.885341, Accuracy 88.105%\n",
      "Epoch 28, Batch 152, LR 0.000022 Loss 4.886247, Accuracy 88.076%\n",
      "Epoch 28, Batch 153, LR 0.000022 Loss 4.886830, Accuracy 88.051%\n",
      "Epoch 28, Batch 154, LR 0.000022 Loss 4.884742, Accuracy 88.053%\n",
      "Epoch 28, Batch 155, LR 0.000022 Loss 4.886277, Accuracy 88.054%\n",
      "Epoch 28, Batch 156, LR 0.000022 Loss 4.881461, Accuracy 88.071%\n",
      "Epoch 28, Batch 157, LR 0.000022 Loss 4.877812, Accuracy 88.087%\n",
      "Epoch 28, Batch 158, LR 0.000022 Loss 4.880084, Accuracy 88.088%\n",
      "Epoch 28, Batch 159, LR 0.000022 Loss 4.880284, Accuracy 88.085%\n",
      "Epoch 28, Batch 160, LR 0.000022 Loss 4.880764, Accuracy 88.081%\n",
      "Epoch 28, Batch 161, LR 0.000022 Loss 4.878133, Accuracy 88.082%\n",
      "Epoch 28, Batch 162, LR 0.000022 Loss 4.884427, Accuracy 88.040%\n",
      "Epoch 28, Batch 163, LR 0.000022 Loss 4.877865, Accuracy 88.075%\n",
      "Epoch 28, Batch 164, LR 0.000022 Loss 4.878169, Accuracy 88.057%\n",
      "Epoch 28, Batch 165, LR 0.000022 Loss 4.879716, Accuracy 88.059%\n",
      "Epoch 28, Batch 166, LR 0.000022 Loss 4.882412, Accuracy 88.051%\n",
      "Epoch 28, Batch 167, LR 0.000022 Loss 4.880261, Accuracy 88.085%\n",
      "Epoch 28, Batch 168, LR 0.000022 Loss 4.878294, Accuracy 88.100%\n",
      "Epoch 28, Batch 169, LR 0.000022 Loss 4.876144, Accuracy 88.101%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 170, LR 0.000022 Loss 4.873147, Accuracy 88.102%\n",
      "Epoch 28, Batch 171, LR 0.000022 Loss 4.869880, Accuracy 88.130%\n",
      "Epoch 28, Batch 172, LR 0.000022 Loss 4.870331, Accuracy 88.131%\n",
      "Epoch 28, Batch 173, LR 0.000022 Loss 4.873452, Accuracy 88.114%\n",
      "Epoch 28, Batch 174, LR 0.000022 Loss 4.873316, Accuracy 88.120%\n",
      "Epoch 28, Batch 175, LR 0.000022 Loss 4.871237, Accuracy 88.134%\n",
      "Epoch 28, Batch 176, LR 0.000022 Loss 4.876448, Accuracy 88.113%\n",
      "Epoch 28, Batch 177, LR 0.000022 Loss 4.877968, Accuracy 88.127%\n",
      "Epoch 28, Batch 178, LR 0.000022 Loss 4.879001, Accuracy 88.123%\n",
      "Epoch 28, Batch 179, LR 0.000022 Loss 4.876884, Accuracy 88.142%\n",
      "Epoch 28, Batch 180, LR 0.000022 Loss 4.876435, Accuracy 88.147%\n",
      "Epoch 28, Batch 181, LR 0.000022 Loss 4.876195, Accuracy 88.152%\n",
      "Epoch 28, Batch 182, LR 0.000022 Loss 4.874831, Accuracy 88.148%\n",
      "Epoch 28, Batch 183, LR 0.000022 Loss 4.875770, Accuracy 88.136%\n",
      "Epoch 28, Batch 184, LR 0.000022 Loss 4.879933, Accuracy 88.124%\n",
      "Epoch 28, Batch 185, LR 0.000022 Loss 4.879984, Accuracy 88.112%\n",
      "Epoch 28, Batch 186, LR 0.000022 Loss 4.878160, Accuracy 88.117%\n",
      "Epoch 28, Batch 187, LR 0.000022 Loss 4.879016, Accuracy 88.110%\n",
      "Epoch 28, Batch 188, LR 0.000022 Loss 4.881136, Accuracy 88.094%\n",
      "Epoch 28, Batch 189, LR 0.000022 Loss 4.880146, Accuracy 88.091%\n",
      "Epoch 28, Batch 190, LR 0.000022 Loss 4.878035, Accuracy 88.109%\n",
      "Epoch 28, Batch 191, LR 0.000022 Loss 4.877096, Accuracy 88.114%\n",
      "Epoch 28, Batch 192, LR 0.000022 Loss 4.874105, Accuracy 88.123%\n",
      "Epoch 28, Batch 193, LR 0.000022 Loss 4.876904, Accuracy 88.107%\n",
      "Epoch 28, Batch 194, LR 0.000022 Loss 4.875514, Accuracy 88.096%\n",
      "Epoch 28, Batch 195, LR 0.000022 Loss 4.874034, Accuracy 88.113%\n",
      "Epoch 28, Batch 196, LR 0.000022 Loss 4.873687, Accuracy 88.110%\n",
      "Epoch 28, Batch 197, LR 0.000022 Loss 4.871621, Accuracy 88.131%\n",
      "Epoch 28, Batch 198, LR 0.000022 Loss 4.870458, Accuracy 88.127%\n",
      "Epoch 28, Batch 199, LR 0.000022 Loss 4.868326, Accuracy 88.140%\n",
      "Epoch 28, Batch 200, LR 0.000022 Loss 4.864654, Accuracy 88.152%\n",
      "Epoch 28, Batch 201, LR 0.000022 Loss 4.864692, Accuracy 88.141%\n",
      "Epoch 28, Batch 202, LR 0.000022 Loss 4.861072, Accuracy 88.165%\n",
      "Epoch 28, Batch 203, LR 0.000022 Loss 4.865511, Accuracy 88.154%\n",
      "Epoch 28, Batch 204, LR 0.000022 Loss 4.863652, Accuracy 88.151%\n",
      "Epoch 28, Batch 205, LR 0.000022 Loss 4.864471, Accuracy 88.136%\n",
      "Epoch 28, Batch 206, LR 0.000022 Loss 4.864617, Accuracy 88.118%\n",
      "Epoch 28, Batch 207, LR 0.000022 Loss 4.863488, Accuracy 88.134%\n",
      "Epoch 28, Batch 208, LR 0.000022 Loss 4.861836, Accuracy 88.127%\n",
      "Epoch 28, Batch 209, LR 0.000022 Loss 4.860378, Accuracy 88.117%\n",
      "Epoch 28, Batch 210, LR 0.000022 Loss 4.860474, Accuracy 88.110%\n",
      "Epoch 28, Batch 211, LR 0.000022 Loss 4.858789, Accuracy 88.107%\n",
      "Epoch 28, Batch 212, LR 0.000022 Loss 4.861677, Accuracy 88.108%\n",
      "Epoch 28, Batch 213, LR 0.000022 Loss 4.863796, Accuracy 88.102%\n",
      "Epoch 28, Batch 214, LR 0.000022 Loss 4.861249, Accuracy 88.113%\n",
      "Epoch 28, Batch 215, LR 0.000022 Loss 4.859149, Accuracy 88.103%\n",
      "Epoch 28, Batch 216, LR 0.000022 Loss 4.857190, Accuracy 88.108%\n",
      "Epoch 28, Batch 217, LR 0.000022 Loss 4.861959, Accuracy 88.087%\n",
      "Epoch 28, Batch 218, LR 0.000022 Loss 4.861675, Accuracy 88.088%\n",
      "Epoch 28, Batch 219, LR 0.000022 Loss 4.862477, Accuracy 88.092%\n",
      "Epoch 28, Batch 220, LR 0.000022 Loss 4.864610, Accuracy 88.068%\n",
      "Epoch 28, Batch 221, LR 0.000022 Loss 4.869088, Accuracy 88.041%\n",
      "Epoch 28, Batch 222, LR 0.000022 Loss 4.867337, Accuracy 88.053%\n",
      "Epoch 28, Batch 223, LR 0.000022 Loss 4.862050, Accuracy 88.075%\n",
      "Epoch 28, Batch 224, LR 0.000022 Loss 4.863659, Accuracy 88.058%\n",
      "Epoch 28, Batch 225, LR 0.000022 Loss 4.862325, Accuracy 88.062%\n",
      "Epoch 28, Batch 226, LR 0.000022 Loss 4.857884, Accuracy 88.074%\n",
      "Epoch 28, Batch 227, LR 0.000022 Loss 4.857633, Accuracy 88.078%\n",
      "Epoch 28, Batch 228, LR 0.000022 Loss 4.856803, Accuracy 88.072%\n",
      "Epoch 28, Batch 229, LR 0.000022 Loss 4.857289, Accuracy 88.077%\n",
      "Epoch 28, Batch 230, LR 0.000022 Loss 4.859246, Accuracy 88.060%\n",
      "Epoch 28, Batch 231, LR 0.000022 Loss 4.859323, Accuracy 88.068%\n",
      "Epoch 28, Batch 232, LR 0.000022 Loss 4.856652, Accuracy 88.079%\n",
      "Epoch 28, Batch 233, LR 0.000022 Loss 4.853243, Accuracy 88.087%\n",
      "Epoch 28, Batch 234, LR 0.000022 Loss 4.855427, Accuracy 88.088%\n",
      "Epoch 28, Batch 235, LR 0.000022 Loss 4.854173, Accuracy 88.102%\n",
      "Epoch 28, Batch 236, LR 0.000022 Loss 4.853571, Accuracy 88.109%\n",
      "Epoch 28, Batch 237, LR 0.000022 Loss 4.852048, Accuracy 88.107%\n",
      "Epoch 28, Batch 238, LR 0.000022 Loss 4.855140, Accuracy 88.097%\n",
      "Epoch 28, Batch 239, LR 0.000022 Loss 4.855160, Accuracy 88.111%\n",
      "Epoch 28, Batch 240, LR 0.000022 Loss 4.856304, Accuracy 88.115%\n",
      "Epoch 28, Batch 241, LR 0.000022 Loss 4.852530, Accuracy 88.135%\n",
      "Epoch 28, Batch 242, LR 0.000022 Loss 4.855919, Accuracy 88.120%\n",
      "Epoch 28, Batch 243, LR 0.000022 Loss 4.852836, Accuracy 88.137%\n",
      "Epoch 28, Batch 244, LR 0.000022 Loss 4.852220, Accuracy 88.144%\n",
      "Epoch 28, Batch 245, LR 0.000022 Loss 4.850337, Accuracy 88.157%\n",
      "Epoch 28, Batch 246, LR 0.000022 Loss 4.851884, Accuracy 88.151%\n",
      "Epoch 28, Batch 247, LR 0.000022 Loss 4.850281, Accuracy 88.152%\n",
      "Epoch 28, Batch 248, LR 0.000022 Loss 4.851205, Accuracy 88.143%\n",
      "Epoch 28, Batch 249, LR 0.000022 Loss 4.854674, Accuracy 88.121%\n",
      "Epoch 28, Batch 250, LR 0.000022 Loss 4.853244, Accuracy 88.122%\n",
      "Epoch 28, Batch 251, LR 0.000022 Loss 4.854972, Accuracy 88.113%\n",
      "Epoch 28, Batch 252, LR 0.000022 Loss 4.854298, Accuracy 88.117%\n",
      "Epoch 28, Batch 253, LR 0.000022 Loss 4.851115, Accuracy 88.124%\n",
      "Epoch 28, Batch 254, LR 0.000022 Loss 4.851100, Accuracy 88.127%\n",
      "Epoch 28, Batch 255, LR 0.000022 Loss 4.851965, Accuracy 88.140%\n",
      "Epoch 28, Batch 256, LR 0.000022 Loss 4.851568, Accuracy 88.153%\n",
      "Epoch 28, Batch 257, LR 0.000022 Loss 4.852897, Accuracy 88.151%\n",
      "Epoch 28, Batch 258, LR 0.000022 Loss 4.851634, Accuracy 88.154%\n",
      "Epoch 28, Batch 259, LR 0.000022 Loss 4.851858, Accuracy 88.155%\n",
      "Epoch 28, Batch 260, LR 0.000022 Loss 4.852572, Accuracy 88.161%\n",
      "Epoch 28, Batch 261, LR 0.000022 Loss 4.852912, Accuracy 88.165%\n",
      "Epoch 28, Batch 262, LR 0.000022 Loss 4.854660, Accuracy 88.159%\n",
      "Epoch 28, Batch 263, LR 0.000022 Loss 4.854107, Accuracy 88.151%\n",
      "Epoch 28, Batch 264, LR 0.000022 Loss 4.852528, Accuracy 88.172%\n",
      "Epoch 28, Batch 265, LR 0.000022 Loss 4.852424, Accuracy 88.169%\n",
      "Epoch 28, Batch 266, LR 0.000022 Loss 4.853900, Accuracy 88.158%\n",
      "Epoch 28, Batch 267, LR 0.000022 Loss 4.854365, Accuracy 88.153%\n",
      "Epoch 28, Batch 268, LR 0.000022 Loss 4.851995, Accuracy 88.168%\n",
      "Epoch 28, Batch 269, LR 0.000022 Loss 4.849297, Accuracy 88.180%\n",
      "Epoch 28, Batch 270, LR 0.000022 Loss 4.848937, Accuracy 88.189%\n",
      "Epoch 28, Batch 271, LR 0.000022 Loss 4.846285, Accuracy 88.195%\n",
      "Epoch 28, Batch 272, LR 0.000022 Loss 4.845637, Accuracy 88.198%\n",
      "Epoch 28, Batch 273, LR 0.000022 Loss 4.844904, Accuracy 88.210%\n",
      "Epoch 28, Batch 274, LR 0.000022 Loss 4.842912, Accuracy 88.216%\n",
      "Epoch 28, Batch 275, LR 0.000022 Loss 4.841614, Accuracy 88.224%\n",
      "Epoch 28, Batch 276, LR 0.000022 Loss 4.843079, Accuracy 88.213%\n",
      "Epoch 28, Batch 277, LR 0.000022 Loss 4.841474, Accuracy 88.230%\n",
      "Epoch 28, Batch 278, LR 0.000022 Loss 4.841075, Accuracy 88.239%\n",
      "Epoch 28, Batch 279, LR 0.000022 Loss 4.840369, Accuracy 88.248%\n",
      "Epoch 28, Batch 280, LR 0.000022 Loss 4.841460, Accuracy 88.234%\n",
      "Epoch 28, Batch 281, LR 0.000022 Loss 4.841627, Accuracy 88.228%\n",
      "Epoch 28, Batch 282, LR 0.000022 Loss 4.841090, Accuracy 88.237%\n",
      "Epoch 28, Batch 283, LR 0.000022 Loss 4.841647, Accuracy 88.240%\n",
      "Epoch 28, Batch 284, LR 0.000022 Loss 4.842683, Accuracy 88.237%\n",
      "Epoch 28, Batch 285, LR 0.000022 Loss 4.842532, Accuracy 88.240%\n",
      "Epoch 28, Batch 286, LR 0.000022 Loss 4.842769, Accuracy 88.238%\n",
      "Epoch 28, Batch 287, LR 0.000022 Loss 4.844990, Accuracy 88.224%\n",
      "Epoch 28, Batch 288, LR 0.000022 Loss 4.844500, Accuracy 88.241%\n",
      "Epoch 28, Batch 289, LR 0.000022 Loss 4.843542, Accuracy 88.252%\n",
      "Epoch 28, Batch 290, LR 0.000022 Loss 4.842280, Accuracy 88.252%\n",
      "Epoch 28, Batch 291, LR 0.000022 Loss 4.844466, Accuracy 88.249%\n",
      "Epoch 28, Batch 292, LR 0.000022 Loss 4.842462, Accuracy 88.260%\n",
      "Epoch 28, Batch 293, LR 0.000022 Loss 4.845969, Accuracy 88.244%\n",
      "Epoch 28, Batch 294, LR 0.000022 Loss 4.846333, Accuracy 88.247%\n",
      "Epoch 28, Batch 295, LR 0.000022 Loss 4.846991, Accuracy 88.257%\n",
      "Epoch 28, Batch 296, LR 0.000022 Loss 4.846170, Accuracy 88.257%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 297, LR 0.000022 Loss 4.845294, Accuracy 88.268%\n",
      "Epoch 28, Batch 298, LR 0.000022 Loss 4.844248, Accuracy 88.271%\n",
      "Epoch 28, Batch 299, LR 0.000022 Loss 4.843504, Accuracy 88.276%\n",
      "Epoch 28, Batch 300, LR 0.000022 Loss 4.842271, Accuracy 88.276%\n",
      "Epoch 28, Batch 301, LR 0.000022 Loss 4.842666, Accuracy 88.281%\n",
      "Epoch 28, Batch 302, LR 0.000022 Loss 4.841529, Accuracy 88.289%\n",
      "Epoch 28, Batch 303, LR 0.000022 Loss 4.841577, Accuracy 88.289%\n",
      "Epoch 28, Batch 304, LR 0.000022 Loss 4.842502, Accuracy 88.281%\n",
      "Epoch 28, Batch 305, LR 0.000022 Loss 4.842463, Accuracy 88.274%\n",
      "Epoch 28, Batch 306, LR 0.000022 Loss 4.843798, Accuracy 88.274%\n",
      "Epoch 28, Batch 307, LR 0.000022 Loss 4.842841, Accuracy 88.276%\n",
      "Epoch 28, Batch 308, LR 0.000022 Loss 4.842150, Accuracy 88.279%\n",
      "Epoch 28, Batch 309, LR 0.000022 Loss 4.841544, Accuracy 88.279%\n",
      "Epoch 28, Batch 310, LR 0.000022 Loss 4.838420, Accuracy 88.291%\n",
      "Epoch 28, Batch 311, LR 0.000022 Loss 4.839857, Accuracy 88.286%\n",
      "Epoch 28, Batch 312, LR 0.000022 Loss 4.839123, Accuracy 88.279%\n",
      "Epoch 28, Batch 313, LR 0.000022 Loss 4.839450, Accuracy 88.271%\n",
      "Epoch 28, Batch 314, LR 0.000022 Loss 4.840622, Accuracy 88.264%\n",
      "Epoch 28, Batch 315, LR 0.000022 Loss 4.842143, Accuracy 88.254%\n",
      "Epoch 28, Batch 316, LR 0.000022 Loss 4.839382, Accuracy 88.257%\n",
      "Epoch 28, Batch 317, LR 0.000022 Loss 4.836426, Accuracy 88.271%\n",
      "Epoch 28, Batch 318, LR 0.000022 Loss 4.837941, Accuracy 88.259%\n",
      "Epoch 28, Batch 319, LR 0.000022 Loss 4.834503, Accuracy 88.276%\n",
      "Epoch 28, Batch 320, LR 0.000022 Loss 4.833809, Accuracy 88.276%\n",
      "Epoch 28, Batch 321, LR 0.000022 Loss 4.832570, Accuracy 88.279%\n",
      "Epoch 28, Batch 322, LR 0.000022 Loss 4.831879, Accuracy 88.276%\n",
      "Epoch 28, Batch 323, LR 0.000022 Loss 4.830862, Accuracy 88.281%\n",
      "Epoch 28, Batch 324, LR 0.000022 Loss 4.832357, Accuracy 88.269%\n",
      "Epoch 28, Batch 325, LR 0.000022 Loss 4.832679, Accuracy 88.260%\n",
      "Epoch 28, Batch 326, LR 0.000022 Loss 4.833851, Accuracy 88.252%\n",
      "Epoch 28, Batch 327, LR 0.000022 Loss 4.835786, Accuracy 88.238%\n",
      "Epoch 28, Batch 328, LR 0.000022 Loss 4.835506, Accuracy 88.243%\n",
      "Epoch 28, Batch 329, LR 0.000022 Loss 4.835967, Accuracy 88.234%\n",
      "Epoch 28, Batch 330, LR 0.000022 Loss 4.835373, Accuracy 88.232%\n",
      "Epoch 28, Batch 331, LR 0.000022 Loss 4.833597, Accuracy 88.236%\n",
      "Epoch 28, Batch 332, LR 0.000022 Loss 4.833418, Accuracy 88.244%\n",
      "Epoch 28, Batch 333, LR 0.000022 Loss 4.831553, Accuracy 88.253%\n",
      "Epoch 28, Batch 334, LR 0.000022 Loss 4.830477, Accuracy 88.263%\n",
      "Epoch 28, Batch 335, LR 0.000022 Loss 4.830948, Accuracy 88.256%\n",
      "Epoch 28, Batch 336, LR 0.000022 Loss 4.831381, Accuracy 88.258%\n",
      "Epoch 28, Batch 337, LR 0.000022 Loss 4.830778, Accuracy 88.256%\n",
      "Epoch 28, Batch 338, LR 0.000022 Loss 4.833847, Accuracy 88.237%\n",
      "Epoch 28, Batch 339, LR 0.000022 Loss 4.835329, Accuracy 88.242%\n",
      "Epoch 28, Batch 340, LR 0.000022 Loss 4.834200, Accuracy 88.244%\n",
      "Epoch 28, Batch 341, LR 0.000022 Loss 4.835845, Accuracy 88.229%\n",
      "Epoch 28, Batch 342, LR 0.000022 Loss 4.836030, Accuracy 88.229%\n",
      "Epoch 28, Batch 343, LR 0.000022 Loss 4.834840, Accuracy 88.236%\n",
      "Epoch 28, Batch 344, LR 0.000022 Loss 4.835031, Accuracy 88.231%\n",
      "Epoch 28, Batch 345, LR 0.000022 Loss 4.834190, Accuracy 88.243%\n",
      "Epoch 28, Batch 346, LR 0.000022 Loss 4.833425, Accuracy 88.250%\n",
      "Epoch 28, Batch 347, LR 0.000022 Loss 4.834774, Accuracy 88.241%\n",
      "Epoch 28, Batch 348, LR 0.000022 Loss 4.835662, Accuracy 88.243%\n",
      "Epoch 28, Batch 349, LR 0.000022 Loss 4.837157, Accuracy 88.239%\n",
      "Epoch 28, Batch 350, LR 0.000022 Loss 4.839403, Accuracy 88.221%\n",
      "Epoch 28, Batch 351, LR 0.000022 Loss 4.838892, Accuracy 88.212%\n",
      "Epoch 28, Batch 352, LR 0.000022 Loss 4.838484, Accuracy 88.217%\n",
      "Epoch 28, Batch 353, LR 0.000021 Loss 4.838837, Accuracy 88.204%\n",
      "Epoch 28, Batch 354, LR 0.000021 Loss 4.839448, Accuracy 88.204%\n",
      "Epoch 28, Batch 355, LR 0.000021 Loss 4.837490, Accuracy 88.220%\n",
      "Epoch 28, Batch 356, LR 0.000021 Loss 4.836461, Accuracy 88.220%\n",
      "Epoch 28, Batch 357, LR 0.000021 Loss 4.836279, Accuracy 88.227%\n",
      "Epoch 28, Batch 358, LR 0.000021 Loss 4.838005, Accuracy 88.220%\n",
      "Epoch 28, Batch 359, LR 0.000021 Loss 4.837673, Accuracy 88.214%\n",
      "Epoch 28, Batch 360, LR 0.000021 Loss 4.836909, Accuracy 88.218%\n",
      "Epoch 28, Batch 361, LR 0.000021 Loss 4.834645, Accuracy 88.223%\n",
      "Epoch 28, Batch 362, LR 0.000021 Loss 4.834044, Accuracy 88.234%\n",
      "Epoch 28, Batch 363, LR 0.000021 Loss 4.834194, Accuracy 88.225%\n",
      "Epoch 28, Batch 364, LR 0.000021 Loss 4.832900, Accuracy 88.234%\n",
      "Epoch 28, Batch 365, LR 0.000021 Loss 4.834000, Accuracy 88.232%\n",
      "Epoch 28, Batch 366, LR 0.000021 Loss 4.834123, Accuracy 88.232%\n",
      "Epoch 28, Batch 367, LR 0.000021 Loss 4.834688, Accuracy 88.230%\n",
      "Epoch 28, Batch 368, LR 0.000021 Loss 4.834147, Accuracy 88.232%\n",
      "Epoch 28, Batch 369, LR 0.000021 Loss 4.833119, Accuracy 88.237%\n",
      "Epoch 28, Batch 370, LR 0.000021 Loss 4.830749, Accuracy 88.252%\n",
      "Epoch 28, Batch 371, LR 0.000021 Loss 4.830472, Accuracy 88.254%\n",
      "Epoch 28, Batch 372, LR 0.000021 Loss 4.832013, Accuracy 88.254%\n",
      "Epoch 28, Batch 373, LR 0.000021 Loss 4.833125, Accuracy 88.246%\n",
      "Epoch 28, Batch 374, LR 0.000021 Loss 4.834069, Accuracy 88.244%\n",
      "Epoch 28, Batch 375, LR 0.000021 Loss 4.832220, Accuracy 88.252%\n",
      "Epoch 28, Batch 376, LR 0.000021 Loss 4.833150, Accuracy 88.248%\n",
      "Epoch 28, Batch 377, LR 0.000021 Loss 4.831141, Accuracy 88.248%\n",
      "Epoch 28, Batch 378, LR 0.000021 Loss 4.828930, Accuracy 88.263%\n",
      "Epoch 28, Batch 379, LR 0.000021 Loss 4.828483, Accuracy 88.269%\n",
      "Epoch 28, Batch 380, LR 0.000021 Loss 4.827346, Accuracy 88.277%\n",
      "Epoch 28, Batch 381, LR 0.000021 Loss 4.827838, Accuracy 88.265%\n",
      "Epoch 28, Batch 382, LR 0.000021 Loss 4.826570, Accuracy 88.277%\n",
      "Epoch 28, Batch 383, LR 0.000021 Loss 4.826502, Accuracy 88.287%\n",
      "Epoch 28, Batch 384, LR 0.000021 Loss 4.826230, Accuracy 88.289%\n",
      "Epoch 28, Batch 385, LR 0.000021 Loss 4.826383, Accuracy 88.293%\n",
      "Epoch 28, Batch 386, LR 0.000021 Loss 4.825095, Accuracy 88.293%\n",
      "Epoch 28, Batch 387, LR 0.000021 Loss 4.828288, Accuracy 88.283%\n",
      "Epoch 28, Batch 388, LR 0.000021 Loss 4.829670, Accuracy 88.279%\n",
      "Epoch 28, Batch 389, LR 0.000021 Loss 4.830383, Accuracy 88.275%\n",
      "Epoch 28, Batch 390, LR 0.000021 Loss 4.829945, Accuracy 88.275%\n",
      "Epoch 28, Batch 391, LR 0.000021 Loss 4.829317, Accuracy 88.269%\n",
      "Epoch 28, Batch 392, LR 0.000021 Loss 4.828744, Accuracy 88.271%\n",
      "Epoch 28, Batch 393, LR 0.000021 Loss 4.829410, Accuracy 88.267%\n",
      "Epoch 28, Batch 394, LR 0.000021 Loss 4.829257, Accuracy 88.277%\n",
      "Epoch 28, Batch 395, LR 0.000021 Loss 4.828511, Accuracy 88.283%\n",
      "Epoch 28, Batch 396, LR 0.000021 Loss 4.828186, Accuracy 88.285%\n",
      "Epoch 28, Batch 397, LR 0.000021 Loss 4.827863, Accuracy 88.277%\n",
      "Epoch 28, Batch 398, LR 0.000021 Loss 4.828101, Accuracy 88.277%\n",
      "Epoch 28, Batch 399, LR 0.000021 Loss 4.826697, Accuracy 88.287%\n",
      "Epoch 28, Batch 400, LR 0.000021 Loss 4.826334, Accuracy 88.291%\n",
      "Epoch 28, Batch 401, LR 0.000021 Loss 4.826548, Accuracy 88.287%\n",
      "Epoch 28, Batch 402, LR 0.000021 Loss 4.826751, Accuracy 88.275%\n",
      "Epoch 28, Batch 403, LR 0.000021 Loss 4.826897, Accuracy 88.270%\n",
      "Epoch 28, Batch 404, LR 0.000021 Loss 4.826068, Accuracy 88.270%\n",
      "Epoch 28, Batch 405, LR 0.000021 Loss 4.825292, Accuracy 88.277%\n",
      "Epoch 28, Batch 406, LR 0.000021 Loss 4.825887, Accuracy 88.270%\n",
      "Epoch 28, Batch 407, LR 0.000021 Loss 4.827153, Accuracy 88.272%\n",
      "Epoch 28, Batch 408, LR 0.000021 Loss 4.827089, Accuracy 88.274%\n",
      "Epoch 28, Batch 409, LR 0.000021 Loss 4.827802, Accuracy 88.272%\n",
      "Epoch 28, Batch 410, LR 0.000021 Loss 4.826918, Accuracy 88.276%\n",
      "Epoch 28, Batch 411, LR 0.000021 Loss 4.826763, Accuracy 88.285%\n",
      "Epoch 28, Batch 412, LR 0.000021 Loss 4.826970, Accuracy 88.285%\n",
      "Epoch 28, Batch 413, LR 0.000021 Loss 4.825013, Accuracy 88.287%\n",
      "Epoch 28, Batch 414, LR 0.000021 Loss 4.826494, Accuracy 88.281%\n",
      "Epoch 28, Batch 415, LR 0.000021 Loss 4.825559, Accuracy 88.285%\n",
      "Epoch 28, Batch 416, LR 0.000021 Loss 4.827692, Accuracy 88.272%\n",
      "Epoch 28, Batch 417, LR 0.000021 Loss 4.825984, Accuracy 88.279%\n",
      "Epoch 28, Batch 418, LR 0.000021 Loss 4.825742, Accuracy 88.281%\n",
      "Epoch 28, Batch 419, LR 0.000021 Loss 4.825648, Accuracy 88.274%\n",
      "Epoch 28, Batch 420, LR 0.000021 Loss 4.826535, Accuracy 88.274%\n",
      "Epoch 28, Batch 421, LR 0.000021 Loss 4.827361, Accuracy 88.274%\n",
      "Epoch 28, Batch 422, LR 0.000021 Loss 4.826293, Accuracy 88.285%\n",
      "Epoch 28, Batch 423, LR 0.000021 Loss 4.825532, Accuracy 88.283%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 424, LR 0.000021 Loss 4.826045, Accuracy 88.283%\n",
      "Epoch 28, Batch 425, LR 0.000021 Loss 4.825594, Accuracy 88.283%\n",
      "Epoch 28, Batch 426, LR 0.000021 Loss 4.824827, Accuracy 88.296%\n",
      "Epoch 28, Batch 427, LR 0.000021 Loss 4.822807, Accuracy 88.305%\n",
      "Epoch 28, Batch 428, LR 0.000021 Loss 4.821310, Accuracy 88.309%\n",
      "Epoch 28, Batch 429, LR 0.000021 Loss 4.820337, Accuracy 88.309%\n",
      "Epoch 28, Batch 430, LR 0.000021 Loss 4.820149, Accuracy 88.312%\n",
      "Epoch 28, Batch 431, LR 0.000021 Loss 4.819496, Accuracy 88.321%\n",
      "Epoch 28, Batch 432, LR 0.000021 Loss 4.821259, Accuracy 88.303%\n",
      "Epoch 28, Batch 433, LR 0.000021 Loss 4.820232, Accuracy 88.299%\n",
      "Epoch 28, Batch 434, LR 0.000021 Loss 4.820469, Accuracy 88.292%\n",
      "Epoch 28, Batch 435, LR 0.000021 Loss 4.820696, Accuracy 88.290%\n",
      "Epoch 28, Batch 436, LR 0.000021 Loss 4.820769, Accuracy 88.290%\n",
      "Epoch 28, Batch 437, LR 0.000021 Loss 4.818906, Accuracy 88.301%\n",
      "Epoch 28, Batch 438, LR 0.000021 Loss 4.818319, Accuracy 88.306%\n",
      "Epoch 28, Batch 439, LR 0.000021 Loss 4.818662, Accuracy 88.315%\n",
      "Epoch 28, Batch 440, LR 0.000021 Loss 4.819470, Accuracy 88.308%\n",
      "Epoch 28, Batch 441, LR 0.000021 Loss 4.819556, Accuracy 88.311%\n",
      "Epoch 28, Batch 442, LR 0.000021 Loss 4.818215, Accuracy 88.317%\n",
      "Epoch 28, Batch 443, LR 0.000021 Loss 4.818568, Accuracy 88.318%\n",
      "Epoch 28, Batch 444, LR 0.000021 Loss 4.820453, Accuracy 88.308%\n",
      "Epoch 28, Batch 445, LR 0.000021 Loss 4.820346, Accuracy 88.313%\n",
      "Epoch 28, Batch 446, LR 0.000021 Loss 4.820941, Accuracy 88.311%\n",
      "Epoch 28, Batch 447, LR 0.000021 Loss 4.819514, Accuracy 88.320%\n",
      "Epoch 28, Batch 448, LR 0.000021 Loss 4.819167, Accuracy 88.321%\n",
      "Epoch 28, Batch 449, LR 0.000021 Loss 4.818608, Accuracy 88.313%\n",
      "Epoch 28, Batch 450, LR 0.000021 Loss 4.817809, Accuracy 88.321%\n",
      "Epoch 28, Batch 451, LR 0.000021 Loss 4.816657, Accuracy 88.333%\n",
      "Epoch 28, Batch 452, LR 0.000021 Loss 4.816476, Accuracy 88.343%\n",
      "Epoch 28, Batch 453, LR 0.000021 Loss 4.815738, Accuracy 88.342%\n",
      "Epoch 28, Batch 454, LR 0.000021 Loss 4.814899, Accuracy 88.347%\n",
      "Epoch 28, Batch 455, LR 0.000021 Loss 4.814614, Accuracy 88.346%\n",
      "Epoch 28, Batch 456, LR 0.000021 Loss 4.815535, Accuracy 88.341%\n",
      "Epoch 28, Batch 457, LR 0.000021 Loss 4.815107, Accuracy 88.339%\n",
      "Epoch 28, Batch 458, LR 0.000021 Loss 4.815754, Accuracy 88.339%\n",
      "Epoch 28, Batch 459, LR 0.000021 Loss 4.816547, Accuracy 88.332%\n",
      "Epoch 28, Batch 460, LR 0.000021 Loss 4.816461, Accuracy 88.331%\n",
      "Epoch 28, Batch 461, LR 0.000021 Loss 4.816345, Accuracy 88.327%\n",
      "Epoch 28, Batch 462, LR 0.000021 Loss 4.815562, Accuracy 88.327%\n",
      "Epoch 28, Batch 463, LR 0.000021 Loss 4.813672, Accuracy 88.328%\n",
      "Epoch 28, Batch 464, LR 0.000021 Loss 4.812095, Accuracy 88.338%\n",
      "Epoch 28, Batch 465, LR 0.000021 Loss 4.812539, Accuracy 88.330%\n",
      "Epoch 28, Batch 466, LR 0.000021 Loss 4.812489, Accuracy 88.333%\n",
      "Epoch 28, Batch 467, LR 0.000021 Loss 4.813772, Accuracy 88.328%\n",
      "Epoch 28, Batch 468, LR 0.000021 Loss 4.812115, Accuracy 88.341%\n",
      "Epoch 28, Batch 469, LR 0.000021 Loss 4.811264, Accuracy 88.348%\n",
      "Epoch 28, Batch 470, LR 0.000021 Loss 4.810662, Accuracy 88.353%\n",
      "Epoch 28, Batch 471, LR 0.000021 Loss 4.811217, Accuracy 88.353%\n",
      "Epoch 28, Batch 472, LR 0.000021 Loss 4.810857, Accuracy 88.349%\n",
      "Epoch 28, Batch 473, LR 0.000021 Loss 4.811185, Accuracy 88.351%\n",
      "Epoch 28, Batch 474, LR 0.000021 Loss 4.810957, Accuracy 88.350%\n",
      "Epoch 28, Batch 475, LR 0.000021 Loss 4.811773, Accuracy 88.349%\n",
      "Epoch 28, Batch 476, LR 0.000021 Loss 4.810693, Accuracy 88.353%\n",
      "Epoch 28, Batch 477, LR 0.000021 Loss 4.809598, Accuracy 88.358%\n",
      "Epoch 28, Batch 478, LR 0.000021 Loss 4.808937, Accuracy 88.366%\n",
      "Epoch 28, Batch 479, LR 0.000021 Loss 4.808123, Accuracy 88.371%\n",
      "Epoch 28, Batch 480, LR 0.000021 Loss 4.807836, Accuracy 88.371%\n",
      "Epoch 28, Batch 481, LR 0.000021 Loss 4.809534, Accuracy 88.356%\n",
      "Epoch 28, Batch 482, LR 0.000021 Loss 4.808657, Accuracy 88.356%\n",
      "Epoch 28, Batch 483, LR 0.000021 Loss 4.808069, Accuracy 88.352%\n",
      "Epoch 28, Batch 484, LR 0.000021 Loss 4.809947, Accuracy 88.344%\n",
      "Epoch 28, Batch 485, LR 0.000021 Loss 4.810958, Accuracy 88.342%\n",
      "Epoch 28, Batch 486, LR 0.000021 Loss 4.810434, Accuracy 88.347%\n",
      "Epoch 28, Batch 487, LR 0.000021 Loss 4.810182, Accuracy 88.349%\n",
      "Epoch 28, Batch 488, LR 0.000021 Loss 4.809500, Accuracy 88.348%\n",
      "Epoch 28, Batch 489, LR 0.000021 Loss 4.809803, Accuracy 88.350%\n",
      "Epoch 28, Batch 490, LR 0.000021 Loss 4.808841, Accuracy 88.353%\n",
      "Epoch 28, Batch 491, LR 0.000021 Loss 4.809176, Accuracy 88.343%\n",
      "Epoch 28, Batch 492, LR 0.000021 Loss 4.808324, Accuracy 88.351%\n",
      "Epoch 28, Batch 493, LR 0.000021 Loss 4.809940, Accuracy 88.332%\n",
      "Epoch 28, Batch 494, LR 0.000021 Loss 4.809470, Accuracy 88.329%\n",
      "Epoch 28, Batch 495, LR 0.000021 Loss 4.808475, Accuracy 88.333%\n",
      "Epoch 28, Batch 496, LR 0.000021 Loss 4.809463, Accuracy 88.327%\n",
      "Epoch 28, Batch 497, LR 0.000021 Loss 4.810733, Accuracy 88.325%\n",
      "Epoch 28, Batch 498, LR 0.000021 Loss 4.810572, Accuracy 88.320%\n",
      "Epoch 28, Batch 499, LR 0.000021 Loss 4.812792, Accuracy 88.303%\n",
      "Epoch 28, Batch 500, LR 0.000021 Loss 4.811118, Accuracy 88.309%\n",
      "Epoch 28, Batch 501, LR 0.000021 Loss 4.808860, Accuracy 88.317%\n",
      "Epoch 28, Batch 502, LR 0.000021 Loss 4.809087, Accuracy 88.315%\n",
      "Epoch 28, Batch 503, LR 0.000021 Loss 4.808293, Accuracy 88.317%\n",
      "Epoch 28, Batch 504, LR 0.000021 Loss 4.809889, Accuracy 88.308%\n",
      "Epoch 28, Batch 505, LR 0.000021 Loss 4.810513, Accuracy 88.303%\n",
      "Epoch 28, Batch 506, LR 0.000021 Loss 4.810086, Accuracy 88.306%\n",
      "Epoch 28, Batch 507, LR 0.000021 Loss 4.811373, Accuracy 88.300%\n",
      "Epoch 28, Batch 508, LR 0.000021 Loss 4.811509, Accuracy 88.300%\n",
      "Epoch 28, Batch 509, LR 0.000021 Loss 4.810035, Accuracy 88.309%\n",
      "Epoch 28, Batch 510, LR 0.000021 Loss 4.810048, Accuracy 88.301%\n",
      "Epoch 28, Batch 511, LR 0.000021 Loss 4.810196, Accuracy 88.312%\n",
      "Epoch 28, Batch 512, LR 0.000021 Loss 4.811142, Accuracy 88.313%\n",
      "Epoch 28, Batch 513, LR 0.000021 Loss 4.811280, Accuracy 88.307%\n",
      "Epoch 28, Batch 514, LR 0.000021 Loss 4.810909, Accuracy 88.309%\n",
      "Epoch 28, Batch 515, LR 0.000021 Loss 4.810640, Accuracy 88.318%\n",
      "Epoch 28, Batch 516, LR 0.000021 Loss 4.811636, Accuracy 88.313%\n",
      "Epoch 28, Batch 517, LR 0.000021 Loss 4.812214, Accuracy 88.310%\n",
      "Epoch 28, Batch 518, LR 0.000021 Loss 4.813269, Accuracy 88.310%\n",
      "Epoch 28, Batch 519, LR 0.000021 Loss 4.813173, Accuracy 88.314%\n",
      "Epoch 28, Batch 520, LR 0.000021 Loss 4.814582, Accuracy 88.307%\n",
      "Epoch 28, Batch 521, LR 0.000021 Loss 4.814901, Accuracy 88.305%\n",
      "Epoch 28, Batch 522, LR 0.000021 Loss 4.814167, Accuracy 88.302%\n",
      "Epoch 28, Batch 523, LR 0.000021 Loss 4.814610, Accuracy 88.299%\n",
      "Epoch 28, Batch 524, LR 0.000021 Loss 4.813243, Accuracy 88.313%\n",
      "Epoch 28, Batch 525, LR 0.000021 Loss 4.814899, Accuracy 88.310%\n",
      "Epoch 28, Batch 526, LR 0.000021 Loss 4.814265, Accuracy 88.314%\n",
      "Epoch 28, Batch 527, LR 0.000021 Loss 4.812870, Accuracy 88.311%\n",
      "Epoch 28, Batch 528, LR 0.000021 Loss 4.812766, Accuracy 88.311%\n",
      "Epoch 28, Batch 529, LR 0.000021 Loss 4.812643, Accuracy 88.311%\n",
      "Epoch 28, Batch 530, LR 0.000021 Loss 4.813233, Accuracy 88.305%\n",
      "Epoch 28, Batch 531, LR 0.000021 Loss 4.812354, Accuracy 88.311%\n",
      "Epoch 28, Batch 532, LR 0.000021 Loss 4.812807, Accuracy 88.308%\n",
      "Epoch 28, Batch 533, LR 0.000021 Loss 4.812379, Accuracy 88.308%\n",
      "Epoch 28, Batch 534, LR 0.000021 Loss 4.812476, Accuracy 88.303%\n",
      "Epoch 28, Batch 535, LR 0.000021 Loss 4.812645, Accuracy 88.302%\n",
      "Epoch 28, Batch 536, LR 0.000021 Loss 4.813070, Accuracy 88.299%\n",
      "Epoch 28, Batch 537, LR 0.000021 Loss 4.813443, Accuracy 88.287%\n",
      "Epoch 28, Batch 538, LR 0.000021 Loss 4.813809, Accuracy 88.289%\n",
      "Epoch 28, Batch 539, LR 0.000021 Loss 4.813956, Accuracy 88.284%\n",
      "Epoch 28, Batch 540, LR 0.000021 Loss 4.814660, Accuracy 88.283%\n",
      "Epoch 28, Batch 541, LR 0.000021 Loss 4.815342, Accuracy 88.277%\n",
      "Epoch 28, Batch 542, LR 0.000021 Loss 4.815482, Accuracy 88.271%\n",
      "Epoch 28, Batch 543, LR 0.000021 Loss 4.816317, Accuracy 88.267%\n",
      "Epoch 28, Batch 544, LR 0.000021 Loss 4.816264, Accuracy 88.261%\n",
      "Epoch 28, Batch 545, LR 0.000021 Loss 4.815737, Accuracy 88.263%\n",
      "Epoch 28, Batch 546, LR 0.000021 Loss 4.815466, Accuracy 88.258%\n",
      "Epoch 28, Batch 547, LR 0.000021 Loss 4.814670, Accuracy 88.263%\n",
      "Epoch 28, Batch 548, LR 0.000021 Loss 4.813556, Accuracy 88.261%\n",
      "Epoch 28, Batch 549, LR 0.000021 Loss 4.813612, Accuracy 88.264%\n",
      "Epoch 28, Batch 550, LR 0.000021 Loss 4.813795, Accuracy 88.264%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 551, LR 0.000021 Loss 4.814501, Accuracy 88.253%\n",
      "Epoch 28, Batch 552, LR 0.000021 Loss 4.814051, Accuracy 88.249%\n",
      "Epoch 28, Batch 553, LR 0.000021 Loss 4.814886, Accuracy 88.237%\n",
      "Epoch 28, Batch 554, LR 0.000021 Loss 4.815309, Accuracy 88.233%\n",
      "Epoch 28, Batch 555, LR 0.000021 Loss 4.816186, Accuracy 88.226%\n",
      "Epoch 28, Batch 556, LR 0.000021 Loss 4.815071, Accuracy 88.233%\n",
      "Epoch 28, Batch 557, LR 0.000021 Loss 4.815230, Accuracy 88.232%\n",
      "Epoch 28, Batch 558, LR 0.000021 Loss 4.815494, Accuracy 88.229%\n",
      "Epoch 28, Batch 559, LR 0.000021 Loss 4.817272, Accuracy 88.230%\n",
      "Epoch 28, Batch 560, LR 0.000021 Loss 4.817852, Accuracy 88.228%\n",
      "Epoch 28, Batch 561, LR 0.000021 Loss 4.819383, Accuracy 88.219%\n",
      "Epoch 28, Batch 562, LR 0.000021 Loss 4.819503, Accuracy 88.219%\n",
      "Epoch 28, Batch 563, LR 0.000021 Loss 4.818396, Accuracy 88.226%\n",
      "Epoch 28, Batch 564, LR 0.000021 Loss 4.818006, Accuracy 88.230%\n",
      "Epoch 28, Batch 565, LR 0.000021 Loss 4.818222, Accuracy 88.229%\n",
      "Epoch 28, Batch 566, LR 0.000021 Loss 4.817045, Accuracy 88.237%\n",
      "Epoch 28, Batch 567, LR 0.000021 Loss 4.816013, Accuracy 88.243%\n",
      "Epoch 28, Batch 568, LR 0.000021 Loss 4.816417, Accuracy 88.237%\n",
      "Epoch 28, Batch 569, LR 0.000021 Loss 4.817761, Accuracy 88.228%\n",
      "Epoch 28, Batch 570, LR 0.000021 Loss 4.817704, Accuracy 88.229%\n",
      "Epoch 28, Batch 571, LR 0.000021 Loss 4.816663, Accuracy 88.232%\n",
      "Epoch 28, Batch 572, LR 0.000021 Loss 4.815835, Accuracy 88.240%\n",
      "Epoch 28, Batch 573, LR 0.000021 Loss 4.815712, Accuracy 88.240%\n",
      "Epoch 28, Batch 574, LR 0.000021 Loss 4.815426, Accuracy 88.246%\n",
      "Epoch 28, Batch 575, LR 0.000021 Loss 4.816162, Accuracy 88.240%\n",
      "Epoch 28, Batch 576, LR 0.000021 Loss 4.816506, Accuracy 88.235%\n",
      "Epoch 28, Batch 577, LR 0.000021 Loss 4.815896, Accuracy 88.239%\n",
      "Epoch 28, Batch 578, LR 0.000021 Loss 4.814576, Accuracy 88.245%\n",
      "Epoch 28, Batch 579, LR 0.000021 Loss 4.813715, Accuracy 88.248%\n",
      "Epoch 28, Batch 580, LR 0.000021 Loss 4.813497, Accuracy 88.245%\n",
      "Epoch 28, Batch 581, LR 0.000021 Loss 4.811832, Accuracy 88.253%\n",
      "Epoch 28, Batch 582, LR 0.000021 Loss 4.811619, Accuracy 88.256%\n",
      "Epoch 28, Batch 583, LR 0.000021 Loss 4.812139, Accuracy 88.252%\n",
      "Epoch 28, Batch 584, LR 0.000021 Loss 4.813565, Accuracy 88.245%\n",
      "Epoch 28, Batch 585, LR 0.000021 Loss 4.813068, Accuracy 88.245%\n",
      "Epoch 28, Batch 586, LR 0.000021 Loss 4.811947, Accuracy 88.243%\n",
      "Epoch 28, Batch 587, LR 0.000021 Loss 4.811458, Accuracy 88.249%\n",
      "Epoch 28, Batch 588, LR 0.000021 Loss 4.810019, Accuracy 88.257%\n",
      "Epoch 28, Batch 589, LR 0.000021 Loss 4.810365, Accuracy 88.263%\n",
      "Epoch 28, Batch 590, LR 0.000021 Loss 4.811953, Accuracy 88.256%\n",
      "Epoch 28, Batch 591, LR 0.000021 Loss 4.812416, Accuracy 88.256%\n",
      "Epoch 28, Batch 592, LR 0.000021 Loss 4.813226, Accuracy 88.252%\n",
      "Epoch 28, Batch 593, LR 0.000021 Loss 4.812565, Accuracy 88.259%\n",
      "Epoch 28, Batch 594, LR 0.000021 Loss 4.812651, Accuracy 88.260%\n",
      "Epoch 28, Batch 595, LR 0.000021 Loss 4.812688, Accuracy 88.258%\n",
      "Epoch 28, Batch 596, LR 0.000021 Loss 4.811815, Accuracy 88.267%\n",
      "Epoch 28, Batch 597, LR 0.000021 Loss 4.811302, Accuracy 88.273%\n",
      "Epoch 28, Batch 598, LR 0.000021 Loss 4.812472, Accuracy 88.268%\n",
      "Epoch 28, Batch 599, LR 0.000021 Loss 4.812347, Accuracy 88.270%\n",
      "Epoch 28, Batch 600, LR 0.000021 Loss 4.811060, Accuracy 88.273%\n",
      "Epoch 28, Batch 601, LR 0.000021 Loss 4.810263, Accuracy 88.277%\n",
      "Epoch 28, Batch 602, LR 0.000021 Loss 4.809771, Accuracy 88.277%\n",
      "Epoch 28, Batch 603, LR 0.000021 Loss 4.809328, Accuracy 88.284%\n",
      "Epoch 28, Batch 604, LR 0.000021 Loss 4.809105, Accuracy 88.286%\n",
      "Epoch 28, Batch 605, LR 0.000021 Loss 4.808515, Accuracy 88.288%\n",
      "Epoch 28, Batch 606, LR 0.000021 Loss 4.808665, Accuracy 88.286%\n",
      "Epoch 28, Batch 607, LR 0.000021 Loss 4.808898, Accuracy 88.289%\n",
      "Epoch 28, Batch 608, LR 0.000021 Loss 4.808615, Accuracy 88.289%\n",
      "Epoch 28, Batch 609, LR 0.000021 Loss 4.808467, Accuracy 88.290%\n",
      "Epoch 28, Batch 610, LR 0.000021 Loss 4.808259, Accuracy 88.289%\n",
      "Epoch 28, Batch 611, LR 0.000021 Loss 4.808567, Accuracy 88.290%\n",
      "Epoch 28, Batch 612, LR 0.000021 Loss 4.808683, Accuracy 88.290%\n",
      "Epoch 28, Batch 613, LR 0.000021 Loss 4.809175, Accuracy 88.288%\n",
      "Epoch 28, Batch 614, LR 0.000021 Loss 4.809086, Accuracy 88.288%\n",
      "Epoch 28, Batch 615, LR 0.000021 Loss 4.808341, Accuracy 88.289%\n",
      "Epoch 28, Batch 616, LR 0.000021 Loss 4.807946, Accuracy 88.291%\n",
      "Epoch 28, Batch 617, LR 0.000021 Loss 4.807966, Accuracy 88.288%\n",
      "Epoch 28, Batch 618, LR 0.000021 Loss 4.808182, Accuracy 88.288%\n",
      "Epoch 28, Batch 619, LR 0.000021 Loss 4.807923, Accuracy 88.286%\n",
      "Epoch 28, Batch 620, LR 0.000021 Loss 4.807564, Accuracy 88.293%\n",
      "Epoch 28, Batch 621, LR 0.000021 Loss 4.807688, Accuracy 88.294%\n",
      "Epoch 28, Batch 622, LR 0.000021 Loss 4.807654, Accuracy 88.296%\n",
      "Epoch 28, Batch 623, LR 0.000021 Loss 4.807169, Accuracy 88.295%\n",
      "Epoch 28, Batch 624, LR 0.000021 Loss 4.807318, Accuracy 88.294%\n",
      "Epoch 28, Batch 625, LR 0.000021 Loss 4.807894, Accuracy 88.290%\n",
      "Epoch 28, Batch 626, LR 0.000021 Loss 4.807184, Accuracy 88.291%\n",
      "Epoch 28, Batch 627, LR 0.000021 Loss 4.806166, Accuracy 88.292%\n",
      "Epoch 28, Batch 628, LR 0.000021 Loss 4.806807, Accuracy 88.291%\n",
      "Epoch 28, Batch 629, LR 0.000021 Loss 4.807597, Accuracy 88.289%\n",
      "Epoch 28, Batch 630, LR 0.000021 Loss 4.808671, Accuracy 88.282%\n",
      "Epoch 28, Batch 631, LR 0.000021 Loss 4.807877, Accuracy 88.285%\n",
      "Epoch 28, Batch 632, LR 0.000021 Loss 4.806734, Accuracy 88.294%\n",
      "Epoch 28, Batch 633, LR 0.000021 Loss 4.806582, Accuracy 88.294%\n",
      "Epoch 28, Batch 634, LR 0.000021 Loss 4.806705, Accuracy 88.292%\n",
      "Epoch 28, Batch 635, LR 0.000021 Loss 4.807284, Accuracy 88.289%\n",
      "Epoch 28, Batch 636, LR 0.000021 Loss 4.807043, Accuracy 88.291%\n",
      "Epoch 28, Batch 637, LR 0.000021 Loss 4.807692, Accuracy 88.285%\n",
      "Epoch 28, Batch 638, LR 0.000021 Loss 4.809259, Accuracy 88.279%\n",
      "Epoch 28, Batch 639, LR 0.000021 Loss 4.808567, Accuracy 88.280%\n",
      "Epoch 28, Batch 640, LR 0.000021 Loss 4.808903, Accuracy 88.274%\n",
      "Epoch 28, Batch 641, LR 0.000021 Loss 4.808069, Accuracy 88.275%\n",
      "Epoch 28, Batch 642, LR 0.000021 Loss 4.808885, Accuracy 88.268%\n",
      "Epoch 28, Batch 643, LR 0.000021 Loss 4.808037, Accuracy 88.270%\n",
      "Epoch 28, Batch 644, LR 0.000021 Loss 4.807383, Accuracy 88.275%\n",
      "Epoch 28, Batch 645, LR 0.000021 Loss 4.807512, Accuracy 88.273%\n",
      "Epoch 28, Batch 646, LR 0.000021 Loss 4.807368, Accuracy 88.273%\n",
      "Epoch 28, Batch 647, LR 0.000021 Loss 4.806529, Accuracy 88.278%\n",
      "Epoch 28, Batch 648, LR 0.000021 Loss 4.806293, Accuracy 88.281%\n",
      "Epoch 28, Batch 649, LR 0.000021 Loss 4.806839, Accuracy 88.281%\n",
      "Epoch 28, Batch 650, LR 0.000021 Loss 4.805953, Accuracy 88.290%\n",
      "Epoch 28, Batch 651, LR 0.000021 Loss 4.805051, Accuracy 88.291%\n",
      "Epoch 28, Batch 652, LR 0.000021 Loss 4.805806, Accuracy 88.294%\n",
      "Epoch 28, Batch 653, LR 0.000021 Loss 4.804923, Accuracy 88.303%\n",
      "Epoch 28, Batch 654, LR 0.000021 Loss 4.804847, Accuracy 88.304%\n",
      "Epoch 28, Batch 655, LR 0.000021 Loss 4.804125, Accuracy 88.303%\n",
      "Epoch 28, Batch 656, LR 0.000021 Loss 4.805395, Accuracy 88.297%\n",
      "Epoch 28, Batch 657, LR 0.000021 Loss 4.804693, Accuracy 88.301%\n",
      "Epoch 28, Batch 658, LR 0.000021 Loss 4.804138, Accuracy 88.299%\n",
      "Epoch 28, Batch 659, LR 0.000021 Loss 4.804796, Accuracy 88.295%\n",
      "Epoch 28, Batch 660, LR 0.000021 Loss 4.805115, Accuracy 88.297%\n",
      "Epoch 28, Batch 661, LR 0.000021 Loss 4.805540, Accuracy 88.293%\n",
      "Epoch 28, Batch 662, LR 0.000021 Loss 4.804957, Accuracy 88.295%\n",
      "Epoch 28, Batch 663, LR 0.000021 Loss 4.806284, Accuracy 88.293%\n",
      "Epoch 28, Batch 664, LR 0.000020 Loss 4.806342, Accuracy 88.298%\n",
      "Epoch 28, Batch 665, LR 0.000020 Loss 4.806141, Accuracy 88.299%\n",
      "Epoch 28, Batch 666, LR 0.000020 Loss 4.805485, Accuracy 88.302%\n",
      "Epoch 28, Batch 667, LR 0.000020 Loss 4.805571, Accuracy 88.299%\n",
      "Epoch 28, Batch 668, LR 0.000020 Loss 4.805904, Accuracy 88.299%\n",
      "Epoch 28, Batch 669, LR 0.000020 Loss 4.805073, Accuracy 88.301%\n",
      "Epoch 28, Batch 670, LR 0.000020 Loss 4.804654, Accuracy 88.306%\n",
      "Epoch 28, Batch 671, LR 0.000020 Loss 4.804833, Accuracy 88.302%\n",
      "Epoch 28, Batch 672, LR 0.000020 Loss 4.805955, Accuracy 88.296%\n",
      "Epoch 28, Batch 673, LR 0.000020 Loss 4.804932, Accuracy 88.300%\n",
      "Epoch 28, Batch 674, LR 0.000020 Loss 4.805703, Accuracy 88.302%\n",
      "Epoch 28, Batch 675, LR 0.000020 Loss 4.806051, Accuracy 88.299%\n",
      "Epoch 28, Batch 676, LR 0.000020 Loss 4.806736, Accuracy 88.301%\n",
      "Epoch 28, Batch 677, LR 0.000020 Loss 4.806627, Accuracy 88.302%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 678, LR 0.000020 Loss 4.806620, Accuracy 88.304%\n",
      "Epoch 28, Batch 679, LR 0.000020 Loss 4.807481, Accuracy 88.297%\n",
      "Epoch 28, Batch 680, LR 0.000020 Loss 4.807956, Accuracy 88.295%\n",
      "Epoch 28, Batch 681, LR 0.000020 Loss 4.808817, Accuracy 88.290%\n",
      "Epoch 28, Batch 682, LR 0.000020 Loss 4.807831, Accuracy 88.295%\n",
      "Epoch 28, Batch 683, LR 0.000020 Loss 4.807714, Accuracy 88.300%\n",
      "Epoch 28, Batch 684, LR 0.000020 Loss 4.808454, Accuracy 88.303%\n",
      "Epoch 28, Batch 685, LR 0.000020 Loss 4.807948, Accuracy 88.305%\n",
      "Epoch 28, Batch 686, LR 0.000020 Loss 4.808416, Accuracy 88.305%\n",
      "Epoch 28, Batch 687, LR 0.000020 Loss 4.810248, Accuracy 88.296%\n",
      "Epoch 28, Batch 688, LR 0.000020 Loss 4.810518, Accuracy 88.296%\n",
      "Epoch 28, Batch 689, LR 0.000020 Loss 4.810402, Accuracy 88.289%\n",
      "Epoch 28, Batch 690, LR 0.000020 Loss 4.810415, Accuracy 88.284%\n",
      "Epoch 28, Batch 691, LR 0.000020 Loss 4.810120, Accuracy 88.279%\n",
      "Epoch 28, Batch 692, LR 0.000020 Loss 4.810046, Accuracy 88.282%\n",
      "Epoch 28, Batch 693, LR 0.000020 Loss 4.809606, Accuracy 88.290%\n",
      "Epoch 28, Batch 694, LR 0.000020 Loss 4.809214, Accuracy 88.293%\n",
      "Epoch 28, Batch 695, LR 0.000020 Loss 4.810629, Accuracy 88.287%\n",
      "Epoch 28, Batch 696, LR 0.000020 Loss 4.809485, Accuracy 88.291%\n",
      "Epoch 28, Batch 697, LR 0.000020 Loss 4.810447, Accuracy 88.289%\n",
      "Epoch 28, Batch 698, LR 0.000020 Loss 4.809909, Accuracy 88.292%\n",
      "Epoch 28, Batch 699, LR 0.000020 Loss 4.809817, Accuracy 88.290%\n",
      "Epoch 28, Batch 700, LR 0.000020 Loss 4.809101, Accuracy 88.294%\n",
      "Epoch 28, Batch 701, LR 0.000020 Loss 4.809403, Accuracy 88.288%\n",
      "Epoch 28, Batch 702, LR 0.000020 Loss 4.809164, Accuracy 88.290%\n",
      "Epoch 28, Batch 703, LR 0.000020 Loss 4.808835, Accuracy 88.288%\n",
      "Epoch 28, Batch 704, LR 0.000020 Loss 4.808427, Accuracy 88.288%\n",
      "Epoch 28, Batch 705, LR 0.000020 Loss 4.807738, Accuracy 88.291%\n",
      "Epoch 28, Batch 706, LR 0.000020 Loss 4.808078, Accuracy 88.290%\n",
      "Epoch 28, Batch 707, LR 0.000020 Loss 4.808240, Accuracy 88.288%\n",
      "Epoch 28, Batch 708, LR 0.000020 Loss 4.809786, Accuracy 88.281%\n",
      "Epoch 28, Batch 709, LR 0.000020 Loss 4.809358, Accuracy 88.285%\n",
      "Epoch 28, Batch 710, LR 0.000020 Loss 4.809572, Accuracy 88.279%\n",
      "Epoch 28, Batch 711, LR 0.000020 Loss 4.808914, Accuracy 88.282%\n",
      "Epoch 28, Batch 712, LR 0.000020 Loss 4.809225, Accuracy 88.279%\n",
      "Epoch 28, Batch 713, LR 0.000020 Loss 4.809029, Accuracy 88.280%\n",
      "Epoch 28, Batch 714, LR 0.000020 Loss 4.808758, Accuracy 88.281%\n",
      "Epoch 28, Batch 715, LR 0.000020 Loss 4.809557, Accuracy 88.277%\n",
      "Epoch 28, Batch 716, LR 0.000020 Loss 4.809691, Accuracy 88.276%\n",
      "Epoch 28, Batch 717, LR 0.000020 Loss 4.809636, Accuracy 88.274%\n",
      "Epoch 28, Batch 718, LR 0.000020 Loss 4.810185, Accuracy 88.271%\n",
      "Epoch 28, Batch 719, LR 0.000020 Loss 4.809661, Accuracy 88.271%\n",
      "Epoch 28, Batch 720, LR 0.000020 Loss 4.810220, Accuracy 88.265%\n",
      "Epoch 28, Batch 721, LR 0.000020 Loss 4.811364, Accuracy 88.261%\n",
      "Epoch 28, Batch 722, LR 0.000020 Loss 4.810364, Accuracy 88.265%\n",
      "Epoch 28, Batch 723, LR 0.000020 Loss 4.811113, Accuracy 88.263%\n",
      "Epoch 28, Batch 724, LR 0.000020 Loss 4.811263, Accuracy 88.261%\n",
      "Epoch 28, Batch 725, LR 0.000020 Loss 4.811791, Accuracy 88.256%\n",
      "Epoch 28, Batch 726, LR 0.000020 Loss 4.811627, Accuracy 88.259%\n",
      "Epoch 28, Batch 727, LR 0.000020 Loss 4.811807, Accuracy 88.261%\n",
      "Epoch 28, Batch 728, LR 0.000020 Loss 4.811720, Accuracy 88.259%\n",
      "Epoch 28, Batch 729, LR 0.000020 Loss 4.811009, Accuracy 88.263%\n",
      "Epoch 28, Batch 730, LR 0.000020 Loss 4.810854, Accuracy 88.263%\n",
      "Epoch 28, Batch 731, LR 0.000020 Loss 4.810174, Accuracy 88.266%\n",
      "Epoch 28, Batch 732, LR 0.000020 Loss 4.810746, Accuracy 88.262%\n",
      "Epoch 28, Batch 733, LR 0.000020 Loss 4.811190, Accuracy 88.259%\n",
      "Epoch 28, Batch 734, LR 0.000020 Loss 4.810437, Accuracy 88.263%\n",
      "Epoch 28, Batch 735, LR 0.000020 Loss 4.810456, Accuracy 88.265%\n",
      "Epoch 28, Batch 736, LR 0.000020 Loss 4.809571, Accuracy 88.271%\n",
      "Epoch 28, Batch 737, LR 0.000020 Loss 4.809098, Accuracy 88.275%\n",
      "Epoch 28, Batch 738, LR 0.000020 Loss 4.808970, Accuracy 88.275%\n",
      "Epoch 28, Batch 739, LR 0.000020 Loss 4.809218, Accuracy 88.276%\n",
      "Epoch 28, Batch 740, LR 0.000020 Loss 4.810231, Accuracy 88.273%\n",
      "Epoch 28, Batch 741, LR 0.000020 Loss 4.810290, Accuracy 88.274%\n",
      "Epoch 28, Batch 742, LR 0.000020 Loss 4.810092, Accuracy 88.274%\n",
      "Epoch 28, Batch 743, LR 0.000020 Loss 4.811497, Accuracy 88.273%\n",
      "Epoch 28, Batch 744, LR 0.000020 Loss 4.811387, Accuracy 88.273%\n",
      "Epoch 28, Batch 745, LR 0.000020 Loss 4.811508, Accuracy 88.274%\n",
      "Epoch 28, Batch 746, LR 0.000020 Loss 4.812606, Accuracy 88.268%\n",
      "Epoch 28, Batch 747, LR 0.000020 Loss 4.813189, Accuracy 88.269%\n",
      "Epoch 28, Batch 748, LR 0.000020 Loss 4.813361, Accuracy 88.268%\n",
      "Epoch 28, Batch 749, LR 0.000020 Loss 4.813389, Accuracy 88.271%\n",
      "Epoch 28, Batch 750, LR 0.000020 Loss 4.813073, Accuracy 88.275%\n",
      "Epoch 28, Batch 751, LR 0.000020 Loss 4.812992, Accuracy 88.277%\n",
      "Epoch 28, Batch 752, LR 0.000020 Loss 4.813206, Accuracy 88.278%\n",
      "Epoch 28, Batch 753, LR 0.000020 Loss 4.813660, Accuracy 88.278%\n",
      "Epoch 28, Batch 754, LR 0.000020 Loss 4.813870, Accuracy 88.276%\n",
      "Epoch 28, Batch 755, LR 0.000020 Loss 4.813260, Accuracy 88.276%\n",
      "Epoch 28, Batch 756, LR 0.000020 Loss 4.813939, Accuracy 88.272%\n",
      "Epoch 28, Batch 757, LR 0.000020 Loss 4.814734, Accuracy 88.270%\n",
      "Epoch 28, Batch 758, LR 0.000020 Loss 4.815123, Accuracy 88.271%\n",
      "Epoch 28, Batch 759, LR 0.000020 Loss 4.815487, Accuracy 88.269%\n",
      "Epoch 28, Batch 760, LR 0.000020 Loss 4.815814, Accuracy 88.267%\n",
      "Epoch 28, Batch 761, LR 0.000020 Loss 4.816833, Accuracy 88.258%\n",
      "Epoch 28, Batch 762, LR 0.000020 Loss 4.817688, Accuracy 88.250%\n",
      "Epoch 28, Batch 763, LR 0.000020 Loss 4.817235, Accuracy 88.255%\n",
      "Epoch 28, Batch 764, LR 0.000020 Loss 4.816739, Accuracy 88.254%\n",
      "Epoch 28, Batch 765, LR 0.000020 Loss 4.816365, Accuracy 88.255%\n",
      "Epoch 28, Batch 766, LR 0.000020 Loss 4.815991, Accuracy 88.255%\n",
      "Epoch 28, Batch 767, LR 0.000020 Loss 4.816556, Accuracy 88.252%\n",
      "Epoch 28, Batch 768, LR 0.000020 Loss 4.815803, Accuracy 88.252%\n",
      "Epoch 28, Batch 769, LR 0.000020 Loss 4.815930, Accuracy 88.255%\n",
      "Epoch 28, Batch 770, LR 0.000020 Loss 4.815565, Accuracy 88.260%\n",
      "Epoch 28, Batch 771, LR 0.000020 Loss 4.815761, Accuracy 88.258%\n",
      "Epoch 28, Batch 772, LR 0.000020 Loss 4.816161, Accuracy 88.256%\n",
      "Epoch 28, Batch 773, LR 0.000020 Loss 4.817616, Accuracy 88.249%\n",
      "Epoch 28, Batch 774, LR 0.000020 Loss 4.818901, Accuracy 88.243%\n",
      "Epoch 28, Batch 775, LR 0.000020 Loss 4.818717, Accuracy 88.242%\n",
      "Epoch 28, Batch 776, LR 0.000020 Loss 4.819597, Accuracy 88.235%\n",
      "Epoch 28, Batch 777, LR 0.000020 Loss 4.820879, Accuracy 88.231%\n",
      "Epoch 28, Batch 778, LR 0.000020 Loss 4.820483, Accuracy 88.236%\n",
      "Epoch 28, Batch 779, LR 0.000020 Loss 4.821220, Accuracy 88.232%\n",
      "Epoch 28, Batch 780, LR 0.000020 Loss 4.821249, Accuracy 88.234%\n",
      "Epoch 28, Batch 781, LR 0.000020 Loss 4.821812, Accuracy 88.232%\n",
      "Epoch 28, Batch 782, LR 0.000020 Loss 4.821859, Accuracy 88.234%\n",
      "Epoch 28, Batch 783, LR 0.000020 Loss 4.822006, Accuracy 88.236%\n",
      "Epoch 28, Batch 784, LR 0.000020 Loss 4.821835, Accuracy 88.234%\n",
      "Epoch 28, Batch 785, LR 0.000020 Loss 4.821720, Accuracy 88.235%\n",
      "Epoch 28, Batch 786, LR 0.000020 Loss 4.821267, Accuracy 88.240%\n",
      "Epoch 28, Batch 787, LR 0.000020 Loss 4.821763, Accuracy 88.240%\n",
      "Epoch 28, Batch 788, LR 0.000020 Loss 4.822195, Accuracy 88.235%\n",
      "Epoch 28, Batch 789, LR 0.000020 Loss 4.822276, Accuracy 88.235%\n",
      "Epoch 28, Batch 790, LR 0.000020 Loss 4.822396, Accuracy 88.239%\n",
      "Epoch 28, Batch 791, LR 0.000020 Loss 4.821747, Accuracy 88.243%\n",
      "Epoch 28, Batch 792, LR 0.000020 Loss 4.821913, Accuracy 88.246%\n",
      "Epoch 28, Batch 793, LR 0.000020 Loss 4.824140, Accuracy 88.235%\n",
      "Epoch 28, Batch 794, LR 0.000020 Loss 4.824268, Accuracy 88.234%\n",
      "Epoch 28, Batch 795, LR 0.000020 Loss 4.824302, Accuracy 88.233%\n",
      "Epoch 28, Batch 796, LR 0.000020 Loss 4.823941, Accuracy 88.238%\n",
      "Epoch 28, Batch 797, LR 0.000020 Loss 4.824018, Accuracy 88.238%\n",
      "Epoch 28, Batch 798, LR 0.000020 Loss 4.823758, Accuracy 88.236%\n",
      "Epoch 28, Batch 799, LR 0.000020 Loss 4.823970, Accuracy 88.230%\n",
      "Epoch 28, Batch 800, LR 0.000020 Loss 4.823618, Accuracy 88.231%\n",
      "Epoch 28, Batch 801, LR 0.000020 Loss 4.823105, Accuracy 88.232%\n",
      "Epoch 28, Batch 802, LR 0.000020 Loss 4.822942, Accuracy 88.233%\n",
      "Epoch 28, Batch 803, LR 0.000020 Loss 4.821937, Accuracy 88.236%\n",
      "Epoch 28, Batch 804, LR 0.000020 Loss 4.822206, Accuracy 88.238%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 805, LR 0.000020 Loss 4.822829, Accuracy 88.231%\n",
      "Epoch 28, Batch 806, LR 0.000020 Loss 4.822989, Accuracy 88.230%\n",
      "Epoch 28, Batch 807, LR 0.000020 Loss 4.823810, Accuracy 88.229%\n",
      "Epoch 28, Batch 808, LR 0.000020 Loss 4.825236, Accuracy 88.224%\n",
      "Epoch 28, Batch 809, LR 0.000020 Loss 4.824853, Accuracy 88.226%\n",
      "Epoch 28, Batch 810, LR 0.000020 Loss 4.825383, Accuracy 88.227%\n",
      "Epoch 28, Batch 811, LR 0.000020 Loss 4.826280, Accuracy 88.226%\n",
      "Epoch 28, Batch 812, LR 0.000020 Loss 4.827182, Accuracy 88.228%\n",
      "Epoch 28, Batch 813, LR 0.000020 Loss 4.827099, Accuracy 88.231%\n",
      "Epoch 28, Batch 814, LR 0.000020 Loss 4.826244, Accuracy 88.238%\n",
      "Epoch 28, Batch 815, LR 0.000020 Loss 4.825734, Accuracy 88.238%\n",
      "Epoch 28, Batch 816, LR 0.000020 Loss 4.825976, Accuracy 88.235%\n",
      "Epoch 28, Batch 817, LR 0.000020 Loss 4.826597, Accuracy 88.232%\n",
      "Epoch 28, Batch 818, LR 0.000020 Loss 4.827230, Accuracy 88.228%\n",
      "Epoch 28, Batch 819, LR 0.000020 Loss 4.826853, Accuracy 88.231%\n",
      "Epoch 28, Batch 820, LR 0.000020 Loss 4.827637, Accuracy 88.229%\n",
      "Epoch 28, Batch 821, LR 0.000020 Loss 4.828423, Accuracy 88.228%\n",
      "Epoch 28, Batch 822, LR 0.000020 Loss 4.828588, Accuracy 88.222%\n",
      "Epoch 28, Batch 823, LR 0.000020 Loss 4.828581, Accuracy 88.225%\n",
      "Epoch 28, Batch 824, LR 0.000020 Loss 4.828698, Accuracy 88.223%\n",
      "Epoch 28, Batch 825, LR 0.000020 Loss 4.828373, Accuracy 88.223%\n",
      "Epoch 28, Batch 826, LR 0.000020 Loss 4.827892, Accuracy 88.225%\n",
      "Epoch 28, Batch 827, LR 0.000020 Loss 4.828076, Accuracy 88.226%\n",
      "Epoch 28, Batch 828, LR 0.000020 Loss 4.827539, Accuracy 88.225%\n",
      "Epoch 28, Batch 829, LR 0.000020 Loss 4.827658, Accuracy 88.225%\n",
      "Epoch 28, Batch 830, LR 0.000020 Loss 4.828128, Accuracy 88.220%\n",
      "Epoch 28, Batch 831, LR 0.000020 Loss 4.828993, Accuracy 88.215%\n",
      "Epoch 28, Batch 832, LR 0.000020 Loss 4.829463, Accuracy 88.215%\n",
      "Epoch 28, Batch 833, LR 0.000020 Loss 4.829846, Accuracy 88.215%\n",
      "Epoch 28, Batch 834, LR 0.000020 Loss 4.829981, Accuracy 88.211%\n",
      "Epoch 28, Batch 835, LR 0.000020 Loss 4.829552, Accuracy 88.214%\n",
      "Epoch 28, Batch 836, LR 0.000020 Loss 4.829044, Accuracy 88.217%\n",
      "Epoch 28, Batch 837, LR 0.000020 Loss 4.828175, Accuracy 88.224%\n",
      "Epoch 28, Batch 838, LR 0.000020 Loss 4.828492, Accuracy 88.224%\n",
      "Epoch 28, Batch 839, LR 0.000020 Loss 4.828519, Accuracy 88.221%\n",
      "Epoch 28, Batch 840, LR 0.000020 Loss 4.829276, Accuracy 88.220%\n",
      "Epoch 28, Batch 841, LR 0.000020 Loss 4.829549, Accuracy 88.218%\n",
      "Epoch 28, Batch 842, LR 0.000020 Loss 4.829879, Accuracy 88.217%\n",
      "Epoch 28, Batch 843, LR 0.000020 Loss 4.830354, Accuracy 88.213%\n",
      "Epoch 28, Batch 844, LR 0.000020 Loss 4.830116, Accuracy 88.214%\n",
      "Epoch 28, Batch 845, LR 0.000020 Loss 4.830116, Accuracy 88.214%\n",
      "Epoch 28, Batch 846, LR 0.000020 Loss 4.829683, Accuracy 88.217%\n",
      "Epoch 28, Batch 847, LR 0.000020 Loss 4.829423, Accuracy 88.219%\n",
      "Epoch 28, Batch 848, LR 0.000020 Loss 4.829974, Accuracy 88.218%\n",
      "Epoch 28, Batch 849, LR 0.000020 Loss 4.829406, Accuracy 88.221%\n",
      "Epoch 28, Batch 850, LR 0.000020 Loss 4.828821, Accuracy 88.226%\n",
      "Epoch 28, Batch 851, LR 0.000020 Loss 4.828824, Accuracy 88.223%\n",
      "Epoch 28, Batch 852, LR 0.000020 Loss 4.828327, Accuracy 88.227%\n",
      "Epoch 28, Batch 853, LR 0.000020 Loss 4.828371, Accuracy 88.224%\n",
      "Epoch 28, Batch 854, LR 0.000020 Loss 4.828207, Accuracy 88.227%\n",
      "Epoch 28, Batch 855, LR 0.000020 Loss 4.827669, Accuracy 88.233%\n",
      "Epoch 28, Batch 856, LR 0.000020 Loss 4.827680, Accuracy 88.232%\n",
      "Epoch 28, Batch 857, LR 0.000020 Loss 4.828160, Accuracy 88.228%\n",
      "Epoch 28, Batch 858, LR 0.000020 Loss 4.827131, Accuracy 88.233%\n",
      "Epoch 28, Batch 859, LR 0.000020 Loss 4.826201, Accuracy 88.237%\n",
      "Epoch 28, Batch 860, LR 0.000020 Loss 4.825813, Accuracy 88.238%\n",
      "Epoch 28, Batch 861, LR 0.000020 Loss 4.824563, Accuracy 88.245%\n",
      "Epoch 28, Batch 862, LR 0.000020 Loss 4.823325, Accuracy 88.250%\n",
      "Epoch 28, Batch 863, LR 0.000020 Loss 4.823119, Accuracy 88.252%\n",
      "Epoch 28, Batch 864, LR 0.000020 Loss 4.822914, Accuracy 88.255%\n",
      "Epoch 28, Batch 865, LR 0.000020 Loss 4.822558, Accuracy 88.258%\n",
      "Epoch 28, Batch 866, LR 0.000020 Loss 4.822101, Accuracy 88.262%\n",
      "Epoch 28, Batch 867, LR 0.000020 Loss 4.822418, Accuracy 88.264%\n",
      "Epoch 28, Batch 868, LR 0.000020 Loss 4.822237, Accuracy 88.265%\n",
      "Epoch 28, Batch 869, LR 0.000020 Loss 4.821893, Accuracy 88.268%\n",
      "Epoch 28, Batch 870, LR 0.000020 Loss 4.821569, Accuracy 88.269%\n",
      "Epoch 28, Batch 871, LR 0.000020 Loss 4.821635, Accuracy 88.271%\n",
      "Epoch 28, Batch 872, LR 0.000020 Loss 4.821283, Accuracy 88.272%\n",
      "Epoch 28, Batch 873, LR 0.000020 Loss 4.821684, Accuracy 88.272%\n",
      "Epoch 28, Batch 874, LR 0.000020 Loss 4.821271, Accuracy 88.273%\n",
      "Epoch 28, Batch 875, LR 0.000020 Loss 4.822225, Accuracy 88.269%\n",
      "Epoch 28, Batch 876, LR 0.000020 Loss 4.822182, Accuracy 88.271%\n",
      "Epoch 28, Batch 877, LR 0.000020 Loss 4.821976, Accuracy 88.271%\n",
      "Epoch 28, Batch 878, LR 0.000020 Loss 4.822943, Accuracy 88.266%\n",
      "Epoch 28, Batch 879, LR 0.000020 Loss 4.822852, Accuracy 88.271%\n",
      "Epoch 28, Batch 880, LR 0.000020 Loss 4.821540, Accuracy 88.279%\n",
      "Epoch 28, Batch 881, LR 0.000020 Loss 4.821646, Accuracy 88.278%\n",
      "Epoch 28, Batch 882, LR 0.000020 Loss 4.821611, Accuracy 88.279%\n",
      "Epoch 28, Batch 883, LR 0.000020 Loss 4.821282, Accuracy 88.281%\n",
      "Epoch 28, Batch 884, LR 0.000020 Loss 4.822156, Accuracy 88.279%\n",
      "Epoch 28, Batch 885, LR 0.000020 Loss 4.822292, Accuracy 88.279%\n",
      "Epoch 28, Batch 886, LR 0.000020 Loss 4.822494, Accuracy 88.279%\n",
      "Epoch 28, Batch 887, LR 0.000020 Loss 4.821766, Accuracy 88.286%\n",
      "Epoch 28, Batch 888, LR 0.000020 Loss 4.821790, Accuracy 88.287%\n",
      "Epoch 28, Batch 889, LR 0.000020 Loss 4.822265, Accuracy 88.287%\n",
      "Epoch 28, Batch 890, LR 0.000020 Loss 4.823312, Accuracy 88.287%\n",
      "Epoch 28, Batch 891, LR 0.000020 Loss 4.823641, Accuracy 88.287%\n",
      "Epoch 28, Batch 892, LR 0.000020 Loss 4.823216, Accuracy 88.290%\n",
      "Epoch 28, Batch 893, LR 0.000020 Loss 4.823048, Accuracy 88.287%\n",
      "Epoch 28, Batch 894, LR 0.000020 Loss 4.823420, Accuracy 88.286%\n",
      "Epoch 28, Batch 895, LR 0.000020 Loss 4.823542, Accuracy 88.283%\n",
      "Epoch 28, Batch 896, LR 0.000020 Loss 4.823257, Accuracy 88.280%\n",
      "Epoch 28, Batch 897, LR 0.000020 Loss 4.822981, Accuracy 88.280%\n",
      "Epoch 28, Batch 898, LR 0.000020 Loss 4.823008, Accuracy 88.280%\n",
      "Epoch 28, Batch 899, LR 0.000020 Loss 4.823084, Accuracy 88.283%\n",
      "Epoch 28, Batch 900, LR 0.000020 Loss 4.824152, Accuracy 88.280%\n",
      "Epoch 28, Batch 901, LR 0.000020 Loss 4.824120, Accuracy 88.279%\n",
      "Epoch 28, Batch 902, LR 0.000020 Loss 4.824587, Accuracy 88.275%\n",
      "Epoch 28, Batch 903, LR 0.000020 Loss 4.824259, Accuracy 88.280%\n",
      "Epoch 28, Batch 904, LR 0.000020 Loss 4.824131, Accuracy 88.282%\n",
      "Epoch 28, Batch 905, LR 0.000020 Loss 4.823632, Accuracy 88.284%\n",
      "Epoch 28, Batch 906, LR 0.000020 Loss 4.823876, Accuracy 88.281%\n",
      "Epoch 28, Batch 907, LR 0.000020 Loss 4.823345, Accuracy 88.281%\n",
      "Epoch 28, Batch 908, LR 0.000020 Loss 4.822535, Accuracy 88.285%\n",
      "Epoch 28, Batch 909, LR 0.000020 Loss 4.822033, Accuracy 88.287%\n",
      "Epoch 28, Batch 910, LR 0.000020 Loss 4.821765, Accuracy 88.286%\n",
      "Epoch 28, Batch 911, LR 0.000020 Loss 4.822273, Accuracy 88.283%\n",
      "Epoch 28, Batch 912, LR 0.000020 Loss 4.822400, Accuracy 88.286%\n",
      "Epoch 28, Batch 913, LR 0.000020 Loss 4.822632, Accuracy 88.286%\n",
      "Epoch 28, Batch 914, LR 0.000020 Loss 4.822031, Accuracy 88.287%\n",
      "Epoch 28, Batch 915, LR 0.000020 Loss 4.821717, Accuracy 88.289%\n",
      "Epoch 28, Batch 916, LR 0.000020 Loss 4.822016, Accuracy 88.287%\n",
      "Epoch 28, Batch 917, LR 0.000020 Loss 4.821961, Accuracy 88.290%\n",
      "Epoch 28, Batch 918, LR 0.000020 Loss 4.822294, Accuracy 88.291%\n",
      "Epoch 28, Batch 919, LR 0.000020 Loss 4.822882, Accuracy 88.289%\n",
      "Epoch 28, Batch 920, LR 0.000020 Loss 4.823207, Accuracy 88.287%\n",
      "Epoch 28, Batch 921, LR 0.000020 Loss 4.822245, Accuracy 88.291%\n",
      "Epoch 28, Batch 922, LR 0.000020 Loss 4.822166, Accuracy 88.288%\n",
      "Epoch 28, Batch 923, LR 0.000020 Loss 4.823163, Accuracy 88.281%\n",
      "Epoch 28, Batch 924, LR 0.000020 Loss 4.822680, Accuracy 88.288%\n",
      "Epoch 28, Batch 925, LR 0.000020 Loss 4.822508, Accuracy 88.286%\n",
      "Epoch 28, Batch 926, LR 0.000020 Loss 4.823235, Accuracy 88.288%\n",
      "Epoch 28, Batch 927, LR 0.000020 Loss 4.822650, Accuracy 88.290%\n",
      "Epoch 28, Batch 928, LR 0.000020 Loss 4.822653, Accuracy 88.288%\n",
      "Epoch 28, Batch 929, LR 0.000020 Loss 4.821895, Accuracy 88.293%\n",
      "Epoch 28, Batch 930, LR 0.000020 Loss 4.822263, Accuracy 88.293%\n",
      "Epoch 28, Batch 931, LR 0.000020 Loss 4.822593, Accuracy 88.290%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 932, LR 0.000020 Loss 4.822462, Accuracy 88.290%\n",
      "Epoch 28, Batch 933, LR 0.000020 Loss 4.821996, Accuracy 88.292%\n",
      "Epoch 28, Batch 934, LR 0.000020 Loss 4.822584, Accuracy 88.286%\n",
      "Epoch 28, Batch 935, LR 0.000020 Loss 4.822556, Accuracy 88.285%\n",
      "Epoch 28, Batch 936, LR 0.000020 Loss 4.821912, Accuracy 88.291%\n",
      "Epoch 28, Batch 937, LR 0.000020 Loss 4.821918, Accuracy 88.290%\n",
      "Epoch 28, Batch 938, LR 0.000020 Loss 4.821683, Accuracy 88.291%\n",
      "Epoch 28, Batch 939, LR 0.000020 Loss 4.822213, Accuracy 88.289%\n",
      "Epoch 28, Batch 940, LR 0.000020 Loss 4.823454, Accuracy 88.279%\n",
      "Epoch 28, Batch 941, LR 0.000020 Loss 4.822995, Accuracy 88.282%\n",
      "Epoch 28, Batch 942, LR 0.000020 Loss 4.822477, Accuracy 88.285%\n",
      "Epoch 28, Batch 943, LR 0.000020 Loss 4.822317, Accuracy 88.288%\n",
      "Epoch 28, Batch 944, LR 0.000020 Loss 4.822644, Accuracy 88.285%\n",
      "Epoch 28, Batch 945, LR 0.000020 Loss 4.822460, Accuracy 88.287%\n",
      "Epoch 28, Batch 946, LR 0.000020 Loss 4.822678, Accuracy 88.282%\n",
      "Epoch 28, Batch 947, LR 0.000020 Loss 4.823188, Accuracy 88.278%\n",
      "Epoch 28, Batch 948, LR 0.000020 Loss 4.822573, Accuracy 88.280%\n",
      "Epoch 28, Batch 949, LR 0.000020 Loss 4.822910, Accuracy 88.281%\n",
      "Epoch 28, Batch 950, LR 0.000020 Loss 4.823374, Accuracy 88.278%\n",
      "Epoch 28, Batch 951, LR 0.000020 Loss 4.823077, Accuracy 88.279%\n",
      "Epoch 28, Batch 952, LR 0.000020 Loss 4.823501, Accuracy 88.276%\n",
      "Epoch 28, Batch 953, LR 0.000020 Loss 4.823597, Accuracy 88.277%\n",
      "Epoch 28, Batch 954, LR 0.000020 Loss 4.824096, Accuracy 88.279%\n",
      "Epoch 28, Batch 955, LR 0.000020 Loss 4.823175, Accuracy 88.285%\n",
      "Epoch 28, Batch 956, LR 0.000020 Loss 4.822739, Accuracy 88.288%\n",
      "Epoch 28, Batch 957, LR 0.000020 Loss 4.823791, Accuracy 88.282%\n",
      "Epoch 28, Batch 958, LR 0.000020 Loss 4.823458, Accuracy 88.282%\n",
      "Epoch 28, Batch 959, LR 0.000020 Loss 4.823462, Accuracy 88.285%\n",
      "Epoch 28, Batch 960, LR 0.000020 Loss 4.823171, Accuracy 88.287%\n",
      "Epoch 28, Batch 961, LR 0.000020 Loss 4.823656, Accuracy 88.284%\n",
      "Epoch 28, Batch 962, LR 0.000020 Loss 4.823751, Accuracy 88.284%\n",
      "Epoch 28, Batch 963, LR 0.000020 Loss 4.824246, Accuracy 88.284%\n",
      "Epoch 28, Batch 964, LR 0.000020 Loss 4.824081, Accuracy 88.286%\n",
      "Epoch 28, Batch 965, LR 0.000020 Loss 4.824026, Accuracy 88.285%\n",
      "Epoch 28, Batch 966, LR 0.000020 Loss 4.824895, Accuracy 88.285%\n",
      "Epoch 28, Batch 967, LR 0.000020 Loss 4.824672, Accuracy 88.284%\n",
      "Epoch 28, Batch 968, LR 0.000020 Loss 4.824169, Accuracy 88.287%\n",
      "Epoch 28, Batch 969, LR 0.000020 Loss 4.824322, Accuracy 88.288%\n",
      "Epoch 28, Batch 970, LR 0.000020 Loss 4.824003, Accuracy 88.290%\n",
      "Epoch 28, Batch 971, LR 0.000020 Loss 4.823486, Accuracy 88.293%\n",
      "Epoch 28, Batch 972, LR 0.000020 Loss 4.823355, Accuracy 88.293%\n",
      "Epoch 28, Batch 973, LR 0.000020 Loss 4.824042, Accuracy 88.291%\n",
      "Epoch 28, Batch 974, LR 0.000020 Loss 4.824048, Accuracy 88.292%\n",
      "Epoch 28, Batch 975, LR 0.000020 Loss 4.823480, Accuracy 88.292%\n",
      "Epoch 28, Batch 976, LR 0.000020 Loss 4.823624, Accuracy 88.292%\n",
      "Epoch 28, Batch 977, LR 0.000020 Loss 4.823184, Accuracy 88.296%\n",
      "Epoch 28, Batch 978, LR 0.000020 Loss 4.823155, Accuracy 88.298%\n",
      "Epoch 28, Batch 979, LR 0.000020 Loss 4.823337, Accuracy 88.298%\n",
      "Epoch 28, Batch 980, LR 0.000019 Loss 4.824171, Accuracy 88.295%\n",
      "Epoch 28, Batch 981, LR 0.000019 Loss 4.823798, Accuracy 88.295%\n",
      "Epoch 28, Batch 982, LR 0.000019 Loss 4.824331, Accuracy 88.288%\n",
      "Epoch 28, Batch 983, LR 0.000019 Loss 4.824914, Accuracy 88.289%\n",
      "Epoch 28, Batch 984, LR 0.000019 Loss 4.825257, Accuracy 88.285%\n",
      "Epoch 28, Batch 985, LR 0.000019 Loss 4.824801, Accuracy 88.288%\n",
      "Epoch 28, Batch 986, LR 0.000019 Loss 4.824177, Accuracy 88.291%\n",
      "Epoch 28, Batch 987, LR 0.000019 Loss 4.824422, Accuracy 88.288%\n",
      "Epoch 28, Batch 988, LR 0.000019 Loss 4.824192, Accuracy 88.290%\n",
      "Epoch 28, Batch 989, LR 0.000019 Loss 4.823815, Accuracy 88.292%\n",
      "Epoch 28, Batch 990, LR 0.000019 Loss 4.823295, Accuracy 88.296%\n",
      "Epoch 28, Batch 991, LR 0.000019 Loss 4.823038, Accuracy 88.300%\n",
      "Epoch 28, Batch 992, LR 0.000019 Loss 4.822496, Accuracy 88.299%\n",
      "Epoch 28, Batch 993, LR 0.000019 Loss 4.822104, Accuracy 88.300%\n",
      "Epoch 28, Batch 994, LR 0.000019 Loss 4.821695, Accuracy 88.299%\n",
      "Epoch 28, Batch 995, LR 0.000019 Loss 4.822111, Accuracy 88.295%\n",
      "Epoch 28, Batch 996, LR 0.000019 Loss 4.822183, Accuracy 88.299%\n",
      "Epoch 28, Batch 997, LR 0.000019 Loss 4.822043, Accuracy 88.299%\n",
      "Epoch 28, Batch 998, LR 0.000019 Loss 4.821517, Accuracy 88.302%\n",
      "Epoch 28, Batch 999, LR 0.000019 Loss 4.821837, Accuracy 88.302%\n",
      "Epoch 28, Batch 1000, LR 0.000019 Loss 4.821769, Accuracy 88.301%\n",
      "Epoch 28, Batch 1001, LR 0.000019 Loss 4.820825, Accuracy 88.302%\n",
      "Epoch 28, Batch 1002, LR 0.000019 Loss 4.819928, Accuracy 88.303%\n",
      "Epoch 28, Batch 1003, LR 0.000019 Loss 4.819816, Accuracy 88.305%\n",
      "Epoch 28, Batch 1004, LR 0.000019 Loss 4.819777, Accuracy 88.307%\n",
      "Epoch 28, Batch 1005, LR 0.000019 Loss 4.819777, Accuracy 88.307%\n",
      "Epoch 28, Batch 1006, LR 0.000019 Loss 4.819138, Accuracy 88.308%\n",
      "Epoch 28, Batch 1007, LR 0.000019 Loss 4.818748, Accuracy 88.309%\n",
      "Epoch 28, Batch 1008, LR 0.000019 Loss 4.817931, Accuracy 88.316%\n",
      "Epoch 28, Batch 1009, LR 0.000019 Loss 4.817414, Accuracy 88.315%\n",
      "Epoch 28, Batch 1010, LR 0.000019 Loss 4.817400, Accuracy 88.316%\n",
      "Epoch 28, Batch 1011, LR 0.000019 Loss 4.817221, Accuracy 88.318%\n",
      "Epoch 28, Batch 1012, LR 0.000019 Loss 4.817173, Accuracy 88.320%\n",
      "Epoch 28, Batch 1013, LR 0.000019 Loss 4.816781, Accuracy 88.324%\n",
      "Epoch 28, Batch 1014, LR 0.000019 Loss 4.816666, Accuracy 88.325%\n",
      "Epoch 28, Batch 1015, LR 0.000019 Loss 4.816282, Accuracy 88.327%\n",
      "Epoch 28, Batch 1016, LR 0.000019 Loss 4.816792, Accuracy 88.327%\n",
      "Epoch 28, Batch 1017, LR 0.000019 Loss 4.816719, Accuracy 88.327%\n",
      "Epoch 28, Batch 1018, LR 0.000019 Loss 4.817070, Accuracy 88.323%\n",
      "Epoch 28, Batch 1019, LR 0.000019 Loss 4.817045, Accuracy 88.325%\n",
      "Epoch 28, Batch 1020, LR 0.000019 Loss 4.817209, Accuracy 88.323%\n",
      "Epoch 28, Batch 1021, LR 0.000019 Loss 4.817302, Accuracy 88.321%\n",
      "Epoch 28, Batch 1022, LR 0.000019 Loss 4.816879, Accuracy 88.323%\n",
      "Epoch 28, Batch 1023, LR 0.000019 Loss 4.816886, Accuracy 88.319%\n",
      "Epoch 28, Batch 1024, LR 0.000019 Loss 4.816012, Accuracy 88.324%\n",
      "Epoch 28, Batch 1025, LR 0.000019 Loss 4.815701, Accuracy 88.323%\n",
      "Epoch 28, Batch 1026, LR 0.000019 Loss 4.816042, Accuracy 88.320%\n",
      "Epoch 28, Batch 1027, LR 0.000019 Loss 4.816299, Accuracy 88.317%\n",
      "Epoch 28, Batch 1028, LR 0.000019 Loss 4.816257, Accuracy 88.316%\n",
      "Epoch 28, Batch 1029, LR 0.000019 Loss 4.816868, Accuracy 88.315%\n",
      "Epoch 28, Batch 1030, LR 0.000019 Loss 4.817414, Accuracy 88.318%\n",
      "Epoch 28, Batch 1031, LR 0.000019 Loss 4.817454, Accuracy 88.320%\n",
      "Epoch 28, Batch 1032, LR 0.000019 Loss 4.816862, Accuracy 88.320%\n",
      "Epoch 28, Batch 1033, LR 0.000019 Loss 4.816657, Accuracy 88.321%\n",
      "Epoch 28, Batch 1034, LR 0.000019 Loss 4.816189, Accuracy 88.324%\n",
      "Epoch 28, Batch 1035, LR 0.000019 Loss 4.815909, Accuracy 88.324%\n",
      "Epoch 28, Batch 1036, LR 0.000019 Loss 4.815682, Accuracy 88.325%\n",
      "Epoch 28, Batch 1037, LR 0.000019 Loss 4.814809, Accuracy 88.329%\n",
      "Epoch 28, Batch 1038, LR 0.000019 Loss 4.814392, Accuracy 88.329%\n",
      "Epoch 28, Batch 1039, LR 0.000019 Loss 4.814468, Accuracy 88.331%\n",
      "Epoch 28, Batch 1040, LR 0.000019 Loss 4.814575, Accuracy 88.333%\n",
      "Epoch 28, Batch 1041, LR 0.000019 Loss 4.814210, Accuracy 88.335%\n",
      "Epoch 28, Batch 1042, LR 0.000019 Loss 4.813970, Accuracy 88.332%\n",
      "Epoch 28, Batch 1043, LR 0.000019 Loss 4.813787, Accuracy 88.337%\n",
      "Epoch 28, Batch 1044, LR 0.000019 Loss 4.813588, Accuracy 88.337%\n",
      "Epoch 28, Batch 1045, LR 0.000019 Loss 4.813509, Accuracy 88.339%\n",
      "Epoch 28, Batch 1046, LR 0.000019 Loss 4.814417, Accuracy 88.335%\n",
      "Epoch 28, Batch 1047, LR 0.000019 Loss 4.813533, Accuracy 88.338%\n",
      "Epoch 28, Loss (train set) 4.813533, Accuracy (train set) 88.338%\n",
      "Epoch 29, Batch 1, LR 0.000019 Loss 5.300954, Accuracy 82.031%\n",
      "Epoch 29, Batch 2, LR 0.000019 Loss 5.001321, Accuracy 85.938%\n",
      "Epoch 29, Batch 3, LR 0.000019 Loss 4.858395, Accuracy 87.500%\n",
      "Epoch 29, Batch 4, LR 0.000019 Loss 4.725427, Accuracy 88.477%\n",
      "Epoch 29, Batch 5, LR 0.000019 Loss 4.665320, Accuracy 88.594%\n",
      "Epoch 29, Batch 6, LR 0.000019 Loss 4.521019, Accuracy 88.932%\n",
      "Epoch 29, Batch 7, LR 0.000019 Loss 4.432688, Accuracy 89.174%\n",
      "Epoch 29, Batch 8, LR 0.000019 Loss 4.538289, Accuracy 88.867%\n",
      "Epoch 29, Batch 9, LR 0.000019 Loss 4.601587, Accuracy 88.976%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 10, LR 0.000019 Loss 4.594380, Accuracy 88.906%\n",
      "Epoch 29, Batch 11, LR 0.000019 Loss 4.602296, Accuracy 88.849%\n",
      "Epoch 29, Batch 12, LR 0.000019 Loss 4.621515, Accuracy 88.802%\n",
      "Epoch 29, Batch 13, LR 0.000019 Loss 4.590758, Accuracy 88.762%\n",
      "Epoch 29, Batch 14, LR 0.000019 Loss 4.528971, Accuracy 89.230%\n",
      "Epoch 29, Batch 15, LR 0.000019 Loss 4.517920, Accuracy 89.115%\n",
      "Epoch 29, Batch 16, LR 0.000019 Loss 4.569259, Accuracy 88.721%\n",
      "Epoch 29, Batch 17, LR 0.000019 Loss 4.584424, Accuracy 88.557%\n",
      "Epoch 29, Batch 18, LR 0.000019 Loss 4.620297, Accuracy 88.498%\n",
      "Epoch 29, Batch 19, LR 0.000019 Loss 4.625748, Accuracy 88.528%\n",
      "Epoch 29, Batch 20, LR 0.000019 Loss 4.637075, Accuracy 88.594%\n",
      "Epoch 29, Batch 21, LR 0.000019 Loss 4.649623, Accuracy 88.504%\n",
      "Epoch 29, Batch 22, LR 0.000019 Loss 4.669288, Accuracy 88.530%\n",
      "Epoch 29, Batch 23, LR 0.000019 Loss 4.683413, Accuracy 88.723%\n",
      "Epoch 29, Batch 24, LR 0.000019 Loss 4.663070, Accuracy 88.900%\n",
      "Epoch 29, Batch 25, LR 0.000019 Loss 4.660979, Accuracy 88.906%\n",
      "Epoch 29, Batch 26, LR 0.000019 Loss 4.688533, Accuracy 89.002%\n",
      "Epoch 29, Batch 27, LR 0.000019 Loss 4.735934, Accuracy 88.686%\n",
      "Epoch 29, Batch 28, LR 0.000019 Loss 4.731553, Accuracy 88.672%\n",
      "Epoch 29, Batch 29, LR 0.000019 Loss 4.725856, Accuracy 88.685%\n",
      "Epoch 29, Batch 30, LR 0.000019 Loss 4.709251, Accuracy 88.802%\n",
      "Epoch 29, Batch 31, LR 0.000019 Loss 4.716033, Accuracy 88.735%\n",
      "Epoch 29, Batch 32, LR 0.000019 Loss 4.725132, Accuracy 88.696%\n",
      "Epoch 29, Batch 33, LR 0.000019 Loss 4.734026, Accuracy 88.565%\n",
      "Epoch 29, Batch 34, LR 0.000019 Loss 4.741450, Accuracy 88.580%\n",
      "Epoch 29, Batch 35, LR 0.000019 Loss 4.756512, Accuracy 88.527%\n",
      "Epoch 29, Batch 36, LR 0.000019 Loss 4.754542, Accuracy 88.607%\n",
      "Epoch 29, Batch 37, LR 0.000019 Loss 4.760648, Accuracy 88.514%\n",
      "Epoch 29, Batch 38, LR 0.000019 Loss 4.745319, Accuracy 88.569%\n",
      "Epoch 29, Batch 39, LR 0.000019 Loss 4.726199, Accuracy 88.642%\n",
      "Epoch 29, Batch 40, LR 0.000019 Loss 4.720954, Accuracy 88.672%\n",
      "Epoch 29, Batch 41, LR 0.000019 Loss 4.707494, Accuracy 88.700%\n",
      "Epoch 29, Batch 42, LR 0.000019 Loss 4.706651, Accuracy 88.746%\n",
      "Epoch 29, Batch 43, LR 0.000019 Loss 4.700771, Accuracy 88.826%\n",
      "Epoch 29, Batch 44, LR 0.000019 Loss 4.704870, Accuracy 88.867%\n",
      "Epoch 29, Batch 45, LR 0.000019 Loss 4.703939, Accuracy 88.872%\n",
      "Epoch 29, Batch 46, LR 0.000019 Loss 4.718421, Accuracy 88.825%\n",
      "Epoch 29, Batch 47, LR 0.000019 Loss 4.700837, Accuracy 88.846%\n",
      "Epoch 29, Batch 48, LR 0.000019 Loss 4.715644, Accuracy 88.818%\n",
      "Epoch 29, Batch 49, LR 0.000019 Loss 4.724305, Accuracy 88.776%\n",
      "Epoch 29, Batch 50, LR 0.000019 Loss 4.735322, Accuracy 88.672%\n",
      "Epoch 29, Batch 51, LR 0.000019 Loss 4.750975, Accuracy 88.542%\n",
      "Epoch 29, Batch 52, LR 0.000019 Loss 4.739330, Accuracy 88.657%\n",
      "Epoch 29, Batch 53, LR 0.000019 Loss 4.729010, Accuracy 88.709%\n",
      "Epoch 29, Batch 54, LR 0.000019 Loss 4.730849, Accuracy 88.614%\n",
      "Epoch 29, Batch 55, LR 0.000019 Loss 4.730066, Accuracy 88.651%\n",
      "Epoch 29, Batch 56, LR 0.000019 Loss 4.727562, Accuracy 88.672%\n",
      "Epoch 29, Batch 57, LR 0.000019 Loss 4.733274, Accuracy 88.679%\n",
      "Epoch 29, Batch 58, LR 0.000019 Loss 4.740856, Accuracy 88.672%\n",
      "Epoch 29, Batch 59, LR 0.000019 Loss 4.744247, Accuracy 88.652%\n",
      "Epoch 29, Batch 60, LR 0.000019 Loss 4.736819, Accuracy 88.724%\n",
      "Epoch 29, Batch 61, LR 0.000019 Loss 4.729142, Accuracy 88.742%\n",
      "Epoch 29, Batch 62, LR 0.000019 Loss 4.741495, Accuracy 88.596%\n",
      "Epoch 29, Batch 63, LR 0.000019 Loss 4.744156, Accuracy 88.566%\n",
      "Epoch 29, Batch 64, LR 0.000019 Loss 4.744755, Accuracy 88.562%\n",
      "Epoch 29, Batch 65, LR 0.000019 Loss 4.753928, Accuracy 88.486%\n",
      "Epoch 29, Batch 66, LR 0.000019 Loss 4.753441, Accuracy 88.506%\n",
      "Epoch 29, Batch 67, LR 0.000019 Loss 4.753277, Accuracy 88.468%\n",
      "Epoch 29, Batch 68, LR 0.000019 Loss 4.739528, Accuracy 88.523%\n",
      "Epoch 29, Batch 69, LR 0.000019 Loss 4.737827, Accuracy 88.542%\n",
      "Epoch 29, Batch 70, LR 0.000019 Loss 4.739549, Accuracy 88.583%\n",
      "Epoch 29, Batch 71, LR 0.000019 Loss 4.745957, Accuracy 88.633%\n",
      "Epoch 29, Batch 72, LR 0.000019 Loss 4.749073, Accuracy 88.628%\n",
      "Epoch 29, Batch 73, LR 0.000019 Loss 4.754646, Accuracy 88.624%\n",
      "Epoch 29, Batch 74, LR 0.000019 Loss 4.738036, Accuracy 88.704%\n",
      "Epoch 29, Batch 75, LR 0.000019 Loss 4.743502, Accuracy 88.719%\n",
      "Epoch 29, Batch 76, LR 0.000019 Loss 4.742378, Accuracy 88.692%\n",
      "Epoch 29, Batch 77, LR 0.000019 Loss 4.744169, Accuracy 88.687%\n",
      "Epoch 29, Batch 78, LR 0.000019 Loss 4.746191, Accuracy 88.642%\n",
      "Epoch 29, Batch 79, LR 0.000019 Loss 4.740554, Accuracy 88.677%\n",
      "Epoch 29, Batch 80, LR 0.000019 Loss 4.742823, Accuracy 88.652%\n",
      "Epoch 29, Batch 81, LR 0.000019 Loss 4.733775, Accuracy 88.657%\n",
      "Epoch 29, Batch 82, LR 0.000019 Loss 4.732154, Accuracy 88.681%\n",
      "Epoch 29, Batch 83, LR 0.000019 Loss 4.727708, Accuracy 88.752%\n",
      "Epoch 29, Batch 84, LR 0.000019 Loss 4.726343, Accuracy 88.783%\n",
      "Epoch 29, Batch 85, LR 0.000019 Loss 4.728831, Accuracy 88.805%\n",
      "Epoch 29, Batch 86, LR 0.000019 Loss 4.728043, Accuracy 88.808%\n",
      "Epoch 29, Batch 87, LR 0.000019 Loss 4.736178, Accuracy 88.757%\n",
      "Epoch 29, Batch 88, LR 0.000019 Loss 4.735454, Accuracy 88.770%\n",
      "Epoch 29, Batch 89, LR 0.000019 Loss 4.740517, Accuracy 88.790%\n",
      "Epoch 29, Batch 90, LR 0.000019 Loss 4.736946, Accuracy 88.819%\n",
      "Epoch 29, Batch 91, LR 0.000019 Loss 4.745312, Accuracy 88.753%\n",
      "Epoch 29, Batch 92, LR 0.000019 Loss 4.748291, Accuracy 88.697%\n",
      "Epoch 29, Batch 93, LR 0.000019 Loss 4.742565, Accuracy 88.735%\n",
      "Epoch 29, Batch 94, LR 0.000019 Loss 4.741576, Accuracy 88.738%\n",
      "Epoch 29, Batch 95, LR 0.000019 Loss 4.744188, Accuracy 88.758%\n",
      "Epoch 29, Batch 96, LR 0.000019 Loss 4.755929, Accuracy 88.704%\n",
      "Epoch 29, Batch 97, LR 0.000019 Loss 4.764084, Accuracy 88.652%\n",
      "Epoch 29, Batch 98, LR 0.000019 Loss 4.761800, Accuracy 88.696%\n",
      "Epoch 29, Batch 99, LR 0.000019 Loss 4.761220, Accuracy 88.676%\n",
      "Epoch 29, Batch 100, LR 0.000019 Loss 4.765963, Accuracy 88.656%\n",
      "Epoch 29, Batch 101, LR 0.000019 Loss 4.770951, Accuracy 88.629%\n",
      "Epoch 29, Batch 102, LR 0.000019 Loss 4.775267, Accuracy 88.595%\n",
      "Epoch 29, Batch 103, LR 0.000019 Loss 4.779957, Accuracy 88.547%\n",
      "Epoch 29, Batch 104, LR 0.000019 Loss 4.788146, Accuracy 88.484%\n",
      "Epoch 29, Batch 105, LR 0.000019 Loss 4.784017, Accuracy 88.482%\n",
      "Epoch 29, Batch 106, LR 0.000019 Loss 4.778037, Accuracy 88.488%\n",
      "Epoch 29, Batch 107, LR 0.000019 Loss 4.778180, Accuracy 88.478%\n",
      "Epoch 29, Batch 108, LR 0.000019 Loss 4.774731, Accuracy 88.505%\n",
      "Epoch 29, Batch 109, LR 0.000019 Loss 4.772131, Accuracy 88.503%\n",
      "Epoch 29, Batch 110, LR 0.000019 Loss 4.776010, Accuracy 88.516%\n",
      "Epoch 29, Batch 111, LR 0.000019 Loss 4.777799, Accuracy 88.471%\n",
      "Epoch 29, Batch 112, LR 0.000019 Loss 4.773857, Accuracy 88.491%\n",
      "Epoch 29, Batch 113, LR 0.000019 Loss 4.770796, Accuracy 88.509%\n",
      "Epoch 29, Batch 114, LR 0.000019 Loss 4.770772, Accuracy 88.487%\n",
      "Epoch 29, Batch 115, LR 0.000019 Loss 4.767934, Accuracy 88.478%\n",
      "Epoch 29, Batch 116, LR 0.000019 Loss 4.769576, Accuracy 88.490%\n",
      "Epoch 29, Batch 117, LR 0.000019 Loss 4.775296, Accuracy 88.475%\n",
      "Epoch 29, Batch 118, LR 0.000019 Loss 4.773128, Accuracy 88.480%\n",
      "Epoch 29, Batch 119, LR 0.000019 Loss 4.771820, Accuracy 88.498%\n",
      "Epoch 29, Batch 120, LR 0.000019 Loss 4.775069, Accuracy 88.464%\n",
      "Epoch 29, Batch 121, LR 0.000019 Loss 4.770512, Accuracy 88.494%\n",
      "Epoch 29, Batch 122, LR 0.000019 Loss 4.767309, Accuracy 88.512%\n",
      "Epoch 29, Batch 123, LR 0.000019 Loss 4.773300, Accuracy 88.478%\n",
      "Epoch 29, Batch 124, LR 0.000019 Loss 4.775016, Accuracy 88.489%\n",
      "Epoch 29, Batch 125, LR 0.000019 Loss 4.774323, Accuracy 88.487%\n",
      "Epoch 29, Batch 126, LR 0.000019 Loss 4.769596, Accuracy 88.517%\n",
      "Epoch 29, Batch 127, LR 0.000019 Loss 4.767977, Accuracy 88.552%\n",
      "Epoch 29, Batch 128, LR 0.000019 Loss 4.763523, Accuracy 88.574%\n",
      "Epoch 29, Batch 129, LR 0.000019 Loss 4.762105, Accuracy 88.578%\n",
      "Epoch 29, Batch 130, LR 0.000019 Loss 4.759073, Accuracy 88.600%\n",
      "Epoch 29, Batch 131, LR 0.000019 Loss 4.756095, Accuracy 88.609%\n",
      "Epoch 29, Batch 132, LR 0.000019 Loss 4.757147, Accuracy 88.613%\n",
      "Epoch 29, Batch 133, LR 0.000019 Loss 4.761170, Accuracy 88.587%\n",
      "Epoch 29, Batch 134, LR 0.000019 Loss 4.764174, Accuracy 88.584%\n",
      "Epoch 29, Batch 135, LR 0.000019 Loss 4.770695, Accuracy 88.547%\n",
      "Epoch 29, Batch 136, LR 0.000019 Loss 4.770107, Accuracy 88.586%\n",
      "Epoch 29, Batch 137, LR 0.000019 Loss 4.774134, Accuracy 88.595%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 138, LR 0.000019 Loss 4.777025, Accuracy 88.581%\n",
      "Epoch 29, Batch 139, LR 0.000019 Loss 4.779363, Accuracy 88.562%\n",
      "Epoch 29, Batch 140, LR 0.000019 Loss 4.776152, Accuracy 88.566%\n",
      "Epoch 29, Batch 141, LR 0.000019 Loss 4.780962, Accuracy 88.520%\n",
      "Epoch 29, Batch 142, LR 0.000019 Loss 4.781455, Accuracy 88.529%\n",
      "Epoch 29, Batch 143, LR 0.000019 Loss 4.779522, Accuracy 88.516%\n",
      "Epoch 29, Batch 144, LR 0.000019 Loss 4.776156, Accuracy 88.536%\n",
      "Epoch 29, Batch 145, LR 0.000019 Loss 4.778248, Accuracy 88.524%\n",
      "Epoch 29, Batch 146, LR 0.000019 Loss 4.778996, Accuracy 88.522%\n",
      "Epoch 29, Batch 147, LR 0.000019 Loss 4.780953, Accuracy 88.499%\n",
      "Epoch 29, Batch 148, LR 0.000019 Loss 4.778516, Accuracy 88.498%\n",
      "Epoch 29, Batch 149, LR 0.000019 Loss 4.780581, Accuracy 88.491%\n",
      "Epoch 29, Batch 150, LR 0.000019 Loss 4.779438, Accuracy 88.479%\n",
      "Epoch 29, Batch 151, LR 0.000019 Loss 4.781083, Accuracy 88.483%\n",
      "Epoch 29, Batch 152, LR 0.000019 Loss 4.779312, Accuracy 88.487%\n",
      "Epoch 29, Batch 153, LR 0.000019 Loss 4.777614, Accuracy 88.480%\n",
      "Epoch 29, Batch 154, LR 0.000019 Loss 4.781365, Accuracy 88.474%\n",
      "Epoch 29, Batch 155, LR 0.000019 Loss 4.780090, Accuracy 88.503%\n",
      "Epoch 29, Batch 156, LR 0.000019 Loss 4.776661, Accuracy 88.522%\n",
      "Epoch 29, Batch 157, LR 0.000019 Loss 4.775674, Accuracy 88.520%\n",
      "Epoch 29, Batch 158, LR 0.000019 Loss 4.772538, Accuracy 88.514%\n",
      "Epoch 29, Batch 159, LR 0.000019 Loss 4.766932, Accuracy 88.527%\n",
      "Epoch 29, Batch 160, LR 0.000019 Loss 4.768727, Accuracy 88.525%\n",
      "Epoch 29, Batch 161, LR 0.000019 Loss 4.775122, Accuracy 88.490%\n",
      "Epoch 29, Batch 162, LR 0.000019 Loss 4.773490, Accuracy 88.493%\n",
      "Epoch 29, Batch 163, LR 0.000019 Loss 4.770795, Accuracy 88.487%\n",
      "Epoch 29, Batch 164, LR 0.000019 Loss 4.767078, Accuracy 88.510%\n",
      "Epoch 29, Batch 165, LR 0.000019 Loss 4.768657, Accuracy 88.499%\n",
      "Epoch 29, Batch 166, LR 0.000019 Loss 4.771006, Accuracy 88.507%\n",
      "Epoch 29, Batch 167, LR 0.000019 Loss 4.771857, Accuracy 88.520%\n",
      "Epoch 29, Batch 168, LR 0.000019 Loss 4.774063, Accuracy 88.500%\n",
      "Epoch 29, Batch 169, LR 0.000019 Loss 4.768307, Accuracy 88.526%\n",
      "Epoch 29, Batch 170, LR 0.000019 Loss 4.768172, Accuracy 88.525%\n",
      "Epoch 29, Batch 171, LR 0.000019 Loss 4.768521, Accuracy 88.519%\n",
      "Epoch 29, Batch 172, LR 0.000019 Loss 4.763566, Accuracy 88.545%\n",
      "Epoch 29, Batch 173, LR 0.000019 Loss 4.755668, Accuracy 88.579%\n",
      "Epoch 29, Batch 174, LR 0.000019 Loss 4.754463, Accuracy 88.569%\n",
      "Epoch 29, Batch 175, LR 0.000019 Loss 4.751896, Accuracy 88.580%\n",
      "Epoch 29, Batch 176, LR 0.000019 Loss 4.752052, Accuracy 88.596%\n",
      "Epoch 29, Batch 177, LR 0.000019 Loss 4.749590, Accuracy 88.617%\n",
      "Epoch 29, Batch 178, LR 0.000019 Loss 4.751611, Accuracy 88.610%\n",
      "Epoch 29, Batch 179, LR 0.000019 Loss 4.748715, Accuracy 88.609%\n",
      "Epoch 29, Batch 180, LR 0.000019 Loss 4.747445, Accuracy 88.624%\n",
      "Epoch 29, Batch 181, LR 0.000019 Loss 4.747088, Accuracy 88.618%\n",
      "Epoch 29, Batch 182, LR 0.000019 Loss 4.749136, Accuracy 88.603%\n",
      "Epoch 29, Batch 183, LR 0.000019 Loss 4.747054, Accuracy 88.614%\n",
      "Epoch 29, Batch 184, LR 0.000019 Loss 4.750949, Accuracy 88.583%\n",
      "Epoch 29, Batch 185, LR 0.000019 Loss 4.747377, Accuracy 88.606%\n",
      "Epoch 29, Batch 186, LR 0.000019 Loss 4.747754, Accuracy 88.613%\n",
      "Epoch 29, Batch 187, LR 0.000019 Loss 4.746564, Accuracy 88.624%\n",
      "Epoch 29, Batch 188, LR 0.000019 Loss 4.747600, Accuracy 88.639%\n",
      "Epoch 29, Batch 189, LR 0.000019 Loss 4.741683, Accuracy 88.653%\n",
      "Epoch 29, Batch 190, LR 0.000019 Loss 4.740025, Accuracy 88.647%\n",
      "Epoch 29, Batch 191, LR 0.000019 Loss 4.741481, Accuracy 88.625%\n",
      "Epoch 29, Batch 192, LR 0.000019 Loss 4.739510, Accuracy 88.643%\n",
      "Epoch 29, Batch 193, LR 0.000019 Loss 4.742052, Accuracy 88.646%\n",
      "Epoch 29, Batch 194, LR 0.000019 Loss 4.745948, Accuracy 88.628%\n",
      "Epoch 29, Batch 195, LR 0.000019 Loss 4.746231, Accuracy 88.626%\n",
      "Epoch 29, Batch 196, LR 0.000019 Loss 4.747839, Accuracy 88.588%\n",
      "Epoch 29, Batch 197, LR 0.000019 Loss 4.749140, Accuracy 88.595%\n",
      "Epoch 29, Batch 198, LR 0.000019 Loss 4.749208, Accuracy 88.609%\n",
      "Epoch 29, Batch 199, LR 0.000019 Loss 4.749504, Accuracy 88.627%\n",
      "Epoch 29, Batch 200, LR 0.000019 Loss 4.749680, Accuracy 88.633%\n",
      "Epoch 29, Batch 201, LR 0.000019 Loss 4.751777, Accuracy 88.616%\n",
      "Epoch 29, Batch 202, LR 0.000019 Loss 4.754543, Accuracy 88.614%\n",
      "Epoch 29, Batch 203, LR 0.000019 Loss 4.755677, Accuracy 88.612%\n",
      "Epoch 29, Batch 204, LR 0.000019 Loss 4.757394, Accuracy 88.591%\n",
      "Epoch 29, Batch 205, LR 0.000019 Loss 4.760492, Accuracy 88.575%\n",
      "Epoch 29, Batch 206, LR 0.000019 Loss 4.760062, Accuracy 88.585%\n",
      "Epoch 29, Batch 207, LR 0.000019 Loss 4.759646, Accuracy 88.591%\n",
      "Epoch 29, Batch 208, LR 0.000019 Loss 4.760418, Accuracy 88.589%\n",
      "Epoch 29, Batch 209, LR 0.000019 Loss 4.760725, Accuracy 88.569%\n",
      "Epoch 29, Batch 210, LR 0.000019 Loss 4.758905, Accuracy 88.579%\n",
      "Epoch 29, Batch 211, LR 0.000019 Loss 4.760425, Accuracy 88.574%\n",
      "Epoch 29, Batch 212, LR 0.000019 Loss 4.760677, Accuracy 88.554%\n",
      "Epoch 29, Batch 213, LR 0.000019 Loss 4.761262, Accuracy 88.560%\n",
      "Epoch 29, Batch 214, LR 0.000019 Loss 4.761034, Accuracy 88.566%\n",
      "Epoch 29, Batch 215, LR 0.000019 Loss 4.762194, Accuracy 88.557%\n",
      "Epoch 29, Batch 216, LR 0.000019 Loss 4.762217, Accuracy 88.549%\n",
      "Epoch 29, Batch 217, LR 0.000019 Loss 4.763097, Accuracy 88.548%\n",
      "Epoch 29, Batch 218, LR 0.000019 Loss 4.761906, Accuracy 88.543%\n",
      "Epoch 29, Batch 219, LR 0.000019 Loss 4.762348, Accuracy 88.560%\n",
      "Epoch 29, Batch 220, LR 0.000019 Loss 4.761829, Accuracy 88.558%\n",
      "Epoch 29, Batch 221, LR 0.000019 Loss 4.760370, Accuracy 88.571%\n",
      "Epoch 29, Batch 222, LR 0.000019 Loss 4.762790, Accuracy 88.552%\n",
      "Epoch 29, Batch 223, LR 0.000019 Loss 4.760356, Accuracy 88.565%\n",
      "Epoch 29, Batch 224, LR 0.000019 Loss 4.757810, Accuracy 88.574%\n",
      "Epoch 29, Batch 225, LR 0.000019 Loss 4.758184, Accuracy 88.562%\n",
      "Epoch 29, Batch 226, LR 0.000019 Loss 4.757907, Accuracy 88.551%\n",
      "Epoch 29, Batch 227, LR 0.000019 Loss 4.755604, Accuracy 88.550%\n",
      "Epoch 29, Batch 228, LR 0.000019 Loss 4.755107, Accuracy 88.545%\n",
      "Epoch 29, Batch 229, LR 0.000019 Loss 4.755783, Accuracy 88.537%\n",
      "Epoch 29, Batch 230, LR 0.000019 Loss 4.754806, Accuracy 88.543%\n",
      "Epoch 29, Batch 231, LR 0.000019 Loss 4.758143, Accuracy 88.518%\n",
      "Epoch 29, Batch 232, LR 0.000019 Loss 4.762695, Accuracy 88.490%\n",
      "Epoch 29, Batch 233, LR 0.000019 Loss 4.759936, Accuracy 88.509%\n",
      "Epoch 29, Batch 234, LR 0.000019 Loss 4.762681, Accuracy 88.502%\n",
      "Epoch 29, Batch 235, LR 0.000019 Loss 4.762276, Accuracy 88.501%\n",
      "Epoch 29, Batch 236, LR 0.000019 Loss 4.758896, Accuracy 88.516%\n",
      "Epoch 29, Batch 237, LR 0.000019 Loss 4.757001, Accuracy 88.509%\n",
      "Epoch 29, Batch 238, LR 0.000019 Loss 4.757815, Accuracy 88.488%\n",
      "Epoch 29, Batch 239, LR 0.000019 Loss 4.759587, Accuracy 88.487%\n",
      "Epoch 29, Batch 240, LR 0.000019 Loss 4.760312, Accuracy 88.496%\n",
      "Epoch 29, Batch 241, LR 0.000019 Loss 4.758746, Accuracy 88.498%\n",
      "Epoch 29, Batch 242, LR 0.000019 Loss 4.757645, Accuracy 88.507%\n",
      "Epoch 29, Batch 243, LR 0.000019 Loss 4.757523, Accuracy 88.500%\n",
      "Epoch 29, Batch 244, LR 0.000019 Loss 4.757157, Accuracy 88.502%\n",
      "Epoch 29, Batch 245, LR 0.000019 Loss 4.753955, Accuracy 88.524%\n",
      "Epoch 29, Batch 246, LR 0.000019 Loss 4.754162, Accuracy 88.526%\n",
      "Epoch 29, Batch 247, LR 0.000019 Loss 4.752115, Accuracy 88.525%\n",
      "Epoch 29, Batch 248, LR 0.000019 Loss 4.752210, Accuracy 88.521%\n",
      "Epoch 29, Batch 249, LR 0.000019 Loss 4.750186, Accuracy 88.526%\n",
      "Epoch 29, Batch 250, LR 0.000019 Loss 4.749423, Accuracy 88.528%\n",
      "Epoch 29, Batch 251, LR 0.000019 Loss 4.747856, Accuracy 88.524%\n",
      "Epoch 29, Batch 252, LR 0.000019 Loss 4.744347, Accuracy 88.551%\n",
      "Epoch 29, Batch 253, LR 0.000019 Loss 4.746539, Accuracy 88.544%\n",
      "Epoch 29, Batch 254, LR 0.000019 Loss 4.746767, Accuracy 88.527%\n",
      "Epoch 29, Batch 255, LR 0.000019 Loss 4.745521, Accuracy 88.554%\n",
      "Epoch 29, Batch 256, LR 0.000018 Loss 4.743434, Accuracy 88.556%\n",
      "Epoch 29, Batch 257, LR 0.000018 Loss 4.746860, Accuracy 88.531%\n",
      "Epoch 29, Batch 258, LR 0.000018 Loss 4.747813, Accuracy 88.517%\n",
      "Epoch 29, Batch 259, LR 0.000018 Loss 4.746530, Accuracy 88.526%\n",
      "Epoch 29, Batch 260, LR 0.000018 Loss 4.746269, Accuracy 88.522%\n",
      "Epoch 29, Batch 261, LR 0.000018 Loss 4.749368, Accuracy 88.509%\n",
      "Epoch 29, Batch 262, LR 0.000018 Loss 4.749470, Accuracy 88.514%\n",
      "Epoch 29, Batch 263, LR 0.000018 Loss 4.748532, Accuracy 88.522%\n",
      "Epoch 29, Batch 264, LR 0.000018 Loss 4.749483, Accuracy 88.518%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 265, LR 0.000018 Loss 4.750283, Accuracy 88.514%\n",
      "Epoch 29, Batch 266, LR 0.000018 Loss 4.750388, Accuracy 88.516%\n",
      "Epoch 29, Batch 267, LR 0.000018 Loss 4.752713, Accuracy 88.512%\n",
      "Epoch 29, Batch 268, LR 0.000018 Loss 4.750261, Accuracy 88.523%\n",
      "Epoch 29, Batch 269, LR 0.000018 Loss 4.750705, Accuracy 88.511%\n",
      "Epoch 29, Batch 270, LR 0.000018 Loss 4.751608, Accuracy 88.504%\n",
      "Epoch 29, Batch 271, LR 0.000018 Loss 4.752788, Accuracy 88.489%\n",
      "Epoch 29, Batch 272, LR 0.000018 Loss 4.750522, Accuracy 88.491%\n",
      "Epoch 29, Batch 273, LR 0.000018 Loss 4.748291, Accuracy 88.490%\n",
      "Epoch 29, Batch 274, LR 0.000018 Loss 4.748569, Accuracy 88.481%\n",
      "Epoch 29, Batch 275, LR 0.000018 Loss 4.749412, Accuracy 88.486%\n",
      "Epoch 29, Batch 276, LR 0.000018 Loss 4.749231, Accuracy 88.494%\n",
      "Epoch 29, Batch 277, LR 0.000018 Loss 4.747559, Accuracy 88.518%\n",
      "Epoch 29, Batch 278, LR 0.000018 Loss 4.748398, Accuracy 88.503%\n",
      "Epoch 29, Batch 279, LR 0.000018 Loss 4.748690, Accuracy 88.497%\n",
      "Epoch 29, Batch 280, LR 0.000018 Loss 4.749001, Accuracy 88.493%\n",
      "Epoch 29, Batch 281, LR 0.000018 Loss 4.749528, Accuracy 88.479%\n",
      "Epoch 29, Batch 282, LR 0.000018 Loss 4.745858, Accuracy 88.495%\n",
      "Epoch 29, Batch 283, LR 0.000018 Loss 4.745307, Accuracy 88.491%\n",
      "Epoch 29, Batch 284, LR 0.000018 Loss 4.746260, Accuracy 88.496%\n",
      "Epoch 29, Batch 285, LR 0.000018 Loss 4.744138, Accuracy 88.501%\n",
      "Epoch 29, Batch 286, LR 0.000018 Loss 4.741821, Accuracy 88.505%\n",
      "Epoch 29, Batch 287, LR 0.000018 Loss 4.744253, Accuracy 88.499%\n",
      "Epoch 29, Batch 288, LR 0.000018 Loss 4.745743, Accuracy 88.471%\n",
      "Epoch 29, Batch 289, LR 0.000018 Loss 4.745060, Accuracy 88.468%\n",
      "Epoch 29, Batch 290, LR 0.000018 Loss 4.743658, Accuracy 88.478%\n",
      "Epoch 29, Batch 291, LR 0.000018 Loss 4.743379, Accuracy 88.491%\n",
      "Epoch 29, Batch 292, LR 0.000018 Loss 4.740712, Accuracy 88.498%\n",
      "Epoch 29, Batch 293, LR 0.000018 Loss 4.742451, Accuracy 88.489%\n",
      "Epoch 29, Batch 294, LR 0.000018 Loss 4.741761, Accuracy 88.489%\n",
      "Epoch 29, Batch 295, LR 0.000018 Loss 4.741281, Accuracy 88.490%\n",
      "Epoch 29, Batch 296, LR 0.000018 Loss 4.744130, Accuracy 88.484%\n",
      "Epoch 29, Batch 297, LR 0.000018 Loss 4.746924, Accuracy 88.476%\n",
      "Epoch 29, Batch 298, LR 0.000018 Loss 4.745866, Accuracy 88.478%\n",
      "Epoch 29, Batch 299, LR 0.000018 Loss 4.745037, Accuracy 88.488%\n",
      "Epoch 29, Batch 300, LR 0.000018 Loss 4.745952, Accuracy 88.490%\n",
      "Epoch 29, Batch 301, LR 0.000018 Loss 4.747498, Accuracy 88.479%\n",
      "Epoch 29, Batch 302, LR 0.000018 Loss 4.745922, Accuracy 88.486%\n",
      "Epoch 29, Batch 303, LR 0.000018 Loss 4.744446, Accuracy 88.488%\n",
      "Epoch 29, Batch 304, LR 0.000018 Loss 4.742723, Accuracy 88.500%\n",
      "Epoch 29, Batch 305, LR 0.000018 Loss 4.740353, Accuracy 88.514%\n",
      "Epoch 29, Batch 306, LR 0.000018 Loss 4.741443, Accuracy 88.503%\n",
      "Epoch 29, Batch 307, LR 0.000018 Loss 4.740745, Accuracy 88.505%\n",
      "Epoch 29, Batch 308, LR 0.000018 Loss 4.742421, Accuracy 88.502%\n",
      "Epoch 29, Batch 309, LR 0.000018 Loss 4.743185, Accuracy 88.516%\n",
      "Epoch 29, Batch 310, LR 0.000018 Loss 4.741762, Accuracy 88.523%\n",
      "Epoch 29, Batch 311, LR 0.000018 Loss 4.741824, Accuracy 88.512%\n",
      "Epoch 29, Batch 312, LR 0.000018 Loss 4.741262, Accuracy 88.512%\n",
      "Epoch 29, Batch 313, LR 0.000018 Loss 4.741710, Accuracy 88.513%\n",
      "Epoch 29, Batch 314, LR 0.000018 Loss 4.742539, Accuracy 88.503%\n",
      "Epoch 29, Batch 315, LR 0.000018 Loss 4.742872, Accuracy 88.502%\n",
      "Epoch 29, Batch 316, LR 0.000018 Loss 4.744392, Accuracy 88.504%\n",
      "Epoch 29, Batch 317, LR 0.000018 Loss 4.744709, Accuracy 88.498%\n",
      "Epoch 29, Batch 318, LR 0.000018 Loss 4.743439, Accuracy 88.505%\n",
      "Epoch 29, Batch 319, LR 0.000018 Loss 4.743346, Accuracy 88.511%\n",
      "Epoch 29, Batch 320, LR 0.000018 Loss 4.744957, Accuracy 88.508%\n",
      "Epoch 29, Batch 321, LR 0.000018 Loss 4.744778, Accuracy 88.510%\n",
      "Epoch 29, Batch 322, LR 0.000018 Loss 4.744562, Accuracy 88.514%\n",
      "Epoch 29, Batch 323, LR 0.000018 Loss 4.746473, Accuracy 88.509%\n",
      "Epoch 29, Batch 324, LR 0.000018 Loss 4.746072, Accuracy 88.508%\n",
      "Epoch 29, Batch 325, LR 0.000018 Loss 4.744486, Accuracy 88.510%\n",
      "Epoch 29, Batch 326, LR 0.000018 Loss 4.745109, Accuracy 88.497%\n",
      "Epoch 29, Batch 327, LR 0.000018 Loss 4.746024, Accuracy 88.496%\n",
      "Epoch 29, Batch 328, LR 0.000018 Loss 4.743662, Accuracy 88.503%\n",
      "Epoch 29, Batch 329, LR 0.000018 Loss 4.743005, Accuracy 88.502%\n",
      "Epoch 29, Batch 330, LR 0.000018 Loss 4.742978, Accuracy 88.509%\n",
      "Epoch 29, Batch 331, LR 0.000018 Loss 4.740877, Accuracy 88.510%\n",
      "Epoch 29, Batch 332, LR 0.000018 Loss 4.741142, Accuracy 88.502%\n",
      "Epoch 29, Batch 333, LR 0.000018 Loss 4.740724, Accuracy 88.502%\n",
      "Epoch 29, Batch 334, LR 0.000018 Loss 4.740172, Accuracy 88.508%\n",
      "Epoch 29, Batch 335, LR 0.000018 Loss 4.738679, Accuracy 88.517%\n",
      "Epoch 29, Batch 336, LR 0.000018 Loss 4.737693, Accuracy 88.518%\n",
      "Epoch 29, Batch 337, LR 0.000018 Loss 4.735985, Accuracy 88.525%\n",
      "Epoch 29, Batch 338, LR 0.000018 Loss 4.735562, Accuracy 88.533%\n",
      "Epoch 29, Batch 339, LR 0.000018 Loss 4.733090, Accuracy 88.537%\n",
      "Epoch 29, Batch 340, LR 0.000018 Loss 4.735324, Accuracy 88.529%\n",
      "Epoch 29, Batch 341, LR 0.000018 Loss 4.735624, Accuracy 88.536%\n",
      "Epoch 29, Batch 342, LR 0.000018 Loss 4.736001, Accuracy 88.537%\n",
      "Epoch 29, Batch 343, LR 0.000018 Loss 4.737053, Accuracy 88.541%\n",
      "Epoch 29, Batch 344, LR 0.000018 Loss 4.737094, Accuracy 88.545%\n",
      "Epoch 29, Batch 345, LR 0.000018 Loss 4.735695, Accuracy 88.548%\n",
      "Epoch 29, Batch 346, LR 0.000018 Loss 4.736081, Accuracy 88.554%\n",
      "Epoch 29, Batch 347, LR 0.000018 Loss 4.734592, Accuracy 88.551%\n",
      "Epoch 29, Batch 348, LR 0.000018 Loss 4.736421, Accuracy 88.544%\n",
      "Epoch 29, Batch 349, LR 0.000018 Loss 4.735528, Accuracy 88.559%\n",
      "Epoch 29, Batch 350, LR 0.000018 Loss 4.734449, Accuracy 88.576%\n",
      "Epoch 29, Batch 351, LR 0.000018 Loss 4.734967, Accuracy 88.575%\n",
      "Epoch 29, Batch 352, LR 0.000018 Loss 4.735376, Accuracy 88.565%\n",
      "Epoch 29, Batch 353, LR 0.000018 Loss 4.735014, Accuracy 88.549%\n",
      "Epoch 29, Batch 354, LR 0.000018 Loss 4.734502, Accuracy 88.553%\n",
      "Epoch 29, Batch 355, LR 0.000018 Loss 4.732551, Accuracy 88.556%\n",
      "Epoch 29, Batch 356, LR 0.000018 Loss 4.733053, Accuracy 88.551%\n",
      "Epoch 29, Batch 357, LR 0.000018 Loss 4.733217, Accuracy 88.555%\n",
      "Epoch 29, Batch 358, LR 0.000018 Loss 4.732626, Accuracy 88.552%\n",
      "Epoch 29, Batch 359, LR 0.000018 Loss 4.734156, Accuracy 88.538%\n",
      "Epoch 29, Batch 360, LR 0.000018 Loss 4.734909, Accuracy 88.533%\n",
      "Epoch 29, Batch 361, LR 0.000018 Loss 4.734579, Accuracy 88.534%\n",
      "Epoch 29, Batch 362, LR 0.000018 Loss 4.734723, Accuracy 88.534%\n",
      "Epoch 29, Batch 363, LR 0.000018 Loss 4.734432, Accuracy 88.542%\n",
      "Epoch 29, Batch 364, LR 0.000018 Loss 4.736522, Accuracy 88.530%\n",
      "Epoch 29, Batch 365, LR 0.000018 Loss 4.737981, Accuracy 88.527%\n",
      "Epoch 29, Batch 366, LR 0.000018 Loss 4.739939, Accuracy 88.522%\n",
      "Epoch 29, Batch 367, LR 0.000018 Loss 4.738779, Accuracy 88.526%\n",
      "Epoch 29, Batch 368, LR 0.000018 Loss 4.738197, Accuracy 88.530%\n",
      "Epoch 29, Batch 369, LR 0.000018 Loss 4.738283, Accuracy 88.525%\n",
      "Epoch 29, Batch 370, LR 0.000018 Loss 4.736235, Accuracy 88.535%\n",
      "Epoch 29, Batch 371, LR 0.000018 Loss 4.736775, Accuracy 88.540%\n",
      "Epoch 29, Batch 372, LR 0.000018 Loss 4.735287, Accuracy 88.552%\n",
      "Epoch 29, Batch 373, LR 0.000018 Loss 4.733779, Accuracy 88.558%\n",
      "Epoch 29, Batch 374, LR 0.000018 Loss 4.732615, Accuracy 88.565%\n",
      "Epoch 29, Batch 375, LR 0.000018 Loss 4.731695, Accuracy 88.569%\n",
      "Epoch 29, Batch 376, LR 0.000018 Loss 4.731455, Accuracy 88.572%\n",
      "Epoch 29, Batch 377, LR 0.000018 Loss 4.730115, Accuracy 88.582%\n",
      "Epoch 29, Batch 378, LR 0.000018 Loss 4.730989, Accuracy 88.575%\n",
      "Epoch 29, Batch 379, LR 0.000018 Loss 4.731631, Accuracy 88.572%\n",
      "Epoch 29, Batch 380, LR 0.000018 Loss 4.731777, Accuracy 88.573%\n",
      "Epoch 29, Batch 381, LR 0.000018 Loss 4.731905, Accuracy 88.577%\n",
      "Epoch 29, Batch 382, LR 0.000018 Loss 4.734094, Accuracy 88.572%\n",
      "Epoch 29, Batch 383, LR 0.000018 Loss 4.733671, Accuracy 88.577%\n",
      "Epoch 29, Batch 384, LR 0.000018 Loss 4.733013, Accuracy 88.580%\n",
      "Epoch 29, Batch 385, LR 0.000018 Loss 4.733530, Accuracy 88.575%\n",
      "Epoch 29, Batch 386, LR 0.000018 Loss 4.734596, Accuracy 88.579%\n",
      "Epoch 29, Batch 387, LR 0.000018 Loss 4.735211, Accuracy 88.576%\n",
      "Epoch 29, Batch 388, LR 0.000018 Loss 4.734690, Accuracy 88.573%\n",
      "Epoch 29, Batch 389, LR 0.000018 Loss 4.735469, Accuracy 88.566%\n",
      "Epoch 29, Batch 390, LR 0.000018 Loss 4.734295, Accuracy 88.566%\n",
      "Epoch 29, Batch 391, LR 0.000018 Loss 4.734227, Accuracy 88.569%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 392, LR 0.000018 Loss 4.735078, Accuracy 88.574%\n",
      "Epoch 29, Batch 393, LR 0.000018 Loss 4.735693, Accuracy 88.577%\n",
      "Epoch 29, Batch 394, LR 0.000018 Loss 4.734017, Accuracy 88.583%\n",
      "Epoch 29, Batch 395, LR 0.000018 Loss 4.733115, Accuracy 88.584%\n",
      "Epoch 29, Batch 396, LR 0.000018 Loss 4.733962, Accuracy 88.575%\n",
      "Epoch 29, Batch 397, LR 0.000018 Loss 4.733645, Accuracy 88.572%\n",
      "Epoch 29, Batch 398, LR 0.000018 Loss 4.731655, Accuracy 88.584%\n",
      "Epoch 29, Batch 399, LR 0.000018 Loss 4.730793, Accuracy 88.596%\n",
      "Epoch 29, Batch 400, LR 0.000018 Loss 4.729357, Accuracy 88.605%\n",
      "Epoch 29, Batch 401, LR 0.000018 Loss 4.728960, Accuracy 88.603%\n",
      "Epoch 29, Batch 402, LR 0.000018 Loss 4.726987, Accuracy 88.616%\n",
      "Epoch 29, Batch 403, LR 0.000018 Loss 4.727330, Accuracy 88.619%\n",
      "Epoch 29, Batch 404, LR 0.000018 Loss 4.727621, Accuracy 88.610%\n",
      "Epoch 29, Batch 405, LR 0.000018 Loss 4.727563, Accuracy 88.605%\n",
      "Epoch 29, Batch 406, LR 0.000018 Loss 4.727792, Accuracy 88.605%\n",
      "Epoch 29, Batch 407, LR 0.000018 Loss 4.727078, Accuracy 88.609%\n",
      "Epoch 29, Batch 408, LR 0.000018 Loss 4.727296, Accuracy 88.611%\n",
      "Epoch 29, Batch 409, LR 0.000018 Loss 4.727023, Accuracy 88.608%\n",
      "Epoch 29, Batch 410, LR 0.000018 Loss 4.726940, Accuracy 88.613%\n",
      "Epoch 29, Batch 411, LR 0.000018 Loss 4.729264, Accuracy 88.601%\n",
      "Epoch 29, Batch 412, LR 0.000018 Loss 4.730661, Accuracy 88.588%\n",
      "Epoch 29, Batch 413, LR 0.000018 Loss 4.730313, Accuracy 88.582%\n",
      "Epoch 29, Batch 414, LR 0.000018 Loss 4.730396, Accuracy 88.579%\n",
      "Epoch 29, Batch 415, LR 0.000018 Loss 4.730858, Accuracy 88.579%\n",
      "Epoch 29, Batch 416, LR 0.000018 Loss 4.730386, Accuracy 88.591%\n",
      "Epoch 29, Batch 417, LR 0.000018 Loss 4.731261, Accuracy 88.581%\n",
      "Epoch 29, Batch 418, LR 0.000018 Loss 4.730449, Accuracy 88.590%\n",
      "Epoch 29, Batch 419, LR 0.000018 Loss 4.731509, Accuracy 88.580%\n",
      "Epoch 29, Batch 420, LR 0.000018 Loss 4.733452, Accuracy 88.568%\n",
      "Epoch 29, Batch 421, LR 0.000018 Loss 4.733370, Accuracy 88.571%\n",
      "Epoch 29, Batch 422, LR 0.000018 Loss 4.731474, Accuracy 88.579%\n",
      "Epoch 29, Batch 423, LR 0.000018 Loss 4.731220, Accuracy 88.577%\n",
      "Epoch 29, Batch 424, LR 0.000018 Loss 4.731470, Accuracy 88.574%\n",
      "Epoch 29, Batch 425, LR 0.000018 Loss 4.731066, Accuracy 88.581%\n",
      "Epoch 29, Batch 426, LR 0.000018 Loss 4.731845, Accuracy 88.571%\n",
      "Epoch 29, Batch 427, LR 0.000018 Loss 4.732181, Accuracy 88.569%\n",
      "Epoch 29, Batch 428, LR 0.000018 Loss 4.732142, Accuracy 88.573%\n",
      "Epoch 29, Batch 429, LR 0.000018 Loss 4.732082, Accuracy 88.573%\n",
      "Epoch 29, Batch 430, LR 0.000018 Loss 4.732195, Accuracy 88.572%\n",
      "Epoch 29, Batch 431, LR 0.000018 Loss 4.732291, Accuracy 88.568%\n",
      "Epoch 29, Batch 432, LR 0.000018 Loss 4.734150, Accuracy 88.563%\n",
      "Epoch 29, Batch 433, LR 0.000018 Loss 4.734427, Accuracy 88.559%\n",
      "Epoch 29, Batch 434, LR 0.000018 Loss 4.733498, Accuracy 88.562%\n",
      "Epoch 29, Batch 435, LR 0.000018 Loss 4.730816, Accuracy 88.569%\n",
      "Epoch 29, Batch 436, LR 0.000018 Loss 4.730111, Accuracy 88.573%\n",
      "Epoch 29, Batch 437, LR 0.000018 Loss 4.728414, Accuracy 88.580%\n",
      "Epoch 29, Batch 438, LR 0.000018 Loss 4.728006, Accuracy 88.577%\n",
      "Epoch 29, Batch 439, LR 0.000018 Loss 4.726785, Accuracy 88.587%\n",
      "Epoch 29, Batch 440, LR 0.000018 Loss 4.726614, Accuracy 88.592%\n",
      "Epoch 29, Batch 441, LR 0.000018 Loss 4.727522, Accuracy 88.593%\n",
      "Epoch 29, Batch 442, LR 0.000018 Loss 4.728103, Accuracy 88.592%\n",
      "Epoch 29, Batch 443, LR 0.000018 Loss 4.729494, Accuracy 88.586%\n",
      "Epoch 29, Batch 444, LR 0.000018 Loss 4.729136, Accuracy 88.587%\n",
      "Epoch 29, Batch 445, LR 0.000018 Loss 4.730483, Accuracy 88.580%\n",
      "Epoch 29, Batch 446, LR 0.000018 Loss 4.728573, Accuracy 88.591%\n",
      "Epoch 29, Batch 447, LR 0.000018 Loss 4.729873, Accuracy 88.594%\n",
      "Epoch 29, Batch 448, LR 0.000018 Loss 4.727844, Accuracy 88.599%\n",
      "Epoch 29, Batch 449, LR 0.000018 Loss 4.728085, Accuracy 88.600%\n",
      "Epoch 29, Batch 450, LR 0.000018 Loss 4.726395, Accuracy 88.608%\n",
      "Epoch 29, Batch 451, LR 0.000018 Loss 4.724430, Accuracy 88.612%\n",
      "Epoch 29, Batch 452, LR 0.000018 Loss 4.725382, Accuracy 88.610%\n",
      "Epoch 29, Batch 453, LR 0.000018 Loss 4.725955, Accuracy 88.602%\n",
      "Epoch 29, Batch 454, LR 0.000018 Loss 4.725212, Accuracy 88.606%\n",
      "Epoch 29, Batch 455, LR 0.000018 Loss 4.724089, Accuracy 88.614%\n",
      "Epoch 29, Batch 456, LR 0.000018 Loss 4.725083, Accuracy 88.605%\n",
      "Epoch 29, Batch 457, LR 0.000018 Loss 4.725181, Accuracy 88.603%\n",
      "Epoch 29, Batch 458, LR 0.000018 Loss 4.725025, Accuracy 88.599%\n",
      "Epoch 29, Batch 459, LR 0.000018 Loss 4.725556, Accuracy 88.598%\n",
      "Epoch 29, Batch 460, LR 0.000018 Loss 4.724662, Accuracy 88.599%\n",
      "Epoch 29, Batch 461, LR 0.000018 Loss 4.723833, Accuracy 88.598%\n",
      "Epoch 29, Batch 462, LR 0.000018 Loss 4.726079, Accuracy 88.586%\n",
      "Epoch 29, Batch 463, LR 0.000018 Loss 4.725900, Accuracy 88.590%\n",
      "Epoch 29, Batch 464, LR 0.000018 Loss 4.725836, Accuracy 88.588%\n",
      "Epoch 29, Batch 465, LR 0.000018 Loss 4.726063, Accuracy 88.587%\n",
      "Epoch 29, Batch 466, LR 0.000018 Loss 4.725778, Accuracy 88.600%\n",
      "Epoch 29, Batch 467, LR 0.000018 Loss 4.724259, Accuracy 88.604%\n",
      "Epoch 29, Batch 468, LR 0.000018 Loss 4.724872, Accuracy 88.598%\n",
      "Epoch 29, Batch 469, LR 0.000018 Loss 4.725953, Accuracy 88.591%\n",
      "Epoch 29, Batch 470, LR 0.000018 Loss 4.727370, Accuracy 88.589%\n",
      "Epoch 29, Batch 471, LR 0.000018 Loss 4.728539, Accuracy 88.575%\n",
      "Epoch 29, Batch 472, LR 0.000018 Loss 4.728040, Accuracy 88.579%\n",
      "Epoch 29, Batch 473, LR 0.000018 Loss 4.728111, Accuracy 88.574%\n",
      "Epoch 29, Batch 474, LR 0.000018 Loss 4.728754, Accuracy 88.568%\n",
      "Epoch 29, Batch 475, LR 0.000018 Loss 4.728750, Accuracy 88.564%\n",
      "Epoch 29, Batch 476, LR 0.000018 Loss 4.728564, Accuracy 88.562%\n",
      "Epoch 29, Batch 477, LR 0.000018 Loss 4.728416, Accuracy 88.560%\n",
      "Epoch 29, Batch 478, LR 0.000018 Loss 4.730279, Accuracy 88.556%\n",
      "Epoch 29, Batch 479, LR 0.000018 Loss 4.729751, Accuracy 88.562%\n",
      "Epoch 29, Batch 480, LR 0.000018 Loss 4.728575, Accuracy 88.571%\n",
      "Epoch 29, Batch 481, LR 0.000018 Loss 4.728948, Accuracy 88.565%\n",
      "Epoch 29, Batch 482, LR 0.000018 Loss 4.728125, Accuracy 88.573%\n",
      "Epoch 29, Batch 483, LR 0.000018 Loss 4.729345, Accuracy 88.569%\n",
      "Epoch 29, Batch 484, LR 0.000018 Loss 4.729685, Accuracy 88.567%\n",
      "Epoch 29, Batch 485, LR 0.000018 Loss 4.728939, Accuracy 88.573%\n",
      "Epoch 29, Batch 486, LR 0.000018 Loss 4.730229, Accuracy 88.561%\n",
      "Epoch 29, Batch 487, LR 0.000018 Loss 4.730520, Accuracy 88.556%\n",
      "Epoch 29, Batch 488, LR 0.000018 Loss 4.729289, Accuracy 88.553%\n",
      "Epoch 29, Batch 489, LR 0.000018 Loss 4.729497, Accuracy 88.548%\n",
      "Epoch 29, Batch 490, LR 0.000018 Loss 4.728261, Accuracy 88.554%\n",
      "Epoch 29, Batch 491, LR 0.000018 Loss 4.728168, Accuracy 88.552%\n",
      "Epoch 29, Batch 492, LR 0.000018 Loss 4.728611, Accuracy 88.556%\n",
      "Epoch 29, Batch 493, LR 0.000018 Loss 4.727257, Accuracy 88.559%\n",
      "Epoch 29, Batch 494, LR 0.000018 Loss 4.728269, Accuracy 88.550%\n",
      "Epoch 29, Batch 495, LR 0.000018 Loss 4.728473, Accuracy 88.546%\n",
      "Epoch 29, Batch 496, LR 0.000018 Loss 4.729417, Accuracy 88.543%\n",
      "Epoch 29, Batch 497, LR 0.000018 Loss 4.729783, Accuracy 88.547%\n",
      "Epoch 29, Batch 498, LR 0.000018 Loss 4.731138, Accuracy 88.545%\n",
      "Epoch 29, Batch 499, LR 0.000018 Loss 4.732681, Accuracy 88.541%\n",
      "Epoch 29, Batch 500, LR 0.000018 Loss 4.733764, Accuracy 88.533%\n",
      "Epoch 29, Batch 501, LR 0.000018 Loss 4.735053, Accuracy 88.529%\n",
      "Epoch 29, Batch 502, LR 0.000018 Loss 4.735206, Accuracy 88.533%\n",
      "Epoch 29, Batch 503, LR 0.000018 Loss 4.736063, Accuracy 88.528%\n",
      "Epoch 29, Batch 504, LR 0.000018 Loss 4.736374, Accuracy 88.523%\n",
      "Epoch 29, Batch 505, LR 0.000018 Loss 4.735979, Accuracy 88.529%\n",
      "Epoch 29, Batch 506, LR 0.000018 Loss 4.734670, Accuracy 88.531%\n",
      "Epoch 29, Batch 507, LR 0.000018 Loss 4.734622, Accuracy 88.528%\n",
      "Epoch 29, Batch 508, LR 0.000018 Loss 4.733994, Accuracy 88.537%\n",
      "Epoch 29, Batch 509, LR 0.000018 Loss 4.733956, Accuracy 88.541%\n",
      "Epoch 29, Batch 510, LR 0.000018 Loss 4.733666, Accuracy 88.543%\n",
      "Epoch 29, Batch 511, LR 0.000018 Loss 4.731979, Accuracy 88.552%\n",
      "Epoch 29, Batch 512, LR 0.000018 Loss 4.731054, Accuracy 88.560%\n",
      "Epoch 29, Batch 513, LR 0.000018 Loss 4.730655, Accuracy 88.563%\n",
      "Epoch 29, Batch 514, LR 0.000018 Loss 4.731184, Accuracy 88.561%\n",
      "Epoch 29, Batch 515, LR 0.000018 Loss 4.730017, Accuracy 88.566%\n",
      "Epoch 29, Batch 516, LR 0.000018 Loss 4.730703, Accuracy 88.558%\n",
      "Epoch 29, Batch 517, LR 0.000018 Loss 4.730421, Accuracy 88.555%\n",
      "Epoch 29, Batch 518, LR 0.000018 Loss 4.730811, Accuracy 88.554%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 519, LR 0.000018 Loss 4.731089, Accuracy 88.551%\n",
      "Epoch 29, Batch 520, LR 0.000018 Loss 4.732555, Accuracy 88.537%\n",
      "Epoch 29, Batch 521, LR 0.000018 Loss 4.732288, Accuracy 88.541%\n",
      "Epoch 29, Batch 522, LR 0.000018 Loss 4.732375, Accuracy 88.537%\n",
      "Epoch 29, Batch 523, LR 0.000018 Loss 4.733034, Accuracy 88.535%\n",
      "Epoch 29, Batch 524, LR 0.000018 Loss 4.732947, Accuracy 88.538%\n",
      "Epoch 29, Batch 525, LR 0.000018 Loss 4.732624, Accuracy 88.539%\n",
      "Epoch 29, Batch 526, LR 0.000018 Loss 4.733004, Accuracy 88.532%\n",
      "Epoch 29, Batch 527, LR 0.000018 Loss 4.733462, Accuracy 88.529%\n",
      "Epoch 29, Batch 528, LR 0.000018 Loss 4.731942, Accuracy 88.536%\n",
      "Epoch 29, Batch 529, LR 0.000018 Loss 4.731971, Accuracy 88.531%\n",
      "Epoch 29, Batch 530, LR 0.000018 Loss 4.731661, Accuracy 88.526%\n",
      "Epoch 29, Batch 531, LR 0.000018 Loss 4.731494, Accuracy 88.524%\n",
      "Epoch 29, Batch 532, LR 0.000018 Loss 4.731423, Accuracy 88.526%\n",
      "Epoch 29, Batch 533, LR 0.000018 Loss 4.732563, Accuracy 88.525%\n",
      "Epoch 29, Batch 534, LR 0.000018 Loss 4.732929, Accuracy 88.520%\n",
      "Epoch 29, Batch 535, LR 0.000018 Loss 4.734033, Accuracy 88.513%\n",
      "Epoch 29, Batch 536, LR 0.000018 Loss 4.733618, Accuracy 88.516%\n",
      "Epoch 29, Batch 537, LR 0.000018 Loss 4.733776, Accuracy 88.520%\n",
      "Epoch 29, Batch 538, LR 0.000018 Loss 4.733935, Accuracy 88.527%\n",
      "Epoch 29, Batch 539, LR 0.000018 Loss 4.735008, Accuracy 88.523%\n",
      "Epoch 29, Batch 540, LR 0.000018 Loss 4.735167, Accuracy 88.521%\n",
      "Epoch 29, Batch 541, LR 0.000018 Loss 4.734298, Accuracy 88.521%\n",
      "Epoch 29, Batch 542, LR 0.000018 Loss 4.735525, Accuracy 88.519%\n",
      "Epoch 29, Batch 543, LR 0.000018 Loss 4.735787, Accuracy 88.519%\n",
      "Epoch 29, Batch 544, LR 0.000018 Loss 4.735881, Accuracy 88.515%\n",
      "Epoch 29, Batch 545, LR 0.000018 Loss 4.735250, Accuracy 88.524%\n",
      "Epoch 29, Batch 546, LR 0.000018 Loss 4.735516, Accuracy 88.517%\n",
      "Epoch 29, Batch 547, LR 0.000018 Loss 4.735708, Accuracy 88.517%\n",
      "Epoch 29, Batch 548, LR 0.000018 Loss 4.736767, Accuracy 88.512%\n",
      "Epoch 29, Batch 549, LR 0.000018 Loss 4.736555, Accuracy 88.512%\n",
      "Epoch 29, Batch 550, LR 0.000018 Loss 4.735607, Accuracy 88.523%\n",
      "Epoch 29, Batch 551, LR 0.000018 Loss 4.735381, Accuracy 88.525%\n",
      "Epoch 29, Batch 552, LR 0.000018 Loss 4.735810, Accuracy 88.515%\n",
      "Epoch 29, Batch 553, LR 0.000018 Loss 4.735469, Accuracy 88.517%\n",
      "Epoch 29, Batch 554, LR 0.000018 Loss 4.734944, Accuracy 88.515%\n",
      "Epoch 29, Batch 555, LR 0.000018 Loss 4.734906, Accuracy 88.511%\n",
      "Epoch 29, Batch 556, LR 0.000018 Loss 4.735052, Accuracy 88.512%\n",
      "Epoch 29, Batch 557, LR 0.000018 Loss 4.735168, Accuracy 88.511%\n",
      "Epoch 29, Batch 558, LR 0.000018 Loss 4.736317, Accuracy 88.501%\n",
      "Epoch 29, Batch 559, LR 0.000018 Loss 4.736871, Accuracy 88.496%\n",
      "Epoch 29, Batch 560, LR 0.000018 Loss 4.735896, Accuracy 88.500%\n",
      "Epoch 29, Batch 561, LR 0.000018 Loss 4.736810, Accuracy 88.496%\n",
      "Epoch 29, Batch 562, LR 0.000018 Loss 4.737235, Accuracy 88.487%\n",
      "Epoch 29, Batch 563, LR 0.000018 Loss 4.736627, Accuracy 88.488%\n",
      "Epoch 29, Batch 564, LR 0.000018 Loss 4.735923, Accuracy 88.486%\n",
      "Epoch 29, Batch 565, LR 0.000018 Loss 4.735480, Accuracy 88.489%\n",
      "Epoch 29, Batch 566, LR 0.000018 Loss 4.735659, Accuracy 88.491%\n",
      "Epoch 29, Batch 567, LR 0.000018 Loss 4.736493, Accuracy 88.481%\n",
      "Epoch 29, Batch 568, LR 0.000018 Loss 4.738571, Accuracy 88.471%\n",
      "Epoch 29, Batch 569, LR 0.000018 Loss 4.738696, Accuracy 88.468%\n",
      "Epoch 29, Batch 570, LR 0.000018 Loss 4.738511, Accuracy 88.470%\n",
      "Epoch 29, Batch 571, LR 0.000018 Loss 4.738323, Accuracy 88.471%\n",
      "Epoch 29, Batch 572, LR 0.000018 Loss 4.736574, Accuracy 88.483%\n",
      "Epoch 29, Batch 573, LR 0.000018 Loss 4.736531, Accuracy 88.482%\n",
      "Epoch 29, Batch 574, LR 0.000018 Loss 4.735704, Accuracy 88.484%\n",
      "Epoch 29, Batch 575, LR 0.000018 Loss 4.736045, Accuracy 88.484%\n",
      "Epoch 29, Batch 576, LR 0.000018 Loss 4.736305, Accuracy 88.482%\n",
      "Epoch 29, Batch 577, LR 0.000018 Loss 4.735142, Accuracy 88.486%\n",
      "Epoch 29, Batch 578, LR 0.000018 Loss 4.735879, Accuracy 88.481%\n",
      "Epoch 29, Batch 579, LR 0.000018 Loss 4.734661, Accuracy 88.492%\n",
      "Epoch 29, Batch 580, LR 0.000018 Loss 4.735537, Accuracy 88.494%\n",
      "Epoch 29, Batch 581, LR 0.000018 Loss 4.735483, Accuracy 88.492%\n",
      "Epoch 29, Batch 582, LR 0.000018 Loss 4.737045, Accuracy 88.484%\n",
      "Epoch 29, Batch 583, LR 0.000018 Loss 4.737563, Accuracy 88.480%\n",
      "Epoch 29, Batch 584, LR 0.000018 Loss 4.736409, Accuracy 88.486%\n",
      "Epoch 29, Batch 585, LR 0.000018 Loss 4.736474, Accuracy 88.490%\n",
      "Epoch 29, Batch 586, LR 0.000017 Loss 4.736722, Accuracy 88.492%\n",
      "Epoch 29, Batch 587, LR 0.000017 Loss 4.735745, Accuracy 88.492%\n",
      "Epoch 29, Batch 588, LR 0.000017 Loss 4.736177, Accuracy 88.490%\n",
      "Epoch 29, Batch 589, LR 0.000017 Loss 4.736572, Accuracy 88.488%\n",
      "Epoch 29, Batch 590, LR 0.000017 Loss 4.735699, Accuracy 88.493%\n",
      "Epoch 29, Batch 591, LR 0.000017 Loss 4.734588, Accuracy 88.498%\n",
      "Epoch 29, Batch 592, LR 0.000017 Loss 4.735385, Accuracy 88.496%\n",
      "Epoch 29, Batch 593, LR 0.000017 Loss 4.735227, Accuracy 88.495%\n",
      "Epoch 29, Batch 594, LR 0.000017 Loss 4.735491, Accuracy 88.496%\n",
      "Epoch 29, Batch 595, LR 0.000017 Loss 4.736263, Accuracy 88.493%\n",
      "Epoch 29, Batch 596, LR 0.000017 Loss 4.736592, Accuracy 88.494%\n",
      "Epoch 29, Batch 597, LR 0.000017 Loss 4.736671, Accuracy 88.496%\n",
      "Epoch 29, Batch 598, LR 0.000017 Loss 4.736294, Accuracy 88.499%\n",
      "Epoch 29, Batch 599, LR 0.000017 Loss 4.737169, Accuracy 88.493%\n",
      "Epoch 29, Batch 600, LR 0.000017 Loss 4.736858, Accuracy 88.496%\n",
      "Epoch 29, Batch 601, LR 0.000017 Loss 4.735964, Accuracy 88.501%\n",
      "Epoch 29, Batch 602, LR 0.000017 Loss 4.736188, Accuracy 88.499%\n",
      "Epoch 29, Batch 603, LR 0.000017 Loss 4.736690, Accuracy 88.495%\n",
      "Epoch 29, Batch 604, LR 0.000017 Loss 4.736117, Accuracy 88.500%\n",
      "Epoch 29, Batch 605, LR 0.000017 Loss 4.736805, Accuracy 88.493%\n",
      "Epoch 29, Batch 606, LR 0.000017 Loss 4.737156, Accuracy 88.494%\n",
      "Epoch 29, Batch 607, LR 0.000017 Loss 4.737581, Accuracy 88.487%\n",
      "Epoch 29, Batch 608, LR 0.000017 Loss 4.736700, Accuracy 88.496%\n",
      "Epoch 29, Batch 609, LR 0.000017 Loss 4.736946, Accuracy 88.495%\n",
      "Epoch 29, Batch 610, LR 0.000017 Loss 4.736833, Accuracy 88.496%\n",
      "Epoch 29, Batch 611, LR 0.000017 Loss 4.737395, Accuracy 88.496%\n",
      "Epoch 29, Batch 612, LR 0.000017 Loss 4.737220, Accuracy 88.493%\n",
      "Epoch 29, Batch 613, LR 0.000017 Loss 4.736865, Accuracy 88.492%\n",
      "Epoch 29, Batch 614, LR 0.000017 Loss 4.735974, Accuracy 88.494%\n",
      "Epoch 29, Batch 615, LR 0.000017 Loss 4.736233, Accuracy 88.497%\n",
      "Epoch 29, Batch 616, LR 0.000017 Loss 4.736581, Accuracy 88.499%\n",
      "Epoch 29, Batch 617, LR 0.000017 Loss 4.737436, Accuracy 88.497%\n",
      "Epoch 29, Batch 618, LR 0.000017 Loss 4.736569, Accuracy 88.504%\n",
      "Epoch 29, Batch 619, LR 0.000017 Loss 4.735488, Accuracy 88.508%\n",
      "Epoch 29, Batch 620, LR 0.000017 Loss 4.734003, Accuracy 88.517%\n",
      "Epoch 29, Batch 621, LR 0.000017 Loss 4.734634, Accuracy 88.515%\n",
      "Epoch 29, Batch 622, LR 0.000017 Loss 4.736443, Accuracy 88.510%\n",
      "Epoch 29, Batch 623, LR 0.000017 Loss 4.738534, Accuracy 88.496%\n",
      "Epoch 29, Batch 624, LR 0.000017 Loss 4.737885, Accuracy 88.495%\n",
      "Epoch 29, Batch 625, LR 0.000017 Loss 4.737285, Accuracy 88.500%\n",
      "Epoch 29, Batch 626, LR 0.000017 Loss 4.736456, Accuracy 88.507%\n",
      "Epoch 29, Batch 627, LR 0.000017 Loss 4.736982, Accuracy 88.507%\n",
      "Epoch 29, Batch 628, LR 0.000017 Loss 4.736051, Accuracy 88.511%\n",
      "Epoch 29, Batch 629, LR 0.000017 Loss 4.735989, Accuracy 88.515%\n",
      "Epoch 29, Batch 630, LR 0.000017 Loss 4.735950, Accuracy 88.519%\n",
      "Epoch 29, Batch 631, LR 0.000017 Loss 4.735558, Accuracy 88.525%\n",
      "Epoch 29, Batch 632, LR 0.000017 Loss 4.734881, Accuracy 88.532%\n",
      "Epoch 29, Batch 633, LR 0.000017 Loss 4.734677, Accuracy 88.527%\n",
      "Epoch 29, Batch 634, LR 0.000017 Loss 4.735325, Accuracy 88.526%\n",
      "Epoch 29, Batch 635, LR 0.000017 Loss 4.735440, Accuracy 88.526%\n",
      "Epoch 29, Batch 636, LR 0.000017 Loss 4.734887, Accuracy 88.529%\n",
      "Epoch 29, Batch 637, LR 0.000017 Loss 4.735268, Accuracy 88.523%\n",
      "Epoch 29, Batch 638, LR 0.000017 Loss 4.735098, Accuracy 88.522%\n",
      "Epoch 29, Batch 639, LR 0.000017 Loss 4.733674, Accuracy 88.526%\n",
      "Epoch 29, Batch 640, LR 0.000017 Loss 4.733013, Accuracy 88.525%\n",
      "Epoch 29, Batch 641, LR 0.000017 Loss 4.732950, Accuracy 88.529%\n",
      "Epoch 29, Batch 642, LR 0.000017 Loss 4.732460, Accuracy 88.534%\n",
      "Epoch 29, Batch 643, LR 0.000017 Loss 4.734109, Accuracy 88.518%\n",
      "Epoch 29, Batch 644, LR 0.000017 Loss 4.735217, Accuracy 88.514%\n",
      "Epoch 29, Batch 645, LR 0.000017 Loss 4.734503, Accuracy 88.517%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 646, LR 0.000017 Loss 4.733824, Accuracy 88.519%\n",
      "Epoch 29, Batch 647, LR 0.000017 Loss 4.734696, Accuracy 88.518%\n",
      "Epoch 29, Batch 648, LR 0.000017 Loss 4.734508, Accuracy 88.516%\n",
      "Epoch 29, Batch 649, LR 0.000017 Loss 4.734368, Accuracy 88.514%\n",
      "Epoch 29, Batch 650, LR 0.000017 Loss 4.735210, Accuracy 88.510%\n",
      "Epoch 29, Batch 651, LR 0.000017 Loss 4.735943, Accuracy 88.506%\n",
      "Epoch 29, Batch 652, LR 0.000017 Loss 4.735950, Accuracy 88.508%\n",
      "Epoch 29, Batch 653, LR 0.000017 Loss 4.736322, Accuracy 88.506%\n",
      "Epoch 29, Batch 654, LR 0.000017 Loss 4.735818, Accuracy 88.508%\n",
      "Epoch 29, Batch 655, LR 0.000017 Loss 4.736918, Accuracy 88.504%\n",
      "Epoch 29, Batch 656, LR 0.000017 Loss 4.737730, Accuracy 88.498%\n",
      "Epoch 29, Batch 657, LR 0.000017 Loss 4.737906, Accuracy 88.492%\n",
      "Epoch 29, Batch 658, LR 0.000017 Loss 4.738510, Accuracy 88.490%\n",
      "Epoch 29, Batch 659, LR 0.000017 Loss 4.738993, Accuracy 88.486%\n",
      "Epoch 29, Batch 660, LR 0.000017 Loss 4.738675, Accuracy 88.487%\n",
      "Epoch 29, Batch 661, LR 0.000017 Loss 4.738523, Accuracy 88.482%\n",
      "Epoch 29, Batch 662, LR 0.000017 Loss 4.738260, Accuracy 88.482%\n",
      "Epoch 29, Batch 663, LR 0.000017 Loss 4.738797, Accuracy 88.482%\n",
      "Epoch 29, Batch 664, LR 0.000017 Loss 4.738917, Accuracy 88.482%\n",
      "Epoch 29, Batch 665, LR 0.000017 Loss 4.739261, Accuracy 88.484%\n",
      "Epoch 29, Batch 666, LR 0.000017 Loss 4.739816, Accuracy 88.491%\n",
      "Epoch 29, Batch 667, LR 0.000017 Loss 4.740415, Accuracy 88.485%\n",
      "Epoch 29, Batch 668, LR 0.000017 Loss 4.740602, Accuracy 88.489%\n",
      "Epoch 29, Batch 669, LR 0.000017 Loss 4.741383, Accuracy 88.486%\n",
      "Epoch 29, Batch 670, LR 0.000017 Loss 4.741446, Accuracy 88.485%\n",
      "Epoch 29, Batch 671, LR 0.000017 Loss 4.741637, Accuracy 88.482%\n",
      "Epoch 29, Batch 672, LR 0.000017 Loss 4.740337, Accuracy 88.487%\n",
      "Epoch 29, Batch 673, LR 0.000017 Loss 4.740586, Accuracy 88.484%\n",
      "Epoch 29, Batch 674, LR 0.000017 Loss 4.740697, Accuracy 88.484%\n",
      "Epoch 29, Batch 675, LR 0.000017 Loss 4.740623, Accuracy 88.485%\n",
      "Epoch 29, Batch 676, LR 0.000017 Loss 4.740984, Accuracy 88.488%\n",
      "Epoch 29, Batch 677, LR 0.000017 Loss 4.742202, Accuracy 88.484%\n",
      "Epoch 29, Batch 678, LR 0.000017 Loss 4.741984, Accuracy 88.486%\n",
      "Epoch 29, Batch 679, LR 0.000017 Loss 4.743379, Accuracy 88.487%\n",
      "Epoch 29, Batch 680, LR 0.000017 Loss 4.744177, Accuracy 88.483%\n",
      "Epoch 29, Batch 681, LR 0.000017 Loss 4.743739, Accuracy 88.485%\n",
      "Epoch 29, Batch 682, LR 0.000017 Loss 4.743977, Accuracy 88.484%\n",
      "Epoch 29, Batch 683, LR 0.000017 Loss 4.743917, Accuracy 88.486%\n",
      "Epoch 29, Batch 684, LR 0.000017 Loss 4.744961, Accuracy 88.481%\n",
      "Epoch 29, Batch 685, LR 0.000017 Loss 4.744931, Accuracy 88.485%\n",
      "Epoch 29, Batch 686, LR 0.000017 Loss 4.745107, Accuracy 88.483%\n",
      "Epoch 29, Batch 687, LR 0.000017 Loss 4.745233, Accuracy 88.483%\n",
      "Epoch 29, Batch 688, LR 0.000017 Loss 4.745172, Accuracy 88.482%\n",
      "Epoch 29, Batch 689, LR 0.000017 Loss 4.744573, Accuracy 88.485%\n",
      "Epoch 29, Batch 690, LR 0.000017 Loss 4.743510, Accuracy 88.492%\n",
      "Epoch 29, Batch 691, LR 0.000017 Loss 4.743785, Accuracy 88.488%\n",
      "Epoch 29, Batch 692, LR 0.000017 Loss 4.743572, Accuracy 88.490%\n",
      "Epoch 29, Batch 693, LR 0.000017 Loss 4.744021, Accuracy 88.485%\n",
      "Epoch 29, Batch 694, LR 0.000017 Loss 4.743880, Accuracy 88.486%\n",
      "Epoch 29, Batch 695, LR 0.000017 Loss 4.744070, Accuracy 88.484%\n",
      "Epoch 29, Batch 696, LR 0.000017 Loss 4.744404, Accuracy 88.481%\n",
      "Epoch 29, Batch 697, LR 0.000017 Loss 4.744608, Accuracy 88.481%\n",
      "Epoch 29, Batch 698, LR 0.000017 Loss 4.745559, Accuracy 88.476%\n",
      "Epoch 29, Batch 699, LR 0.000017 Loss 4.745598, Accuracy 88.481%\n",
      "Epoch 29, Batch 700, LR 0.000017 Loss 4.746193, Accuracy 88.480%\n",
      "Epoch 29, Batch 701, LR 0.000017 Loss 4.746689, Accuracy 88.479%\n",
      "Epoch 29, Batch 702, LR 0.000017 Loss 4.746232, Accuracy 88.477%\n",
      "Epoch 29, Batch 703, LR 0.000017 Loss 4.745642, Accuracy 88.478%\n",
      "Epoch 29, Batch 704, LR 0.000017 Loss 4.745243, Accuracy 88.475%\n",
      "Epoch 29, Batch 705, LR 0.000017 Loss 4.745785, Accuracy 88.472%\n",
      "Epoch 29, Batch 706, LR 0.000017 Loss 4.744458, Accuracy 88.478%\n",
      "Epoch 29, Batch 707, LR 0.000017 Loss 4.743805, Accuracy 88.475%\n",
      "Epoch 29, Batch 708, LR 0.000017 Loss 4.743074, Accuracy 88.475%\n",
      "Epoch 29, Batch 709, LR 0.000017 Loss 4.743652, Accuracy 88.475%\n",
      "Epoch 29, Batch 710, LR 0.000017 Loss 4.743191, Accuracy 88.476%\n",
      "Epoch 29, Batch 711, LR 0.000017 Loss 4.743317, Accuracy 88.475%\n",
      "Epoch 29, Batch 712, LR 0.000017 Loss 4.744257, Accuracy 88.478%\n",
      "Epoch 29, Batch 713, LR 0.000017 Loss 4.743557, Accuracy 88.482%\n",
      "Epoch 29, Batch 714, LR 0.000017 Loss 4.742929, Accuracy 88.485%\n",
      "Epoch 29, Batch 715, LR 0.000017 Loss 4.742989, Accuracy 88.482%\n",
      "Epoch 29, Batch 716, LR 0.000017 Loss 4.741683, Accuracy 88.490%\n",
      "Epoch 29, Batch 717, LR 0.000017 Loss 4.741036, Accuracy 88.489%\n",
      "Epoch 29, Batch 718, LR 0.000017 Loss 4.740809, Accuracy 88.489%\n",
      "Epoch 29, Batch 719, LR 0.000017 Loss 4.741274, Accuracy 88.492%\n",
      "Epoch 29, Batch 720, LR 0.000017 Loss 4.741039, Accuracy 88.495%\n",
      "Epoch 29, Batch 721, LR 0.000017 Loss 4.740873, Accuracy 88.491%\n",
      "Epoch 29, Batch 722, LR 0.000017 Loss 4.741177, Accuracy 88.491%\n",
      "Epoch 29, Batch 723, LR 0.000017 Loss 4.741014, Accuracy 88.485%\n",
      "Epoch 29, Batch 724, LR 0.000017 Loss 4.741679, Accuracy 88.484%\n",
      "Epoch 29, Batch 725, LR 0.000017 Loss 4.741807, Accuracy 88.486%\n",
      "Epoch 29, Batch 726, LR 0.000017 Loss 4.740865, Accuracy 88.489%\n",
      "Epoch 29, Batch 727, LR 0.000017 Loss 4.740721, Accuracy 88.494%\n",
      "Epoch 29, Batch 728, LR 0.000017 Loss 4.740922, Accuracy 88.493%\n",
      "Epoch 29, Batch 729, LR 0.000017 Loss 4.742842, Accuracy 88.483%\n",
      "Epoch 29, Batch 730, LR 0.000017 Loss 4.742813, Accuracy 88.484%\n",
      "Epoch 29, Batch 731, LR 0.000017 Loss 4.741899, Accuracy 88.489%\n",
      "Epoch 29, Batch 732, LR 0.000017 Loss 4.741704, Accuracy 88.496%\n",
      "Epoch 29, Batch 733, LR 0.000017 Loss 4.740863, Accuracy 88.499%\n",
      "Epoch 29, Batch 734, LR 0.000017 Loss 4.741395, Accuracy 88.494%\n",
      "Epoch 29, Batch 735, LR 0.000017 Loss 4.741530, Accuracy 88.490%\n",
      "Epoch 29, Batch 736, LR 0.000017 Loss 4.742596, Accuracy 88.486%\n",
      "Epoch 29, Batch 737, LR 0.000017 Loss 4.742533, Accuracy 88.491%\n",
      "Epoch 29, Batch 738, LR 0.000017 Loss 4.742296, Accuracy 88.490%\n",
      "Epoch 29, Batch 739, LR 0.000017 Loss 4.742216, Accuracy 88.490%\n",
      "Epoch 29, Batch 740, LR 0.000017 Loss 4.741646, Accuracy 88.493%\n",
      "Epoch 29, Batch 741, LR 0.000017 Loss 4.741229, Accuracy 88.493%\n",
      "Epoch 29, Batch 742, LR 0.000017 Loss 4.741443, Accuracy 88.498%\n",
      "Epoch 29, Batch 743, LR 0.000017 Loss 4.740770, Accuracy 88.498%\n",
      "Epoch 29, Batch 744, LR 0.000017 Loss 4.740638, Accuracy 88.500%\n",
      "Epoch 29, Batch 745, LR 0.000017 Loss 4.739731, Accuracy 88.508%\n",
      "Epoch 29, Batch 746, LR 0.000017 Loss 4.739447, Accuracy 88.507%\n",
      "Epoch 29, Batch 747, LR 0.000017 Loss 4.740487, Accuracy 88.503%\n",
      "Epoch 29, Batch 748, LR 0.000017 Loss 4.740094, Accuracy 88.504%\n",
      "Epoch 29, Batch 749, LR 0.000017 Loss 4.739669, Accuracy 88.511%\n",
      "Epoch 29, Batch 750, LR 0.000017 Loss 4.740506, Accuracy 88.513%\n",
      "Epoch 29, Batch 751, LR 0.000017 Loss 4.740123, Accuracy 88.517%\n",
      "Epoch 29, Batch 752, LR 0.000017 Loss 4.739676, Accuracy 88.517%\n",
      "Epoch 29, Batch 753, LR 0.000017 Loss 4.739622, Accuracy 88.514%\n",
      "Epoch 29, Batch 754, LR 0.000017 Loss 4.739987, Accuracy 88.511%\n",
      "Epoch 29, Batch 755, LR 0.000017 Loss 4.739157, Accuracy 88.512%\n",
      "Epoch 29, Batch 756, LR 0.000017 Loss 4.739268, Accuracy 88.507%\n",
      "Epoch 29, Batch 757, LR 0.000017 Loss 4.738649, Accuracy 88.506%\n",
      "Epoch 29, Batch 758, LR 0.000017 Loss 4.737908, Accuracy 88.511%\n",
      "Epoch 29, Batch 759, LR 0.000017 Loss 4.738018, Accuracy 88.510%\n",
      "Epoch 29, Batch 760, LR 0.000017 Loss 4.736762, Accuracy 88.517%\n",
      "Epoch 29, Batch 761, LR 0.000017 Loss 4.737185, Accuracy 88.518%\n",
      "Epoch 29, Batch 762, LR 0.000017 Loss 4.737281, Accuracy 88.518%\n",
      "Epoch 29, Batch 763, LR 0.000017 Loss 4.737462, Accuracy 88.518%\n",
      "Epoch 29, Batch 764, LR 0.000017 Loss 4.736904, Accuracy 88.517%\n",
      "Epoch 29, Batch 765, LR 0.000017 Loss 4.736765, Accuracy 88.516%\n",
      "Epoch 29, Batch 766, LR 0.000017 Loss 4.735364, Accuracy 88.517%\n",
      "Epoch 29, Batch 767, LR 0.000017 Loss 4.734848, Accuracy 88.515%\n",
      "Epoch 29, Batch 768, LR 0.000017 Loss 4.736320, Accuracy 88.503%\n",
      "Epoch 29, Batch 769, LR 0.000017 Loss 4.736814, Accuracy 88.502%\n",
      "Epoch 29, Batch 770, LR 0.000017 Loss 4.736258, Accuracy 88.501%\n",
      "Epoch 29, Batch 771, LR 0.000017 Loss 4.736585, Accuracy 88.504%\n",
      "Epoch 29, Batch 772, LR 0.000017 Loss 4.736867, Accuracy 88.505%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 773, LR 0.000017 Loss 4.738327, Accuracy 88.504%\n",
      "Epoch 29, Batch 774, LR 0.000017 Loss 4.738804, Accuracy 88.503%\n",
      "Epoch 29, Batch 775, LR 0.000017 Loss 4.738035, Accuracy 88.506%\n",
      "Epoch 29, Batch 776, LR 0.000017 Loss 4.738579, Accuracy 88.502%\n",
      "Epoch 29, Batch 777, LR 0.000017 Loss 4.738112, Accuracy 88.504%\n",
      "Epoch 29, Batch 778, LR 0.000017 Loss 4.738798, Accuracy 88.497%\n",
      "Epoch 29, Batch 779, LR 0.000017 Loss 4.738006, Accuracy 88.499%\n",
      "Epoch 29, Batch 780, LR 0.000017 Loss 4.737554, Accuracy 88.503%\n",
      "Epoch 29, Batch 781, LR 0.000017 Loss 4.737201, Accuracy 88.505%\n",
      "Epoch 29, Batch 782, LR 0.000017 Loss 4.737868, Accuracy 88.499%\n",
      "Epoch 29, Batch 783, LR 0.000017 Loss 4.736639, Accuracy 88.500%\n",
      "Epoch 29, Batch 784, LR 0.000017 Loss 4.736194, Accuracy 88.503%\n",
      "Epoch 29, Batch 785, LR 0.000017 Loss 4.734927, Accuracy 88.509%\n",
      "Epoch 29, Batch 786, LR 0.000017 Loss 4.734804, Accuracy 88.508%\n",
      "Epoch 29, Batch 787, LR 0.000017 Loss 4.734342, Accuracy 88.511%\n",
      "Epoch 29, Batch 788, LR 0.000017 Loss 4.735251, Accuracy 88.500%\n",
      "Epoch 29, Batch 789, LR 0.000017 Loss 4.736209, Accuracy 88.490%\n",
      "Epoch 29, Batch 790, LR 0.000017 Loss 4.735717, Accuracy 88.496%\n",
      "Epoch 29, Batch 791, LR 0.000017 Loss 4.735847, Accuracy 88.498%\n",
      "Epoch 29, Batch 792, LR 0.000017 Loss 4.735717, Accuracy 88.496%\n",
      "Epoch 29, Batch 793, LR 0.000017 Loss 4.736238, Accuracy 88.491%\n",
      "Epoch 29, Batch 794, LR 0.000017 Loss 4.736138, Accuracy 88.490%\n",
      "Epoch 29, Batch 795, LR 0.000017 Loss 4.736127, Accuracy 88.487%\n",
      "Epoch 29, Batch 796, LR 0.000017 Loss 4.736141, Accuracy 88.484%\n",
      "Epoch 29, Batch 797, LR 0.000017 Loss 4.736065, Accuracy 88.486%\n",
      "Epoch 29, Batch 798, LR 0.000017 Loss 4.735627, Accuracy 88.487%\n",
      "Epoch 29, Batch 799, LR 0.000017 Loss 4.734746, Accuracy 88.487%\n",
      "Epoch 29, Batch 800, LR 0.000017 Loss 4.734677, Accuracy 88.487%\n",
      "Epoch 29, Batch 801, LR 0.000017 Loss 4.734099, Accuracy 88.489%\n",
      "Epoch 29, Batch 802, LR 0.000017 Loss 4.734171, Accuracy 88.486%\n",
      "Epoch 29, Batch 803, LR 0.000017 Loss 4.734018, Accuracy 88.489%\n",
      "Epoch 29, Batch 804, LR 0.000017 Loss 4.734886, Accuracy 88.488%\n",
      "Epoch 29, Batch 805, LR 0.000017 Loss 4.734863, Accuracy 88.488%\n",
      "Epoch 29, Batch 806, LR 0.000017 Loss 4.735376, Accuracy 88.486%\n",
      "Epoch 29, Batch 807, LR 0.000017 Loss 4.735142, Accuracy 88.488%\n",
      "Epoch 29, Batch 808, LR 0.000017 Loss 4.734781, Accuracy 88.489%\n",
      "Epoch 29, Batch 809, LR 0.000017 Loss 4.734674, Accuracy 88.491%\n",
      "Epoch 29, Batch 810, LR 0.000017 Loss 4.734525, Accuracy 88.489%\n",
      "Epoch 29, Batch 811, LR 0.000017 Loss 4.735076, Accuracy 88.485%\n",
      "Epoch 29, Batch 812, LR 0.000017 Loss 4.735214, Accuracy 88.485%\n",
      "Epoch 29, Batch 813, LR 0.000017 Loss 4.735398, Accuracy 88.487%\n",
      "Epoch 29, Batch 814, LR 0.000017 Loss 4.734889, Accuracy 88.490%\n",
      "Epoch 29, Batch 815, LR 0.000017 Loss 4.734655, Accuracy 88.490%\n",
      "Epoch 29, Batch 816, LR 0.000017 Loss 4.734681, Accuracy 88.490%\n",
      "Epoch 29, Batch 817, LR 0.000017 Loss 4.733968, Accuracy 88.495%\n",
      "Epoch 29, Batch 818, LR 0.000017 Loss 4.733823, Accuracy 88.496%\n",
      "Epoch 29, Batch 819, LR 0.000017 Loss 4.734854, Accuracy 88.490%\n",
      "Epoch 29, Batch 820, LR 0.000017 Loss 4.735077, Accuracy 88.489%\n",
      "Epoch 29, Batch 821, LR 0.000017 Loss 4.734785, Accuracy 88.489%\n",
      "Epoch 29, Batch 822, LR 0.000017 Loss 4.734536, Accuracy 88.492%\n",
      "Epoch 29, Batch 823, LR 0.000017 Loss 4.736233, Accuracy 88.484%\n",
      "Epoch 29, Batch 824, LR 0.000017 Loss 4.736776, Accuracy 88.482%\n",
      "Epoch 29, Batch 825, LR 0.000017 Loss 4.737914, Accuracy 88.474%\n",
      "Epoch 29, Batch 826, LR 0.000017 Loss 4.737936, Accuracy 88.470%\n",
      "Epoch 29, Batch 827, LR 0.000017 Loss 4.737740, Accuracy 88.472%\n",
      "Epoch 29, Batch 828, LR 0.000017 Loss 4.737685, Accuracy 88.475%\n",
      "Epoch 29, Batch 829, LR 0.000017 Loss 4.738333, Accuracy 88.471%\n",
      "Epoch 29, Batch 830, LR 0.000017 Loss 4.738520, Accuracy 88.470%\n",
      "Epoch 29, Batch 831, LR 0.000017 Loss 4.738790, Accuracy 88.468%\n",
      "Epoch 29, Batch 832, LR 0.000017 Loss 4.738904, Accuracy 88.466%\n",
      "Epoch 29, Batch 833, LR 0.000017 Loss 4.738899, Accuracy 88.469%\n",
      "Epoch 29, Batch 834, LR 0.000017 Loss 4.739523, Accuracy 88.470%\n",
      "Epoch 29, Batch 835, LR 0.000017 Loss 4.739721, Accuracy 88.471%\n",
      "Epoch 29, Batch 836, LR 0.000017 Loss 4.739632, Accuracy 88.472%\n",
      "Epoch 29, Batch 837, LR 0.000017 Loss 4.740220, Accuracy 88.470%\n",
      "Epoch 29, Batch 838, LR 0.000017 Loss 4.741178, Accuracy 88.461%\n",
      "Epoch 29, Batch 839, LR 0.000017 Loss 4.740614, Accuracy 88.465%\n",
      "Epoch 29, Batch 840, LR 0.000017 Loss 4.739923, Accuracy 88.470%\n",
      "Epoch 29, Batch 841, LR 0.000017 Loss 4.740125, Accuracy 88.471%\n",
      "Epoch 29, Batch 842, LR 0.000017 Loss 4.740375, Accuracy 88.471%\n",
      "Epoch 29, Batch 843, LR 0.000017 Loss 4.741159, Accuracy 88.466%\n",
      "Epoch 29, Batch 844, LR 0.000017 Loss 4.740681, Accuracy 88.464%\n",
      "Epoch 29, Batch 845, LR 0.000017 Loss 4.739773, Accuracy 88.465%\n",
      "Epoch 29, Batch 846, LR 0.000017 Loss 4.738876, Accuracy 88.467%\n",
      "Epoch 29, Batch 847, LR 0.000017 Loss 4.738940, Accuracy 88.467%\n",
      "Epoch 29, Batch 848, LR 0.000017 Loss 4.739651, Accuracy 88.469%\n",
      "Epoch 29, Batch 849, LR 0.000017 Loss 4.740315, Accuracy 88.471%\n",
      "Epoch 29, Batch 850, LR 0.000017 Loss 4.739721, Accuracy 88.474%\n",
      "Epoch 29, Batch 851, LR 0.000017 Loss 4.739847, Accuracy 88.475%\n",
      "Epoch 29, Batch 852, LR 0.000017 Loss 4.738880, Accuracy 88.479%\n",
      "Epoch 29, Batch 853, LR 0.000017 Loss 4.740208, Accuracy 88.471%\n",
      "Epoch 29, Batch 854, LR 0.000017 Loss 4.739539, Accuracy 88.475%\n",
      "Epoch 29, Batch 855, LR 0.000017 Loss 4.740016, Accuracy 88.470%\n",
      "Epoch 29, Batch 856, LR 0.000017 Loss 4.740124, Accuracy 88.472%\n",
      "Epoch 29, Batch 857, LR 0.000017 Loss 4.739853, Accuracy 88.473%\n",
      "Epoch 29, Batch 858, LR 0.000017 Loss 4.740035, Accuracy 88.473%\n",
      "Epoch 29, Batch 859, LR 0.000017 Loss 4.739620, Accuracy 88.476%\n",
      "Epoch 29, Batch 860, LR 0.000017 Loss 4.740182, Accuracy 88.473%\n",
      "Epoch 29, Batch 861, LR 0.000017 Loss 4.739937, Accuracy 88.475%\n",
      "Epoch 29, Batch 862, LR 0.000017 Loss 4.739906, Accuracy 88.477%\n",
      "Epoch 29, Batch 863, LR 0.000017 Loss 4.739682, Accuracy 88.474%\n",
      "Epoch 29, Batch 864, LR 0.000017 Loss 4.739074, Accuracy 88.483%\n",
      "Epoch 29, Batch 865, LR 0.000017 Loss 4.738524, Accuracy 88.487%\n",
      "Epoch 29, Batch 866, LR 0.000017 Loss 4.738491, Accuracy 88.486%\n",
      "Epoch 29, Batch 867, LR 0.000017 Loss 4.738165, Accuracy 88.482%\n",
      "Epoch 29, Batch 868, LR 0.000017 Loss 4.737394, Accuracy 88.486%\n",
      "Epoch 29, Batch 869, LR 0.000017 Loss 4.738109, Accuracy 88.484%\n",
      "Epoch 29, Batch 870, LR 0.000017 Loss 4.738244, Accuracy 88.482%\n",
      "Epoch 29, Batch 871, LR 0.000017 Loss 4.737427, Accuracy 88.486%\n",
      "Epoch 29, Batch 872, LR 0.000017 Loss 4.737597, Accuracy 88.491%\n",
      "Epoch 29, Batch 873, LR 0.000017 Loss 4.736966, Accuracy 88.495%\n",
      "Epoch 29, Batch 874, LR 0.000017 Loss 4.736874, Accuracy 88.495%\n",
      "Epoch 29, Batch 875, LR 0.000017 Loss 4.737175, Accuracy 88.490%\n",
      "Epoch 29, Batch 876, LR 0.000017 Loss 4.738074, Accuracy 88.490%\n",
      "Epoch 29, Batch 877, LR 0.000017 Loss 4.738002, Accuracy 88.489%\n",
      "Epoch 29, Batch 878, LR 0.000017 Loss 4.739014, Accuracy 88.487%\n",
      "Epoch 29, Batch 879, LR 0.000017 Loss 4.738973, Accuracy 88.487%\n",
      "Epoch 29, Batch 880, LR 0.000017 Loss 4.739496, Accuracy 88.485%\n",
      "Epoch 29, Batch 881, LR 0.000017 Loss 4.740008, Accuracy 88.482%\n",
      "Epoch 29, Batch 882, LR 0.000017 Loss 4.739401, Accuracy 88.482%\n",
      "Epoch 29, Batch 883, LR 0.000017 Loss 4.739997, Accuracy 88.483%\n",
      "Epoch 29, Batch 884, LR 0.000017 Loss 4.740127, Accuracy 88.481%\n",
      "Epoch 29, Batch 885, LR 0.000017 Loss 4.740117, Accuracy 88.483%\n",
      "Epoch 29, Batch 886, LR 0.000017 Loss 4.739726, Accuracy 88.483%\n",
      "Epoch 29, Batch 887, LR 0.000017 Loss 4.740050, Accuracy 88.480%\n",
      "Epoch 29, Batch 888, LR 0.000017 Loss 4.740131, Accuracy 88.481%\n",
      "Epoch 29, Batch 889, LR 0.000017 Loss 4.740357, Accuracy 88.480%\n",
      "Epoch 29, Batch 890, LR 0.000017 Loss 4.739986, Accuracy 88.485%\n",
      "Epoch 29, Batch 891, LR 0.000017 Loss 4.739570, Accuracy 88.487%\n",
      "Epoch 29, Batch 892, LR 0.000017 Loss 4.740113, Accuracy 88.484%\n",
      "Epoch 29, Batch 893, LR 0.000017 Loss 4.739783, Accuracy 88.486%\n",
      "Epoch 29, Batch 894, LR 0.000017 Loss 4.739928, Accuracy 88.482%\n",
      "Epoch 29, Batch 895, LR 0.000017 Loss 4.739769, Accuracy 88.482%\n",
      "Epoch 29, Batch 896, LR 0.000017 Loss 4.740344, Accuracy 88.481%\n",
      "Epoch 29, Batch 897, LR 0.000017 Loss 4.740612, Accuracy 88.482%\n",
      "Epoch 29, Batch 898, LR 0.000017 Loss 4.739931, Accuracy 88.483%\n",
      "Epoch 29, Batch 899, LR 0.000017 Loss 4.739426, Accuracy 88.488%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 900, LR 0.000017 Loss 4.739515, Accuracy 88.490%\n",
      "Epoch 29, Batch 901, LR 0.000017 Loss 4.739038, Accuracy 88.488%\n",
      "Epoch 29, Batch 902, LR 0.000017 Loss 4.738385, Accuracy 88.487%\n",
      "Epoch 29, Batch 903, LR 0.000017 Loss 4.738165, Accuracy 88.491%\n",
      "Epoch 29, Batch 904, LR 0.000017 Loss 4.738669, Accuracy 88.489%\n",
      "Epoch 29, Batch 905, LR 0.000017 Loss 4.737790, Accuracy 88.490%\n",
      "Epoch 29, Batch 906, LR 0.000017 Loss 4.736948, Accuracy 88.498%\n",
      "Epoch 29, Batch 907, LR 0.000017 Loss 4.737028, Accuracy 88.502%\n",
      "Epoch 29, Batch 908, LR 0.000017 Loss 4.737718, Accuracy 88.498%\n",
      "Epoch 29, Batch 909, LR 0.000017 Loss 4.737768, Accuracy 88.496%\n",
      "Epoch 29, Batch 910, LR 0.000017 Loss 4.738486, Accuracy 88.494%\n",
      "Epoch 29, Batch 911, LR 0.000017 Loss 4.738640, Accuracy 88.488%\n",
      "Epoch 29, Batch 912, LR 0.000017 Loss 4.738438, Accuracy 88.485%\n",
      "Epoch 29, Batch 913, LR 0.000017 Loss 4.739455, Accuracy 88.481%\n",
      "Epoch 29, Batch 914, LR 0.000017 Loss 4.738991, Accuracy 88.483%\n",
      "Epoch 29, Batch 915, LR 0.000017 Loss 4.738917, Accuracy 88.484%\n",
      "Epoch 29, Batch 916, LR 0.000017 Loss 4.739432, Accuracy 88.480%\n",
      "Epoch 29, Batch 917, LR 0.000017 Loss 4.738458, Accuracy 88.487%\n",
      "Epoch 29, Batch 918, LR 0.000017 Loss 4.738721, Accuracy 88.486%\n",
      "Epoch 29, Batch 919, LR 0.000017 Loss 4.738967, Accuracy 88.486%\n",
      "Epoch 29, Batch 920, LR 0.000017 Loss 4.738511, Accuracy 88.490%\n",
      "Epoch 29, Batch 921, LR 0.000017 Loss 4.738885, Accuracy 88.488%\n",
      "Epoch 29, Batch 922, LR 0.000017 Loss 4.739287, Accuracy 88.485%\n",
      "Epoch 29, Batch 923, LR 0.000016 Loss 4.739352, Accuracy 88.487%\n",
      "Epoch 29, Batch 924, LR 0.000016 Loss 4.738984, Accuracy 88.488%\n",
      "Epoch 29, Batch 925, LR 0.000016 Loss 4.739308, Accuracy 88.486%\n",
      "Epoch 29, Batch 926, LR 0.000016 Loss 4.738813, Accuracy 88.488%\n",
      "Epoch 29, Batch 927, LR 0.000016 Loss 4.739438, Accuracy 88.486%\n",
      "Epoch 29, Batch 928, LR 0.000016 Loss 4.739564, Accuracy 88.489%\n",
      "Epoch 29, Batch 929, LR 0.000016 Loss 4.740242, Accuracy 88.485%\n",
      "Epoch 29, Batch 930, LR 0.000016 Loss 4.739965, Accuracy 88.490%\n",
      "Epoch 29, Batch 931, LR 0.000016 Loss 4.739394, Accuracy 88.492%\n",
      "Epoch 29, Batch 932, LR 0.000016 Loss 4.739698, Accuracy 88.488%\n",
      "Epoch 29, Batch 933, LR 0.000016 Loss 4.739575, Accuracy 88.490%\n",
      "Epoch 29, Batch 934, LR 0.000016 Loss 4.739646, Accuracy 88.489%\n",
      "Epoch 29, Batch 935, LR 0.000016 Loss 4.738441, Accuracy 88.492%\n",
      "Epoch 29, Batch 936, LR 0.000016 Loss 4.737994, Accuracy 88.493%\n",
      "Epoch 29, Batch 937, LR 0.000016 Loss 4.737661, Accuracy 88.501%\n",
      "Epoch 29, Batch 938, LR 0.000016 Loss 4.737189, Accuracy 88.500%\n",
      "Epoch 29, Batch 939, LR 0.000016 Loss 4.736885, Accuracy 88.500%\n",
      "Epoch 29, Batch 940, LR 0.000016 Loss 4.736510, Accuracy 88.501%\n",
      "Epoch 29, Batch 941, LR 0.000016 Loss 4.735536, Accuracy 88.507%\n",
      "Epoch 29, Batch 942, LR 0.000016 Loss 4.736001, Accuracy 88.508%\n",
      "Epoch 29, Batch 943, LR 0.000016 Loss 4.735894, Accuracy 88.508%\n",
      "Epoch 29, Batch 944, LR 0.000016 Loss 4.734760, Accuracy 88.513%\n",
      "Epoch 29, Batch 945, LR 0.000016 Loss 4.735582, Accuracy 88.511%\n",
      "Epoch 29, Batch 946, LR 0.000016 Loss 4.734642, Accuracy 88.518%\n",
      "Epoch 29, Batch 947, LR 0.000016 Loss 4.735196, Accuracy 88.516%\n",
      "Epoch 29, Batch 948, LR 0.000016 Loss 4.734127, Accuracy 88.521%\n",
      "Epoch 29, Batch 949, LR 0.000016 Loss 4.734085, Accuracy 88.521%\n",
      "Epoch 29, Batch 950, LR 0.000016 Loss 4.734989, Accuracy 88.518%\n",
      "Epoch 29, Batch 951, LR 0.000016 Loss 4.734464, Accuracy 88.522%\n",
      "Epoch 29, Batch 952, LR 0.000016 Loss 4.733610, Accuracy 88.529%\n",
      "Epoch 29, Batch 953, LR 0.000016 Loss 4.733220, Accuracy 88.533%\n",
      "Epoch 29, Batch 954, LR 0.000016 Loss 4.733195, Accuracy 88.534%\n",
      "Epoch 29, Batch 955, LR 0.000016 Loss 4.733354, Accuracy 88.535%\n",
      "Epoch 29, Batch 956, LR 0.000016 Loss 4.733231, Accuracy 88.528%\n",
      "Epoch 29, Batch 957, LR 0.000016 Loss 4.732342, Accuracy 88.529%\n",
      "Epoch 29, Batch 958, LR 0.000016 Loss 4.732412, Accuracy 88.530%\n",
      "Epoch 29, Batch 959, LR 0.000016 Loss 4.732453, Accuracy 88.529%\n",
      "Epoch 29, Batch 960, LR 0.000016 Loss 4.732313, Accuracy 88.529%\n",
      "Epoch 29, Batch 961, LR 0.000016 Loss 4.732041, Accuracy 88.533%\n",
      "Epoch 29, Batch 962, LR 0.000016 Loss 4.731990, Accuracy 88.532%\n",
      "Epoch 29, Batch 963, LR 0.000016 Loss 4.732146, Accuracy 88.533%\n",
      "Epoch 29, Batch 964, LR 0.000016 Loss 4.731368, Accuracy 88.535%\n",
      "Epoch 29, Batch 965, LR 0.000016 Loss 4.731592, Accuracy 88.533%\n",
      "Epoch 29, Batch 966, LR 0.000016 Loss 4.731344, Accuracy 88.534%\n",
      "Epoch 29, Batch 967, LR 0.000016 Loss 4.731614, Accuracy 88.535%\n",
      "Epoch 29, Batch 968, LR 0.000016 Loss 4.730542, Accuracy 88.539%\n",
      "Epoch 29, Batch 969, LR 0.000016 Loss 4.730487, Accuracy 88.538%\n",
      "Epoch 29, Batch 970, LR 0.000016 Loss 4.729550, Accuracy 88.543%\n",
      "Epoch 29, Batch 971, LR 0.000016 Loss 4.730133, Accuracy 88.537%\n",
      "Epoch 29, Batch 972, LR 0.000016 Loss 4.730837, Accuracy 88.533%\n",
      "Epoch 29, Batch 973, LR 0.000016 Loss 4.731032, Accuracy 88.528%\n",
      "Epoch 29, Batch 974, LR 0.000016 Loss 4.730769, Accuracy 88.530%\n",
      "Epoch 29, Batch 975, LR 0.000016 Loss 4.730646, Accuracy 88.526%\n",
      "Epoch 29, Batch 976, LR 0.000016 Loss 4.729750, Accuracy 88.532%\n",
      "Epoch 29, Batch 977, LR 0.000016 Loss 4.729900, Accuracy 88.535%\n",
      "Epoch 29, Batch 978, LR 0.000016 Loss 4.729423, Accuracy 88.538%\n",
      "Epoch 29, Batch 979, LR 0.000016 Loss 4.728646, Accuracy 88.543%\n",
      "Epoch 29, Batch 980, LR 0.000016 Loss 4.728182, Accuracy 88.541%\n",
      "Epoch 29, Batch 981, LR 0.000016 Loss 4.728060, Accuracy 88.541%\n",
      "Epoch 29, Batch 982, LR 0.000016 Loss 4.728062, Accuracy 88.541%\n",
      "Epoch 29, Batch 983, LR 0.000016 Loss 4.727698, Accuracy 88.541%\n",
      "Epoch 29, Batch 984, LR 0.000016 Loss 4.727327, Accuracy 88.542%\n",
      "Epoch 29, Batch 985, LR 0.000016 Loss 4.727705, Accuracy 88.537%\n",
      "Epoch 29, Batch 986, LR 0.000016 Loss 4.728421, Accuracy 88.532%\n",
      "Epoch 29, Batch 987, LR 0.000016 Loss 4.728185, Accuracy 88.533%\n",
      "Epoch 29, Batch 988, LR 0.000016 Loss 4.727290, Accuracy 88.540%\n",
      "Epoch 29, Batch 989, LR 0.000016 Loss 4.726984, Accuracy 88.540%\n",
      "Epoch 29, Batch 990, LR 0.000016 Loss 4.726970, Accuracy 88.539%\n",
      "Epoch 29, Batch 991, LR 0.000016 Loss 4.727421, Accuracy 88.536%\n",
      "Epoch 29, Batch 992, LR 0.000016 Loss 4.727454, Accuracy 88.534%\n",
      "Epoch 29, Batch 993, LR 0.000016 Loss 4.727386, Accuracy 88.535%\n",
      "Epoch 29, Batch 994, LR 0.000016 Loss 4.727551, Accuracy 88.533%\n",
      "Epoch 29, Batch 995, LR 0.000016 Loss 4.727552, Accuracy 88.535%\n",
      "Epoch 29, Batch 996, LR 0.000016 Loss 4.727881, Accuracy 88.533%\n",
      "Epoch 29, Batch 997, LR 0.000016 Loss 4.727726, Accuracy 88.535%\n",
      "Epoch 29, Batch 998, LR 0.000016 Loss 4.728204, Accuracy 88.533%\n",
      "Epoch 29, Batch 999, LR 0.000016 Loss 4.727617, Accuracy 88.535%\n",
      "Epoch 29, Batch 1000, LR 0.000016 Loss 4.727175, Accuracy 88.538%\n",
      "Epoch 29, Batch 1001, LR 0.000016 Loss 4.726731, Accuracy 88.545%\n",
      "Epoch 29, Batch 1002, LR 0.000016 Loss 4.726832, Accuracy 88.546%\n",
      "Epoch 29, Batch 1003, LR 0.000016 Loss 4.727346, Accuracy 88.542%\n",
      "Epoch 29, Batch 1004, LR 0.000016 Loss 4.726698, Accuracy 88.547%\n",
      "Epoch 29, Batch 1005, LR 0.000016 Loss 4.726238, Accuracy 88.549%\n",
      "Epoch 29, Batch 1006, LR 0.000016 Loss 4.726041, Accuracy 88.552%\n",
      "Epoch 29, Batch 1007, LR 0.000016 Loss 4.725933, Accuracy 88.554%\n",
      "Epoch 29, Batch 1008, LR 0.000016 Loss 4.726195, Accuracy 88.553%\n",
      "Epoch 29, Batch 1009, LR 0.000016 Loss 4.726221, Accuracy 88.555%\n",
      "Epoch 29, Batch 1010, LR 0.000016 Loss 4.725871, Accuracy 88.554%\n",
      "Epoch 29, Batch 1011, LR 0.000016 Loss 4.726144, Accuracy 88.555%\n",
      "Epoch 29, Batch 1012, LR 0.000016 Loss 4.725725, Accuracy 88.557%\n",
      "Epoch 29, Batch 1013, LR 0.000016 Loss 4.726142, Accuracy 88.552%\n",
      "Epoch 29, Batch 1014, LR 0.000016 Loss 4.726585, Accuracy 88.549%\n",
      "Epoch 29, Batch 1015, LR 0.000016 Loss 4.726724, Accuracy 88.552%\n",
      "Epoch 29, Batch 1016, LR 0.000016 Loss 4.726400, Accuracy 88.554%\n",
      "Epoch 29, Batch 1017, LR 0.000016 Loss 4.726323, Accuracy 88.552%\n",
      "Epoch 29, Batch 1018, LR 0.000016 Loss 4.727219, Accuracy 88.546%\n",
      "Epoch 29, Batch 1019, LR 0.000016 Loss 4.726305, Accuracy 88.550%\n",
      "Epoch 29, Batch 1020, LR 0.000016 Loss 4.725396, Accuracy 88.553%\n",
      "Epoch 29, Batch 1021, LR 0.000016 Loss 4.725264, Accuracy 88.548%\n",
      "Epoch 29, Batch 1022, LR 0.000016 Loss 4.726030, Accuracy 88.542%\n",
      "Epoch 29, Batch 1023, LR 0.000016 Loss 4.725330, Accuracy 88.544%\n",
      "Epoch 29, Batch 1024, LR 0.000016 Loss 4.725510, Accuracy 88.546%\n",
      "Epoch 29, Batch 1025, LR 0.000016 Loss 4.725460, Accuracy 88.547%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 1026, LR 0.000016 Loss 4.725753, Accuracy 88.547%\n",
      "Epoch 29, Batch 1027, LR 0.000016 Loss 4.725552, Accuracy 88.547%\n",
      "Epoch 29, Batch 1028, LR 0.000016 Loss 4.725123, Accuracy 88.549%\n",
      "Epoch 29, Batch 1029, LR 0.000016 Loss 4.724663, Accuracy 88.551%\n",
      "Epoch 29, Batch 1030, LR 0.000016 Loss 4.724552, Accuracy 88.552%\n",
      "Epoch 29, Batch 1031, LR 0.000016 Loss 4.724707, Accuracy 88.550%\n",
      "Epoch 29, Batch 1032, LR 0.000016 Loss 4.724866, Accuracy 88.550%\n",
      "Epoch 29, Batch 1033, LR 0.000016 Loss 4.724470, Accuracy 88.550%\n",
      "Epoch 29, Batch 1034, LR 0.000016 Loss 4.724100, Accuracy 88.552%\n",
      "Epoch 29, Batch 1035, LR 0.000016 Loss 4.724758, Accuracy 88.553%\n",
      "Epoch 29, Batch 1036, LR 0.000016 Loss 4.724356, Accuracy 88.556%\n",
      "Epoch 29, Batch 1037, LR 0.000016 Loss 4.725018, Accuracy 88.552%\n",
      "Epoch 29, Batch 1038, LR 0.000016 Loss 4.724766, Accuracy 88.554%\n",
      "Epoch 29, Batch 1039, LR 0.000016 Loss 4.724418, Accuracy 88.554%\n",
      "Epoch 29, Batch 1040, LR 0.000016 Loss 4.724506, Accuracy 88.553%\n",
      "Epoch 29, Batch 1041, LR 0.000016 Loss 4.724551, Accuracy 88.552%\n",
      "Epoch 29, Batch 1042, LR 0.000016 Loss 4.724458, Accuracy 88.553%\n",
      "Epoch 29, Batch 1043, LR 0.000016 Loss 4.724819, Accuracy 88.552%\n",
      "Epoch 29, Batch 1044, LR 0.000016 Loss 4.724527, Accuracy 88.554%\n",
      "Epoch 29, Batch 1045, LR 0.000016 Loss 4.724642, Accuracy 88.554%\n",
      "Epoch 29, Batch 1046, LR 0.000016 Loss 4.724594, Accuracy 88.553%\n",
      "Epoch 29, Batch 1047, LR 0.000016 Loss 4.725415, Accuracy 88.547%\n",
      "Epoch 29, Loss (train set) 4.725415, Accuracy (train set) 88.547%\n",
      "Epoch 29, Accuracy (validation set) 70.494%\n",
      "Epoch 29, EER (test set) 5.420%\n",
      "Epoch 30, Batch 1, LR 0.000016 Loss 4.001073, Accuracy 92.969%\n",
      "Epoch 30, Batch 2, LR 0.000016 Loss 4.504705, Accuracy 89.844%\n",
      "Epoch 30, Batch 3, LR 0.000016 Loss 4.298646, Accuracy 90.625%\n",
      "Epoch 30, Batch 4, LR 0.000016 Loss 4.255789, Accuracy 91.602%\n",
      "Epoch 30, Batch 5, LR 0.000016 Loss 4.393449, Accuracy 90.938%\n",
      "Epoch 30, Batch 6, LR 0.000016 Loss 4.511328, Accuracy 89.844%\n",
      "Epoch 30, Batch 7, LR 0.000016 Loss 4.545770, Accuracy 89.397%\n",
      "Epoch 30, Batch 8, LR 0.000016 Loss 4.593973, Accuracy 89.551%\n",
      "Epoch 30, Batch 9, LR 0.000016 Loss 4.588109, Accuracy 89.844%\n",
      "Epoch 30, Batch 10, LR 0.000016 Loss 4.598692, Accuracy 89.609%\n",
      "Epoch 30, Batch 11, LR 0.000016 Loss 4.576333, Accuracy 89.276%\n",
      "Epoch 30, Batch 12, LR 0.000016 Loss 4.566454, Accuracy 89.258%\n",
      "Epoch 30, Batch 13, LR 0.000016 Loss 4.582819, Accuracy 89.363%\n",
      "Epoch 30, Batch 14, LR 0.000016 Loss 4.585894, Accuracy 89.342%\n",
      "Epoch 30, Batch 15, LR 0.000016 Loss 4.588801, Accuracy 89.271%\n",
      "Epoch 30, Batch 16, LR 0.000016 Loss 4.568882, Accuracy 89.355%\n",
      "Epoch 30, Batch 17, LR 0.000016 Loss 4.554740, Accuracy 89.568%\n",
      "Epoch 30, Batch 18, LR 0.000016 Loss 4.587865, Accuracy 89.149%\n",
      "Epoch 30, Batch 19, LR 0.000016 Loss 4.587423, Accuracy 89.062%\n",
      "Epoch 30, Batch 20, LR 0.000016 Loss 4.572069, Accuracy 89.219%\n",
      "Epoch 30, Batch 21, LR 0.000016 Loss 4.574087, Accuracy 89.137%\n",
      "Epoch 30, Batch 22, LR 0.000016 Loss 4.584922, Accuracy 89.205%\n",
      "Epoch 30, Batch 23, LR 0.000016 Loss 4.586545, Accuracy 89.164%\n",
      "Epoch 30, Batch 24, LR 0.000016 Loss 4.623315, Accuracy 88.997%\n",
      "Epoch 30, Batch 25, LR 0.000016 Loss 4.621693, Accuracy 88.938%\n",
      "Epoch 30, Batch 26, LR 0.000016 Loss 4.631949, Accuracy 88.972%\n",
      "Epoch 30, Batch 27, LR 0.000016 Loss 4.616982, Accuracy 88.947%\n",
      "Epoch 30, Batch 28, LR 0.000016 Loss 4.605053, Accuracy 89.062%\n",
      "Epoch 30, Batch 29, LR 0.000016 Loss 4.629894, Accuracy 89.036%\n",
      "Epoch 30, Batch 30, LR 0.000016 Loss 4.599511, Accuracy 89.062%\n",
      "Epoch 30, Batch 31, LR 0.000016 Loss 4.598615, Accuracy 89.088%\n",
      "Epoch 30, Batch 32, LR 0.000016 Loss 4.593821, Accuracy 89.136%\n",
      "Epoch 30, Batch 33, LR 0.000016 Loss 4.579766, Accuracy 89.276%\n",
      "Epoch 30, Batch 34, LR 0.000016 Loss 4.591067, Accuracy 89.177%\n",
      "Epoch 30, Batch 35, LR 0.000016 Loss 4.605720, Accuracy 89.152%\n",
      "Epoch 30, Batch 36, LR 0.000016 Loss 4.613756, Accuracy 89.106%\n",
      "Epoch 30, Batch 37, LR 0.000016 Loss 4.636896, Accuracy 88.957%\n",
      "Epoch 30, Batch 38, LR 0.000016 Loss 4.644217, Accuracy 88.939%\n",
      "Epoch 30, Batch 39, LR 0.000016 Loss 4.635534, Accuracy 89.002%\n",
      "Epoch 30, Batch 40, LR 0.000016 Loss 4.624169, Accuracy 89.043%\n",
      "Epoch 30, Batch 41, LR 0.000016 Loss 4.619335, Accuracy 89.062%\n",
      "Epoch 30, Batch 42, LR 0.000016 Loss 4.616335, Accuracy 89.062%\n",
      "Epoch 30, Batch 43, LR 0.000016 Loss 4.626391, Accuracy 88.972%\n",
      "Epoch 30, Batch 44, LR 0.000016 Loss 4.627543, Accuracy 88.938%\n",
      "Epoch 30, Batch 45, LR 0.000016 Loss 4.626606, Accuracy 89.062%\n",
      "Epoch 30, Batch 46, LR 0.000016 Loss 4.634175, Accuracy 89.079%\n",
      "Epoch 30, Batch 47, LR 0.000016 Loss 4.634492, Accuracy 89.062%\n",
      "Epoch 30, Batch 48, LR 0.000016 Loss 4.649976, Accuracy 89.030%\n",
      "Epoch 30, Batch 49, LR 0.000016 Loss 4.657636, Accuracy 88.967%\n",
      "Epoch 30, Batch 50, LR 0.000016 Loss 4.647285, Accuracy 89.031%\n",
      "Epoch 30, Batch 51, LR 0.000016 Loss 4.653123, Accuracy 89.062%\n",
      "Epoch 30, Batch 52, LR 0.000016 Loss 4.652609, Accuracy 89.017%\n",
      "Epoch 30, Batch 53, LR 0.000016 Loss 4.656922, Accuracy 88.900%\n",
      "Epoch 30, Batch 54, LR 0.000016 Loss 4.652374, Accuracy 88.903%\n",
      "Epoch 30, Batch 55, LR 0.000016 Loss 4.651856, Accuracy 88.949%\n",
      "Epoch 30, Batch 56, LR 0.000016 Loss 4.665612, Accuracy 88.839%\n",
      "Epoch 30, Batch 57, LR 0.000016 Loss 4.661066, Accuracy 88.871%\n",
      "Epoch 30, Batch 58, LR 0.000016 Loss 4.650517, Accuracy 88.901%\n",
      "Epoch 30, Batch 59, LR 0.000016 Loss 4.652991, Accuracy 88.811%\n",
      "Epoch 30, Batch 60, LR 0.000016 Loss 4.660802, Accuracy 88.763%\n",
      "Epoch 30, Batch 61, LR 0.000016 Loss 4.660051, Accuracy 88.755%\n",
      "Epoch 30, Batch 62, LR 0.000016 Loss 4.657324, Accuracy 88.798%\n",
      "Epoch 30, Batch 63, LR 0.000016 Loss 4.666297, Accuracy 88.777%\n",
      "Epoch 30, Batch 64, LR 0.000016 Loss 4.662681, Accuracy 88.794%\n",
      "Epoch 30, Batch 65, LR 0.000016 Loss 4.650945, Accuracy 88.798%\n",
      "Epoch 30, Batch 66, LR 0.000016 Loss 4.649036, Accuracy 88.778%\n",
      "Epoch 30, Batch 67, LR 0.000016 Loss 4.651353, Accuracy 88.771%\n",
      "Epoch 30, Batch 68, LR 0.000016 Loss 4.655432, Accuracy 88.764%\n",
      "Epoch 30, Batch 69, LR 0.000016 Loss 4.660043, Accuracy 88.779%\n",
      "Epoch 30, Batch 70, LR 0.000016 Loss 4.660364, Accuracy 88.772%\n",
      "Epoch 30, Batch 71, LR 0.000016 Loss 4.658254, Accuracy 88.765%\n",
      "Epoch 30, Batch 72, LR 0.000016 Loss 4.649931, Accuracy 88.791%\n",
      "Epoch 30, Batch 73, LR 0.000016 Loss 4.646045, Accuracy 88.784%\n",
      "Epoch 30, Batch 74, LR 0.000016 Loss 4.644741, Accuracy 88.788%\n",
      "Epoch 30, Batch 75, LR 0.000016 Loss 4.635416, Accuracy 88.812%\n",
      "Epoch 30, Batch 76, LR 0.000016 Loss 4.628181, Accuracy 88.847%\n",
      "Epoch 30, Batch 77, LR 0.000016 Loss 4.622353, Accuracy 88.870%\n",
      "Epoch 30, Batch 78, LR 0.000016 Loss 4.637303, Accuracy 88.802%\n",
      "Epoch 30, Batch 79, LR 0.000016 Loss 4.640814, Accuracy 88.845%\n",
      "Epoch 30, Batch 80, LR 0.000016 Loss 4.651877, Accuracy 88.760%\n",
      "Epoch 30, Batch 81, LR 0.000016 Loss 4.647366, Accuracy 88.792%\n",
      "Epoch 30, Batch 82, LR 0.000016 Loss 4.649300, Accuracy 88.796%\n",
      "Epoch 30, Batch 83, LR 0.000016 Loss 4.649337, Accuracy 88.799%\n",
      "Epoch 30, Batch 84, LR 0.000016 Loss 4.646242, Accuracy 88.858%\n",
      "Epoch 30, Batch 85, LR 0.000016 Loss 4.649300, Accuracy 88.814%\n",
      "Epoch 30, Batch 86, LR 0.000016 Loss 4.648424, Accuracy 88.799%\n",
      "Epoch 30, Batch 87, LR 0.000016 Loss 4.644200, Accuracy 88.820%\n",
      "Epoch 30, Batch 88, LR 0.000016 Loss 4.636083, Accuracy 88.849%\n",
      "Epoch 30, Batch 89, LR 0.000016 Loss 4.643113, Accuracy 88.834%\n",
      "Epoch 30, Batch 90, LR 0.000016 Loss 4.644792, Accuracy 88.785%\n",
      "Epoch 30, Batch 91, LR 0.000016 Loss 4.650458, Accuracy 88.753%\n",
      "Epoch 30, Batch 92, LR 0.000016 Loss 4.641006, Accuracy 88.808%\n",
      "Epoch 30, Batch 93, LR 0.000016 Loss 4.635871, Accuracy 88.836%\n",
      "Epoch 30, Batch 94, LR 0.000016 Loss 4.634123, Accuracy 88.846%\n",
      "Epoch 30, Batch 95, LR 0.000016 Loss 4.636933, Accuracy 88.824%\n",
      "Epoch 30, Batch 96, LR 0.000016 Loss 4.640246, Accuracy 88.835%\n",
      "Epoch 30, Batch 97, LR 0.000016 Loss 4.641963, Accuracy 88.837%\n",
      "Epoch 30, Batch 98, LR 0.000016 Loss 4.647799, Accuracy 88.807%\n",
      "Epoch 30, Batch 99, LR 0.000016 Loss 4.649918, Accuracy 88.786%\n",
      "Epoch 30, Batch 100, LR 0.000016 Loss 4.649677, Accuracy 88.789%\n",
      "Epoch 30, Batch 101, LR 0.000016 Loss 4.647255, Accuracy 88.800%\n",
      "Epoch 30, Batch 102, LR 0.000016 Loss 4.645994, Accuracy 88.817%\n",
      "Epoch 30, Batch 103, LR 0.000016 Loss 4.641351, Accuracy 88.858%\n",
      "Epoch 30, Batch 104, LR 0.000016 Loss 4.639970, Accuracy 88.875%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 105, LR 0.000016 Loss 4.639270, Accuracy 88.862%\n",
      "Epoch 30, Batch 106, LR 0.000016 Loss 4.647381, Accuracy 88.878%\n",
      "Epoch 30, Batch 107, LR 0.000016 Loss 4.644215, Accuracy 88.887%\n",
      "Epoch 30, Batch 108, LR 0.000016 Loss 4.643981, Accuracy 88.882%\n",
      "Epoch 30, Batch 109, LR 0.000016 Loss 4.642773, Accuracy 88.898%\n",
      "Epoch 30, Batch 110, LR 0.000016 Loss 4.640985, Accuracy 88.956%\n",
      "Epoch 30, Batch 111, LR 0.000016 Loss 4.639736, Accuracy 88.999%\n",
      "Epoch 30, Batch 112, LR 0.000016 Loss 4.636944, Accuracy 89.007%\n",
      "Epoch 30, Batch 113, LR 0.000016 Loss 4.628799, Accuracy 89.049%\n",
      "Epoch 30, Batch 114, LR 0.000016 Loss 4.634545, Accuracy 89.042%\n",
      "Epoch 30, Batch 115, LR 0.000016 Loss 4.633244, Accuracy 89.069%\n",
      "Epoch 30, Batch 116, LR 0.000016 Loss 4.633117, Accuracy 89.069%\n",
      "Epoch 30, Batch 117, LR 0.000016 Loss 4.632127, Accuracy 89.056%\n",
      "Epoch 30, Batch 118, LR 0.000016 Loss 4.628216, Accuracy 89.076%\n",
      "Epoch 30, Batch 119, LR 0.000016 Loss 4.632738, Accuracy 89.062%\n",
      "Epoch 30, Batch 120, LR 0.000016 Loss 4.638725, Accuracy 89.069%\n",
      "Epoch 30, Batch 121, LR 0.000016 Loss 4.640683, Accuracy 89.043%\n",
      "Epoch 30, Batch 122, LR 0.000016 Loss 4.635364, Accuracy 89.056%\n",
      "Epoch 30, Batch 123, LR 0.000016 Loss 4.634807, Accuracy 89.056%\n",
      "Epoch 30, Batch 124, LR 0.000016 Loss 4.635467, Accuracy 89.075%\n",
      "Epoch 30, Batch 125, LR 0.000016 Loss 4.632137, Accuracy 89.069%\n",
      "Epoch 30, Batch 126, LR 0.000016 Loss 4.637004, Accuracy 89.025%\n",
      "Epoch 30, Batch 127, LR 0.000016 Loss 4.635400, Accuracy 89.032%\n",
      "Epoch 30, Batch 128, LR 0.000016 Loss 4.633669, Accuracy 89.038%\n",
      "Epoch 30, Batch 129, LR 0.000016 Loss 4.634994, Accuracy 89.026%\n",
      "Epoch 30, Batch 130, LR 0.000016 Loss 4.630081, Accuracy 89.069%\n",
      "Epoch 30, Batch 131, LR 0.000016 Loss 4.634964, Accuracy 89.039%\n",
      "Epoch 30, Batch 132, LR 0.000016 Loss 4.631529, Accuracy 89.051%\n",
      "Epoch 30, Batch 133, LR 0.000016 Loss 4.631178, Accuracy 89.062%\n",
      "Epoch 30, Batch 134, LR 0.000016 Loss 4.629605, Accuracy 89.074%\n",
      "Epoch 30, Batch 135, LR 0.000016 Loss 4.629112, Accuracy 89.080%\n",
      "Epoch 30, Batch 136, LR 0.000016 Loss 4.621741, Accuracy 89.108%\n",
      "Epoch 30, Batch 137, LR 0.000016 Loss 4.621747, Accuracy 89.114%\n",
      "Epoch 30, Batch 138, LR 0.000016 Loss 4.622719, Accuracy 89.085%\n",
      "Epoch 30, Batch 139, LR 0.000016 Loss 4.620942, Accuracy 89.113%\n",
      "Epoch 30, Batch 140, LR 0.000016 Loss 4.619371, Accuracy 89.107%\n",
      "Epoch 30, Batch 141, LR 0.000016 Loss 4.619245, Accuracy 89.118%\n",
      "Epoch 30, Batch 142, LR 0.000016 Loss 4.623071, Accuracy 89.107%\n",
      "Epoch 30, Batch 143, LR 0.000016 Loss 4.624757, Accuracy 89.112%\n",
      "Epoch 30, Batch 144, LR 0.000016 Loss 4.625478, Accuracy 89.133%\n",
      "Epoch 30, Batch 145, LR 0.000016 Loss 4.621626, Accuracy 89.133%\n",
      "Epoch 30, Batch 146, LR 0.000016 Loss 4.618590, Accuracy 89.159%\n",
      "Epoch 30, Batch 147, LR 0.000016 Loss 4.617263, Accuracy 89.126%\n",
      "Epoch 30, Batch 148, LR 0.000016 Loss 4.618073, Accuracy 89.110%\n",
      "Epoch 30, Batch 149, LR 0.000016 Loss 4.615691, Accuracy 89.125%\n",
      "Epoch 30, Batch 150, LR 0.000016 Loss 4.611316, Accuracy 89.146%\n",
      "Epoch 30, Batch 151, LR 0.000016 Loss 4.612816, Accuracy 89.130%\n",
      "Epoch 30, Batch 152, LR 0.000016 Loss 4.616539, Accuracy 89.114%\n",
      "Epoch 30, Batch 153, LR 0.000016 Loss 4.619383, Accuracy 89.073%\n",
      "Epoch 30, Batch 154, LR 0.000016 Loss 4.617921, Accuracy 89.083%\n",
      "Epoch 30, Batch 155, LR 0.000016 Loss 4.619955, Accuracy 89.062%\n",
      "Epoch 30, Batch 156, LR 0.000016 Loss 4.626744, Accuracy 89.002%\n",
      "Epoch 30, Batch 157, LR 0.000016 Loss 4.624014, Accuracy 89.003%\n",
      "Epoch 30, Batch 158, LR 0.000016 Loss 4.626319, Accuracy 88.998%\n",
      "Epoch 30, Batch 159, LR 0.000016 Loss 4.626836, Accuracy 88.964%\n",
      "Epoch 30, Batch 160, LR 0.000016 Loss 4.628535, Accuracy 88.975%\n",
      "Epoch 30, Batch 161, LR 0.000016 Loss 4.623338, Accuracy 88.980%\n",
      "Epoch 30, Batch 162, LR 0.000016 Loss 4.621851, Accuracy 88.985%\n",
      "Epoch 30, Batch 163, LR 0.000016 Loss 4.618014, Accuracy 88.991%\n",
      "Epoch 30, Batch 164, LR 0.000016 Loss 4.621976, Accuracy 88.996%\n",
      "Epoch 30, Batch 165, LR 0.000016 Loss 4.627376, Accuracy 88.968%\n",
      "Epoch 30, Batch 166, LR 0.000016 Loss 4.624371, Accuracy 88.997%\n",
      "Epoch 30, Batch 167, LR 0.000016 Loss 4.621585, Accuracy 89.006%\n",
      "Epoch 30, Batch 168, LR 0.000016 Loss 4.622268, Accuracy 89.007%\n",
      "Epoch 30, Batch 169, LR 0.000016 Loss 4.624428, Accuracy 89.002%\n",
      "Epoch 30, Batch 170, LR 0.000016 Loss 4.620356, Accuracy 89.007%\n",
      "Epoch 30, Batch 171, LR 0.000016 Loss 4.623645, Accuracy 89.003%\n",
      "Epoch 30, Batch 172, LR 0.000016 Loss 4.626851, Accuracy 88.976%\n",
      "Epoch 30, Batch 173, LR 0.000016 Loss 4.627073, Accuracy 88.972%\n",
      "Epoch 30, Batch 174, LR 0.000016 Loss 4.628043, Accuracy 88.977%\n",
      "Epoch 30, Batch 175, LR 0.000016 Loss 4.626703, Accuracy 88.973%\n",
      "Epoch 30, Batch 176, LR 0.000016 Loss 4.624561, Accuracy 88.983%\n",
      "Epoch 30, Batch 177, LR 0.000016 Loss 4.623383, Accuracy 88.979%\n",
      "Epoch 30, Batch 178, LR 0.000016 Loss 4.621379, Accuracy 88.983%\n",
      "Epoch 30, Batch 179, LR 0.000016 Loss 4.620816, Accuracy 89.001%\n",
      "Epoch 30, Batch 180, LR 0.000016 Loss 4.619844, Accuracy 89.015%\n",
      "Epoch 30, Batch 181, LR 0.000016 Loss 4.619300, Accuracy 89.028%\n",
      "Epoch 30, Batch 182, LR 0.000016 Loss 4.622716, Accuracy 89.020%\n",
      "Epoch 30, Batch 183, LR 0.000016 Loss 4.623705, Accuracy 89.011%\n",
      "Epoch 30, Batch 184, LR 0.000016 Loss 4.626441, Accuracy 89.003%\n",
      "Epoch 30, Batch 185, LR 0.000016 Loss 4.629774, Accuracy 88.986%\n",
      "Epoch 30, Batch 186, LR 0.000016 Loss 4.629898, Accuracy 88.978%\n",
      "Epoch 30, Batch 187, LR 0.000016 Loss 4.626217, Accuracy 88.983%\n",
      "Epoch 30, Batch 188, LR 0.000016 Loss 4.625972, Accuracy 88.984%\n",
      "Epoch 30, Batch 189, LR 0.000016 Loss 4.623458, Accuracy 88.984%\n",
      "Epoch 30, Batch 190, LR 0.000016 Loss 4.621349, Accuracy 88.988%\n",
      "Epoch 30, Batch 191, LR 0.000016 Loss 4.625648, Accuracy 88.944%\n",
      "Epoch 30, Batch 192, LR 0.000016 Loss 4.628598, Accuracy 88.928%\n",
      "Epoch 30, Batch 193, LR 0.000016 Loss 4.632171, Accuracy 88.892%\n",
      "Epoch 30, Batch 194, LR 0.000016 Loss 4.631766, Accuracy 88.877%\n",
      "Epoch 30, Batch 195, LR 0.000016 Loss 4.634034, Accuracy 88.870%\n",
      "Epoch 30, Batch 196, LR 0.000016 Loss 4.636657, Accuracy 88.843%\n",
      "Epoch 30, Batch 197, LR 0.000016 Loss 4.635960, Accuracy 88.844%\n",
      "Epoch 30, Batch 198, LR 0.000016 Loss 4.638458, Accuracy 88.810%\n",
      "Epoch 30, Batch 199, LR 0.000016 Loss 4.635151, Accuracy 88.835%\n",
      "Epoch 30, Batch 200, LR 0.000016 Loss 4.631433, Accuracy 88.855%\n",
      "Epoch 30, Batch 201, LR 0.000016 Loss 4.629742, Accuracy 88.872%\n",
      "Epoch 30, Batch 202, LR 0.000016 Loss 4.631800, Accuracy 88.865%\n",
      "Epoch 30, Batch 203, LR 0.000016 Loss 4.631151, Accuracy 88.859%\n",
      "Epoch 30, Batch 204, LR 0.000016 Loss 4.629727, Accuracy 88.867%\n",
      "Epoch 30, Batch 205, LR 0.000016 Loss 4.629801, Accuracy 88.887%\n",
      "Epoch 30, Batch 206, LR 0.000016 Loss 4.633193, Accuracy 88.858%\n",
      "Epoch 30, Batch 207, LR 0.000016 Loss 4.633738, Accuracy 88.859%\n",
      "Epoch 30, Batch 208, LR 0.000016 Loss 4.632705, Accuracy 88.863%\n",
      "Epoch 30, Batch 209, LR 0.000016 Loss 4.631633, Accuracy 88.879%\n",
      "Epoch 30, Batch 210, LR 0.000016 Loss 4.631826, Accuracy 88.876%\n",
      "Epoch 30, Batch 211, LR 0.000016 Loss 4.633504, Accuracy 88.874%\n",
      "Epoch 30, Batch 212, LR 0.000016 Loss 4.633521, Accuracy 88.871%\n",
      "Epoch 30, Batch 213, LR 0.000016 Loss 4.633209, Accuracy 88.864%\n",
      "Epoch 30, Batch 214, LR 0.000016 Loss 4.633823, Accuracy 88.869%\n",
      "Epoch 30, Batch 215, LR 0.000016 Loss 4.637082, Accuracy 88.855%\n",
      "Epoch 30, Batch 216, LR 0.000016 Loss 4.636087, Accuracy 88.849%\n",
      "Epoch 30, Batch 217, LR 0.000016 Loss 4.635375, Accuracy 88.854%\n",
      "Epoch 30, Batch 218, LR 0.000016 Loss 4.635562, Accuracy 88.862%\n",
      "Epoch 30, Batch 219, LR 0.000016 Loss 4.636844, Accuracy 88.852%\n",
      "Epoch 30, Batch 220, LR 0.000016 Loss 4.636035, Accuracy 88.864%\n",
      "Epoch 30, Batch 221, LR 0.000016 Loss 4.638232, Accuracy 88.847%\n",
      "Epoch 30, Batch 222, LR 0.000015 Loss 4.640293, Accuracy 88.830%\n",
      "Epoch 30, Batch 223, LR 0.000015 Loss 4.638979, Accuracy 88.835%\n",
      "Epoch 30, Batch 224, LR 0.000015 Loss 4.636584, Accuracy 88.839%\n",
      "Epoch 30, Batch 225, LR 0.000015 Loss 4.639122, Accuracy 88.830%\n",
      "Epoch 30, Batch 226, LR 0.000015 Loss 4.636975, Accuracy 88.834%\n",
      "Epoch 30, Batch 227, LR 0.000015 Loss 4.639352, Accuracy 88.839%\n",
      "Epoch 30, Batch 228, LR 0.000015 Loss 4.639144, Accuracy 88.829%\n",
      "Epoch 30, Batch 229, LR 0.000015 Loss 4.640105, Accuracy 88.817%\n",
      "Epoch 30, Batch 230, LR 0.000015 Loss 4.642542, Accuracy 88.811%\n",
      "Epoch 30, Batch 231, LR 0.000015 Loss 4.643820, Accuracy 88.802%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 232, LR 0.000015 Loss 4.641511, Accuracy 88.820%\n",
      "Epoch 30, Batch 233, LR 0.000015 Loss 4.642367, Accuracy 88.801%\n",
      "Epoch 30, Batch 234, LR 0.000015 Loss 4.639901, Accuracy 88.802%\n",
      "Epoch 30, Batch 235, LR 0.000015 Loss 4.639232, Accuracy 88.790%\n",
      "Epoch 30, Batch 236, LR 0.000015 Loss 4.639363, Accuracy 88.784%\n",
      "Epoch 30, Batch 237, LR 0.000015 Loss 4.640171, Accuracy 88.772%\n",
      "Epoch 30, Batch 238, LR 0.000015 Loss 4.635834, Accuracy 88.780%\n",
      "Epoch 30, Batch 239, LR 0.000015 Loss 4.633815, Accuracy 88.788%\n",
      "Epoch 30, Batch 240, LR 0.000015 Loss 4.634630, Accuracy 88.799%\n",
      "Epoch 30, Batch 241, LR 0.000015 Loss 4.634804, Accuracy 88.790%\n",
      "Epoch 30, Batch 242, LR 0.000015 Loss 4.634566, Accuracy 88.801%\n",
      "Epoch 30, Batch 243, LR 0.000015 Loss 4.634059, Accuracy 88.812%\n",
      "Epoch 30, Batch 244, LR 0.000015 Loss 4.634279, Accuracy 88.816%\n",
      "Epoch 30, Batch 245, LR 0.000015 Loss 4.632586, Accuracy 88.830%\n",
      "Epoch 30, Batch 246, LR 0.000015 Loss 4.633204, Accuracy 88.827%\n",
      "Epoch 30, Batch 247, LR 0.000015 Loss 4.633867, Accuracy 88.828%\n",
      "Epoch 30, Batch 248, LR 0.000015 Loss 4.635138, Accuracy 88.826%\n",
      "Epoch 30, Batch 249, LR 0.000015 Loss 4.636034, Accuracy 88.824%\n",
      "Epoch 30, Batch 250, LR 0.000015 Loss 4.637322, Accuracy 88.825%\n",
      "Epoch 30, Batch 251, LR 0.000015 Loss 4.638270, Accuracy 88.820%\n",
      "Epoch 30, Batch 252, LR 0.000015 Loss 4.638045, Accuracy 88.811%\n",
      "Epoch 30, Batch 253, LR 0.000015 Loss 4.640835, Accuracy 88.794%\n",
      "Epoch 30, Batch 254, LR 0.000015 Loss 4.640596, Accuracy 88.792%\n",
      "Epoch 30, Batch 255, LR 0.000015 Loss 4.637915, Accuracy 88.796%\n",
      "Epoch 30, Batch 256, LR 0.000015 Loss 4.639710, Accuracy 88.779%\n",
      "Epoch 30, Batch 257, LR 0.000015 Loss 4.637673, Accuracy 88.792%\n",
      "Epoch 30, Batch 258, LR 0.000015 Loss 4.636724, Accuracy 88.787%\n",
      "Epoch 30, Batch 259, LR 0.000015 Loss 4.637366, Accuracy 88.788%\n",
      "Epoch 30, Batch 260, LR 0.000015 Loss 4.637272, Accuracy 88.783%\n",
      "Epoch 30, Batch 261, LR 0.000015 Loss 4.637184, Accuracy 88.781%\n",
      "Epoch 30, Batch 262, LR 0.000015 Loss 4.635908, Accuracy 88.782%\n",
      "Epoch 30, Batch 263, LR 0.000015 Loss 4.636938, Accuracy 88.771%\n",
      "Epoch 30, Batch 264, LR 0.000015 Loss 4.636815, Accuracy 88.770%\n",
      "Epoch 30, Batch 265, LR 0.000015 Loss 4.637138, Accuracy 88.779%\n",
      "Epoch 30, Batch 266, LR 0.000015 Loss 4.639408, Accuracy 88.766%\n",
      "Epoch 30, Batch 267, LR 0.000015 Loss 4.640135, Accuracy 88.761%\n",
      "Epoch 30, Batch 268, LR 0.000015 Loss 4.638335, Accuracy 88.771%\n",
      "Epoch 30, Batch 269, LR 0.000015 Loss 4.636732, Accuracy 88.792%\n",
      "Epoch 30, Batch 270, LR 0.000015 Loss 4.637414, Accuracy 88.796%\n",
      "Epoch 30, Batch 271, LR 0.000015 Loss 4.636055, Accuracy 88.792%\n",
      "Epoch 30, Batch 272, LR 0.000015 Loss 4.634372, Accuracy 88.790%\n",
      "Epoch 30, Batch 273, LR 0.000015 Loss 4.634142, Accuracy 88.793%\n",
      "Epoch 30, Batch 274, LR 0.000015 Loss 4.634261, Accuracy 88.783%\n",
      "Epoch 30, Batch 275, LR 0.000015 Loss 4.635334, Accuracy 88.784%\n",
      "Epoch 30, Batch 276, LR 0.000015 Loss 4.636260, Accuracy 88.785%\n",
      "Epoch 30, Batch 277, LR 0.000015 Loss 4.635822, Accuracy 88.795%\n",
      "Epoch 30, Batch 278, LR 0.000015 Loss 4.632315, Accuracy 88.815%\n",
      "Epoch 30, Batch 279, LR 0.000015 Loss 4.632447, Accuracy 88.805%\n",
      "Epoch 30, Batch 280, LR 0.000015 Loss 4.629675, Accuracy 88.814%\n",
      "Epoch 30, Batch 281, LR 0.000015 Loss 4.630191, Accuracy 88.804%\n",
      "Epoch 30, Batch 282, LR 0.000015 Loss 4.631146, Accuracy 88.791%\n",
      "Epoch 30, Batch 283, LR 0.000015 Loss 4.634591, Accuracy 88.775%\n",
      "Epoch 30, Batch 284, LR 0.000015 Loss 4.633509, Accuracy 88.779%\n",
      "Epoch 30, Batch 285, LR 0.000015 Loss 4.632180, Accuracy 88.794%\n",
      "Epoch 30, Batch 286, LR 0.000015 Loss 4.632718, Accuracy 88.795%\n",
      "Epoch 30, Batch 287, LR 0.000015 Loss 4.634458, Accuracy 88.777%\n",
      "Epoch 30, Batch 288, LR 0.000015 Loss 4.633464, Accuracy 88.772%\n",
      "Epoch 30, Batch 289, LR 0.000015 Loss 4.633994, Accuracy 88.776%\n",
      "Epoch 30, Batch 290, LR 0.000015 Loss 4.634412, Accuracy 88.780%\n",
      "Epoch 30, Batch 291, LR 0.000015 Loss 4.634666, Accuracy 88.791%\n",
      "Epoch 30, Batch 292, LR 0.000015 Loss 4.636125, Accuracy 88.784%\n",
      "Epoch 30, Batch 293, LR 0.000015 Loss 4.636293, Accuracy 88.785%\n",
      "Epoch 30, Batch 294, LR 0.000015 Loss 4.635813, Accuracy 88.783%\n",
      "Epoch 30, Batch 295, LR 0.000015 Loss 4.637069, Accuracy 88.769%\n",
      "Epoch 30, Batch 296, LR 0.000015 Loss 4.635251, Accuracy 88.788%\n",
      "Epoch 30, Batch 297, LR 0.000015 Loss 4.636006, Accuracy 88.786%\n",
      "Epoch 30, Batch 298, LR 0.000015 Loss 4.638088, Accuracy 88.769%\n",
      "Epoch 30, Batch 299, LR 0.000015 Loss 4.638498, Accuracy 88.767%\n",
      "Epoch 30, Batch 300, LR 0.000015 Loss 4.636615, Accuracy 88.784%\n",
      "Epoch 30, Batch 301, LR 0.000015 Loss 4.631579, Accuracy 88.803%\n",
      "Epoch 30, Batch 302, LR 0.000015 Loss 4.631230, Accuracy 88.822%\n",
      "Epoch 30, Batch 303, LR 0.000015 Loss 4.629802, Accuracy 88.836%\n",
      "Epoch 30, Batch 304, LR 0.000015 Loss 4.630391, Accuracy 88.836%\n",
      "Epoch 30, Batch 305, LR 0.000015 Loss 4.629615, Accuracy 88.837%\n",
      "Epoch 30, Batch 306, LR 0.000015 Loss 4.629547, Accuracy 88.833%\n",
      "Epoch 30, Batch 307, LR 0.000015 Loss 4.631166, Accuracy 88.818%\n",
      "Epoch 30, Batch 308, LR 0.000015 Loss 4.630774, Accuracy 88.811%\n",
      "Epoch 30, Batch 309, LR 0.000015 Loss 4.630760, Accuracy 88.815%\n",
      "Epoch 30, Batch 310, LR 0.000015 Loss 4.628549, Accuracy 88.828%\n",
      "Epoch 30, Batch 311, LR 0.000015 Loss 4.629164, Accuracy 88.816%\n",
      "Epoch 30, Batch 312, LR 0.000015 Loss 4.631373, Accuracy 88.800%\n",
      "Epoch 30, Batch 313, LR 0.000015 Loss 4.632232, Accuracy 88.793%\n",
      "Epoch 30, Batch 314, LR 0.000015 Loss 4.632574, Accuracy 88.789%\n",
      "Epoch 30, Batch 315, LR 0.000015 Loss 4.630058, Accuracy 88.800%\n",
      "Epoch 30, Batch 316, LR 0.000015 Loss 4.628730, Accuracy 88.808%\n",
      "Epoch 30, Batch 317, LR 0.000015 Loss 4.629144, Accuracy 88.816%\n",
      "Epoch 30, Batch 318, LR 0.000015 Loss 4.628013, Accuracy 88.819%\n",
      "Epoch 30, Batch 319, LR 0.000015 Loss 4.629530, Accuracy 88.810%\n",
      "Epoch 30, Batch 320, LR 0.000015 Loss 4.630580, Accuracy 88.806%\n",
      "Epoch 30, Batch 321, LR 0.000015 Loss 4.632707, Accuracy 88.797%\n",
      "Epoch 30, Batch 322, LR 0.000015 Loss 4.635112, Accuracy 88.776%\n",
      "Epoch 30, Batch 323, LR 0.000015 Loss 4.635934, Accuracy 88.767%\n",
      "Epoch 30, Batch 324, LR 0.000015 Loss 4.635626, Accuracy 88.778%\n",
      "Epoch 30, Batch 325, LR 0.000015 Loss 4.635269, Accuracy 88.781%\n",
      "Epoch 30, Batch 326, LR 0.000015 Loss 4.635789, Accuracy 88.775%\n",
      "Epoch 30, Batch 327, LR 0.000015 Loss 4.635055, Accuracy 88.783%\n",
      "Epoch 30, Batch 328, LR 0.000015 Loss 4.634690, Accuracy 88.784%\n",
      "Epoch 30, Batch 329, LR 0.000015 Loss 4.637495, Accuracy 88.768%\n",
      "Epoch 30, Batch 330, LR 0.000015 Loss 4.638319, Accuracy 88.764%\n",
      "Epoch 30, Batch 331, LR 0.000015 Loss 4.637312, Accuracy 88.770%\n",
      "Epoch 30, Batch 332, LR 0.000015 Loss 4.638913, Accuracy 88.766%\n",
      "Epoch 30, Batch 333, LR 0.000015 Loss 4.636469, Accuracy 88.767%\n",
      "Epoch 30, Batch 334, LR 0.000015 Loss 4.636744, Accuracy 88.754%\n",
      "Epoch 30, Batch 335, LR 0.000015 Loss 4.637635, Accuracy 88.743%\n",
      "Epoch 30, Batch 336, LR 0.000015 Loss 4.638481, Accuracy 88.742%\n",
      "Epoch 30, Batch 337, LR 0.000015 Loss 4.639503, Accuracy 88.736%\n",
      "Epoch 30, Batch 338, LR 0.000015 Loss 4.640677, Accuracy 88.732%\n",
      "Epoch 30, Batch 339, LR 0.000015 Loss 4.642686, Accuracy 88.721%\n",
      "Epoch 30, Batch 340, LR 0.000015 Loss 4.640837, Accuracy 88.739%\n",
      "Epoch 30, Batch 341, LR 0.000015 Loss 4.638242, Accuracy 88.760%\n",
      "Epoch 30, Batch 342, LR 0.000015 Loss 4.638147, Accuracy 88.754%\n",
      "Epoch 30, Batch 343, LR 0.000015 Loss 4.637616, Accuracy 88.769%\n",
      "Epoch 30, Batch 344, LR 0.000015 Loss 4.638887, Accuracy 88.767%\n",
      "Epoch 30, Batch 345, LR 0.000015 Loss 4.639372, Accuracy 88.773%\n",
      "Epoch 30, Batch 346, LR 0.000015 Loss 4.639082, Accuracy 88.771%\n",
      "Epoch 30, Batch 347, LR 0.000015 Loss 4.639444, Accuracy 88.772%\n",
      "Epoch 30, Batch 348, LR 0.000015 Loss 4.638204, Accuracy 88.780%\n",
      "Epoch 30, Batch 349, LR 0.000015 Loss 4.635544, Accuracy 88.794%\n",
      "Epoch 30, Batch 350, LR 0.000015 Loss 4.634820, Accuracy 88.799%\n",
      "Epoch 30, Batch 351, LR 0.000015 Loss 4.634238, Accuracy 88.791%\n",
      "Epoch 30, Batch 352, LR 0.000015 Loss 4.634302, Accuracy 88.787%\n",
      "Epoch 30, Batch 353, LR 0.000015 Loss 4.633831, Accuracy 88.779%\n",
      "Epoch 30, Batch 354, LR 0.000015 Loss 4.635213, Accuracy 88.773%\n",
      "Epoch 30, Batch 355, LR 0.000015 Loss 4.635131, Accuracy 88.779%\n",
      "Epoch 30, Batch 356, LR 0.000015 Loss 4.633847, Accuracy 88.790%\n",
      "Epoch 30, Batch 357, LR 0.000015 Loss 4.634836, Accuracy 88.785%\n",
      "Epoch 30, Batch 358, LR 0.000015 Loss 4.636976, Accuracy 88.768%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 359, LR 0.000015 Loss 4.637156, Accuracy 88.758%\n",
      "Epoch 30, Batch 360, LR 0.000015 Loss 4.637260, Accuracy 88.761%\n",
      "Epoch 30, Batch 361, LR 0.000015 Loss 4.637775, Accuracy 88.762%\n",
      "Epoch 30, Batch 362, LR 0.000015 Loss 4.638323, Accuracy 88.767%\n",
      "Epoch 30, Batch 363, LR 0.000015 Loss 4.638683, Accuracy 88.765%\n",
      "Epoch 30, Batch 364, LR 0.000015 Loss 4.638465, Accuracy 88.762%\n",
      "Epoch 30, Batch 365, LR 0.000015 Loss 4.637667, Accuracy 88.771%\n",
      "Epoch 30, Batch 366, LR 0.000015 Loss 4.639653, Accuracy 88.755%\n",
      "Epoch 30, Batch 367, LR 0.000015 Loss 4.635860, Accuracy 88.779%\n",
      "Epoch 30, Batch 368, LR 0.000015 Loss 4.635104, Accuracy 88.782%\n",
      "Epoch 30, Batch 369, LR 0.000015 Loss 4.634409, Accuracy 88.794%\n",
      "Epoch 30, Batch 370, LR 0.000015 Loss 4.634521, Accuracy 88.792%\n",
      "Epoch 30, Batch 371, LR 0.000015 Loss 4.635304, Accuracy 88.797%\n",
      "Epoch 30, Batch 372, LR 0.000015 Loss 4.637162, Accuracy 88.781%\n",
      "Epoch 30, Batch 373, LR 0.000015 Loss 4.636843, Accuracy 88.784%\n",
      "Epoch 30, Batch 374, LR 0.000015 Loss 4.639034, Accuracy 88.766%\n",
      "Epoch 30, Batch 375, LR 0.000015 Loss 4.637909, Accuracy 88.771%\n",
      "Epoch 30, Batch 376, LR 0.000015 Loss 4.636778, Accuracy 88.774%\n",
      "Epoch 30, Batch 377, LR 0.000015 Loss 4.637953, Accuracy 88.766%\n",
      "Epoch 30, Batch 378, LR 0.000015 Loss 4.637598, Accuracy 88.765%\n",
      "Epoch 30, Batch 379, LR 0.000015 Loss 4.639408, Accuracy 88.757%\n",
      "Epoch 30, Batch 380, LR 0.000015 Loss 4.639292, Accuracy 88.754%\n",
      "Epoch 30, Batch 381, LR 0.000015 Loss 4.639529, Accuracy 88.757%\n",
      "Epoch 30, Batch 382, LR 0.000015 Loss 4.639411, Accuracy 88.762%\n",
      "Epoch 30, Batch 383, LR 0.000015 Loss 4.640857, Accuracy 88.746%\n",
      "Epoch 30, Batch 384, LR 0.000015 Loss 4.640128, Accuracy 88.747%\n",
      "Epoch 30, Batch 385, LR 0.000015 Loss 4.639764, Accuracy 88.754%\n",
      "Epoch 30, Batch 386, LR 0.000015 Loss 4.639483, Accuracy 88.751%\n",
      "Epoch 30, Batch 387, LR 0.000015 Loss 4.639296, Accuracy 88.754%\n",
      "Epoch 30, Batch 388, LR 0.000015 Loss 4.641962, Accuracy 88.744%\n",
      "Epoch 30, Batch 389, LR 0.000015 Loss 4.642853, Accuracy 88.743%\n",
      "Epoch 30, Batch 390, LR 0.000015 Loss 4.642090, Accuracy 88.754%\n",
      "Epoch 30, Batch 391, LR 0.000015 Loss 4.642061, Accuracy 88.759%\n",
      "Epoch 30, Batch 392, LR 0.000015 Loss 4.640438, Accuracy 88.772%\n",
      "Epoch 30, Batch 393, LR 0.000015 Loss 4.639823, Accuracy 88.776%\n",
      "Epoch 30, Batch 394, LR 0.000015 Loss 4.638294, Accuracy 88.791%\n",
      "Epoch 30, Batch 395, LR 0.000015 Loss 4.639236, Accuracy 88.786%\n",
      "Epoch 30, Batch 396, LR 0.000015 Loss 4.639471, Accuracy 88.778%\n",
      "Epoch 30, Batch 397, LR 0.000015 Loss 4.639411, Accuracy 88.783%\n",
      "Epoch 30, Batch 398, LR 0.000015 Loss 4.639418, Accuracy 88.784%\n",
      "Epoch 30, Batch 399, LR 0.000015 Loss 4.638364, Accuracy 88.790%\n",
      "Epoch 30, Batch 400, LR 0.000015 Loss 4.639192, Accuracy 88.789%\n",
      "Epoch 30, Batch 401, LR 0.000015 Loss 4.639713, Accuracy 88.786%\n",
      "Epoch 30, Batch 402, LR 0.000015 Loss 4.641382, Accuracy 88.787%\n",
      "Epoch 30, Batch 403, LR 0.000015 Loss 4.639991, Accuracy 88.795%\n",
      "Epoch 30, Batch 404, LR 0.000015 Loss 4.640392, Accuracy 88.800%\n",
      "Epoch 30, Batch 405, LR 0.000015 Loss 4.642719, Accuracy 88.789%\n",
      "Epoch 30, Batch 406, LR 0.000015 Loss 4.641004, Accuracy 88.801%\n",
      "Epoch 30, Batch 407, LR 0.000015 Loss 4.641717, Accuracy 88.801%\n",
      "Epoch 30, Batch 408, LR 0.000015 Loss 4.643554, Accuracy 88.783%\n",
      "Epoch 30, Batch 409, LR 0.000015 Loss 4.644731, Accuracy 88.772%\n",
      "Epoch 30, Batch 410, LR 0.000015 Loss 4.644972, Accuracy 88.761%\n",
      "Epoch 30, Batch 411, LR 0.000015 Loss 4.643140, Accuracy 88.774%\n",
      "Epoch 30, Batch 412, LR 0.000015 Loss 4.644881, Accuracy 88.770%\n",
      "Epoch 30, Batch 413, LR 0.000015 Loss 4.644913, Accuracy 88.767%\n",
      "Epoch 30, Batch 414, LR 0.000015 Loss 4.644071, Accuracy 88.778%\n",
      "Epoch 30, Batch 415, LR 0.000015 Loss 4.644431, Accuracy 88.780%\n",
      "Epoch 30, Batch 416, LR 0.000015 Loss 4.644299, Accuracy 88.786%\n",
      "Epoch 30, Batch 417, LR 0.000015 Loss 4.644503, Accuracy 88.780%\n",
      "Epoch 30, Batch 418, LR 0.000015 Loss 4.645137, Accuracy 88.771%\n",
      "Epoch 30, Batch 419, LR 0.000015 Loss 4.648036, Accuracy 88.766%\n",
      "Epoch 30, Batch 420, LR 0.000015 Loss 4.647702, Accuracy 88.772%\n",
      "Epoch 30, Batch 421, LR 0.000015 Loss 4.648838, Accuracy 88.762%\n",
      "Epoch 30, Batch 422, LR 0.000015 Loss 4.648123, Accuracy 88.763%\n",
      "Epoch 30, Batch 423, LR 0.000015 Loss 4.648499, Accuracy 88.771%\n",
      "Epoch 30, Batch 424, LR 0.000015 Loss 4.647281, Accuracy 88.775%\n",
      "Epoch 30, Batch 425, LR 0.000015 Loss 4.648166, Accuracy 88.767%\n",
      "Epoch 30, Batch 426, LR 0.000015 Loss 4.647846, Accuracy 88.775%\n",
      "Epoch 30, Batch 427, LR 0.000015 Loss 4.648908, Accuracy 88.779%\n",
      "Epoch 30, Batch 428, LR 0.000015 Loss 4.650077, Accuracy 88.783%\n",
      "Epoch 30, Batch 429, LR 0.000015 Loss 4.647484, Accuracy 88.797%\n",
      "Epoch 30, Batch 430, LR 0.000015 Loss 4.647563, Accuracy 88.797%\n",
      "Epoch 30, Batch 431, LR 0.000015 Loss 4.648738, Accuracy 88.791%\n",
      "Epoch 30, Batch 432, LR 0.000015 Loss 4.649199, Accuracy 88.791%\n",
      "Epoch 30, Batch 433, LR 0.000015 Loss 4.649682, Accuracy 88.788%\n",
      "Epoch 30, Batch 434, LR 0.000015 Loss 4.648500, Accuracy 88.791%\n",
      "Epoch 30, Batch 435, LR 0.000015 Loss 4.648717, Accuracy 88.784%\n",
      "Epoch 30, Batch 436, LR 0.000015 Loss 4.649736, Accuracy 88.779%\n",
      "Epoch 30, Batch 437, LR 0.000015 Loss 4.650028, Accuracy 88.776%\n",
      "Epoch 30, Batch 438, LR 0.000015 Loss 4.650686, Accuracy 88.774%\n",
      "Epoch 30, Batch 439, LR 0.000015 Loss 4.650262, Accuracy 88.771%\n",
      "Epoch 30, Batch 440, LR 0.000015 Loss 4.650607, Accuracy 88.773%\n",
      "Epoch 30, Batch 441, LR 0.000015 Loss 4.650856, Accuracy 88.772%\n",
      "Epoch 30, Batch 442, LR 0.000015 Loss 4.651647, Accuracy 88.773%\n",
      "Epoch 30, Batch 443, LR 0.000015 Loss 4.649954, Accuracy 88.772%\n",
      "Epoch 30, Batch 444, LR 0.000015 Loss 4.651719, Accuracy 88.758%\n",
      "Epoch 30, Batch 445, LR 0.000015 Loss 4.653784, Accuracy 88.750%\n",
      "Epoch 30, Batch 446, LR 0.000015 Loss 4.654705, Accuracy 88.747%\n",
      "Epoch 30, Batch 447, LR 0.000015 Loss 4.655291, Accuracy 88.750%\n",
      "Epoch 30, Batch 448, LR 0.000015 Loss 4.654021, Accuracy 88.763%\n",
      "Epoch 30, Batch 449, LR 0.000015 Loss 4.652334, Accuracy 88.770%\n",
      "Epoch 30, Batch 450, LR 0.000015 Loss 4.652426, Accuracy 88.780%\n",
      "Epoch 30, Batch 451, LR 0.000015 Loss 4.653886, Accuracy 88.775%\n",
      "Epoch 30, Batch 452, LR 0.000015 Loss 4.654985, Accuracy 88.776%\n",
      "Epoch 30, Batch 453, LR 0.000015 Loss 4.655831, Accuracy 88.774%\n",
      "Epoch 30, Batch 454, LR 0.000015 Loss 4.653913, Accuracy 88.791%\n",
      "Epoch 30, Batch 455, LR 0.000015 Loss 4.655166, Accuracy 88.776%\n",
      "Epoch 30, Batch 456, LR 0.000015 Loss 4.655791, Accuracy 88.775%\n",
      "Epoch 30, Batch 457, LR 0.000015 Loss 4.656286, Accuracy 88.777%\n",
      "Epoch 30, Batch 458, LR 0.000015 Loss 4.657003, Accuracy 88.769%\n",
      "Epoch 30, Batch 459, LR 0.000015 Loss 4.655787, Accuracy 88.771%\n",
      "Epoch 30, Batch 460, LR 0.000015 Loss 4.656680, Accuracy 88.765%\n",
      "Epoch 30, Batch 461, LR 0.000015 Loss 4.655648, Accuracy 88.771%\n",
      "Epoch 30, Batch 462, LR 0.000015 Loss 4.654889, Accuracy 88.773%\n",
      "Epoch 30, Batch 463, LR 0.000015 Loss 4.653855, Accuracy 88.774%\n",
      "Epoch 30, Batch 464, LR 0.000015 Loss 4.652734, Accuracy 88.780%\n",
      "Epoch 30, Batch 465, LR 0.000015 Loss 4.653374, Accuracy 88.775%\n",
      "Epoch 30, Batch 466, LR 0.000015 Loss 4.651324, Accuracy 88.783%\n",
      "Epoch 30, Batch 467, LR 0.000015 Loss 4.649796, Accuracy 88.785%\n",
      "Epoch 30, Batch 468, LR 0.000015 Loss 4.650468, Accuracy 88.784%\n",
      "Epoch 30, Batch 469, LR 0.000015 Loss 4.650282, Accuracy 88.791%\n",
      "Epoch 30, Batch 470, LR 0.000015 Loss 4.650889, Accuracy 88.788%\n",
      "Epoch 30, Batch 471, LR 0.000015 Loss 4.651424, Accuracy 88.790%\n",
      "Epoch 30, Batch 472, LR 0.000015 Loss 4.651366, Accuracy 88.796%\n",
      "Epoch 30, Batch 473, LR 0.000015 Loss 4.650484, Accuracy 88.797%\n",
      "Epoch 30, Batch 474, LR 0.000015 Loss 4.649973, Accuracy 88.797%\n",
      "Epoch 30, Batch 475, LR 0.000015 Loss 4.652052, Accuracy 88.788%\n",
      "Epoch 30, Batch 476, LR 0.000015 Loss 4.652237, Accuracy 88.790%\n",
      "Epoch 30, Batch 477, LR 0.000015 Loss 4.652163, Accuracy 88.789%\n",
      "Epoch 30, Batch 478, LR 0.000015 Loss 4.652770, Accuracy 88.785%\n",
      "Epoch 30, Batch 479, LR 0.000015 Loss 4.651917, Accuracy 88.782%\n",
      "Epoch 30, Batch 480, LR 0.000015 Loss 4.651747, Accuracy 88.784%\n",
      "Epoch 30, Batch 481, LR 0.000015 Loss 4.651544, Accuracy 88.782%\n",
      "Epoch 30, Batch 482, LR 0.000015 Loss 4.651385, Accuracy 88.782%\n",
      "Epoch 30, Batch 483, LR 0.000015 Loss 4.651010, Accuracy 88.781%\n",
      "Epoch 30, Batch 484, LR 0.000015 Loss 4.650054, Accuracy 88.783%\n",
      "Epoch 30, Batch 485, LR 0.000015 Loss 4.650818, Accuracy 88.776%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 486, LR 0.000015 Loss 4.651138, Accuracy 88.773%\n",
      "Epoch 30, Batch 487, LR 0.000015 Loss 4.650633, Accuracy 88.780%\n",
      "Epoch 30, Batch 488, LR 0.000015 Loss 4.649469, Accuracy 88.786%\n",
      "Epoch 30, Batch 489, LR 0.000015 Loss 4.649685, Accuracy 88.783%\n",
      "Epoch 30, Batch 490, LR 0.000015 Loss 4.650830, Accuracy 88.777%\n",
      "Epoch 30, Batch 491, LR 0.000015 Loss 4.650558, Accuracy 88.776%\n",
      "Epoch 30, Batch 492, LR 0.000015 Loss 4.650393, Accuracy 88.775%\n",
      "Epoch 30, Batch 493, LR 0.000015 Loss 4.649136, Accuracy 88.784%\n",
      "Epoch 30, Batch 494, LR 0.000015 Loss 4.649244, Accuracy 88.784%\n",
      "Epoch 30, Batch 495, LR 0.000015 Loss 4.649844, Accuracy 88.777%\n",
      "Epoch 30, Batch 496, LR 0.000015 Loss 4.649498, Accuracy 88.777%\n",
      "Epoch 30, Batch 497, LR 0.000015 Loss 4.650683, Accuracy 88.769%\n",
      "Epoch 30, Batch 498, LR 0.000015 Loss 4.651134, Accuracy 88.768%\n",
      "Epoch 30, Batch 499, LR 0.000015 Loss 4.652132, Accuracy 88.759%\n",
      "Epoch 30, Batch 500, LR 0.000015 Loss 4.652230, Accuracy 88.763%\n",
      "Epoch 30, Batch 501, LR 0.000015 Loss 4.651818, Accuracy 88.758%\n",
      "Epoch 30, Batch 502, LR 0.000015 Loss 4.653084, Accuracy 88.754%\n",
      "Epoch 30, Batch 503, LR 0.000015 Loss 4.653322, Accuracy 88.760%\n",
      "Epoch 30, Batch 504, LR 0.000015 Loss 4.653393, Accuracy 88.760%\n",
      "Epoch 30, Batch 505, LR 0.000015 Loss 4.654924, Accuracy 88.753%\n",
      "Epoch 30, Batch 506, LR 0.000015 Loss 4.654202, Accuracy 88.765%\n",
      "Epoch 30, Batch 507, LR 0.000015 Loss 4.653665, Accuracy 88.768%\n",
      "Epoch 30, Batch 508, LR 0.000015 Loss 4.654147, Accuracy 88.767%\n",
      "Epoch 30, Batch 509, LR 0.000015 Loss 4.655248, Accuracy 88.762%\n",
      "Epoch 30, Batch 510, LR 0.000015 Loss 4.654364, Accuracy 88.768%\n",
      "Epoch 30, Batch 511, LR 0.000015 Loss 4.653546, Accuracy 88.767%\n",
      "Epoch 30, Batch 512, LR 0.000015 Loss 4.653428, Accuracy 88.770%\n",
      "Epoch 30, Batch 513, LR 0.000015 Loss 4.653905, Accuracy 88.773%\n",
      "Epoch 30, Batch 514, LR 0.000015 Loss 4.653179, Accuracy 88.781%\n",
      "Epoch 30, Batch 515, LR 0.000015 Loss 4.652252, Accuracy 88.785%\n",
      "Epoch 30, Batch 516, LR 0.000015 Loss 4.653015, Accuracy 88.779%\n",
      "Epoch 30, Batch 517, LR 0.000015 Loss 4.652327, Accuracy 88.778%\n",
      "Epoch 30, Batch 518, LR 0.000015 Loss 4.652219, Accuracy 88.782%\n",
      "Epoch 30, Batch 519, LR 0.000015 Loss 4.652244, Accuracy 88.778%\n",
      "Epoch 30, Batch 520, LR 0.000015 Loss 4.652231, Accuracy 88.774%\n",
      "Epoch 30, Batch 521, LR 0.000015 Loss 4.651230, Accuracy 88.781%\n",
      "Epoch 30, Batch 522, LR 0.000015 Loss 4.650900, Accuracy 88.778%\n",
      "Epoch 30, Batch 523, LR 0.000015 Loss 4.650002, Accuracy 88.780%\n",
      "Epoch 30, Batch 524, LR 0.000015 Loss 4.649031, Accuracy 88.785%\n",
      "Epoch 30, Batch 525, LR 0.000015 Loss 4.649648, Accuracy 88.780%\n",
      "Epoch 30, Batch 526, LR 0.000015 Loss 4.649215, Accuracy 88.782%\n",
      "Epoch 30, Batch 527, LR 0.000015 Loss 4.648601, Accuracy 88.785%\n",
      "Epoch 30, Batch 528, LR 0.000015 Loss 4.647945, Accuracy 88.792%\n",
      "Epoch 30, Batch 529, LR 0.000015 Loss 4.648857, Accuracy 88.795%\n",
      "Epoch 30, Batch 530, LR 0.000015 Loss 4.649542, Accuracy 88.790%\n",
      "Epoch 30, Batch 531, LR 0.000015 Loss 4.650115, Accuracy 88.787%\n",
      "Epoch 30, Batch 532, LR 0.000015 Loss 4.649647, Accuracy 88.783%\n",
      "Epoch 30, Batch 533, LR 0.000015 Loss 4.649361, Accuracy 88.787%\n",
      "Epoch 30, Batch 534, LR 0.000015 Loss 4.649814, Accuracy 88.789%\n",
      "Epoch 30, Batch 535, LR 0.000015 Loss 4.650230, Accuracy 88.789%\n",
      "Epoch 30, Batch 536, LR 0.000015 Loss 4.650500, Accuracy 88.783%\n",
      "Epoch 30, Batch 537, LR 0.000015 Loss 4.650553, Accuracy 88.777%\n",
      "Epoch 30, Batch 538, LR 0.000015 Loss 4.650493, Accuracy 88.774%\n",
      "Epoch 30, Batch 539, LR 0.000015 Loss 4.650316, Accuracy 88.776%\n",
      "Epoch 30, Batch 540, LR 0.000015 Loss 4.650233, Accuracy 88.785%\n",
      "Epoch 30, Batch 541, LR 0.000015 Loss 4.650534, Accuracy 88.784%\n",
      "Epoch 30, Batch 542, LR 0.000015 Loss 4.650999, Accuracy 88.792%\n",
      "Epoch 30, Batch 543, LR 0.000015 Loss 4.650489, Accuracy 88.795%\n",
      "Epoch 30, Batch 544, LR 0.000015 Loss 4.650733, Accuracy 88.793%\n",
      "Epoch 30, Batch 545, LR 0.000015 Loss 4.649050, Accuracy 88.799%\n",
      "Epoch 30, Batch 546, LR 0.000015 Loss 4.649349, Accuracy 88.798%\n",
      "Epoch 30, Batch 547, LR 0.000015 Loss 4.649423, Accuracy 88.801%\n",
      "Epoch 30, Batch 548, LR 0.000015 Loss 4.649779, Accuracy 88.803%\n",
      "Epoch 30, Batch 549, LR 0.000015 Loss 4.649606, Accuracy 88.805%\n",
      "Epoch 30, Batch 550, LR 0.000015 Loss 4.648804, Accuracy 88.812%\n",
      "Epoch 30, Batch 551, LR 0.000015 Loss 4.649703, Accuracy 88.804%\n",
      "Epoch 30, Batch 552, LR 0.000015 Loss 4.649313, Accuracy 88.803%\n",
      "Epoch 30, Batch 553, LR 0.000015 Loss 4.649075, Accuracy 88.801%\n",
      "Epoch 30, Batch 554, LR 0.000015 Loss 4.649544, Accuracy 88.796%\n",
      "Epoch 30, Batch 555, LR 0.000015 Loss 4.649712, Accuracy 88.799%\n",
      "Epoch 30, Batch 556, LR 0.000015 Loss 4.650202, Accuracy 88.803%\n",
      "Epoch 30, Batch 557, LR 0.000015 Loss 4.649865, Accuracy 88.800%\n",
      "Epoch 30, Batch 558, LR 0.000015 Loss 4.649150, Accuracy 88.802%\n",
      "Epoch 30, Batch 559, LR 0.000015 Loss 4.648957, Accuracy 88.798%\n",
      "Epoch 30, Batch 560, LR 0.000015 Loss 4.648360, Accuracy 88.806%\n",
      "Epoch 30, Batch 561, LR 0.000015 Loss 4.649305, Accuracy 88.809%\n",
      "Epoch 30, Batch 562, LR 0.000015 Loss 4.649877, Accuracy 88.807%\n",
      "Epoch 30, Batch 563, LR 0.000015 Loss 4.651939, Accuracy 88.799%\n",
      "Epoch 30, Batch 564, LR 0.000015 Loss 4.651735, Accuracy 88.797%\n",
      "Epoch 30, Batch 565, LR 0.000015 Loss 4.650437, Accuracy 88.801%\n",
      "Epoch 30, Batch 566, LR 0.000015 Loss 4.651809, Accuracy 88.797%\n",
      "Epoch 30, Batch 567, LR 0.000015 Loss 4.652501, Accuracy 88.784%\n",
      "Epoch 30, Batch 568, LR 0.000015 Loss 4.651604, Accuracy 88.789%\n",
      "Epoch 30, Batch 569, LR 0.000015 Loss 4.651980, Accuracy 88.787%\n",
      "Epoch 30, Batch 570, LR 0.000015 Loss 4.652587, Accuracy 88.782%\n",
      "Epoch 30, Batch 571, LR 0.000015 Loss 4.652435, Accuracy 88.783%\n",
      "Epoch 30, Batch 572, LR 0.000015 Loss 4.653365, Accuracy 88.776%\n",
      "Epoch 30, Batch 573, LR 0.000015 Loss 4.652241, Accuracy 88.782%\n",
      "Epoch 30, Batch 574, LR 0.000015 Loss 4.652542, Accuracy 88.779%\n",
      "Epoch 30, Batch 575, LR 0.000015 Loss 4.652886, Accuracy 88.779%\n",
      "Epoch 30, Batch 576, LR 0.000014 Loss 4.653573, Accuracy 88.779%\n",
      "Epoch 30, Batch 577, LR 0.000014 Loss 4.652517, Accuracy 88.781%\n",
      "Epoch 30, Batch 578, LR 0.000014 Loss 4.653305, Accuracy 88.775%\n",
      "Epoch 30, Batch 579, LR 0.000014 Loss 4.653508, Accuracy 88.772%\n",
      "Epoch 30, Batch 580, LR 0.000014 Loss 4.653541, Accuracy 88.773%\n",
      "Epoch 30, Batch 581, LR 0.000014 Loss 4.652936, Accuracy 88.776%\n",
      "Epoch 30, Batch 582, LR 0.000014 Loss 4.652485, Accuracy 88.782%\n",
      "Epoch 30, Batch 583, LR 0.000014 Loss 4.651303, Accuracy 88.789%\n",
      "Epoch 30, Batch 584, LR 0.000014 Loss 4.651913, Accuracy 88.788%\n",
      "Epoch 30, Batch 585, LR 0.000014 Loss 4.652743, Accuracy 88.789%\n",
      "Epoch 30, Batch 586, LR 0.000014 Loss 4.653259, Accuracy 88.793%\n",
      "Epoch 30, Batch 587, LR 0.000014 Loss 4.653139, Accuracy 88.796%\n",
      "Epoch 30, Batch 588, LR 0.000014 Loss 4.652787, Accuracy 88.801%\n",
      "Epoch 30, Batch 589, LR 0.000014 Loss 4.652875, Accuracy 88.800%\n",
      "Epoch 30, Batch 590, LR 0.000014 Loss 4.653186, Accuracy 88.802%\n",
      "Epoch 30, Batch 591, LR 0.000014 Loss 4.653666, Accuracy 88.801%\n",
      "Epoch 30, Batch 592, LR 0.000014 Loss 4.653346, Accuracy 88.801%\n",
      "Epoch 30, Batch 593, LR 0.000014 Loss 4.654067, Accuracy 88.800%\n",
      "Epoch 30, Batch 594, LR 0.000014 Loss 4.653537, Accuracy 88.806%\n",
      "Epoch 30, Batch 595, LR 0.000014 Loss 4.654142, Accuracy 88.797%\n",
      "Epoch 30, Batch 596, LR 0.000014 Loss 4.654093, Accuracy 88.794%\n",
      "Epoch 30, Batch 597, LR 0.000014 Loss 4.653885, Accuracy 88.799%\n",
      "Epoch 30, Batch 598, LR 0.000014 Loss 4.654418, Accuracy 88.796%\n",
      "Epoch 30, Batch 599, LR 0.000014 Loss 4.653573, Accuracy 88.799%\n",
      "Epoch 30, Batch 600, LR 0.000014 Loss 4.653555, Accuracy 88.797%\n",
      "Epoch 30, Batch 601, LR 0.000014 Loss 4.654342, Accuracy 88.786%\n",
      "Epoch 30, Batch 602, LR 0.000014 Loss 4.653962, Accuracy 88.791%\n",
      "Epoch 30, Batch 603, LR 0.000014 Loss 4.653003, Accuracy 88.797%\n",
      "Epoch 30, Batch 604, LR 0.000014 Loss 4.654185, Accuracy 88.795%\n",
      "Epoch 30, Batch 605, LR 0.000014 Loss 4.654645, Accuracy 88.793%\n",
      "Epoch 30, Batch 606, LR 0.000014 Loss 4.654365, Accuracy 88.803%\n",
      "Epoch 30, Batch 607, LR 0.000014 Loss 4.655200, Accuracy 88.799%\n",
      "Epoch 30, Batch 608, LR 0.000014 Loss 4.655756, Accuracy 88.800%\n",
      "Epoch 30, Batch 609, LR 0.000014 Loss 4.656154, Accuracy 88.802%\n",
      "Epoch 30, Batch 610, LR 0.000014 Loss 4.655958, Accuracy 88.799%\n",
      "Epoch 30, Batch 611, LR 0.000014 Loss 4.655070, Accuracy 88.805%\n",
      "Epoch 30, Batch 612, LR 0.000014 Loss 4.654789, Accuracy 88.810%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 613, LR 0.000014 Loss 4.654822, Accuracy 88.810%\n",
      "Epoch 30, Batch 614, LR 0.000014 Loss 4.655320, Accuracy 88.805%\n",
      "Epoch 30, Batch 615, LR 0.000014 Loss 4.656199, Accuracy 88.796%\n",
      "Epoch 30, Batch 616, LR 0.000014 Loss 4.656736, Accuracy 88.797%\n",
      "Epoch 30, Batch 617, LR 0.000014 Loss 4.656658, Accuracy 88.799%\n",
      "Epoch 30, Batch 618, LR 0.000014 Loss 4.656403, Accuracy 88.803%\n",
      "Epoch 30, Batch 619, LR 0.000014 Loss 4.655956, Accuracy 88.806%\n",
      "Epoch 30, Batch 620, LR 0.000014 Loss 4.656281, Accuracy 88.802%\n",
      "Epoch 30, Batch 621, LR 0.000014 Loss 4.656728, Accuracy 88.802%\n",
      "Epoch 30, Batch 622, LR 0.000014 Loss 4.656302, Accuracy 88.805%\n",
      "Epoch 30, Batch 623, LR 0.000014 Loss 4.656576, Accuracy 88.804%\n",
      "Epoch 30, Batch 624, LR 0.000014 Loss 4.656190, Accuracy 88.807%\n",
      "Epoch 30, Batch 625, LR 0.000014 Loss 4.656077, Accuracy 88.806%\n",
      "Epoch 30, Batch 626, LR 0.000014 Loss 4.655499, Accuracy 88.803%\n",
      "Epoch 30, Batch 627, LR 0.000014 Loss 4.655969, Accuracy 88.810%\n",
      "Epoch 30, Batch 628, LR 0.000014 Loss 4.655495, Accuracy 88.814%\n",
      "Epoch 30, Batch 629, LR 0.000014 Loss 4.654018, Accuracy 88.817%\n",
      "Epoch 30, Batch 630, LR 0.000014 Loss 4.653588, Accuracy 88.819%\n",
      "Epoch 30, Batch 631, LR 0.000014 Loss 4.653607, Accuracy 88.820%\n",
      "Epoch 30, Batch 632, LR 0.000014 Loss 4.654320, Accuracy 88.815%\n",
      "Epoch 30, Batch 633, LR 0.000014 Loss 4.654445, Accuracy 88.812%\n",
      "Epoch 30, Batch 634, LR 0.000014 Loss 4.654501, Accuracy 88.814%\n",
      "Epoch 30, Batch 635, LR 0.000014 Loss 4.653854, Accuracy 88.813%\n",
      "Epoch 30, Batch 636, LR 0.000014 Loss 4.652356, Accuracy 88.814%\n",
      "Epoch 30, Batch 637, LR 0.000014 Loss 4.652877, Accuracy 88.817%\n",
      "Epoch 30, Batch 638, LR 0.000014 Loss 4.652649, Accuracy 88.816%\n",
      "Epoch 30, Batch 639, LR 0.000014 Loss 4.651643, Accuracy 88.823%\n",
      "Epoch 30, Batch 640, LR 0.000014 Loss 4.651913, Accuracy 88.827%\n",
      "Epoch 30, Batch 641, LR 0.000014 Loss 4.652884, Accuracy 88.822%\n",
      "Epoch 30, Batch 642, LR 0.000014 Loss 4.652995, Accuracy 88.823%\n",
      "Epoch 30, Batch 643, LR 0.000014 Loss 4.653576, Accuracy 88.817%\n",
      "Epoch 30, Batch 644, LR 0.000014 Loss 4.653750, Accuracy 88.816%\n",
      "Epoch 30, Batch 645, LR 0.000014 Loss 4.654676, Accuracy 88.815%\n",
      "Epoch 30, Batch 646, LR 0.000014 Loss 4.654738, Accuracy 88.817%\n",
      "Epoch 30, Batch 647, LR 0.000014 Loss 4.654475, Accuracy 88.815%\n",
      "Epoch 30, Batch 648, LR 0.000014 Loss 4.654374, Accuracy 88.817%\n",
      "Epoch 30, Batch 649, LR 0.000014 Loss 4.653839, Accuracy 88.819%\n",
      "Epoch 30, Batch 650, LR 0.000014 Loss 4.653632, Accuracy 88.819%\n",
      "Epoch 30, Batch 651, LR 0.000014 Loss 4.653271, Accuracy 88.820%\n",
      "Epoch 30, Batch 652, LR 0.000014 Loss 4.654036, Accuracy 88.823%\n",
      "Epoch 30, Batch 653, LR 0.000014 Loss 4.653363, Accuracy 88.829%\n",
      "Epoch 30, Batch 654, LR 0.000014 Loss 4.654296, Accuracy 88.822%\n",
      "Epoch 30, Batch 655, LR 0.000014 Loss 4.654100, Accuracy 88.829%\n",
      "Epoch 30, Batch 656, LR 0.000014 Loss 4.652523, Accuracy 88.837%\n",
      "Epoch 30, Batch 657, LR 0.000014 Loss 4.652240, Accuracy 88.835%\n",
      "Epoch 30, Batch 658, LR 0.000014 Loss 4.652349, Accuracy 88.839%\n",
      "Epoch 30, Batch 659, LR 0.000014 Loss 4.652452, Accuracy 88.843%\n",
      "Epoch 30, Batch 660, LR 0.000014 Loss 4.652512, Accuracy 88.841%\n",
      "Epoch 30, Batch 661, LR 0.000014 Loss 4.652353, Accuracy 88.838%\n",
      "Epoch 30, Batch 662, LR 0.000014 Loss 4.652402, Accuracy 88.831%\n",
      "Epoch 30, Batch 663, LR 0.000014 Loss 4.652706, Accuracy 88.830%\n",
      "Epoch 30, Batch 664, LR 0.000014 Loss 4.652098, Accuracy 88.831%\n",
      "Epoch 30, Batch 665, LR 0.000014 Loss 4.652029, Accuracy 88.833%\n",
      "Epoch 30, Batch 666, LR 0.000014 Loss 4.651659, Accuracy 88.835%\n",
      "Epoch 30, Batch 667, LR 0.000014 Loss 4.651581, Accuracy 88.836%\n",
      "Epoch 30, Batch 668, LR 0.000014 Loss 4.650873, Accuracy 88.832%\n",
      "Epoch 30, Batch 669, LR 0.000014 Loss 4.651296, Accuracy 88.831%\n",
      "Epoch 30, Batch 670, LR 0.000014 Loss 4.652406, Accuracy 88.826%\n",
      "Epoch 30, Batch 671, LR 0.000014 Loss 4.651669, Accuracy 88.828%\n",
      "Epoch 30, Batch 672, LR 0.000014 Loss 4.652514, Accuracy 88.821%\n",
      "Epoch 30, Batch 673, LR 0.000014 Loss 4.653336, Accuracy 88.812%\n",
      "Epoch 30, Batch 674, LR 0.000014 Loss 4.652874, Accuracy 88.817%\n",
      "Epoch 30, Batch 675, LR 0.000014 Loss 4.653247, Accuracy 88.818%\n",
      "Epoch 30, Batch 676, LR 0.000014 Loss 4.652875, Accuracy 88.817%\n",
      "Epoch 30, Batch 677, LR 0.000014 Loss 4.652498, Accuracy 88.820%\n",
      "Epoch 30, Batch 678, LR 0.000014 Loss 4.651478, Accuracy 88.827%\n",
      "Epoch 30, Batch 679, LR 0.000014 Loss 4.651275, Accuracy 88.827%\n",
      "Epoch 30, Batch 680, LR 0.000014 Loss 4.651363, Accuracy 88.826%\n",
      "Epoch 30, Batch 681, LR 0.000014 Loss 4.652751, Accuracy 88.820%\n",
      "Epoch 30, Batch 682, LR 0.000014 Loss 4.652044, Accuracy 88.824%\n",
      "Epoch 30, Batch 683, LR 0.000014 Loss 4.652165, Accuracy 88.827%\n",
      "Epoch 30, Batch 684, LR 0.000014 Loss 4.651338, Accuracy 88.832%\n",
      "Epoch 30, Batch 685, LR 0.000014 Loss 4.650758, Accuracy 88.834%\n",
      "Epoch 30, Batch 686, LR 0.000014 Loss 4.652219, Accuracy 88.832%\n",
      "Epoch 30, Batch 687, LR 0.000014 Loss 4.652734, Accuracy 88.833%\n",
      "Epoch 30, Batch 688, LR 0.000014 Loss 4.652856, Accuracy 88.829%\n",
      "Epoch 30, Batch 689, LR 0.000014 Loss 4.652383, Accuracy 88.827%\n",
      "Epoch 30, Batch 690, LR 0.000014 Loss 4.652875, Accuracy 88.827%\n",
      "Epoch 30, Batch 691, LR 0.000014 Loss 4.653015, Accuracy 88.828%\n",
      "Epoch 30, Batch 692, LR 0.000014 Loss 4.652496, Accuracy 88.827%\n",
      "Epoch 30, Batch 693, LR 0.000014 Loss 4.652912, Accuracy 88.825%\n",
      "Epoch 30, Batch 694, LR 0.000014 Loss 4.653158, Accuracy 88.824%\n",
      "Epoch 30, Batch 695, LR 0.000014 Loss 4.653162, Accuracy 88.823%\n",
      "Epoch 30, Batch 696, LR 0.000014 Loss 4.653226, Accuracy 88.822%\n",
      "Epoch 30, Batch 697, LR 0.000014 Loss 4.653418, Accuracy 88.824%\n",
      "Epoch 30, Batch 698, LR 0.000014 Loss 4.652944, Accuracy 88.826%\n",
      "Epoch 30, Batch 699, LR 0.000014 Loss 4.652494, Accuracy 88.830%\n",
      "Epoch 30, Batch 700, LR 0.000014 Loss 4.653022, Accuracy 88.819%\n",
      "Epoch 30, Batch 701, LR 0.000014 Loss 4.652883, Accuracy 88.826%\n",
      "Epoch 30, Batch 702, LR 0.000014 Loss 4.653267, Accuracy 88.829%\n",
      "Epoch 30, Batch 703, LR 0.000014 Loss 4.653629, Accuracy 88.831%\n",
      "Epoch 30, Batch 704, LR 0.000014 Loss 4.652980, Accuracy 88.833%\n",
      "Epoch 30, Batch 705, LR 0.000014 Loss 4.653139, Accuracy 88.838%\n",
      "Epoch 30, Batch 706, LR 0.000014 Loss 4.653319, Accuracy 88.836%\n",
      "Epoch 30, Batch 707, LR 0.000014 Loss 4.655026, Accuracy 88.828%\n",
      "Epoch 30, Batch 708, LR 0.000014 Loss 4.655421, Accuracy 88.834%\n",
      "Epoch 30, Batch 709, LR 0.000014 Loss 4.656177, Accuracy 88.837%\n",
      "Epoch 30, Batch 710, LR 0.000014 Loss 4.657167, Accuracy 88.835%\n",
      "Epoch 30, Batch 711, LR 0.000014 Loss 4.656410, Accuracy 88.844%\n",
      "Epoch 30, Batch 712, LR 0.000014 Loss 4.656463, Accuracy 88.843%\n",
      "Epoch 30, Batch 713, LR 0.000014 Loss 4.656067, Accuracy 88.842%\n",
      "Epoch 30, Batch 714, LR 0.000014 Loss 4.656004, Accuracy 88.837%\n",
      "Epoch 30, Batch 715, LR 0.000014 Loss 4.656491, Accuracy 88.836%\n",
      "Epoch 30, Batch 716, LR 0.000014 Loss 4.656240, Accuracy 88.837%\n",
      "Epoch 30, Batch 717, LR 0.000014 Loss 4.656724, Accuracy 88.830%\n",
      "Epoch 30, Batch 718, LR 0.000014 Loss 4.658260, Accuracy 88.825%\n",
      "Epoch 30, Batch 719, LR 0.000014 Loss 4.657458, Accuracy 88.828%\n",
      "Epoch 30, Batch 720, LR 0.000014 Loss 4.657494, Accuracy 88.823%\n",
      "Epoch 30, Batch 721, LR 0.000014 Loss 4.658175, Accuracy 88.820%\n",
      "Epoch 30, Batch 722, LR 0.000014 Loss 4.658284, Accuracy 88.818%\n",
      "Epoch 30, Batch 723, LR 0.000014 Loss 4.657467, Accuracy 88.822%\n",
      "Epoch 30, Batch 724, LR 0.000014 Loss 4.656963, Accuracy 88.822%\n",
      "Epoch 30, Batch 725, LR 0.000014 Loss 4.656367, Accuracy 88.827%\n",
      "Epoch 30, Batch 726, LR 0.000014 Loss 4.655547, Accuracy 88.830%\n",
      "Epoch 30, Batch 727, LR 0.000014 Loss 4.655819, Accuracy 88.825%\n",
      "Epoch 30, Batch 728, LR 0.000014 Loss 4.656516, Accuracy 88.826%\n",
      "Epoch 30, Batch 729, LR 0.000014 Loss 4.655718, Accuracy 88.831%\n",
      "Epoch 30, Batch 730, LR 0.000014 Loss 4.654418, Accuracy 88.832%\n",
      "Epoch 30, Batch 731, LR 0.000014 Loss 4.654037, Accuracy 88.836%\n",
      "Epoch 30, Batch 732, LR 0.000014 Loss 4.654796, Accuracy 88.833%\n",
      "Epoch 30, Batch 733, LR 0.000014 Loss 4.655560, Accuracy 88.833%\n",
      "Epoch 30, Batch 734, LR 0.000014 Loss 4.655085, Accuracy 88.835%\n",
      "Epoch 30, Batch 735, LR 0.000014 Loss 4.655358, Accuracy 88.834%\n",
      "Epoch 30, Batch 736, LR 0.000014 Loss 4.656351, Accuracy 88.831%\n",
      "Epoch 30, Batch 737, LR 0.000014 Loss 4.655788, Accuracy 88.831%\n",
      "Epoch 30, Batch 738, LR 0.000014 Loss 4.656324, Accuracy 88.829%\n",
      "Epoch 30, Batch 739, LR 0.000014 Loss 4.657159, Accuracy 88.828%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 740, LR 0.000014 Loss 4.656697, Accuracy 88.832%\n",
      "Epoch 30, Batch 741, LR 0.000014 Loss 4.656707, Accuracy 88.833%\n",
      "Epoch 30, Batch 742, LR 0.000014 Loss 4.657349, Accuracy 88.830%\n",
      "Epoch 30, Batch 743, LR 0.000014 Loss 4.657070, Accuracy 88.832%\n",
      "Epoch 30, Batch 744, LR 0.000014 Loss 4.657838, Accuracy 88.825%\n",
      "Epoch 30, Batch 745, LR 0.000014 Loss 4.658948, Accuracy 88.821%\n",
      "Epoch 30, Batch 746, LR 0.000014 Loss 4.659601, Accuracy 88.815%\n",
      "Epoch 30, Batch 747, LR 0.000014 Loss 4.658614, Accuracy 88.815%\n",
      "Epoch 30, Batch 748, LR 0.000014 Loss 4.658325, Accuracy 88.812%\n",
      "Epoch 30, Batch 749, LR 0.000014 Loss 4.659035, Accuracy 88.811%\n",
      "Epoch 30, Batch 750, LR 0.000014 Loss 4.659042, Accuracy 88.809%\n",
      "Epoch 30, Batch 751, LR 0.000014 Loss 4.659235, Accuracy 88.810%\n",
      "Epoch 30, Batch 752, LR 0.000014 Loss 4.659435, Accuracy 88.808%\n",
      "Epoch 30, Batch 753, LR 0.000014 Loss 4.658500, Accuracy 88.810%\n",
      "Epoch 30, Batch 754, LR 0.000014 Loss 4.659072, Accuracy 88.808%\n",
      "Epoch 30, Batch 755, LR 0.000014 Loss 4.658423, Accuracy 88.806%\n",
      "Epoch 30, Batch 756, LR 0.000014 Loss 4.657895, Accuracy 88.807%\n",
      "Epoch 30, Batch 757, LR 0.000014 Loss 4.658652, Accuracy 88.808%\n",
      "Epoch 30, Batch 758, LR 0.000014 Loss 4.658192, Accuracy 88.812%\n",
      "Epoch 30, Batch 759, LR 0.000014 Loss 4.658162, Accuracy 88.806%\n",
      "Epoch 30, Batch 760, LR 0.000014 Loss 4.657780, Accuracy 88.809%\n",
      "Epoch 30, Batch 761, LR 0.000014 Loss 4.657724, Accuracy 88.807%\n",
      "Epoch 30, Batch 762, LR 0.000014 Loss 4.659154, Accuracy 88.804%\n",
      "Epoch 30, Batch 763, LR 0.000014 Loss 4.658290, Accuracy 88.811%\n",
      "Epoch 30, Batch 764, LR 0.000014 Loss 4.658503, Accuracy 88.806%\n",
      "Epoch 30, Batch 765, LR 0.000014 Loss 4.659411, Accuracy 88.798%\n",
      "Epoch 30, Batch 766, LR 0.000014 Loss 4.659516, Accuracy 88.801%\n",
      "Epoch 30, Batch 767, LR 0.000014 Loss 4.659928, Accuracy 88.805%\n",
      "Epoch 30, Batch 768, LR 0.000014 Loss 4.660642, Accuracy 88.803%\n",
      "Epoch 30, Batch 769, LR 0.000014 Loss 4.660699, Accuracy 88.805%\n",
      "Epoch 30, Batch 770, LR 0.000014 Loss 4.659931, Accuracy 88.808%\n",
      "Epoch 30, Batch 771, LR 0.000014 Loss 4.659316, Accuracy 88.813%\n",
      "Epoch 30, Batch 772, LR 0.000014 Loss 4.659474, Accuracy 88.815%\n",
      "Epoch 30, Batch 773, LR 0.000014 Loss 4.660118, Accuracy 88.809%\n",
      "Epoch 30, Batch 774, LR 0.000014 Loss 4.659895, Accuracy 88.807%\n",
      "Epoch 30, Batch 775, LR 0.000014 Loss 4.660251, Accuracy 88.807%\n",
      "Epoch 30, Batch 776, LR 0.000014 Loss 4.660332, Accuracy 88.809%\n",
      "Epoch 30, Batch 777, LR 0.000014 Loss 4.660729, Accuracy 88.806%\n",
      "Epoch 30, Batch 778, LR 0.000014 Loss 4.661222, Accuracy 88.805%\n",
      "Epoch 30, Batch 779, LR 0.000014 Loss 4.661575, Accuracy 88.801%\n",
      "Epoch 30, Batch 780, LR 0.000014 Loss 4.661765, Accuracy 88.798%\n",
      "Epoch 30, Batch 781, LR 0.000014 Loss 4.661483, Accuracy 88.803%\n",
      "Epoch 30, Batch 782, LR 0.000014 Loss 4.662393, Accuracy 88.797%\n",
      "Epoch 30, Batch 783, LR 0.000014 Loss 4.661735, Accuracy 88.801%\n",
      "Epoch 30, Batch 784, LR 0.000014 Loss 4.661346, Accuracy 88.798%\n",
      "Epoch 30, Batch 785, LR 0.000014 Loss 4.661181, Accuracy 88.801%\n",
      "Epoch 30, Batch 786, LR 0.000014 Loss 4.661135, Accuracy 88.800%\n",
      "Epoch 30, Batch 787, LR 0.000014 Loss 4.661889, Accuracy 88.792%\n",
      "Epoch 30, Batch 788, LR 0.000014 Loss 4.661160, Accuracy 88.796%\n",
      "Epoch 30, Batch 789, LR 0.000014 Loss 4.661470, Accuracy 88.792%\n",
      "Epoch 30, Batch 790, LR 0.000014 Loss 4.661569, Accuracy 88.788%\n",
      "Epoch 30, Batch 791, LR 0.000014 Loss 4.661881, Accuracy 88.784%\n",
      "Epoch 30, Batch 792, LR 0.000014 Loss 4.662308, Accuracy 88.779%\n",
      "Epoch 30, Batch 793, LR 0.000014 Loss 4.661458, Accuracy 88.780%\n",
      "Epoch 30, Batch 794, LR 0.000014 Loss 4.661142, Accuracy 88.782%\n",
      "Epoch 30, Batch 795, LR 0.000014 Loss 4.661114, Accuracy 88.779%\n",
      "Epoch 30, Batch 796, LR 0.000014 Loss 4.660251, Accuracy 88.784%\n",
      "Epoch 30, Batch 797, LR 0.000014 Loss 4.660889, Accuracy 88.778%\n",
      "Epoch 30, Batch 798, LR 0.000014 Loss 4.660082, Accuracy 88.783%\n",
      "Epoch 30, Batch 799, LR 0.000014 Loss 4.660779, Accuracy 88.784%\n",
      "Epoch 30, Batch 800, LR 0.000014 Loss 4.660138, Accuracy 88.786%\n",
      "Epoch 30, Batch 801, LR 0.000014 Loss 4.660062, Accuracy 88.790%\n",
      "Epoch 30, Batch 802, LR 0.000014 Loss 4.660006, Accuracy 88.795%\n",
      "Epoch 30, Batch 803, LR 0.000014 Loss 4.660347, Accuracy 88.796%\n",
      "Epoch 30, Batch 804, LR 0.000014 Loss 4.660654, Accuracy 88.795%\n",
      "Epoch 30, Batch 805, LR 0.000014 Loss 4.661156, Accuracy 88.796%\n",
      "Epoch 30, Batch 806, LR 0.000014 Loss 4.660766, Accuracy 88.800%\n",
      "Epoch 30, Batch 807, LR 0.000014 Loss 4.659630, Accuracy 88.802%\n",
      "Epoch 30, Batch 808, LR 0.000014 Loss 4.659796, Accuracy 88.800%\n",
      "Epoch 30, Batch 809, LR 0.000014 Loss 4.661487, Accuracy 88.797%\n",
      "Epoch 30, Batch 810, LR 0.000014 Loss 4.661408, Accuracy 88.796%\n",
      "Epoch 30, Batch 811, LR 0.000014 Loss 4.660417, Accuracy 88.797%\n",
      "Epoch 30, Batch 812, LR 0.000014 Loss 4.660848, Accuracy 88.795%\n",
      "Epoch 30, Batch 813, LR 0.000014 Loss 4.661181, Accuracy 88.796%\n",
      "Epoch 30, Batch 814, LR 0.000014 Loss 4.661371, Accuracy 88.795%\n",
      "Epoch 30, Batch 815, LR 0.000014 Loss 4.660614, Accuracy 88.799%\n",
      "Epoch 30, Batch 816, LR 0.000014 Loss 4.661143, Accuracy 88.798%\n",
      "Epoch 30, Batch 817, LR 0.000014 Loss 4.660756, Accuracy 88.800%\n",
      "Epoch 30, Batch 818, LR 0.000014 Loss 4.660274, Accuracy 88.802%\n",
      "Epoch 30, Batch 819, LR 0.000014 Loss 4.660474, Accuracy 88.802%\n",
      "Epoch 30, Batch 820, LR 0.000014 Loss 4.660687, Accuracy 88.799%\n",
      "Epoch 30, Batch 821, LR 0.000014 Loss 4.661505, Accuracy 88.794%\n",
      "Epoch 30, Batch 822, LR 0.000014 Loss 4.661320, Accuracy 88.796%\n",
      "Epoch 30, Batch 823, LR 0.000014 Loss 4.662072, Accuracy 88.796%\n",
      "Epoch 30, Batch 824, LR 0.000014 Loss 4.662872, Accuracy 88.794%\n",
      "Epoch 30, Batch 825, LR 0.000014 Loss 4.663407, Accuracy 88.790%\n",
      "Epoch 30, Batch 826, LR 0.000014 Loss 4.664005, Accuracy 88.790%\n",
      "Epoch 30, Batch 827, LR 0.000014 Loss 4.663385, Accuracy 88.793%\n",
      "Epoch 30, Batch 828, LR 0.000014 Loss 4.664363, Accuracy 88.787%\n",
      "Epoch 30, Batch 829, LR 0.000014 Loss 4.665006, Accuracy 88.783%\n",
      "Epoch 30, Batch 830, LR 0.000014 Loss 4.665200, Accuracy 88.783%\n",
      "Epoch 30, Batch 831, LR 0.000014 Loss 4.665061, Accuracy 88.783%\n",
      "Epoch 30, Batch 832, LR 0.000014 Loss 4.665348, Accuracy 88.780%\n",
      "Epoch 30, Batch 833, LR 0.000014 Loss 4.665495, Accuracy 88.779%\n",
      "Epoch 30, Batch 834, LR 0.000014 Loss 4.666675, Accuracy 88.771%\n",
      "Epoch 30, Batch 835, LR 0.000014 Loss 4.667663, Accuracy 88.762%\n",
      "Epoch 30, Batch 836, LR 0.000014 Loss 4.667596, Accuracy 88.758%\n",
      "Epoch 30, Batch 837, LR 0.000014 Loss 4.668159, Accuracy 88.754%\n",
      "Epoch 30, Batch 838, LR 0.000014 Loss 4.669174, Accuracy 88.749%\n",
      "Epoch 30, Batch 839, LR 0.000014 Loss 4.669585, Accuracy 88.748%\n",
      "Epoch 30, Batch 840, LR 0.000014 Loss 4.669187, Accuracy 88.748%\n",
      "Epoch 30, Batch 841, LR 0.000014 Loss 4.668242, Accuracy 88.751%\n",
      "Epoch 30, Batch 842, LR 0.000014 Loss 4.668681, Accuracy 88.750%\n",
      "Epoch 30, Batch 843, LR 0.000014 Loss 4.667829, Accuracy 88.752%\n",
      "Epoch 30, Batch 844, LR 0.000014 Loss 4.668206, Accuracy 88.753%\n",
      "Epoch 30, Batch 845, LR 0.000014 Loss 4.667598, Accuracy 88.755%\n",
      "Epoch 30, Batch 846, LR 0.000014 Loss 4.667367, Accuracy 88.758%\n",
      "Epoch 30, Batch 847, LR 0.000014 Loss 4.667073, Accuracy 88.759%\n",
      "Epoch 30, Batch 848, LR 0.000014 Loss 4.666593, Accuracy 88.758%\n",
      "Epoch 30, Batch 849, LR 0.000014 Loss 4.665923, Accuracy 88.764%\n",
      "Epoch 30, Batch 850, LR 0.000014 Loss 4.665741, Accuracy 88.768%\n",
      "Epoch 30, Batch 851, LR 0.000014 Loss 4.665456, Accuracy 88.772%\n",
      "Epoch 30, Batch 852, LR 0.000014 Loss 4.664882, Accuracy 88.772%\n",
      "Epoch 30, Batch 853, LR 0.000014 Loss 4.664738, Accuracy 88.770%\n",
      "Epoch 30, Batch 854, LR 0.000014 Loss 4.664994, Accuracy 88.767%\n",
      "Epoch 30, Batch 855, LR 0.000014 Loss 4.665447, Accuracy 88.764%\n",
      "Epoch 30, Batch 856, LR 0.000014 Loss 4.665900, Accuracy 88.759%\n",
      "Epoch 30, Batch 857, LR 0.000014 Loss 4.666046, Accuracy 88.760%\n",
      "Epoch 30, Batch 858, LR 0.000014 Loss 4.666015, Accuracy 88.764%\n",
      "Epoch 30, Batch 859, LR 0.000014 Loss 4.665769, Accuracy 88.764%\n",
      "Epoch 30, Batch 860, LR 0.000014 Loss 4.666475, Accuracy 88.760%\n",
      "Epoch 30, Batch 861, LR 0.000014 Loss 4.667022, Accuracy 88.754%\n",
      "Epoch 30, Batch 862, LR 0.000014 Loss 4.666731, Accuracy 88.756%\n",
      "Epoch 30, Batch 863, LR 0.000014 Loss 4.667114, Accuracy 88.755%\n",
      "Epoch 30, Batch 864, LR 0.000014 Loss 4.666975, Accuracy 88.757%\n",
      "Epoch 30, Batch 865, LR 0.000014 Loss 4.666973, Accuracy 88.757%\n",
      "Epoch 30, Batch 866, LR 0.000014 Loss 4.665598, Accuracy 88.764%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 867, LR 0.000014 Loss 4.665370, Accuracy 88.771%\n",
      "Epoch 30, Batch 868, LR 0.000014 Loss 4.665038, Accuracy 88.773%\n",
      "Epoch 30, Batch 869, LR 0.000014 Loss 4.665098, Accuracy 88.774%\n",
      "Epoch 30, Batch 870, LR 0.000014 Loss 4.665005, Accuracy 88.775%\n",
      "Epoch 30, Batch 871, LR 0.000014 Loss 4.665273, Accuracy 88.773%\n",
      "Epoch 30, Batch 872, LR 0.000014 Loss 4.664728, Accuracy 88.773%\n",
      "Epoch 30, Batch 873, LR 0.000014 Loss 4.664542, Accuracy 88.772%\n",
      "Epoch 30, Batch 874, LR 0.000014 Loss 4.663848, Accuracy 88.775%\n",
      "Epoch 30, Batch 875, LR 0.000014 Loss 4.664564, Accuracy 88.772%\n",
      "Epoch 30, Batch 876, LR 0.000014 Loss 4.664082, Accuracy 88.775%\n",
      "Epoch 30, Batch 877, LR 0.000014 Loss 4.664273, Accuracy 88.775%\n",
      "Epoch 30, Batch 878, LR 0.000014 Loss 4.664105, Accuracy 88.779%\n",
      "Epoch 30, Batch 879, LR 0.000014 Loss 4.664623, Accuracy 88.775%\n",
      "Epoch 30, Batch 880, LR 0.000014 Loss 4.664480, Accuracy 88.773%\n",
      "Epoch 30, Batch 881, LR 0.000014 Loss 4.664516, Accuracy 88.773%\n",
      "Epoch 30, Batch 882, LR 0.000014 Loss 4.664518, Accuracy 88.775%\n",
      "Epoch 30, Batch 883, LR 0.000014 Loss 4.664353, Accuracy 88.777%\n",
      "Epoch 30, Batch 884, LR 0.000014 Loss 4.664674, Accuracy 88.776%\n",
      "Epoch 30, Batch 885, LR 0.000014 Loss 4.664023, Accuracy 88.781%\n",
      "Epoch 30, Batch 886, LR 0.000014 Loss 4.663547, Accuracy 88.783%\n",
      "Epoch 30, Batch 887, LR 0.000014 Loss 4.663230, Accuracy 88.783%\n",
      "Epoch 30, Batch 888, LR 0.000014 Loss 4.663434, Accuracy 88.785%\n",
      "Epoch 30, Batch 889, LR 0.000014 Loss 4.663087, Accuracy 88.789%\n",
      "Epoch 30, Batch 890, LR 0.000014 Loss 4.663101, Accuracy 88.790%\n",
      "Epoch 30, Batch 891, LR 0.000014 Loss 4.662734, Accuracy 88.794%\n",
      "Epoch 30, Batch 892, LR 0.000014 Loss 4.662473, Accuracy 88.794%\n",
      "Epoch 30, Batch 893, LR 0.000014 Loss 4.663261, Accuracy 88.789%\n",
      "Epoch 30, Batch 894, LR 0.000014 Loss 4.663344, Accuracy 88.790%\n",
      "Epoch 30, Batch 895, LR 0.000014 Loss 4.663042, Accuracy 88.793%\n",
      "Epoch 30, Batch 896, LR 0.000014 Loss 4.663107, Accuracy 88.794%\n",
      "Epoch 30, Batch 897, LR 0.000014 Loss 4.663332, Accuracy 88.793%\n",
      "Epoch 30, Batch 898, LR 0.000014 Loss 4.664272, Accuracy 88.788%\n",
      "Epoch 30, Batch 899, LR 0.000014 Loss 4.664250, Accuracy 88.786%\n",
      "Epoch 30, Batch 900, LR 0.000014 Loss 4.663147, Accuracy 88.793%\n",
      "Epoch 30, Batch 901, LR 0.000014 Loss 4.663291, Accuracy 88.790%\n",
      "Epoch 30, Batch 902, LR 0.000014 Loss 4.663536, Accuracy 88.792%\n",
      "Epoch 30, Batch 903, LR 0.000014 Loss 4.662927, Accuracy 88.796%\n",
      "Epoch 30, Batch 904, LR 0.000014 Loss 4.662570, Accuracy 88.796%\n",
      "Epoch 30, Batch 905, LR 0.000014 Loss 4.662412, Accuracy 88.800%\n",
      "Epoch 30, Batch 906, LR 0.000014 Loss 4.662555, Accuracy 88.799%\n",
      "Epoch 30, Batch 907, LR 0.000014 Loss 4.662123, Accuracy 88.802%\n",
      "Epoch 30, Batch 908, LR 0.000014 Loss 4.661459, Accuracy 88.804%\n",
      "Epoch 30, Batch 909, LR 0.000014 Loss 4.660583, Accuracy 88.808%\n",
      "Epoch 30, Batch 910, LR 0.000014 Loss 4.660151, Accuracy 88.814%\n",
      "Epoch 30, Batch 911, LR 0.000014 Loss 4.660057, Accuracy 88.816%\n",
      "Epoch 30, Batch 912, LR 0.000014 Loss 4.660776, Accuracy 88.814%\n",
      "Epoch 30, Batch 913, LR 0.000014 Loss 4.660313, Accuracy 88.814%\n",
      "Epoch 30, Batch 914, LR 0.000014 Loss 4.660272, Accuracy 88.813%\n",
      "Epoch 30, Batch 915, LR 0.000014 Loss 4.661105, Accuracy 88.809%\n",
      "Epoch 30, Batch 916, LR 0.000014 Loss 4.661660, Accuracy 88.807%\n",
      "Epoch 30, Batch 917, LR 0.000014 Loss 4.661330, Accuracy 88.810%\n",
      "Epoch 30, Batch 918, LR 0.000014 Loss 4.661169, Accuracy 88.811%\n",
      "Epoch 30, Batch 919, LR 0.000014 Loss 4.660720, Accuracy 88.813%\n",
      "Epoch 30, Batch 920, LR 0.000014 Loss 4.660054, Accuracy 88.813%\n",
      "Epoch 30, Batch 921, LR 0.000014 Loss 4.660435, Accuracy 88.812%\n",
      "Epoch 30, Batch 922, LR 0.000014 Loss 4.660467, Accuracy 88.813%\n",
      "Epoch 30, Batch 923, LR 0.000014 Loss 4.660364, Accuracy 88.814%\n",
      "Epoch 30, Batch 924, LR 0.000014 Loss 4.658774, Accuracy 88.818%\n",
      "Epoch 30, Batch 925, LR 0.000014 Loss 4.659306, Accuracy 88.819%\n",
      "Epoch 30, Batch 926, LR 0.000014 Loss 4.659826, Accuracy 88.817%\n",
      "Epoch 30, Batch 927, LR 0.000014 Loss 4.660196, Accuracy 88.817%\n",
      "Epoch 30, Batch 928, LR 0.000014 Loss 4.660711, Accuracy 88.815%\n",
      "Epoch 30, Batch 929, LR 0.000014 Loss 4.661020, Accuracy 88.814%\n",
      "Epoch 30, Batch 930, LR 0.000014 Loss 4.660845, Accuracy 88.815%\n",
      "Epoch 30, Batch 931, LR 0.000014 Loss 4.660836, Accuracy 88.817%\n",
      "Epoch 30, Batch 932, LR 0.000014 Loss 4.661430, Accuracy 88.813%\n",
      "Epoch 30, Batch 933, LR 0.000014 Loss 4.661420, Accuracy 88.809%\n",
      "Epoch 30, Batch 934, LR 0.000014 Loss 4.661257, Accuracy 88.812%\n",
      "Epoch 30, Batch 935, LR 0.000014 Loss 4.661711, Accuracy 88.811%\n",
      "Epoch 30, Batch 936, LR 0.000014 Loss 4.661398, Accuracy 88.811%\n",
      "Epoch 30, Batch 937, LR 0.000014 Loss 4.661277, Accuracy 88.817%\n",
      "Epoch 30, Batch 938, LR 0.000014 Loss 4.660989, Accuracy 88.816%\n",
      "Epoch 30, Batch 939, LR 0.000014 Loss 4.660876, Accuracy 88.813%\n",
      "Epoch 30, Batch 940, LR 0.000014 Loss 4.660310, Accuracy 88.815%\n",
      "Epoch 30, Batch 941, LR 0.000013 Loss 4.660747, Accuracy 88.809%\n",
      "Epoch 30, Batch 942, LR 0.000013 Loss 4.661181, Accuracy 88.807%\n",
      "Epoch 30, Batch 943, LR 0.000013 Loss 4.660641, Accuracy 88.807%\n",
      "Epoch 30, Batch 944, LR 0.000013 Loss 4.662223, Accuracy 88.798%\n",
      "Epoch 30, Batch 945, LR 0.000013 Loss 4.661898, Accuracy 88.799%\n",
      "Epoch 30, Batch 946, LR 0.000013 Loss 4.661709, Accuracy 88.799%\n",
      "Epoch 30, Batch 947, LR 0.000013 Loss 4.661718, Accuracy 88.799%\n",
      "Epoch 30, Batch 948, LR 0.000013 Loss 4.662364, Accuracy 88.799%\n",
      "Epoch 30, Batch 949, LR 0.000013 Loss 4.661535, Accuracy 88.802%\n",
      "Epoch 30, Batch 950, LR 0.000013 Loss 4.661803, Accuracy 88.803%\n",
      "Epoch 30, Batch 951, LR 0.000013 Loss 4.661729, Accuracy 88.802%\n",
      "Epoch 30, Batch 952, LR 0.000013 Loss 4.660832, Accuracy 88.811%\n",
      "Epoch 30, Batch 953, LR 0.000013 Loss 4.660422, Accuracy 88.817%\n",
      "Epoch 30, Batch 954, LR 0.000013 Loss 4.660372, Accuracy 88.816%\n",
      "Epoch 30, Batch 955, LR 0.000013 Loss 4.660604, Accuracy 88.817%\n",
      "Epoch 30, Batch 956, LR 0.000013 Loss 4.660431, Accuracy 88.817%\n",
      "Epoch 30, Batch 957, LR 0.000013 Loss 4.660618, Accuracy 88.816%\n",
      "Epoch 30, Batch 958, LR 0.000013 Loss 4.660293, Accuracy 88.820%\n",
      "Epoch 30, Batch 959, LR 0.000013 Loss 4.660918, Accuracy 88.815%\n",
      "Epoch 30, Batch 960, LR 0.000013 Loss 4.660743, Accuracy 88.817%\n",
      "Epoch 30, Batch 961, LR 0.000013 Loss 4.660151, Accuracy 88.823%\n",
      "Epoch 30, Batch 962, LR 0.000013 Loss 4.660029, Accuracy 88.821%\n",
      "Epoch 30, Batch 963, LR 0.000013 Loss 4.659838, Accuracy 88.817%\n",
      "Epoch 30, Batch 964, LR 0.000013 Loss 4.659817, Accuracy 88.815%\n",
      "Epoch 30, Batch 965, LR 0.000013 Loss 4.660202, Accuracy 88.813%\n",
      "Epoch 30, Batch 966, LR 0.000013 Loss 4.659884, Accuracy 88.813%\n",
      "Epoch 30, Batch 967, LR 0.000013 Loss 4.659645, Accuracy 88.815%\n",
      "Epoch 30, Batch 968, LR 0.000013 Loss 4.659524, Accuracy 88.816%\n",
      "Epoch 30, Batch 969, LR 0.000013 Loss 4.660027, Accuracy 88.816%\n",
      "Epoch 30, Batch 970, LR 0.000013 Loss 4.660166, Accuracy 88.815%\n",
      "Epoch 30, Batch 971, LR 0.000013 Loss 4.660419, Accuracy 88.813%\n",
      "Epoch 30, Batch 972, LR 0.000013 Loss 4.659888, Accuracy 88.817%\n",
      "Epoch 30, Batch 973, LR 0.000013 Loss 4.659347, Accuracy 88.821%\n",
      "Epoch 30, Batch 974, LR 0.000013 Loss 4.659244, Accuracy 88.821%\n",
      "Epoch 30, Batch 975, LR 0.000013 Loss 4.658283, Accuracy 88.825%\n",
      "Epoch 30, Batch 976, LR 0.000013 Loss 4.658789, Accuracy 88.826%\n",
      "Epoch 30, Batch 977, LR 0.000013 Loss 4.658546, Accuracy 88.826%\n",
      "Epoch 30, Batch 978, LR 0.000013 Loss 4.657874, Accuracy 88.828%\n",
      "Epoch 30, Batch 979, LR 0.000013 Loss 4.658314, Accuracy 88.829%\n",
      "Epoch 30, Batch 980, LR 0.000013 Loss 4.658528, Accuracy 88.828%\n",
      "Epoch 30, Batch 981, LR 0.000013 Loss 4.658420, Accuracy 88.828%\n",
      "Epoch 30, Batch 982, LR 0.000013 Loss 4.659285, Accuracy 88.825%\n",
      "Epoch 30, Batch 983, LR 0.000013 Loss 4.659919, Accuracy 88.822%\n",
      "Epoch 30, Batch 984, LR 0.000013 Loss 4.660291, Accuracy 88.816%\n",
      "Epoch 30, Batch 985, LR 0.000013 Loss 4.659959, Accuracy 88.815%\n",
      "Epoch 30, Batch 986, LR 0.000013 Loss 4.660306, Accuracy 88.816%\n",
      "Epoch 30, Batch 987, LR 0.000013 Loss 4.659773, Accuracy 88.822%\n",
      "Epoch 30, Batch 988, LR 0.000013 Loss 4.659352, Accuracy 88.824%\n",
      "Epoch 30, Batch 989, LR 0.000013 Loss 4.658787, Accuracy 88.829%\n",
      "Epoch 30, Batch 990, LR 0.000013 Loss 4.658701, Accuracy 88.827%\n",
      "Epoch 30, Batch 991, LR 0.000013 Loss 4.657983, Accuracy 88.832%\n",
      "Epoch 30, Batch 992, LR 0.000013 Loss 4.657937, Accuracy 88.833%\n",
      "Epoch 30, Batch 993, LR 0.000013 Loss 4.658229, Accuracy 88.831%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 994, LR 0.000013 Loss 4.658849, Accuracy 88.827%\n",
      "Epoch 30, Batch 995, LR 0.000013 Loss 4.659256, Accuracy 88.825%\n",
      "Epoch 30, Batch 996, LR 0.000013 Loss 4.659686, Accuracy 88.822%\n",
      "Epoch 30, Batch 997, LR 0.000013 Loss 4.660004, Accuracy 88.820%\n",
      "Epoch 30, Batch 998, LR 0.000013 Loss 4.660592, Accuracy 88.814%\n",
      "Epoch 30, Batch 999, LR 0.000013 Loss 4.660905, Accuracy 88.814%\n",
      "Epoch 30, Batch 1000, LR 0.000013 Loss 4.660684, Accuracy 88.815%\n",
      "Epoch 30, Batch 1001, LR 0.000013 Loss 4.660685, Accuracy 88.814%\n",
      "Epoch 30, Batch 1002, LR 0.000013 Loss 4.660615, Accuracy 88.816%\n",
      "Epoch 30, Batch 1003, LR 0.000013 Loss 4.661167, Accuracy 88.815%\n",
      "Epoch 30, Batch 1004, LR 0.000013 Loss 4.661308, Accuracy 88.817%\n",
      "Epoch 30, Batch 1005, LR 0.000013 Loss 4.661163, Accuracy 88.820%\n",
      "Epoch 30, Batch 1006, LR 0.000013 Loss 4.660857, Accuracy 88.819%\n",
      "Epoch 30, Batch 1007, LR 0.000013 Loss 4.660772, Accuracy 88.820%\n",
      "Epoch 30, Batch 1008, LR 0.000013 Loss 4.660269, Accuracy 88.821%\n",
      "Epoch 30, Batch 1009, LR 0.000013 Loss 4.660374, Accuracy 88.824%\n",
      "Epoch 30, Batch 1010, LR 0.000013 Loss 4.660317, Accuracy 88.824%\n",
      "Epoch 30, Batch 1011, LR 0.000013 Loss 4.661068, Accuracy 88.819%\n",
      "Epoch 30, Batch 1012, LR 0.000013 Loss 4.660884, Accuracy 88.820%\n",
      "Epoch 30, Batch 1013, LR 0.000013 Loss 4.661469, Accuracy 88.820%\n",
      "Epoch 30, Batch 1014, LR 0.000013 Loss 4.661688, Accuracy 88.817%\n",
      "Epoch 30, Batch 1015, LR 0.000013 Loss 4.661833, Accuracy 88.817%\n",
      "Epoch 30, Batch 1016, LR 0.000013 Loss 4.662045, Accuracy 88.813%\n",
      "Epoch 30, Batch 1017, LR 0.000013 Loss 4.661494, Accuracy 88.816%\n",
      "Epoch 30, Batch 1018, LR 0.000013 Loss 4.661444, Accuracy 88.814%\n",
      "Epoch 30, Batch 1019, LR 0.000013 Loss 4.661081, Accuracy 88.813%\n",
      "Epoch 30, Batch 1020, LR 0.000013 Loss 4.661302, Accuracy 88.816%\n",
      "Epoch 30, Batch 1021, LR 0.000013 Loss 4.661122, Accuracy 88.817%\n",
      "Epoch 30, Batch 1022, LR 0.000013 Loss 4.661542, Accuracy 88.815%\n",
      "Epoch 30, Batch 1023, LR 0.000013 Loss 4.661806, Accuracy 88.811%\n",
      "Epoch 30, Batch 1024, LR 0.000013 Loss 4.661648, Accuracy 88.814%\n",
      "Epoch 30, Batch 1025, LR 0.000013 Loss 4.661738, Accuracy 88.811%\n",
      "Epoch 30, Batch 1026, LR 0.000013 Loss 4.661260, Accuracy 88.812%\n",
      "Epoch 30, Batch 1027, LR 0.000013 Loss 4.661108, Accuracy 88.814%\n",
      "Epoch 30, Batch 1028, LR 0.000013 Loss 4.660374, Accuracy 88.816%\n",
      "Epoch 30, Batch 1029, LR 0.000013 Loss 4.660519, Accuracy 88.817%\n",
      "Epoch 30, Batch 1030, LR 0.000013 Loss 4.660470, Accuracy 88.818%\n",
      "Epoch 30, Batch 1031, LR 0.000013 Loss 4.661225, Accuracy 88.809%\n",
      "Epoch 30, Batch 1032, LR 0.000013 Loss 4.661892, Accuracy 88.809%\n",
      "Epoch 30, Batch 1033, LR 0.000013 Loss 4.662343, Accuracy 88.808%\n",
      "Epoch 30, Batch 1034, LR 0.000013 Loss 4.662034, Accuracy 88.810%\n",
      "Epoch 30, Batch 1035, LR 0.000013 Loss 4.661889, Accuracy 88.809%\n",
      "Epoch 30, Batch 1036, LR 0.000013 Loss 4.661855, Accuracy 88.808%\n",
      "Epoch 30, Batch 1037, LR 0.000013 Loss 4.661473, Accuracy 88.809%\n",
      "Epoch 30, Batch 1038, LR 0.000013 Loss 4.660810, Accuracy 88.810%\n",
      "Epoch 30, Batch 1039, LR 0.000013 Loss 4.661763, Accuracy 88.806%\n",
      "Epoch 30, Batch 1040, LR 0.000013 Loss 4.660883, Accuracy 88.807%\n",
      "Epoch 30, Batch 1041, LR 0.000013 Loss 4.661054, Accuracy 88.807%\n",
      "Epoch 30, Batch 1042, LR 0.000013 Loss 4.661378, Accuracy 88.805%\n",
      "Epoch 30, Batch 1043, LR 0.000013 Loss 4.662136, Accuracy 88.800%\n",
      "Epoch 30, Batch 1044, LR 0.000013 Loss 4.662025, Accuracy 88.801%\n",
      "Epoch 30, Batch 1045, LR 0.000013 Loss 4.662264, Accuracy 88.799%\n",
      "Epoch 30, Batch 1046, LR 0.000013 Loss 4.663070, Accuracy 88.793%\n",
      "Epoch 30, Batch 1047, LR 0.000013 Loss 4.663803, Accuracy 88.789%\n",
      "Epoch 30, Loss (train set) 4.663803, Accuracy (train set) 88.789%\n",
      "Epoch 31, Batch 1, LR 0.000013 Loss 4.703454, Accuracy 87.500%\n",
      "Epoch 31, Batch 2, LR 0.000013 Loss 4.430223, Accuracy 89.062%\n",
      "Epoch 31, Batch 3, LR 0.000013 Loss 4.564907, Accuracy 88.021%\n",
      "Epoch 31, Batch 4, LR 0.000013 Loss 4.476771, Accuracy 88.672%\n",
      "Epoch 31, Batch 5, LR 0.000013 Loss 4.440488, Accuracy 89.062%\n",
      "Epoch 31, Batch 6, LR 0.000013 Loss 4.550640, Accuracy 88.672%\n",
      "Epoch 31, Batch 7, LR 0.000013 Loss 4.692299, Accuracy 87.835%\n",
      "Epoch 31, Batch 8, LR 0.000013 Loss 4.684359, Accuracy 87.793%\n",
      "Epoch 31, Batch 9, LR 0.000013 Loss 4.658988, Accuracy 87.760%\n",
      "Epoch 31, Batch 10, LR 0.000013 Loss 4.587386, Accuracy 87.891%\n",
      "Epoch 31, Batch 11, LR 0.000013 Loss 4.626701, Accuracy 87.997%\n",
      "Epoch 31, Batch 12, LR 0.000013 Loss 4.597559, Accuracy 88.281%\n",
      "Epoch 31, Batch 13, LR 0.000013 Loss 4.549141, Accuracy 88.642%\n",
      "Epoch 31, Batch 14, LR 0.000013 Loss 4.539851, Accuracy 88.728%\n",
      "Epoch 31, Batch 15, LR 0.000013 Loss 4.556318, Accuracy 88.698%\n",
      "Epoch 31, Batch 16, LR 0.000013 Loss 4.555344, Accuracy 88.623%\n",
      "Epoch 31, Batch 17, LR 0.000013 Loss 4.531907, Accuracy 88.787%\n",
      "Epoch 31, Batch 18, LR 0.000013 Loss 4.549170, Accuracy 88.585%\n",
      "Epoch 31, Batch 19, LR 0.000013 Loss 4.543752, Accuracy 88.569%\n",
      "Epoch 31, Batch 20, LR 0.000013 Loss 4.544528, Accuracy 88.594%\n",
      "Epoch 31, Batch 21, LR 0.000013 Loss 4.533451, Accuracy 88.802%\n",
      "Epoch 31, Batch 22, LR 0.000013 Loss 4.531968, Accuracy 88.849%\n",
      "Epoch 31, Batch 23, LR 0.000013 Loss 4.533359, Accuracy 88.757%\n",
      "Epoch 31, Batch 24, LR 0.000013 Loss 4.528015, Accuracy 88.704%\n",
      "Epoch 31, Batch 25, LR 0.000013 Loss 4.527118, Accuracy 88.844%\n",
      "Epoch 31, Batch 26, LR 0.000013 Loss 4.507646, Accuracy 88.822%\n",
      "Epoch 31, Batch 27, LR 0.000013 Loss 4.508863, Accuracy 88.889%\n",
      "Epoch 31, Batch 28, LR 0.000013 Loss 4.536997, Accuracy 88.783%\n",
      "Epoch 31, Batch 29, LR 0.000013 Loss 4.535293, Accuracy 88.874%\n",
      "Epoch 31, Batch 30, LR 0.000013 Loss 4.526202, Accuracy 88.880%\n",
      "Epoch 31, Batch 31, LR 0.000013 Loss 4.523835, Accuracy 88.861%\n",
      "Epoch 31, Batch 32, LR 0.000013 Loss 4.537974, Accuracy 88.794%\n",
      "Epoch 31, Batch 33, LR 0.000013 Loss 4.540462, Accuracy 88.755%\n",
      "Epoch 31, Batch 34, LR 0.000013 Loss 4.549339, Accuracy 88.741%\n",
      "Epoch 31, Batch 35, LR 0.000013 Loss 4.548940, Accuracy 88.772%\n",
      "Epoch 31, Batch 36, LR 0.000013 Loss 4.561463, Accuracy 88.780%\n",
      "Epoch 31, Batch 37, LR 0.000013 Loss 4.555985, Accuracy 88.767%\n",
      "Epoch 31, Batch 38, LR 0.000013 Loss 4.563101, Accuracy 88.795%\n",
      "Epoch 31, Batch 39, LR 0.000013 Loss 4.581395, Accuracy 88.602%\n",
      "Epoch 31, Batch 40, LR 0.000013 Loss 4.566500, Accuracy 88.730%\n",
      "Epoch 31, Batch 41, LR 0.000013 Loss 4.568123, Accuracy 88.700%\n",
      "Epoch 31, Batch 42, LR 0.000013 Loss 4.569766, Accuracy 88.728%\n",
      "Epoch 31, Batch 43, LR 0.000013 Loss 4.574260, Accuracy 88.772%\n",
      "Epoch 31, Batch 44, LR 0.000013 Loss 4.577494, Accuracy 88.761%\n",
      "Epoch 31, Batch 45, LR 0.000013 Loss 4.585053, Accuracy 88.715%\n",
      "Epoch 31, Batch 46, LR 0.000013 Loss 4.571809, Accuracy 88.808%\n",
      "Epoch 31, Batch 47, LR 0.000013 Loss 4.576901, Accuracy 88.747%\n",
      "Epoch 31, Batch 48, LR 0.000013 Loss 4.580860, Accuracy 88.753%\n",
      "Epoch 31, Batch 49, LR 0.000013 Loss 4.586252, Accuracy 88.680%\n",
      "Epoch 31, Batch 50, LR 0.000013 Loss 4.577213, Accuracy 88.719%\n",
      "Epoch 31, Batch 51, LR 0.000013 Loss 4.577272, Accuracy 88.771%\n",
      "Epoch 31, Batch 52, LR 0.000013 Loss 4.591614, Accuracy 88.672%\n",
      "Epoch 31, Batch 53, LR 0.000013 Loss 4.608139, Accuracy 88.620%\n",
      "Epoch 31, Batch 54, LR 0.000013 Loss 4.605060, Accuracy 88.686%\n",
      "Epoch 31, Batch 55, LR 0.000013 Loss 4.600050, Accuracy 88.693%\n",
      "Epoch 31, Batch 56, LR 0.000013 Loss 4.615908, Accuracy 88.658%\n",
      "Epoch 31, Batch 57, LR 0.000013 Loss 4.604152, Accuracy 88.761%\n",
      "Epoch 31, Batch 58, LR 0.000013 Loss 4.612337, Accuracy 88.712%\n",
      "Epoch 31, Batch 59, LR 0.000013 Loss 4.616290, Accuracy 88.705%\n",
      "Epoch 31, Batch 60, LR 0.000013 Loss 4.628805, Accuracy 88.620%\n",
      "Epoch 31, Batch 61, LR 0.000013 Loss 4.635596, Accuracy 88.627%\n",
      "Epoch 31, Batch 62, LR 0.000013 Loss 4.641534, Accuracy 88.596%\n",
      "Epoch 31, Batch 63, LR 0.000013 Loss 4.645880, Accuracy 88.529%\n",
      "Epoch 31, Batch 64, LR 0.000013 Loss 4.641419, Accuracy 88.525%\n",
      "Epoch 31, Batch 65, LR 0.000013 Loss 4.645616, Accuracy 88.486%\n",
      "Epoch 31, Batch 66, LR 0.000013 Loss 4.637030, Accuracy 88.530%\n",
      "Epoch 31, Batch 67, LR 0.000013 Loss 4.627311, Accuracy 88.549%\n",
      "Epoch 31, Batch 68, LR 0.000013 Loss 4.625748, Accuracy 88.568%\n",
      "Epoch 31, Batch 69, LR 0.000013 Loss 4.628342, Accuracy 88.576%\n",
      "Epoch 31, Batch 70, LR 0.000013 Loss 4.634943, Accuracy 88.527%\n",
      "Epoch 31, Batch 71, LR 0.000013 Loss 4.645562, Accuracy 88.435%\n",
      "Epoch 31, Batch 72, LR 0.000013 Loss 4.642345, Accuracy 88.487%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 73, LR 0.000013 Loss 4.642460, Accuracy 88.506%\n",
      "Epoch 31, Batch 74, LR 0.000013 Loss 4.636115, Accuracy 88.503%\n",
      "Epoch 31, Batch 75, LR 0.000013 Loss 4.633825, Accuracy 88.542%\n",
      "Epoch 31, Batch 76, LR 0.000013 Loss 4.632075, Accuracy 88.538%\n",
      "Epoch 31, Batch 77, LR 0.000013 Loss 4.624120, Accuracy 88.626%\n",
      "Epoch 31, Batch 78, LR 0.000013 Loss 4.615598, Accuracy 88.672%\n",
      "Epoch 31, Batch 79, LR 0.000013 Loss 4.623847, Accuracy 88.608%\n",
      "Epoch 31, Batch 80, LR 0.000013 Loss 4.631916, Accuracy 88.574%\n",
      "Epoch 31, Batch 81, LR 0.000013 Loss 4.637897, Accuracy 88.561%\n",
      "Epoch 31, Batch 82, LR 0.000013 Loss 4.646249, Accuracy 88.548%\n",
      "Epoch 31, Batch 83, LR 0.000013 Loss 4.635694, Accuracy 88.601%\n",
      "Epoch 31, Batch 84, LR 0.000013 Loss 4.637856, Accuracy 88.607%\n",
      "Epoch 31, Batch 85, LR 0.000013 Loss 4.640323, Accuracy 88.612%\n",
      "Epoch 31, Batch 86, LR 0.000013 Loss 4.631988, Accuracy 88.663%\n",
      "Epoch 31, Batch 87, LR 0.000013 Loss 4.631314, Accuracy 88.667%\n",
      "Epoch 31, Batch 88, LR 0.000013 Loss 4.627487, Accuracy 88.707%\n",
      "Epoch 31, Batch 89, LR 0.000013 Loss 4.631298, Accuracy 88.711%\n",
      "Epoch 31, Batch 90, LR 0.000013 Loss 4.635874, Accuracy 88.663%\n",
      "Epoch 31, Batch 91, LR 0.000013 Loss 4.634471, Accuracy 88.676%\n",
      "Epoch 31, Batch 92, LR 0.000013 Loss 4.631816, Accuracy 88.706%\n",
      "Epoch 31, Batch 93, LR 0.000013 Loss 4.622921, Accuracy 88.752%\n",
      "Epoch 31, Batch 94, LR 0.000013 Loss 4.625325, Accuracy 88.738%\n",
      "Epoch 31, Batch 95, LR 0.000013 Loss 4.618202, Accuracy 88.750%\n",
      "Epoch 31, Batch 96, LR 0.000013 Loss 4.615590, Accuracy 88.770%\n",
      "Epoch 31, Batch 97, LR 0.000013 Loss 4.609934, Accuracy 88.789%\n",
      "Epoch 31, Batch 98, LR 0.000013 Loss 4.606491, Accuracy 88.799%\n",
      "Epoch 31, Batch 99, LR 0.000013 Loss 4.610288, Accuracy 88.810%\n",
      "Epoch 31, Batch 100, LR 0.000013 Loss 4.613706, Accuracy 88.797%\n",
      "Epoch 31, Batch 101, LR 0.000013 Loss 4.624639, Accuracy 88.722%\n",
      "Epoch 31, Batch 102, LR 0.000013 Loss 4.624932, Accuracy 88.741%\n",
      "Epoch 31, Batch 103, LR 0.000013 Loss 4.630886, Accuracy 88.698%\n",
      "Epoch 31, Batch 104, LR 0.000013 Loss 4.628431, Accuracy 88.747%\n",
      "Epoch 31, Batch 105, LR 0.000013 Loss 4.633774, Accuracy 88.728%\n",
      "Epoch 31, Batch 106, LR 0.000013 Loss 4.637003, Accuracy 88.716%\n",
      "Epoch 31, Batch 107, LR 0.000013 Loss 4.643616, Accuracy 88.727%\n",
      "Epoch 31, Batch 108, LR 0.000013 Loss 4.640133, Accuracy 88.737%\n",
      "Epoch 31, Batch 109, LR 0.000013 Loss 4.637731, Accuracy 88.769%\n",
      "Epoch 31, Batch 110, LR 0.000013 Loss 4.636777, Accuracy 88.736%\n",
      "Epoch 31, Batch 111, LR 0.000013 Loss 4.639414, Accuracy 88.746%\n",
      "Epoch 31, Batch 112, LR 0.000013 Loss 4.632343, Accuracy 88.790%\n",
      "Epoch 31, Batch 113, LR 0.000013 Loss 4.628215, Accuracy 88.821%\n",
      "Epoch 31, Batch 114, LR 0.000013 Loss 4.631542, Accuracy 88.782%\n",
      "Epoch 31, Batch 115, LR 0.000013 Loss 4.630054, Accuracy 88.784%\n",
      "Epoch 31, Batch 116, LR 0.000013 Loss 4.627148, Accuracy 88.807%\n",
      "Epoch 31, Batch 117, LR 0.000013 Loss 4.628320, Accuracy 88.802%\n",
      "Epoch 31, Batch 118, LR 0.000013 Loss 4.624158, Accuracy 88.798%\n",
      "Epoch 31, Batch 119, LR 0.000013 Loss 4.618991, Accuracy 88.793%\n",
      "Epoch 31, Batch 120, LR 0.000013 Loss 4.626475, Accuracy 88.757%\n",
      "Epoch 31, Batch 121, LR 0.000013 Loss 4.625655, Accuracy 88.798%\n",
      "Epoch 31, Batch 122, LR 0.000013 Loss 4.626386, Accuracy 88.787%\n",
      "Epoch 31, Batch 123, LR 0.000013 Loss 4.628011, Accuracy 88.770%\n",
      "Epoch 31, Batch 124, LR 0.000013 Loss 4.627891, Accuracy 88.773%\n",
      "Epoch 31, Batch 125, LR 0.000013 Loss 4.631732, Accuracy 88.756%\n",
      "Epoch 31, Batch 126, LR 0.000013 Loss 4.633007, Accuracy 88.771%\n",
      "Epoch 31, Batch 127, LR 0.000013 Loss 4.633641, Accuracy 88.736%\n",
      "Epoch 31, Batch 128, LR 0.000013 Loss 4.629940, Accuracy 88.733%\n",
      "Epoch 31, Batch 129, LR 0.000013 Loss 4.632386, Accuracy 88.729%\n",
      "Epoch 31, Batch 130, LR 0.000013 Loss 4.639472, Accuracy 88.696%\n",
      "Epoch 31, Batch 131, LR 0.000013 Loss 4.642654, Accuracy 88.687%\n",
      "Epoch 31, Batch 132, LR 0.000013 Loss 4.645442, Accuracy 88.696%\n",
      "Epoch 31, Batch 133, LR 0.000013 Loss 4.645862, Accuracy 88.675%\n",
      "Epoch 31, Batch 134, LR 0.000013 Loss 4.642792, Accuracy 88.695%\n",
      "Epoch 31, Batch 135, LR 0.000013 Loss 4.641434, Accuracy 88.709%\n",
      "Epoch 31, Batch 136, LR 0.000013 Loss 4.643655, Accuracy 88.689%\n",
      "Epoch 31, Batch 137, LR 0.000013 Loss 4.645900, Accuracy 88.669%\n",
      "Epoch 31, Batch 138, LR 0.000013 Loss 4.649391, Accuracy 88.644%\n",
      "Epoch 31, Batch 139, LR 0.000013 Loss 4.650366, Accuracy 88.630%\n",
      "Epoch 31, Batch 140, LR 0.000013 Loss 4.647224, Accuracy 88.627%\n",
      "Epoch 31, Batch 141, LR 0.000013 Loss 4.641995, Accuracy 88.658%\n",
      "Epoch 31, Batch 142, LR 0.000013 Loss 4.640835, Accuracy 88.672%\n",
      "Epoch 31, Batch 143, LR 0.000013 Loss 4.637026, Accuracy 88.707%\n",
      "Epoch 31, Batch 144, LR 0.000013 Loss 4.634070, Accuracy 88.726%\n",
      "Epoch 31, Batch 145, LR 0.000013 Loss 4.636133, Accuracy 88.734%\n",
      "Epoch 31, Batch 146, LR 0.000013 Loss 4.633726, Accuracy 88.720%\n",
      "Epoch 31, Batch 147, LR 0.000013 Loss 4.627481, Accuracy 88.744%\n",
      "Epoch 31, Batch 148, LR 0.000013 Loss 4.623063, Accuracy 88.772%\n",
      "Epoch 31, Batch 149, LR 0.000013 Loss 4.620137, Accuracy 88.769%\n",
      "Epoch 31, Batch 150, LR 0.000013 Loss 4.622902, Accuracy 88.760%\n",
      "Epoch 31, Batch 151, LR 0.000013 Loss 4.622434, Accuracy 88.752%\n",
      "Epoch 31, Batch 152, LR 0.000013 Loss 4.621439, Accuracy 88.770%\n",
      "Epoch 31, Batch 153, LR 0.000013 Loss 4.618851, Accuracy 88.792%\n",
      "Epoch 31, Batch 154, LR 0.000013 Loss 4.615495, Accuracy 88.809%\n",
      "Epoch 31, Batch 155, LR 0.000013 Loss 4.616622, Accuracy 88.765%\n",
      "Epoch 31, Batch 156, LR 0.000013 Loss 4.617774, Accuracy 88.772%\n",
      "Epoch 31, Batch 157, LR 0.000013 Loss 4.624862, Accuracy 88.759%\n",
      "Epoch 31, Batch 158, LR 0.000013 Loss 4.622811, Accuracy 88.751%\n",
      "Epoch 31, Batch 159, LR 0.000013 Loss 4.621099, Accuracy 88.773%\n",
      "Epoch 31, Batch 160, LR 0.000013 Loss 4.620444, Accuracy 88.770%\n",
      "Epoch 31, Batch 161, LR 0.000013 Loss 4.624815, Accuracy 88.752%\n",
      "Epoch 31, Batch 162, LR 0.000013 Loss 4.620457, Accuracy 88.788%\n",
      "Epoch 31, Batch 163, LR 0.000013 Loss 4.621318, Accuracy 88.785%\n",
      "Epoch 31, Batch 164, LR 0.000013 Loss 4.621113, Accuracy 88.781%\n",
      "Epoch 31, Batch 165, LR 0.000013 Loss 4.616779, Accuracy 88.821%\n",
      "Epoch 31, Batch 166, LR 0.000013 Loss 4.617158, Accuracy 88.818%\n",
      "Epoch 31, Batch 167, LR 0.000013 Loss 4.618587, Accuracy 88.833%\n",
      "Epoch 31, Batch 168, LR 0.000013 Loss 4.616663, Accuracy 88.830%\n",
      "Epoch 31, Batch 169, LR 0.000013 Loss 4.618709, Accuracy 88.817%\n",
      "Epoch 31, Batch 170, LR 0.000013 Loss 4.615973, Accuracy 88.819%\n",
      "Epoch 31, Batch 171, LR 0.000013 Loss 4.615951, Accuracy 88.829%\n",
      "Epoch 31, Batch 172, LR 0.000013 Loss 4.616476, Accuracy 88.822%\n",
      "Epoch 31, Batch 173, LR 0.000013 Loss 4.615255, Accuracy 88.828%\n",
      "Epoch 31, Batch 174, LR 0.000013 Loss 4.615996, Accuracy 88.842%\n",
      "Epoch 31, Batch 175, LR 0.000013 Loss 4.614484, Accuracy 88.853%\n",
      "Epoch 31, Batch 176, LR 0.000013 Loss 4.614319, Accuracy 88.858%\n",
      "Epoch 31, Batch 177, LR 0.000013 Loss 4.611482, Accuracy 88.882%\n",
      "Epoch 31, Batch 178, LR 0.000013 Loss 4.607877, Accuracy 88.891%\n",
      "Epoch 31, Batch 179, LR 0.000013 Loss 4.609689, Accuracy 88.884%\n",
      "Epoch 31, Batch 180, LR 0.000013 Loss 4.612895, Accuracy 88.876%\n",
      "Epoch 31, Batch 181, LR 0.000013 Loss 4.605754, Accuracy 88.916%\n",
      "Epoch 31, Batch 182, LR 0.000013 Loss 4.602998, Accuracy 88.929%\n",
      "Epoch 31, Batch 183, LR 0.000013 Loss 4.598790, Accuracy 88.947%\n",
      "Epoch 31, Batch 184, LR 0.000013 Loss 4.599786, Accuracy 88.935%\n",
      "Epoch 31, Batch 185, LR 0.000013 Loss 4.596265, Accuracy 88.940%\n",
      "Epoch 31, Batch 186, LR 0.000013 Loss 4.598338, Accuracy 88.936%\n",
      "Epoch 31, Batch 187, LR 0.000013 Loss 4.600813, Accuracy 88.916%\n",
      "Epoch 31, Batch 188, LR 0.000013 Loss 4.604947, Accuracy 88.900%\n",
      "Epoch 31, Batch 189, LR 0.000013 Loss 4.607450, Accuracy 88.897%\n",
      "Epoch 31, Batch 190, LR 0.000013 Loss 4.611412, Accuracy 88.873%\n",
      "Epoch 31, Batch 191, LR 0.000013 Loss 4.613264, Accuracy 88.870%\n",
      "Epoch 31, Batch 192, LR 0.000013 Loss 4.610018, Accuracy 88.896%\n",
      "Epoch 31, Batch 193, LR 0.000013 Loss 4.611323, Accuracy 88.872%\n",
      "Epoch 31, Batch 194, LR 0.000013 Loss 4.610691, Accuracy 88.881%\n",
      "Epoch 31, Batch 195, LR 0.000013 Loss 4.610061, Accuracy 88.890%\n",
      "Epoch 31, Batch 196, LR 0.000013 Loss 4.609912, Accuracy 88.883%\n",
      "Epoch 31, Batch 197, LR 0.000013 Loss 4.609494, Accuracy 88.872%\n",
      "Epoch 31, Batch 198, LR 0.000013 Loss 4.609415, Accuracy 88.881%\n",
      "Epoch 31, Batch 199, LR 0.000013 Loss 4.609736, Accuracy 88.890%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 200, LR 0.000013 Loss 4.608764, Accuracy 88.898%\n",
      "Epoch 31, Batch 201, LR 0.000013 Loss 4.609060, Accuracy 88.926%\n",
      "Epoch 31, Batch 202, LR 0.000013 Loss 4.607821, Accuracy 88.919%\n",
      "Epoch 31, Batch 203, LR 0.000013 Loss 4.608830, Accuracy 88.893%\n",
      "Epoch 31, Batch 204, LR 0.000013 Loss 4.607443, Accuracy 88.898%\n",
      "Epoch 31, Batch 205, LR 0.000013 Loss 4.611152, Accuracy 88.872%\n",
      "Epoch 31, Batch 206, LR 0.000013 Loss 4.611318, Accuracy 88.858%\n",
      "Epoch 31, Batch 207, LR 0.000013 Loss 4.609226, Accuracy 88.874%\n",
      "Epoch 31, Batch 208, LR 0.000013 Loss 4.610549, Accuracy 88.867%\n",
      "Epoch 31, Batch 209, LR 0.000013 Loss 4.614195, Accuracy 88.857%\n",
      "Epoch 31, Batch 210, LR 0.000013 Loss 4.611277, Accuracy 88.869%\n",
      "Epoch 31, Batch 211, LR 0.000013 Loss 4.609700, Accuracy 88.892%\n",
      "Epoch 31, Batch 212, LR 0.000013 Loss 4.609774, Accuracy 88.900%\n",
      "Epoch 31, Batch 213, LR 0.000013 Loss 4.611635, Accuracy 88.879%\n",
      "Epoch 31, Batch 214, LR 0.000013 Loss 4.612869, Accuracy 88.862%\n",
      "Epoch 31, Batch 215, LR 0.000013 Loss 4.610718, Accuracy 88.870%\n",
      "Epoch 31, Batch 216, LR 0.000013 Loss 4.610507, Accuracy 88.871%\n",
      "Epoch 31, Batch 217, LR 0.000013 Loss 4.611530, Accuracy 88.872%\n",
      "Epoch 31, Batch 218, LR 0.000013 Loss 4.614955, Accuracy 88.883%\n",
      "Epoch 31, Batch 219, LR 0.000013 Loss 4.612385, Accuracy 88.891%\n",
      "Epoch 31, Batch 220, LR 0.000013 Loss 4.617161, Accuracy 88.892%\n",
      "Epoch 31, Batch 221, LR 0.000013 Loss 4.615088, Accuracy 88.907%\n",
      "Epoch 31, Batch 222, LR 0.000013 Loss 4.613181, Accuracy 88.922%\n",
      "Epoch 31, Batch 223, LR 0.000013 Loss 4.617158, Accuracy 88.912%\n",
      "Epoch 31, Batch 224, LR 0.000013 Loss 4.617359, Accuracy 88.906%\n",
      "Epoch 31, Batch 225, LR 0.000013 Loss 4.618153, Accuracy 88.910%\n",
      "Epoch 31, Batch 226, LR 0.000013 Loss 4.619812, Accuracy 88.910%\n",
      "Epoch 31, Batch 227, LR 0.000013 Loss 4.617061, Accuracy 88.908%\n",
      "Epoch 31, Batch 228, LR 0.000013 Loss 4.616842, Accuracy 88.922%\n",
      "Epoch 31, Batch 229, LR 0.000013 Loss 4.618875, Accuracy 88.919%\n",
      "Epoch 31, Batch 230, LR 0.000013 Loss 4.622279, Accuracy 88.913%\n",
      "Epoch 31, Batch 231, LR 0.000013 Loss 4.620548, Accuracy 88.920%\n",
      "Epoch 31, Batch 232, LR 0.000013 Loss 4.615382, Accuracy 88.935%\n",
      "Epoch 31, Batch 233, LR 0.000013 Loss 4.617776, Accuracy 88.925%\n",
      "Epoch 31, Batch 234, LR 0.000013 Loss 4.618356, Accuracy 88.926%\n",
      "Epoch 31, Batch 235, LR 0.000013 Loss 4.618128, Accuracy 88.939%\n",
      "Epoch 31, Batch 236, LR 0.000013 Loss 4.617233, Accuracy 88.940%\n",
      "Epoch 31, Batch 237, LR 0.000013 Loss 4.617574, Accuracy 88.944%\n",
      "Epoch 31, Batch 238, LR 0.000013 Loss 4.619421, Accuracy 88.928%\n",
      "Epoch 31, Batch 239, LR 0.000013 Loss 4.618253, Accuracy 88.938%\n",
      "Epoch 31, Batch 240, LR 0.000013 Loss 4.617987, Accuracy 88.936%\n",
      "Epoch 31, Batch 241, LR 0.000013 Loss 4.617137, Accuracy 88.926%\n",
      "Epoch 31, Batch 242, LR 0.000013 Loss 4.618896, Accuracy 88.911%\n",
      "Epoch 31, Batch 243, LR 0.000013 Loss 4.620234, Accuracy 88.915%\n",
      "Epoch 31, Batch 244, LR 0.000013 Loss 4.621359, Accuracy 88.915%\n",
      "Epoch 31, Batch 245, LR 0.000013 Loss 4.622061, Accuracy 88.900%\n",
      "Epoch 31, Batch 246, LR 0.000013 Loss 4.621350, Accuracy 88.897%\n",
      "Epoch 31, Batch 247, LR 0.000013 Loss 4.621240, Accuracy 88.882%\n",
      "Epoch 31, Batch 248, LR 0.000013 Loss 4.619027, Accuracy 88.896%\n",
      "Epoch 31, Batch 249, LR 0.000013 Loss 4.616212, Accuracy 88.915%\n",
      "Epoch 31, Batch 250, LR 0.000013 Loss 4.618202, Accuracy 88.906%\n",
      "Epoch 31, Batch 251, LR 0.000013 Loss 4.619847, Accuracy 88.904%\n",
      "Epoch 31, Batch 252, LR 0.000013 Loss 4.617453, Accuracy 88.911%\n",
      "Epoch 31, Batch 253, LR 0.000013 Loss 4.615399, Accuracy 88.936%\n",
      "Epoch 31, Batch 254, LR 0.000013 Loss 4.614317, Accuracy 88.949%\n",
      "Epoch 31, Batch 255, LR 0.000013 Loss 4.612910, Accuracy 88.940%\n",
      "Epoch 31, Batch 256, LR 0.000013 Loss 4.611946, Accuracy 88.940%\n",
      "Epoch 31, Batch 257, LR 0.000013 Loss 4.610687, Accuracy 88.944%\n",
      "Epoch 31, Batch 258, LR 0.000013 Loss 4.609415, Accuracy 88.957%\n",
      "Epoch 31, Batch 259, LR 0.000013 Loss 4.610254, Accuracy 88.966%\n",
      "Epoch 31, Batch 260, LR 0.000013 Loss 4.608588, Accuracy 88.978%\n",
      "Epoch 31, Batch 261, LR 0.000013 Loss 4.609575, Accuracy 88.982%\n",
      "Epoch 31, Batch 262, LR 0.000013 Loss 4.614019, Accuracy 88.979%\n",
      "Epoch 31, Batch 263, LR 0.000013 Loss 4.612616, Accuracy 88.985%\n",
      "Epoch 31, Batch 264, LR 0.000013 Loss 4.612154, Accuracy 88.983%\n",
      "Epoch 31, Batch 265, LR 0.000013 Loss 4.611536, Accuracy 88.986%\n",
      "Epoch 31, Batch 266, LR 0.000013 Loss 4.608754, Accuracy 88.992%\n",
      "Epoch 31, Batch 267, LR 0.000013 Loss 4.607398, Accuracy 88.992%\n",
      "Epoch 31, Batch 268, LR 0.000013 Loss 4.605981, Accuracy 89.004%\n",
      "Epoch 31, Batch 269, LR 0.000013 Loss 4.603967, Accuracy 89.007%\n",
      "Epoch 31, Batch 270, LR 0.000013 Loss 4.604490, Accuracy 89.022%\n",
      "Epoch 31, Batch 271, LR 0.000012 Loss 4.602733, Accuracy 89.025%\n",
      "Epoch 31, Batch 272, LR 0.000012 Loss 4.603872, Accuracy 89.011%\n",
      "Epoch 31, Batch 273, LR 0.000012 Loss 4.604355, Accuracy 88.997%\n",
      "Epoch 31, Batch 274, LR 0.000012 Loss 4.603714, Accuracy 89.003%\n",
      "Epoch 31, Batch 275, LR 0.000012 Loss 4.604328, Accuracy 89.006%\n",
      "Epoch 31, Batch 276, LR 0.000012 Loss 4.605062, Accuracy 89.009%\n",
      "Epoch 31, Batch 277, LR 0.000012 Loss 4.604447, Accuracy 89.006%\n",
      "Epoch 31, Batch 278, LR 0.000012 Loss 4.604006, Accuracy 89.003%\n",
      "Epoch 31, Batch 279, LR 0.000012 Loss 4.607520, Accuracy 88.992%\n",
      "Epoch 31, Batch 280, LR 0.000012 Loss 4.610186, Accuracy 88.973%\n",
      "Epoch 31, Batch 281, LR 0.000012 Loss 4.613581, Accuracy 88.957%\n",
      "Epoch 31, Batch 282, LR 0.000012 Loss 4.614363, Accuracy 88.949%\n",
      "Epoch 31, Batch 283, LR 0.000012 Loss 4.615230, Accuracy 88.949%\n",
      "Epoch 31, Batch 284, LR 0.000012 Loss 4.612284, Accuracy 88.955%\n",
      "Epoch 31, Batch 285, LR 0.000012 Loss 4.611782, Accuracy 88.953%\n",
      "Epoch 31, Batch 286, LR 0.000012 Loss 4.610597, Accuracy 88.937%\n",
      "Epoch 31, Batch 287, LR 0.000012 Loss 4.611518, Accuracy 88.932%\n",
      "Epoch 31, Batch 288, LR 0.000012 Loss 4.610646, Accuracy 88.935%\n",
      "Epoch 31, Batch 289, LR 0.000012 Loss 4.609523, Accuracy 88.941%\n",
      "Epoch 31, Batch 290, LR 0.000012 Loss 4.605824, Accuracy 88.966%\n",
      "Epoch 31, Batch 291, LR 0.000012 Loss 4.604982, Accuracy 88.969%\n",
      "Epoch 31, Batch 292, LR 0.000012 Loss 4.605264, Accuracy 88.964%\n",
      "Epoch 31, Batch 293, LR 0.000012 Loss 4.604197, Accuracy 88.967%\n",
      "Epoch 31, Batch 294, LR 0.000012 Loss 4.606147, Accuracy 88.964%\n",
      "Epoch 31, Batch 295, LR 0.000012 Loss 4.602686, Accuracy 88.988%\n",
      "Epoch 31, Batch 296, LR 0.000012 Loss 4.602439, Accuracy 88.981%\n",
      "Epoch 31, Batch 297, LR 0.000012 Loss 4.603911, Accuracy 88.970%\n",
      "Epoch 31, Batch 298, LR 0.000012 Loss 4.603861, Accuracy 88.979%\n",
      "Epoch 31, Batch 299, LR 0.000012 Loss 4.603519, Accuracy 88.976%\n",
      "Epoch 31, Batch 300, LR 0.000012 Loss 4.604109, Accuracy 88.977%\n",
      "Epoch 31, Batch 301, LR 0.000012 Loss 4.604469, Accuracy 88.974%\n",
      "Epoch 31, Batch 302, LR 0.000012 Loss 4.605709, Accuracy 88.972%\n",
      "Epoch 31, Batch 303, LR 0.000012 Loss 4.604278, Accuracy 88.977%\n",
      "Epoch 31, Batch 304, LR 0.000012 Loss 4.603875, Accuracy 88.988%\n",
      "Epoch 31, Batch 305, LR 0.000012 Loss 4.603554, Accuracy 88.988%\n",
      "Epoch 31, Batch 306, LR 0.000012 Loss 4.603564, Accuracy 88.978%\n",
      "Epoch 31, Batch 307, LR 0.000012 Loss 4.603945, Accuracy 88.981%\n",
      "Epoch 31, Batch 308, LR 0.000012 Loss 4.604848, Accuracy 88.976%\n",
      "Epoch 31, Batch 309, LR 0.000012 Loss 4.604518, Accuracy 88.969%\n",
      "Epoch 31, Batch 310, LR 0.000012 Loss 4.604808, Accuracy 88.977%\n",
      "Epoch 31, Batch 311, LR 0.000012 Loss 4.603689, Accuracy 88.990%\n",
      "Epoch 31, Batch 312, LR 0.000012 Loss 4.604960, Accuracy 88.985%\n",
      "Epoch 31, Batch 313, LR 0.000012 Loss 4.604287, Accuracy 88.985%\n",
      "Epoch 31, Batch 314, LR 0.000012 Loss 4.605539, Accuracy 88.978%\n",
      "Epoch 31, Batch 315, LR 0.000012 Loss 4.607783, Accuracy 88.958%\n",
      "Epoch 31, Batch 316, LR 0.000012 Loss 4.606537, Accuracy 88.961%\n",
      "Epoch 31, Batch 317, LR 0.000012 Loss 4.608471, Accuracy 88.959%\n",
      "Epoch 31, Batch 318, LR 0.000012 Loss 4.608550, Accuracy 88.959%\n",
      "Epoch 31, Batch 319, LR 0.000012 Loss 4.607465, Accuracy 88.957%\n",
      "Epoch 31, Batch 320, LR 0.000012 Loss 4.609297, Accuracy 88.945%\n",
      "Epoch 31, Batch 321, LR 0.000012 Loss 4.607534, Accuracy 88.960%\n",
      "Epoch 31, Batch 322, LR 0.000012 Loss 4.606079, Accuracy 88.978%\n",
      "Epoch 31, Batch 323, LR 0.000012 Loss 4.604243, Accuracy 88.980%\n",
      "Epoch 31, Batch 324, LR 0.000012 Loss 4.603476, Accuracy 88.985%\n",
      "Epoch 31, Batch 325, LR 0.000012 Loss 4.602132, Accuracy 88.993%\n",
      "Epoch 31, Batch 326, LR 0.000012 Loss 4.601048, Accuracy 88.995%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 327, LR 0.000012 Loss 4.600399, Accuracy 88.998%\n",
      "Epoch 31, Batch 328, LR 0.000012 Loss 4.601591, Accuracy 88.991%\n",
      "Epoch 31, Batch 329, LR 0.000012 Loss 4.603448, Accuracy 88.979%\n",
      "Epoch 31, Batch 330, LR 0.000012 Loss 4.601428, Accuracy 88.987%\n",
      "Epoch 31, Batch 331, LR 0.000012 Loss 4.600299, Accuracy 89.003%\n",
      "Epoch 31, Batch 332, LR 0.000012 Loss 4.601328, Accuracy 89.006%\n",
      "Epoch 31, Batch 333, LR 0.000012 Loss 4.601528, Accuracy 89.004%\n",
      "Epoch 31, Batch 334, LR 0.000012 Loss 4.601462, Accuracy 88.999%\n",
      "Epoch 31, Batch 335, LR 0.000012 Loss 4.600922, Accuracy 88.997%\n",
      "Epoch 31, Batch 336, LR 0.000012 Loss 4.602268, Accuracy 88.990%\n",
      "Epoch 31, Batch 337, LR 0.000012 Loss 4.601086, Accuracy 88.998%\n",
      "Epoch 31, Batch 338, LR 0.000012 Loss 4.601543, Accuracy 88.995%\n",
      "Epoch 31, Batch 339, LR 0.000012 Loss 4.601875, Accuracy 88.991%\n",
      "Epoch 31, Batch 340, LR 0.000012 Loss 4.602925, Accuracy 88.996%\n",
      "Epoch 31, Batch 341, LR 0.000012 Loss 4.601188, Accuracy 89.010%\n",
      "Epoch 31, Batch 342, LR 0.000012 Loss 4.601044, Accuracy 89.015%\n",
      "Epoch 31, Batch 343, LR 0.000012 Loss 4.598104, Accuracy 89.028%\n",
      "Epoch 31, Batch 344, LR 0.000012 Loss 4.599764, Accuracy 89.010%\n",
      "Epoch 31, Batch 345, LR 0.000012 Loss 4.600350, Accuracy 89.017%\n",
      "Epoch 31, Batch 346, LR 0.000012 Loss 4.599808, Accuracy 89.020%\n",
      "Epoch 31, Batch 347, LR 0.000012 Loss 4.601592, Accuracy 89.004%\n",
      "Epoch 31, Batch 348, LR 0.000012 Loss 4.601288, Accuracy 88.995%\n",
      "Epoch 31, Batch 349, LR 0.000012 Loss 4.601484, Accuracy 88.998%\n",
      "Epoch 31, Batch 350, LR 0.000012 Loss 4.601560, Accuracy 88.996%\n",
      "Epoch 31, Batch 351, LR 0.000012 Loss 4.605801, Accuracy 88.976%\n",
      "Epoch 31, Batch 352, LR 0.000012 Loss 4.605066, Accuracy 88.985%\n",
      "Epoch 31, Batch 353, LR 0.000012 Loss 4.605070, Accuracy 88.987%\n",
      "Epoch 31, Batch 354, LR 0.000012 Loss 4.605167, Accuracy 88.996%\n",
      "Epoch 31, Batch 355, LR 0.000012 Loss 4.605289, Accuracy 88.996%\n",
      "Epoch 31, Batch 356, LR 0.000012 Loss 4.606217, Accuracy 88.990%\n",
      "Epoch 31, Batch 357, LR 0.000012 Loss 4.608041, Accuracy 88.984%\n",
      "Epoch 31, Batch 358, LR 0.000012 Loss 4.606454, Accuracy 88.982%\n",
      "Epoch 31, Batch 359, LR 0.000012 Loss 4.605647, Accuracy 88.989%\n",
      "Epoch 31, Batch 360, LR 0.000012 Loss 4.606900, Accuracy 88.991%\n",
      "Epoch 31, Batch 361, LR 0.000012 Loss 4.607834, Accuracy 88.982%\n",
      "Epoch 31, Batch 362, LR 0.000012 Loss 4.607999, Accuracy 88.989%\n",
      "Epoch 31, Batch 363, LR 0.000012 Loss 4.609092, Accuracy 88.987%\n",
      "Epoch 31, Batch 364, LR 0.000012 Loss 4.609595, Accuracy 88.979%\n",
      "Epoch 31, Batch 365, LR 0.000012 Loss 4.610411, Accuracy 88.975%\n",
      "Epoch 31, Batch 366, LR 0.000012 Loss 4.611465, Accuracy 88.969%\n",
      "Epoch 31, Batch 367, LR 0.000012 Loss 4.611366, Accuracy 88.973%\n",
      "Epoch 31, Batch 368, LR 0.000012 Loss 4.612578, Accuracy 88.967%\n",
      "Epoch 31, Batch 369, LR 0.000012 Loss 4.612454, Accuracy 88.974%\n",
      "Epoch 31, Batch 370, LR 0.000012 Loss 4.612137, Accuracy 88.978%\n",
      "Epoch 31, Batch 371, LR 0.000012 Loss 4.610852, Accuracy 88.978%\n",
      "Epoch 31, Batch 372, LR 0.000012 Loss 4.611523, Accuracy 88.978%\n",
      "Epoch 31, Batch 373, LR 0.000012 Loss 4.610953, Accuracy 88.987%\n",
      "Epoch 31, Batch 374, LR 0.000012 Loss 4.611121, Accuracy 88.987%\n",
      "Epoch 31, Batch 375, LR 0.000012 Loss 4.609458, Accuracy 88.990%\n",
      "Epoch 31, Batch 376, LR 0.000012 Loss 4.610469, Accuracy 88.988%\n",
      "Epoch 31, Batch 377, LR 0.000012 Loss 4.610134, Accuracy 88.992%\n",
      "Epoch 31, Batch 378, LR 0.000012 Loss 4.612298, Accuracy 88.984%\n",
      "Epoch 31, Batch 379, LR 0.000012 Loss 4.615197, Accuracy 88.982%\n",
      "Epoch 31, Batch 380, LR 0.000012 Loss 4.613052, Accuracy 88.984%\n",
      "Epoch 31, Batch 381, LR 0.000012 Loss 4.612846, Accuracy 88.976%\n",
      "Epoch 31, Batch 382, LR 0.000012 Loss 4.611481, Accuracy 88.989%\n",
      "Epoch 31, Batch 383, LR 0.000012 Loss 4.610943, Accuracy 88.991%\n",
      "Epoch 31, Batch 384, LR 0.000012 Loss 4.612948, Accuracy 88.989%\n",
      "Epoch 31, Batch 385, LR 0.000012 Loss 4.613898, Accuracy 88.985%\n",
      "Epoch 31, Batch 386, LR 0.000012 Loss 4.614548, Accuracy 88.982%\n",
      "Epoch 31, Batch 387, LR 0.000012 Loss 4.613954, Accuracy 88.984%\n",
      "Epoch 31, Batch 388, LR 0.000012 Loss 4.614483, Accuracy 88.982%\n",
      "Epoch 31, Batch 389, LR 0.000012 Loss 4.614208, Accuracy 88.984%\n",
      "Epoch 31, Batch 390, LR 0.000012 Loss 4.613074, Accuracy 88.990%\n",
      "Epoch 31, Batch 391, LR 0.000012 Loss 4.612814, Accuracy 88.987%\n",
      "Epoch 31, Batch 392, LR 0.000012 Loss 4.612643, Accuracy 88.987%\n",
      "Epoch 31, Batch 393, LR 0.000012 Loss 4.613139, Accuracy 88.985%\n",
      "Epoch 31, Batch 394, LR 0.000012 Loss 4.612319, Accuracy 88.993%\n",
      "Epoch 31, Batch 395, LR 0.000012 Loss 4.611865, Accuracy 88.993%\n",
      "Epoch 31, Batch 396, LR 0.000012 Loss 4.612738, Accuracy 88.990%\n",
      "Epoch 31, Batch 397, LR 0.000012 Loss 4.613842, Accuracy 88.972%\n",
      "Epoch 31, Batch 398, LR 0.000012 Loss 4.614479, Accuracy 88.968%\n",
      "Epoch 31, Batch 399, LR 0.000012 Loss 4.618167, Accuracy 88.955%\n",
      "Epoch 31, Batch 400, LR 0.000012 Loss 4.617146, Accuracy 88.959%\n",
      "Epoch 31, Batch 401, LR 0.000012 Loss 4.618840, Accuracy 88.948%\n",
      "Epoch 31, Batch 402, LR 0.000012 Loss 4.619026, Accuracy 88.944%\n",
      "Epoch 31, Batch 403, LR 0.000012 Loss 4.619826, Accuracy 88.940%\n",
      "Epoch 31, Batch 404, LR 0.000012 Loss 4.620980, Accuracy 88.931%\n",
      "Epoch 31, Batch 405, LR 0.000012 Loss 4.620540, Accuracy 88.933%\n",
      "Epoch 31, Batch 406, LR 0.000012 Loss 4.619366, Accuracy 88.939%\n",
      "Epoch 31, Batch 407, LR 0.000012 Loss 4.619719, Accuracy 88.936%\n",
      "Epoch 31, Batch 408, LR 0.000012 Loss 4.619949, Accuracy 88.930%\n",
      "Epoch 31, Batch 409, LR 0.000012 Loss 4.621404, Accuracy 88.929%\n",
      "Epoch 31, Batch 410, LR 0.000012 Loss 4.623971, Accuracy 88.921%\n",
      "Epoch 31, Batch 411, LR 0.000012 Loss 4.624812, Accuracy 88.918%\n",
      "Epoch 31, Batch 412, LR 0.000012 Loss 4.625635, Accuracy 88.920%\n",
      "Epoch 31, Batch 413, LR 0.000012 Loss 4.625740, Accuracy 88.915%\n",
      "Epoch 31, Batch 414, LR 0.000012 Loss 4.624592, Accuracy 88.921%\n",
      "Epoch 31, Batch 415, LR 0.000012 Loss 4.625038, Accuracy 88.919%\n",
      "Epoch 31, Batch 416, LR 0.000012 Loss 4.622548, Accuracy 88.925%\n",
      "Epoch 31, Batch 417, LR 0.000012 Loss 4.621707, Accuracy 88.933%\n",
      "Epoch 31, Batch 418, LR 0.000012 Loss 4.622219, Accuracy 88.935%\n",
      "Epoch 31, Batch 419, LR 0.000012 Loss 4.623429, Accuracy 88.932%\n",
      "Epoch 31, Batch 420, LR 0.000012 Loss 4.623100, Accuracy 88.934%\n",
      "Epoch 31, Batch 421, LR 0.000012 Loss 4.621711, Accuracy 88.934%\n",
      "Epoch 31, Batch 422, LR 0.000012 Loss 4.623175, Accuracy 88.937%\n",
      "Epoch 31, Batch 423, LR 0.000012 Loss 4.622919, Accuracy 88.931%\n",
      "Epoch 31, Batch 424, LR 0.000012 Loss 4.621818, Accuracy 88.939%\n",
      "Epoch 31, Batch 425, LR 0.000012 Loss 4.620618, Accuracy 88.949%\n",
      "Epoch 31, Batch 426, LR 0.000012 Loss 4.620788, Accuracy 88.945%\n",
      "Epoch 31, Batch 427, LR 0.000012 Loss 4.619962, Accuracy 88.942%\n",
      "Epoch 31, Batch 428, LR 0.000012 Loss 4.618842, Accuracy 88.948%\n",
      "Epoch 31, Batch 429, LR 0.000012 Loss 4.620128, Accuracy 88.951%\n",
      "Epoch 31, Batch 430, LR 0.000012 Loss 4.619140, Accuracy 88.952%\n",
      "Epoch 31, Batch 431, LR 0.000012 Loss 4.617731, Accuracy 88.956%\n",
      "Epoch 31, Batch 432, LR 0.000012 Loss 4.617197, Accuracy 88.952%\n",
      "Epoch 31, Batch 433, LR 0.000012 Loss 4.617326, Accuracy 88.947%\n",
      "Epoch 31, Batch 434, LR 0.000012 Loss 4.616033, Accuracy 88.953%\n",
      "Epoch 31, Batch 435, LR 0.000012 Loss 4.616439, Accuracy 88.951%\n",
      "Epoch 31, Batch 436, LR 0.000012 Loss 4.615859, Accuracy 88.955%\n",
      "Epoch 31, Batch 437, LR 0.000012 Loss 4.616272, Accuracy 88.953%\n",
      "Epoch 31, Batch 438, LR 0.000012 Loss 4.615854, Accuracy 88.948%\n",
      "Epoch 31, Batch 439, LR 0.000012 Loss 4.616138, Accuracy 88.943%\n",
      "Epoch 31, Batch 440, LR 0.000012 Loss 4.617567, Accuracy 88.936%\n",
      "Epoch 31, Batch 441, LR 0.000012 Loss 4.617200, Accuracy 88.940%\n",
      "Epoch 31, Batch 442, LR 0.000012 Loss 4.617391, Accuracy 88.942%\n",
      "Epoch 31, Batch 443, LR 0.000012 Loss 4.617683, Accuracy 88.943%\n",
      "Epoch 31, Batch 444, LR 0.000012 Loss 4.617678, Accuracy 88.946%\n",
      "Epoch 31, Batch 445, LR 0.000012 Loss 4.619042, Accuracy 88.945%\n",
      "Epoch 31, Batch 446, LR 0.000012 Loss 4.620208, Accuracy 88.935%\n",
      "Epoch 31, Batch 447, LR 0.000012 Loss 4.619224, Accuracy 88.942%\n",
      "Epoch 31, Batch 448, LR 0.000012 Loss 4.619416, Accuracy 88.946%\n",
      "Epoch 31, Batch 449, LR 0.000012 Loss 4.619857, Accuracy 88.937%\n",
      "Epoch 31, Batch 450, LR 0.000012 Loss 4.621054, Accuracy 88.931%\n",
      "Epoch 31, Batch 451, LR 0.000012 Loss 4.619678, Accuracy 88.938%\n",
      "Epoch 31, Batch 452, LR 0.000012 Loss 4.622919, Accuracy 88.919%\n",
      "Epoch 31, Batch 453, LR 0.000012 Loss 4.621732, Accuracy 88.928%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 454, LR 0.000012 Loss 4.622264, Accuracy 88.927%\n",
      "Epoch 31, Batch 455, LR 0.000012 Loss 4.621446, Accuracy 88.929%\n",
      "Epoch 31, Batch 456, LR 0.000012 Loss 4.622195, Accuracy 88.920%\n",
      "Epoch 31, Batch 457, LR 0.000012 Loss 4.621989, Accuracy 88.924%\n",
      "Epoch 31, Batch 458, LR 0.000012 Loss 4.622739, Accuracy 88.916%\n",
      "Epoch 31, Batch 459, LR 0.000012 Loss 4.621104, Accuracy 88.920%\n",
      "Epoch 31, Batch 460, LR 0.000012 Loss 4.621894, Accuracy 88.918%\n",
      "Epoch 31, Batch 461, LR 0.000012 Loss 4.623581, Accuracy 88.910%\n",
      "Epoch 31, Batch 462, LR 0.000012 Loss 4.624447, Accuracy 88.909%\n",
      "Epoch 31, Batch 463, LR 0.000012 Loss 4.624444, Accuracy 88.911%\n",
      "Epoch 31, Batch 464, LR 0.000012 Loss 4.623886, Accuracy 88.921%\n",
      "Epoch 31, Batch 465, LR 0.000012 Loss 4.622737, Accuracy 88.926%\n",
      "Epoch 31, Batch 466, LR 0.000012 Loss 4.623065, Accuracy 88.930%\n",
      "Epoch 31, Batch 467, LR 0.000012 Loss 4.622300, Accuracy 88.934%\n",
      "Epoch 31, Batch 468, LR 0.000012 Loss 4.623428, Accuracy 88.922%\n",
      "Epoch 31, Batch 469, LR 0.000012 Loss 4.621712, Accuracy 88.933%\n",
      "Epoch 31, Batch 470, LR 0.000012 Loss 4.621529, Accuracy 88.938%\n",
      "Epoch 31, Batch 471, LR 0.000012 Loss 4.622011, Accuracy 88.931%\n",
      "Epoch 31, Batch 472, LR 0.000012 Loss 4.622180, Accuracy 88.942%\n",
      "Epoch 31, Batch 473, LR 0.000012 Loss 4.622346, Accuracy 88.944%\n",
      "Epoch 31, Batch 474, LR 0.000012 Loss 4.623356, Accuracy 88.937%\n",
      "Epoch 31, Batch 475, LR 0.000012 Loss 4.624041, Accuracy 88.939%\n",
      "Epoch 31, Batch 476, LR 0.000012 Loss 4.623788, Accuracy 88.936%\n",
      "Epoch 31, Batch 477, LR 0.000012 Loss 4.623851, Accuracy 88.933%\n",
      "Epoch 31, Batch 478, LR 0.000012 Loss 4.624928, Accuracy 88.925%\n",
      "Epoch 31, Batch 479, LR 0.000012 Loss 4.623291, Accuracy 88.932%\n",
      "Epoch 31, Batch 480, LR 0.000012 Loss 4.623339, Accuracy 88.931%\n",
      "Epoch 31, Batch 481, LR 0.000012 Loss 4.623841, Accuracy 88.929%\n",
      "Epoch 31, Batch 482, LR 0.000012 Loss 4.624167, Accuracy 88.923%\n",
      "Epoch 31, Batch 483, LR 0.000012 Loss 4.624465, Accuracy 88.928%\n",
      "Epoch 31, Batch 484, LR 0.000012 Loss 4.623939, Accuracy 88.927%\n",
      "Epoch 31, Batch 485, LR 0.000012 Loss 4.624418, Accuracy 88.921%\n",
      "Epoch 31, Batch 486, LR 0.000012 Loss 4.625283, Accuracy 88.919%\n",
      "Epoch 31, Batch 487, LR 0.000012 Loss 4.624343, Accuracy 88.928%\n",
      "Epoch 31, Batch 488, LR 0.000012 Loss 4.624794, Accuracy 88.934%\n",
      "Epoch 31, Batch 489, LR 0.000012 Loss 4.625049, Accuracy 88.933%\n",
      "Epoch 31, Batch 490, LR 0.000012 Loss 4.625129, Accuracy 88.929%\n",
      "Epoch 31, Batch 491, LR 0.000012 Loss 4.625797, Accuracy 88.922%\n",
      "Epoch 31, Batch 492, LR 0.000012 Loss 4.626339, Accuracy 88.918%\n",
      "Epoch 31, Batch 493, LR 0.000012 Loss 4.627148, Accuracy 88.921%\n",
      "Epoch 31, Batch 494, LR 0.000012 Loss 4.626473, Accuracy 88.920%\n",
      "Epoch 31, Batch 495, LR 0.000012 Loss 4.626266, Accuracy 88.920%\n",
      "Epoch 31, Batch 496, LR 0.000012 Loss 4.625735, Accuracy 88.929%\n",
      "Epoch 31, Batch 497, LR 0.000012 Loss 4.626379, Accuracy 88.927%\n",
      "Epoch 31, Batch 498, LR 0.000012 Loss 4.626050, Accuracy 88.923%\n",
      "Epoch 31, Batch 499, LR 0.000012 Loss 4.624494, Accuracy 88.929%\n",
      "Epoch 31, Batch 500, LR 0.000012 Loss 4.624800, Accuracy 88.928%\n",
      "Epoch 31, Batch 501, LR 0.000012 Loss 4.624448, Accuracy 88.928%\n",
      "Epoch 31, Batch 502, LR 0.000012 Loss 4.624060, Accuracy 88.935%\n",
      "Epoch 31, Batch 503, LR 0.000012 Loss 4.625054, Accuracy 88.937%\n",
      "Epoch 31, Batch 504, LR 0.000012 Loss 4.625443, Accuracy 88.934%\n",
      "Epoch 31, Batch 505, LR 0.000012 Loss 4.627780, Accuracy 88.917%\n",
      "Epoch 31, Batch 506, LR 0.000012 Loss 4.629319, Accuracy 88.911%\n",
      "Epoch 31, Batch 507, LR 0.000012 Loss 4.629251, Accuracy 88.916%\n",
      "Epoch 31, Batch 508, LR 0.000012 Loss 4.629137, Accuracy 88.923%\n",
      "Epoch 31, Batch 509, LR 0.000012 Loss 4.629610, Accuracy 88.918%\n",
      "Epoch 31, Batch 510, LR 0.000012 Loss 4.630101, Accuracy 88.920%\n",
      "Epoch 31, Batch 511, LR 0.000012 Loss 4.631790, Accuracy 88.911%\n",
      "Epoch 31, Batch 512, LR 0.000012 Loss 4.631666, Accuracy 88.910%\n",
      "Epoch 31, Batch 513, LR 0.000012 Loss 4.631231, Accuracy 88.912%\n",
      "Epoch 31, Batch 514, LR 0.000012 Loss 4.630566, Accuracy 88.921%\n",
      "Epoch 31, Batch 515, LR 0.000012 Loss 4.631896, Accuracy 88.915%\n",
      "Epoch 31, Batch 516, LR 0.000012 Loss 4.631700, Accuracy 88.919%\n",
      "Epoch 31, Batch 517, LR 0.000012 Loss 4.631720, Accuracy 88.920%\n",
      "Epoch 31, Batch 518, LR 0.000012 Loss 4.631894, Accuracy 88.918%\n",
      "Epoch 31, Batch 519, LR 0.000012 Loss 4.632408, Accuracy 88.915%\n",
      "Epoch 31, Batch 520, LR 0.000012 Loss 4.631809, Accuracy 88.921%\n",
      "Epoch 31, Batch 521, LR 0.000012 Loss 4.630725, Accuracy 88.925%\n",
      "Epoch 31, Batch 522, LR 0.000012 Loss 4.629699, Accuracy 88.929%\n",
      "Epoch 31, Batch 523, LR 0.000012 Loss 4.630264, Accuracy 88.924%\n",
      "Epoch 31, Batch 524, LR 0.000012 Loss 4.629248, Accuracy 88.930%\n",
      "Epoch 31, Batch 525, LR 0.000012 Loss 4.630161, Accuracy 88.930%\n",
      "Epoch 31, Batch 526, LR 0.000012 Loss 4.630572, Accuracy 88.926%\n",
      "Epoch 31, Batch 527, LR 0.000012 Loss 4.630629, Accuracy 88.929%\n",
      "Epoch 31, Batch 528, LR 0.000012 Loss 4.630448, Accuracy 88.929%\n",
      "Epoch 31, Batch 529, LR 0.000012 Loss 4.629767, Accuracy 88.933%\n",
      "Epoch 31, Batch 530, LR 0.000012 Loss 4.629121, Accuracy 88.942%\n",
      "Epoch 31, Batch 531, LR 0.000012 Loss 4.629837, Accuracy 88.934%\n",
      "Epoch 31, Batch 532, LR 0.000012 Loss 4.630289, Accuracy 88.929%\n",
      "Epoch 31, Batch 533, LR 0.000012 Loss 4.630333, Accuracy 88.929%\n",
      "Epoch 31, Batch 534, LR 0.000012 Loss 4.629211, Accuracy 88.935%\n",
      "Epoch 31, Batch 535, LR 0.000012 Loss 4.627573, Accuracy 88.944%\n",
      "Epoch 31, Batch 536, LR 0.000012 Loss 4.628715, Accuracy 88.943%\n",
      "Epoch 31, Batch 537, LR 0.000012 Loss 4.629109, Accuracy 88.934%\n",
      "Epoch 31, Batch 538, LR 0.000012 Loss 4.628825, Accuracy 88.933%\n",
      "Epoch 31, Batch 539, LR 0.000012 Loss 4.629116, Accuracy 88.938%\n",
      "Epoch 31, Batch 540, LR 0.000012 Loss 4.628007, Accuracy 88.938%\n",
      "Epoch 31, Batch 541, LR 0.000012 Loss 4.629278, Accuracy 88.930%\n",
      "Epoch 31, Batch 542, LR 0.000012 Loss 4.630255, Accuracy 88.924%\n",
      "Epoch 31, Batch 543, LR 0.000012 Loss 4.629353, Accuracy 88.927%\n",
      "Epoch 31, Batch 544, LR 0.000012 Loss 4.628514, Accuracy 88.933%\n",
      "Epoch 31, Batch 545, LR 0.000012 Loss 4.628164, Accuracy 88.938%\n",
      "Epoch 31, Batch 546, LR 0.000012 Loss 4.627912, Accuracy 88.942%\n",
      "Epoch 31, Batch 547, LR 0.000012 Loss 4.627706, Accuracy 88.947%\n",
      "Epoch 31, Batch 548, LR 0.000012 Loss 4.629007, Accuracy 88.943%\n",
      "Epoch 31, Batch 549, LR 0.000012 Loss 4.628557, Accuracy 88.940%\n",
      "Epoch 31, Batch 550, LR 0.000012 Loss 4.629083, Accuracy 88.940%\n",
      "Epoch 31, Batch 551, LR 0.000012 Loss 4.628170, Accuracy 88.946%\n",
      "Epoch 31, Batch 552, LR 0.000012 Loss 4.629522, Accuracy 88.941%\n",
      "Epoch 31, Batch 553, LR 0.000012 Loss 4.630183, Accuracy 88.941%\n",
      "Epoch 31, Batch 554, LR 0.000012 Loss 4.629722, Accuracy 88.941%\n",
      "Epoch 31, Batch 555, LR 0.000012 Loss 4.629366, Accuracy 88.939%\n",
      "Epoch 31, Batch 556, LR 0.000012 Loss 4.630396, Accuracy 88.929%\n",
      "Epoch 31, Batch 557, LR 0.000012 Loss 4.630719, Accuracy 88.932%\n",
      "Epoch 31, Batch 558, LR 0.000012 Loss 4.631938, Accuracy 88.928%\n",
      "Epoch 31, Batch 559, LR 0.000012 Loss 4.630498, Accuracy 88.933%\n",
      "Epoch 31, Batch 560, LR 0.000012 Loss 4.630035, Accuracy 88.936%\n",
      "Epoch 31, Batch 561, LR 0.000012 Loss 4.630181, Accuracy 88.936%\n",
      "Epoch 31, Batch 562, LR 0.000012 Loss 4.629668, Accuracy 88.944%\n",
      "Epoch 31, Batch 563, LR 0.000012 Loss 4.630458, Accuracy 88.936%\n",
      "Epoch 31, Batch 564, LR 0.000012 Loss 4.630390, Accuracy 88.938%\n",
      "Epoch 31, Batch 565, LR 0.000012 Loss 4.629044, Accuracy 88.946%\n",
      "Epoch 31, Batch 566, LR 0.000012 Loss 4.629248, Accuracy 88.944%\n",
      "Epoch 31, Batch 567, LR 0.000012 Loss 4.628799, Accuracy 88.950%\n",
      "Epoch 31, Batch 568, LR 0.000012 Loss 4.628525, Accuracy 88.951%\n",
      "Epoch 31, Batch 569, LR 0.000012 Loss 4.627779, Accuracy 88.955%\n",
      "Epoch 31, Batch 570, LR 0.000012 Loss 4.627767, Accuracy 88.954%\n",
      "Epoch 31, Batch 571, LR 0.000012 Loss 4.627002, Accuracy 88.957%\n",
      "Epoch 31, Batch 572, LR 0.000012 Loss 4.626250, Accuracy 88.961%\n",
      "Epoch 31, Batch 573, LR 0.000012 Loss 4.627486, Accuracy 88.956%\n",
      "Epoch 31, Batch 574, LR 0.000012 Loss 4.626814, Accuracy 88.956%\n",
      "Epoch 31, Batch 575, LR 0.000012 Loss 4.627011, Accuracy 88.957%\n",
      "Epoch 31, Batch 576, LR 0.000012 Loss 4.626463, Accuracy 88.961%\n",
      "Epoch 31, Batch 577, LR 0.000012 Loss 4.625915, Accuracy 88.965%\n",
      "Epoch 31, Batch 578, LR 0.000012 Loss 4.626319, Accuracy 88.961%\n",
      "Epoch 31, Batch 579, LR 0.000012 Loss 4.625760, Accuracy 88.963%\n",
      "Epoch 31, Batch 580, LR 0.000012 Loss 4.625033, Accuracy 88.971%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 581, LR 0.000012 Loss 4.624712, Accuracy 88.972%\n",
      "Epoch 31, Batch 582, LR 0.000012 Loss 4.625454, Accuracy 88.973%\n",
      "Epoch 31, Batch 583, LR 0.000012 Loss 4.625665, Accuracy 88.969%\n",
      "Epoch 31, Batch 584, LR 0.000012 Loss 4.626208, Accuracy 88.966%\n",
      "Epoch 31, Batch 585, LR 0.000012 Loss 4.626237, Accuracy 88.968%\n",
      "Epoch 31, Batch 586, LR 0.000012 Loss 4.625037, Accuracy 88.976%\n",
      "Epoch 31, Batch 587, LR 0.000012 Loss 4.624931, Accuracy 88.977%\n",
      "Epoch 31, Batch 588, LR 0.000012 Loss 4.625127, Accuracy 88.977%\n",
      "Epoch 31, Batch 589, LR 0.000012 Loss 4.626073, Accuracy 88.967%\n",
      "Epoch 31, Batch 590, LR 0.000012 Loss 4.625721, Accuracy 88.971%\n",
      "Epoch 31, Batch 591, LR 0.000012 Loss 4.624739, Accuracy 88.970%\n",
      "Epoch 31, Batch 592, LR 0.000012 Loss 4.624920, Accuracy 88.966%\n",
      "Epoch 31, Batch 593, LR 0.000012 Loss 4.624315, Accuracy 88.976%\n",
      "Epoch 31, Batch 594, LR 0.000012 Loss 4.624867, Accuracy 88.974%\n",
      "Epoch 31, Batch 595, LR 0.000012 Loss 4.625356, Accuracy 88.973%\n",
      "Epoch 31, Batch 596, LR 0.000012 Loss 4.625261, Accuracy 88.969%\n",
      "Epoch 31, Batch 597, LR 0.000012 Loss 4.624945, Accuracy 88.968%\n",
      "Epoch 31, Batch 598, LR 0.000012 Loss 4.624584, Accuracy 88.972%\n",
      "Epoch 31, Batch 599, LR 0.000012 Loss 4.624782, Accuracy 88.971%\n",
      "Epoch 31, Batch 600, LR 0.000012 Loss 4.624683, Accuracy 88.966%\n",
      "Epoch 31, Batch 601, LR 0.000012 Loss 4.624677, Accuracy 88.969%\n",
      "Epoch 31, Batch 602, LR 0.000012 Loss 4.625213, Accuracy 88.959%\n",
      "Epoch 31, Batch 603, LR 0.000012 Loss 4.626473, Accuracy 88.952%\n",
      "Epoch 31, Batch 604, LR 0.000012 Loss 4.626927, Accuracy 88.946%\n",
      "Epoch 31, Batch 605, LR 0.000012 Loss 4.627242, Accuracy 88.949%\n",
      "Epoch 31, Batch 606, LR 0.000012 Loss 4.627269, Accuracy 88.948%\n",
      "Epoch 31, Batch 607, LR 0.000012 Loss 4.628238, Accuracy 88.945%\n",
      "Epoch 31, Batch 608, LR 0.000012 Loss 4.628508, Accuracy 88.944%\n",
      "Epoch 31, Batch 609, LR 0.000012 Loss 4.627887, Accuracy 88.941%\n",
      "Epoch 31, Batch 610, LR 0.000012 Loss 4.626885, Accuracy 88.946%\n",
      "Epoch 31, Batch 611, LR 0.000012 Loss 4.626327, Accuracy 88.946%\n",
      "Epoch 31, Batch 612, LR 0.000012 Loss 4.627368, Accuracy 88.943%\n",
      "Epoch 31, Batch 613, LR 0.000012 Loss 4.628354, Accuracy 88.940%\n",
      "Epoch 31, Batch 614, LR 0.000012 Loss 4.628456, Accuracy 88.940%\n",
      "Epoch 31, Batch 615, LR 0.000012 Loss 4.629206, Accuracy 88.937%\n",
      "Epoch 31, Batch 616, LR 0.000012 Loss 4.628326, Accuracy 88.941%\n",
      "Epoch 31, Batch 617, LR 0.000012 Loss 4.628410, Accuracy 88.938%\n",
      "Epoch 31, Batch 618, LR 0.000012 Loss 4.629660, Accuracy 88.935%\n",
      "Epoch 31, Batch 619, LR 0.000012 Loss 4.629784, Accuracy 88.927%\n",
      "Epoch 31, Batch 620, LR 0.000012 Loss 4.630107, Accuracy 88.926%\n",
      "Epoch 31, Batch 621, LR 0.000012 Loss 4.630586, Accuracy 88.919%\n",
      "Epoch 31, Batch 622, LR 0.000012 Loss 4.629656, Accuracy 88.916%\n",
      "Epoch 31, Batch 623, LR 0.000012 Loss 4.629677, Accuracy 88.921%\n",
      "Epoch 31, Batch 624, LR 0.000012 Loss 4.630144, Accuracy 88.922%\n",
      "Epoch 31, Batch 625, LR 0.000012 Loss 4.631100, Accuracy 88.918%\n",
      "Epoch 31, Batch 626, LR 0.000012 Loss 4.630767, Accuracy 88.919%\n",
      "Epoch 31, Batch 627, LR 0.000012 Loss 4.630612, Accuracy 88.919%\n",
      "Epoch 31, Batch 628, LR 0.000012 Loss 4.631463, Accuracy 88.917%\n",
      "Epoch 31, Batch 629, LR 0.000012 Loss 4.631087, Accuracy 88.918%\n",
      "Epoch 31, Batch 630, LR 0.000012 Loss 4.631140, Accuracy 88.915%\n",
      "Epoch 31, Batch 631, LR 0.000012 Loss 4.631091, Accuracy 88.914%\n",
      "Epoch 31, Batch 632, LR 0.000012 Loss 4.630353, Accuracy 88.914%\n",
      "Epoch 31, Batch 633, LR 0.000012 Loss 4.631912, Accuracy 88.907%\n",
      "Epoch 31, Batch 634, LR 0.000012 Loss 4.632069, Accuracy 88.906%\n",
      "Epoch 31, Batch 635, LR 0.000012 Loss 4.632344, Accuracy 88.909%\n",
      "Epoch 31, Batch 636, LR 0.000012 Loss 4.631892, Accuracy 88.905%\n",
      "Epoch 31, Batch 637, LR 0.000012 Loss 4.632369, Accuracy 88.904%\n",
      "Epoch 31, Batch 638, LR 0.000012 Loss 4.631393, Accuracy 88.907%\n",
      "Epoch 31, Batch 639, LR 0.000012 Loss 4.631746, Accuracy 88.901%\n",
      "Epoch 31, Batch 640, LR 0.000012 Loss 4.630934, Accuracy 88.906%\n",
      "Epoch 31, Batch 641, LR 0.000012 Loss 4.632253, Accuracy 88.899%\n",
      "Epoch 31, Batch 642, LR 0.000012 Loss 4.633163, Accuracy 88.898%\n",
      "Epoch 31, Batch 643, LR 0.000012 Loss 4.633150, Accuracy 88.901%\n",
      "Epoch 31, Batch 644, LR 0.000012 Loss 4.632417, Accuracy 88.906%\n",
      "Epoch 31, Batch 645, LR 0.000012 Loss 4.632062, Accuracy 88.910%\n",
      "Epoch 31, Batch 646, LR 0.000012 Loss 4.631875, Accuracy 88.910%\n",
      "Epoch 31, Batch 647, LR 0.000012 Loss 4.632366, Accuracy 88.903%\n",
      "Epoch 31, Batch 648, LR 0.000012 Loss 4.631733, Accuracy 88.907%\n",
      "Epoch 31, Batch 649, LR 0.000012 Loss 4.632482, Accuracy 88.900%\n",
      "Epoch 31, Batch 650, LR 0.000012 Loss 4.633032, Accuracy 88.900%\n",
      "Epoch 31, Batch 651, LR 0.000012 Loss 4.631665, Accuracy 88.908%\n",
      "Epoch 31, Batch 652, LR 0.000012 Loss 4.631410, Accuracy 88.908%\n",
      "Epoch 31, Batch 653, LR 0.000012 Loss 4.630939, Accuracy 88.912%\n",
      "Epoch 31, Batch 654, LR 0.000012 Loss 4.631126, Accuracy 88.911%\n",
      "Epoch 31, Batch 655, LR 0.000012 Loss 4.630736, Accuracy 88.911%\n",
      "Epoch 31, Batch 656, LR 0.000012 Loss 4.631468, Accuracy 88.905%\n",
      "Epoch 31, Batch 657, LR 0.000012 Loss 4.630652, Accuracy 88.910%\n",
      "Epoch 31, Batch 658, LR 0.000012 Loss 4.630444, Accuracy 88.906%\n",
      "Epoch 31, Batch 659, LR 0.000012 Loss 4.630903, Accuracy 88.901%\n",
      "Epoch 31, Batch 660, LR 0.000012 Loss 4.631717, Accuracy 88.899%\n",
      "Epoch 31, Batch 661, LR 0.000011 Loss 4.631580, Accuracy 88.905%\n",
      "Epoch 31, Batch 662, LR 0.000011 Loss 4.631790, Accuracy 88.906%\n",
      "Epoch 31, Batch 663, LR 0.000011 Loss 4.631799, Accuracy 88.910%\n",
      "Epoch 31, Batch 664, LR 0.000011 Loss 4.631073, Accuracy 88.911%\n",
      "Epoch 31, Batch 665, LR 0.000011 Loss 4.631182, Accuracy 88.909%\n",
      "Epoch 31, Batch 666, LR 0.000011 Loss 4.631605, Accuracy 88.909%\n",
      "Epoch 31, Batch 667, LR 0.000011 Loss 4.632219, Accuracy 88.906%\n",
      "Epoch 31, Batch 668, LR 0.000011 Loss 4.632678, Accuracy 88.905%\n",
      "Epoch 31, Batch 669, LR 0.000011 Loss 4.632826, Accuracy 88.905%\n",
      "Epoch 31, Batch 670, LR 0.000011 Loss 4.633224, Accuracy 88.900%\n",
      "Epoch 31, Batch 671, LR 0.000011 Loss 4.632865, Accuracy 88.908%\n",
      "Epoch 31, Batch 672, LR 0.000011 Loss 4.632097, Accuracy 88.911%\n",
      "Epoch 31, Batch 673, LR 0.000011 Loss 4.631637, Accuracy 88.914%\n",
      "Epoch 31, Batch 674, LR 0.000011 Loss 4.631484, Accuracy 88.915%\n",
      "Epoch 31, Batch 675, LR 0.000011 Loss 4.631940, Accuracy 88.911%\n",
      "Epoch 31, Batch 676, LR 0.000011 Loss 4.633711, Accuracy 88.904%\n",
      "Epoch 31, Batch 677, LR 0.000011 Loss 4.634042, Accuracy 88.895%\n",
      "Epoch 31, Batch 678, LR 0.000011 Loss 4.633785, Accuracy 88.899%\n",
      "Epoch 31, Batch 679, LR 0.000011 Loss 4.633934, Accuracy 88.899%\n",
      "Epoch 31, Batch 680, LR 0.000011 Loss 4.634489, Accuracy 88.897%\n",
      "Epoch 31, Batch 681, LR 0.000011 Loss 4.634828, Accuracy 88.895%\n",
      "Epoch 31, Batch 682, LR 0.000011 Loss 4.634170, Accuracy 88.904%\n",
      "Epoch 31, Batch 683, LR 0.000011 Loss 4.632971, Accuracy 88.910%\n",
      "Epoch 31, Batch 684, LR 0.000011 Loss 4.632188, Accuracy 88.913%\n",
      "Epoch 31, Batch 685, LR 0.000011 Loss 4.631565, Accuracy 88.912%\n",
      "Epoch 31, Batch 686, LR 0.000011 Loss 4.633181, Accuracy 88.903%\n",
      "Epoch 31, Batch 687, LR 0.000011 Loss 4.632843, Accuracy 88.907%\n",
      "Epoch 31, Batch 688, LR 0.000011 Loss 4.632323, Accuracy 88.913%\n",
      "Epoch 31, Batch 689, LR 0.000011 Loss 4.632123, Accuracy 88.909%\n",
      "Epoch 31, Batch 690, LR 0.000011 Loss 4.631733, Accuracy 88.910%\n",
      "Epoch 31, Batch 691, LR 0.000011 Loss 4.630921, Accuracy 88.912%\n",
      "Epoch 31, Batch 692, LR 0.000011 Loss 4.630619, Accuracy 88.912%\n",
      "Epoch 31, Batch 693, LR 0.000011 Loss 4.629796, Accuracy 88.917%\n",
      "Epoch 31, Batch 694, LR 0.000011 Loss 4.631008, Accuracy 88.912%\n",
      "Epoch 31, Batch 695, LR 0.000011 Loss 4.631167, Accuracy 88.908%\n",
      "Epoch 31, Batch 696, LR 0.000011 Loss 4.631353, Accuracy 88.910%\n",
      "Epoch 31, Batch 697, LR 0.000011 Loss 4.632318, Accuracy 88.904%\n",
      "Epoch 31, Batch 698, LR 0.000011 Loss 4.631232, Accuracy 88.913%\n",
      "Epoch 31, Batch 699, LR 0.000011 Loss 4.630746, Accuracy 88.910%\n",
      "Epoch 31, Batch 700, LR 0.000011 Loss 4.631477, Accuracy 88.906%\n",
      "Epoch 31, Batch 701, LR 0.000011 Loss 4.631718, Accuracy 88.904%\n",
      "Epoch 31, Batch 702, LR 0.000011 Loss 4.632707, Accuracy 88.901%\n",
      "Epoch 31, Batch 703, LR 0.000011 Loss 4.632738, Accuracy 88.905%\n",
      "Epoch 31, Batch 704, LR 0.000011 Loss 4.631429, Accuracy 88.910%\n",
      "Epoch 31, Batch 705, LR 0.000011 Loss 4.631903, Accuracy 88.904%\n",
      "Epoch 31, Batch 706, LR 0.000011 Loss 4.632384, Accuracy 88.904%\n",
      "Epoch 31, Batch 707, LR 0.000011 Loss 4.632473, Accuracy 88.908%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 708, LR 0.000011 Loss 4.632728, Accuracy 88.906%\n",
      "Epoch 31, Batch 709, LR 0.000011 Loss 4.632526, Accuracy 88.909%\n",
      "Epoch 31, Batch 710, LR 0.000011 Loss 4.632827, Accuracy 88.904%\n",
      "Epoch 31, Batch 711, LR 0.000011 Loss 4.632365, Accuracy 88.908%\n",
      "Epoch 31, Batch 712, LR 0.000011 Loss 4.631447, Accuracy 88.913%\n",
      "Epoch 31, Batch 713, LR 0.000011 Loss 4.631185, Accuracy 88.916%\n",
      "Epoch 31, Batch 714, LR 0.000011 Loss 4.630581, Accuracy 88.913%\n",
      "Epoch 31, Batch 715, LR 0.000011 Loss 4.631170, Accuracy 88.910%\n",
      "Epoch 31, Batch 716, LR 0.000011 Loss 4.630502, Accuracy 88.913%\n",
      "Epoch 31, Batch 717, LR 0.000011 Loss 4.630395, Accuracy 88.913%\n",
      "Epoch 31, Batch 718, LR 0.000011 Loss 4.630284, Accuracy 88.913%\n",
      "Epoch 31, Batch 719, LR 0.000011 Loss 4.631563, Accuracy 88.908%\n",
      "Epoch 31, Batch 720, LR 0.000011 Loss 4.631736, Accuracy 88.907%\n",
      "Epoch 31, Batch 721, LR 0.000011 Loss 4.631299, Accuracy 88.913%\n",
      "Epoch 31, Batch 722, LR 0.000011 Loss 4.632115, Accuracy 88.911%\n",
      "Epoch 31, Batch 723, LR 0.000011 Loss 4.632720, Accuracy 88.907%\n",
      "Epoch 31, Batch 724, LR 0.000011 Loss 4.632997, Accuracy 88.911%\n",
      "Epoch 31, Batch 725, LR 0.000011 Loss 4.632482, Accuracy 88.916%\n",
      "Epoch 31, Batch 726, LR 0.000011 Loss 4.632201, Accuracy 88.918%\n",
      "Epoch 31, Batch 727, LR 0.000011 Loss 4.631831, Accuracy 88.919%\n",
      "Epoch 31, Batch 728, LR 0.000011 Loss 4.632551, Accuracy 88.915%\n",
      "Epoch 31, Batch 729, LR 0.000011 Loss 4.633044, Accuracy 88.910%\n",
      "Epoch 31, Batch 730, LR 0.000011 Loss 4.633428, Accuracy 88.911%\n",
      "Epoch 31, Batch 731, LR 0.000011 Loss 4.633099, Accuracy 88.911%\n",
      "Epoch 31, Batch 732, LR 0.000011 Loss 4.632917, Accuracy 88.914%\n",
      "Epoch 31, Batch 733, LR 0.000011 Loss 4.632505, Accuracy 88.914%\n",
      "Epoch 31, Batch 734, LR 0.000011 Loss 4.631761, Accuracy 88.919%\n",
      "Epoch 31, Batch 735, LR 0.000011 Loss 4.631968, Accuracy 88.919%\n",
      "Epoch 31, Batch 736, LR 0.000011 Loss 4.631537, Accuracy 88.923%\n",
      "Epoch 31, Batch 737, LR 0.000011 Loss 4.630719, Accuracy 88.932%\n",
      "Epoch 31, Batch 738, LR 0.000011 Loss 4.630521, Accuracy 88.935%\n",
      "Epoch 31, Batch 739, LR 0.000011 Loss 4.630952, Accuracy 88.935%\n",
      "Epoch 31, Batch 740, LR 0.000011 Loss 4.631597, Accuracy 88.931%\n",
      "Epoch 31, Batch 741, LR 0.000011 Loss 4.631299, Accuracy 88.933%\n",
      "Epoch 31, Batch 742, LR 0.000011 Loss 4.630640, Accuracy 88.938%\n",
      "Epoch 31, Batch 743, LR 0.000011 Loss 4.630739, Accuracy 88.943%\n",
      "Epoch 31, Batch 744, LR 0.000011 Loss 4.631002, Accuracy 88.940%\n",
      "Epoch 31, Batch 745, LR 0.000011 Loss 4.630088, Accuracy 88.945%\n",
      "Epoch 31, Batch 746, LR 0.000011 Loss 4.630458, Accuracy 88.943%\n",
      "Epoch 31, Batch 747, LR 0.000011 Loss 4.629980, Accuracy 88.943%\n",
      "Epoch 31, Batch 748, LR 0.000011 Loss 4.630324, Accuracy 88.938%\n",
      "Epoch 31, Batch 749, LR 0.000011 Loss 4.629990, Accuracy 88.940%\n",
      "Epoch 31, Batch 750, LR 0.000011 Loss 4.629734, Accuracy 88.938%\n",
      "Epoch 31, Batch 751, LR 0.000011 Loss 4.629998, Accuracy 88.938%\n",
      "Epoch 31, Batch 752, LR 0.000011 Loss 4.630472, Accuracy 88.936%\n",
      "Epoch 31, Batch 753, LR 0.000011 Loss 4.629220, Accuracy 88.940%\n",
      "Epoch 31, Batch 754, LR 0.000011 Loss 4.628764, Accuracy 88.941%\n",
      "Epoch 31, Batch 755, LR 0.000011 Loss 4.628659, Accuracy 88.942%\n",
      "Epoch 31, Batch 756, LR 0.000011 Loss 4.629124, Accuracy 88.944%\n",
      "Epoch 31, Batch 757, LR 0.000011 Loss 4.629319, Accuracy 88.940%\n",
      "Epoch 31, Batch 758, LR 0.000011 Loss 4.629319, Accuracy 88.942%\n",
      "Epoch 31, Batch 759, LR 0.000011 Loss 4.629708, Accuracy 88.940%\n",
      "Epoch 31, Batch 760, LR 0.000011 Loss 4.629396, Accuracy 88.943%\n",
      "Epoch 31, Batch 761, LR 0.000011 Loss 4.628971, Accuracy 88.942%\n",
      "Epoch 31, Batch 762, LR 0.000011 Loss 4.629752, Accuracy 88.942%\n",
      "Epoch 31, Batch 763, LR 0.000011 Loss 4.630594, Accuracy 88.939%\n",
      "Epoch 31, Batch 764, LR 0.000011 Loss 4.631100, Accuracy 88.935%\n",
      "Epoch 31, Batch 765, LR 0.000011 Loss 4.630432, Accuracy 88.935%\n",
      "Epoch 31, Batch 766, LR 0.000011 Loss 4.630523, Accuracy 88.930%\n",
      "Epoch 31, Batch 767, LR 0.000011 Loss 4.630411, Accuracy 88.930%\n",
      "Epoch 31, Batch 768, LR 0.000011 Loss 4.628903, Accuracy 88.939%\n",
      "Epoch 31, Batch 769, LR 0.000011 Loss 4.628937, Accuracy 88.938%\n",
      "Epoch 31, Batch 770, LR 0.000011 Loss 4.629031, Accuracy 88.935%\n",
      "Epoch 31, Batch 771, LR 0.000011 Loss 4.629319, Accuracy 88.936%\n",
      "Epoch 31, Batch 772, LR 0.000011 Loss 4.628569, Accuracy 88.943%\n",
      "Epoch 31, Batch 773, LR 0.000011 Loss 4.628163, Accuracy 88.948%\n",
      "Epoch 31, Batch 774, LR 0.000011 Loss 4.627784, Accuracy 88.946%\n",
      "Epoch 31, Batch 775, LR 0.000011 Loss 4.628551, Accuracy 88.944%\n",
      "Epoch 31, Batch 776, LR 0.000011 Loss 4.627643, Accuracy 88.951%\n",
      "Epoch 31, Batch 777, LR 0.000011 Loss 4.628523, Accuracy 88.943%\n",
      "Epoch 31, Batch 778, LR 0.000011 Loss 4.628599, Accuracy 88.947%\n",
      "Epoch 31, Batch 779, LR 0.000011 Loss 4.628592, Accuracy 88.945%\n",
      "Epoch 31, Batch 780, LR 0.000011 Loss 4.628865, Accuracy 88.942%\n",
      "Epoch 31, Batch 781, LR 0.000011 Loss 4.629076, Accuracy 88.945%\n",
      "Epoch 31, Batch 782, LR 0.000011 Loss 4.628250, Accuracy 88.950%\n",
      "Epoch 31, Batch 783, LR 0.000011 Loss 4.628761, Accuracy 88.949%\n",
      "Epoch 31, Batch 784, LR 0.000011 Loss 4.628122, Accuracy 88.952%\n",
      "Epoch 31, Batch 785, LR 0.000011 Loss 4.627628, Accuracy 88.951%\n",
      "Epoch 31, Batch 786, LR 0.000011 Loss 4.627435, Accuracy 88.949%\n",
      "Epoch 31, Batch 787, LR 0.000011 Loss 4.626810, Accuracy 88.948%\n",
      "Epoch 31, Batch 788, LR 0.000011 Loss 4.626488, Accuracy 88.950%\n",
      "Epoch 31, Batch 789, LR 0.000011 Loss 4.626696, Accuracy 88.948%\n",
      "Epoch 31, Batch 790, LR 0.000011 Loss 4.626142, Accuracy 88.952%\n",
      "Epoch 31, Batch 791, LR 0.000011 Loss 4.626611, Accuracy 88.946%\n",
      "Epoch 31, Batch 792, LR 0.000011 Loss 4.626950, Accuracy 88.946%\n",
      "Epoch 31, Batch 793, LR 0.000011 Loss 4.626907, Accuracy 88.945%\n",
      "Epoch 31, Batch 794, LR 0.000011 Loss 4.626904, Accuracy 88.944%\n",
      "Epoch 31, Batch 795, LR 0.000011 Loss 4.625948, Accuracy 88.949%\n",
      "Epoch 31, Batch 796, LR 0.000011 Loss 4.627055, Accuracy 88.944%\n",
      "Epoch 31, Batch 797, LR 0.000011 Loss 4.627622, Accuracy 88.939%\n",
      "Epoch 31, Batch 798, LR 0.000011 Loss 4.627736, Accuracy 88.938%\n",
      "Epoch 31, Batch 799, LR 0.000011 Loss 4.627434, Accuracy 88.942%\n",
      "Epoch 31, Batch 800, LR 0.000011 Loss 4.627626, Accuracy 88.939%\n",
      "Epoch 31, Batch 801, LR 0.000011 Loss 4.627336, Accuracy 88.939%\n",
      "Epoch 31, Batch 802, LR 0.000011 Loss 4.626932, Accuracy 88.939%\n",
      "Epoch 31, Batch 803, LR 0.000011 Loss 4.627381, Accuracy 88.937%\n",
      "Epoch 31, Batch 804, LR 0.000011 Loss 4.626262, Accuracy 88.941%\n",
      "Epoch 31, Batch 805, LR 0.000011 Loss 4.625967, Accuracy 88.944%\n",
      "Epoch 31, Batch 806, LR 0.000011 Loss 4.625947, Accuracy 88.942%\n",
      "Epoch 31, Batch 807, LR 0.000011 Loss 4.626332, Accuracy 88.941%\n",
      "Epoch 31, Batch 808, LR 0.000011 Loss 4.626708, Accuracy 88.938%\n",
      "Epoch 31, Batch 809, LR 0.000011 Loss 4.626243, Accuracy 88.942%\n",
      "Epoch 31, Batch 810, LR 0.000011 Loss 4.626622, Accuracy 88.941%\n",
      "Epoch 31, Batch 811, LR 0.000011 Loss 4.626879, Accuracy 88.941%\n",
      "Epoch 31, Batch 812, LR 0.000011 Loss 4.627100, Accuracy 88.936%\n",
      "Epoch 31, Batch 813, LR 0.000011 Loss 4.627344, Accuracy 88.933%\n",
      "Epoch 31, Batch 814, LR 0.000011 Loss 4.627183, Accuracy 88.934%\n",
      "Epoch 31, Batch 815, LR 0.000011 Loss 4.627027, Accuracy 88.934%\n",
      "Epoch 31, Batch 816, LR 0.000011 Loss 4.626530, Accuracy 88.940%\n",
      "Epoch 31, Batch 817, LR 0.000011 Loss 4.626242, Accuracy 88.944%\n",
      "Epoch 31, Batch 818, LR 0.000011 Loss 4.626787, Accuracy 88.944%\n",
      "Epoch 31, Batch 819, LR 0.000011 Loss 4.625902, Accuracy 88.945%\n",
      "Epoch 31, Batch 820, LR 0.000011 Loss 4.625466, Accuracy 88.947%\n",
      "Epoch 31, Batch 821, LR 0.000011 Loss 4.625964, Accuracy 88.945%\n",
      "Epoch 31, Batch 822, LR 0.000011 Loss 4.625879, Accuracy 88.944%\n",
      "Epoch 31, Batch 823, LR 0.000011 Loss 4.625712, Accuracy 88.947%\n",
      "Epoch 31, Batch 824, LR 0.000011 Loss 4.626654, Accuracy 88.942%\n",
      "Epoch 31, Batch 825, LR 0.000011 Loss 4.627395, Accuracy 88.937%\n",
      "Epoch 31, Batch 826, LR 0.000011 Loss 4.627957, Accuracy 88.931%\n",
      "Epoch 31, Batch 827, LR 0.000011 Loss 4.627561, Accuracy 88.935%\n",
      "Epoch 31, Batch 828, LR 0.000011 Loss 4.626967, Accuracy 88.934%\n",
      "Epoch 31, Batch 829, LR 0.000011 Loss 4.627419, Accuracy 88.932%\n",
      "Epoch 31, Batch 830, LR 0.000011 Loss 4.627170, Accuracy 88.931%\n",
      "Epoch 31, Batch 831, LR 0.000011 Loss 4.627880, Accuracy 88.931%\n",
      "Epoch 31, Batch 832, LR 0.000011 Loss 4.627847, Accuracy 88.931%\n",
      "Epoch 31, Batch 833, LR 0.000011 Loss 4.628077, Accuracy 88.931%\n",
      "Epoch 31, Batch 834, LR 0.000011 Loss 4.627631, Accuracy 88.933%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 835, LR 0.000011 Loss 4.628310, Accuracy 88.930%\n",
      "Epoch 31, Batch 836, LR 0.000011 Loss 4.627984, Accuracy 88.928%\n",
      "Epoch 31, Batch 837, LR 0.000011 Loss 4.627898, Accuracy 88.926%\n",
      "Epoch 31, Batch 838, LR 0.000011 Loss 4.628365, Accuracy 88.924%\n",
      "Epoch 31, Batch 839, LR 0.000011 Loss 4.628146, Accuracy 88.927%\n",
      "Epoch 31, Batch 840, LR 0.000011 Loss 4.627320, Accuracy 88.929%\n",
      "Epoch 31, Batch 841, LR 0.000011 Loss 4.627347, Accuracy 88.927%\n",
      "Epoch 31, Batch 842, LR 0.000011 Loss 4.627922, Accuracy 88.927%\n",
      "Epoch 31, Batch 843, LR 0.000011 Loss 4.628240, Accuracy 88.930%\n",
      "Epoch 31, Batch 844, LR 0.000011 Loss 4.629889, Accuracy 88.920%\n",
      "Epoch 31, Batch 845, LR 0.000011 Loss 4.629674, Accuracy 88.921%\n",
      "Epoch 31, Batch 846, LR 0.000011 Loss 4.629843, Accuracy 88.918%\n",
      "Epoch 31, Batch 847, LR 0.000011 Loss 4.629429, Accuracy 88.920%\n",
      "Epoch 31, Batch 848, LR 0.000011 Loss 4.628594, Accuracy 88.924%\n",
      "Epoch 31, Batch 849, LR 0.000011 Loss 4.628583, Accuracy 88.926%\n",
      "Epoch 31, Batch 850, LR 0.000011 Loss 4.627828, Accuracy 88.932%\n",
      "Epoch 31, Batch 851, LR 0.000011 Loss 4.627087, Accuracy 88.938%\n",
      "Epoch 31, Batch 852, LR 0.000011 Loss 4.627359, Accuracy 88.934%\n",
      "Epoch 31, Batch 853, LR 0.000011 Loss 4.627622, Accuracy 88.933%\n",
      "Epoch 31, Batch 854, LR 0.000011 Loss 4.627923, Accuracy 88.934%\n",
      "Epoch 31, Batch 855, LR 0.000011 Loss 4.627619, Accuracy 88.931%\n",
      "Epoch 31, Batch 856, LR 0.000011 Loss 4.627652, Accuracy 88.930%\n",
      "Epoch 31, Batch 857, LR 0.000011 Loss 4.627837, Accuracy 88.927%\n",
      "Epoch 31, Batch 858, LR 0.000011 Loss 4.628810, Accuracy 88.924%\n",
      "Epoch 31, Batch 859, LR 0.000011 Loss 4.628057, Accuracy 88.928%\n",
      "Epoch 31, Batch 860, LR 0.000011 Loss 4.627321, Accuracy 88.933%\n",
      "Epoch 31, Batch 861, LR 0.000011 Loss 4.626031, Accuracy 88.938%\n",
      "Epoch 31, Batch 862, LR 0.000011 Loss 4.626042, Accuracy 88.938%\n",
      "Epoch 31, Batch 863, LR 0.000011 Loss 4.626356, Accuracy 88.934%\n",
      "Epoch 31, Batch 864, LR 0.000011 Loss 4.627353, Accuracy 88.929%\n",
      "Epoch 31, Batch 865, LR 0.000011 Loss 4.627278, Accuracy 88.927%\n",
      "Epoch 31, Batch 866, LR 0.000011 Loss 4.627688, Accuracy 88.925%\n",
      "Epoch 31, Batch 867, LR 0.000011 Loss 4.627651, Accuracy 88.928%\n",
      "Epoch 31, Batch 868, LR 0.000011 Loss 4.627320, Accuracy 88.931%\n",
      "Epoch 31, Batch 869, LR 0.000011 Loss 4.627299, Accuracy 88.931%\n",
      "Epoch 31, Batch 870, LR 0.000011 Loss 4.627023, Accuracy 88.935%\n",
      "Epoch 31, Batch 871, LR 0.000011 Loss 4.627105, Accuracy 88.937%\n",
      "Epoch 31, Batch 872, LR 0.000011 Loss 4.627221, Accuracy 88.941%\n",
      "Epoch 31, Batch 873, LR 0.000011 Loss 4.627644, Accuracy 88.936%\n",
      "Epoch 31, Batch 874, LR 0.000011 Loss 4.627238, Accuracy 88.936%\n",
      "Epoch 31, Batch 875, LR 0.000011 Loss 4.626817, Accuracy 88.938%\n",
      "Epoch 31, Batch 876, LR 0.000011 Loss 4.626762, Accuracy 88.939%\n",
      "Epoch 31, Batch 877, LR 0.000011 Loss 4.627261, Accuracy 88.934%\n",
      "Epoch 31, Batch 878, LR 0.000011 Loss 4.628259, Accuracy 88.928%\n",
      "Epoch 31, Batch 879, LR 0.000011 Loss 4.628343, Accuracy 88.931%\n",
      "Epoch 31, Batch 880, LR 0.000011 Loss 4.629063, Accuracy 88.928%\n",
      "Epoch 31, Batch 881, LR 0.000011 Loss 4.629420, Accuracy 88.930%\n",
      "Epoch 31, Batch 882, LR 0.000011 Loss 4.628619, Accuracy 88.933%\n",
      "Epoch 31, Batch 883, LR 0.000011 Loss 4.628751, Accuracy 88.931%\n",
      "Epoch 31, Batch 884, LR 0.000011 Loss 4.629428, Accuracy 88.926%\n",
      "Epoch 31, Batch 885, LR 0.000011 Loss 4.629414, Accuracy 88.926%\n",
      "Epoch 31, Batch 886, LR 0.000011 Loss 4.629314, Accuracy 88.927%\n",
      "Epoch 31, Batch 887, LR 0.000011 Loss 4.628757, Accuracy 88.930%\n",
      "Epoch 31, Batch 888, LR 0.000011 Loss 4.628502, Accuracy 88.931%\n",
      "Epoch 31, Batch 889, LR 0.000011 Loss 4.628641, Accuracy 88.935%\n",
      "Epoch 31, Batch 890, LR 0.000011 Loss 4.628929, Accuracy 88.934%\n",
      "Epoch 31, Batch 891, LR 0.000011 Loss 4.629604, Accuracy 88.928%\n",
      "Epoch 31, Batch 892, LR 0.000011 Loss 4.629519, Accuracy 88.927%\n",
      "Epoch 31, Batch 893, LR 0.000011 Loss 4.629377, Accuracy 88.925%\n",
      "Epoch 31, Batch 894, LR 0.000011 Loss 4.628898, Accuracy 88.929%\n",
      "Epoch 31, Batch 895, LR 0.000011 Loss 4.628821, Accuracy 88.929%\n",
      "Epoch 31, Batch 896, LR 0.000011 Loss 4.629750, Accuracy 88.927%\n",
      "Epoch 31, Batch 897, LR 0.000011 Loss 4.629682, Accuracy 88.928%\n",
      "Epoch 31, Batch 898, LR 0.000011 Loss 4.629775, Accuracy 88.929%\n",
      "Epoch 31, Batch 899, LR 0.000011 Loss 4.630095, Accuracy 88.925%\n",
      "Epoch 31, Batch 900, LR 0.000011 Loss 4.629472, Accuracy 88.928%\n",
      "Epoch 31, Batch 901, LR 0.000011 Loss 4.630476, Accuracy 88.923%\n",
      "Epoch 31, Batch 902, LR 0.000011 Loss 4.630144, Accuracy 88.927%\n",
      "Epoch 31, Batch 903, LR 0.000011 Loss 4.630594, Accuracy 88.928%\n",
      "Epoch 31, Batch 904, LR 0.000011 Loss 4.630811, Accuracy 88.925%\n",
      "Epoch 31, Batch 905, LR 0.000011 Loss 4.629710, Accuracy 88.932%\n",
      "Epoch 31, Batch 906, LR 0.000011 Loss 4.629966, Accuracy 88.930%\n",
      "Epoch 31, Batch 907, LR 0.000011 Loss 4.629893, Accuracy 88.929%\n",
      "Epoch 31, Batch 908, LR 0.000011 Loss 4.630308, Accuracy 88.927%\n",
      "Epoch 31, Batch 909, LR 0.000011 Loss 4.630634, Accuracy 88.922%\n",
      "Epoch 31, Batch 910, LR 0.000011 Loss 4.631451, Accuracy 88.921%\n",
      "Epoch 31, Batch 911, LR 0.000011 Loss 4.631613, Accuracy 88.915%\n",
      "Epoch 31, Batch 912, LR 0.000011 Loss 4.632680, Accuracy 88.910%\n",
      "Epoch 31, Batch 913, LR 0.000011 Loss 4.632450, Accuracy 88.912%\n",
      "Epoch 31, Batch 914, LR 0.000011 Loss 4.632970, Accuracy 88.909%\n",
      "Epoch 31, Batch 915, LR 0.000011 Loss 4.633351, Accuracy 88.912%\n",
      "Epoch 31, Batch 916, LR 0.000011 Loss 4.632831, Accuracy 88.914%\n",
      "Epoch 31, Batch 917, LR 0.000011 Loss 4.632527, Accuracy 88.917%\n",
      "Epoch 31, Batch 918, LR 0.000011 Loss 4.632534, Accuracy 88.918%\n",
      "Epoch 31, Batch 919, LR 0.000011 Loss 4.632046, Accuracy 88.923%\n",
      "Epoch 31, Batch 920, LR 0.000011 Loss 4.631382, Accuracy 88.925%\n",
      "Epoch 31, Batch 921, LR 0.000011 Loss 4.631171, Accuracy 88.928%\n",
      "Epoch 31, Batch 922, LR 0.000011 Loss 4.631459, Accuracy 88.927%\n",
      "Epoch 31, Batch 923, LR 0.000011 Loss 4.631516, Accuracy 88.924%\n",
      "Epoch 31, Batch 924, LR 0.000011 Loss 4.630967, Accuracy 88.924%\n",
      "Epoch 31, Batch 925, LR 0.000011 Loss 4.630642, Accuracy 88.926%\n",
      "Epoch 31, Batch 926, LR 0.000011 Loss 4.630827, Accuracy 88.924%\n",
      "Epoch 31, Batch 927, LR 0.000011 Loss 4.630808, Accuracy 88.923%\n",
      "Epoch 31, Batch 928, LR 0.000011 Loss 4.630825, Accuracy 88.927%\n",
      "Epoch 31, Batch 929, LR 0.000011 Loss 4.631260, Accuracy 88.926%\n",
      "Epoch 31, Batch 930, LR 0.000011 Loss 4.631197, Accuracy 88.927%\n",
      "Epoch 31, Batch 931, LR 0.000011 Loss 4.630831, Accuracy 88.931%\n",
      "Epoch 31, Batch 932, LR 0.000011 Loss 4.630701, Accuracy 88.931%\n",
      "Epoch 31, Batch 933, LR 0.000011 Loss 4.629836, Accuracy 88.937%\n",
      "Epoch 31, Batch 934, LR 0.000011 Loss 4.629291, Accuracy 88.938%\n",
      "Epoch 31, Batch 935, LR 0.000011 Loss 4.628979, Accuracy 88.937%\n",
      "Epoch 31, Batch 936, LR 0.000011 Loss 4.628381, Accuracy 88.936%\n",
      "Epoch 31, Batch 937, LR 0.000011 Loss 4.627787, Accuracy 88.935%\n",
      "Epoch 31, Batch 938, LR 0.000011 Loss 4.627647, Accuracy 88.933%\n",
      "Epoch 31, Batch 939, LR 0.000011 Loss 4.627181, Accuracy 88.938%\n",
      "Epoch 31, Batch 940, LR 0.000011 Loss 4.627540, Accuracy 88.932%\n",
      "Epoch 31, Batch 941, LR 0.000011 Loss 4.627122, Accuracy 88.936%\n",
      "Epoch 31, Batch 942, LR 0.000011 Loss 4.626933, Accuracy 88.934%\n",
      "Epoch 31, Batch 943, LR 0.000011 Loss 4.626579, Accuracy 88.935%\n",
      "Epoch 31, Batch 944, LR 0.000011 Loss 4.626600, Accuracy 88.938%\n",
      "Epoch 31, Batch 945, LR 0.000011 Loss 4.626272, Accuracy 88.939%\n",
      "Epoch 31, Batch 946, LR 0.000011 Loss 4.625263, Accuracy 88.944%\n",
      "Epoch 31, Batch 947, LR 0.000011 Loss 4.626777, Accuracy 88.936%\n",
      "Epoch 31, Batch 948, LR 0.000011 Loss 4.628000, Accuracy 88.927%\n",
      "Epoch 31, Batch 949, LR 0.000011 Loss 4.628195, Accuracy 88.926%\n",
      "Epoch 31, Batch 950, LR 0.000011 Loss 4.628143, Accuracy 88.930%\n",
      "Epoch 31, Batch 951, LR 0.000011 Loss 4.627922, Accuracy 88.931%\n",
      "Epoch 31, Batch 952, LR 0.000011 Loss 4.628057, Accuracy 88.931%\n",
      "Epoch 31, Batch 953, LR 0.000011 Loss 4.627988, Accuracy 88.933%\n",
      "Epoch 31, Batch 954, LR 0.000011 Loss 4.628403, Accuracy 88.931%\n",
      "Epoch 31, Batch 955, LR 0.000011 Loss 4.628408, Accuracy 88.930%\n",
      "Epoch 31, Batch 956, LR 0.000011 Loss 4.628213, Accuracy 88.928%\n",
      "Epoch 31, Batch 957, LR 0.000011 Loss 4.628572, Accuracy 88.928%\n",
      "Epoch 31, Batch 958, LR 0.000011 Loss 4.629194, Accuracy 88.921%\n",
      "Epoch 31, Batch 959, LR 0.000011 Loss 4.629618, Accuracy 88.921%\n",
      "Epoch 31, Batch 960, LR 0.000011 Loss 4.630216, Accuracy 88.921%\n",
      "Epoch 31, Batch 961, LR 0.000011 Loss 4.629999, Accuracy 88.921%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 962, LR 0.000011 Loss 4.630043, Accuracy 88.920%\n",
      "Epoch 31, Batch 963, LR 0.000011 Loss 4.630188, Accuracy 88.921%\n",
      "Epoch 31, Batch 964, LR 0.000011 Loss 4.630383, Accuracy 88.919%\n",
      "Epoch 31, Batch 965, LR 0.000011 Loss 4.630328, Accuracy 88.920%\n",
      "Epoch 31, Batch 966, LR 0.000011 Loss 4.630501, Accuracy 88.918%\n",
      "Epoch 31, Batch 967, LR 0.000011 Loss 4.630622, Accuracy 88.915%\n",
      "Epoch 31, Batch 968, LR 0.000011 Loss 4.630028, Accuracy 88.916%\n",
      "Epoch 31, Batch 969, LR 0.000011 Loss 4.630209, Accuracy 88.917%\n",
      "Epoch 31, Batch 970, LR 0.000011 Loss 4.630006, Accuracy 88.916%\n",
      "Epoch 31, Batch 971, LR 0.000011 Loss 4.630709, Accuracy 88.910%\n",
      "Epoch 31, Batch 972, LR 0.000011 Loss 4.630477, Accuracy 88.913%\n",
      "Epoch 31, Batch 973, LR 0.000011 Loss 4.630907, Accuracy 88.911%\n",
      "Epoch 31, Batch 974, LR 0.000011 Loss 4.631195, Accuracy 88.910%\n",
      "Epoch 31, Batch 975, LR 0.000011 Loss 4.631266, Accuracy 88.911%\n",
      "Epoch 31, Batch 976, LR 0.000011 Loss 4.631516, Accuracy 88.910%\n",
      "Epoch 31, Batch 977, LR 0.000011 Loss 4.631281, Accuracy 88.909%\n",
      "Epoch 31, Batch 978, LR 0.000011 Loss 4.631313, Accuracy 88.911%\n",
      "Epoch 31, Batch 979, LR 0.000011 Loss 4.631460, Accuracy 88.908%\n",
      "Epoch 31, Batch 980, LR 0.000011 Loss 4.631771, Accuracy 88.907%\n",
      "Epoch 31, Batch 981, LR 0.000011 Loss 4.631093, Accuracy 88.907%\n",
      "Epoch 31, Batch 982, LR 0.000011 Loss 4.630733, Accuracy 88.907%\n",
      "Epoch 31, Batch 983, LR 0.000011 Loss 4.630767, Accuracy 88.908%\n",
      "Epoch 31, Batch 984, LR 0.000011 Loss 4.630878, Accuracy 88.909%\n",
      "Epoch 31, Batch 985, LR 0.000011 Loss 4.630610, Accuracy 88.909%\n",
      "Epoch 31, Batch 986, LR 0.000011 Loss 4.630316, Accuracy 88.909%\n",
      "Epoch 31, Batch 987, LR 0.000011 Loss 4.630047, Accuracy 88.912%\n",
      "Epoch 31, Batch 988, LR 0.000011 Loss 4.629898, Accuracy 88.915%\n",
      "Epoch 31, Batch 989, LR 0.000011 Loss 4.630016, Accuracy 88.912%\n",
      "Epoch 31, Batch 990, LR 0.000011 Loss 4.629295, Accuracy 88.917%\n",
      "Epoch 31, Batch 991, LR 0.000011 Loss 4.628765, Accuracy 88.919%\n",
      "Epoch 31, Batch 992, LR 0.000011 Loss 4.628101, Accuracy 88.922%\n",
      "Epoch 31, Batch 993, LR 0.000011 Loss 4.627038, Accuracy 88.926%\n",
      "Epoch 31, Batch 994, LR 0.000011 Loss 4.627015, Accuracy 88.927%\n",
      "Epoch 31, Batch 995, LR 0.000011 Loss 4.627175, Accuracy 88.927%\n",
      "Epoch 31, Batch 996, LR 0.000011 Loss 4.627105, Accuracy 88.924%\n",
      "Epoch 31, Batch 997, LR 0.000011 Loss 4.626639, Accuracy 88.925%\n",
      "Epoch 31, Batch 998, LR 0.000011 Loss 4.626085, Accuracy 88.929%\n",
      "Epoch 31, Batch 999, LR 0.000011 Loss 4.626744, Accuracy 88.927%\n",
      "Epoch 31, Batch 1000, LR 0.000011 Loss 4.627076, Accuracy 88.927%\n",
      "Epoch 31, Batch 1001, LR 0.000011 Loss 4.627335, Accuracy 88.926%\n",
      "Epoch 31, Batch 1002, LR 0.000011 Loss 4.627466, Accuracy 88.924%\n",
      "Epoch 31, Batch 1003, LR 0.000011 Loss 4.628031, Accuracy 88.922%\n",
      "Epoch 31, Batch 1004, LR 0.000011 Loss 4.627934, Accuracy 88.918%\n",
      "Epoch 31, Batch 1005, LR 0.000011 Loss 4.627489, Accuracy 88.919%\n",
      "Epoch 31, Batch 1006, LR 0.000011 Loss 4.626866, Accuracy 88.920%\n",
      "Epoch 31, Batch 1007, LR 0.000011 Loss 4.626425, Accuracy 88.920%\n",
      "Epoch 31, Batch 1008, LR 0.000011 Loss 4.626043, Accuracy 88.921%\n",
      "Epoch 31, Batch 1009, LR 0.000011 Loss 4.625328, Accuracy 88.922%\n",
      "Epoch 31, Batch 1010, LR 0.000011 Loss 4.624963, Accuracy 88.921%\n",
      "Epoch 31, Batch 1011, LR 0.000011 Loss 4.625048, Accuracy 88.921%\n",
      "Epoch 31, Batch 1012, LR 0.000011 Loss 4.625354, Accuracy 88.924%\n",
      "Epoch 31, Batch 1013, LR 0.000011 Loss 4.625055, Accuracy 88.928%\n",
      "Epoch 31, Batch 1014, LR 0.000011 Loss 4.624481, Accuracy 88.930%\n",
      "Epoch 31, Batch 1015, LR 0.000011 Loss 4.624275, Accuracy 88.932%\n",
      "Epoch 31, Batch 1016, LR 0.000011 Loss 4.623980, Accuracy 88.932%\n",
      "Epoch 31, Batch 1017, LR 0.000011 Loss 4.623098, Accuracy 88.934%\n",
      "Epoch 31, Batch 1018, LR 0.000011 Loss 4.623536, Accuracy 88.931%\n",
      "Epoch 31, Batch 1019, LR 0.000011 Loss 4.623552, Accuracy 88.926%\n",
      "Epoch 31, Batch 1020, LR 0.000011 Loss 4.623259, Accuracy 88.928%\n",
      "Epoch 31, Batch 1021, LR 0.000011 Loss 4.622985, Accuracy 88.931%\n",
      "Epoch 31, Batch 1022, LR 0.000011 Loss 4.623541, Accuracy 88.928%\n",
      "Epoch 31, Batch 1023, LR 0.000011 Loss 4.623640, Accuracy 88.927%\n",
      "Epoch 31, Batch 1024, LR 0.000011 Loss 4.623095, Accuracy 88.932%\n",
      "Epoch 31, Batch 1025, LR 0.000011 Loss 4.623460, Accuracy 88.925%\n",
      "Epoch 31, Batch 1026, LR 0.000011 Loss 4.623878, Accuracy 88.924%\n",
      "Epoch 31, Batch 1027, LR 0.000011 Loss 4.623990, Accuracy 88.925%\n",
      "Epoch 31, Batch 1028, LR 0.000011 Loss 4.624329, Accuracy 88.922%\n",
      "Epoch 31, Batch 1029, LR 0.000011 Loss 4.623862, Accuracy 88.927%\n",
      "Epoch 31, Batch 1030, LR 0.000011 Loss 4.624597, Accuracy 88.921%\n",
      "Epoch 31, Batch 1031, LR 0.000011 Loss 4.625458, Accuracy 88.916%\n",
      "Epoch 31, Batch 1032, LR 0.000011 Loss 4.625028, Accuracy 88.916%\n",
      "Epoch 31, Batch 1033, LR 0.000011 Loss 4.624749, Accuracy 88.921%\n",
      "Epoch 31, Batch 1034, LR 0.000011 Loss 4.625571, Accuracy 88.920%\n",
      "Epoch 31, Batch 1035, LR 0.000011 Loss 4.625397, Accuracy 88.921%\n",
      "Epoch 31, Batch 1036, LR 0.000011 Loss 4.625086, Accuracy 88.924%\n",
      "Epoch 31, Batch 1037, LR 0.000011 Loss 4.624827, Accuracy 88.928%\n",
      "Epoch 31, Batch 1038, LR 0.000011 Loss 4.624281, Accuracy 88.932%\n",
      "Epoch 31, Batch 1039, LR 0.000011 Loss 4.623787, Accuracy 88.936%\n",
      "Epoch 31, Batch 1040, LR 0.000011 Loss 4.623027, Accuracy 88.940%\n",
      "Epoch 31, Batch 1041, LR 0.000011 Loss 4.623344, Accuracy 88.937%\n",
      "Epoch 31, Batch 1042, LR 0.000011 Loss 4.623738, Accuracy 88.935%\n",
      "Epoch 31, Batch 1043, LR 0.000011 Loss 4.623929, Accuracy 88.937%\n",
      "Epoch 31, Batch 1044, LR 0.000011 Loss 4.623852, Accuracy 88.938%\n",
      "Epoch 31, Batch 1045, LR 0.000011 Loss 4.623573, Accuracy 88.938%\n",
      "Epoch 31, Batch 1046, LR 0.000011 Loss 4.623078, Accuracy 88.941%\n",
      "Epoch 31, Batch 1047, LR 0.000011 Loss 4.623293, Accuracy 88.947%\n",
      "Epoch 31, Loss (train set) 4.623293, Accuracy (train set) 88.947%\n",
      "Epoch 32, Batch 1, LR 0.000011 Loss 4.321923, Accuracy 89.062%\n",
      "Epoch 32, Batch 2, LR 0.000011 Loss 4.449035, Accuracy 88.672%\n",
      "Epoch 32, Batch 3, LR 0.000011 Loss 4.319184, Accuracy 89.323%\n",
      "Epoch 32, Batch 4, LR 0.000011 Loss 4.430145, Accuracy 89.062%\n",
      "Epoch 32, Batch 5, LR 0.000011 Loss 4.469671, Accuracy 88.438%\n",
      "Epoch 32, Batch 6, LR 0.000011 Loss 4.458676, Accuracy 89.193%\n",
      "Epoch 32, Batch 7, LR 0.000011 Loss 4.554839, Accuracy 88.728%\n",
      "Epoch 32, Batch 8, LR 0.000011 Loss 4.594969, Accuracy 88.672%\n",
      "Epoch 32, Batch 9, LR 0.000011 Loss 4.629704, Accuracy 88.542%\n",
      "Epoch 32, Batch 10, LR 0.000011 Loss 4.559668, Accuracy 88.594%\n",
      "Epoch 32, Batch 11, LR 0.000011 Loss 4.579712, Accuracy 88.636%\n",
      "Epoch 32, Batch 12, LR 0.000011 Loss 4.589434, Accuracy 88.672%\n",
      "Epoch 32, Batch 13, LR 0.000011 Loss 4.624367, Accuracy 88.702%\n",
      "Epoch 32, Batch 14, LR 0.000011 Loss 4.639402, Accuracy 88.672%\n",
      "Epoch 32, Batch 15, LR 0.000011 Loss 4.652734, Accuracy 88.750%\n",
      "Epoch 32, Batch 16, LR 0.000011 Loss 4.622842, Accuracy 88.574%\n",
      "Epoch 32, Batch 17, LR 0.000011 Loss 4.643179, Accuracy 88.327%\n",
      "Epoch 32, Batch 18, LR 0.000011 Loss 4.601297, Accuracy 88.628%\n",
      "Epoch 32, Batch 19, LR 0.000010 Loss 4.635574, Accuracy 88.363%\n",
      "Epoch 32, Batch 20, LR 0.000010 Loss 4.632082, Accuracy 88.438%\n",
      "Epoch 32, Batch 21, LR 0.000010 Loss 4.588737, Accuracy 88.690%\n",
      "Epoch 32, Batch 22, LR 0.000010 Loss 4.591623, Accuracy 88.743%\n",
      "Epoch 32, Batch 23, LR 0.000010 Loss 4.582992, Accuracy 88.757%\n",
      "Epoch 32, Batch 24, LR 0.000010 Loss 4.580103, Accuracy 88.770%\n",
      "Epoch 32, Batch 25, LR 0.000010 Loss 4.586072, Accuracy 88.938%\n",
      "Epoch 32, Batch 26, LR 0.000010 Loss 4.589126, Accuracy 88.852%\n",
      "Epoch 32, Batch 27, LR 0.000010 Loss 4.585257, Accuracy 88.889%\n",
      "Epoch 32, Batch 28, LR 0.000010 Loss 4.585774, Accuracy 88.979%\n",
      "Epoch 32, Batch 29, LR 0.000010 Loss 4.566624, Accuracy 89.089%\n",
      "Epoch 32, Batch 30, LR 0.000010 Loss 4.545821, Accuracy 89.193%\n",
      "Epoch 32, Batch 31, LR 0.000010 Loss 4.551997, Accuracy 89.340%\n",
      "Epoch 32, Batch 32, LR 0.000010 Loss 4.542834, Accuracy 89.307%\n",
      "Epoch 32, Batch 33, LR 0.000010 Loss 4.541911, Accuracy 89.299%\n",
      "Epoch 32, Batch 34, LR 0.000010 Loss 4.554565, Accuracy 89.200%\n",
      "Epoch 32, Batch 35, LR 0.000010 Loss 4.568067, Accuracy 89.219%\n",
      "Epoch 32, Batch 36, LR 0.000010 Loss 4.565037, Accuracy 89.236%\n",
      "Epoch 32, Batch 37, LR 0.000010 Loss 4.557128, Accuracy 89.337%\n",
      "Epoch 32, Batch 38, LR 0.000010 Loss 4.543511, Accuracy 89.371%\n",
      "Epoch 32, Batch 39, LR 0.000010 Loss 4.558995, Accuracy 89.323%\n",
      "Epoch 32, Batch 40, LR 0.000010 Loss 4.556630, Accuracy 89.414%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 41, LR 0.000010 Loss 4.579009, Accuracy 89.272%\n",
      "Epoch 32, Batch 42, LR 0.000010 Loss 4.586889, Accuracy 89.286%\n",
      "Epoch 32, Batch 43, LR 0.000010 Loss 4.585820, Accuracy 89.299%\n",
      "Epoch 32, Batch 44, LR 0.000010 Loss 4.583990, Accuracy 89.276%\n",
      "Epoch 32, Batch 45, LR 0.000010 Loss 4.594450, Accuracy 89.323%\n",
      "Epoch 32, Batch 46, LR 0.000010 Loss 4.597385, Accuracy 89.283%\n",
      "Epoch 32, Batch 47, LR 0.000010 Loss 4.598052, Accuracy 89.312%\n",
      "Epoch 32, Batch 48, LR 0.000010 Loss 4.619749, Accuracy 89.258%\n",
      "Epoch 32, Batch 49, LR 0.000010 Loss 4.622930, Accuracy 89.174%\n",
      "Epoch 32, Batch 50, LR 0.000010 Loss 4.633508, Accuracy 89.094%\n",
      "Epoch 32, Batch 51, LR 0.000010 Loss 4.630353, Accuracy 89.047%\n",
      "Epoch 32, Batch 52, LR 0.000010 Loss 4.626912, Accuracy 89.062%\n",
      "Epoch 32, Batch 53, LR 0.000010 Loss 4.625690, Accuracy 89.062%\n",
      "Epoch 32, Batch 54, LR 0.000010 Loss 4.622220, Accuracy 89.005%\n",
      "Epoch 32, Batch 55, LR 0.000010 Loss 4.623124, Accuracy 89.034%\n",
      "Epoch 32, Batch 56, LR 0.000010 Loss 4.617807, Accuracy 89.035%\n",
      "Epoch 32, Batch 57, LR 0.000010 Loss 4.609683, Accuracy 89.021%\n",
      "Epoch 32, Batch 58, LR 0.000010 Loss 4.602544, Accuracy 89.022%\n",
      "Epoch 32, Batch 59, LR 0.000010 Loss 4.604879, Accuracy 88.996%\n",
      "Epoch 32, Batch 60, LR 0.000010 Loss 4.601038, Accuracy 88.971%\n",
      "Epoch 32, Batch 61, LR 0.000010 Loss 4.604803, Accuracy 88.896%\n",
      "Epoch 32, Batch 62, LR 0.000010 Loss 4.609182, Accuracy 88.899%\n",
      "Epoch 32, Batch 63, LR 0.000010 Loss 4.611571, Accuracy 88.876%\n",
      "Epoch 32, Batch 64, LR 0.000010 Loss 4.618738, Accuracy 88.855%\n",
      "Epoch 32, Batch 65, LR 0.000010 Loss 4.625362, Accuracy 88.798%\n",
      "Epoch 32, Batch 66, LR 0.000010 Loss 4.634823, Accuracy 88.731%\n",
      "Epoch 32, Batch 67, LR 0.000010 Loss 4.643427, Accuracy 88.678%\n",
      "Epoch 32, Batch 68, LR 0.000010 Loss 4.634031, Accuracy 88.752%\n",
      "Epoch 32, Batch 69, LR 0.000010 Loss 4.640564, Accuracy 88.734%\n",
      "Epoch 32, Batch 70, LR 0.000010 Loss 4.637969, Accuracy 88.761%\n",
      "Epoch 32, Batch 71, LR 0.000010 Loss 4.638833, Accuracy 88.787%\n",
      "Epoch 32, Batch 72, LR 0.000010 Loss 4.631325, Accuracy 88.845%\n",
      "Epoch 32, Batch 73, LR 0.000010 Loss 4.622641, Accuracy 88.870%\n",
      "Epoch 32, Batch 74, LR 0.000010 Loss 4.619278, Accuracy 88.904%\n",
      "Epoch 32, Batch 75, LR 0.000010 Loss 4.620287, Accuracy 88.917%\n",
      "Epoch 32, Batch 76, LR 0.000010 Loss 4.615846, Accuracy 88.970%\n",
      "Epoch 32, Batch 77, LR 0.000010 Loss 4.604696, Accuracy 89.012%\n",
      "Epoch 32, Batch 78, LR 0.000010 Loss 4.617228, Accuracy 88.922%\n",
      "Epoch 32, Batch 79, LR 0.000010 Loss 4.610608, Accuracy 89.003%\n",
      "Epoch 32, Batch 80, LR 0.000010 Loss 4.597630, Accuracy 89.043%\n",
      "Epoch 32, Batch 81, LR 0.000010 Loss 4.596601, Accuracy 89.034%\n",
      "Epoch 32, Batch 82, LR 0.000010 Loss 4.603599, Accuracy 89.043%\n",
      "Epoch 32, Batch 83, LR 0.000010 Loss 4.609957, Accuracy 88.968%\n",
      "Epoch 32, Batch 84, LR 0.000010 Loss 4.606048, Accuracy 88.997%\n",
      "Epoch 32, Batch 85, LR 0.000010 Loss 4.611607, Accuracy 88.998%\n",
      "Epoch 32, Batch 86, LR 0.000010 Loss 4.620509, Accuracy 88.972%\n",
      "Epoch 32, Batch 87, LR 0.000010 Loss 4.627471, Accuracy 88.928%\n",
      "Epoch 32, Batch 88, LR 0.000010 Loss 4.637090, Accuracy 88.841%\n",
      "Epoch 32, Batch 89, LR 0.000010 Loss 4.635364, Accuracy 88.817%\n",
      "Epoch 32, Batch 90, LR 0.000010 Loss 4.634784, Accuracy 88.819%\n",
      "Epoch 32, Batch 91, LR 0.000010 Loss 4.632193, Accuracy 88.822%\n",
      "Epoch 32, Batch 92, LR 0.000010 Loss 4.632698, Accuracy 88.825%\n",
      "Epoch 32, Batch 93, LR 0.000010 Loss 4.631746, Accuracy 88.827%\n",
      "Epoch 32, Batch 94, LR 0.000010 Loss 4.629702, Accuracy 88.830%\n",
      "Epoch 32, Batch 95, LR 0.000010 Loss 4.625788, Accuracy 88.865%\n",
      "Epoch 32, Batch 96, LR 0.000010 Loss 4.630684, Accuracy 88.883%\n",
      "Epoch 32, Batch 97, LR 0.000010 Loss 4.628201, Accuracy 88.885%\n",
      "Epoch 32, Batch 98, LR 0.000010 Loss 4.629293, Accuracy 88.895%\n",
      "Epoch 32, Batch 99, LR 0.000010 Loss 4.625347, Accuracy 88.920%\n",
      "Epoch 32, Batch 100, LR 0.000010 Loss 4.623892, Accuracy 88.914%\n",
      "Epoch 32, Batch 101, LR 0.000010 Loss 4.618303, Accuracy 88.962%\n",
      "Epoch 32, Batch 102, LR 0.000010 Loss 4.622090, Accuracy 88.940%\n",
      "Epoch 32, Batch 103, LR 0.000010 Loss 4.622055, Accuracy 88.934%\n",
      "Epoch 32, Batch 104, LR 0.000010 Loss 4.623293, Accuracy 88.957%\n",
      "Epoch 32, Batch 105, LR 0.000010 Loss 4.622865, Accuracy 88.951%\n",
      "Epoch 32, Batch 106, LR 0.000010 Loss 4.623657, Accuracy 88.952%\n",
      "Epoch 32, Batch 107, LR 0.000010 Loss 4.627070, Accuracy 88.938%\n",
      "Epoch 32, Batch 108, LR 0.000010 Loss 4.628560, Accuracy 88.947%\n",
      "Epoch 32, Batch 109, LR 0.000010 Loss 4.626605, Accuracy 88.969%\n",
      "Epoch 32, Batch 110, LR 0.000010 Loss 4.629914, Accuracy 88.956%\n",
      "Epoch 32, Batch 111, LR 0.000010 Loss 4.632698, Accuracy 88.943%\n",
      "Epoch 32, Batch 112, LR 0.000010 Loss 4.638950, Accuracy 88.909%\n",
      "Epoch 32, Batch 113, LR 0.000010 Loss 4.640536, Accuracy 88.897%\n",
      "Epoch 32, Batch 114, LR 0.000010 Loss 4.651550, Accuracy 88.836%\n",
      "Epoch 32, Batch 115, LR 0.000010 Loss 4.647281, Accuracy 88.852%\n",
      "Epoch 32, Batch 116, LR 0.000010 Loss 4.646786, Accuracy 88.860%\n",
      "Epoch 32, Batch 117, LR 0.000010 Loss 4.646526, Accuracy 88.889%\n",
      "Epoch 32, Batch 118, LR 0.000010 Loss 4.659626, Accuracy 88.851%\n",
      "Epoch 32, Batch 119, LR 0.000010 Loss 4.660787, Accuracy 88.852%\n",
      "Epoch 32, Batch 120, LR 0.000010 Loss 4.662887, Accuracy 88.874%\n",
      "Epoch 32, Batch 121, LR 0.000010 Loss 4.663316, Accuracy 88.888%\n",
      "Epoch 32, Batch 122, LR 0.000010 Loss 4.655243, Accuracy 88.909%\n",
      "Epoch 32, Batch 123, LR 0.000010 Loss 4.654133, Accuracy 88.910%\n",
      "Epoch 32, Batch 124, LR 0.000010 Loss 4.658527, Accuracy 88.886%\n",
      "Epoch 32, Batch 125, LR 0.000010 Loss 4.653427, Accuracy 88.906%\n",
      "Epoch 32, Batch 126, LR 0.000010 Loss 4.656662, Accuracy 88.876%\n",
      "Epoch 32, Batch 127, LR 0.000010 Loss 4.655770, Accuracy 88.909%\n",
      "Epoch 32, Batch 128, LR 0.000010 Loss 4.655143, Accuracy 88.940%\n",
      "Epoch 32, Batch 129, LR 0.000010 Loss 4.651121, Accuracy 88.966%\n",
      "Epoch 32, Batch 130, LR 0.000010 Loss 4.647912, Accuracy 88.990%\n",
      "Epoch 32, Batch 131, LR 0.000010 Loss 4.644079, Accuracy 89.021%\n",
      "Epoch 32, Batch 132, LR 0.000010 Loss 4.647146, Accuracy 88.997%\n",
      "Epoch 32, Batch 133, LR 0.000010 Loss 4.649013, Accuracy 88.986%\n",
      "Epoch 32, Batch 134, LR 0.000010 Loss 4.648888, Accuracy 88.981%\n",
      "Epoch 32, Batch 135, LR 0.000010 Loss 4.641912, Accuracy 89.022%\n",
      "Epoch 32, Batch 136, LR 0.000010 Loss 4.638418, Accuracy 89.040%\n",
      "Epoch 32, Batch 137, LR 0.000010 Loss 4.635786, Accuracy 89.074%\n",
      "Epoch 32, Batch 138, LR 0.000010 Loss 4.637493, Accuracy 89.074%\n",
      "Epoch 32, Batch 139, LR 0.000010 Loss 4.634465, Accuracy 89.079%\n",
      "Epoch 32, Batch 140, LR 0.000010 Loss 4.631503, Accuracy 89.079%\n",
      "Epoch 32, Batch 141, LR 0.000010 Loss 4.631469, Accuracy 89.068%\n",
      "Epoch 32, Batch 142, LR 0.000010 Loss 4.631392, Accuracy 89.068%\n",
      "Epoch 32, Batch 143, LR 0.000010 Loss 4.627836, Accuracy 89.106%\n",
      "Epoch 32, Batch 144, LR 0.000010 Loss 4.629515, Accuracy 89.106%\n",
      "Epoch 32, Batch 145, LR 0.000010 Loss 4.629194, Accuracy 89.116%\n",
      "Epoch 32, Batch 146, LR 0.000010 Loss 4.628340, Accuracy 89.132%\n",
      "Epoch 32, Batch 147, LR 0.000010 Loss 4.628424, Accuracy 89.132%\n",
      "Epoch 32, Batch 148, LR 0.000010 Loss 4.630182, Accuracy 89.142%\n",
      "Epoch 32, Batch 149, LR 0.000010 Loss 4.627789, Accuracy 89.152%\n",
      "Epoch 32, Batch 150, LR 0.000010 Loss 4.620469, Accuracy 89.182%\n",
      "Epoch 32, Batch 151, LR 0.000010 Loss 4.621336, Accuracy 89.166%\n",
      "Epoch 32, Batch 152, LR 0.000010 Loss 4.616548, Accuracy 89.165%\n",
      "Epoch 32, Batch 153, LR 0.000010 Loss 4.616129, Accuracy 89.190%\n",
      "Epoch 32, Batch 154, LR 0.000010 Loss 4.617911, Accuracy 89.189%\n",
      "Epoch 32, Batch 155, LR 0.000010 Loss 4.617544, Accuracy 89.189%\n",
      "Epoch 32, Batch 156, LR 0.000010 Loss 4.616793, Accuracy 89.203%\n",
      "Epoch 32, Batch 157, LR 0.000010 Loss 4.614671, Accuracy 89.217%\n",
      "Epoch 32, Batch 158, LR 0.000010 Loss 4.614611, Accuracy 89.201%\n",
      "Epoch 32, Batch 159, LR 0.000010 Loss 4.618409, Accuracy 89.180%\n",
      "Epoch 32, Batch 160, LR 0.000010 Loss 4.619675, Accuracy 89.185%\n",
      "Epoch 32, Batch 161, LR 0.000010 Loss 4.619631, Accuracy 89.155%\n",
      "Epoch 32, Batch 162, LR 0.000010 Loss 4.621965, Accuracy 89.159%\n",
      "Epoch 32, Batch 163, LR 0.000010 Loss 4.621235, Accuracy 89.158%\n",
      "Epoch 32, Batch 164, LR 0.000010 Loss 4.619301, Accuracy 89.167%\n",
      "Epoch 32, Batch 165, LR 0.000010 Loss 4.622213, Accuracy 89.176%\n",
      "Epoch 32, Batch 166, LR 0.000010 Loss 4.617459, Accuracy 89.208%\n",
      "Epoch 32, Batch 167, LR 0.000010 Loss 4.621137, Accuracy 89.184%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 168, LR 0.000010 Loss 4.621465, Accuracy 89.202%\n",
      "Epoch 32, Batch 169, LR 0.000010 Loss 4.620669, Accuracy 89.192%\n",
      "Epoch 32, Batch 170, LR 0.000010 Loss 4.620219, Accuracy 89.187%\n",
      "Epoch 32, Batch 171, LR 0.000010 Loss 4.616246, Accuracy 89.200%\n",
      "Epoch 32, Batch 172, LR 0.000010 Loss 4.614105, Accuracy 89.181%\n",
      "Epoch 32, Batch 173, LR 0.000010 Loss 4.609075, Accuracy 89.184%\n",
      "Epoch 32, Batch 174, LR 0.000010 Loss 4.612116, Accuracy 89.170%\n",
      "Epoch 32, Batch 175, LR 0.000010 Loss 4.613643, Accuracy 89.161%\n",
      "Epoch 32, Batch 176, LR 0.000010 Loss 4.613400, Accuracy 89.138%\n",
      "Epoch 32, Batch 177, LR 0.000010 Loss 4.612804, Accuracy 89.142%\n",
      "Epoch 32, Batch 178, LR 0.000010 Loss 4.613298, Accuracy 89.150%\n",
      "Epoch 32, Batch 179, LR 0.000010 Loss 4.610193, Accuracy 89.163%\n",
      "Epoch 32, Batch 180, LR 0.000010 Loss 4.615623, Accuracy 89.167%\n",
      "Epoch 32, Batch 181, LR 0.000010 Loss 4.618109, Accuracy 89.175%\n",
      "Epoch 32, Batch 182, LR 0.000010 Loss 4.616281, Accuracy 89.196%\n",
      "Epoch 32, Batch 183, LR 0.000010 Loss 4.619039, Accuracy 89.186%\n",
      "Epoch 32, Batch 184, LR 0.000010 Loss 4.619617, Accuracy 89.194%\n",
      "Epoch 32, Batch 185, LR 0.000010 Loss 4.624038, Accuracy 89.181%\n",
      "Epoch 32, Batch 186, LR 0.000010 Loss 4.623354, Accuracy 89.176%\n",
      "Epoch 32, Batch 187, LR 0.000010 Loss 4.621691, Accuracy 89.175%\n",
      "Epoch 32, Batch 188, LR 0.000010 Loss 4.619074, Accuracy 89.175%\n",
      "Epoch 32, Batch 189, LR 0.000010 Loss 4.616913, Accuracy 89.195%\n",
      "Epoch 32, Batch 190, LR 0.000010 Loss 4.621924, Accuracy 89.169%\n",
      "Epoch 32, Batch 191, LR 0.000010 Loss 4.621978, Accuracy 89.161%\n",
      "Epoch 32, Batch 192, LR 0.000010 Loss 4.625121, Accuracy 89.136%\n",
      "Epoch 32, Batch 193, LR 0.000010 Loss 4.624953, Accuracy 89.164%\n",
      "Epoch 32, Batch 194, LR 0.000010 Loss 4.621813, Accuracy 89.171%\n",
      "Epoch 32, Batch 195, LR 0.000010 Loss 4.621703, Accuracy 89.163%\n",
      "Epoch 32, Batch 196, LR 0.000010 Loss 4.622708, Accuracy 89.162%\n",
      "Epoch 32, Batch 197, LR 0.000010 Loss 4.621577, Accuracy 89.166%\n",
      "Epoch 32, Batch 198, LR 0.000010 Loss 4.622730, Accuracy 89.161%\n",
      "Epoch 32, Batch 199, LR 0.000010 Loss 4.621116, Accuracy 89.168%\n",
      "Epoch 32, Batch 200, LR 0.000010 Loss 4.622236, Accuracy 89.164%\n",
      "Epoch 32, Batch 201, LR 0.000010 Loss 4.620436, Accuracy 89.175%\n",
      "Epoch 32, Batch 202, LR 0.000010 Loss 4.622010, Accuracy 89.175%\n",
      "Epoch 32, Batch 203, LR 0.000010 Loss 4.625801, Accuracy 89.163%\n",
      "Epoch 32, Batch 204, LR 0.000010 Loss 4.629618, Accuracy 89.154%\n",
      "Epoch 32, Batch 205, LR 0.000010 Loss 4.631485, Accuracy 89.135%\n",
      "Epoch 32, Batch 206, LR 0.000010 Loss 4.634030, Accuracy 89.123%\n",
      "Epoch 32, Batch 207, LR 0.000010 Loss 4.633523, Accuracy 89.119%\n",
      "Epoch 32, Batch 208, LR 0.000010 Loss 4.634701, Accuracy 89.108%\n",
      "Epoch 32, Batch 209, LR 0.000010 Loss 4.634923, Accuracy 89.096%\n",
      "Epoch 32, Batch 210, LR 0.000010 Loss 4.636746, Accuracy 89.085%\n",
      "Epoch 32, Batch 211, LR 0.000010 Loss 4.641822, Accuracy 89.070%\n",
      "Epoch 32, Batch 212, LR 0.000010 Loss 4.642215, Accuracy 89.066%\n",
      "Epoch 32, Batch 213, LR 0.000010 Loss 4.640640, Accuracy 89.074%\n",
      "Epoch 32, Batch 214, LR 0.000010 Loss 4.638895, Accuracy 89.092%\n",
      "Epoch 32, Batch 215, LR 0.000010 Loss 4.639838, Accuracy 89.088%\n",
      "Epoch 32, Batch 216, LR 0.000010 Loss 4.638929, Accuracy 89.081%\n",
      "Epoch 32, Batch 217, LR 0.000010 Loss 4.636309, Accuracy 89.091%\n",
      "Epoch 32, Batch 218, LR 0.000010 Loss 4.636519, Accuracy 89.091%\n",
      "Epoch 32, Batch 219, LR 0.000010 Loss 4.635577, Accuracy 89.102%\n",
      "Epoch 32, Batch 220, LR 0.000010 Loss 4.630847, Accuracy 89.119%\n",
      "Epoch 32, Batch 221, LR 0.000010 Loss 4.633882, Accuracy 89.108%\n",
      "Epoch 32, Batch 222, LR 0.000010 Loss 4.634870, Accuracy 89.112%\n",
      "Epoch 32, Batch 223, LR 0.000010 Loss 4.633308, Accuracy 89.122%\n",
      "Epoch 32, Batch 224, LR 0.000010 Loss 4.632553, Accuracy 89.132%\n",
      "Epoch 32, Batch 225, LR 0.000010 Loss 4.632941, Accuracy 89.142%\n",
      "Epoch 32, Batch 226, LR 0.000010 Loss 4.629602, Accuracy 89.142%\n",
      "Epoch 32, Batch 227, LR 0.000010 Loss 4.632421, Accuracy 89.135%\n",
      "Epoch 32, Batch 228, LR 0.000010 Loss 4.631752, Accuracy 89.152%\n",
      "Epoch 32, Batch 229, LR 0.000010 Loss 4.634823, Accuracy 89.144%\n",
      "Epoch 32, Batch 230, LR 0.000010 Loss 4.633484, Accuracy 89.151%\n",
      "Epoch 32, Batch 231, LR 0.000010 Loss 4.634660, Accuracy 89.140%\n",
      "Epoch 32, Batch 232, LR 0.000010 Loss 4.635127, Accuracy 89.130%\n",
      "Epoch 32, Batch 233, LR 0.000010 Loss 4.630696, Accuracy 89.150%\n",
      "Epoch 32, Batch 234, LR 0.000010 Loss 4.631191, Accuracy 89.153%\n",
      "Epoch 32, Batch 235, LR 0.000010 Loss 4.629190, Accuracy 89.162%\n",
      "Epoch 32, Batch 236, LR 0.000010 Loss 4.629155, Accuracy 89.168%\n",
      "Epoch 32, Batch 237, LR 0.000010 Loss 4.632457, Accuracy 89.138%\n",
      "Epoch 32, Batch 238, LR 0.000010 Loss 4.634384, Accuracy 89.148%\n",
      "Epoch 32, Batch 239, LR 0.000010 Loss 4.634466, Accuracy 89.154%\n",
      "Epoch 32, Batch 240, LR 0.000010 Loss 4.633642, Accuracy 89.157%\n",
      "Epoch 32, Batch 241, LR 0.000010 Loss 4.633972, Accuracy 89.163%\n",
      "Epoch 32, Batch 242, LR 0.000010 Loss 4.632973, Accuracy 89.172%\n",
      "Epoch 32, Batch 243, LR 0.000010 Loss 4.631269, Accuracy 89.169%\n",
      "Epoch 32, Batch 244, LR 0.000010 Loss 4.631133, Accuracy 89.181%\n",
      "Epoch 32, Batch 245, LR 0.000010 Loss 4.629506, Accuracy 89.184%\n",
      "Epoch 32, Batch 246, LR 0.000010 Loss 4.633021, Accuracy 89.167%\n",
      "Epoch 32, Batch 247, LR 0.000010 Loss 4.630063, Accuracy 89.183%\n",
      "Epoch 32, Batch 248, LR 0.000010 Loss 4.628796, Accuracy 89.189%\n",
      "Epoch 32, Batch 249, LR 0.000010 Loss 4.629534, Accuracy 89.182%\n",
      "Epoch 32, Batch 250, LR 0.000010 Loss 4.632150, Accuracy 89.194%\n",
      "Epoch 32, Batch 251, LR 0.000010 Loss 4.633785, Accuracy 89.187%\n",
      "Epoch 32, Batch 252, LR 0.000010 Loss 4.633135, Accuracy 89.187%\n",
      "Epoch 32, Batch 253, LR 0.000010 Loss 4.632526, Accuracy 89.183%\n",
      "Epoch 32, Batch 254, LR 0.000010 Loss 4.633843, Accuracy 89.170%\n",
      "Epoch 32, Batch 255, LR 0.000010 Loss 4.637431, Accuracy 89.151%\n",
      "Epoch 32, Batch 256, LR 0.000010 Loss 4.636295, Accuracy 89.160%\n",
      "Epoch 32, Batch 257, LR 0.000010 Loss 4.635974, Accuracy 89.154%\n",
      "Epoch 32, Batch 258, LR 0.000010 Loss 4.637330, Accuracy 89.150%\n",
      "Epoch 32, Batch 259, LR 0.000010 Loss 4.635450, Accuracy 89.159%\n",
      "Epoch 32, Batch 260, LR 0.000010 Loss 4.635263, Accuracy 89.156%\n",
      "Epoch 32, Batch 261, LR 0.000010 Loss 4.635092, Accuracy 89.164%\n",
      "Epoch 32, Batch 262, LR 0.000010 Loss 4.633250, Accuracy 89.182%\n",
      "Epoch 32, Batch 263, LR 0.000010 Loss 4.632592, Accuracy 89.184%\n",
      "Epoch 32, Batch 264, LR 0.000010 Loss 4.631022, Accuracy 89.199%\n",
      "Epoch 32, Batch 265, LR 0.000010 Loss 4.635490, Accuracy 89.177%\n",
      "Epoch 32, Batch 266, LR 0.000010 Loss 4.633043, Accuracy 89.198%\n",
      "Epoch 32, Batch 267, LR 0.000010 Loss 4.632682, Accuracy 89.206%\n",
      "Epoch 32, Batch 268, LR 0.000010 Loss 4.629343, Accuracy 89.214%\n",
      "Epoch 32, Batch 269, LR 0.000010 Loss 4.628784, Accuracy 89.214%\n",
      "Epoch 32, Batch 270, LR 0.000010 Loss 4.626603, Accuracy 89.219%\n",
      "Epoch 32, Batch 271, LR 0.000010 Loss 4.627086, Accuracy 89.215%\n",
      "Epoch 32, Batch 272, LR 0.000010 Loss 4.629384, Accuracy 89.195%\n",
      "Epoch 32, Batch 273, LR 0.000010 Loss 4.627896, Accuracy 89.211%\n",
      "Epoch 32, Batch 274, LR 0.000010 Loss 4.627450, Accuracy 89.222%\n",
      "Epoch 32, Batch 275, LR 0.000010 Loss 4.625396, Accuracy 89.233%\n",
      "Epoch 32, Batch 276, LR 0.000010 Loss 4.625977, Accuracy 89.235%\n",
      "Epoch 32, Batch 277, LR 0.000010 Loss 4.627850, Accuracy 89.218%\n",
      "Epoch 32, Batch 278, LR 0.000010 Loss 4.627412, Accuracy 89.214%\n",
      "Epoch 32, Batch 279, LR 0.000010 Loss 4.628628, Accuracy 89.203%\n",
      "Epoch 32, Batch 280, LR 0.000010 Loss 4.630612, Accuracy 89.199%\n",
      "Epoch 32, Batch 281, LR 0.000010 Loss 4.630311, Accuracy 89.218%\n",
      "Epoch 32, Batch 282, LR 0.000010 Loss 4.631500, Accuracy 89.220%\n",
      "Epoch 32, Batch 283, LR 0.000010 Loss 4.633252, Accuracy 89.203%\n",
      "Epoch 32, Batch 284, LR 0.000010 Loss 4.632217, Accuracy 89.206%\n",
      "Epoch 32, Batch 285, LR 0.000010 Loss 4.633149, Accuracy 89.191%\n",
      "Epoch 32, Batch 286, LR 0.000010 Loss 4.634327, Accuracy 89.177%\n",
      "Epoch 32, Batch 287, LR 0.000010 Loss 4.635868, Accuracy 89.169%\n",
      "Epoch 32, Batch 288, LR 0.000010 Loss 4.637778, Accuracy 89.160%\n",
      "Epoch 32, Batch 289, LR 0.000010 Loss 4.637611, Accuracy 89.165%\n",
      "Epoch 32, Batch 290, LR 0.000010 Loss 4.636987, Accuracy 89.170%\n",
      "Epoch 32, Batch 291, LR 0.000010 Loss 4.636836, Accuracy 89.173%\n",
      "Epoch 32, Batch 292, LR 0.000010 Loss 4.637332, Accuracy 89.170%\n",
      "Epoch 32, Batch 293, LR 0.000010 Loss 4.633163, Accuracy 89.182%\n",
      "Epoch 32, Batch 294, LR 0.000010 Loss 4.634189, Accuracy 89.174%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 295, LR 0.000010 Loss 4.633055, Accuracy 89.179%\n",
      "Epoch 32, Batch 296, LR 0.000010 Loss 4.634671, Accuracy 89.171%\n",
      "Epoch 32, Batch 297, LR 0.000010 Loss 4.634461, Accuracy 89.184%\n",
      "Epoch 32, Batch 298, LR 0.000010 Loss 4.633343, Accuracy 89.183%\n",
      "Epoch 32, Batch 299, LR 0.000010 Loss 4.632220, Accuracy 89.185%\n",
      "Epoch 32, Batch 300, LR 0.000010 Loss 4.632480, Accuracy 89.177%\n",
      "Epoch 32, Batch 301, LR 0.000010 Loss 4.634018, Accuracy 89.164%\n",
      "Epoch 32, Batch 302, LR 0.000010 Loss 4.636290, Accuracy 89.143%\n",
      "Epoch 32, Batch 303, LR 0.000010 Loss 4.636454, Accuracy 89.153%\n",
      "Epoch 32, Batch 304, LR 0.000010 Loss 4.635726, Accuracy 89.158%\n",
      "Epoch 32, Batch 305, LR 0.000010 Loss 4.634957, Accuracy 89.152%\n",
      "Epoch 32, Batch 306, LR 0.000010 Loss 4.636269, Accuracy 89.137%\n",
      "Epoch 32, Batch 307, LR 0.000010 Loss 4.639103, Accuracy 89.121%\n",
      "Epoch 32, Batch 308, LR 0.000010 Loss 4.638969, Accuracy 89.118%\n",
      "Epoch 32, Batch 309, LR 0.000010 Loss 4.637993, Accuracy 89.116%\n",
      "Epoch 32, Batch 310, LR 0.000010 Loss 4.636762, Accuracy 89.128%\n",
      "Epoch 32, Batch 311, LR 0.000010 Loss 4.636131, Accuracy 89.125%\n",
      "Epoch 32, Batch 312, LR 0.000010 Loss 4.635846, Accuracy 89.130%\n",
      "Epoch 32, Batch 313, LR 0.000010 Loss 4.636455, Accuracy 89.125%\n",
      "Epoch 32, Batch 314, LR 0.000010 Loss 4.636659, Accuracy 89.125%\n",
      "Epoch 32, Batch 315, LR 0.000010 Loss 4.636744, Accuracy 89.127%\n",
      "Epoch 32, Batch 316, LR 0.000010 Loss 4.634559, Accuracy 89.127%\n",
      "Epoch 32, Batch 317, LR 0.000010 Loss 4.635848, Accuracy 89.122%\n",
      "Epoch 32, Batch 318, LR 0.000010 Loss 4.634112, Accuracy 89.124%\n",
      "Epoch 32, Batch 319, LR 0.000010 Loss 4.634875, Accuracy 89.124%\n",
      "Epoch 32, Batch 320, LR 0.000010 Loss 4.634733, Accuracy 89.128%\n",
      "Epoch 32, Batch 321, LR 0.000010 Loss 4.635241, Accuracy 89.138%\n",
      "Epoch 32, Batch 322, LR 0.000010 Loss 4.634194, Accuracy 89.140%\n",
      "Epoch 32, Batch 323, LR 0.000010 Loss 4.635855, Accuracy 89.137%\n",
      "Epoch 32, Batch 324, LR 0.000010 Loss 4.636107, Accuracy 89.142%\n",
      "Epoch 32, Batch 325, LR 0.000010 Loss 4.634424, Accuracy 89.144%\n",
      "Epoch 32, Batch 326, LR 0.000010 Loss 4.634347, Accuracy 89.137%\n",
      "Epoch 32, Batch 327, LR 0.000010 Loss 4.632824, Accuracy 89.134%\n",
      "Epoch 32, Batch 328, LR 0.000010 Loss 4.634876, Accuracy 89.122%\n",
      "Epoch 32, Batch 329, LR 0.000010 Loss 4.634606, Accuracy 89.122%\n",
      "Epoch 32, Batch 330, LR 0.000010 Loss 4.634099, Accuracy 89.131%\n",
      "Epoch 32, Batch 331, LR 0.000010 Loss 4.634675, Accuracy 89.119%\n",
      "Epoch 32, Batch 332, LR 0.000010 Loss 4.633146, Accuracy 89.126%\n",
      "Epoch 32, Batch 333, LR 0.000010 Loss 4.632823, Accuracy 89.116%\n",
      "Epoch 32, Batch 334, LR 0.000010 Loss 4.631342, Accuracy 89.126%\n",
      "Epoch 32, Batch 335, LR 0.000010 Loss 4.632651, Accuracy 89.128%\n",
      "Epoch 32, Batch 336, LR 0.000010 Loss 4.633140, Accuracy 89.114%\n",
      "Epoch 32, Batch 337, LR 0.000010 Loss 4.631433, Accuracy 89.118%\n",
      "Epoch 32, Batch 338, LR 0.000010 Loss 4.630919, Accuracy 89.120%\n",
      "Epoch 32, Batch 339, LR 0.000010 Loss 4.629133, Accuracy 89.127%\n",
      "Epoch 32, Batch 340, LR 0.000010 Loss 4.628767, Accuracy 89.129%\n",
      "Epoch 32, Batch 341, LR 0.000010 Loss 4.629722, Accuracy 89.117%\n",
      "Epoch 32, Batch 342, LR 0.000010 Loss 4.628175, Accuracy 89.122%\n",
      "Epoch 32, Batch 343, LR 0.000010 Loss 4.626580, Accuracy 89.126%\n",
      "Epoch 32, Batch 344, LR 0.000010 Loss 4.628182, Accuracy 89.112%\n",
      "Epoch 32, Batch 345, LR 0.000010 Loss 4.630202, Accuracy 89.099%\n",
      "Epoch 32, Batch 346, LR 0.000010 Loss 4.630193, Accuracy 89.085%\n",
      "Epoch 32, Batch 347, LR 0.000010 Loss 4.630782, Accuracy 89.087%\n",
      "Epoch 32, Batch 348, LR 0.000010 Loss 4.628487, Accuracy 89.096%\n",
      "Epoch 32, Batch 349, LR 0.000010 Loss 4.628929, Accuracy 89.096%\n",
      "Epoch 32, Batch 350, LR 0.000010 Loss 4.628474, Accuracy 89.094%\n",
      "Epoch 32, Batch 351, LR 0.000010 Loss 4.627786, Accuracy 89.091%\n",
      "Epoch 32, Batch 352, LR 0.000010 Loss 4.629868, Accuracy 89.094%\n",
      "Epoch 32, Batch 353, LR 0.000010 Loss 4.629158, Accuracy 89.100%\n",
      "Epoch 32, Batch 354, LR 0.000010 Loss 4.629194, Accuracy 89.102%\n",
      "Epoch 32, Batch 355, LR 0.000010 Loss 4.630241, Accuracy 89.100%\n",
      "Epoch 32, Batch 356, LR 0.000010 Loss 4.630872, Accuracy 89.098%\n",
      "Epoch 32, Batch 357, LR 0.000010 Loss 4.629633, Accuracy 89.095%\n",
      "Epoch 32, Batch 358, LR 0.000010 Loss 4.627281, Accuracy 89.104%\n",
      "Epoch 32, Batch 359, LR 0.000010 Loss 4.626952, Accuracy 89.102%\n",
      "Epoch 32, Batch 360, LR 0.000010 Loss 4.628195, Accuracy 89.093%\n",
      "Epoch 32, Batch 361, LR 0.000010 Loss 4.630036, Accuracy 89.082%\n",
      "Epoch 32, Batch 362, LR 0.000010 Loss 4.630908, Accuracy 89.078%\n",
      "Epoch 32, Batch 363, LR 0.000010 Loss 4.628432, Accuracy 89.093%\n",
      "Epoch 32, Batch 364, LR 0.000010 Loss 4.627047, Accuracy 89.097%\n",
      "Epoch 32, Batch 365, LR 0.000010 Loss 4.626649, Accuracy 89.095%\n",
      "Epoch 32, Batch 366, LR 0.000010 Loss 4.626945, Accuracy 89.090%\n",
      "Epoch 32, Batch 367, LR 0.000010 Loss 4.624685, Accuracy 89.107%\n",
      "Epoch 32, Batch 368, LR 0.000010 Loss 4.624006, Accuracy 89.111%\n",
      "Epoch 32, Batch 369, LR 0.000010 Loss 4.624867, Accuracy 89.105%\n",
      "Epoch 32, Batch 370, LR 0.000010 Loss 4.623613, Accuracy 89.120%\n",
      "Epoch 32, Batch 371, LR 0.000010 Loss 4.624574, Accuracy 89.119%\n",
      "Epoch 32, Batch 372, LR 0.000010 Loss 4.623650, Accuracy 89.115%\n",
      "Epoch 32, Batch 373, LR 0.000010 Loss 4.622828, Accuracy 89.117%\n",
      "Epoch 32, Batch 374, LR 0.000010 Loss 4.623965, Accuracy 89.111%\n",
      "Epoch 32, Batch 375, LR 0.000010 Loss 4.624006, Accuracy 89.108%\n",
      "Epoch 32, Batch 376, LR 0.000010 Loss 4.622090, Accuracy 89.119%\n",
      "Epoch 32, Batch 377, LR 0.000010 Loss 4.623269, Accuracy 89.108%\n",
      "Epoch 32, Batch 378, LR 0.000010 Loss 4.622949, Accuracy 89.104%\n",
      "Epoch 32, Batch 379, LR 0.000010 Loss 4.623498, Accuracy 89.102%\n",
      "Epoch 32, Batch 380, LR 0.000010 Loss 4.623029, Accuracy 89.108%\n",
      "Epoch 32, Batch 381, LR 0.000010 Loss 4.622016, Accuracy 89.120%\n",
      "Epoch 32, Batch 382, LR 0.000010 Loss 4.623661, Accuracy 89.120%\n",
      "Epoch 32, Batch 383, LR 0.000010 Loss 4.624863, Accuracy 89.111%\n",
      "Epoch 32, Batch 384, LR 0.000010 Loss 4.624682, Accuracy 89.109%\n",
      "Epoch 32, Batch 385, LR 0.000010 Loss 4.625364, Accuracy 89.105%\n",
      "Epoch 32, Batch 386, LR 0.000010 Loss 4.623474, Accuracy 89.115%\n",
      "Epoch 32, Batch 387, LR 0.000010 Loss 4.624052, Accuracy 89.105%\n",
      "Epoch 32, Batch 388, LR 0.000010 Loss 4.626726, Accuracy 89.097%\n",
      "Epoch 32, Batch 389, LR 0.000010 Loss 4.624493, Accuracy 89.107%\n",
      "Epoch 32, Batch 390, LR 0.000010 Loss 4.624554, Accuracy 89.107%\n",
      "Epoch 32, Batch 391, LR 0.000010 Loss 4.624102, Accuracy 89.108%\n",
      "Epoch 32, Batch 392, LR 0.000010 Loss 4.623320, Accuracy 89.112%\n",
      "Epoch 32, Batch 393, LR 0.000010 Loss 4.624793, Accuracy 89.102%\n",
      "Epoch 32, Batch 394, LR 0.000010 Loss 4.624845, Accuracy 89.104%\n",
      "Epoch 32, Batch 395, LR 0.000010 Loss 4.624744, Accuracy 89.102%\n",
      "Epoch 32, Batch 396, LR 0.000010 Loss 4.624890, Accuracy 89.102%\n",
      "Epoch 32, Batch 397, LR 0.000010 Loss 4.622448, Accuracy 89.112%\n",
      "Epoch 32, Batch 398, LR 0.000010 Loss 4.622881, Accuracy 89.112%\n",
      "Epoch 32, Batch 399, LR 0.000010 Loss 4.622410, Accuracy 89.119%\n",
      "Epoch 32, Batch 400, LR 0.000010 Loss 4.623584, Accuracy 89.115%\n",
      "Epoch 32, Batch 401, LR 0.000010 Loss 4.624224, Accuracy 89.113%\n",
      "Epoch 32, Batch 402, LR 0.000010 Loss 4.624683, Accuracy 89.107%\n",
      "Epoch 32, Batch 403, LR 0.000010 Loss 4.624021, Accuracy 89.105%\n",
      "Epoch 32, Batch 404, LR 0.000010 Loss 4.622373, Accuracy 89.117%\n",
      "Epoch 32, Batch 405, LR 0.000010 Loss 4.623393, Accuracy 89.111%\n",
      "Epoch 32, Batch 406, LR 0.000010 Loss 4.622631, Accuracy 89.109%\n",
      "Epoch 32, Batch 407, LR 0.000010 Loss 4.622223, Accuracy 89.107%\n",
      "Epoch 32, Batch 408, LR 0.000010 Loss 4.624274, Accuracy 89.095%\n",
      "Epoch 32, Batch 409, LR 0.000010 Loss 4.625298, Accuracy 89.087%\n",
      "Epoch 32, Batch 410, LR 0.000010 Loss 4.624889, Accuracy 89.091%\n",
      "Epoch 32, Batch 411, LR 0.000010 Loss 4.624239, Accuracy 89.097%\n",
      "Epoch 32, Batch 412, LR 0.000010 Loss 4.625713, Accuracy 89.093%\n",
      "Epoch 32, Batch 413, LR 0.000010 Loss 4.624248, Accuracy 89.093%\n",
      "Epoch 32, Batch 414, LR 0.000010 Loss 4.625059, Accuracy 89.087%\n",
      "Epoch 32, Batch 415, LR 0.000010 Loss 4.624223, Accuracy 89.093%\n",
      "Epoch 32, Batch 416, LR 0.000010 Loss 4.622993, Accuracy 89.098%\n",
      "Epoch 32, Batch 417, LR 0.000010 Loss 4.621932, Accuracy 89.102%\n",
      "Epoch 32, Batch 418, LR 0.000010 Loss 4.621425, Accuracy 89.100%\n",
      "Epoch 32, Batch 419, LR 0.000010 Loss 4.621972, Accuracy 89.105%\n",
      "Epoch 32, Batch 420, LR 0.000010 Loss 4.621247, Accuracy 89.105%\n",
      "Epoch 32, Batch 421, LR 0.000010 Loss 4.620160, Accuracy 89.105%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 422, LR 0.000010 Loss 4.619918, Accuracy 89.103%\n",
      "Epoch 32, Batch 423, LR 0.000010 Loss 4.617929, Accuracy 89.111%\n",
      "Epoch 32, Batch 424, LR 0.000010 Loss 4.618843, Accuracy 89.101%\n",
      "Epoch 32, Batch 425, LR 0.000010 Loss 4.618533, Accuracy 89.105%\n",
      "Epoch 32, Batch 426, LR 0.000010 Loss 4.617356, Accuracy 89.114%\n",
      "Epoch 32, Batch 427, LR 0.000010 Loss 4.619236, Accuracy 89.099%\n",
      "Epoch 32, Batch 428, LR 0.000010 Loss 4.619368, Accuracy 89.101%\n",
      "Epoch 32, Batch 429, LR 0.000010 Loss 4.618600, Accuracy 89.097%\n",
      "Epoch 32, Batch 430, LR 0.000010 Loss 4.617902, Accuracy 89.097%\n",
      "Epoch 32, Batch 431, LR 0.000010 Loss 4.617540, Accuracy 89.097%\n",
      "Epoch 32, Batch 432, LR 0.000010 Loss 4.615837, Accuracy 89.108%\n",
      "Epoch 32, Batch 433, LR 0.000010 Loss 4.616053, Accuracy 89.109%\n",
      "Epoch 32, Batch 434, LR 0.000010 Loss 4.615850, Accuracy 89.109%\n",
      "Epoch 32, Batch 435, LR 0.000010 Loss 4.615529, Accuracy 89.118%\n",
      "Epoch 32, Batch 436, LR 0.000010 Loss 4.615867, Accuracy 89.123%\n",
      "Epoch 32, Batch 437, LR 0.000010 Loss 4.618097, Accuracy 89.114%\n",
      "Epoch 32, Batch 438, LR 0.000010 Loss 4.618054, Accuracy 89.116%\n",
      "Epoch 32, Batch 439, LR 0.000010 Loss 4.618261, Accuracy 89.121%\n",
      "Epoch 32, Batch 440, LR 0.000010 Loss 4.619306, Accuracy 89.116%\n",
      "Epoch 32, Batch 441, LR 0.000009 Loss 4.618064, Accuracy 89.117%\n",
      "Epoch 32, Batch 442, LR 0.000009 Loss 4.617304, Accuracy 89.123%\n",
      "Epoch 32, Batch 443, LR 0.000009 Loss 4.616135, Accuracy 89.131%\n",
      "Epoch 32, Batch 444, LR 0.000009 Loss 4.616132, Accuracy 89.133%\n",
      "Epoch 32, Batch 445, LR 0.000009 Loss 4.614237, Accuracy 89.142%\n",
      "Epoch 32, Batch 446, LR 0.000009 Loss 4.614772, Accuracy 89.136%\n",
      "Epoch 32, Batch 447, LR 0.000009 Loss 4.614876, Accuracy 89.139%\n",
      "Epoch 32, Batch 448, LR 0.000009 Loss 4.615009, Accuracy 89.137%\n",
      "Epoch 32, Batch 449, LR 0.000009 Loss 4.615324, Accuracy 89.141%\n",
      "Epoch 32, Batch 450, LR 0.000009 Loss 4.615279, Accuracy 89.139%\n",
      "Epoch 32, Batch 451, LR 0.000009 Loss 4.614924, Accuracy 89.135%\n",
      "Epoch 32, Batch 452, LR 0.000009 Loss 4.615110, Accuracy 89.133%\n",
      "Epoch 32, Batch 453, LR 0.000009 Loss 4.615760, Accuracy 89.135%\n",
      "Epoch 32, Batch 454, LR 0.000009 Loss 4.613251, Accuracy 89.145%\n",
      "Epoch 32, Batch 455, LR 0.000009 Loss 4.612415, Accuracy 89.154%\n",
      "Epoch 32, Batch 456, LR 0.000009 Loss 4.613067, Accuracy 89.150%\n",
      "Epoch 32, Batch 457, LR 0.000009 Loss 4.612537, Accuracy 89.153%\n",
      "Epoch 32, Batch 458, LR 0.000009 Loss 4.614509, Accuracy 89.146%\n",
      "Epoch 32, Batch 459, LR 0.000009 Loss 4.615857, Accuracy 89.136%\n",
      "Epoch 32, Batch 460, LR 0.000009 Loss 4.615088, Accuracy 89.142%\n",
      "Epoch 32, Batch 461, LR 0.000009 Loss 4.615136, Accuracy 89.140%\n",
      "Epoch 32, Batch 462, LR 0.000009 Loss 4.615792, Accuracy 89.140%\n",
      "Epoch 32, Batch 463, LR 0.000009 Loss 4.615789, Accuracy 89.149%\n",
      "Epoch 32, Batch 464, LR 0.000009 Loss 4.616250, Accuracy 89.152%\n",
      "Epoch 32, Batch 465, LR 0.000009 Loss 4.617062, Accuracy 89.155%\n",
      "Epoch 32, Batch 466, LR 0.000009 Loss 4.617772, Accuracy 89.153%\n",
      "Epoch 32, Batch 467, LR 0.000009 Loss 4.618493, Accuracy 89.151%\n",
      "Epoch 32, Batch 468, LR 0.000009 Loss 4.619170, Accuracy 89.148%\n",
      "Epoch 32, Batch 469, LR 0.000009 Loss 4.618182, Accuracy 89.149%\n",
      "Epoch 32, Batch 470, LR 0.000009 Loss 4.618004, Accuracy 89.149%\n",
      "Epoch 32, Batch 471, LR 0.000009 Loss 4.617771, Accuracy 89.145%\n",
      "Epoch 32, Batch 472, LR 0.000009 Loss 4.619445, Accuracy 89.142%\n",
      "Epoch 32, Batch 473, LR 0.000009 Loss 4.620513, Accuracy 89.138%\n",
      "Epoch 32, Batch 474, LR 0.000009 Loss 4.619873, Accuracy 89.143%\n",
      "Epoch 32, Batch 475, LR 0.000009 Loss 4.617314, Accuracy 89.148%\n",
      "Epoch 32, Batch 476, LR 0.000009 Loss 4.616151, Accuracy 89.151%\n",
      "Epoch 32, Batch 477, LR 0.000009 Loss 4.616872, Accuracy 89.151%\n",
      "Epoch 32, Batch 478, LR 0.000009 Loss 4.618262, Accuracy 89.144%\n",
      "Epoch 32, Batch 479, LR 0.000009 Loss 4.618170, Accuracy 89.146%\n",
      "Epoch 32, Batch 480, LR 0.000009 Loss 4.617861, Accuracy 89.150%\n",
      "Epoch 32, Batch 481, LR 0.000009 Loss 4.617294, Accuracy 89.157%\n",
      "Epoch 32, Batch 482, LR 0.000009 Loss 4.616888, Accuracy 89.158%\n",
      "Epoch 32, Batch 483, LR 0.000009 Loss 4.616560, Accuracy 89.164%\n",
      "Epoch 32, Batch 484, LR 0.000009 Loss 4.615810, Accuracy 89.164%\n",
      "Epoch 32, Batch 485, LR 0.000009 Loss 4.616206, Accuracy 89.167%\n",
      "Epoch 32, Batch 486, LR 0.000009 Loss 4.614313, Accuracy 89.178%\n",
      "Epoch 32, Batch 487, LR 0.000009 Loss 4.613107, Accuracy 89.186%\n",
      "Epoch 32, Batch 488, LR 0.000009 Loss 4.613343, Accuracy 89.184%\n",
      "Epoch 32, Batch 489, LR 0.000009 Loss 4.614333, Accuracy 89.176%\n",
      "Epoch 32, Batch 490, LR 0.000009 Loss 4.613751, Accuracy 89.179%\n",
      "Epoch 32, Batch 491, LR 0.000009 Loss 4.613601, Accuracy 89.182%\n",
      "Epoch 32, Batch 492, LR 0.000009 Loss 4.613888, Accuracy 89.177%\n",
      "Epoch 32, Batch 493, LR 0.000009 Loss 4.614225, Accuracy 89.173%\n",
      "Epoch 32, Batch 494, LR 0.000009 Loss 4.614077, Accuracy 89.175%\n",
      "Epoch 32, Batch 495, LR 0.000009 Loss 4.612710, Accuracy 89.178%\n",
      "Epoch 32, Batch 496, LR 0.000009 Loss 4.613277, Accuracy 89.176%\n",
      "Epoch 32, Batch 497, LR 0.000009 Loss 4.612219, Accuracy 89.182%\n",
      "Epoch 32, Batch 498, LR 0.000009 Loss 4.613292, Accuracy 89.172%\n",
      "Epoch 32, Batch 499, LR 0.000009 Loss 4.613231, Accuracy 89.177%\n",
      "Epoch 32, Batch 500, LR 0.000009 Loss 4.613239, Accuracy 89.172%\n",
      "Epoch 32, Batch 501, LR 0.000009 Loss 4.612950, Accuracy 89.175%\n",
      "Epoch 32, Batch 502, LR 0.000009 Loss 4.612376, Accuracy 89.176%\n",
      "Epoch 32, Batch 503, LR 0.000009 Loss 4.611645, Accuracy 89.174%\n",
      "Epoch 32, Batch 504, LR 0.000009 Loss 4.610029, Accuracy 89.187%\n",
      "Epoch 32, Batch 505, LR 0.000009 Loss 4.608961, Accuracy 89.196%\n",
      "Epoch 32, Batch 506, LR 0.000009 Loss 4.608466, Accuracy 89.197%\n",
      "Epoch 32, Batch 507, LR 0.000009 Loss 4.608494, Accuracy 89.198%\n",
      "Epoch 32, Batch 508, LR 0.000009 Loss 4.609102, Accuracy 89.187%\n",
      "Epoch 32, Batch 509, LR 0.000009 Loss 4.610118, Accuracy 89.175%\n",
      "Epoch 32, Batch 510, LR 0.000009 Loss 4.611137, Accuracy 89.167%\n",
      "Epoch 32, Batch 511, LR 0.000009 Loss 4.611159, Accuracy 89.163%\n",
      "Epoch 32, Batch 512, LR 0.000009 Loss 4.612037, Accuracy 89.162%\n",
      "Epoch 32, Batch 513, LR 0.000009 Loss 4.611882, Accuracy 89.158%\n",
      "Epoch 32, Batch 514, LR 0.000009 Loss 4.611474, Accuracy 89.163%\n",
      "Epoch 32, Batch 515, LR 0.000009 Loss 4.611070, Accuracy 89.167%\n",
      "Epoch 32, Batch 516, LR 0.000009 Loss 4.610385, Accuracy 89.173%\n",
      "Epoch 32, Batch 517, LR 0.000009 Loss 4.610659, Accuracy 89.165%\n",
      "Epoch 32, Batch 518, LR 0.000009 Loss 4.610061, Accuracy 89.164%\n",
      "Epoch 32, Batch 519, LR 0.000009 Loss 4.611212, Accuracy 89.156%\n",
      "Epoch 32, Batch 520, LR 0.000009 Loss 4.611270, Accuracy 89.159%\n",
      "Epoch 32, Batch 521, LR 0.000009 Loss 4.612246, Accuracy 89.157%\n",
      "Epoch 32, Batch 522, LR 0.000009 Loss 4.610851, Accuracy 89.164%\n",
      "Epoch 32, Batch 523, LR 0.000009 Loss 4.609624, Accuracy 89.167%\n",
      "Epoch 32, Batch 524, LR 0.000009 Loss 4.609432, Accuracy 89.171%\n",
      "Epoch 32, Batch 525, LR 0.000009 Loss 4.608839, Accuracy 89.179%\n",
      "Epoch 32, Batch 526, LR 0.000009 Loss 4.608447, Accuracy 89.180%\n",
      "Epoch 32, Batch 527, LR 0.000009 Loss 4.609784, Accuracy 89.171%\n",
      "Epoch 32, Batch 528, LR 0.000009 Loss 4.609128, Accuracy 89.172%\n",
      "Epoch 32, Batch 529, LR 0.000009 Loss 4.608750, Accuracy 89.173%\n",
      "Epoch 32, Batch 530, LR 0.000009 Loss 4.608555, Accuracy 89.176%\n",
      "Epoch 32, Batch 531, LR 0.000009 Loss 4.608788, Accuracy 89.173%\n",
      "Epoch 32, Batch 532, LR 0.000009 Loss 4.608500, Accuracy 89.171%\n",
      "Epoch 32, Batch 533, LR 0.000009 Loss 4.608812, Accuracy 89.164%\n",
      "Epoch 32, Batch 534, LR 0.000009 Loss 4.608354, Accuracy 89.169%\n",
      "Epoch 32, Batch 535, LR 0.000009 Loss 4.609474, Accuracy 89.165%\n",
      "Epoch 32, Batch 536, LR 0.000009 Loss 4.608821, Accuracy 89.169%\n",
      "Epoch 32, Batch 537, LR 0.000009 Loss 4.608222, Accuracy 89.166%\n",
      "Epoch 32, Batch 538, LR 0.000009 Loss 4.608411, Accuracy 89.166%\n",
      "Epoch 32, Batch 539, LR 0.000009 Loss 4.607835, Accuracy 89.165%\n",
      "Epoch 32, Batch 540, LR 0.000009 Loss 4.606848, Accuracy 89.168%\n",
      "Epoch 32, Batch 541, LR 0.000009 Loss 4.607674, Accuracy 89.165%\n",
      "Epoch 32, Batch 542, LR 0.000009 Loss 4.608052, Accuracy 89.165%\n",
      "Epoch 32, Batch 543, LR 0.000009 Loss 4.609023, Accuracy 89.162%\n",
      "Epoch 32, Batch 544, LR 0.000009 Loss 4.607310, Accuracy 89.163%\n",
      "Epoch 32, Batch 545, LR 0.000009 Loss 4.606268, Accuracy 89.180%\n",
      "Epoch 32, Batch 546, LR 0.000009 Loss 4.607712, Accuracy 89.178%\n",
      "Epoch 32, Batch 547, LR 0.000009 Loss 4.608027, Accuracy 89.181%\n",
      "Epoch 32, Batch 548, LR 0.000009 Loss 4.608701, Accuracy 89.174%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 549, LR 0.000009 Loss 4.609518, Accuracy 89.166%\n",
      "Epoch 32, Batch 550, LR 0.000009 Loss 4.609292, Accuracy 89.159%\n",
      "Epoch 32, Batch 551, LR 0.000009 Loss 4.608351, Accuracy 89.165%\n",
      "Epoch 32, Batch 552, LR 0.000009 Loss 4.608717, Accuracy 89.171%\n",
      "Epoch 32, Batch 553, LR 0.000009 Loss 4.607757, Accuracy 89.173%\n",
      "Epoch 32, Batch 554, LR 0.000009 Loss 4.607531, Accuracy 89.170%\n",
      "Epoch 32, Batch 555, LR 0.000009 Loss 4.607133, Accuracy 89.174%\n",
      "Epoch 32, Batch 556, LR 0.000009 Loss 4.606957, Accuracy 89.175%\n",
      "Epoch 32, Batch 557, LR 0.000009 Loss 4.608316, Accuracy 89.166%\n",
      "Epoch 32, Batch 558, LR 0.000009 Loss 4.608218, Accuracy 89.168%\n",
      "Epoch 32, Batch 559, LR 0.000009 Loss 4.607828, Accuracy 89.167%\n",
      "Epoch 32, Batch 560, LR 0.000009 Loss 4.607412, Accuracy 89.171%\n",
      "Epoch 32, Batch 561, LR 0.000009 Loss 4.606191, Accuracy 89.179%\n",
      "Epoch 32, Batch 562, LR 0.000009 Loss 4.606087, Accuracy 89.176%\n",
      "Epoch 32, Batch 563, LR 0.000009 Loss 4.605554, Accuracy 89.172%\n",
      "Epoch 32, Batch 564, LR 0.000009 Loss 4.606195, Accuracy 89.169%\n",
      "Epoch 32, Batch 565, LR 0.000009 Loss 4.606459, Accuracy 89.169%\n",
      "Epoch 32, Batch 566, LR 0.000009 Loss 4.608567, Accuracy 89.159%\n",
      "Epoch 32, Batch 567, LR 0.000009 Loss 4.608495, Accuracy 89.160%\n",
      "Epoch 32, Batch 568, LR 0.000009 Loss 4.609816, Accuracy 89.156%\n",
      "Epoch 32, Batch 569, LR 0.000009 Loss 4.610004, Accuracy 89.153%\n",
      "Epoch 32, Batch 570, LR 0.000009 Loss 4.609312, Accuracy 89.153%\n",
      "Epoch 32, Batch 571, LR 0.000009 Loss 4.607714, Accuracy 89.160%\n",
      "Epoch 32, Batch 572, LR 0.000009 Loss 4.606896, Accuracy 89.159%\n",
      "Epoch 32, Batch 573, LR 0.000009 Loss 4.607744, Accuracy 89.154%\n",
      "Epoch 32, Batch 574, LR 0.000009 Loss 4.607636, Accuracy 89.151%\n",
      "Epoch 32, Batch 575, LR 0.000009 Loss 4.605855, Accuracy 89.162%\n",
      "Epoch 32, Batch 576, LR 0.000009 Loss 4.604837, Accuracy 89.166%\n",
      "Epoch 32, Batch 577, LR 0.000009 Loss 4.605926, Accuracy 89.156%\n",
      "Epoch 32, Batch 578, LR 0.000009 Loss 4.606389, Accuracy 89.154%\n",
      "Epoch 32, Batch 579, LR 0.000009 Loss 4.606539, Accuracy 89.150%\n",
      "Epoch 32, Batch 580, LR 0.000009 Loss 4.606347, Accuracy 89.150%\n",
      "Epoch 32, Batch 581, LR 0.000009 Loss 4.607104, Accuracy 89.146%\n",
      "Epoch 32, Batch 582, LR 0.000009 Loss 4.607276, Accuracy 89.144%\n",
      "Epoch 32, Batch 583, LR 0.000009 Loss 4.606562, Accuracy 89.143%\n",
      "Epoch 32, Batch 584, LR 0.000009 Loss 4.606643, Accuracy 89.136%\n",
      "Epoch 32, Batch 585, LR 0.000009 Loss 4.608631, Accuracy 89.124%\n",
      "Epoch 32, Batch 586, LR 0.000009 Loss 4.607617, Accuracy 89.126%\n",
      "Epoch 32, Batch 587, LR 0.000009 Loss 4.607993, Accuracy 89.128%\n",
      "Epoch 32, Batch 588, LR 0.000009 Loss 4.608386, Accuracy 89.121%\n",
      "Epoch 32, Batch 589, LR 0.000009 Loss 4.608680, Accuracy 89.120%\n",
      "Epoch 32, Batch 590, LR 0.000009 Loss 4.609352, Accuracy 89.113%\n",
      "Epoch 32, Batch 591, LR 0.000009 Loss 4.608547, Accuracy 89.117%\n",
      "Epoch 32, Batch 592, LR 0.000009 Loss 4.608320, Accuracy 89.119%\n",
      "Epoch 32, Batch 593, LR 0.000009 Loss 4.608740, Accuracy 89.119%\n",
      "Epoch 32, Batch 594, LR 0.000009 Loss 4.608183, Accuracy 89.128%\n",
      "Epoch 32, Batch 595, LR 0.000009 Loss 4.608686, Accuracy 89.120%\n",
      "Epoch 32, Batch 596, LR 0.000009 Loss 4.608936, Accuracy 89.116%\n",
      "Epoch 32, Batch 597, LR 0.000009 Loss 4.607581, Accuracy 89.116%\n",
      "Epoch 32, Batch 598, LR 0.000009 Loss 4.607716, Accuracy 89.119%\n",
      "Epoch 32, Batch 599, LR 0.000009 Loss 4.607972, Accuracy 89.117%\n",
      "Epoch 32, Batch 600, LR 0.000009 Loss 4.609631, Accuracy 89.113%\n",
      "Epoch 32, Batch 601, LR 0.000009 Loss 4.609025, Accuracy 89.109%\n",
      "Epoch 32, Batch 602, LR 0.000009 Loss 4.609599, Accuracy 89.108%\n",
      "Epoch 32, Batch 603, LR 0.000009 Loss 4.610061, Accuracy 89.101%\n",
      "Epoch 32, Batch 604, LR 0.000009 Loss 4.607908, Accuracy 89.108%\n",
      "Epoch 32, Batch 605, LR 0.000009 Loss 4.607286, Accuracy 89.110%\n",
      "Epoch 32, Batch 606, LR 0.000009 Loss 4.605772, Accuracy 89.117%\n",
      "Epoch 32, Batch 607, LR 0.000009 Loss 4.604853, Accuracy 89.122%\n",
      "Epoch 32, Batch 608, LR 0.000009 Loss 4.604122, Accuracy 89.124%\n",
      "Epoch 32, Batch 609, LR 0.000009 Loss 4.604042, Accuracy 89.129%\n",
      "Epoch 32, Batch 610, LR 0.000009 Loss 4.602765, Accuracy 89.136%\n",
      "Epoch 32, Batch 611, LR 0.000009 Loss 4.603002, Accuracy 89.132%\n",
      "Epoch 32, Batch 612, LR 0.000009 Loss 4.602499, Accuracy 89.131%\n",
      "Epoch 32, Batch 613, LR 0.000009 Loss 4.603357, Accuracy 89.130%\n",
      "Epoch 32, Batch 614, LR 0.000009 Loss 4.603346, Accuracy 89.130%\n",
      "Epoch 32, Batch 615, LR 0.000009 Loss 4.603260, Accuracy 89.126%\n",
      "Epoch 32, Batch 616, LR 0.000009 Loss 4.603308, Accuracy 89.118%\n",
      "Epoch 32, Batch 617, LR 0.000009 Loss 4.602072, Accuracy 89.121%\n",
      "Epoch 32, Batch 618, LR 0.000009 Loss 4.601441, Accuracy 89.124%\n",
      "Epoch 32, Batch 619, LR 0.000009 Loss 4.600719, Accuracy 89.124%\n",
      "Epoch 32, Batch 620, LR 0.000009 Loss 4.600820, Accuracy 89.127%\n",
      "Epoch 32, Batch 621, LR 0.000009 Loss 4.600730, Accuracy 89.127%\n",
      "Epoch 32, Batch 622, LR 0.000009 Loss 4.600440, Accuracy 89.130%\n",
      "Epoch 32, Batch 623, LR 0.000009 Loss 4.599073, Accuracy 89.135%\n",
      "Epoch 32, Batch 624, LR 0.000009 Loss 4.598603, Accuracy 89.138%\n",
      "Epoch 32, Batch 625, LR 0.000009 Loss 4.598660, Accuracy 89.135%\n",
      "Epoch 32, Batch 626, LR 0.000009 Loss 4.598637, Accuracy 89.140%\n",
      "Epoch 32, Batch 627, LR 0.000009 Loss 4.598067, Accuracy 89.135%\n",
      "Epoch 32, Batch 628, LR 0.000009 Loss 4.598396, Accuracy 89.131%\n",
      "Epoch 32, Batch 629, LR 0.000009 Loss 4.597576, Accuracy 89.132%\n",
      "Epoch 32, Batch 630, LR 0.000009 Loss 4.597806, Accuracy 89.131%\n",
      "Epoch 32, Batch 631, LR 0.000009 Loss 4.598669, Accuracy 89.131%\n",
      "Epoch 32, Batch 632, LR 0.000009 Loss 4.597338, Accuracy 89.134%\n",
      "Epoch 32, Batch 633, LR 0.000009 Loss 4.596556, Accuracy 89.143%\n",
      "Epoch 32, Batch 634, LR 0.000009 Loss 4.596775, Accuracy 89.145%\n",
      "Epoch 32, Batch 635, LR 0.000009 Loss 4.596390, Accuracy 89.145%\n",
      "Epoch 32, Batch 636, LR 0.000009 Loss 4.596439, Accuracy 89.141%\n",
      "Epoch 32, Batch 637, LR 0.000009 Loss 4.596632, Accuracy 89.142%\n",
      "Epoch 32, Batch 638, LR 0.000009 Loss 4.596442, Accuracy 89.140%\n",
      "Epoch 32, Batch 639, LR 0.000009 Loss 4.597718, Accuracy 89.135%\n",
      "Epoch 32, Batch 640, LR 0.000009 Loss 4.597302, Accuracy 89.137%\n",
      "Epoch 32, Batch 641, LR 0.000009 Loss 4.599765, Accuracy 89.126%\n",
      "Epoch 32, Batch 642, LR 0.000009 Loss 4.599109, Accuracy 89.127%\n",
      "Epoch 32, Batch 643, LR 0.000009 Loss 4.598497, Accuracy 89.124%\n",
      "Epoch 32, Batch 644, LR 0.000009 Loss 4.598121, Accuracy 89.123%\n",
      "Epoch 32, Batch 645, LR 0.000009 Loss 4.597198, Accuracy 89.127%\n",
      "Epoch 32, Batch 646, LR 0.000009 Loss 4.597961, Accuracy 89.115%\n",
      "Epoch 32, Batch 647, LR 0.000009 Loss 4.598677, Accuracy 89.113%\n",
      "Epoch 32, Batch 648, LR 0.000009 Loss 4.598913, Accuracy 89.108%\n",
      "Epoch 32, Batch 649, LR 0.000009 Loss 4.598110, Accuracy 89.114%\n",
      "Epoch 32, Batch 650, LR 0.000009 Loss 4.599062, Accuracy 89.108%\n",
      "Epoch 32, Batch 651, LR 0.000009 Loss 4.598530, Accuracy 89.107%\n",
      "Epoch 32, Batch 652, LR 0.000009 Loss 4.598369, Accuracy 89.109%\n",
      "Epoch 32, Batch 653, LR 0.000009 Loss 4.598598, Accuracy 89.107%\n",
      "Epoch 32, Batch 654, LR 0.000009 Loss 4.597570, Accuracy 89.111%\n",
      "Epoch 32, Batch 655, LR 0.000009 Loss 4.598062, Accuracy 89.113%\n",
      "Epoch 32, Batch 656, LR 0.000009 Loss 4.597509, Accuracy 89.116%\n",
      "Epoch 32, Batch 657, LR 0.000009 Loss 4.597616, Accuracy 89.118%\n",
      "Epoch 32, Batch 658, LR 0.000009 Loss 4.597467, Accuracy 89.121%\n",
      "Epoch 32, Batch 659, LR 0.000009 Loss 4.597381, Accuracy 89.121%\n",
      "Epoch 32, Batch 660, LR 0.000009 Loss 4.596853, Accuracy 89.121%\n",
      "Epoch 32, Batch 661, LR 0.000009 Loss 4.596725, Accuracy 89.120%\n",
      "Epoch 32, Batch 662, LR 0.000009 Loss 4.596833, Accuracy 89.127%\n",
      "Epoch 32, Batch 663, LR 0.000009 Loss 4.596933, Accuracy 89.126%\n",
      "Epoch 32, Batch 664, LR 0.000009 Loss 4.597604, Accuracy 89.120%\n",
      "Epoch 32, Batch 665, LR 0.000009 Loss 4.598021, Accuracy 89.117%\n",
      "Epoch 32, Batch 666, LR 0.000009 Loss 4.599782, Accuracy 89.111%\n",
      "Epoch 32, Batch 667, LR 0.000009 Loss 4.600296, Accuracy 89.107%\n",
      "Epoch 32, Batch 668, LR 0.000009 Loss 4.599799, Accuracy 89.108%\n",
      "Epoch 32, Batch 669, LR 0.000009 Loss 4.598971, Accuracy 89.109%\n",
      "Epoch 32, Batch 670, LR 0.000009 Loss 4.599138, Accuracy 89.109%\n",
      "Epoch 32, Batch 671, LR 0.000009 Loss 4.600225, Accuracy 89.102%\n",
      "Epoch 32, Batch 672, LR 0.000009 Loss 4.599285, Accuracy 89.104%\n",
      "Epoch 32, Batch 673, LR 0.000009 Loss 4.599380, Accuracy 89.108%\n",
      "Epoch 32, Batch 674, LR 0.000009 Loss 4.598183, Accuracy 89.111%\n",
      "Epoch 32, Batch 675, LR 0.000009 Loss 4.598250, Accuracy 89.112%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 676, LR 0.000009 Loss 4.598386, Accuracy 89.111%\n",
      "Epoch 32, Batch 677, LR 0.000009 Loss 4.598803, Accuracy 89.112%\n",
      "Epoch 32, Batch 678, LR 0.000009 Loss 4.598976, Accuracy 89.114%\n",
      "Epoch 32, Batch 679, LR 0.000009 Loss 4.599743, Accuracy 89.109%\n",
      "Epoch 32, Batch 680, LR 0.000009 Loss 4.600051, Accuracy 89.107%\n",
      "Epoch 32, Batch 681, LR 0.000009 Loss 4.600443, Accuracy 89.105%\n",
      "Epoch 32, Batch 682, LR 0.000009 Loss 4.600524, Accuracy 89.104%\n",
      "Epoch 32, Batch 683, LR 0.000009 Loss 4.600593, Accuracy 89.100%\n",
      "Epoch 32, Batch 684, LR 0.000009 Loss 4.601080, Accuracy 89.102%\n",
      "Epoch 32, Batch 685, LR 0.000009 Loss 4.601135, Accuracy 89.102%\n",
      "Epoch 32, Batch 686, LR 0.000009 Loss 4.601799, Accuracy 89.093%\n",
      "Epoch 32, Batch 687, LR 0.000009 Loss 4.601313, Accuracy 89.093%\n",
      "Epoch 32, Batch 688, LR 0.000009 Loss 4.600827, Accuracy 89.094%\n",
      "Epoch 32, Batch 689, LR 0.000009 Loss 4.601683, Accuracy 89.093%\n",
      "Epoch 32, Batch 690, LR 0.000009 Loss 4.601460, Accuracy 89.098%\n",
      "Epoch 32, Batch 691, LR 0.000009 Loss 4.601321, Accuracy 89.100%\n",
      "Epoch 32, Batch 692, LR 0.000009 Loss 4.601761, Accuracy 89.094%\n",
      "Epoch 32, Batch 693, LR 0.000009 Loss 4.601149, Accuracy 89.099%\n",
      "Epoch 32, Batch 694, LR 0.000009 Loss 4.600326, Accuracy 89.104%\n",
      "Epoch 32, Batch 695, LR 0.000009 Loss 4.600841, Accuracy 89.098%\n",
      "Epoch 32, Batch 696, LR 0.000009 Loss 4.600706, Accuracy 89.101%\n",
      "Epoch 32, Batch 697, LR 0.000009 Loss 4.599987, Accuracy 89.101%\n",
      "Epoch 32, Batch 698, LR 0.000009 Loss 4.599949, Accuracy 89.098%\n",
      "Epoch 32, Batch 699, LR 0.000009 Loss 4.600039, Accuracy 89.095%\n",
      "Epoch 32, Batch 700, LR 0.000009 Loss 4.598974, Accuracy 89.105%\n",
      "Epoch 32, Batch 701, LR 0.000009 Loss 4.599583, Accuracy 89.103%\n",
      "Epoch 32, Batch 702, LR 0.000009 Loss 4.599558, Accuracy 89.101%\n",
      "Epoch 32, Batch 703, LR 0.000009 Loss 4.600033, Accuracy 89.105%\n",
      "Epoch 32, Batch 704, LR 0.000009 Loss 4.599640, Accuracy 89.105%\n",
      "Epoch 32, Batch 705, LR 0.000009 Loss 4.599938, Accuracy 89.101%\n",
      "Epoch 32, Batch 706, LR 0.000009 Loss 4.599396, Accuracy 89.102%\n",
      "Epoch 32, Batch 707, LR 0.000009 Loss 4.598856, Accuracy 89.107%\n",
      "Epoch 32, Batch 708, LR 0.000009 Loss 4.598597, Accuracy 89.106%\n",
      "Epoch 32, Batch 709, LR 0.000009 Loss 4.599609, Accuracy 89.107%\n",
      "Epoch 32, Batch 710, LR 0.000009 Loss 4.600095, Accuracy 89.104%\n",
      "Epoch 32, Batch 711, LR 0.000009 Loss 4.600449, Accuracy 89.103%\n",
      "Epoch 32, Batch 712, LR 0.000009 Loss 4.600316, Accuracy 89.102%\n",
      "Epoch 32, Batch 713, LR 0.000009 Loss 4.599151, Accuracy 89.106%\n",
      "Epoch 32, Batch 714, LR 0.000009 Loss 4.600533, Accuracy 89.099%\n",
      "Epoch 32, Batch 715, LR 0.000009 Loss 4.600015, Accuracy 89.100%\n",
      "Epoch 32, Batch 716, LR 0.000009 Loss 4.599656, Accuracy 89.101%\n",
      "Epoch 32, Batch 717, LR 0.000009 Loss 4.600608, Accuracy 89.097%\n",
      "Epoch 32, Batch 718, LR 0.000009 Loss 4.601765, Accuracy 89.088%\n",
      "Epoch 32, Batch 719, LR 0.000009 Loss 4.602130, Accuracy 89.090%\n",
      "Epoch 32, Batch 720, LR 0.000009 Loss 4.601673, Accuracy 89.087%\n",
      "Epoch 32, Batch 721, LR 0.000009 Loss 4.602263, Accuracy 89.085%\n",
      "Epoch 32, Batch 722, LR 0.000009 Loss 4.602396, Accuracy 89.080%\n",
      "Epoch 32, Batch 723, LR 0.000009 Loss 4.602822, Accuracy 89.079%\n",
      "Epoch 32, Batch 724, LR 0.000009 Loss 4.601796, Accuracy 89.081%\n",
      "Epoch 32, Batch 725, LR 0.000009 Loss 4.601695, Accuracy 89.082%\n",
      "Epoch 32, Batch 726, LR 0.000009 Loss 4.601988, Accuracy 89.085%\n",
      "Epoch 32, Batch 727, LR 0.000009 Loss 4.601779, Accuracy 89.085%\n",
      "Epoch 32, Batch 728, LR 0.000009 Loss 4.601522, Accuracy 89.084%\n",
      "Epoch 32, Batch 729, LR 0.000009 Loss 4.601166, Accuracy 89.083%\n",
      "Epoch 32, Batch 730, LR 0.000009 Loss 4.600701, Accuracy 89.087%\n",
      "Epoch 32, Batch 731, LR 0.000009 Loss 4.600888, Accuracy 89.086%\n",
      "Epoch 32, Batch 732, LR 0.000009 Loss 4.601425, Accuracy 89.082%\n",
      "Epoch 32, Batch 733, LR 0.000009 Loss 4.601395, Accuracy 89.080%\n",
      "Epoch 32, Batch 734, LR 0.000009 Loss 4.600330, Accuracy 89.083%\n",
      "Epoch 32, Batch 735, LR 0.000009 Loss 4.600060, Accuracy 89.084%\n",
      "Epoch 32, Batch 736, LR 0.000009 Loss 4.599036, Accuracy 89.088%\n",
      "Epoch 32, Batch 737, LR 0.000009 Loss 4.599274, Accuracy 89.091%\n",
      "Epoch 32, Batch 738, LR 0.000009 Loss 4.599913, Accuracy 89.092%\n",
      "Epoch 32, Batch 739, LR 0.000009 Loss 4.599936, Accuracy 89.089%\n",
      "Epoch 32, Batch 740, LR 0.000009 Loss 4.600632, Accuracy 89.088%\n",
      "Epoch 32, Batch 741, LR 0.000009 Loss 4.601043, Accuracy 89.086%\n",
      "Epoch 32, Batch 742, LR 0.000009 Loss 4.600583, Accuracy 89.085%\n",
      "Epoch 32, Batch 743, LR 0.000009 Loss 4.599713, Accuracy 89.091%\n",
      "Epoch 32, Batch 744, LR 0.000009 Loss 4.599870, Accuracy 89.091%\n",
      "Epoch 32, Batch 745, LR 0.000009 Loss 4.599446, Accuracy 89.096%\n",
      "Epoch 32, Batch 746, LR 0.000009 Loss 4.599512, Accuracy 89.097%\n",
      "Epoch 32, Batch 747, LR 0.000009 Loss 4.599958, Accuracy 89.096%\n",
      "Epoch 32, Batch 748, LR 0.000009 Loss 4.600770, Accuracy 89.088%\n",
      "Epoch 32, Batch 749, LR 0.000009 Loss 4.599262, Accuracy 89.090%\n",
      "Epoch 32, Batch 750, LR 0.000009 Loss 4.599062, Accuracy 89.090%\n",
      "Epoch 32, Batch 751, LR 0.000009 Loss 4.599955, Accuracy 89.084%\n",
      "Epoch 32, Batch 752, LR 0.000009 Loss 4.599573, Accuracy 89.085%\n",
      "Epoch 32, Batch 753, LR 0.000009 Loss 4.600119, Accuracy 89.087%\n",
      "Epoch 32, Batch 754, LR 0.000009 Loss 4.600051, Accuracy 89.088%\n",
      "Epoch 32, Batch 755, LR 0.000009 Loss 4.600797, Accuracy 89.086%\n",
      "Epoch 32, Batch 756, LR 0.000009 Loss 4.600876, Accuracy 89.079%\n",
      "Epoch 32, Batch 757, LR 0.000009 Loss 4.601635, Accuracy 89.072%\n",
      "Epoch 32, Batch 758, LR 0.000009 Loss 4.602675, Accuracy 89.068%\n",
      "Epoch 32, Batch 759, LR 0.000009 Loss 4.603386, Accuracy 89.068%\n",
      "Epoch 32, Batch 760, LR 0.000009 Loss 4.602531, Accuracy 89.071%\n",
      "Epoch 32, Batch 761, LR 0.000009 Loss 4.601794, Accuracy 89.077%\n",
      "Epoch 32, Batch 762, LR 0.000009 Loss 4.601458, Accuracy 89.080%\n",
      "Epoch 32, Batch 763, LR 0.000009 Loss 4.601458, Accuracy 89.083%\n",
      "Epoch 32, Batch 764, LR 0.000009 Loss 4.600703, Accuracy 89.086%\n",
      "Epoch 32, Batch 765, LR 0.000009 Loss 4.600398, Accuracy 89.090%\n",
      "Epoch 32, Batch 766, LR 0.000009 Loss 4.601086, Accuracy 89.085%\n",
      "Epoch 32, Batch 767, LR 0.000009 Loss 4.601374, Accuracy 89.083%\n",
      "Epoch 32, Batch 768, LR 0.000009 Loss 4.601430, Accuracy 89.081%\n",
      "Epoch 32, Batch 769, LR 0.000009 Loss 4.600364, Accuracy 89.087%\n",
      "Epoch 32, Batch 770, LR 0.000009 Loss 4.600828, Accuracy 89.080%\n",
      "Epoch 32, Batch 771, LR 0.000009 Loss 4.600889, Accuracy 89.082%\n",
      "Epoch 32, Batch 772, LR 0.000009 Loss 4.600573, Accuracy 89.089%\n",
      "Epoch 32, Batch 773, LR 0.000009 Loss 4.599758, Accuracy 89.094%\n",
      "Epoch 32, Batch 774, LR 0.000009 Loss 4.599888, Accuracy 89.093%\n",
      "Epoch 32, Batch 775, LR 0.000009 Loss 4.599316, Accuracy 89.095%\n",
      "Epoch 32, Batch 776, LR 0.000009 Loss 4.599567, Accuracy 89.094%\n",
      "Epoch 32, Batch 777, LR 0.000009 Loss 4.599654, Accuracy 89.099%\n",
      "Epoch 32, Batch 778, LR 0.000009 Loss 4.598535, Accuracy 89.104%\n",
      "Epoch 32, Batch 779, LR 0.000009 Loss 4.599676, Accuracy 89.098%\n",
      "Epoch 32, Batch 780, LR 0.000009 Loss 4.598994, Accuracy 89.102%\n",
      "Epoch 32, Batch 781, LR 0.000009 Loss 4.598999, Accuracy 89.106%\n",
      "Epoch 32, Batch 782, LR 0.000009 Loss 4.598499, Accuracy 89.105%\n",
      "Epoch 32, Batch 783, LR 0.000009 Loss 4.598125, Accuracy 89.108%\n",
      "Epoch 32, Batch 784, LR 0.000009 Loss 4.597165, Accuracy 89.112%\n",
      "Epoch 32, Batch 785, LR 0.000009 Loss 4.596917, Accuracy 89.116%\n",
      "Epoch 32, Batch 786, LR 0.000009 Loss 4.597012, Accuracy 89.115%\n",
      "Epoch 32, Batch 787, LR 0.000009 Loss 4.596714, Accuracy 89.111%\n",
      "Epoch 32, Batch 788, LR 0.000009 Loss 4.596274, Accuracy 89.114%\n",
      "Epoch 32, Batch 789, LR 0.000009 Loss 4.595540, Accuracy 89.117%\n",
      "Epoch 32, Batch 790, LR 0.000009 Loss 4.595011, Accuracy 89.117%\n",
      "Epoch 32, Batch 791, LR 0.000009 Loss 4.594838, Accuracy 89.118%\n",
      "Epoch 32, Batch 792, LR 0.000009 Loss 4.596006, Accuracy 89.113%\n",
      "Epoch 32, Batch 793, LR 0.000009 Loss 4.595681, Accuracy 89.110%\n",
      "Epoch 32, Batch 794, LR 0.000009 Loss 4.595085, Accuracy 89.116%\n",
      "Epoch 32, Batch 795, LR 0.000009 Loss 4.593842, Accuracy 89.120%\n",
      "Epoch 32, Batch 796, LR 0.000009 Loss 4.593646, Accuracy 89.122%\n",
      "Epoch 32, Batch 797, LR 0.000009 Loss 4.593240, Accuracy 89.129%\n",
      "Epoch 32, Batch 798, LR 0.000009 Loss 4.592689, Accuracy 89.134%\n",
      "Epoch 32, Batch 799, LR 0.000009 Loss 4.592323, Accuracy 89.133%\n",
      "Epoch 32, Batch 800, LR 0.000009 Loss 4.592479, Accuracy 89.131%\n",
      "Epoch 32, Batch 801, LR 0.000009 Loss 4.591646, Accuracy 89.134%\n",
      "Epoch 32, Batch 802, LR 0.000009 Loss 4.590916, Accuracy 89.135%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 803, LR 0.000009 Loss 4.590328, Accuracy 89.134%\n",
      "Epoch 32, Batch 804, LR 0.000009 Loss 4.590486, Accuracy 89.134%\n",
      "Epoch 32, Batch 805, LR 0.000009 Loss 4.589779, Accuracy 89.140%\n",
      "Epoch 32, Batch 806, LR 0.000009 Loss 4.590309, Accuracy 89.141%\n",
      "Epoch 32, Batch 807, LR 0.000009 Loss 4.590800, Accuracy 89.136%\n",
      "Epoch 32, Batch 808, LR 0.000009 Loss 4.590877, Accuracy 89.135%\n",
      "Epoch 32, Batch 809, LR 0.000009 Loss 4.591068, Accuracy 89.131%\n",
      "Epoch 32, Batch 810, LR 0.000009 Loss 4.591317, Accuracy 89.133%\n",
      "Epoch 32, Batch 811, LR 0.000009 Loss 4.590544, Accuracy 89.140%\n",
      "Epoch 32, Batch 812, LR 0.000009 Loss 4.590166, Accuracy 89.139%\n",
      "Epoch 32, Batch 813, LR 0.000009 Loss 4.589897, Accuracy 89.138%\n",
      "Epoch 32, Batch 814, LR 0.000009 Loss 4.589382, Accuracy 89.141%\n",
      "Epoch 32, Batch 815, LR 0.000009 Loss 4.589677, Accuracy 89.140%\n",
      "Epoch 32, Batch 816, LR 0.000009 Loss 4.590056, Accuracy 89.134%\n",
      "Epoch 32, Batch 817, LR 0.000009 Loss 4.590138, Accuracy 89.131%\n",
      "Epoch 32, Batch 818, LR 0.000009 Loss 4.589640, Accuracy 89.132%\n",
      "Epoch 32, Batch 819, LR 0.000009 Loss 4.589552, Accuracy 89.133%\n",
      "Epoch 32, Batch 820, LR 0.000009 Loss 4.590382, Accuracy 89.128%\n",
      "Epoch 32, Batch 821, LR 0.000009 Loss 4.590032, Accuracy 89.133%\n",
      "Epoch 32, Batch 822, LR 0.000009 Loss 4.590177, Accuracy 89.133%\n",
      "Epoch 32, Batch 823, LR 0.000009 Loss 4.590923, Accuracy 89.132%\n",
      "Epoch 32, Batch 824, LR 0.000009 Loss 4.590036, Accuracy 89.138%\n",
      "Epoch 32, Batch 825, LR 0.000009 Loss 4.588981, Accuracy 89.146%\n",
      "Epoch 32, Batch 826, LR 0.000009 Loss 4.589894, Accuracy 89.137%\n",
      "Epoch 32, Batch 827, LR 0.000009 Loss 4.590337, Accuracy 89.133%\n",
      "Epoch 32, Batch 828, LR 0.000009 Loss 4.589297, Accuracy 89.134%\n",
      "Epoch 32, Batch 829, LR 0.000009 Loss 4.589605, Accuracy 89.128%\n",
      "Epoch 32, Batch 830, LR 0.000009 Loss 4.589310, Accuracy 89.128%\n",
      "Epoch 32, Batch 831, LR 0.000009 Loss 4.589236, Accuracy 89.126%\n",
      "Epoch 32, Batch 832, LR 0.000009 Loss 4.589697, Accuracy 89.121%\n",
      "Epoch 32, Batch 833, LR 0.000009 Loss 4.590314, Accuracy 89.118%\n",
      "Epoch 32, Batch 834, LR 0.000009 Loss 4.590255, Accuracy 89.118%\n",
      "Epoch 32, Batch 835, LR 0.000009 Loss 4.590045, Accuracy 89.117%\n",
      "Epoch 32, Batch 836, LR 0.000009 Loss 4.590655, Accuracy 89.117%\n",
      "Epoch 32, Batch 837, LR 0.000009 Loss 4.590360, Accuracy 89.117%\n",
      "Epoch 32, Batch 838, LR 0.000009 Loss 4.591104, Accuracy 89.114%\n",
      "Epoch 32, Batch 839, LR 0.000009 Loss 4.591223, Accuracy 89.111%\n",
      "Epoch 32, Batch 840, LR 0.000009 Loss 4.591594, Accuracy 89.107%\n",
      "Epoch 32, Batch 841, LR 0.000009 Loss 4.591787, Accuracy 89.105%\n",
      "Epoch 32, Batch 842, LR 0.000009 Loss 4.591504, Accuracy 89.105%\n",
      "Epoch 32, Batch 843, LR 0.000009 Loss 4.591004, Accuracy 89.106%\n",
      "Epoch 32, Batch 844, LR 0.000009 Loss 4.591090, Accuracy 89.106%\n",
      "Epoch 32, Batch 845, LR 0.000009 Loss 4.591151, Accuracy 89.109%\n",
      "Epoch 32, Batch 846, LR 0.000009 Loss 4.590603, Accuracy 89.113%\n",
      "Epoch 32, Batch 847, LR 0.000009 Loss 4.590938, Accuracy 89.110%\n",
      "Epoch 32, Batch 848, LR 0.000009 Loss 4.590410, Accuracy 89.114%\n",
      "Epoch 32, Batch 849, LR 0.000009 Loss 4.590424, Accuracy 89.114%\n",
      "Epoch 32, Batch 850, LR 0.000009 Loss 4.589769, Accuracy 89.118%\n",
      "Epoch 32, Batch 851, LR 0.000009 Loss 4.590579, Accuracy 89.114%\n",
      "Epoch 32, Batch 852, LR 0.000009 Loss 4.590662, Accuracy 89.116%\n",
      "Epoch 32, Batch 853, LR 0.000009 Loss 4.589860, Accuracy 89.118%\n",
      "Epoch 32, Batch 854, LR 0.000009 Loss 4.588578, Accuracy 89.124%\n",
      "Epoch 32, Batch 855, LR 0.000009 Loss 4.588868, Accuracy 89.122%\n",
      "Epoch 32, Batch 856, LR 0.000009 Loss 4.588951, Accuracy 89.124%\n",
      "Epoch 32, Batch 857, LR 0.000009 Loss 4.587718, Accuracy 89.125%\n",
      "Epoch 32, Batch 858, LR 0.000009 Loss 4.587885, Accuracy 89.123%\n",
      "Epoch 32, Batch 859, LR 0.000009 Loss 4.587880, Accuracy 89.123%\n",
      "Epoch 32, Batch 860, LR 0.000009 Loss 4.588711, Accuracy 89.116%\n",
      "Epoch 32, Batch 861, LR 0.000009 Loss 4.587276, Accuracy 89.121%\n",
      "Epoch 32, Batch 862, LR 0.000009 Loss 4.587049, Accuracy 89.127%\n",
      "Epoch 32, Batch 863, LR 0.000009 Loss 4.587601, Accuracy 89.126%\n",
      "Epoch 32, Batch 864, LR 0.000009 Loss 4.587375, Accuracy 89.129%\n",
      "Epoch 32, Batch 865, LR 0.000009 Loss 4.586315, Accuracy 89.134%\n",
      "Epoch 32, Batch 866, LR 0.000009 Loss 4.586651, Accuracy 89.135%\n",
      "Epoch 32, Batch 867, LR 0.000009 Loss 4.586345, Accuracy 89.135%\n",
      "Epoch 32, Batch 868, LR 0.000009 Loss 4.585945, Accuracy 89.138%\n",
      "Epoch 32, Batch 869, LR 0.000009 Loss 4.586245, Accuracy 89.136%\n",
      "Epoch 32, Batch 870, LR 0.000009 Loss 4.586509, Accuracy 89.136%\n",
      "Epoch 32, Batch 871, LR 0.000009 Loss 4.586346, Accuracy 89.136%\n",
      "Epoch 32, Batch 872, LR 0.000009 Loss 4.586572, Accuracy 89.134%\n",
      "Epoch 32, Batch 873, LR 0.000009 Loss 4.586012, Accuracy 89.139%\n",
      "Epoch 32, Batch 874, LR 0.000009 Loss 4.585992, Accuracy 89.143%\n",
      "Epoch 32, Batch 875, LR 0.000009 Loss 4.585613, Accuracy 89.145%\n",
      "Epoch 32, Batch 876, LR 0.000009 Loss 4.586103, Accuracy 89.139%\n",
      "Epoch 32, Batch 877, LR 0.000009 Loss 4.585881, Accuracy 89.140%\n",
      "Epoch 32, Batch 878, LR 0.000009 Loss 4.585582, Accuracy 89.143%\n",
      "Epoch 32, Batch 879, LR 0.000009 Loss 4.585678, Accuracy 89.141%\n",
      "Epoch 32, Batch 880, LR 0.000009 Loss 4.585991, Accuracy 89.137%\n",
      "Epoch 32, Batch 881, LR 0.000009 Loss 4.585663, Accuracy 89.141%\n",
      "Epoch 32, Batch 882, LR 0.000009 Loss 4.585324, Accuracy 89.148%\n",
      "Epoch 32, Batch 883, LR 0.000009 Loss 4.585825, Accuracy 89.143%\n",
      "Epoch 32, Batch 884, LR 0.000008 Loss 4.587393, Accuracy 89.135%\n",
      "Epoch 32, Batch 885, LR 0.000008 Loss 4.586942, Accuracy 89.138%\n",
      "Epoch 32, Batch 886, LR 0.000008 Loss 4.588035, Accuracy 89.134%\n",
      "Epoch 32, Batch 887, LR 0.000008 Loss 4.588209, Accuracy 89.136%\n",
      "Epoch 32, Batch 888, LR 0.000008 Loss 4.589047, Accuracy 89.130%\n",
      "Epoch 32, Batch 889, LR 0.000008 Loss 4.589096, Accuracy 89.128%\n",
      "Epoch 32, Batch 890, LR 0.000008 Loss 4.589812, Accuracy 89.120%\n",
      "Epoch 32, Batch 891, LR 0.000008 Loss 4.589342, Accuracy 89.119%\n",
      "Epoch 32, Batch 892, LR 0.000008 Loss 4.589647, Accuracy 89.119%\n",
      "Epoch 32, Batch 893, LR 0.000008 Loss 4.589801, Accuracy 89.115%\n",
      "Epoch 32, Batch 894, LR 0.000008 Loss 4.589789, Accuracy 89.118%\n",
      "Epoch 32, Batch 895, LR 0.000008 Loss 4.590355, Accuracy 89.118%\n",
      "Epoch 32, Batch 896, LR 0.000008 Loss 4.589898, Accuracy 89.120%\n",
      "Epoch 32, Batch 897, LR 0.000008 Loss 4.589334, Accuracy 89.123%\n",
      "Epoch 32, Batch 898, LR 0.000008 Loss 4.589864, Accuracy 89.120%\n",
      "Epoch 32, Batch 899, LR 0.000008 Loss 4.590437, Accuracy 89.118%\n",
      "Epoch 32, Batch 900, LR 0.000008 Loss 4.589549, Accuracy 89.124%\n",
      "Epoch 32, Batch 901, LR 0.000008 Loss 4.590319, Accuracy 89.121%\n",
      "Epoch 32, Batch 902, LR 0.000008 Loss 4.589753, Accuracy 89.124%\n",
      "Epoch 32, Batch 903, LR 0.000008 Loss 4.589976, Accuracy 89.122%\n",
      "Epoch 32, Batch 904, LR 0.000008 Loss 4.589489, Accuracy 89.127%\n",
      "Epoch 32, Batch 905, LR 0.000008 Loss 4.590536, Accuracy 89.119%\n",
      "Epoch 32, Batch 906, LR 0.000008 Loss 4.590427, Accuracy 89.119%\n",
      "Epoch 32, Batch 907, LR 0.000008 Loss 4.590287, Accuracy 89.120%\n",
      "Epoch 32, Batch 908, LR 0.000008 Loss 4.589994, Accuracy 89.121%\n",
      "Epoch 32, Batch 909, LR 0.000008 Loss 4.589444, Accuracy 89.122%\n",
      "Epoch 32, Batch 910, LR 0.000008 Loss 4.589151, Accuracy 89.120%\n",
      "Epoch 32, Batch 911, LR 0.000008 Loss 4.587981, Accuracy 89.122%\n",
      "Epoch 32, Batch 912, LR 0.000008 Loss 4.587247, Accuracy 89.121%\n",
      "Epoch 32, Batch 913, LR 0.000008 Loss 4.586968, Accuracy 89.122%\n",
      "Epoch 32, Batch 914, LR 0.000008 Loss 4.586234, Accuracy 89.125%\n",
      "Epoch 32, Batch 915, LR 0.000008 Loss 4.586257, Accuracy 89.124%\n",
      "Epoch 32, Batch 916, LR 0.000008 Loss 4.586000, Accuracy 89.122%\n",
      "Epoch 32, Batch 917, LR 0.000008 Loss 4.586230, Accuracy 89.120%\n",
      "Epoch 32, Batch 918, LR 0.000008 Loss 4.585938, Accuracy 89.123%\n",
      "Epoch 32, Batch 919, LR 0.000008 Loss 4.585841, Accuracy 89.123%\n",
      "Epoch 32, Batch 920, LR 0.000008 Loss 4.585200, Accuracy 89.123%\n",
      "Epoch 32, Batch 921, LR 0.000008 Loss 4.584803, Accuracy 89.127%\n",
      "Epoch 32, Batch 922, LR 0.000008 Loss 4.584177, Accuracy 89.130%\n",
      "Epoch 32, Batch 923, LR 0.000008 Loss 4.583730, Accuracy 89.129%\n",
      "Epoch 32, Batch 924, LR 0.000008 Loss 4.583690, Accuracy 89.132%\n",
      "Epoch 32, Batch 925, LR 0.000008 Loss 4.584024, Accuracy 89.128%\n",
      "Epoch 32, Batch 926, LR 0.000008 Loss 4.584526, Accuracy 89.123%\n",
      "Epoch 32, Batch 927, LR 0.000008 Loss 4.584417, Accuracy 89.123%\n",
      "Epoch 32, Batch 928, LR 0.000008 Loss 4.584248, Accuracy 89.124%\n",
      "Epoch 32, Batch 929, LR 0.000008 Loss 4.583469, Accuracy 89.125%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 930, LR 0.000008 Loss 4.582758, Accuracy 89.127%\n",
      "Epoch 32, Batch 931, LR 0.000008 Loss 4.582666, Accuracy 89.130%\n",
      "Epoch 32, Batch 932, LR 0.000008 Loss 4.582411, Accuracy 89.131%\n",
      "Epoch 32, Batch 933, LR 0.000008 Loss 4.582211, Accuracy 89.136%\n",
      "Epoch 32, Batch 934, LR 0.000008 Loss 4.582569, Accuracy 89.137%\n",
      "Epoch 32, Batch 935, LR 0.000008 Loss 4.582149, Accuracy 89.142%\n",
      "Epoch 32, Batch 936, LR 0.000008 Loss 4.582579, Accuracy 89.139%\n",
      "Epoch 32, Batch 937, LR 0.000008 Loss 4.582087, Accuracy 89.141%\n",
      "Epoch 32, Batch 938, LR 0.000008 Loss 4.582523, Accuracy 89.136%\n",
      "Epoch 32, Batch 939, LR 0.000008 Loss 4.582161, Accuracy 89.138%\n",
      "Epoch 32, Batch 940, LR 0.000008 Loss 4.582193, Accuracy 89.138%\n",
      "Epoch 32, Batch 941, LR 0.000008 Loss 4.582769, Accuracy 89.140%\n",
      "Epoch 32, Batch 942, LR 0.000008 Loss 4.582761, Accuracy 89.140%\n",
      "Epoch 32, Batch 943, LR 0.000008 Loss 4.583482, Accuracy 89.135%\n",
      "Epoch 32, Batch 944, LR 0.000008 Loss 4.583283, Accuracy 89.136%\n",
      "Epoch 32, Batch 945, LR 0.000008 Loss 4.583251, Accuracy 89.138%\n",
      "Epoch 32, Batch 946, LR 0.000008 Loss 4.583233, Accuracy 89.138%\n",
      "Epoch 32, Batch 947, LR 0.000008 Loss 4.582993, Accuracy 89.138%\n",
      "Epoch 32, Batch 948, LR 0.000008 Loss 4.583653, Accuracy 89.137%\n",
      "Epoch 32, Batch 949, LR 0.000008 Loss 4.584231, Accuracy 89.136%\n",
      "Epoch 32, Batch 950, LR 0.000008 Loss 4.584082, Accuracy 89.134%\n",
      "Epoch 32, Batch 951, LR 0.000008 Loss 4.583588, Accuracy 89.136%\n",
      "Epoch 32, Batch 952, LR 0.000008 Loss 4.583131, Accuracy 89.137%\n",
      "Epoch 32, Batch 953, LR 0.000008 Loss 4.582530, Accuracy 89.140%\n",
      "Epoch 32, Batch 954, LR 0.000008 Loss 4.582365, Accuracy 89.139%\n",
      "Epoch 32, Batch 955, LR 0.000008 Loss 4.582450, Accuracy 89.137%\n",
      "Epoch 32, Batch 956, LR 0.000008 Loss 4.582964, Accuracy 89.139%\n",
      "Epoch 32, Batch 957, LR 0.000008 Loss 4.583180, Accuracy 89.137%\n",
      "Epoch 32, Batch 958, LR 0.000008 Loss 4.582757, Accuracy 89.138%\n",
      "Epoch 32, Batch 959, LR 0.000008 Loss 4.582991, Accuracy 89.137%\n",
      "Epoch 32, Batch 960, LR 0.000008 Loss 4.582727, Accuracy 89.138%\n",
      "Epoch 32, Batch 961, LR 0.000008 Loss 4.582436, Accuracy 89.136%\n",
      "Epoch 32, Batch 962, LR 0.000008 Loss 4.582671, Accuracy 89.136%\n",
      "Epoch 32, Batch 963, LR 0.000008 Loss 4.582976, Accuracy 89.133%\n",
      "Epoch 32, Batch 964, LR 0.000008 Loss 4.583133, Accuracy 89.133%\n",
      "Epoch 32, Batch 965, LR 0.000008 Loss 4.583229, Accuracy 89.135%\n",
      "Epoch 32, Batch 966, LR 0.000008 Loss 4.582834, Accuracy 89.135%\n",
      "Epoch 32, Batch 967, LR 0.000008 Loss 4.583375, Accuracy 89.131%\n",
      "Epoch 32, Batch 968, LR 0.000008 Loss 4.583311, Accuracy 89.133%\n",
      "Epoch 32, Batch 969, LR 0.000008 Loss 4.583122, Accuracy 89.131%\n",
      "Epoch 32, Batch 970, LR 0.000008 Loss 4.583844, Accuracy 89.128%\n",
      "Epoch 32, Batch 971, LR 0.000008 Loss 4.584037, Accuracy 89.130%\n",
      "Epoch 32, Batch 972, LR 0.000008 Loss 4.584129, Accuracy 89.128%\n",
      "Epoch 32, Batch 973, LR 0.000008 Loss 4.583952, Accuracy 89.128%\n",
      "Epoch 32, Batch 974, LR 0.000008 Loss 4.584365, Accuracy 89.124%\n",
      "Epoch 32, Batch 975, LR 0.000008 Loss 4.583685, Accuracy 89.128%\n",
      "Epoch 32, Batch 976, LR 0.000008 Loss 4.584892, Accuracy 89.123%\n",
      "Epoch 32, Batch 977, LR 0.000008 Loss 4.584898, Accuracy 89.123%\n",
      "Epoch 32, Batch 978, LR 0.000008 Loss 4.584538, Accuracy 89.126%\n",
      "Epoch 32, Batch 979, LR 0.000008 Loss 4.584396, Accuracy 89.131%\n",
      "Epoch 32, Batch 980, LR 0.000008 Loss 4.584671, Accuracy 89.129%\n",
      "Epoch 32, Batch 981, LR 0.000008 Loss 4.584184, Accuracy 89.132%\n",
      "Epoch 32, Batch 982, LR 0.000008 Loss 4.584507, Accuracy 89.128%\n",
      "Epoch 32, Batch 983, LR 0.000008 Loss 4.584680, Accuracy 89.125%\n",
      "Epoch 32, Batch 984, LR 0.000008 Loss 4.584822, Accuracy 89.128%\n",
      "Epoch 32, Batch 985, LR 0.000008 Loss 4.584313, Accuracy 89.128%\n",
      "Epoch 32, Batch 986, LR 0.000008 Loss 4.583988, Accuracy 89.128%\n",
      "Epoch 32, Batch 987, LR 0.000008 Loss 4.584368, Accuracy 89.124%\n",
      "Epoch 32, Batch 988, LR 0.000008 Loss 4.583788, Accuracy 89.126%\n",
      "Epoch 32, Batch 989, LR 0.000008 Loss 4.583181, Accuracy 89.128%\n",
      "Epoch 32, Batch 990, LR 0.000008 Loss 4.582724, Accuracy 89.129%\n",
      "Epoch 32, Batch 991, LR 0.000008 Loss 4.582513, Accuracy 89.129%\n",
      "Epoch 32, Batch 992, LR 0.000008 Loss 4.582502, Accuracy 89.126%\n",
      "Epoch 32, Batch 993, LR 0.000008 Loss 4.582019, Accuracy 89.128%\n",
      "Epoch 32, Batch 994, LR 0.000008 Loss 4.582154, Accuracy 89.125%\n",
      "Epoch 32, Batch 995, LR 0.000008 Loss 4.581742, Accuracy 89.128%\n",
      "Epoch 32, Batch 996, LR 0.000008 Loss 4.581559, Accuracy 89.132%\n",
      "Epoch 32, Batch 997, LR 0.000008 Loss 4.581271, Accuracy 89.131%\n",
      "Epoch 32, Batch 998, LR 0.000008 Loss 4.581773, Accuracy 89.136%\n",
      "Epoch 32, Batch 999, LR 0.000008 Loss 4.581517, Accuracy 89.137%\n",
      "Epoch 32, Batch 1000, LR 0.000008 Loss 4.581844, Accuracy 89.134%\n",
      "Epoch 32, Batch 1001, LR 0.000008 Loss 4.582230, Accuracy 89.134%\n",
      "Epoch 32, Batch 1002, LR 0.000008 Loss 4.582136, Accuracy 89.136%\n",
      "Epoch 32, Batch 1003, LR 0.000008 Loss 4.582238, Accuracy 89.133%\n",
      "Epoch 32, Batch 1004, LR 0.000008 Loss 4.582488, Accuracy 89.135%\n",
      "Epoch 32, Batch 1005, LR 0.000008 Loss 4.582560, Accuracy 89.134%\n",
      "Epoch 32, Batch 1006, LR 0.000008 Loss 4.582428, Accuracy 89.136%\n",
      "Epoch 32, Batch 1007, LR 0.000008 Loss 4.582743, Accuracy 89.135%\n",
      "Epoch 32, Batch 1008, LR 0.000008 Loss 4.582285, Accuracy 89.136%\n",
      "Epoch 32, Batch 1009, LR 0.000008 Loss 4.582830, Accuracy 89.134%\n",
      "Epoch 32, Batch 1010, LR 0.000008 Loss 4.583244, Accuracy 89.132%\n",
      "Epoch 32, Batch 1011, LR 0.000008 Loss 4.583708, Accuracy 89.128%\n",
      "Epoch 32, Batch 1012, LR 0.000008 Loss 4.582810, Accuracy 89.130%\n",
      "Epoch 32, Batch 1013, LR 0.000008 Loss 4.582450, Accuracy 89.130%\n",
      "Epoch 32, Batch 1014, LR 0.000008 Loss 4.582211, Accuracy 89.131%\n",
      "Epoch 32, Batch 1015, LR 0.000008 Loss 4.581951, Accuracy 89.133%\n",
      "Epoch 32, Batch 1016, LR 0.000008 Loss 4.581618, Accuracy 89.132%\n",
      "Epoch 32, Batch 1017, LR 0.000008 Loss 4.580797, Accuracy 89.136%\n",
      "Epoch 32, Batch 1018, LR 0.000008 Loss 4.581039, Accuracy 89.135%\n",
      "Epoch 32, Batch 1019, LR 0.000008 Loss 4.580616, Accuracy 89.138%\n",
      "Epoch 32, Batch 1020, LR 0.000008 Loss 4.580592, Accuracy 89.138%\n",
      "Epoch 32, Batch 1021, LR 0.000008 Loss 4.580379, Accuracy 89.141%\n",
      "Epoch 32, Batch 1022, LR 0.000008 Loss 4.579693, Accuracy 89.144%\n",
      "Epoch 32, Batch 1023, LR 0.000008 Loss 4.579343, Accuracy 89.145%\n",
      "Epoch 32, Batch 1024, LR 0.000008 Loss 4.579241, Accuracy 89.143%\n",
      "Epoch 32, Batch 1025, LR 0.000008 Loss 4.578908, Accuracy 89.142%\n",
      "Epoch 32, Batch 1026, LR 0.000008 Loss 4.578867, Accuracy 89.142%\n",
      "Epoch 32, Batch 1027, LR 0.000008 Loss 4.578908, Accuracy 89.142%\n",
      "Epoch 32, Batch 1028, LR 0.000008 Loss 4.579134, Accuracy 89.138%\n",
      "Epoch 32, Batch 1029, LR 0.000008 Loss 4.579551, Accuracy 89.135%\n",
      "Epoch 32, Batch 1030, LR 0.000008 Loss 4.578567, Accuracy 89.141%\n",
      "Epoch 32, Batch 1031, LR 0.000008 Loss 4.578482, Accuracy 89.141%\n",
      "Epoch 32, Batch 1032, LR 0.000008 Loss 4.577836, Accuracy 89.144%\n",
      "Epoch 32, Batch 1033, LR 0.000008 Loss 4.578246, Accuracy 89.140%\n",
      "Epoch 32, Batch 1034, LR 0.000008 Loss 4.577717, Accuracy 89.143%\n",
      "Epoch 32, Batch 1035, LR 0.000008 Loss 4.577659, Accuracy 89.146%\n",
      "Epoch 32, Batch 1036, LR 0.000008 Loss 4.577570, Accuracy 89.148%\n",
      "Epoch 32, Batch 1037, LR 0.000008 Loss 4.577560, Accuracy 89.146%\n",
      "Epoch 32, Batch 1038, LR 0.000008 Loss 4.576994, Accuracy 89.148%\n",
      "Epoch 32, Batch 1039, LR 0.000008 Loss 4.576857, Accuracy 89.150%\n",
      "Epoch 32, Batch 1040, LR 0.000008 Loss 4.576818, Accuracy 89.152%\n",
      "Epoch 32, Batch 1041, LR 0.000008 Loss 4.577026, Accuracy 89.151%\n",
      "Epoch 32, Batch 1042, LR 0.000008 Loss 4.576865, Accuracy 89.150%\n",
      "Epoch 32, Batch 1043, LR 0.000008 Loss 4.576356, Accuracy 89.155%\n",
      "Epoch 32, Batch 1044, LR 0.000008 Loss 4.575624, Accuracy 89.157%\n",
      "Epoch 32, Batch 1045, LR 0.000008 Loss 4.575449, Accuracy 89.158%\n",
      "Epoch 32, Batch 1046, LR 0.000008 Loss 4.575772, Accuracy 89.159%\n",
      "Epoch 32, Batch 1047, LR 0.000008 Loss 4.575247, Accuracy 89.160%\n",
      "Epoch 32, Loss (train set) 4.575247, Accuracy (train set) 89.160%\n",
      "Epoch 33, Batch 1, LR 0.000008 Loss 4.555364, Accuracy 90.625%\n",
      "Epoch 33, Batch 2, LR 0.000008 Loss 4.582975, Accuracy 89.453%\n",
      "Epoch 33, Batch 3, LR 0.000008 Loss 4.670048, Accuracy 89.844%\n",
      "Epoch 33, Batch 4, LR 0.000008 Loss 4.709315, Accuracy 89.648%\n",
      "Epoch 33, Batch 5, LR 0.000008 Loss 4.794403, Accuracy 88.750%\n",
      "Epoch 33, Batch 6, LR 0.000008 Loss 4.851888, Accuracy 87.891%\n",
      "Epoch 33, Batch 7, LR 0.000008 Loss 4.874507, Accuracy 87.277%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 8, LR 0.000008 Loss 4.911915, Accuracy 87.012%\n",
      "Epoch 33, Batch 9, LR 0.000008 Loss 4.911630, Accuracy 87.413%\n",
      "Epoch 33, Batch 10, LR 0.000008 Loss 4.837645, Accuracy 87.969%\n",
      "Epoch 33, Batch 11, LR 0.000008 Loss 4.795615, Accuracy 87.926%\n",
      "Epoch 33, Batch 12, LR 0.000008 Loss 4.793915, Accuracy 87.826%\n",
      "Epoch 33, Batch 13, LR 0.000008 Loss 4.833638, Accuracy 87.680%\n",
      "Epoch 33, Batch 14, LR 0.000008 Loss 4.791147, Accuracy 87.835%\n",
      "Epoch 33, Batch 15, LR 0.000008 Loss 4.792603, Accuracy 88.073%\n",
      "Epoch 33, Batch 16, LR 0.000008 Loss 4.758953, Accuracy 88.281%\n",
      "Epoch 33, Batch 17, LR 0.000008 Loss 4.750072, Accuracy 88.373%\n",
      "Epoch 33, Batch 18, LR 0.000008 Loss 4.726410, Accuracy 88.542%\n",
      "Epoch 33, Batch 19, LR 0.000008 Loss 4.712657, Accuracy 88.569%\n",
      "Epoch 33, Batch 20, LR 0.000008 Loss 4.736172, Accuracy 88.594%\n",
      "Epoch 33, Batch 21, LR 0.000008 Loss 4.688364, Accuracy 88.728%\n",
      "Epoch 33, Batch 22, LR 0.000008 Loss 4.647654, Accuracy 88.885%\n",
      "Epoch 33, Batch 23, LR 0.000008 Loss 4.663819, Accuracy 88.621%\n",
      "Epoch 33, Batch 24, LR 0.000008 Loss 4.641932, Accuracy 88.737%\n",
      "Epoch 33, Batch 25, LR 0.000008 Loss 4.643520, Accuracy 88.625%\n",
      "Epoch 33, Batch 26, LR 0.000008 Loss 4.623247, Accuracy 88.702%\n",
      "Epoch 33, Batch 27, LR 0.000008 Loss 4.622418, Accuracy 88.686%\n",
      "Epoch 33, Batch 28, LR 0.000008 Loss 4.607405, Accuracy 88.811%\n",
      "Epoch 33, Batch 29, LR 0.000008 Loss 4.601449, Accuracy 88.874%\n",
      "Epoch 33, Batch 30, LR 0.000008 Loss 4.576103, Accuracy 89.062%\n",
      "Epoch 33, Batch 31, LR 0.000008 Loss 4.588346, Accuracy 89.037%\n",
      "Epoch 33, Batch 32, LR 0.000008 Loss 4.628842, Accuracy 88.867%\n",
      "Epoch 33, Batch 33, LR 0.000008 Loss 4.642814, Accuracy 88.873%\n",
      "Epoch 33, Batch 34, LR 0.000008 Loss 4.633751, Accuracy 88.833%\n",
      "Epoch 33, Batch 35, LR 0.000008 Loss 4.615522, Accuracy 88.906%\n",
      "Epoch 33, Batch 36, LR 0.000008 Loss 4.623185, Accuracy 88.976%\n",
      "Epoch 33, Batch 37, LR 0.000008 Loss 4.627869, Accuracy 88.894%\n",
      "Epoch 33, Batch 38, LR 0.000008 Loss 4.651876, Accuracy 88.775%\n",
      "Epoch 33, Batch 39, LR 0.000008 Loss 4.645486, Accuracy 88.822%\n",
      "Epoch 33, Batch 40, LR 0.000008 Loss 4.655863, Accuracy 88.770%\n",
      "Epoch 33, Batch 41, LR 0.000008 Loss 4.689983, Accuracy 88.624%\n",
      "Epoch 33, Batch 42, LR 0.000008 Loss 4.674038, Accuracy 88.672%\n",
      "Epoch 33, Batch 43, LR 0.000008 Loss 4.655645, Accuracy 88.699%\n",
      "Epoch 33, Batch 44, LR 0.000008 Loss 4.647264, Accuracy 88.690%\n",
      "Epoch 33, Batch 45, LR 0.000008 Loss 4.647879, Accuracy 88.698%\n",
      "Epoch 33, Batch 46, LR 0.000008 Loss 4.631242, Accuracy 88.757%\n",
      "Epoch 33, Batch 47, LR 0.000008 Loss 4.614198, Accuracy 88.846%\n",
      "Epoch 33, Batch 48, LR 0.000008 Loss 4.611083, Accuracy 88.900%\n",
      "Epoch 33, Batch 49, LR 0.000008 Loss 4.616108, Accuracy 88.903%\n",
      "Epoch 33, Batch 50, LR 0.000008 Loss 4.629085, Accuracy 88.828%\n",
      "Epoch 33, Batch 51, LR 0.000008 Loss 4.628451, Accuracy 88.894%\n",
      "Epoch 33, Batch 52, LR 0.000008 Loss 4.627508, Accuracy 88.897%\n",
      "Epoch 33, Batch 53, LR 0.000008 Loss 4.629034, Accuracy 88.900%\n",
      "Epoch 33, Batch 54, LR 0.000008 Loss 4.638136, Accuracy 88.889%\n",
      "Epoch 33, Batch 55, LR 0.000008 Loss 4.633498, Accuracy 88.949%\n",
      "Epoch 33, Batch 56, LR 0.000008 Loss 4.627547, Accuracy 88.951%\n",
      "Epoch 33, Batch 57, LR 0.000008 Loss 4.629192, Accuracy 88.939%\n",
      "Epoch 33, Batch 58, LR 0.000008 Loss 4.623513, Accuracy 88.982%\n",
      "Epoch 33, Batch 59, LR 0.000008 Loss 4.620685, Accuracy 88.983%\n",
      "Epoch 33, Batch 60, LR 0.000008 Loss 4.631696, Accuracy 88.971%\n",
      "Epoch 33, Batch 61, LR 0.000008 Loss 4.631725, Accuracy 88.986%\n",
      "Epoch 33, Batch 62, LR 0.000008 Loss 4.624056, Accuracy 88.974%\n",
      "Epoch 33, Batch 63, LR 0.000008 Loss 4.618676, Accuracy 88.976%\n",
      "Epoch 33, Batch 64, LR 0.000008 Loss 4.615974, Accuracy 88.989%\n",
      "Epoch 33, Batch 65, LR 0.000008 Loss 4.608971, Accuracy 89.050%\n",
      "Epoch 33, Batch 66, LR 0.000008 Loss 4.610305, Accuracy 89.027%\n",
      "Epoch 33, Batch 67, LR 0.000008 Loss 4.611015, Accuracy 89.004%\n",
      "Epoch 33, Batch 68, LR 0.000008 Loss 4.610822, Accuracy 88.959%\n",
      "Epoch 33, Batch 69, LR 0.000008 Loss 4.612854, Accuracy 88.915%\n",
      "Epoch 33, Batch 70, LR 0.000008 Loss 4.601736, Accuracy 88.973%\n",
      "Epoch 33, Batch 71, LR 0.000008 Loss 4.598933, Accuracy 89.040%\n",
      "Epoch 33, Batch 72, LR 0.000008 Loss 4.605925, Accuracy 89.030%\n",
      "Epoch 33, Batch 73, LR 0.000008 Loss 4.601368, Accuracy 89.030%\n",
      "Epoch 33, Batch 74, LR 0.000008 Loss 4.588592, Accuracy 89.115%\n",
      "Epoch 33, Batch 75, LR 0.000008 Loss 4.590647, Accuracy 89.094%\n",
      "Epoch 33, Batch 76, LR 0.000008 Loss 4.588989, Accuracy 89.114%\n",
      "Epoch 33, Batch 77, LR 0.000008 Loss 4.583310, Accuracy 89.164%\n",
      "Epoch 33, Batch 78, LR 0.000008 Loss 4.582652, Accuracy 89.193%\n",
      "Epoch 33, Batch 79, LR 0.000008 Loss 4.581320, Accuracy 89.231%\n",
      "Epoch 33, Batch 80, LR 0.000008 Loss 4.582690, Accuracy 89.199%\n",
      "Epoch 33, Batch 81, LR 0.000008 Loss 4.584842, Accuracy 89.178%\n",
      "Epoch 33, Batch 82, LR 0.000008 Loss 4.585569, Accuracy 89.167%\n",
      "Epoch 33, Batch 83, LR 0.000008 Loss 4.583482, Accuracy 89.157%\n",
      "Epoch 33, Batch 84, LR 0.000008 Loss 4.585231, Accuracy 89.174%\n",
      "Epoch 33, Batch 85, LR 0.000008 Loss 4.576200, Accuracy 89.191%\n",
      "Epoch 33, Batch 86, LR 0.000008 Loss 4.577742, Accuracy 89.199%\n",
      "Epoch 33, Batch 87, LR 0.000008 Loss 4.583243, Accuracy 89.197%\n",
      "Epoch 33, Batch 88, LR 0.000008 Loss 4.585386, Accuracy 89.160%\n",
      "Epoch 33, Batch 89, LR 0.000008 Loss 4.596874, Accuracy 89.080%\n",
      "Epoch 33, Batch 90, LR 0.000008 Loss 4.594811, Accuracy 89.071%\n",
      "Epoch 33, Batch 91, LR 0.000008 Loss 4.612120, Accuracy 89.045%\n",
      "Epoch 33, Batch 92, LR 0.000008 Loss 4.609702, Accuracy 89.054%\n",
      "Epoch 33, Batch 93, LR 0.000008 Loss 4.617900, Accuracy 89.037%\n",
      "Epoch 33, Batch 94, LR 0.000008 Loss 4.614860, Accuracy 89.038%\n",
      "Epoch 33, Batch 95, LR 0.000008 Loss 4.622232, Accuracy 89.030%\n",
      "Epoch 33, Batch 96, LR 0.000008 Loss 4.623517, Accuracy 89.014%\n",
      "Epoch 33, Batch 97, LR 0.000008 Loss 4.614146, Accuracy 89.038%\n",
      "Epoch 33, Batch 98, LR 0.000008 Loss 4.614701, Accuracy 89.023%\n",
      "Epoch 33, Batch 99, LR 0.000008 Loss 4.610937, Accuracy 89.047%\n",
      "Epoch 33, Batch 100, LR 0.000008 Loss 4.612064, Accuracy 89.023%\n",
      "Epoch 33, Batch 101, LR 0.000008 Loss 4.606654, Accuracy 89.016%\n",
      "Epoch 33, Batch 102, LR 0.000008 Loss 4.600230, Accuracy 89.047%\n",
      "Epoch 33, Batch 103, LR 0.000008 Loss 4.597529, Accuracy 89.085%\n",
      "Epoch 33, Batch 104, LR 0.000008 Loss 4.592512, Accuracy 89.100%\n",
      "Epoch 33, Batch 105, LR 0.000008 Loss 4.591669, Accuracy 89.107%\n",
      "Epoch 33, Batch 106, LR 0.000008 Loss 4.594278, Accuracy 89.092%\n",
      "Epoch 33, Batch 107, LR 0.000008 Loss 4.591329, Accuracy 89.092%\n",
      "Epoch 33, Batch 108, LR 0.000008 Loss 4.591341, Accuracy 89.106%\n",
      "Epoch 33, Batch 109, LR 0.000008 Loss 4.590360, Accuracy 89.120%\n",
      "Epoch 33, Batch 110, LR 0.000008 Loss 4.592416, Accuracy 89.105%\n",
      "Epoch 33, Batch 111, LR 0.000008 Loss 4.586879, Accuracy 89.154%\n",
      "Epoch 33, Batch 112, LR 0.000008 Loss 4.582100, Accuracy 89.209%\n",
      "Epoch 33, Batch 113, LR 0.000008 Loss 4.578945, Accuracy 89.235%\n",
      "Epoch 33, Batch 114, LR 0.000008 Loss 4.575320, Accuracy 89.213%\n",
      "Epoch 33, Batch 115, LR 0.000008 Loss 4.572306, Accuracy 89.219%\n",
      "Epoch 33, Batch 116, LR 0.000008 Loss 4.570665, Accuracy 89.211%\n",
      "Epoch 33, Batch 117, LR 0.000008 Loss 4.571848, Accuracy 89.183%\n",
      "Epoch 33, Batch 118, LR 0.000008 Loss 4.573602, Accuracy 89.208%\n",
      "Epoch 33, Batch 119, LR 0.000008 Loss 4.573982, Accuracy 89.194%\n",
      "Epoch 33, Batch 120, LR 0.000008 Loss 4.581098, Accuracy 89.160%\n",
      "Epoch 33, Batch 121, LR 0.000008 Loss 4.578797, Accuracy 89.179%\n",
      "Epoch 33, Batch 122, LR 0.000008 Loss 4.574992, Accuracy 89.210%\n",
      "Epoch 33, Batch 123, LR 0.000008 Loss 4.582069, Accuracy 89.132%\n",
      "Epoch 33, Batch 124, LR 0.000008 Loss 4.582268, Accuracy 89.132%\n",
      "Epoch 33, Batch 125, LR 0.000008 Loss 4.579645, Accuracy 89.131%\n",
      "Epoch 33, Batch 126, LR 0.000008 Loss 4.583053, Accuracy 89.100%\n",
      "Epoch 33, Batch 127, LR 0.000008 Loss 4.583049, Accuracy 89.081%\n",
      "Epoch 33, Batch 128, LR 0.000008 Loss 4.586451, Accuracy 89.044%\n",
      "Epoch 33, Batch 129, LR 0.000008 Loss 4.589124, Accuracy 89.038%\n",
      "Epoch 33, Batch 130, LR 0.000008 Loss 4.588682, Accuracy 89.008%\n",
      "Epoch 33, Batch 131, LR 0.000008 Loss 4.588191, Accuracy 89.027%\n",
      "Epoch 33, Batch 132, LR 0.000008 Loss 4.587745, Accuracy 89.074%\n",
      "Epoch 33, Batch 133, LR 0.000008 Loss 4.581368, Accuracy 89.133%\n",
      "Epoch 33, Batch 134, LR 0.000008 Loss 4.583229, Accuracy 89.138%\n",
      "Epoch 33, Batch 135, LR 0.000008 Loss 4.586689, Accuracy 89.103%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 136, LR 0.000008 Loss 4.582548, Accuracy 89.114%\n",
      "Epoch 33, Batch 137, LR 0.000008 Loss 4.578751, Accuracy 89.137%\n",
      "Epoch 33, Batch 138, LR 0.000008 Loss 4.580895, Accuracy 89.136%\n",
      "Epoch 33, Batch 139, LR 0.000008 Loss 4.578623, Accuracy 89.152%\n",
      "Epoch 33, Batch 140, LR 0.000008 Loss 4.581413, Accuracy 89.124%\n",
      "Epoch 33, Batch 141, LR 0.000008 Loss 4.578239, Accuracy 89.146%\n",
      "Epoch 33, Batch 142, LR 0.000008 Loss 4.580238, Accuracy 89.134%\n",
      "Epoch 33, Batch 143, LR 0.000008 Loss 4.582645, Accuracy 89.134%\n",
      "Epoch 33, Batch 144, LR 0.000008 Loss 4.576794, Accuracy 89.149%\n",
      "Epoch 33, Batch 145, LR 0.000008 Loss 4.573177, Accuracy 89.170%\n",
      "Epoch 33, Batch 146, LR 0.000008 Loss 4.574499, Accuracy 89.153%\n",
      "Epoch 33, Batch 147, LR 0.000008 Loss 4.577385, Accuracy 89.153%\n",
      "Epoch 33, Batch 148, LR 0.000008 Loss 4.577000, Accuracy 89.147%\n",
      "Epoch 33, Batch 149, LR 0.000008 Loss 4.574663, Accuracy 89.162%\n",
      "Epoch 33, Batch 150, LR 0.000008 Loss 4.573448, Accuracy 89.161%\n",
      "Epoch 33, Batch 151, LR 0.000008 Loss 4.571685, Accuracy 89.202%\n",
      "Epoch 33, Batch 152, LR 0.000008 Loss 4.566445, Accuracy 89.232%\n",
      "Epoch 33, Batch 153, LR 0.000008 Loss 4.567563, Accuracy 89.231%\n",
      "Epoch 33, Batch 154, LR 0.000008 Loss 4.563977, Accuracy 89.230%\n",
      "Epoch 33, Batch 155, LR 0.000008 Loss 4.566246, Accuracy 89.204%\n",
      "Epoch 33, Batch 156, LR 0.000008 Loss 4.567143, Accuracy 89.203%\n",
      "Epoch 33, Batch 157, LR 0.000008 Loss 4.565584, Accuracy 89.222%\n",
      "Epoch 33, Batch 158, LR 0.000008 Loss 4.564758, Accuracy 89.231%\n",
      "Epoch 33, Batch 159, LR 0.000008 Loss 4.561257, Accuracy 89.259%\n",
      "Epoch 33, Batch 160, LR 0.000008 Loss 4.563097, Accuracy 89.238%\n",
      "Epoch 33, Batch 161, LR 0.000008 Loss 4.565440, Accuracy 89.257%\n",
      "Epoch 33, Batch 162, LR 0.000008 Loss 4.564574, Accuracy 89.260%\n",
      "Epoch 33, Batch 163, LR 0.000008 Loss 4.565584, Accuracy 89.269%\n",
      "Epoch 33, Batch 164, LR 0.000008 Loss 4.563464, Accuracy 89.272%\n",
      "Epoch 33, Batch 165, LR 0.000008 Loss 4.563947, Accuracy 89.266%\n",
      "Epoch 33, Batch 166, LR 0.000008 Loss 4.566791, Accuracy 89.270%\n",
      "Epoch 33, Batch 167, LR 0.000008 Loss 4.565383, Accuracy 89.268%\n",
      "Epoch 33, Batch 168, LR 0.000008 Loss 4.566191, Accuracy 89.244%\n",
      "Epoch 33, Batch 169, LR 0.000008 Loss 4.569324, Accuracy 89.243%\n",
      "Epoch 33, Batch 170, LR 0.000008 Loss 4.567537, Accuracy 89.251%\n",
      "Epoch 33, Batch 171, LR 0.000008 Loss 4.565305, Accuracy 89.259%\n",
      "Epoch 33, Batch 172, LR 0.000008 Loss 4.562360, Accuracy 89.276%\n",
      "Epoch 33, Batch 173, LR 0.000008 Loss 4.561724, Accuracy 89.288%\n",
      "Epoch 33, Batch 174, LR 0.000008 Loss 4.567620, Accuracy 89.265%\n",
      "Epoch 33, Batch 175, LR 0.000008 Loss 4.565196, Accuracy 89.263%\n",
      "Epoch 33, Batch 176, LR 0.000008 Loss 4.567377, Accuracy 89.262%\n",
      "Epoch 33, Batch 177, LR 0.000008 Loss 4.565229, Accuracy 89.283%\n",
      "Epoch 33, Batch 178, LR 0.000008 Loss 4.564326, Accuracy 89.291%\n",
      "Epoch 33, Batch 179, LR 0.000008 Loss 4.559460, Accuracy 89.316%\n",
      "Epoch 33, Batch 180, LR 0.000008 Loss 4.561043, Accuracy 89.314%\n",
      "Epoch 33, Batch 181, LR 0.000008 Loss 4.558940, Accuracy 89.313%\n",
      "Epoch 33, Batch 182, LR 0.000008 Loss 4.558296, Accuracy 89.324%\n",
      "Epoch 33, Batch 183, LR 0.000008 Loss 4.558222, Accuracy 89.323%\n",
      "Epoch 33, Batch 184, LR 0.000008 Loss 4.557176, Accuracy 89.334%\n",
      "Epoch 33, Batch 185, LR 0.000008 Loss 4.555706, Accuracy 89.329%\n",
      "Epoch 33, Batch 186, LR 0.000008 Loss 4.552502, Accuracy 89.336%\n",
      "Epoch 33, Batch 187, LR 0.000008 Loss 4.553501, Accuracy 89.322%\n",
      "Epoch 33, Batch 188, LR 0.000008 Loss 4.555277, Accuracy 89.316%\n",
      "Epoch 33, Batch 189, LR 0.000008 Loss 4.554072, Accuracy 89.319%\n",
      "Epoch 33, Batch 190, LR 0.000008 Loss 4.556615, Accuracy 89.305%\n",
      "Epoch 33, Batch 191, LR 0.000008 Loss 4.555134, Accuracy 89.308%\n",
      "Epoch 33, Batch 192, LR 0.000008 Loss 4.555416, Accuracy 89.290%\n",
      "Epoch 33, Batch 193, LR 0.000008 Loss 4.556186, Accuracy 89.297%\n",
      "Epoch 33, Batch 194, LR 0.000008 Loss 4.556042, Accuracy 89.300%\n",
      "Epoch 33, Batch 195, LR 0.000008 Loss 4.555218, Accuracy 89.299%\n",
      "Epoch 33, Batch 196, LR 0.000008 Loss 4.557313, Accuracy 89.298%\n",
      "Epoch 33, Batch 197, LR 0.000008 Loss 4.557723, Accuracy 89.277%\n",
      "Epoch 33, Batch 198, LR 0.000008 Loss 4.557814, Accuracy 89.276%\n",
      "Epoch 33, Batch 199, LR 0.000008 Loss 4.559119, Accuracy 89.251%\n",
      "Epoch 33, Batch 200, LR 0.000008 Loss 4.564341, Accuracy 89.230%\n",
      "Epoch 33, Batch 201, LR 0.000008 Loss 4.568270, Accuracy 89.214%\n",
      "Epoch 33, Batch 202, LR 0.000008 Loss 4.566782, Accuracy 89.221%\n",
      "Epoch 33, Batch 203, LR 0.000008 Loss 4.565683, Accuracy 89.236%\n",
      "Epoch 33, Batch 204, LR 0.000008 Loss 4.566514, Accuracy 89.231%\n",
      "Epoch 33, Batch 205, LR 0.000008 Loss 4.565120, Accuracy 89.249%\n",
      "Epoch 33, Batch 206, LR 0.000008 Loss 4.563543, Accuracy 89.279%\n",
      "Epoch 33, Batch 207, LR 0.000008 Loss 4.560262, Accuracy 89.300%\n",
      "Epoch 33, Batch 208, LR 0.000008 Loss 4.558364, Accuracy 89.314%\n",
      "Epoch 33, Batch 209, LR 0.000008 Loss 4.558493, Accuracy 89.332%\n",
      "Epoch 33, Batch 210, LR 0.000008 Loss 4.558531, Accuracy 89.330%\n",
      "Epoch 33, Batch 211, LR 0.000008 Loss 4.558371, Accuracy 89.336%\n",
      "Epoch 33, Batch 212, LR 0.000008 Loss 4.555685, Accuracy 89.346%\n",
      "Epoch 33, Batch 213, LR 0.000008 Loss 4.553811, Accuracy 89.338%\n",
      "Epoch 33, Batch 214, LR 0.000008 Loss 4.552833, Accuracy 89.344%\n",
      "Epoch 33, Batch 215, LR 0.000008 Loss 4.551470, Accuracy 89.357%\n",
      "Epoch 33, Batch 216, LR 0.000008 Loss 4.548824, Accuracy 89.366%\n",
      "Epoch 33, Batch 217, LR 0.000008 Loss 4.553346, Accuracy 89.336%\n",
      "Epoch 33, Batch 218, LR 0.000008 Loss 4.554324, Accuracy 89.335%\n",
      "Epoch 33, Batch 219, LR 0.000008 Loss 4.553071, Accuracy 89.337%\n",
      "Epoch 33, Batch 220, LR 0.000008 Loss 4.554282, Accuracy 89.329%\n",
      "Epoch 33, Batch 221, LR 0.000008 Loss 4.556493, Accuracy 89.306%\n",
      "Epoch 33, Batch 222, LR 0.000008 Loss 4.557126, Accuracy 89.309%\n",
      "Epoch 33, Batch 223, LR 0.000008 Loss 4.559441, Accuracy 89.297%\n",
      "Epoch 33, Batch 224, LR 0.000008 Loss 4.561473, Accuracy 89.275%\n",
      "Epoch 33, Batch 225, LR 0.000008 Loss 4.558874, Accuracy 89.292%\n",
      "Epoch 33, Batch 226, LR 0.000008 Loss 4.559228, Accuracy 89.277%\n",
      "Epoch 33, Batch 227, LR 0.000008 Loss 4.558681, Accuracy 89.276%\n",
      "Epoch 33, Batch 228, LR 0.000008 Loss 4.558699, Accuracy 89.272%\n",
      "Epoch 33, Batch 229, LR 0.000008 Loss 4.558040, Accuracy 89.277%\n",
      "Epoch 33, Batch 230, LR 0.000008 Loss 4.556459, Accuracy 89.276%\n",
      "Epoch 33, Batch 231, LR 0.000008 Loss 4.558016, Accuracy 89.276%\n",
      "Epoch 33, Batch 232, LR 0.000008 Loss 4.559718, Accuracy 89.261%\n",
      "Epoch 33, Batch 233, LR 0.000008 Loss 4.558844, Accuracy 89.267%\n",
      "Epoch 33, Batch 234, LR 0.000008 Loss 4.560603, Accuracy 89.256%\n",
      "Epoch 33, Batch 235, LR 0.000008 Loss 4.561800, Accuracy 89.255%\n",
      "Epoch 33, Batch 236, LR 0.000008 Loss 4.559699, Accuracy 89.261%\n",
      "Epoch 33, Batch 237, LR 0.000008 Loss 4.555708, Accuracy 89.267%\n",
      "Epoch 33, Batch 238, LR 0.000008 Loss 4.554982, Accuracy 89.266%\n",
      "Epoch 33, Batch 239, LR 0.000008 Loss 4.554014, Accuracy 89.272%\n",
      "Epoch 33, Batch 240, LR 0.000008 Loss 4.551643, Accuracy 89.268%\n",
      "Epoch 33, Batch 241, LR 0.000008 Loss 4.551880, Accuracy 89.267%\n",
      "Epoch 33, Batch 242, LR 0.000008 Loss 4.554076, Accuracy 89.269%\n",
      "Epoch 33, Batch 243, LR 0.000008 Loss 4.553131, Accuracy 89.271%\n",
      "Epoch 33, Batch 244, LR 0.000008 Loss 4.553584, Accuracy 89.258%\n",
      "Epoch 33, Batch 245, LR 0.000008 Loss 4.551927, Accuracy 89.270%\n",
      "Epoch 33, Batch 246, LR 0.000008 Loss 4.552286, Accuracy 89.269%\n",
      "Epoch 33, Batch 247, LR 0.000008 Loss 4.551692, Accuracy 89.290%\n",
      "Epoch 33, Batch 248, LR 0.000008 Loss 4.550823, Accuracy 89.289%\n",
      "Epoch 33, Batch 249, LR 0.000008 Loss 4.549687, Accuracy 89.295%\n",
      "Epoch 33, Batch 250, LR 0.000008 Loss 4.551976, Accuracy 89.266%\n",
      "Epoch 33, Batch 251, LR 0.000008 Loss 4.549845, Accuracy 89.277%\n",
      "Epoch 33, Batch 252, LR 0.000008 Loss 4.551056, Accuracy 89.270%\n",
      "Epoch 33, Batch 253, LR 0.000008 Loss 4.549075, Accuracy 89.285%\n",
      "Epoch 33, Batch 254, LR 0.000008 Loss 4.550347, Accuracy 89.275%\n",
      "Epoch 33, Batch 255, LR 0.000008 Loss 4.551528, Accuracy 89.271%\n",
      "Epoch 33, Batch 256, LR 0.000008 Loss 4.554375, Accuracy 89.255%\n",
      "Epoch 33, Batch 257, LR 0.000008 Loss 4.555937, Accuracy 89.251%\n",
      "Epoch 33, Batch 258, LR 0.000008 Loss 4.554729, Accuracy 89.262%\n",
      "Epoch 33, Batch 259, LR 0.000008 Loss 4.555783, Accuracy 89.253%\n",
      "Epoch 33, Batch 260, LR 0.000008 Loss 4.555706, Accuracy 89.240%\n",
      "Epoch 33, Batch 261, LR 0.000008 Loss 4.557633, Accuracy 89.221%\n",
      "Epoch 33, Batch 262, LR 0.000008 Loss 4.558680, Accuracy 89.218%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 263, LR 0.000008 Loss 4.559994, Accuracy 89.220%\n",
      "Epoch 33, Batch 264, LR 0.000008 Loss 4.563296, Accuracy 89.205%\n",
      "Epoch 33, Batch 265, LR 0.000008 Loss 4.563011, Accuracy 89.210%\n",
      "Epoch 33, Batch 266, LR 0.000008 Loss 4.561156, Accuracy 89.218%\n",
      "Epoch 33, Batch 267, LR 0.000008 Loss 4.562004, Accuracy 89.226%\n",
      "Epoch 33, Batch 268, LR 0.000008 Loss 4.560558, Accuracy 89.226%\n",
      "Epoch 33, Batch 269, LR 0.000008 Loss 4.559983, Accuracy 89.228%\n",
      "Epoch 33, Batch 270, LR 0.000008 Loss 4.560862, Accuracy 89.216%\n",
      "Epoch 33, Batch 271, LR 0.000008 Loss 4.559165, Accuracy 89.233%\n",
      "Epoch 33, Batch 272, LR 0.000008 Loss 4.559359, Accuracy 89.229%\n",
      "Epoch 33, Batch 273, LR 0.000008 Loss 4.560040, Accuracy 89.220%\n",
      "Epoch 33, Batch 274, LR 0.000008 Loss 4.561015, Accuracy 89.211%\n",
      "Epoch 33, Batch 275, LR 0.000008 Loss 4.562197, Accuracy 89.207%\n",
      "Epoch 33, Batch 276, LR 0.000008 Loss 4.562094, Accuracy 89.213%\n",
      "Epoch 33, Batch 277, LR 0.000008 Loss 4.561807, Accuracy 89.215%\n",
      "Epoch 33, Batch 278, LR 0.000008 Loss 4.562838, Accuracy 89.214%\n",
      "Epoch 33, Batch 279, LR 0.000008 Loss 4.562058, Accuracy 89.214%\n",
      "Epoch 33, Batch 280, LR 0.000008 Loss 4.559553, Accuracy 89.222%\n",
      "Epoch 33, Batch 281, LR 0.000008 Loss 4.557700, Accuracy 89.224%\n",
      "Epoch 33, Batch 282, LR 0.000008 Loss 4.558780, Accuracy 89.220%\n",
      "Epoch 33, Batch 283, LR 0.000008 Loss 4.556595, Accuracy 89.228%\n",
      "Epoch 33, Batch 284, LR 0.000008 Loss 4.556010, Accuracy 89.236%\n",
      "Epoch 33, Batch 285, LR 0.000008 Loss 4.557209, Accuracy 89.227%\n",
      "Epoch 33, Batch 286, LR 0.000008 Loss 4.560256, Accuracy 89.199%\n",
      "Epoch 33, Batch 287, LR 0.000008 Loss 4.560178, Accuracy 89.199%\n",
      "Epoch 33, Batch 288, LR 0.000008 Loss 4.561968, Accuracy 89.195%\n",
      "Epoch 33, Batch 289, LR 0.000008 Loss 4.563917, Accuracy 89.181%\n",
      "Epoch 33, Batch 290, LR 0.000008 Loss 4.565067, Accuracy 89.192%\n",
      "Epoch 33, Batch 291, LR 0.000008 Loss 4.564433, Accuracy 89.194%\n",
      "Epoch 33, Batch 292, LR 0.000008 Loss 4.567110, Accuracy 89.183%\n",
      "Epoch 33, Batch 293, LR 0.000008 Loss 4.564568, Accuracy 89.193%\n",
      "Epoch 33, Batch 294, LR 0.000008 Loss 4.565367, Accuracy 89.195%\n",
      "Epoch 33, Batch 295, LR 0.000008 Loss 4.564288, Accuracy 89.192%\n",
      "Epoch 33, Batch 296, LR 0.000008 Loss 4.562372, Accuracy 89.200%\n",
      "Epoch 33, Batch 297, LR 0.000008 Loss 4.561208, Accuracy 89.202%\n",
      "Epoch 33, Batch 298, LR 0.000008 Loss 4.559741, Accuracy 89.225%\n",
      "Epoch 33, Batch 299, LR 0.000008 Loss 4.558382, Accuracy 89.240%\n",
      "Epoch 33, Batch 300, LR 0.000008 Loss 4.559399, Accuracy 89.237%\n",
      "Epoch 33, Batch 301, LR 0.000008 Loss 4.558911, Accuracy 89.239%\n",
      "Epoch 33, Batch 302, LR 0.000008 Loss 4.559814, Accuracy 89.236%\n",
      "Epoch 33, Batch 303, LR 0.000008 Loss 4.560618, Accuracy 89.235%\n",
      "Epoch 33, Batch 304, LR 0.000007 Loss 4.560880, Accuracy 89.237%\n",
      "Epoch 33, Batch 305, LR 0.000007 Loss 4.560893, Accuracy 89.234%\n",
      "Epoch 33, Batch 306, LR 0.000007 Loss 4.559173, Accuracy 89.249%\n",
      "Epoch 33, Batch 307, LR 0.000007 Loss 4.557549, Accuracy 89.258%\n",
      "Epoch 33, Batch 308, LR 0.000007 Loss 4.557391, Accuracy 89.263%\n",
      "Epoch 33, Batch 309, LR 0.000007 Loss 4.559116, Accuracy 89.247%\n",
      "Epoch 33, Batch 310, LR 0.000007 Loss 4.557879, Accuracy 89.252%\n",
      "Epoch 33, Batch 311, LR 0.000007 Loss 4.556082, Accuracy 89.253%\n",
      "Epoch 33, Batch 312, LR 0.000007 Loss 4.556485, Accuracy 89.260%\n",
      "Epoch 33, Batch 313, LR 0.000007 Loss 4.556890, Accuracy 89.250%\n",
      "Epoch 33, Batch 314, LR 0.000007 Loss 4.555907, Accuracy 89.254%\n",
      "Epoch 33, Batch 315, LR 0.000007 Loss 4.556614, Accuracy 89.246%\n",
      "Epoch 33, Batch 316, LR 0.000007 Loss 4.557290, Accuracy 89.245%\n",
      "Epoch 33, Batch 317, LR 0.000007 Loss 4.556960, Accuracy 89.242%\n",
      "Epoch 33, Batch 318, LR 0.000007 Loss 4.554325, Accuracy 89.259%\n",
      "Epoch 33, Batch 319, LR 0.000007 Loss 4.555879, Accuracy 89.251%\n",
      "Epoch 33, Batch 320, LR 0.000007 Loss 4.556951, Accuracy 89.253%\n",
      "Epoch 33, Batch 321, LR 0.000007 Loss 4.557612, Accuracy 89.257%\n",
      "Epoch 33, Batch 322, LR 0.000007 Loss 4.557784, Accuracy 89.259%\n",
      "Epoch 33, Batch 323, LR 0.000007 Loss 4.556957, Accuracy 89.268%\n",
      "Epoch 33, Batch 324, LR 0.000007 Loss 4.555130, Accuracy 89.277%\n",
      "Epoch 33, Batch 325, LR 0.000007 Loss 4.555460, Accuracy 89.267%\n",
      "Epoch 33, Batch 326, LR 0.000007 Loss 4.556706, Accuracy 89.257%\n",
      "Epoch 33, Batch 327, LR 0.000007 Loss 4.557115, Accuracy 89.254%\n",
      "Epoch 33, Batch 328, LR 0.000007 Loss 4.557334, Accuracy 89.246%\n",
      "Epoch 33, Batch 329, LR 0.000007 Loss 4.557286, Accuracy 89.243%\n",
      "Epoch 33, Batch 330, LR 0.000007 Loss 4.556944, Accuracy 89.245%\n",
      "Epoch 33, Batch 331, LR 0.000007 Loss 4.555760, Accuracy 89.268%\n",
      "Epoch 33, Batch 332, LR 0.000007 Loss 4.553625, Accuracy 89.279%\n",
      "Epoch 33, Batch 333, LR 0.000007 Loss 4.553027, Accuracy 89.288%\n",
      "Epoch 33, Batch 334, LR 0.000007 Loss 4.554021, Accuracy 89.285%\n",
      "Epoch 33, Batch 335, LR 0.000007 Loss 4.553405, Accuracy 89.282%\n",
      "Epoch 33, Batch 336, LR 0.000007 Loss 4.553157, Accuracy 89.283%\n",
      "Epoch 33, Batch 337, LR 0.000007 Loss 4.552729, Accuracy 89.299%\n",
      "Epoch 33, Batch 338, LR 0.000007 Loss 4.552334, Accuracy 89.305%\n",
      "Epoch 33, Batch 339, LR 0.000007 Loss 4.552701, Accuracy 89.300%\n",
      "Epoch 33, Batch 340, LR 0.000007 Loss 4.554380, Accuracy 89.295%\n",
      "Epoch 33, Batch 341, LR 0.000007 Loss 4.556280, Accuracy 89.276%\n",
      "Epoch 33, Batch 342, LR 0.000007 Loss 4.556837, Accuracy 89.275%\n",
      "Epoch 33, Batch 343, LR 0.000007 Loss 4.556460, Accuracy 89.281%\n",
      "Epoch 33, Batch 344, LR 0.000007 Loss 4.555422, Accuracy 89.292%\n",
      "Epoch 33, Batch 345, LR 0.000007 Loss 4.555417, Accuracy 89.296%\n",
      "Epoch 33, Batch 346, LR 0.000007 Loss 4.555448, Accuracy 89.295%\n",
      "Epoch 33, Batch 347, LR 0.000007 Loss 4.555808, Accuracy 89.301%\n",
      "Epoch 33, Batch 348, LR 0.000007 Loss 4.554655, Accuracy 89.314%\n",
      "Epoch 33, Batch 349, LR 0.000007 Loss 4.553363, Accuracy 89.320%\n",
      "Epoch 33, Batch 350, LR 0.000007 Loss 4.553560, Accuracy 89.319%\n",
      "Epoch 33, Batch 351, LR 0.000007 Loss 4.552625, Accuracy 89.327%\n",
      "Epoch 33, Batch 352, LR 0.000007 Loss 4.552604, Accuracy 89.327%\n",
      "Epoch 33, Batch 353, LR 0.000007 Loss 4.552358, Accuracy 89.330%\n",
      "Epoch 33, Batch 354, LR 0.000007 Loss 4.552020, Accuracy 89.332%\n",
      "Epoch 33, Batch 355, LR 0.000007 Loss 4.549544, Accuracy 89.340%\n",
      "Epoch 33, Batch 356, LR 0.000007 Loss 4.549943, Accuracy 89.332%\n",
      "Epoch 33, Batch 357, LR 0.000007 Loss 4.550627, Accuracy 89.325%\n",
      "Epoch 33, Batch 358, LR 0.000007 Loss 4.548712, Accuracy 89.327%\n",
      "Epoch 33, Batch 359, LR 0.000007 Loss 4.549247, Accuracy 89.308%\n",
      "Epoch 33, Batch 360, LR 0.000007 Loss 4.545731, Accuracy 89.316%\n",
      "Epoch 33, Batch 361, LR 0.000007 Loss 4.544782, Accuracy 89.318%\n",
      "Epoch 33, Batch 362, LR 0.000007 Loss 4.544417, Accuracy 89.317%\n",
      "Epoch 33, Batch 363, LR 0.000007 Loss 4.544253, Accuracy 89.316%\n",
      "Epoch 33, Batch 364, LR 0.000007 Loss 4.545035, Accuracy 89.318%\n",
      "Epoch 33, Batch 365, LR 0.000007 Loss 4.546452, Accuracy 89.311%\n",
      "Epoch 33, Batch 366, LR 0.000007 Loss 4.546059, Accuracy 89.317%\n",
      "Epoch 33, Batch 367, LR 0.000007 Loss 4.546045, Accuracy 89.312%\n",
      "Epoch 33, Batch 368, LR 0.000007 Loss 4.548610, Accuracy 89.300%\n",
      "Epoch 33, Batch 369, LR 0.000007 Loss 4.547152, Accuracy 89.304%\n",
      "Epoch 33, Batch 370, LR 0.000007 Loss 4.548536, Accuracy 89.299%\n",
      "Epoch 33, Batch 371, LR 0.000007 Loss 4.548632, Accuracy 89.292%\n",
      "Epoch 33, Batch 372, LR 0.000007 Loss 4.548431, Accuracy 89.294%\n",
      "Epoch 33, Batch 373, LR 0.000007 Loss 4.549074, Accuracy 89.295%\n",
      "Epoch 33, Batch 374, LR 0.000007 Loss 4.550647, Accuracy 89.286%\n",
      "Epoch 33, Batch 375, LR 0.000007 Loss 4.551747, Accuracy 89.283%\n",
      "Epoch 33, Batch 376, LR 0.000007 Loss 4.553000, Accuracy 89.283%\n",
      "Epoch 33, Batch 377, LR 0.000007 Loss 4.552681, Accuracy 89.293%\n",
      "Epoch 33, Batch 378, LR 0.000007 Loss 4.553632, Accuracy 89.296%\n",
      "Epoch 33, Batch 379, LR 0.000007 Loss 4.553802, Accuracy 89.295%\n",
      "Epoch 33, Batch 380, LR 0.000007 Loss 4.552987, Accuracy 89.297%\n",
      "Epoch 33, Batch 381, LR 0.000007 Loss 4.554189, Accuracy 89.292%\n",
      "Epoch 33, Batch 382, LR 0.000007 Loss 4.553697, Accuracy 89.287%\n",
      "Epoch 33, Batch 383, LR 0.000007 Loss 4.554229, Accuracy 89.287%\n",
      "Epoch 33, Batch 384, LR 0.000007 Loss 4.554631, Accuracy 89.288%\n",
      "Epoch 33, Batch 385, LR 0.000007 Loss 4.554510, Accuracy 89.292%\n",
      "Epoch 33, Batch 386, LR 0.000007 Loss 4.555148, Accuracy 89.293%\n",
      "Epoch 33, Batch 387, LR 0.000007 Loss 4.555315, Accuracy 89.293%\n",
      "Epoch 33, Batch 388, LR 0.000007 Loss 4.556981, Accuracy 89.282%\n",
      "Epoch 33, Batch 389, LR 0.000007 Loss 4.558288, Accuracy 89.281%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 390, LR 0.000007 Loss 4.557821, Accuracy 89.289%\n",
      "Epoch 33, Batch 391, LR 0.000007 Loss 4.557884, Accuracy 89.292%\n",
      "Epoch 33, Batch 392, LR 0.000007 Loss 4.557886, Accuracy 89.296%\n",
      "Epoch 33, Batch 393, LR 0.000007 Loss 4.556817, Accuracy 89.311%\n",
      "Epoch 33, Batch 394, LR 0.000007 Loss 4.557622, Accuracy 89.310%\n",
      "Epoch 33, Batch 395, LR 0.000007 Loss 4.557877, Accuracy 89.302%\n",
      "Epoch 33, Batch 396, LR 0.000007 Loss 4.560313, Accuracy 89.289%\n",
      "Epoch 33, Batch 397, LR 0.000007 Loss 4.560849, Accuracy 89.289%\n",
      "Epoch 33, Batch 398, LR 0.000007 Loss 4.560066, Accuracy 89.286%\n",
      "Epoch 33, Batch 399, LR 0.000007 Loss 4.560750, Accuracy 89.284%\n",
      "Epoch 33, Batch 400, LR 0.000007 Loss 4.560247, Accuracy 89.287%\n",
      "Epoch 33, Batch 401, LR 0.000007 Loss 4.560533, Accuracy 89.288%\n",
      "Epoch 33, Batch 402, LR 0.000007 Loss 4.559932, Accuracy 89.288%\n",
      "Epoch 33, Batch 403, LR 0.000007 Loss 4.560284, Accuracy 89.282%\n",
      "Epoch 33, Batch 404, LR 0.000007 Loss 4.559866, Accuracy 89.285%\n",
      "Epoch 33, Batch 405, LR 0.000007 Loss 4.558916, Accuracy 89.296%\n",
      "Epoch 33, Batch 406, LR 0.000007 Loss 4.558091, Accuracy 89.299%\n",
      "Epoch 33, Batch 407, LR 0.000007 Loss 4.556664, Accuracy 89.299%\n",
      "Epoch 33, Batch 408, LR 0.000007 Loss 4.556873, Accuracy 89.298%\n",
      "Epoch 33, Batch 409, LR 0.000007 Loss 4.556384, Accuracy 89.299%\n",
      "Epoch 33, Batch 410, LR 0.000007 Loss 4.556687, Accuracy 89.295%\n",
      "Epoch 33, Batch 411, LR 0.000007 Loss 4.557356, Accuracy 89.293%\n",
      "Epoch 33, Batch 412, LR 0.000007 Loss 4.556533, Accuracy 89.290%\n",
      "Epoch 33, Batch 413, LR 0.000007 Loss 4.556234, Accuracy 89.299%\n",
      "Epoch 33, Batch 414, LR 0.000007 Loss 4.555609, Accuracy 89.293%\n",
      "Epoch 33, Batch 415, LR 0.000007 Loss 4.554920, Accuracy 89.288%\n",
      "Epoch 33, Batch 416, LR 0.000007 Loss 4.557102, Accuracy 89.284%\n",
      "Epoch 33, Batch 417, LR 0.000007 Loss 4.557117, Accuracy 89.282%\n",
      "Epoch 33, Batch 418, LR 0.000007 Loss 4.557588, Accuracy 89.285%\n",
      "Epoch 33, Batch 419, LR 0.000007 Loss 4.556963, Accuracy 89.288%\n",
      "Epoch 33, Batch 420, LR 0.000007 Loss 4.555060, Accuracy 89.295%\n",
      "Epoch 33, Batch 421, LR 0.000007 Loss 4.555043, Accuracy 89.300%\n",
      "Epoch 33, Batch 422, LR 0.000007 Loss 4.556511, Accuracy 89.298%\n",
      "Epoch 33, Batch 423, LR 0.000007 Loss 4.555857, Accuracy 89.308%\n",
      "Epoch 33, Batch 424, LR 0.000007 Loss 4.555415, Accuracy 89.308%\n",
      "Epoch 33, Batch 425, LR 0.000007 Loss 4.554773, Accuracy 89.307%\n",
      "Epoch 33, Batch 426, LR 0.000007 Loss 4.556798, Accuracy 89.305%\n",
      "Epoch 33, Batch 427, LR 0.000007 Loss 4.556863, Accuracy 89.308%\n",
      "Epoch 33, Batch 428, LR 0.000007 Loss 4.556287, Accuracy 89.316%\n",
      "Epoch 33, Batch 429, LR 0.000007 Loss 4.556402, Accuracy 89.319%\n",
      "Epoch 33, Batch 430, LR 0.000007 Loss 4.554739, Accuracy 89.331%\n",
      "Epoch 33, Batch 431, LR 0.000007 Loss 4.554351, Accuracy 89.334%\n",
      "Epoch 33, Batch 432, LR 0.000007 Loss 4.554914, Accuracy 89.323%\n",
      "Epoch 33, Batch 433, LR 0.000007 Loss 4.555630, Accuracy 89.317%\n",
      "Epoch 33, Batch 434, LR 0.000007 Loss 4.555955, Accuracy 89.318%\n",
      "Epoch 33, Batch 435, LR 0.000007 Loss 4.555378, Accuracy 89.314%\n",
      "Epoch 33, Batch 436, LR 0.000007 Loss 4.556284, Accuracy 89.310%\n",
      "Epoch 33, Batch 437, LR 0.000007 Loss 4.554948, Accuracy 89.315%\n",
      "Epoch 33, Batch 438, LR 0.000007 Loss 4.555183, Accuracy 89.318%\n",
      "Epoch 33, Batch 439, LR 0.000007 Loss 4.556082, Accuracy 89.315%\n",
      "Epoch 33, Batch 440, LR 0.000007 Loss 4.554679, Accuracy 89.325%\n",
      "Epoch 33, Batch 441, LR 0.000007 Loss 4.554284, Accuracy 89.325%\n",
      "Epoch 33, Batch 442, LR 0.000007 Loss 4.554396, Accuracy 89.324%\n",
      "Epoch 33, Batch 443, LR 0.000007 Loss 4.553108, Accuracy 89.324%\n",
      "Epoch 33, Batch 444, LR 0.000007 Loss 4.552821, Accuracy 89.316%\n",
      "Epoch 33, Batch 445, LR 0.000007 Loss 4.552391, Accuracy 89.307%\n",
      "Epoch 33, Batch 446, LR 0.000007 Loss 4.551560, Accuracy 89.308%\n",
      "Epoch 33, Batch 447, LR 0.000007 Loss 4.551652, Accuracy 89.307%\n",
      "Epoch 33, Batch 448, LR 0.000007 Loss 4.549673, Accuracy 89.314%\n",
      "Epoch 33, Batch 449, LR 0.000007 Loss 4.550368, Accuracy 89.313%\n",
      "Epoch 33, Batch 450, LR 0.000007 Loss 4.548701, Accuracy 89.323%\n",
      "Epoch 33, Batch 451, LR 0.000007 Loss 4.549533, Accuracy 89.322%\n",
      "Epoch 33, Batch 452, LR 0.000007 Loss 4.550526, Accuracy 89.325%\n",
      "Epoch 33, Batch 453, LR 0.000007 Loss 4.550203, Accuracy 89.323%\n",
      "Epoch 33, Batch 454, LR 0.000007 Loss 4.550916, Accuracy 89.321%\n",
      "Epoch 33, Batch 455, LR 0.000007 Loss 4.553075, Accuracy 89.310%\n",
      "Epoch 33, Batch 456, LR 0.000007 Loss 4.552799, Accuracy 89.314%\n",
      "Epoch 33, Batch 457, LR 0.000007 Loss 4.552991, Accuracy 89.314%\n",
      "Epoch 33, Batch 458, LR 0.000007 Loss 4.552536, Accuracy 89.320%\n",
      "Epoch 33, Batch 459, LR 0.000007 Loss 4.550849, Accuracy 89.326%\n",
      "Epoch 33, Batch 460, LR 0.000007 Loss 4.550019, Accuracy 89.331%\n",
      "Epoch 33, Batch 461, LR 0.000007 Loss 4.550267, Accuracy 89.332%\n",
      "Epoch 33, Batch 462, LR 0.000007 Loss 4.549832, Accuracy 89.335%\n",
      "Epoch 33, Batch 463, LR 0.000007 Loss 4.549334, Accuracy 89.334%\n",
      "Epoch 33, Batch 464, LR 0.000007 Loss 4.549551, Accuracy 89.340%\n",
      "Epoch 33, Batch 465, LR 0.000007 Loss 4.547998, Accuracy 89.350%\n",
      "Epoch 33, Batch 466, LR 0.000007 Loss 4.546333, Accuracy 89.354%\n",
      "Epoch 33, Batch 467, LR 0.000007 Loss 4.547194, Accuracy 89.350%\n",
      "Epoch 33, Batch 468, LR 0.000007 Loss 4.546749, Accuracy 89.353%\n",
      "Epoch 33, Batch 469, LR 0.000007 Loss 4.546483, Accuracy 89.354%\n",
      "Epoch 33, Batch 470, LR 0.000007 Loss 4.546096, Accuracy 89.353%\n",
      "Epoch 33, Batch 471, LR 0.000007 Loss 4.545639, Accuracy 89.353%\n",
      "Epoch 33, Batch 472, LR 0.000007 Loss 4.546898, Accuracy 89.349%\n",
      "Epoch 33, Batch 473, LR 0.000007 Loss 4.546668, Accuracy 89.345%\n",
      "Epoch 33, Batch 474, LR 0.000007 Loss 4.545413, Accuracy 89.348%\n",
      "Epoch 33, Batch 475, LR 0.000007 Loss 4.544026, Accuracy 89.354%\n",
      "Epoch 33, Batch 476, LR 0.000007 Loss 4.543738, Accuracy 89.360%\n",
      "Epoch 33, Batch 477, LR 0.000007 Loss 4.542400, Accuracy 89.370%\n",
      "Epoch 33, Batch 478, LR 0.000007 Loss 4.542694, Accuracy 89.371%\n",
      "Epoch 33, Batch 479, LR 0.000007 Loss 4.542601, Accuracy 89.367%\n",
      "Epoch 33, Batch 480, LR 0.000007 Loss 4.542372, Accuracy 89.364%\n",
      "Epoch 33, Batch 481, LR 0.000007 Loss 4.543134, Accuracy 89.361%\n",
      "Epoch 33, Batch 482, LR 0.000007 Loss 4.543682, Accuracy 89.364%\n",
      "Epoch 33, Batch 483, LR 0.000007 Loss 4.543310, Accuracy 89.365%\n",
      "Epoch 33, Batch 484, LR 0.000007 Loss 4.543798, Accuracy 89.355%\n",
      "Epoch 33, Batch 485, LR 0.000007 Loss 4.544503, Accuracy 89.352%\n",
      "Epoch 33, Batch 486, LR 0.000007 Loss 4.545592, Accuracy 89.350%\n",
      "Epoch 33, Batch 487, LR 0.000007 Loss 4.545041, Accuracy 89.346%\n",
      "Epoch 33, Batch 488, LR 0.000007 Loss 4.545628, Accuracy 89.338%\n",
      "Epoch 33, Batch 489, LR 0.000007 Loss 4.546167, Accuracy 89.340%\n",
      "Epoch 33, Batch 490, LR 0.000007 Loss 4.547389, Accuracy 89.337%\n",
      "Epoch 33, Batch 491, LR 0.000007 Loss 4.546809, Accuracy 89.339%\n",
      "Epoch 33, Batch 492, LR 0.000007 Loss 4.546195, Accuracy 89.344%\n",
      "Epoch 33, Batch 493, LR 0.000007 Loss 4.546237, Accuracy 89.337%\n",
      "Epoch 33, Batch 494, LR 0.000007 Loss 4.547524, Accuracy 89.330%\n",
      "Epoch 33, Batch 495, LR 0.000007 Loss 4.547758, Accuracy 89.328%\n",
      "Epoch 33, Batch 496, LR 0.000007 Loss 4.545927, Accuracy 89.340%\n",
      "Epoch 33, Batch 497, LR 0.000007 Loss 4.547145, Accuracy 89.334%\n",
      "Epoch 33, Batch 498, LR 0.000007 Loss 4.547653, Accuracy 89.326%\n",
      "Epoch 33, Batch 499, LR 0.000007 Loss 4.546414, Accuracy 89.327%\n",
      "Epoch 33, Batch 500, LR 0.000007 Loss 4.546809, Accuracy 89.331%\n",
      "Epoch 33, Batch 501, LR 0.000007 Loss 4.547104, Accuracy 89.331%\n",
      "Epoch 33, Batch 502, LR 0.000007 Loss 4.548012, Accuracy 89.322%\n",
      "Epoch 33, Batch 503, LR 0.000007 Loss 4.546580, Accuracy 89.323%\n",
      "Epoch 33, Batch 504, LR 0.000007 Loss 4.545608, Accuracy 89.324%\n",
      "Epoch 33, Batch 505, LR 0.000007 Loss 4.544526, Accuracy 89.329%\n",
      "Epoch 33, Batch 506, LR 0.000007 Loss 4.544269, Accuracy 89.325%\n",
      "Epoch 33, Batch 507, LR 0.000007 Loss 4.543724, Accuracy 89.324%\n",
      "Epoch 33, Batch 508, LR 0.000007 Loss 4.543567, Accuracy 89.321%\n",
      "Epoch 33, Batch 509, LR 0.000007 Loss 4.543105, Accuracy 89.326%\n",
      "Epoch 33, Batch 510, LR 0.000007 Loss 4.544098, Accuracy 89.317%\n",
      "Epoch 33, Batch 511, LR 0.000007 Loss 4.543988, Accuracy 89.318%\n",
      "Epoch 33, Batch 512, LR 0.000007 Loss 4.543846, Accuracy 89.317%\n",
      "Epoch 33, Batch 513, LR 0.000007 Loss 4.543750, Accuracy 89.318%\n",
      "Epoch 33, Batch 514, LR 0.000007 Loss 4.543662, Accuracy 89.319%\n",
      "Epoch 33, Batch 515, LR 0.000007 Loss 4.544086, Accuracy 89.317%\n",
      "Epoch 33, Batch 516, LR 0.000007 Loss 4.544998, Accuracy 89.312%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 517, LR 0.000007 Loss 4.545386, Accuracy 89.318%\n",
      "Epoch 33, Batch 518, LR 0.000007 Loss 4.544572, Accuracy 89.322%\n",
      "Epoch 33, Batch 519, LR 0.000007 Loss 4.545513, Accuracy 89.320%\n",
      "Epoch 33, Batch 520, LR 0.000007 Loss 4.544829, Accuracy 89.321%\n",
      "Epoch 33, Batch 521, LR 0.000007 Loss 4.543982, Accuracy 89.319%\n",
      "Epoch 33, Batch 522, LR 0.000007 Loss 4.543782, Accuracy 89.317%\n",
      "Epoch 33, Batch 523, LR 0.000007 Loss 4.544304, Accuracy 89.312%\n",
      "Epoch 33, Batch 524, LR 0.000007 Loss 4.543561, Accuracy 89.311%\n",
      "Epoch 33, Batch 525, LR 0.000007 Loss 4.544309, Accuracy 89.308%\n",
      "Epoch 33, Batch 526, LR 0.000007 Loss 4.543076, Accuracy 89.309%\n",
      "Epoch 33, Batch 527, LR 0.000007 Loss 4.542892, Accuracy 89.298%\n",
      "Epoch 33, Batch 528, LR 0.000007 Loss 4.542945, Accuracy 89.299%\n",
      "Epoch 33, Batch 529, LR 0.000007 Loss 4.542708, Accuracy 89.300%\n",
      "Epoch 33, Batch 530, LR 0.000007 Loss 4.542470, Accuracy 89.301%\n",
      "Epoch 33, Batch 531, LR 0.000007 Loss 4.542984, Accuracy 89.298%\n",
      "Epoch 33, Batch 532, LR 0.000007 Loss 4.542877, Accuracy 89.299%\n",
      "Epoch 33, Batch 533, LR 0.000007 Loss 4.541072, Accuracy 89.304%\n",
      "Epoch 33, Batch 534, LR 0.000007 Loss 4.541691, Accuracy 89.305%\n",
      "Epoch 33, Batch 535, LR 0.000007 Loss 4.542640, Accuracy 89.309%\n",
      "Epoch 33, Batch 536, LR 0.000007 Loss 4.542426, Accuracy 89.312%\n",
      "Epoch 33, Batch 537, LR 0.000007 Loss 4.541580, Accuracy 89.320%\n",
      "Epoch 33, Batch 538, LR 0.000007 Loss 4.541264, Accuracy 89.315%\n",
      "Epoch 33, Batch 539, LR 0.000007 Loss 4.541020, Accuracy 89.312%\n",
      "Epoch 33, Batch 540, LR 0.000007 Loss 4.540975, Accuracy 89.316%\n",
      "Epoch 33, Batch 541, LR 0.000007 Loss 4.540152, Accuracy 89.312%\n",
      "Epoch 33, Batch 542, LR 0.000007 Loss 4.540651, Accuracy 89.313%\n",
      "Epoch 33, Batch 543, LR 0.000007 Loss 4.541468, Accuracy 89.311%\n",
      "Epoch 33, Batch 544, LR 0.000007 Loss 4.541204, Accuracy 89.317%\n",
      "Epoch 33, Batch 545, LR 0.000007 Loss 4.541413, Accuracy 89.322%\n",
      "Epoch 33, Batch 546, LR 0.000007 Loss 4.541121, Accuracy 89.326%\n",
      "Epoch 33, Batch 547, LR 0.000007 Loss 4.540851, Accuracy 89.328%\n",
      "Epoch 33, Batch 548, LR 0.000007 Loss 4.540272, Accuracy 89.333%\n",
      "Epoch 33, Batch 549, LR 0.000007 Loss 4.541151, Accuracy 89.331%\n",
      "Epoch 33, Batch 550, LR 0.000007 Loss 4.541916, Accuracy 89.328%\n",
      "Epoch 33, Batch 551, LR 0.000007 Loss 4.541998, Accuracy 89.329%\n",
      "Epoch 33, Batch 552, LR 0.000007 Loss 4.542049, Accuracy 89.327%\n",
      "Epoch 33, Batch 553, LR 0.000007 Loss 4.540622, Accuracy 89.334%\n",
      "Epoch 33, Batch 554, LR 0.000007 Loss 4.540902, Accuracy 89.333%\n",
      "Epoch 33, Batch 555, LR 0.000007 Loss 4.539723, Accuracy 89.337%\n",
      "Epoch 33, Batch 556, LR 0.000007 Loss 4.541181, Accuracy 89.329%\n",
      "Epoch 33, Batch 557, LR 0.000007 Loss 4.541090, Accuracy 89.330%\n",
      "Epoch 33, Batch 558, LR 0.000007 Loss 4.541400, Accuracy 89.329%\n",
      "Epoch 33, Batch 559, LR 0.000007 Loss 4.540889, Accuracy 89.334%\n",
      "Epoch 33, Batch 560, LR 0.000007 Loss 4.540509, Accuracy 89.337%\n",
      "Epoch 33, Batch 561, LR 0.000007 Loss 4.539649, Accuracy 89.344%\n",
      "Epoch 33, Batch 562, LR 0.000007 Loss 4.539389, Accuracy 89.346%\n",
      "Epoch 33, Batch 563, LR 0.000007 Loss 4.539158, Accuracy 89.358%\n",
      "Epoch 33, Batch 564, LR 0.000007 Loss 4.538578, Accuracy 89.360%\n",
      "Epoch 33, Batch 565, LR 0.000007 Loss 4.538222, Accuracy 89.360%\n",
      "Epoch 33, Batch 566, LR 0.000007 Loss 4.537312, Accuracy 89.362%\n",
      "Epoch 33, Batch 567, LR 0.000007 Loss 4.537971, Accuracy 89.352%\n",
      "Epoch 33, Batch 568, LR 0.000007 Loss 4.538267, Accuracy 89.351%\n",
      "Epoch 33, Batch 569, LR 0.000007 Loss 4.537625, Accuracy 89.351%\n",
      "Epoch 33, Batch 570, LR 0.000007 Loss 4.537690, Accuracy 89.353%\n",
      "Epoch 33, Batch 571, LR 0.000007 Loss 4.539156, Accuracy 89.351%\n",
      "Epoch 33, Batch 572, LR 0.000007 Loss 4.538728, Accuracy 89.355%\n",
      "Epoch 33, Batch 573, LR 0.000007 Loss 4.539434, Accuracy 89.354%\n",
      "Epoch 33, Batch 574, LR 0.000007 Loss 4.537365, Accuracy 89.365%\n",
      "Epoch 33, Batch 575, LR 0.000007 Loss 4.537774, Accuracy 89.363%\n",
      "Epoch 33, Batch 576, LR 0.000007 Loss 4.538144, Accuracy 89.364%\n",
      "Epoch 33, Batch 577, LR 0.000007 Loss 4.536318, Accuracy 89.369%\n",
      "Epoch 33, Batch 578, LR 0.000007 Loss 4.535119, Accuracy 89.376%\n",
      "Epoch 33, Batch 579, LR 0.000007 Loss 4.534776, Accuracy 89.377%\n",
      "Epoch 33, Batch 580, LR 0.000007 Loss 4.536062, Accuracy 89.372%\n",
      "Epoch 33, Batch 581, LR 0.000007 Loss 4.534077, Accuracy 89.383%\n",
      "Epoch 33, Batch 582, LR 0.000007 Loss 4.532648, Accuracy 89.395%\n",
      "Epoch 33, Batch 583, LR 0.000007 Loss 4.534308, Accuracy 89.385%\n",
      "Epoch 33, Batch 584, LR 0.000007 Loss 4.534229, Accuracy 89.392%\n",
      "Epoch 33, Batch 585, LR 0.000007 Loss 4.534673, Accuracy 89.388%\n",
      "Epoch 33, Batch 586, LR 0.000007 Loss 4.534648, Accuracy 89.392%\n",
      "Epoch 33, Batch 587, LR 0.000007 Loss 4.534800, Accuracy 89.395%\n",
      "Epoch 33, Batch 588, LR 0.000007 Loss 4.534367, Accuracy 89.396%\n",
      "Epoch 33, Batch 589, LR 0.000007 Loss 4.534233, Accuracy 89.397%\n",
      "Epoch 33, Batch 590, LR 0.000007 Loss 4.532944, Accuracy 89.401%\n",
      "Epoch 33, Batch 591, LR 0.000007 Loss 4.534176, Accuracy 89.397%\n",
      "Epoch 33, Batch 592, LR 0.000007 Loss 4.532741, Accuracy 89.404%\n",
      "Epoch 33, Batch 593, LR 0.000007 Loss 4.532114, Accuracy 89.406%\n",
      "Epoch 33, Batch 594, LR 0.000007 Loss 4.532397, Accuracy 89.404%\n",
      "Epoch 33, Batch 595, LR 0.000007 Loss 4.534068, Accuracy 89.396%\n",
      "Epoch 33, Batch 596, LR 0.000007 Loss 4.532566, Accuracy 89.402%\n",
      "Epoch 33, Batch 597, LR 0.000007 Loss 4.532209, Accuracy 89.396%\n",
      "Epoch 33, Batch 598, LR 0.000007 Loss 4.532242, Accuracy 89.397%\n",
      "Epoch 33, Batch 599, LR 0.000007 Loss 4.532470, Accuracy 89.395%\n",
      "Epoch 33, Batch 600, LR 0.000007 Loss 4.532382, Accuracy 89.393%\n",
      "Epoch 33, Batch 601, LR 0.000007 Loss 4.533886, Accuracy 89.390%\n",
      "Epoch 33, Batch 602, LR 0.000007 Loss 4.534216, Accuracy 89.392%\n",
      "Epoch 33, Batch 603, LR 0.000007 Loss 4.533900, Accuracy 89.394%\n",
      "Epoch 33, Batch 604, LR 0.000007 Loss 4.533284, Accuracy 89.395%\n",
      "Epoch 33, Batch 605, LR 0.000007 Loss 4.532874, Accuracy 89.400%\n",
      "Epoch 33, Batch 606, LR 0.000007 Loss 4.534623, Accuracy 89.391%\n",
      "Epoch 33, Batch 607, LR 0.000007 Loss 4.536562, Accuracy 89.382%\n",
      "Epoch 33, Batch 608, LR 0.000007 Loss 4.536565, Accuracy 89.379%\n",
      "Epoch 33, Batch 609, LR 0.000007 Loss 4.538514, Accuracy 89.374%\n",
      "Epoch 33, Batch 610, LR 0.000007 Loss 4.536988, Accuracy 89.376%\n",
      "Epoch 33, Batch 611, LR 0.000007 Loss 4.538776, Accuracy 89.368%\n",
      "Epoch 33, Batch 612, LR 0.000007 Loss 4.538955, Accuracy 89.368%\n",
      "Epoch 33, Batch 613, LR 0.000007 Loss 4.538383, Accuracy 89.371%\n",
      "Epoch 33, Batch 614, LR 0.000007 Loss 4.539087, Accuracy 89.365%\n",
      "Epoch 33, Batch 615, LR 0.000007 Loss 4.538771, Accuracy 89.370%\n",
      "Epoch 33, Batch 616, LR 0.000007 Loss 4.539024, Accuracy 89.372%\n",
      "Epoch 33, Batch 617, LR 0.000007 Loss 4.538903, Accuracy 89.369%\n",
      "Epoch 33, Batch 618, LR 0.000007 Loss 4.538451, Accuracy 89.370%\n",
      "Epoch 33, Batch 619, LR 0.000007 Loss 4.537429, Accuracy 89.369%\n",
      "Epoch 33, Batch 620, LR 0.000007 Loss 4.538059, Accuracy 89.370%\n",
      "Epoch 33, Batch 621, LR 0.000007 Loss 4.537525, Accuracy 89.368%\n",
      "Epoch 33, Batch 622, LR 0.000007 Loss 4.537825, Accuracy 89.365%\n",
      "Epoch 33, Batch 623, LR 0.000007 Loss 4.538055, Accuracy 89.363%\n",
      "Epoch 33, Batch 624, LR 0.000007 Loss 4.538575, Accuracy 89.359%\n",
      "Epoch 33, Batch 625, LR 0.000007 Loss 4.538316, Accuracy 89.364%\n",
      "Epoch 33, Batch 626, LR 0.000007 Loss 4.538538, Accuracy 89.367%\n",
      "Epoch 33, Batch 627, LR 0.000007 Loss 4.538881, Accuracy 89.370%\n",
      "Epoch 33, Batch 628, LR 0.000007 Loss 4.538441, Accuracy 89.371%\n",
      "Epoch 33, Batch 629, LR 0.000007 Loss 4.538073, Accuracy 89.373%\n",
      "Epoch 33, Batch 630, LR 0.000007 Loss 4.539406, Accuracy 89.368%\n",
      "Epoch 33, Batch 631, LR 0.000007 Loss 4.538635, Accuracy 89.375%\n",
      "Epoch 33, Batch 632, LR 0.000007 Loss 4.538233, Accuracy 89.375%\n",
      "Epoch 33, Batch 633, LR 0.000007 Loss 4.538496, Accuracy 89.372%\n",
      "Epoch 33, Batch 634, LR 0.000007 Loss 4.538068, Accuracy 89.380%\n",
      "Epoch 33, Batch 635, LR 0.000007 Loss 4.538084, Accuracy 89.376%\n",
      "Epoch 33, Batch 636, LR 0.000007 Loss 4.538736, Accuracy 89.381%\n",
      "Epoch 33, Batch 637, LR 0.000007 Loss 4.538670, Accuracy 89.380%\n",
      "Epoch 33, Batch 638, LR 0.000007 Loss 4.538864, Accuracy 89.387%\n",
      "Epoch 33, Batch 639, LR 0.000007 Loss 4.537917, Accuracy 89.393%\n",
      "Epoch 33, Batch 640, LR 0.000007 Loss 4.538289, Accuracy 89.391%\n",
      "Epoch 33, Batch 641, LR 0.000007 Loss 4.537610, Accuracy 89.394%\n",
      "Epoch 33, Batch 642, LR 0.000007 Loss 4.538605, Accuracy 89.393%\n",
      "Epoch 33, Batch 643, LR 0.000007 Loss 4.539359, Accuracy 89.389%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 644, LR 0.000007 Loss 4.539714, Accuracy 89.392%\n",
      "Epoch 33, Batch 645, LR 0.000007 Loss 4.540165, Accuracy 89.388%\n",
      "Epoch 33, Batch 646, LR 0.000007 Loss 4.540710, Accuracy 89.384%\n",
      "Epoch 33, Batch 647, LR 0.000007 Loss 4.540933, Accuracy 89.382%\n",
      "Epoch 33, Batch 648, LR 0.000007 Loss 4.541468, Accuracy 89.387%\n",
      "Epoch 33, Batch 649, LR 0.000007 Loss 4.542975, Accuracy 89.375%\n",
      "Epoch 33, Batch 650, LR 0.000007 Loss 4.543051, Accuracy 89.376%\n",
      "Epoch 33, Batch 651, LR 0.000007 Loss 4.543087, Accuracy 89.372%\n",
      "Epoch 33, Batch 652, LR 0.000007 Loss 4.542228, Accuracy 89.378%\n",
      "Epoch 33, Batch 653, LR 0.000007 Loss 4.541989, Accuracy 89.375%\n",
      "Epoch 33, Batch 654, LR 0.000007 Loss 4.542753, Accuracy 89.372%\n",
      "Epoch 33, Batch 655, LR 0.000007 Loss 4.542144, Accuracy 89.371%\n",
      "Epoch 33, Batch 656, LR 0.000007 Loss 4.542120, Accuracy 89.375%\n",
      "Epoch 33, Batch 657, LR 0.000007 Loss 4.542608, Accuracy 89.373%\n",
      "Epoch 33, Batch 658, LR 0.000007 Loss 4.542659, Accuracy 89.368%\n",
      "Epoch 33, Batch 659, LR 0.000007 Loss 4.541864, Accuracy 89.368%\n",
      "Epoch 33, Batch 660, LR 0.000007 Loss 4.542436, Accuracy 89.364%\n",
      "Epoch 33, Batch 661, LR 0.000007 Loss 4.543114, Accuracy 89.364%\n",
      "Epoch 33, Batch 662, LR 0.000007 Loss 4.542575, Accuracy 89.373%\n",
      "Epoch 33, Batch 663, LR 0.000007 Loss 4.542122, Accuracy 89.375%\n",
      "Epoch 33, Batch 664, LR 0.000007 Loss 4.542234, Accuracy 89.370%\n",
      "Epoch 33, Batch 665, LR 0.000007 Loss 4.542319, Accuracy 89.366%\n",
      "Epoch 33, Batch 666, LR 0.000007 Loss 4.542707, Accuracy 89.363%\n",
      "Epoch 33, Batch 667, LR 0.000007 Loss 4.541797, Accuracy 89.361%\n",
      "Epoch 33, Batch 668, LR 0.000007 Loss 4.541929, Accuracy 89.360%\n",
      "Epoch 33, Batch 669, LR 0.000007 Loss 4.541428, Accuracy 89.368%\n",
      "Epoch 33, Batch 670, LR 0.000007 Loss 4.540658, Accuracy 89.374%\n",
      "Epoch 33, Batch 671, LR 0.000007 Loss 4.540626, Accuracy 89.366%\n",
      "Epoch 33, Batch 672, LR 0.000007 Loss 4.540028, Accuracy 89.365%\n",
      "Epoch 33, Batch 673, LR 0.000007 Loss 4.540559, Accuracy 89.362%\n",
      "Epoch 33, Batch 674, LR 0.000007 Loss 4.539628, Accuracy 89.364%\n",
      "Epoch 33, Batch 675, LR 0.000007 Loss 4.539071, Accuracy 89.369%\n",
      "Epoch 33, Batch 676, LR 0.000007 Loss 4.538651, Accuracy 89.372%\n",
      "Epoch 33, Batch 677, LR 0.000007 Loss 4.538034, Accuracy 89.371%\n",
      "Epoch 33, Batch 678, LR 0.000007 Loss 4.538160, Accuracy 89.370%\n",
      "Epoch 33, Batch 679, LR 0.000007 Loss 4.538035, Accuracy 89.375%\n",
      "Epoch 33, Batch 680, LR 0.000007 Loss 4.537288, Accuracy 89.376%\n",
      "Epoch 33, Batch 681, LR 0.000007 Loss 4.536303, Accuracy 89.380%\n",
      "Epoch 33, Batch 682, LR 0.000007 Loss 4.536236, Accuracy 89.378%\n",
      "Epoch 33, Batch 683, LR 0.000007 Loss 4.536613, Accuracy 89.374%\n",
      "Epoch 33, Batch 684, LR 0.000007 Loss 4.536714, Accuracy 89.364%\n",
      "Epoch 33, Batch 685, LR 0.000007 Loss 4.536425, Accuracy 89.361%\n",
      "Epoch 33, Batch 686, LR 0.000007 Loss 4.535660, Accuracy 89.364%\n",
      "Epoch 33, Batch 687, LR 0.000007 Loss 4.535563, Accuracy 89.367%\n",
      "Epoch 33, Batch 688, LR 0.000007 Loss 4.535096, Accuracy 89.366%\n",
      "Epoch 33, Batch 689, LR 0.000007 Loss 4.534609, Accuracy 89.368%\n",
      "Epoch 33, Batch 690, LR 0.000007 Loss 4.534346, Accuracy 89.364%\n",
      "Epoch 33, Batch 691, LR 0.000007 Loss 4.534674, Accuracy 89.361%\n",
      "Epoch 33, Batch 692, LR 0.000007 Loss 4.535360, Accuracy 89.362%\n",
      "Epoch 33, Batch 693, LR 0.000007 Loss 4.535179, Accuracy 89.365%\n",
      "Epoch 33, Batch 694, LR 0.000007 Loss 4.533855, Accuracy 89.372%\n",
      "Epoch 33, Batch 695, LR 0.000007 Loss 4.533112, Accuracy 89.374%\n",
      "Epoch 33, Batch 696, LR 0.000007 Loss 4.532836, Accuracy 89.376%\n",
      "Epoch 33, Batch 697, LR 0.000007 Loss 4.532275, Accuracy 89.373%\n",
      "Epoch 33, Batch 698, LR 0.000007 Loss 4.532431, Accuracy 89.374%\n",
      "Epoch 33, Batch 699, LR 0.000007 Loss 4.533994, Accuracy 89.362%\n",
      "Epoch 33, Batch 700, LR 0.000007 Loss 4.533697, Accuracy 89.360%\n",
      "Epoch 33, Batch 701, LR 0.000007 Loss 4.532939, Accuracy 89.363%\n",
      "Epoch 33, Batch 702, LR 0.000007 Loss 4.532678, Accuracy 89.363%\n",
      "Epoch 33, Batch 703, LR 0.000007 Loss 4.532006, Accuracy 89.366%\n",
      "Epoch 33, Batch 704, LR 0.000007 Loss 4.532389, Accuracy 89.360%\n",
      "Epoch 33, Batch 705, LR 0.000007 Loss 4.533155, Accuracy 89.361%\n",
      "Epoch 33, Batch 706, LR 0.000007 Loss 4.532482, Accuracy 89.366%\n",
      "Epoch 33, Batch 707, LR 0.000007 Loss 4.532012, Accuracy 89.372%\n",
      "Epoch 33, Batch 708, LR 0.000007 Loss 4.531556, Accuracy 89.375%\n",
      "Epoch 33, Batch 709, LR 0.000007 Loss 4.532236, Accuracy 89.373%\n",
      "Epoch 33, Batch 710, LR 0.000007 Loss 4.531961, Accuracy 89.376%\n",
      "Epoch 33, Batch 711, LR 0.000007 Loss 4.531237, Accuracy 89.383%\n",
      "Epoch 33, Batch 712, LR 0.000007 Loss 4.530869, Accuracy 89.384%\n",
      "Epoch 33, Batch 713, LR 0.000007 Loss 4.532327, Accuracy 89.374%\n",
      "Epoch 33, Batch 714, LR 0.000007 Loss 4.532552, Accuracy 89.371%\n",
      "Epoch 33, Batch 715, LR 0.000007 Loss 4.532203, Accuracy 89.374%\n",
      "Epoch 33, Batch 716, LR 0.000007 Loss 4.532623, Accuracy 89.375%\n",
      "Epoch 33, Batch 717, LR 0.000007 Loss 4.532166, Accuracy 89.382%\n",
      "Epoch 33, Batch 718, LR 0.000007 Loss 4.532054, Accuracy 89.377%\n",
      "Epoch 33, Batch 719, LR 0.000007 Loss 4.532632, Accuracy 89.372%\n",
      "Epoch 33, Batch 720, LR 0.000007 Loss 4.533543, Accuracy 89.373%\n",
      "Epoch 33, Batch 721, LR 0.000007 Loss 4.534033, Accuracy 89.372%\n",
      "Epoch 33, Batch 722, LR 0.000007 Loss 4.534060, Accuracy 89.373%\n",
      "Epoch 33, Batch 723, LR 0.000007 Loss 4.533675, Accuracy 89.381%\n",
      "Epoch 33, Batch 724, LR 0.000007 Loss 4.533118, Accuracy 89.384%\n",
      "Epoch 33, Batch 725, LR 0.000007 Loss 4.533025, Accuracy 89.389%\n",
      "Epoch 33, Batch 726, LR 0.000007 Loss 4.533301, Accuracy 89.391%\n",
      "Epoch 33, Batch 727, LR 0.000007 Loss 4.533905, Accuracy 89.389%\n",
      "Epoch 33, Batch 728, LR 0.000007 Loss 4.533450, Accuracy 89.392%\n",
      "Epoch 33, Batch 729, LR 0.000007 Loss 4.532234, Accuracy 89.399%\n",
      "Epoch 33, Batch 730, LR 0.000007 Loss 4.531615, Accuracy 89.402%\n",
      "Epoch 33, Batch 731, LR 0.000007 Loss 4.531234, Accuracy 89.406%\n",
      "Epoch 33, Batch 732, LR 0.000007 Loss 4.531506, Accuracy 89.406%\n",
      "Epoch 33, Batch 733, LR 0.000007 Loss 4.531832, Accuracy 89.405%\n",
      "Epoch 33, Batch 734, LR 0.000007 Loss 4.531903, Accuracy 89.400%\n",
      "Epoch 33, Batch 735, LR 0.000007 Loss 4.532152, Accuracy 89.399%\n",
      "Epoch 33, Batch 736, LR 0.000007 Loss 4.532281, Accuracy 89.400%\n",
      "Epoch 33, Batch 737, LR 0.000007 Loss 4.531630, Accuracy 89.402%\n",
      "Epoch 33, Batch 738, LR 0.000007 Loss 4.531739, Accuracy 89.403%\n",
      "Epoch 33, Batch 739, LR 0.000007 Loss 4.531850, Accuracy 89.407%\n",
      "Epoch 33, Batch 740, LR 0.000007 Loss 4.532580, Accuracy 89.406%\n",
      "Epoch 33, Batch 741, LR 0.000007 Loss 4.533233, Accuracy 89.401%\n",
      "Epoch 33, Batch 742, LR 0.000007 Loss 4.532995, Accuracy 89.404%\n",
      "Epoch 33, Batch 743, LR 0.000007 Loss 4.534029, Accuracy 89.396%\n",
      "Epoch 33, Batch 744, LR 0.000007 Loss 4.534121, Accuracy 89.397%\n",
      "Epoch 33, Batch 745, LR 0.000007 Loss 4.534729, Accuracy 89.397%\n",
      "Epoch 33, Batch 746, LR 0.000007 Loss 4.534878, Accuracy 89.399%\n",
      "Epoch 33, Batch 747, LR 0.000007 Loss 4.533790, Accuracy 89.401%\n",
      "Epoch 33, Batch 748, LR 0.000007 Loss 4.533972, Accuracy 89.403%\n",
      "Epoch 33, Batch 749, LR 0.000007 Loss 4.534037, Accuracy 89.400%\n",
      "Epoch 33, Batch 750, LR 0.000007 Loss 4.533466, Accuracy 89.405%\n",
      "Epoch 33, Batch 751, LR 0.000007 Loss 4.532898, Accuracy 89.409%\n",
      "Epoch 33, Batch 752, LR 0.000007 Loss 4.532319, Accuracy 89.411%\n",
      "Epoch 33, Batch 753, LR 0.000007 Loss 4.531401, Accuracy 89.418%\n",
      "Epoch 33, Batch 754, LR 0.000007 Loss 4.531044, Accuracy 89.419%\n",
      "Epoch 33, Batch 755, LR 0.000007 Loss 4.530790, Accuracy 89.421%\n",
      "Epoch 33, Batch 756, LR 0.000007 Loss 4.531512, Accuracy 89.419%\n",
      "Epoch 33, Batch 757, LR 0.000007 Loss 4.531086, Accuracy 89.424%\n",
      "Epoch 33, Batch 758, LR 0.000007 Loss 4.530659, Accuracy 89.425%\n",
      "Epoch 33, Batch 759, LR 0.000007 Loss 4.529946, Accuracy 89.431%\n",
      "Epoch 33, Batch 760, LR 0.000007 Loss 4.530187, Accuracy 89.434%\n",
      "Epoch 33, Batch 761, LR 0.000007 Loss 4.530018, Accuracy 89.434%\n",
      "Epoch 33, Batch 762, LR 0.000007 Loss 4.529332, Accuracy 89.437%\n",
      "Epoch 33, Batch 763, LR 0.000007 Loss 4.529174, Accuracy 89.439%\n",
      "Epoch 33, Batch 764, LR 0.000007 Loss 4.527585, Accuracy 89.442%\n",
      "Epoch 33, Batch 765, LR 0.000007 Loss 4.527522, Accuracy 89.441%\n",
      "Epoch 33, Batch 766, LR 0.000007 Loss 4.527577, Accuracy 89.444%\n",
      "Epoch 33, Batch 767, LR 0.000007 Loss 4.528132, Accuracy 89.442%\n",
      "Epoch 33, Batch 768, LR 0.000007 Loss 4.527944, Accuracy 89.442%\n",
      "Epoch 33, Batch 769, LR 0.000007 Loss 4.527376, Accuracy 89.444%\n",
      "Epoch 33, Batch 770, LR 0.000007 Loss 4.527631, Accuracy 89.442%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 771, LR 0.000007 Loss 4.527774, Accuracy 89.446%\n",
      "Epoch 33, Batch 772, LR 0.000007 Loss 4.526768, Accuracy 89.452%\n",
      "Epoch 33, Batch 773, LR 0.000007 Loss 4.526538, Accuracy 89.451%\n",
      "Epoch 33, Batch 774, LR 0.000007 Loss 4.526499, Accuracy 89.455%\n",
      "Epoch 33, Batch 775, LR 0.000007 Loss 4.527153, Accuracy 89.454%\n",
      "Epoch 33, Batch 776, LR 0.000007 Loss 4.527174, Accuracy 89.450%\n",
      "Epoch 33, Batch 777, LR 0.000007 Loss 4.527101, Accuracy 89.452%\n",
      "Epoch 33, Batch 778, LR 0.000007 Loss 4.526752, Accuracy 89.453%\n",
      "Epoch 33, Batch 779, LR 0.000007 Loss 4.526491, Accuracy 89.459%\n",
      "Epoch 33, Batch 780, LR 0.000007 Loss 4.526237, Accuracy 89.460%\n",
      "Epoch 33, Batch 781, LR 0.000007 Loss 4.526013, Accuracy 89.458%\n",
      "Epoch 33, Batch 782, LR 0.000007 Loss 4.525960, Accuracy 89.459%\n",
      "Epoch 33, Batch 783, LR 0.000007 Loss 4.525433, Accuracy 89.466%\n",
      "Epoch 33, Batch 784, LR 0.000007 Loss 4.525173, Accuracy 89.465%\n",
      "Epoch 33, Batch 785, LR 0.000007 Loss 4.524787, Accuracy 89.467%\n",
      "Epoch 33, Batch 786, LR 0.000007 Loss 4.524310, Accuracy 89.470%\n",
      "Epoch 33, Batch 787, LR 0.000007 Loss 4.524681, Accuracy 89.468%\n",
      "Epoch 33, Batch 788, LR 0.000007 Loss 4.524236, Accuracy 89.466%\n",
      "Epoch 33, Batch 789, LR 0.000007 Loss 4.524598, Accuracy 89.466%\n",
      "Epoch 33, Batch 790, LR 0.000007 Loss 4.525207, Accuracy 89.468%\n",
      "Epoch 33, Batch 791, LR 0.000007 Loss 4.525754, Accuracy 89.468%\n",
      "Epoch 33, Batch 792, LR 0.000007 Loss 4.525612, Accuracy 89.467%\n",
      "Epoch 33, Batch 793, LR 0.000007 Loss 4.526318, Accuracy 89.465%\n",
      "Epoch 33, Batch 794, LR 0.000007 Loss 4.528469, Accuracy 89.454%\n",
      "Epoch 33, Batch 795, LR 0.000007 Loss 4.528173, Accuracy 89.453%\n",
      "Epoch 33, Batch 796, LR 0.000007 Loss 4.528811, Accuracy 89.450%\n",
      "Epoch 33, Batch 797, LR 0.000007 Loss 4.528118, Accuracy 89.453%\n",
      "Epoch 33, Batch 798, LR 0.000007 Loss 4.528270, Accuracy 89.454%\n",
      "Epoch 33, Batch 799, LR 0.000007 Loss 4.527582, Accuracy 89.459%\n",
      "Epoch 33, Batch 800, LR 0.000006 Loss 4.527445, Accuracy 89.457%\n",
      "Epoch 33, Batch 801, LR 0.000006 Loss 4.527198, Accuracy 89.458%\n",
      "Epoch 33, Batch 802, LR 0.000006 Loss 4.526516, Accuracy 89.464%\n",
      "Epoch 33, Batch 803, LR 0.000006 Loss 4.527242, Accuracy 89.462%\n",
      "Epoch 33, Batch 804, LR 0.000006 Loss 4.526935, Accuracy 89.463%\n",
      "Epoch 33, Batch 805, LR 0.000006 Loss 4.526349, Accuracy 89.466%\n",
      "Epoch 33, Batch 806, LR 0.000006 Loss 4.527029, Accuracy 89.466%\n",
      "Epoch 33, Batch 807, LR 0.000006 Loss 4.526687, Accuracy 89.464%\n",
      "Epoch 33, Batch 808, LR 0.000006 Loss 4.527016, Accuracy 89.463%\n",
      "Epoch 33, Batch 809, LR 0.000006 Loss 4.527067, Accuracy 89.461%\n",
      "Epoch 33, Batch 810, LR 0.000006 Loss 4.527107, Accuracy 89.460%\n",
      "Epoch 33, Batch 811, LR 0.000006 Loss 4.526827, Accuracy 89.458%\n",
      "Epoch 33, Batch 812, LR 0.000006 Loss 4.526309, Accuracy 89.456%\n",
      "Epoch 33, Batch 813, LR 0.000006 Loss 4.527042, Accuracy 89.454%\n",
      "Epoch 33, Batch 814, LR 0.000006 Loss 4.527552, Accuracy 89.448%\n",
      "Epoch 33, Batch 815, LR 0.000006 Loss 4.526891, Accuracy 89.454%\n",
      "Epoch 33, Batch 816, LR 0.000006 Loss 4.526370, Accuracy 89.456%\n",
      "Epoch 33, Batch 817, LR 0.000006 Loss 4.526206, Accuracy 89.459%\n",
      "Epoch 33, Batch 818, LR 0.000006 Loss 4.526023, Accuracy 89.459%\n",
      "Epoch 33, Batch 819, LR 0.000006 Loss 4.525726, Accuracy 89.456%\n",
      "Epoch 33, Batch 820, LR 0.000006 Loss 4.525935, Accuracy 89.456%\n",
      "Epoch 33, Batch 821, LR 0.000006 Loss 4.526259, Accuracy 89.457%\n",
      "Epoch 33, Batch 822, LR 0.000006 Loss 4.526486, Accuracy 89.458%\n",
      "Epoch 33, Batch 823, LR 0.000006 Loss 4.526935, Accuracy 89.453%\n",
      "Epoch 33, Batch 824, LR 0.000006 Loss 4.526904, Accuracy 89.450%\n",
      "Epoch 33, Batch 825, LR 0.000006 Loss 4.527470, Accuracy 89.445%\n",
      "Epoch 33, Batch 826, LR 0.000006 Loss 4.528151, Accuracy 89.447%\n",
      "Epoch 33, Batch 827, LR 0.000006 Loss 4.527915, Accuracy 89.451%\n",
      "Epoch 33, Batch 828, LR 0.000006 Loss 4.528197, Accuracy 89.447%\n",
      "Epoch 33, Batch 829, LR 0.000006 Loss 4.528350, Accuracy 89.446%\n",
      "Epoch 33, Batch 830, LR 0.000006 Loss 4.528579, Accuracy 89.445%\n",
      "Epoch 33, Batch 831, LR 0.000006 Loss 4.528267, Accuracy 89.447%\n",
      "Epoch 33, Batch 832, LR 0.000006 Loss 4.528704, Accuracy 89.441%\n",
      "Epoch 33, Batch 833, LR 0.000006 Loss 4.528720, Accuracy 89.441%\n",
      "Epoch 33, Batch 834, LR 0.000006 Loss 4.528913, Accuracy 89.443%\n",
      "Epoch 33, Batch 835, LR 0.000006 Loss 4.529080, Accuracy 89.440%\n",
      "Epoch 33, Batch 836, LR 0.000006 Loss 4.530094, Accuracy 89.436%\n",
      "Epoch 33, Batch 837, LR 0.000006 Loss 4.529939, Accuracy 89.436%\n",
      "Epoch 33, Batch 838, LR 0.000006 Loss 4.529699, Accuracy 89.437%\n",
      "Epoch 33, Batch 839, LR 0.000006 Loss 4.529442, Accuracy 89.438%\n",
      "Epoch 33, Batch 840, LR 0.000006 Loss 4.530765, Accuracy 89.435%\n",
      "Epoch 33, Batch 841, LR 0.000006 Loss 4.530179, Accuracy 89.437%\n",
      "Epoch 33, Batch 842, LR 0.000006 Loss 4.530201, Accuracy 89.437%\n",
      "Epoch 33, Batch 843, LR 0.000006 Loss 4.530297, Accuracy 89.435%\n",
      "Epoch 33, Batch 844, LR 0.000006 Loss 4.530680, Accuracy 89.436%\n",
      "Epoch 33, Batch 845, LR 0.000006 Loss 4.531125, Accuracy 89.432%\n",
      "Epoch 33, Batch 846, LR 0.000006 Loss 4.532635, Accuracy 89.424%\n",
      "Epoch 33, Batch 847, LR 0.000006 Loss 4.532592, Accuracy 89.424%\n",
      "Epoch 33, Batch 848, LR 0.000006 Loss 4.531539, Accuracy 89.432%\n",
      "Epoch 33, Batch 849, LR 0.000006 Loss 4.531210, Accuracy 89.438%\n",
      "Epoch 33, Batch 850, LR 0.000006 Loss 4.530988, Accuracy 89.439%\n",
      "Epoch 33, Batch 851, LR 0.000006 Loss 4.529994, Accuracy 89.443%\n",
      "Epoch 33, Batch 852, LR 0.000006 Loss 4.529406, Accuracy 89.444%\n",
      "Epoch 33, Batch 853, LR 0.000006 Loss 4.529188, Accuracy 89.444%\n",
      "Epoch 33, Batch 854, LR 0.000006 Loss 4.529757, Accuracy 89.443%\n",
      "Epoch 33, Batch 855, LR 0.000006 Loss 4.529781, Accuracy 89.440%\n",
      "Epoch 33, Batch 856, LR 0.000006 Loss 4.529396, Accuracy 89.440%\n",
      "Epoch 33, Batch 857, LR 0.000006 Loss 4.530568, Accuracy 89.437%\n",
      "Epoch 33, Batch 858, LR 0.000006 Loss 4.530804, Accuracy 89.438%\n",
      "Epoch 33, Batch 859, LR 0.000006 Loss 4.530671, Accuracy 89.437%\n",
      "Epoch 33, Batch 860, LR 0.000006 Loss 4.530169, Accuracy 89.437%\n",
      "Epoch 33, Batch 861, LR 0.000006 Loss 4.530975, Accuracy 89.437%\n",
      "Epoch 33, Batch 862, LR 0.000006 Loss 4.530551, Accuracy 89.435%\n",
      "Epoch 33, Batch 863, LR 0.000006 Loss 4.530064, Accuracy 89.435%\n",
      "Epoch 33, Batch 864, LR 0.000006 Loss 4.529795, Accuracy 89.437%\n",
      "Epoch 33, Batch 865, LR 0.000006 Loss 4.530880, Accuracy 89.429%\n",
      "Epoch 33, Batch 866, LR 0.000006 Loss 4.531487, Accuracy 89.424%\n",
      "Epoch 33, Batch 867, LR 0.000006 Loss 4.531650, Accuracy 89.423%\n",
      "Epoch 33, Batch 868, LR 0.000006 Loss 4.531313, Accuracy 89.422%\n",
      "Epoch 33, Batch 869, LR 0.000006 Loss 4.530453, Accuracy 89.426%\n",
      "Epoch 33, Batch 870, LR 0.000006 Loss 4.530142, Accuracy 89.428%\n",
      "Epoch 33, Batch 871, LR 0.000006 Loss 4.530205, Accuracy 89.428%\n",
      "Epoch 33, Batch 872, LR 0.000006 Loss 4.529269, Accuracy 89.436%\n",
      "Epoch 33, Batch 873, LR 0.000006 Loss 4.529513, Accuracy 89.436%\n",
      "Epoch 33, Batch 874, LR 0.000006 Loss 4.529534, Accuracy 89.440%\n",
      "Epoch 33, Batch 875, LR 0.000006 Loss 4.528726, Accuracy 89.445%\n",
      "Epoch 33, Batch 876, LR 0.000006 Loss 4.528652, Accuracy 89.442%\n",
      "Epoch 33, Batch 877, LR 0.000006 Loss 4.528107, Accuracy 89.442%\n",
      "Epoch 33, Batch 878, LR 0.000006 Loss 4.527690, Accuracy 89.442%\n",
      "Epoch 33, Batch 879, LR 0.000006 Loss 4.527409, Accuracy 89.441%\n",
      "Epoch 33, Batch 880, LR 0.000006 Loss 4.527289, Accuracy 89.439%\n",
      "Epoch 33, Batch 881, LR 0.000006 Loss 4.526803, Accuracy 89.443%\n",
      "Epoch 33, Batch 882, LR 0.000006 Loss 4.527432, Accuracy 89.437%\n",
      "Epoch 33, Batch 883, LR 0.000006 Loss 4.526755, Accuracy 89.443%\n",
      "Epoch 33, Batch 884, LR 0.000006 Loss 4.526951, Accuracy 89.441%\n",
      "Epoch 33, Batch 885, LR 0.000006 Loss 4.527451, Accuracy 89.439%\n",
      "Epoch 33, Batch 886, LR 0.000006 Loss 4.527676, Accuracy 89.439%\n",
      "Epoch 33, Batch 887, LR 0.000006 Loss 4.527926, Accuracy 89.438%\n",
      "Epoch 33, Batch 888, LR 0.000006 Loss 4.527640, Accuracy 89.442%\n",
      "Epoch 33, Batch 889, LR 0.000006 Loss 4.527534, Accuracy 89.442%\n",
      "Epoch 33, Batch 890, LR 0.000006 Loss 4.527454, Accuracy 89.443%\n",
      "Epoch 33, Batch 891, LR 0.000006 Loss 4.526768, Accuracy 89.444%\n",
      "Epoch 33, Batch 892, LR 0.000006 Loss 4.526921, Accuracy 89.444%\n",
      "Epoch 33, Batch 893, LR 0.000006 Loss 4.527988, Accuracy 89.437%\n",
      "Epoch 33, Batch 894, LR 0.000006 Loss 4.527928, Accuracy 89.439%\n",
      "Epoch 33, Batch 895, LR 0.000006 Loss 4.527324, Accuracy 89.440%\n",
      "Epoch 33, Batch 896, LR 0.000006 Loss 4.527471, Accuracy 89.438%\n",
      "Epoch 33, Batch 897, LR 0.000006 Loss 4.526972, Accuracy 89.444%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 898, LR 0.000006 Loss 4.526862, Accuracy 89.441%\n",
      "Epoch 33, Batch 899, LR 0.000006 Loss 4.526694, Accuracy 89.442%\n",
      "Epoch 33, Batch 900, LR 0.000006 Loss 4.526803, Accuracy 89.438%\n",
      "Epoch 33, Batch 901, LR 0.000006 Loss 4.525436, Accuracy 89.444%\n",
      "Epoch 33, Batch 902, LR 0.000006 Loss 4.525364, Accuracy 89.446%\n",
      "Epoch 33, Batch 903, LR 0.000006 Loss 4.525270, Accuracy 89.449%\n",
      "Epoch 33, Batch 904, LR 0.000006 Loss 4.525307, Accuracy 89.446%\n",
      "Epoch 33, Batch 905, LR 0.000006 Loss 4.525154, Accuracy 89.447%\n",
      "Epoch 33, Batch 906, LR 0.000006 Loss 4.525278, Accuracy 89.444%\n",
      "Epoch 33, Batch 907, LR 0.000006 Loss 4.526212, Accuracy 89.437%\n",
      "Epoch 33, Batch 908, LR 0.000006 Loss 4.525902, Accuracy 89.442%\n",
      "Epoch 33, Batch 909, LR 0.000006 Loss 4.525750, Accuracy 89.441%\n",
      "Epoch 33, Batch 910, LR 0.000006 Loss 4.526094, Accuracy 89.439%\n",
      "Epoch 33, Batch 911, LR 0.000006 Loss 4.525533, Accuracy 89.444%\n",
      "Epoch 33, Batch 912, LR 0.000006 Loss 4.525831, Accuracy 89.441%\n",
      "Epoch 33, Batch 913, LR 0.000006 Loss 4.524907, Accuracy 89.449%\n",
      "Epoch 33, Batch 914, LR 0.000006 Loss 4.524406, Accuracy 89.456%\n",
      "Epoch 33, Batch 915, LR 0.000006 Loss 4.524326, Accuracy 89.460%\n",
      "Epoch 33, Batch 916, LR 0.000006 Loss 4.523830, Accuracy 89.463%\n",
      "Epoch 33, Batch 917, LR 0.000006 Loss 4.524132, Accuracy 89.462%\n",
      "Epoch 33, Batch 918, LR 0.000006 Loss 4.524746, Accuracy 89.461%\n",
      "Epoch 33, Batch 919, LR 0.000006 Loss 4.524899, Accuracy 89.466%\n",
      "Epoch 33, Batch 920, LR 0.000006 Loss 4.524559, Accuracy 89.468%\n",
      "Epoch 33, Batch 921, LR 0.000006 Loss 4.524736, Accuracy 89.466%\n",
      "Epoch 33, Batch 922, LR 0.000006 Loss 4.524998, Accuracy 89.461%\n",
      "Epoch 33, Batch 923, LR 0.000006 Loss 4.525309, Accuracy 89.461%\n",
      "Epoch 33, Batch 924, LR 0.000006 Loss 4.525899, Accuracy 89.461%\n",
      "Epoch 33, Batch 925, LR 0.000006 Loss 4.525816, Accuracy 89.458%\n",
      "Epoch 33, Batch 926, LR 0.000006 Loss 4.526148, Accuracy 89.454%\n",
      "Epoch 33, Batch 927, LR 0.000006 Loss 4.526781, Accuracy 89.453%\n",
      "Epoch 33, Batch 928, LR 0.000006 Loss 4.526825, Accuracy 89.450%\n",
      "Epoch 33, Batch 929, LR 0.000006 Loss 4.526952, Accuracy 89.448%\n",
      "Epoch 33, Batch 930, LR 0.000006 Loss 4.526372, Accuracy 89.449%\n",
      "Epoch 33, Batch 931, LR 0.000006 Loss 4.526162, Accuracy 89.448%\n",
      "Epoch 33, Batch 932, LR 0.000006 Loss 4.525763, Accuracy 89.449%\n",
      "Epoch 33, Batch 933, LR 0.000006 Loss 4.525060, Accuracy 89.452%\n",
      "Epoch 33, Batch 934, LR 0.000006 Loss 4.525732, Accuracy 89.447%\n",
      "Epoch 33, Batch 935, LR 0.000006 Loss 4.526242, Accuracy 89.447%\n",
      "Epoch 33, Batch 936, LR 0.000006 Loss 4.526006, Accuracy 89.448%\n",
      "Epoch 33, Batch 937, LR 0.000006 Loss 4.526125, Accuracy 89.447%\n",
      "Epoch 33, Batch 938, LR 0.000006 Loss 4.526164, Accuracy 89.441%\n",
      "Epoch 33, Batch 939, LR 0.000006 Loss 4.526855, Accuracy 89.440%\n",
      "Epoch 33, Batch 940, LR 0.000006 Loss 4.526468, Accuracy 89.442%\n",
      "Epoch 33, Batch 941, LR 0.000006 Loss 4.526067, Accuracy 89.442%\n",
      "Epoch 33, Batch 942, LR 0.000006 Loss 4.525533, Accuracy 89.446%\n",
      "Epoch 33, Batch 943, LR 0.000006 Loss 4.524714, Accuracy 89.449%\n",
      "Epoch 33, Batch 944, LR 0.000006 Loss 4.524396, Accuracy 89.450%\n",
      "Epoch 33, Batch 945, LR 0.000006 Loss 4.524816, Accuracy 89.446%\n",
      "Epoch 33, Batch 946, LR 0.000006 Loss 4.525868, Accuracy 89.437%\n",
      "Epoch 33, Batch 947, LR 0.000006 Loss 4.526433, Accuracy 89.434%\n",
      "Epoch 33, Batch 948, LR 0.000006 Loss 4.526737, Accuracy 89.433%\n",
      "Epoch 33, Batch 949, LR 0.000006 Loss 4.526785, Accuracy 89.428%\n",
      "Epoch 33, Batch 950, LR 0.000006 Loss 4.527680, Accuracy 89.421%\n",
      "Epoch 33, Batch 951, LR 0.000006 Loss 4.527272, Accuracy 89.423%\n",
      "Epoch 33, Batch 952, LR 0.000006 Loss 4.527248, Accuracy 89.424%\n",
      "Epoch 33, Batch 953, LR 0.000006 Loss 4.526876, Accuracy 89.423%\n",
      "Epoch 33, Batch 954, LR 0.000006 Loss 4.526213, Accuracy 89.429%\n",
      "Epoch 33, Batch 955, LR 0.000006 Loss 4.527650, Accuracy 89.421%\n",
      "Epoch 33, Batch 956, LR 0.000006 Loss 4.528505, Accuracy 89.416%\n",
      "Epoch 33, Batch 957, LR 0.000006 Loss 4.527291, Accuracy 89.420%\n",
      "Epoch 33, Batch 958, LR 0.000006 Loss 4.527346, Accuracy 89.421%\n",
      "Epoch 33, Batch 959, LR 0.000006 Loss 4.527619, Accuracy 89.422%\n",
      "Epoch 33, Batch 960, LR 0.000006 Loss 4.527652, Accuracy 89.420%\n",
      "Epoch 33, Batch 961, LR 0.000006 Loss 4.527371, Accuracy 89.420%\n",
      "Epoch 33, Batch 962, LR 0.000006 Loss 4.526482, Accuracy 89.426%\n",
      "Epoch 33, Batch 963, LR 0.000006 Loss 4.526893, Accuracy 89.424%\n",
      "Epoch 33, Batch 964, LR 0.000006 Loss 4.526938, Accuracy 89.425%\n",
      "Epoch 33, Batch 965, LR 0.000006 Loss 4.527069, Accuracy 89.425%\n",
      "Epoch 33, Batch 966, LR 0.000006 Loss 4.527961, Accuracy 89.418%\n",
      "Epoch 33, Batch 967, LR 0.000006 Loss 4.527818, Accuracy 89.421%\n",
      "Epoch 33, Batch 968, LR 0.000006 Loss 4.527823, Accuracy 89.421%\n",
      "Epoch 33, Batch 969, LR 0.000006 Loss 4.527301, Accuracy 89.425%\n",
      "Epoch 33, Batch 970, LR 0.000006 Loss 4.527996, Accuracy 89.423%\n",
      "Epoch 33, Batch 971, LR 0.000006 Loss 4.527919, Accuracy 89.424%\n",
      "Epoch 33, Batch 972, LR 0.000006 Loss 4.527436, Accuracy 89.425%\n",
      "Epoch 33, Batch 973, LR 0.000006 Loss 4.526786, Accuracy 89.428%\n",
      "Epoch 33, Batch 974, LR 0.000006 Loss 4.526694, Accuracy 89.429%\n",
      "Epoch 33, Batch 975, LR 0.000006 Loss 4.526480, Accuracy 89.434%\n",
      "Epoch 33, Batch 976, LR 0.000006 Loss 4.525919, Accuracy 89.436%\n",
      "Epoch 33, Batch 977, LR 0.000006 Loss 4.526415, Accuracy 89.434%\n",
      "Epoch 33, Batch 978, LR 0.000006 Loss 4.525907, Accuracy 89.434%\n",
      "Epoch 33, Batch 979, LR 0.000006 Loss 4.526745, Accuracy 89.430%\n",
      "Epoch 33, Batch 980, LR 0.000006 Loss 4.526198, Accuracy 89.433%\n",
      "Epoch 33, Batch 981, LR 0.000006 Loss 4.525986, Accuracy 89.433%\n",
      "Epoch 33, Batch 982, LR 0.000006 Loss 4.525664, Accuracy 89.438%\n",
      "Epoch 33, Batch 983, LR 0.000006 Loss 4.525605, Accuracy 89.439%\n",
      "Epoch 33, Batch 984, LR 0.000006 Loss 4.525886, Accuracy 89.441%\n",
      "Epoch 33, Batch 985, LR 0.000006 Loss 4.525635, Accuracy 89.442%\n",
      "Epoch 33, Batch 986, LR 0.000006 Loss 4.525925, Accuracy 89.438%\n",
      "Epoch 33, Batch 987, LR 0.000006 Loss 4.526416, Accuracy 89.435%\n",
      "Epoch 33, Batch 988, LR 0.000006 Loss 4.526378, Accuracy 89.437%\n",
      "Epoch 33, Batch 989, LR 0.000006 Loss 4.526365, Accuracy 89.435%\n",
      "Epoch 33, Batch 990, LR 0.000006 Loss 4.526214, Accuracy 89.437%\n",
      "Epoch 33, Batch 991, LR 0.000006 Loss 4.526543, Accuracy 89.438%\n",
      "Epoch 33, Batch 992, LR 0.000006 Loss 4.526413, Accuracy 89.437%\n",
      "Epoch 33, Batch 993, LR 0.000006 Loss 4.525559, Accuracy 89.439%\n",
      "Epoch 33, Batch 994, LR 0.000006 Loss 4.525637, Accuracy 89.439%\n",
      "Epoch 33, Batch 995, LR 0.000006 Loss 4.525596, Accuracy 89.439%\n",
      "Epoch 33, Batch 996, LR 0.000006 Loss 4.525391, Accuracy 89.436%\n",
      "Epoch 33, Batch 997, LR 0.000006 Loss 4.525482, Accuracy 89.438%\n",
      "Epoch 33, Batch 998, LR 0.000006 Loss 4.525257, Accuracy 89.441%\n",
      "Epoch 33, Batch 999, LR 0.000006 Loss 4.525167, Accuracy 89.444%\n",
      "Epoch 33, Batch 1000, LR 0.000006 Loss 4.526148, Accuracy 89.439%\n",
      "Epoch 33, Batch 1001, LR 0.000006 Loss 4.525157, Accuracy 89.440%\n",
      "Epoch 33, Batch 1002, LR 0.000006 Loss 4.523823, Accuracy 89.441%\n",
      "Epoch 33, Batch 1003, LR 0.000006 Loss 4.523269, Accuracy 89.443%\n",
      "Epoch 33, Batch 1004, LR 0.000006 Loss 4.523079, Accuracy 89.445%\n",
      "Epoch 33, Batch 1005, LR 0.000006 Loss 4.522931, Accuracy 89.447%\n",
      "Epoch 33, Batch 1006, LR 0.000006 Loss 4.523316, Accuracy 89.447%\n",
      "Epoch 33, Batch 1007, LR 0.000006 Loss 4.522821, Accuracy 89.449%\n",
      "Epoch 33, Batch 1008, LR 0.000006 Loss 4.522870, Accuracy 89.445%\n",
      "Epoch 33, Batch 1009, LR 0.000006 Loss 4.523448, Accuracy 89.437%\n",
      "Epoch 33, Batch 1010, LR 0.000006 Loss 4.523159, Accuracy 89.439%\n",
      "Epoch 33, Batch 1011, LR 0.000006 Loss 4.523259, Accuracy 89.438%\n",
      "Epoch 33, Batch 1012, LR 0.000006 Loss 4.522899, Accuracy 89.435%\n",
      "Epoch 33, Batch 1013, LR 0.000006 Loss 4.522315, Accuracy 89.438%\n",
      "Epoch 33, Batch 1014, LR 0.000006 Loss 4.522875, Accuracy 89.434%\n",
      "Epoch 33, Batch 1015, LR 0.000006 Loss 4.523137, Accuracy 89.432%\n",
      "Epoch 33, Batch 1016, LR 0.000006 Loss 4.523540, Accuracy 89.432%\n",
      "Epoch 33, Batch 1017, LR 0.000006 Loss 4.523495, Accuracy 89.432%\n",
      "Epoch 33, Batch 1018, LR 0.000006 Loss 4.523684, Accuracy 89.435%\n",
      "Epoch 33, Batch 1019, LR 0.000006 Loss 4.523333, Accuracy 89.435%\n",
      "Epoch 33, Batch 1020, LR 0.000006 Loss 4.523120, Accuracy 89.437%\n",
      "Epoch 33, Batch 1021, LR 0.000006 Loss 4.523306, Accuracy 89.437%\n",
      "Epoch 33, Batch 1022, LR 0.000006 Loss 4.523015, Accuracy 89.439%\n",
      "Epoch 33, Batch 1023, LR 0.000006 Loss 4.522764, Accuracy 89.437%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 1024, LR 0.000006 Loss 4.522539, Accuracy 89.438%\n",
      "Epoch 33, Batch 1025, LR 0.000006 Loss 4.521906, Accuracy 89.441%\n",
      "Epoch 33, Batch 1026, LR 0.000006 Loss 4.521881, Accuracy 89.438%\n",
      "Epoch 33, Batch 1027, LR 0.000006 Loss 4.522484, Accuracy 89.434%\n",
      "Epoch 33, Batch 1028, LR 0.000006 Loss 4.521501, Accuracy 89.439%\n",
      "Epoch 33, Batch 1029, LR 0.000006 Loss 4.522054, Accuracy 89.435%\n",
      "Epoch 33, Batch 1030, LR 0.000006 Loss 4.522290, Accuracy 89.436%\n",
      "Epoch 33, Batch 1031, LR 0.000006 Loss 4.522876, Accuracy 89.435%\n",
      "Epoch 33, Batch 1032, LR 0.000006 Loss 4.523449, Accuracy 89.432%\n",
      "Epoch 33, Batch 1033, LR 0.000006 Loss 4.523652, Accuracy 89.432%\n",
      "Epoch 33, Batch 1034, LR 0.000006 Loss 4.524204, Accuracy 89.433%\n",
      "Epoch 33, Batch 1035, LR 0.000006 Loss 4.523969, Accuracy 89.438%\n",
      "Epoch 33, Batch 1036, LR 0.000006 Loss 4.523920, Accuracy 89.438%\n",
      "Epoch 33, Batch 1037, LR 0.000006 Loss 4.523392, Accuracy 89.441%\n",
      "Epoch 33, Batch 1038, LR 0.000006 Loss 4.523864, Accuracy 89.437%\n",
      "Epoch 33, Batch 1039, LR 0.000006 Loss 4.524555, Accuracy 89.436%\n",
      "Epoch 33, Batch 1040, LR 0.000006 Loss 4.523688, Accuracy 89.441%\n",
      "Epoch 33, Batch 1041, LR 0.000006 Loss 4.523214, Accuracy 89.444%\n",
      "Epoch 33, Batch 1042, LR 0.000006 Loss 4.522831, Accuracy 89.446%\n",
      "Epoch 33, Batch 1043, LR 0.000006 Loss 4.522958, Accuracy 89.445%\n",
      "Epoch 33, Batch 1044, LR 0.000006 Loss 4.523797, Accuracy 89.441%\n",
      "Epoch 33, Batch 1045, LR 0.000006 Loss 4.523926, Accuracy 89.442%\n",
      "Epoch 33, Batch 1046, LR 0.000006 Loss 4.523340, Accuracy 89.444%\n",
      "Epoch 33, Batch 1047, LR 0.000006 Loss 4.523329, Accuracy 89.443%\n",
      "Epoch 33, Loss (train set) 4.523329, Accuracy (train set) 89.443%\n",
      "Epoch 34, Batch 1, LR 0.000006 Loss 4.145025, Accuracy 92.188%\n",
      "Epoch 34, Batch 2, LR 0.000006 Loss 4.214675, Accuracy 91.797%\n",
      "Epoch 34, Batch 3, LR 0.000006 Loss 4.305057, Accuracy 91.667%\n",
      "Epoch 34, Batch 4, LR 0.000006 Loss 4.559095, Accuracy 89.648%\n",
      "Epoch 34, Batch 5, LR 0.000006 Loss 4.494325, Accuracy 90.156%\n",
      "Epoch 34, Batch 6, LR 0.000006 Loss 4.580033, Accuracy 89.453%\n",
      "Epoch 34, Batch 7, LR 0.000006 Loss 4.529271, Accuracy 90.067%\n",
      "Epoch 34, Batch 8, LR 0.000006 Loss 4.623559, Accuracy 90.137%\n",
      "Epoch 34, Batch 9, LR 0.000006 Loss 4.687270, Accuracy 90.017%\n",
      "Epoch 34, Batch 10, LR 0.000006 Loss 4.720532, Accuracy 89.531%\n",
      "Epoch 34, Batch 11, LR 0.000006 Loss 4.709836, Accuracy 89.489%\n",
      "Epoch 34, Batch 12, LR 0.000006 Loss 4.648303, Accuracy 89.388%\n",
      "Epoch 34, Batch 13, LR 0.000006 Loss 4.638584, Accuracy 89.183%\n",
      "Epoch 34, Batch 14, LR 0.000006 Loss 4.638186, Accuracy 89.342%\n",
      "Epoch 34, Batch 15, LR 0.000006 Loss 4.633866, Accuracy 89.219%\n",
      "Epoch 34, Batch 16, LR 0.000006 Loss 4.593248, Accuracy 89.502%\n",
      "Epoch 34, Batch 17, LR 0.000006 Loss 4.545853, Accuracy 89.522%\n",
      "Epoch 34, Batch 18, LR 0.000006 Loss 4.517226, Accuracy 89.540%\n",
      "Epoch 34, Batch 19, LR 0.000006 Loss 4.536773, Accuracy 89.268%\n",
      "Epoch 34, Batch 20, LR 0.000006 Loss 4.567000, Accuracy 89.141%\n",
      "Epoch 34, Batch 21, LR 0.000006 Loss 4.588923, Accuracy 89.137%\n",
      "Epoch 34, Batch 22, LR 0.000006 Loss 4.580267, Accuracy 89.027%\n",
      "Epoch 34, Batch 23, LR 0.000006 Loss 4.566848, Accuracy 89.029%\n",
      "Epoch 34, Batch 24, LR 0.000006 Loss 4.560044, Accuracy 89.030%\n",
      "Epoch 34, Batch 25, LR 0.000006 Loss 4.545836, Accuracy 89.125%\n",
      "Epoch 34, Batch 26, LR 0.000006 Loss 4.535450, Accuracy 88.942%\n",
      "Epoch 34, Batch 27, LR 0.000006 Loss 4.539033, Accuracy 88.947%\n",
      "Epoch 34, Batch 28, LR 0.000006 Loss 4.557418, Accuracy 88.923%\n",
      "Epoch 34, Batch 29, LR 0.000006 Loss 4.542032, Accuracy 88.982%\n",
      "Epoch 34, Batch 30, LR 0.000006 Loss 4.552015, Accuracy 88.984%\n",
      "Epoch 34, Batch 31, LR 0.000006 Loss 4.526477, Accuracy 89.138%\n",
      "Epoch 34, Batch 32, LR 0.000006 Loss 4.552129, Accuracy 89.087%\n",
      "Epoch 34, Batch 33, LR 0.000006 Loss 4.541180, Accuracy 89.228%\n",
      "Epoch 34, Batch 34, LR 0.000006 Loss 4.539658, Accuracy 89.223%\n",
      "Epoch 34, Batch 35, LR 0.000006 Loss 4.524855, Accuracy 89.353%\n",
      "Epoch 34, Batch 36, LR 0.000006 Loss 4.527432, Accuracy 89.345%\n",
      "Epoch 34, Batch 37, LR 0.000006 Loss 4.540140, Accuracy 89.274%\n",
      "Epoch 34, Batch 38, LR 0.000006 Loss 4.540386, Accuracy 89.289%\n",
      "Epoch 34, Batch 39, LR 0.000006 Loss 4.530670, Accuracy 89.363%\n",
      "Epoch 34, Batch 40, LR 0.000006 Loss 4.532856, Accuracy 89.336%\n",
      "Epoch 34, Batch 41, LR 0.000006 Loss 4.542760, Accuracy 89.272%\n",
      "Epoch 34, Batch 42, LR 0.000006 Loss 4.550038, Accuracy 89.304%\n",
      "Epoch 34, Batch 43, LR 0.000006 Loss 4.558829, Accuracy 89.335%\n",
      "Epoch 34, Batch 44, LR 0.000006 Loss 4.543514, Accuracy 89.418%\n",
      "Epoch 34, Batch 45, LR 0.000006 Loss 4.545702, Accuracy 89.392%\n",
      "Epoch 34, Batch 46, LR 0.000006 Loss 4.547457, Accuracy 89.402%\n",
      "Epoch 34, Batch 47, LR 0.000006 Loss 4.536920, Accuracy 89.445%\n",
      "Epoch 34, Batch 48, LR 0.000006 Loss 4.546391, Accuracy 89.388%\n",
      "Epoch 34, Batch 49, LR 0.000006 Loss 4.544231, Accuracy 89.334%\n",
      "Epoch 34, Batch 50, LR 0.000006 Loss 4.542205, Accuracy 89.391%\n",
      "Epoch 34, Batch 51, LR 0.000006 Loss 4.546118, Accuracy 89.338%\n",
      "Epoch 34, Batch 52, LR 0.000006 Loss 4.531703, Accuracy 89.378%\n",
      "Epoch 34, Batch 53, LR 0.000006 Loss 4.533472, Accuracy 89.387%\n",
      "Epoch 34, Batch 54, LR 0.000006 Loss 4.534677, Accuracy 89.381%\n",
      "Epoch 34, Batch 55, LR 0.000006 Loss 4.548532, Accuracy 89.304%\n",
      "Epoch 34, Batch 56, LR 0.000006 Loss 4.546872, Accuracy 89.355%\n",
      "Epoch 34, Batch 57, LR 0.000006 Loss 4.543503, Accuracy 89.337%\n",
      "Epoch 34, Batch 58, LR 0.000006 Loss 4.536338, Accuracy 89.386%\n",
      "Epoch 34, Batch 59, LR 0.000006 Loss 4.537185, Accuracy 89.394%\n",
      "Epoch 34, Batch 60, LR 0.000006 Loss 4.546137, Accuracy 89.375%\n",
      "Epoch 34, Batch 61, LR 0.000006 Loss 4.550871, Accuracy 89.370%\n",
      "Epoch 34, Batch 62, LR 0.000006 Loss 4.551590, Accuracy 89.365%\n",
      "Epoch 34, Batch 63, LR 0.000006 Loss 4.537723, Accuracy 89.447%\n",
      "Epoch 34, Batch 64, LR 0.000006 Loss 4.537948, Accuracy 89.429%\n",
      "Epoch 34, Batch 65, LR 0.000006 Loss 4.526100, Accuracy 89.495%\n",
      "Epoch 34, Batch 66, LR 0.000006 Loss 4.525639, Accuracy 89.453%\n",
      "Epoch 34, Batch 67, LR 0.000006 Loss 4.515747, Accuracy 89.506%\n",
      "Epoch 34, Batch 68, LR 0.000006 Loss 4.513908, Accuracy 89.499%\n",
      "Epoch 34, Batch 69, LR 0.000006 Loss 4.507559, Accuracy 89.515%\n",
      "Epoch 34, Batch 70, LR 0.000006 Loss 4.514890, Accuracy 89.475%\n",
      "Epoch 34, Batch 71, LR 0.000006 Loss 4.508750, Accuracy 89.503%\n",
      "Epoch 34, Batch 72, LR 0.000006 Loss 4.506602, Accuracy 89.486%\n",
      "Epoch 34, Batch 73, LR 0.000006 Loss 4.503976, Accuracy 89.491%\n",
      "Epoch 34, Batch 74, LR 0.000006 Loss 4.502987, Accuracy 89.559%\n",
      "Epoch 34, Batch 75, LR 0.000006 Loss 4.494260, Accuracy 89.604%\n",
      "Epoch 34, Batch 76, LR 0.000006 Loss 4.495543, Accuracy 89.566%\n",
      "Epoch 34, Batch 77, LR 0.000006 Loss 4.508097, Accuracy 89.539%\n",
      "Epoch 34, Batch 78, LR 0.000006 Loss 4.511768, Accuracy 89.533%\n",
      "Epoch 34, Batch 79, LR 0.000006 Loss 4.515275, Accuracy 89.498%\n",
      "Epoch 34, Batch 80, LR 0.000006 Loss 4.508190, Accuracy 89.492%\n",
      "Epoch 34, Batch 81, LR 0.000006 Loss 4.504949, Accuracy 89.477%\n",
      "Epoch 34, Batch 82, LR 0.000006 Loss 4.494265, Accuracy 89.520%\n",
      "Epoch 34, Batch 83, LR 0.000006 Loss 4.491491, Accuracy 89.533%\n",
      "Epoch 34, Batch 84, LR 0.000006 Loss 4.493643, Accuracy 89.593%\n",
      "Epoch 34, Batch 85, LR 0.000006 Loss 4.492687, Accuracy 89.596%\n",
      "Epoch 34, Batch 86, LR 0.000006 Loss 4.491825, Accuracy 89.526%\n",
      "Epoch 34, Batch 87, LR 0.000006 Loss 4.493774, Accuracy 89.511%\n",
      "Epoch 34, Batch 88, LR 0.000006 Loss 4.492762, Accuracy 89.489%\n",
      "Epoch 34, Batch 89, LR 0.000006 Loss 4.497416, Accuracy 89.458%\n",
      "Epoch 34, Batch 90, LR 0.000006 Loss 4.501359, Accuracy 89.436%\n",
      "Epoch 34, Batch 91, LR 0.000006 Loss 4.509303, Accuracy 89.406%\n",
      "Epoch 34, Batch 92, LR 0.000006 Loss 4.508524, Accuracy 89.402%\n",
      "Epoch 34, Batch 93, LR 0.000006 Loss 4.512921, Accuracy 89.323%\n",
      "Epoch 34, Batch 94, LR 0.000006 Loss 4.512150, Accuracy 89.320%\n",
      "Epoch 34, Batch 95, LR 0.000006 Loss 4.515427, Accuracy 89.276%\n",
      "Epoch 34, Batch 96, LR 0.000006 Loss 4.509086, Accuracy 89.299%\n",
      "Epoch 34, Batch 97, LR 0.000006 Loss 4.506119, Accuracy 89.328%\n",
      "Epoch 34, Batch 98, LR 0.000006 Loss 4.507627, Accuracy 89.318%\n",
      "Epoch 34, Batch 99, LR 0.000006 Loss 4.501682, Accuracy 89.354%\n",
      "Epoch 34, Batch 100, LR 0.000006 Loss 4.502161, Accuracy 89.359%\n",
      "Epoch 34, Batch 101, LR 0.000006 Loss 4.502699, Accuracy 89.356%\n",
      "Epoch 34, Batch 102, LR 0.000006 Loss 4.504802, Accuracy 89.338%\n",
      "Epoch 34, Batch 103, LR 0.000006 Loss 4.504643, Accuracy 89.373%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 104, LR 0.000006 Loss 4.496689, Accuracy 89.401%\n",
      "Epoch 34, Batch 105, LR 0.000006 Loss 4.494830, Accuracy 89.405%\n",
      "Epoch 34, Batch 106, LR 0.000006 Loss 4.494753, Accuracy 89.379%\n",
      "Epoch 34, Batch 107, LR 0.000006 Loss 4.488252, Accuracy 89.413%\n",
      "Epoch 34, Batch 108, LR 0.000006 Loss 4.488940, Accuracy 89.446%\n",
      "Epoch 34, Batch 109, LR 0.000006 Loss 4.490148, Accuracy 89.435%\n",
      "Epoch 34, Batch 110, LR 0.000006 Loss 4.483819, Accuracy 89.496%\n",
      "Epoch 34, Batch 111, LR 0.000006 Loss 4.481548, Accuracy 89.506%\n",
      "Epoch 34, Batch 112, LR 0.000006 Loss 4.482267, Accuracy 89.467%\n",
      "Epoch 34, Batch 113, LR 0.000006 Loss 4.485422, Accuracy 89.450%\n",
      "Epoch 34, Batch 114, LR 0.000006 Loss 4.476101, Accuracy 89.494%\n",
      "Epoch 34, Batch 115, LR 0.000006 Loss 4.473209, Accuracy 89.490%\n",
      "Epoch 34, Batch 116, LR 0.000006 Loss 4.475748, Accuracy 89.500%\n",
      "Epoch 34, Batch 117, LR 0.000006 Loss 4.475529, Accuracy 89.523%\n",
      "Epoch 34, Batch 118, LR 0.000006 Loss 4.480644, Accuracy 89.493%\n",
      "Epoch 34, Batch 119, LR 0.000006 Loss 4.475266, Accuracy 89.509%\n",
      "Epoch 34, Batch 120, LR 0.000006 Loss 4.476109, Accuracy 89.525%\n",
      "Epoch 34, Batch 121, LR 0.000006 Loss 4.483966, Accuracy 89.489%\n",
      "Epoch 34, Batch 122, LR 0.000006 Loss 4.487489, Accuracy 89.492%\n",
      "Epoch 34, Batch 123, LR 0.000006 Loss 4.485949, Accuracy 89.494%\n",
      "Epoch 34, Batch 124, LR 0.000006 Loss 4.483137, Accuracy 89.491%\n",
      "Epoch 34, Batch 125, LR 0.000006 Loss 4.481786, Accuracy 89.481%\n",
      "Epoch 34, Batch 126, LR 0.000006 Loss 4.478797, Accuracy 89.484%\n",
      "Epoch 34, Batch 127, LR 0.000006 Loss 4.481075, Accuracy 89.487%\n",
      "Epoch 34, Batch 128, LR 0.000006 Loss 4.483435, Accuracy 89.459%\n",
      "Epoch 34, Batch 129, LR 0.000006 Loss 4.479127, Accuracy 89.486%\n",
      "Epoch 34, Batch 130, LR 0.000006 Loss 4.477631, Accuracy 89.489%\n",
      "Epoch 34, Batch 131, LR 0.000006 Loss 4.481979, Accuracy 89.504%\n",
      "Epoch 34, Batch 132, LR 0.000006 Loss 4.477818, Accuracy 89.518%\n",
      "Epoch 34, Batch 133, LR 0.000006 Loss 4.474412, Accuracy 89.532%\n",
      "Epoch 34, Batch 134, LR 0.000006 Loss 4.474541, Accuracy 89.517%\n",
      "Epoch 34, Batch 135, LR 0.000006 Loss 4.475790, Accuracy 89.531%\n",
      "Epoch 34, Batch 136, LR 0.000006 Loss 4.476384, Accuracy 89.534%\n",
      "Epoch 34, Batch 137, LR 0.000006 Loss 4.478592, Accuracy 89.519%\n",
      "Epoch 34, Batch 138, LR 0.000006 Loss 4.477219, Accuracy 89.544%\n",
      "Epoch 34, Batch 139, LR 0.000006 Loss 4.475937, Accuracy 89.540%\n",
      "Epoch 34, Batch 140, LR 0.000006 Loss 4.471954, Accuracy 89.565%\n",
      "Epoch 34, Batch 141, LR 0.000006 Loss 4.472606, Accuracy 89.545%\n",
      "Epoch 34, Batch 142, LR 0.000006 Loss 4.468077, Accuracy 89.569%\n",
      "Epoch 34, Batch 143, LR 0.000006 Loss 4.472620, Accuracy 89.510%\n",
      "Epoch 34, Batch 144, LR 0.000006 Loss 4.470346, Accuracy 89.524%\n",
      "Epoch 34, Batch 145, LR 0.000006 Loss 4.468890, Accuracy 89.520%\n",
      "Epoch 34, Batch 146, LR 0.000006 Loss 4.469564, Accuracy 89.528%\n",
      "Epoch 34, Batch 147, LR 0.000006 Loss 4.463090, Accuracy 89.551%\n",
      "Epoch 34, Batch 148, LR 0.000006 Loss 4.458974, Accuracy 89.575%\n",
      "Epoch 34, Batch 149, LR 0.000006 Loss 4.461653, Accuracy 89.587%\n",
      "Epoch 34, Batch 150, LR 0.000006 Loss 4.462288, Accuracy 89.573%\n",
      "Epoch 34, Batch 151, LR 0.000006 Loss 4.461066, Accuracy 89.575%\n",
      "Epoch 34, Batch 152, LR 0.000006 Loss 4.464988, Accuracy 89.576%\n",
      "Epoch 34, Batch 153, LR 0.000006 Loss 4.467355, Accuracy 89.558%\n",
      "Epoch 34, Batch 154, LR 0.000006 Loss 4.463856, Accuracy 89.580%\n",
      "Epoch 34, Batch 155, LR 0.000006 Loss 4.462096, Accuracy 89.587%\n",
      "Epoch 34, Batch 156, LR 0.000006 Loss 4.458970, Accuracy 89.603%\n",
      "Epoch 34, Batch 157, LR 0.000006 Loss 4.456751, Accuracy 89.605%\n",
      "Epoch 34, Batch 158, LR 0.000006 Loss 4.455216, Accuracy 89.621%\n",
      "Epoch 34, Batch 159, LR 0.000006 Loss 4.454761, Accuracy 89.647%\n",
      "Epoch 34, Batch 160, LR 0.000006 Loss 4.456726, Accuracy 89.624%\n",
      "Epoch 34, Batch 161, LR 0.000006 Loss 4.461143, Accuracy 89.596%\n",
      "Epoch 34, Batch 162, LR 0.000006 Loss 4.463516, Accuracy 89.583%\n",
      "Epoch 34, Batch 163, LR 0.000006 Loss 4.465241, Accuracy 89.537%\n",
      "Epoch 34, Batch 164, LR 0.000006 Loss 4.468261, Accuracy 89.520%\n",
      "Epoch 34, Batch 165, LR 0.000006 Loss 4.468881, Accuracy 89.522%\n",
      "Epoch 34, Batch 166, LR 0.000006 Loss 4.467766, Accuracy 89.514%\n",
      "Epoch 34, Batch 167, LR 0.000006 Loss 4.463220, Accuracy 89.530%\n",
      "Epoch 34, Batch 168, LR 0.000006 Loss 4.462359, Accuracy 89.528%\n",
      "Epoch 34, Batch 169, LR 0.000006 Loss 4.459513, Accuracy 89.534%\n",
      "Epoch 34, Batch 170, LR 0.000006 Loss 4.457894, Accuracy 89.522%\n",
      "Epoch 34, Batch 171, LR 0.000006 Loss 4.458842, Accuracy 89.510%\n",
      "Epoch 34, Batch 172, LR 0.000006 Loss 4.459710, Accuracy 89.517%\n",
      "Epoch 34, Batch 173, LR 0.000006 Loss 4.460048, Accuracy 89.505%\n",
      "Epoch 34, Batch 174, LR 0.000006 Loss 4.459787, Accuracy 89.494%\n",
      "Epoch 34, Batch 175, LR 0.000006 Loss 4.457411, Accuracy 89.513%\n",
      "Epoch 34, Batch 176, LR 0.000006 Loss 4.460907, Accuracy 89.489%\n",
      "Epoch 34, Batch 177, LR 0.000006 Loss 4.462361, Accuracy 89.486%\n",
      "Epoch 34, Batch 178, LR 0.000006 Loss 4.464281, Accuracy 89.484%\n",
      "Epoch 34, Batch 179, LR 0.000006 Loss 4.460926, Accuracy 89.486%\n",
      "Epoch 34, Batch 180, LR 0.000006 Loss 4.466474, Accuracy 89.488%\n",
      "Epoch 34, Batch 181, LR 0.000006 Loss 4.467894, Accuracy 89.503%\n",
      "Epoch 34, Batch 182, LR 0.000006 Loss 4.466224, Accuracy 89.518%\n",
      "Epoch 34, Batch 183, LR 0.000006 Loss 4.469525, Accuracy 89.506%\n",
      "Epoch 34, Batch 184, LR 0.000006 Loss 4.469358, Accuracy 89.517%\n",
      "Epoch 34, Batch 185, LR 0.000006 Loss 4.469960, Accuracy 89.523%\n",
      "Epoch 34, Batch 186, LR 0.000006 Loss 4.469146, Accuracy 89.525%\n",
      "Epoch 34, Batch 187, LR 0.000006 Loss 4.466512, Accuracy 89.535%\n",
      "Epoch 34, Batch 188, LR 0.000006 Loss 4.465937, Accuracy 89.528%\n",
      "Epoch 34, Batch 189, LR 0.000006 Loss 4.465995, Accuracy 89.546%\n",
      "Epoch 34, Batch 190, LR 0.000006 Loss 4.462581, Accuracy 89.568%\n",
      "Epoch 34, Batch 191, LR 0.000006 Loss 4.466243, Accuracy 89.553%\n",
      "Epoch 34, Batch 192, LR 0.000006 Loss 4.469738, Accuracy 89.551%\n",
      "Epoch 34, Batch 193, LR 0.000006 Loss 4.470370, Accuracy 89.548%\n",
      "Epoch 34, Batch 194, LR 0.000006 Loss 4.472772, Accuracy 89.546%\n",
      "Epoch 34, Batch 195, LR 0.000006 Loss 4.470427, Accuracy 89.559%\n",
      "Epoch 34, Batch 196, LR 0.000006 Loss 4.470419, Accuracy 89.569%\n",
      "Epoch 34, Batch 197, LR 0.000006 Loss 4.472071, Accuracy 89.570%\n",
      "Epoch 34, Batch 198, LR 0.000006 Loss 4.472043, Accuracy 89.564%\n",
      "Epoch 34, Batch 199, LR 0.000006 Loss 4.471635, Accuracy 89.549%\n",
      "Epoch 34, Batch 200, LR 0.000006 Loss 4.471472, Accuracy 89.547%\n",
      "Epoch 34, Batch 201, LR 0.000006 Loss 4.474740, Accuracy 89.533%\n",
      "Epoch 34, Batch 202, LR 0.000006 Loss 4.475187, Accuracy 89.530%\n",
      "Epoch 34, Batch 203, LR 0.000006 Loss 4.477441, Accuracy 89.497%\n",
      "Epoch 34, Batch 204, LR 0.000006 Loss 4.474567, Accuracy 89.511%\n",
      "Epoch 34, Batch 205, LR 0.000006 Loss 4.472642, Accuracy 89.516%\n",
      "Epoch 34, Batch 206, LR 0.000006 Loss 4.471190, Accuracy 89.537%\n",
      "Epoch 34, Batch 207, LR 0.000006 Loss 4.471268, Accuracy 89.549%\n",
      "Epoch 34, Batch 208, LR 0.000006 Loss 4.473970, Accuracy 89.517%\n",
      "Epoch 34, Batch 209, LR 0.000006 Loss 4.475427, Accuracy 89.511%\n",
      "Epoch 34, Batch 210, LR 0.000006 Loss 4.478865, Accuracy 89.483%\n",
      "Epoch 34, Batch 211, LR 0.000006 Loss 4.477489, Accuracy 89.481%\n",
      "Epoch 34, Batch 212, LR 0.000006 Loss 4.478233, Accuracy 89.486%\n",
      "Epoch 34, Batch 213, LR 0.000006 Loss 4.480473, Accuracy 89.462%\n",
      "Epoch 34, Batch 214, LR 0.000006 Loss 4.480728, Accuracy 89.464%\n",
      "Epoch 34, Batch 215, LR 0.000006 Loss 4.480268, Accuracy 89.466%\n",
      "Epoch 34, Batch 216, LR 0.000006 Loss 4.480016, Accuracy 89.471%\n",
      "Epoch 34, Batch 217, LR 0.000006 Loss 4.476334, Accuracy 89.487%\n",
      "Epoch 34, Batch 218, LR 0.000006 Loss 4.476623, Accuracy 89.475%\n",
      "Epoch 34, Batch 219, LR 0.000006 Loss 4.477435, Accuracy 89.466%\n",
      "Epoch 34, Batch 220, LR 0.000006 Loss 4.475547, Accuracy 89.457%\n",
      "Epoch 34, Batch 221, LR 0.000006 Loss 4.473753, Accuracy 89.469%\n",
      "Epoch 34, Batch 222, LR 0.000006 Loss 4.475988, Accuracy 89.467%\n",
      "Epoch 34, Batch 223, LR 0.000006 Loss 4.477059, Accuracy 89.465%\n",
      "Epoch 34, Batch 224, LR 0.000006 Loss 4.476976, Accuracy 89.474%\n",
      "Epoch 34, Batch 225, LR 0.000006 Loss 4.474493, Accuracy 89.483%\n",
      "Epoch 34, Batch 226, LR 0.000006 Loss 4.474102, Accuracy 89.484%\n",
      "Epoch 34, Batch 227, LR 0.000006 Loss 4.472307, Accuracy 89.489%\n",
      "Epoch 34, Batch 228, LR 0.000006 Loss 4.474756, Accuracy 89.470%\n",
      "Epoch 34, Batch 229, LR 0.000006 Loss 4.477442, Accuracy 89.438%\n",
      "Epoch 34, Batch 230, LR 0.000006 Loss 4.478170, Accuracy 89.423%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 231, LR 0.000006 Loss 4.478040, Accuracy 89.435%\n",
      "Epoch 34, Batch 232, LR 0.000006 Loss 4.479886, Accuracy 89.430%\n",
      "Epoch 34, Batch 233, LR 0.000006 Loss 4.481201, Accuracy 89.405%\n",
      "Epoch 34, Batch 234, LR 0.000006 Loss 4.479537, Accuracy 89.410%\n",
      "Epoch 34, Batch 235, LR 0.000006 Loss 4.483722, Accuracy 89.382%\n",
      "Epoch 34, Batch 236, LR 0.000006 Loss 4.485451, Accuracy 89.384%\n",
      "Epoch 34, Batch 237, LR 0.000006 Loss 4.487436, Accuracy 89.379%\n",
      "Epoch 34, Batch 238, LR 0.000006 Loss 4.489387, Accuracy 89.378%\n",
      "Epoch 34, Batch 239, LR 0.000006 Loss 4.490077, Accuracy 89.363%\n",
      "Epoch 34, Batch 240, LR 0.000006 Loss 4.489713, Accuracy 89.362%\n",
      "Epoch 34, Batch 241, LR 0.000006 Loss 4.487028, Accuracy 89.370%\n",
      "Epoch 34, Batch 242, LR 0.000006 Loss 4.487210, Accuracy 89.376%\n",
      "Epoch 34, Batch 243, LR 0.000006 Loss 4.490220, Accuracy 89.355%\n",
      "Epoch 34, Batch 244, LR 0.000006 Loss 4.486707, Accuracy 89.376%\n",
      "Epoch 34, Batch 245, LR 0.000006 Loss 4.487340, Accuracy 89.381%\n",
      "Epoch 34, Batch 246, LR 0.000006 Loss 4.485908, Accuracy 89.399%\n",
      "Epoch 34, Batch 247, LR 0.000006 Loss 4.484766, Accuracy 89.391%\n",
      "Epoch 34, Batch 248, LR 0.000006 Loss 4.483523, Accuracy 89.403%\n",
      "Epoch 34, Batch 249, LR 0.000006 Loss 4.480943, Accuracy 89.414%\n",
      "Epoch 34, Batch 250, LR 0.000006 Loss 4.481398, Accuracy 89.419%\n",
      "Epoch 34, Batch 251, LR 0.000006 Loss 4.478231, Accuracy 89.445%\n",
      "Epoch 34, Batch 252, LR 0.000006 Loss 4.480870, Accuracy 89.428%\n",
      "Epoch 34, Batch 253, LR 0.000006 Loss 4.483426, Accuracy 89.405%\n",
      "Epoch 34, Batch 254, LR 0.000006 Loss 4.484270, Accuracy 89.410%\n",
      "Epoch 34, Batch 255, LR 0.000006 Loss 4.483385, Accuracy 89.415%\n",
      "Epoch 34, Batch 256, LR 0.000006 Loss 4.481457, Accuracy 89.423%\n",
      "Epoch 34, Batch 257, LR 0.000006 Loss 4.481574, Accuracy 89.433%\n",
      "Epoch 34, Batch 258, LR 0.000006 Loss 4.479921, Accuracy 89.441%\n",
      "Epoch 34, Batch 259, LR 0.000006 Loss 4.481047, Accuracy 89.424%\n",
      "Epoch 34, Batch 260, LR 0.000006 Loss 4.479987, Accuracy 89.438%\n",
      "Epoch 34, Batch 261, LR 0.000006 Loss 4.477287, Accuracy 89.458%\n",
      "Epoch 34, Batch 262, LR 0.000006 Loss 4.475880, Accuracy 89.456%\n",
      "Epoch 34, Batch 263, LR 0.000006 Loss 4.474227, Accuracy 89.464%\n",
      "Epoch 34, Batch 264, LR 0.000006 Loss 4.474803, Accuracy 89.453%\n",
      "Epoch 34, Batch 265, LR 0.000006 Loss 4.474006, Accuracy 89.460%\n",
      "Epoch 34, Batch 266, LR 0.000006 Loss 4.475476, Accuracy 89.441%\n",
      "Epoch 34, Batch 267, LR 0.000006 Loss 4.477267, Accuracy 89.419%\n",
      "Epoch 34, Batch 268, LR 0.000006 Loss 4.477473, Accuracy 89.418%\n",
      "Epoch 34, Batch 269, LR 0.000006 Loss 4.477792, Accuracy 89.420%\n",
      "Epoch 34, Batch 270, LR 0.000006 Loss 4.479899, Accuracy 89.410%\n",
      "Epoch 34, Batch 271, LR 0.000006 Loss 4.479903, Accuracy 89.426%\n",
      "Epoch 34, Batch 272, LR 0.000006 Loss 4.477290, Accuracy 89.436%\n",
      "Epoch 34, Batch 273, LR 0.000006 Loss 4.479760, Accuracy 89.432%\n",
      "Epoch 34, Batch 274, LR 0.000006 Loss 4.482306, Accuracy 89.416%\n",
      "Epoch 34, Batch 275, LR 0.000006 Loss 4.483436, Accuracy 89.423%\n",
      "Epoch 34, Batch 276, LR 0.000006 Loss 4.481677, Accuracy 89.442%\n",
      "Epoch 34, Batch 277, LR 0.000006 Loss 4.481993, Accuracy 89.446%\n",
      "Epoch 34, Batch 278, LR 0.000006 Loss 4.486527, Accuracy 89.431%\n",
      "Epoch 34, Batch 279, LR 0.000006 Loss 4.483836, Accuracy 89.438%\n",
      "Epoch 34, Batch 280, LR 0.000006 Loss 4.481407, Accuracy 89.448%\n",
      "Epoch 34, Batch 281, LR 0.000006 Loss 4.479763, Accuracy 89.452%\n",
      "Epoch 34, Batch 282, LR 0.000006 Loss 4.476328, Accuracy 89.464%\n",
      "Epoch 34, Batch 283, LR 0.000006 Loss 4.475558, Accuracy 89.474%\n",
      "Epoch 34, Batch 284, LR 0.000006 Loss 4.476092, Accuracy 89.470%\n",
      "Epoch 34, Batch 285, LR 0.000006 Loss 4.475345, Accuracy 89.465%\n",
      "Epoch 34, Batch 286, LR 0.000006 Loss 4.475316, Accuracy 89.470%\n",
      "Epoch 34, Batch 287, LR 0.000005 Loss 4.475314, Accuracy 89.465%\n",
      "Epoch 34, Batch 288, LR 0.000005 Loss 4.475273, Accuracy 89.461%\n",
      "Epoch 34, Batch 289, LR 0.000005 Loss 4.473219, Accuracy 89.476%\n",
      "Epoch 34, Batch 290, LR 0.000005 Loss 4.471862, Accuracy 89.483%\n",
      "Epoch 34, Batch 291, LR 0.000005 Loss 4.473600, Accuracy 89.463%\n",
      "Epoch 34, Batch 292, LR 0.000005 Loss 4.473958, Accuracy 89.461%\n",
      "Epoch 34, Batch 293, LR 0.000005 Loss 4.472972, Accuracy 89.462%\n",
      "Epoch 34, Batch 294, LR 0.000005 Loss 4.474833, Accuracy 89.440%\n",
      "Epoch 34, Batch 295, LR 0.000005 Loss 4.475666, Accuracy 89.436%\n",
      "Epoch 34, Batch 296, LR 0.000005 Loss 4.477348, Accuracy 89.427%\n",
      "Epoch 34, Batch 297, LR 0.000005 Loss 4.480749, Accuracy 89.404%\n",
      "Epoch 34, Batch 298, LR 0.000005 Loss 4.480787, Accuracy 89.409%\n",
      "Epoch 34, Batch 299, LR 0.000005 Loss 4.481629, Accuracy 89.405%\n",
      "Epoch 34, Batch 300, LR 0.000005 Loss 4.481714, Accuracy 89.396%\n",
      "Epoch 34, Batch 301, LR 0.000005 Loss 4.481211, Accuracy 89.397%\n",
      "Epoch 34, Batch 302, LR 0.000005 Loss 4.481348, Accuracy 89.386%\n",
      "Epoch 34, Batch 303, LR 0.000005 Loss 4.479501, Accuracy 89.405%\n",
      "Epoch 34, Batch 304, LR 0.000005 Loss 4.481151, Accuracy 89.391%\n",
      "Epoch 34, Batch 305, LR 0.000005 Loss 4.480378, Accuracy 89.401%\n",
      "Epoch 34, Batch 306, LR 0.000005 Loss 4.480477, Accuracy 89.400%\n",
      "Epoch 34, Batch 307, LR 0.000005 Loss 4.479121, Accuracy 89.401%\n",
      "Epoch 34, Batch 308, LR 0.000005 Loss 4.478817, Accuracy 89.397%\n",
      "Epoch 34, Batch 309, LR 0.000005 Loss 4.477370, Accuracy 89.394%\n",
      "Epoch 34, Batch 310, LR 0.000005 Loss 4.477823, Accuracy 89.398%\n",
      "Epoch 34, Batch 311, LR 0.000005 Loss 4.478190, Accuracy 89.389%\n",
      "Epoch 34, Batch 312, LR 0.000005 Loss 4.476793, Accuracy 89.383%\n",
      "Epoch 34, Batch 313, LR 0.000005 Loss 4.477918, Accuracy 89.387%\n",
      "Epoch 34, Batch 314, LR 0.000005 Loss 4.476258, Accuracy 89.386%\n",
      "Epoch 34, Batch 315, LR 0.000005 Loss 4.479335, Accuracy 89.380%\n",
      "Epoch 34, Batch 316, LR 0.000005 Loss 4.479851, Accuracy 89.381%\n",
      "Epoch 34, Batch 317, LR 0.000005 Loss 4.479242, Accuracy 89.383%\n",
      "Epoch 34, Batch 318, LR 0.000005 Loss 4.480861, Accuracy 89.379%\n",
      "Epoch 34, Batch 319, LR 0.000005 Loss 4.480039, Accuracy 89.391%\n",
      "Epoch 34, Batch 320, LR 0.000005 Loss 4.478351, Accuracy 89.392%\n",
      "Epoch 34, Batch 321, LR 0.000005 Loss 4.476777, Accuracy 89.401%\n",
      "Epoch 34, Batch 322, LR 0.000005 Loss 4.479348, Accuracy 89.392%\n",
      "Epoch 34, Batch 323, LR 0.000005 Loss 4.478669, Accuracy 89.394%\n",
      "Epoch 34, Batch 324, LR 0.000005 Loss 4.478807, Accuracy 89.395%\n",
      "Epoch 34, Batch 325, LR 0.000005 Loss 4.481400, Accuracy 89.382%\n",
      "Epoch 34, Batch 326, LR 0.000005 Loss 4.481784, Accuracy 89.376%\n",
      "Epoch 34, Batch 327, LR 0.000005 Loss 4.483868, Accuracy 89.364%\n",
      "Epoch 34, Batch 328, LR 0.000005 Loss 4.483450, Accuracy 89.367%\n",
      "Epoch 34, Batch 329, LR 0.000005 Loss 4.484481, Accuracy 89.369%\n",
      "Epoch 34, Batch 330, LR 0.000005 Loss 4.485793, Accuracy 89.375%\n",
      "Epoch 34, Batch 331, LR 0.000005 Loss 4.484653, Accuracy 89.376%\n",
      "Epoch 34, Batch 332, LR 0.000005 Loss 4.483274, Accuracy 89.385%\n",
      "Epoch 34, Batch 333, LR 0.000005 Loss 4.483016, Accuracy 89.396%\n",
      "Epoch 34, Batch 334, LR 0.000005 Loss 4.482638, Accuracy 89.402%\n",
      "Epoch 34, Batch 335, LR 0.000005 Loss 4.479218, Accuracy 89.412%\n",
      "Epoch 34, Batch 336, LR 0.000005 Loss 4.479714, Accuracy 89.409%\n",
      "Epoch 34, Batch 337, LR 0.000005 Loss 4.479179, Accuracy 89.415%\n",
      "Epoch 34, Batch 338, LR 0.000005 Loss 4.478767, Accuracy 89.414%\n",
      "Epoch 34, Batch 339, LR 0.000005 Loss 4.477732, Accuracy 89.413%\n",
      "Epoch 34, Batch 340, LR 0.000005 Loss 4.475972, Accuracy 89.416%\n",
      "Epoch 34, Batch 341, LR 0.000005 Loss 4.474828, Accuracy 89.431%\n",
      "Epoch 34, Batch 342, LR 0.000005 Loss 4.472955, Accuracy 89.449%\n",
      "Epoch 34, Batch 343, LR 0.000005 Loss 4.471594, Accuracy 89.447%\n",
      "Epoch 34, Batch 344, LR 0.000005 Loss 4.474263, Accuracy 89.435%\n",
      "Epoch 34, Batch 345, LR 0.000005 Loss 4.473867, Accuracy 89.434%\n",
      "Epoch 34, Batch 346, LR 0.000005 Loss 4.470653, Accuracy 89.444%\n",
      "Epoch 34, Batch 347, LR 0.000005 Loss 4.470900, Accuracy 89.445%\n",
      "Epoch 34, Batch 348, LR 0.000005 Loss 4.469312, Accuracy 89.449%\n",
      "Epoch 34, Batch 349, LR 0.000005 Loss 4.469477, Accuracy 89.445%\n",
      "Epoch 34, Batch 350, LR 0.000005 Loss 4.470854, Accuracy 89.444%\n",
      "Epoch 34, Batch 351, LR 0.000005 Loss 4.471009, Accuracy 89.445%\n",
      "Epoch 34, Batch 352, LR 0.000005 Loss 4.470986, Accuracy 89.455%\n",
      "Epoch 34, Batch 353, LR 0.000005 Loss 4.468520, Accuracy 89.468%\n",
      "Epoch 34, Batch 354, LR 0.000005 Loss 4.465800, Accuracy 89.477%\n",
      "Epoch 34, Batch 355, LR 0.000005 Loss 4.467116, Accuracy 89.478%\n",
      "Epoch 34, Batch 356, LR 0.000005 Loss 4.468556, Accuracy 89.475%\n",
      "Epoch 34, Batch 357, LR 0.000005 Loss 4.469559, Accuracy 89.467%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 358, LR 0.000005 Loss 4.466765, Accuracy 89.475%\n",
      "Epoch 34, Batch 359, LR 0.000005 Loss 4.466742, Accuracy 89.476%\n",
      "Epoch 34, Batch 360, LR 0.000005 Loss 4.467544, Accuracy 89.477%\n",
      "Epoch 34, Batch 361, LR 0.000005 Loss 4.466101, Accuracy 89.491%\n",
      "Epoch 34, Batch 362, LR 0.000005 Loss 4.465702, Accuracy 89.492%\n",
      "Epoch 34, Batch 363, LR 0.000005 Loss 4.465584, Accuracy 89.493%\n",
      "Epoch 34, Batch 364, LR 0.000005 Loss 4.464996, Accuracy 89.502%\n",
      "Epoch 34, Batch 365, LR 0.000005 Loss 4.465958, Accuracy 89.493%\n",
      "Epoch 34, Batch 366, LR 0.000005 Loss 4.466097, Accuracy 89.500%\n",
      "Epoch 34, Batch 367, LR 0.000005 Loss 4.466373, Accuracy 89.507%\n",
      "Epoch 34, Batch 368, LR 0.000005 Loss 4.467702, Accuracy 89.510%\n",
      "Epoch 34, Batch 369, LR 0.000005 Loss 4.469253, Accuracy 89.501%\n",
      "Epoch 34, Batch 370, LR 0.000005 Loss 4.469148, Accuracy 89.508%\n",
      "Epoch 34, Batch 371, LR 0.000005 Loss 4.469684, Accuracy 89.509%\n",
      "Epoch 34, Batch 372, LR 0.000005 Loss 4.468999, Accuracy 89.508%\n",
      "Epoch 34, Batch 373, LR 0.000005 Loss 4.469922, Accuracy 89.507%\n",
      "Epoch 34, Batch 374, LR 0.000005 Loss 4.469612, Accuracy 89.512%\n",
      "Epoch 34, Batch 375, LR 0.000005 Loss 4.470822, Accuracy 89.498%\n",
      "Epoch 34, Batch 376, LR 0.000005 Loss 4.472095, Accuracy 89.495%\n",
      "Epoch 34, Batch 377, LR 0.000005 Loss 4.471699, Accuracy 89.512%\n",
      "Epoch 34, Batch 378, LR 0.000005 Loss 4.474137, Accuracy 89.505%\n",
      "Epoch 34, Batch 379, LR 0.000005 Loss 4.474984, Accuracy 89.495%\n",
      "Epoch 34, Batch 380, LR 0.000005 Loss 4.473432, Accuracy 89.498%\n",
      "Epoch 34, Batch 381, LR 0.000005 Loss 4.472267, Accuracy 89.499%\n",
      "Epoch 34, Batch 382, LR 0.000005 Loss 4.472453, Accuracy 89.508%\n",
      "Epoch 34, Batch 383, LR 0.000005 Loss 4.473605, Accuracy 89.503%\n",
      "Epoch 34, Batch 384, LR 0.000005 Loss 4.472632, Accuracy 89.510%\n",
      "Epoch 34, Batch 385, LR 0.000005 Loss 4.472556, Accuracy 89.511%\n",
      "Epoch 34, Batch 386, LR 0.000005 Loss 4.471272, Accuracy 89.514%\n",
      "Epoch 34, Batch 387, LR 0.000005 Loss 4.469920, Accuracy 89.519%\n",
      "Epoch 34, Batch 388, LR 0.000005 Loss 4.469168, Accuracy 89.526%\n",
      "Epoch 34, Batch 389, LR 0.000005 Loss 4.469637, Accuracy 89.516%\n",
      "Epoch 34, Batch 390, LR 0.000005 Loss 4.468616, Accuracy 89.523%\n",
      "Epoch 34, Batch 391, LR 0.000005 Loss 4.468067, Accuracy 89.524%\n",
      "Epoch 34, Batch 392, LR 0.000005 Loss 4.468975, Accuracy 89.523%\n",
      "Epoch 34, Batch 393, LR 0.000005 Loss 4.467954, Accuracy 89.530%\n",
      "Epoch 34, Batch 394, LR 0.000005 Loss 4.468164, Accuracy 89.536%\n",
      "Epoch 34, Batch 395, LR 0.000005 Loss 4.468549, Accuracy 89.533%\n",
      "Epoch 34, Batch 396, LR 0.000005 Loss 4.469055, Accuracy 89.536%\n",
      "Epoch 34, Batch 397, LR 0.000005 Loss 4.469109, Accuracy 89.535%\n",
      "Epoch 34, Batch 398, LR 0.000005 Loss 4.467544, Accuracy 89.543%\n",
      "Epoch 34, Batch 399, LR 0.000005 Loss 4.467564, Accuracy 89.550%\n",
      "Epoch 34, Batch 400, LR 0.000005 Loss 4.468526, Accuracy 89.541%\n",
      "Epoch 34, Batch 401, LR 0.000005 Loss 4.469656, Accuracy 89.534%\n",
      "Epoch 34, Batch 402, LR 0.000005 Loss 4.468115, Accuracy 89.535%\n",
      "Epoch 34, Batch 403, LR 0.000005 Loss 4.470502, Accuracy 89.526%\n",
      "Epoch 34, Batch 404, LR 0.000005 Loss 4.469973, Accuracy 89.525%\n",
      "Epoch 34, Batch 405, LR 0.000005 Loss 4.469238, Accuracy 89.527%\n",
      "Epoch 34, Batch 406, LR 0.000005 Loss 4.469458, Accuracy 89.515%\n",
      "Epoch 34, Batch 407, LR 0.000005 Loss 4.469344, Accuracy 89.512%\n",
      "Epoch 34, Batch 408, LR 0.000005 Loss 4.469424, Accuracy 89.512%\n",
      "Epoch 34, Batch 409, LR 0.000005 Loss 4.468837, Accuracy 89.509%\n",
      "Epoch 34, Batch 410, LR 0.000005 Loss 4.468149, Accuracy 89.516%\n",
      "Epoch 34, Batch 411, LR 0.000005 Loss 4.465962, Accuracy 89.519%\n",
      "Epoch 34, Batch 412, LR 0.000005 Loss 4.466558, Accuracy 89.523%\n",
      "Epoch 34, Batch 413, LR 0.000005 Loss 4.466805, Accuracy 89.518%\n",
      "Epoch 34, Batch 414, LR 0.000005 Loss 4.466523, Accuracy 89.529%\n",
      "Epoch 34, Batch 415, LR 0.000005 Loss 4.465523, Accuracy 89.529%\n",
      "Epoch 34, Batch 416, LR 0.000005 Loss 4.466256, Accuracy 89.517%\n",
      "Epoch 34, Batch 417, LR 0.000005 Loss 4.465742, Accuracy 89.525%\n",
      "Epoch 34, Batch 418, LR 0.000005 Loss 4.466770, Accuracy 89.520%\n",
      "Epoch 34, Batch 419, LR 0.000005 Loss 4.467515, Accuracy 89.512%\n",
      "Epoch 34, Batch 420, LR 0.000005 Loss 4.467783, Accuracy 89.507%\n",
      "Epoch 34, Batch 421, LR 0.000005 Loss 4.466022, Accuracy 89.513%\n",
      "Epoch 34, Batch 422, LR 0.000005 Loss 4.466342, Accuracy 89.512%\n",
      "Epoch 34, Batch 423, LR 0.000005 Loss 4.467735, Accuracy 89.506%\n",
      "Epoch 34, Batch 424, LR 0.000005 Loss 4.467787, Accuracy 89.503%\n",
      "Epoch 34, Batch 425, LR 0.000005 Loss 4.467265, Accuracy 89.506%\n",
      "Epoch 34, Batch 426, LR 0.000005 Loss 4.468787, Accuracy 89.499%\n",
      "Epoch 34, Batch 427, LR 0.000005 Loss 4.469193, Accuracy 89.505%\n",
      "Epoch 34, Batch 428, LR 0.000005 Loss 4.468934, Accuracy 89.510%\n",
      "Epoch 34, Batch 429, LR 0.000005 Loss 4.469323, Accuracy 89.510%\n",
      "Epoch 34, Batch 430, LR 0.000005 Loss 4.469654, Accuracy 89.506%\n",
      "Epoch 34, Batch 431, LR 0.000005 Loss 4.470458, Accuracy 89.496%\n",
      "Epoch 34, Batch 432, LR 0.000005 Loss 4.471803, Accuracy 89.489%\n",
      "Epoch 34, Batch 433, LR 0.000005 Loss 4.470336, Accuracy 89.497%\n",
      "Epoch 34, Batch 434, LR 0.000005 Loss 4.470320, Accuracy 89.509%\n",
      "Epoch 34, Batch 435, LR 0.000005 Loss 4.471074, Accuracy 89.501%\n",
      "Epoch 34, Batch 436, LR 0.000005 Loss 4.470142, Accuracy 89.507%\n",
      "Epoch 34, Batch 437, LR 0.000005 Loss 4.471551, Accuracy 89.501%\n",
      "Epoch 34, Batch 438, LR 0.000005 Loss 4.472471, Accuracy 89.498%\n",
      "Epoch 34, Batch 439, LR 0.000005 Loss 4.472339, Accuracy 89.497%\n",
      "Epoch 34, Batch 440, LR 0.000005 Loss 4.473904, Accuracy 89.496%\n",
      "Epoch 34, Batch 441, LR 0.000005 Loss 4.473452, Accuracy 89.495%\n",
      "Epoch 34, Batch 442, LR 0.000005 Loss 4.473706, Accuracy 89.496%\n",
      "Epoch 34, Batch 443, LR 0.000005 Loss 4.475190, Accuracy 89.491%\n",
      "Epoch 34, Batch 444, LR 0.000005 Loss 4.475809, Accuracy 89.492%\n",
      "Epoch 34, Batch 445, LR 0.000005 Loss 4.476279, Accuracy 89.494%\n",
      "Epoch 34, Batch 446, LR 0.000005 Loss 4.477331, Accuracy 89.490%\n",
      "Epoch 34, Batch 447, LR 0.000005 Loss 4.478344, Accuracy 89.482%\n",
      "Epoch 34, Batch 448, LR 0.000005 Loss 4.478201, Accuracy 89.479%\n",
      "Epoch 34, Batch 449, LR 0.000005 Loss 4.478887, Accuracy 89.475%\n",
      "Epoch 34, Batch 450, LR 0.000005 Loss 4.478998, Accuracy 89.476%\n",
      "Epoch 34, Batch 451, LR 0.000005 Loss 4.479182, Accuracy 89.477%\n",
      "Epoch 34, Batch 452, LR 0.000005 Loss 4.478352, Accuracy 89.481%\n",
      "Epoch 34, Batch 453, LR 0.000005 Loss 4.478069, Accuracy 89.471%\n",
      "Epoch 34, Batch 454, LR 0.000005 Loss 4.477650, Accuracy 89.474%\n",
      "Epoch 34, Batch 455, LR 0.000005 Loss 4.477254, Accuracy 89.476%\n",
      "Epoch 34, Batch 456, LR 0.000005 Loss 4.477595, Accuracy 89.475%\n",
      "Epoch 34, Batch 457, LR 0.000005 Loss 4.477414, Accuracy 89.486%\n",
      "Epoch 34, Batch 458, LR 0.000005 Loss 4.478439, Accuracy 89.477%\n",
      "Epoch 34, Batch 459, LR 0.000005 Loss 4.478687, Accuracy 89.474%\n",
      "Epoch 34, Batch 460, LR 0.000005 Loss 4.480189, Accuracy 89.470%\n",
      "Epoch 34, Batch 461, LR 0.000005 Loss 4.480000, Accuracy 89.468%\n",
      "Epoch 34, Batch 462, LR 0.000005 Loss 4.479978, Accuracy 89.472%\n",
      "Epoch 34, Batch 463, LR 0.000005 Loss 4.479141, Accuracy 89.471%\n",
      "Epoch 34, Batch 464, LR 0.000005 Loss 4.478997, Accuracy 89.472%\n",
      "Epoch 34, Batch 465, LR 0.000005 Loss 4.478481, Accuracy 89.471%\n",
      "Epoch 34, Batch 466, LR 0.000005 Loss 4.478535, Accuracy 89.477%\n",
      "Epoch 34, Batch 467, LR 0.000005 Loss 4.478817, Accuracy 89.472%\n",
      "Epoch 34, Batch 468, LR 0.000005 Loss 4.479122, Accuracy 89.471%\n",
      "Epoch 34, Batch 469, LR 0.000005 Loss 4.481713, Accuracy 89.464%\n",
      "Epoch 34, Batch 470, LR 0.000005 Loss 4.482036, Accuracy 89.466%\n",
      "Epoch 34, Batch 471, LR 0.000005 Loss 4.481420, Accuracy 89.467%\n",
      "Epoch 34, Batch 472, LR 0.000005 Loss 4.482104, Accuracy 89.466%\n",
      "Epoch 34, Batch 473, LR 0.000005 Loss 4.483752, Accuracy 89.464%\n",
      "Epoch 34, Batch 474, LR 0.000005 Loss 4.483544, Accuracy 89.466%\n",
      "Epoch 34, Batch 475, LR 0.000005 Loss 4.484185, Accuracy 89.469%\n",
      "Epoch 34, Batch 476, LR 0.000005 Loss 4.483521, Accuracy 89.478%\n",
      "Epoch 34, Batch 477, LR 0.000005 Loss 4.482883, Accuracy 89.480%\n",
      "Epoch 34, Batch 478, LR 0.000005 Loss 4.482930, Accuracy 89.473%\n",
      "Epoch 34, Batch 479, LR 0.000005 Loss 4.481623, Accuracy 89.478%\n",
      "Epoch 34, Batch 480, LR 0.000005 Loss 4.480982, Accuracy 89.481%\n",
      "Epoch 34, Batch 481, LR 0.000005 Loss 4.480543, Accuracy 89.486%\n",
      "Epoch 34, Batch 482, LR 0.000005 Loss 4.481054, Accuracy 89.482%\n",
      "Epoch 34, Batch 483, LR 0.000005 Loss 4.479344, Accuracy 89.486%\n",
      "Epoch 34, Batch 484, LR 0.000005 Loss 4.480282, Accuracy 89.489%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 485, LR 0.000005 Loss 4.480108, Accuracy 89.486%\n",
      "Epoch 34, Batch 486, LR 0.000005 Loss 4.480706, Accuracy 89.490%\n",
      "Epoch 34, Batch 487, LR 0.000005 Loss 4.480918, Accuracy 89.497%\n",
      "Epoch 34, Batch 488, LR 0.000005 Loss 4.481338, Accuracy 89.496%\n",
      "Epoch 34, Batch 489, LR 0.000005 Loss 4.481475, Accuracy 89.500%\n",
      "Epoch 34, Batch 490, LR 0.000005 Loss 4.480088, Accuracy 89.504%\n",
      "Epoch 34, Batch 491, LR 0.000005 Loss 4.479862, Accuracy 89.508%\n",
      "Epoch 34, Batch 492, LR 0.000005 Loss 4.479679, Accuracy 89.504%\n",
      "Epoch 34, Batch 493, LR 0.000005 Loss 4.479273, Accuracy 89.500%\n",
      "Epoch 34, Batch 494, LR 0.000005 Loss 4.478194, Accuracy 89.502%\n",
      "Epoch 34, Batch 495, LR 0.000005 Loss 4.477770, Accuracy 89.508%\n",
      "Epoch 34, Batch 496, LR 0.000005 Loss 4.478125, Accuracy 89.510%\n",
      "Epoch 34, Batch 497, LR 0.000005 Loss 4.479304, Accuracy 89.499%\n",
      "Epoch 34, Batch 498, LR 0.000005 Loss 4.479319, Accuracy 89.503%\n",
      "Epoch 34, Batch 499, LR 0.000005 Loss 4.479133, Accuracy 89.507%\n",
      "Epoch 34, Batch 500, LR 0.000005 Loss 4.481189, Accuracy 89.502%\n",
      "Epoch 34, Batch 501, LR 0.000005 Loss 4.481963, Accuracy 89.505%\n",
      "Epoch 34, Batch 502, LR 0.000005 Loss 4.482352, Accuracy 89.503%\n",
      "Epoch 34, Batch 503, LR 0.000005 Loss 4.482175, Accuracy 89.504%\n",
      "Epoch 34, Batch 504, LR 0.000005 Loss 4.482344, Accuracy 89.507%\n",
      "Epoch 34, Batch 505, LR 0.000005 Loss 4.482156, Accuracy 89.505%\n",
      "Epoch 34, Batch 506, LR 0.000005 Loss 4.481327, Accuracy 89.513%\n",
      "Epoch 34, Batch 507, LR 0.000005 Loss 4.483647, Accuracy 89.508%\n",
      "Epoch 34, Batch 508, LR 0.000005 Loss 4.484029, Accuracy 89.501%\n",
      "Epoch 34, Batch 509, LR 0.000005 Loss 4.485075, Accuracy 89.495%\n",
      "Epoch 34, Batch 510, LR 0.000005 Loss 4.484303, Accuracy 89.499%\n",
      "Epoch 34, Batch 511, LR 0.000005 Loss 4.483275, Accuracy 89.498%\n",
      "Epoch 34, Batch 512, LR 0.000005 Loss 4.483172, Accuracy 89.494%\n",
      "Epoch 34, Batch 513, LR 0.000005 Loss 4.484171, Accuracy 89.495%\n",
      "Epoch 34, Batch 514, LR 0.000005 Loss 4.485414, Accuracy 89.487%\n",
      "Epoch 34, Batch 515, LR 0.000005 Loss 4.484128, Accuracy 89.492%\n",
      "Epoch 34, Batch 516, LR 0.000005 Loss 4.484815, Accuracy 89.485%\n",
      "Epoch 34, Batch 517, LR 0.000005 Loss 4.485140, Accuracy 89.483%\n",
      "Epoch 34, Batch 518, LR 0.000005 Loss 4.484522, Accuracy 89.491%\n",
      "Epoch 34, Batch 519, LR 0.000005 Loss 4.484507, Accuracy 89.492%\n",
      "Epoch 34, Batch 520, LR 0.000005 Loss 4.484753, Accuracy 89.488%\n",
      "Epoch 34, Batch 521, LR 0.000005 Loss 4.484631, Accuracy 89.487%\n",
      "Epoch 34, Batch 522, LR 0.000005 Loss 4.486496, Accuracy 89.482%\n",
      "Epoch 34, Batch 523, LR 0.000005 Loss 4.485400, Accuracy 89.482%\n",
      "Epoch 34, Batch 524, LR 0.000005 Loss 4.485561, Accuracy 89.480%\n",
      "Epoch 34, Batch 525, LR 0.000005 Loss 4.485111, Accuracy 89.482%\n",
      "Epoch 34, Batch 526, LR 0.000005 Loss 4.485048, Accuracy 89.484%\n",
      "Epoch 34, Batch 527, LR 0.000005 Loss 4.486920, Accuracy 89.475%\n",
      "Epoch 34, Batch 528, LR 0.000005 Loss 4.488134, Accuracy 89.463%\n",
      "Epoch 34, Batch 529, LR 0.000005 Loss 4.488137, Accuracy 89.466%\n",
      "Epoch 34, Batch 530, LR 0.000005 Loss 4.486913, Accuracy 89.475%\n",
      "Epoch 34, Batch 531, LR 0.000005 Loss 4.488064, Accuracy 89.476%\n",
      "Epoch 34, Batch 532, LR 0.000005 Loss 4.488347, Accuracy 89.471%\n",
      "Epoch 34, Batch 533, LR 0.000005 Loss 4.487448, Accuracy 89.473%\n",
      "Epoch 34, Batch 534, LR 0.000005 Loss 4.487792, Accuracy 89.477%\n",
      "Epoch 34, Batch 535, LR 0.000005 Loss 4.486854, Accuracy 89.477%\n",
      "Epoch 34, Batch 536, LR 0.000005 Loss 4.486179, Accuracy 89.475%\n",
      "Epoch 34, Batch 537, LR 0.000005 Loss 4.485599, Accuracy 89.476%\n",
      "Epoch 34, Batch 538, LR 0.000005 Loss 4.485466, Accuracy 89.472%\n",
      "Epoch 34, Batch 539, LR 0.000005 Loss 4.486420, Accuracy 89.468%\n",
      "Epoch 34, Batch 540, LR 0.000005 Loss 4.485454, Accuracy 89.476%\n",
      "Epoch 34, Batch 541, LR 0.000005 Loss 4.486102, Accuracy 89.473%\n",
      "Epoch 34, Batch 542, LR 0.000005 Loss 4.487031, Accuracy 89.468%\n",
      "Epoch 34, Batch 543, LR 0.000005 Loss 4.486109, Accuracy 89.477%\n",
      "Epoch 34, Batch 544, LR 0.000005 Loss 4.486475, Accuracy 89.478%\n",
      "Epoch 34, Batch 545, LR 0.000005 Loss 4.487021, Accuracy 89.472%\n",
      "Epoch 34, Batch 546, LR 0.000005 Loss 4.486890, Accuracy 89.472%\n",
      "Epoch 34, Batch 547, LR 0.000005 Loss 4.487159, Accuracy 89.475%\n",
      "Epoch 34, Batch 548, LR 0.000005 Loss 4.487272, Accuracy 89.472%\n",
      "Epoch 34, Batch 549, LR 0.000005 Loss 4.486535, Accuracy 89.475%\n",
      "Epoch 34, Batch 550, LR 0.000005 Loss 4.486548, Accuracy 89.473%\n",
      "Epoch 34, Batch 551, LR 0.000005 Loss 4.485983, Accuracy 89.475%\n",
      "Epoch 34, Batch 552, LR 0.000005 Loss 4.486386, Accuracy 89.470%\n",
      "Epoch 34, Batch 553, LR 0.000005 Loss 4.487063, Accuracy 89.464%\n",
      "Epoch 34, Batch 554, LR 0.000005 Loss 4.487456, Accuracy 89.456%\n",
      "Epoch 34, Batch 555, LR 0.000005 Loss 4.487887, Accuracy 89.455%\n",
      "Epoch 34, Batch 556, LR 0.000005 Loss 4.487470, Accuracy 89.460%\n",
      "Epoch 34, Batch 557, LR 0.000005 Loss 4.486255, Accuracy 89.468%\n",
      "Epoch 34, Batch 558, LR 0.000005 Loss 4.486096, Accuracy 89.469%\n",
      "Epoch 34, Batch 559, LR 0.000005 Loss 4.486749, Accuracy 89.464%\n",
      "Epoch 34, Batch 560, LR 0.000005 Loss 4.487491, Accuracy 89.463%\n",
      "Epoch 34, Batch 561, LR 0.000005 Loss 4.487630, Accuracy 89.471%\n",
      "Epoch 34, Batch 562, LR 0.000005 Loss 4.488730, Accuracy 89.459%\n",
      "Epoch 34, Batch 563, LR 0.000005 Loss 4.488950, Accuracy 89.455%\n",
      "Epoch 34, Batch 564, LR 0.000005 Loss 4.488982, Accuracy 89.459%\n",
      "Epoch 34, Batch 565, LR 0.000005 Loss 4.489765, Accuracy 89.458%\n",
      "Epoch 34, Batch 566, LR 0.000005 Loss 4.489361, Accuracy 89.453%\n",
      "Epoch 34, Batch 567, LR 0.000005 Loss 4.487348, Accuracy 89.462%\n",
      "Epoch 34, Batch 568, LR 0.000005 Loss 4.486151, Accuracy 89.470%\n",
      "Epoch 34, Batch 569, LR 0.000005 Loss 4.487433, Accuracy 89.463%\n",
      "Epoch 34, Batch 570, LR 0.000005 Loss 4.486639, Accuracy 89.465%\n",
      "Epoch 34, Batch 571, LR 0.000005 Loss 4.486891, Accuracy 89.465%\n",
      "Epoch 34, Batch 572, LR 0.000005 Loss 4.488387, Accuracy 89.461%\n",
      "Epoch 34, Batch 573, LR 0.000005 Loss 4.487947, Accuracy 89.458%\n",
      "Epoch 34, Batch 574, LR 0.000005 Loss 4.487815, Accuracy 89.461%\n",
      "Epoch 34, Batch 575, LR 0.000005 Loss 4.488039, Accuracy 89.461%\n",
      "Epoch 34, Batch 576, LR 0.000005 Loss 4.488736, Accuracy 89.459%\n",
      "Epoch 34, Batch 577, LR 0.000005 Loss 4.487789, Accuracy 89.459%\n",
      "Epoch 34, Batch 578, LR 0.000005 Loss 4.487946, Accuracy 89.452%\n",
      "Epoch 34, Batch 579, LR 0.000005 Loss 4.488159, Accuracy 89.451%\n",
      "Epoch 34, Batch 580, LR 0.000005 Loss 4.489127, Accuracy 89.442%\n",
      "Epoch 34, Batch 581, LR 0.000005 Loss 4.489161, Accuracy 89.444%\n",
      "Epoch 34, Batch 582, LR 0.000005 Loss 4.490971, Accuracy 89.432%\n",
      "Epoch 34, Batch 583, LR 0.000005 Loss 4.490964, Accuracy 89.436%\n",
      "Epoch 34, Batch 584, LR 0.000005 Loss 4.490023, Accuracy 89.441%\n",
      "Epoch 34, Batch 585, LR 0.000005 Loss 4.490665, Accuracy 89.436%\n",
      "Epoch 34, Batch 586, LR 0.000005 Loss 4.491180, Accuracy 89.436%\n",
      "Epoch 34, Batch 587, LR 0.000005 Loss 4.491538, Accuracy 89.436%\n",
      "Epoch 34, Batch 588, LR 0.000005 Loss 4.492101, Accuracy 89.435%\n",
      "Epoch 34, Batch 589, LR 0.000005 Loss 4.491624, Accuracy 89.437%\n",
      "Epoch 34, Batch 590, LR 0.000005 Loss 4.492219, Accuracy 89.435%\n",
      "Epoch 34, Batch 591, LR 0.000005 Loss 4.492317, Accuracy 89.435%\n",
      "Epoch 34, Batch 592, LR 0.000005 Loss 4.493870, Accuracy 89.431%\n",
      "Epoch 34, Batch 593, LR 0.000005 Loss 4.493441, Accuracy 89.427%\n",
      "Epoch 34, Batch 594, LR 0.000005 Loss 4.492793, Accuracy 89.429%\n",
      "Epoch 34, Batch 595, LR 0.000005 Loss 4.494266, Accuracy 89.426%\n",
      "Epoch 34, Batch 596, LR 0.000005 Loss 4.493719, Accuracy 89.426%\n",
      "Epoch 34, Batch 597, LR 0.000005 Loss 4.493125, Accuracy 89.429%\n",
      "Epoch 34, Batch 598, LR 0.000005 Loss 4.492226, Accuracy 89.432%\n",
      "Epoch 34, Batch 599, LR 0.000005 Loss 4.491752, Accuracy 89.434%\n",
      "Epoch 34, Batch 600, LR 0.000005 Loss 4.491881, Accuracy 89.435%\n",
      "Epoch 34, Batch 601, LR 0.000005 Loss 4.491817, Accuracy 89.429%\n",
      "Epoch 34, Batch 602, LR 0.000005 Loss 4.491599, Accuracy 89.432%\n",
      "Epoch 34, Batch 603, LR 0.000005 Loss 4.492038, Accuracy 89.436%\n",
      "Epoch 34, Batch 604, LR 0.000005 Loss 4.492499, Accuracy 89.434%\n",
      "Epoch 34, Batch 605, LR 0.000005 Loss 4.493404, Accuracy 89.437%\n",
      "Epoch 34, Batch 606, LR 0.000005 Loss 4.493627, Accuracy 89.438%\n",
      "Epoch 34, Batch 607, LR 0.000005 Loss 4.495162, Accuracy 89.428%\n",
      "Epoch 34, Batch 608, LR 0.000005 Loss 4.495280, Accuracy 89.431%\n",
      "Epoch 34, Batch 609, LR 0.000005 Loss 4.494790, Accuracy 89.438%\n",
      "Epoch 34, Batch 610, LR 0.000005 Loss 4.494237, Accuracy 89.439%\n",
      "Epoch 34, Batch 611, LR 0.000005 Loss 4.495925, Accuracy 89.431%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 612, LR 0.000005 Loss 4.495221, Accuracy 89.434%\n",
      "Epoch 34, Batch 613, LR 0.000005 Loss 4.495247, Accuracy 89.437%\n",
      "Epoch 34, Batch 614, LR 0.000005 Loss 4.494334, Accuracy 89.445%\n",
      "Epoch 34, Batch 615, LR 0.000005 Loss 4.494626, Accuracy 89.446%\n",
      "Epoch 34, Batch 616, LR 0.000005 Loss 4.494276, Accuracy 89.446%\n",
      "Epoch 34, Batch 617, LR 0.000005 Loss 4.494128, Accuracy 89.451%\n",
      "Epoch 34, Batch 618, LR 0.000005 Loss 4.495193, Accuracy 89.438%\n",
      "Epoch 34, Batch 619, LR 0.000005 Loss 4.495554, Accuracy 89.439%\n",
      "Epoch 34, Batch 620, LR 0.000005 Loss 4.495934, Accuracy 89.441%\n",
      "Epoch 34, Batch 621, LR 0.000005 Loss 4.495657, Accuracy 89.442%\n",
      "Epoch 34, Batch 622, LR 0.000005 Loss 4.495604, Accuracy 89.452%\n",
      "Epoch 34, Batch 623, LR 0.000005 Loss 4.495929, Accuracy 89.452%\n",
      "Epoch 34, Batch 624, LR 0.000005 Loss 4.495749, Accuracy 89.454%\n",
      "Epoch 34, Batch 625, LR 0.000005 Loss 4.496639, Accuracy 89.456%\n",
      "Epoch 34, Batch 626, LR 0.000005 Loss 4.496817, Accuracy 89.453%\n",
      "Epoch 34, Batch 627, LR 0.000005 Loss 4.497756, Accuracy 89.446%\n",
      "Epoch 34, Batch 628, LR 0.000005 Loss 4.497318, Accuracy 89.443%\n",
      "Epoch 34, Batch 629, LR 0.000005 Loss 4.497642, Accuracy 89.443%\n",
      "Epoch 34, Batch 630, LR 0.000005 Loss 4.498170, Accuracy 89.436%\n",
      "Epoch 34, Batch 631, LR 0.000005 Loss 4.497618, Accuracy 89.436%\n",
      "Epoch 34, Batch 632, LR 0.000005 Loss 4.497534, Accuracy 89.432%\n",
      "Epoch 34, Batch 633, LR 0.000005 Loss 4.496690, Accuracy 89.433%\n",
      "Epoch 34, Batch 634, LR 0.000005 Loss 4.497032, Accuracy 89.428%\n",
      "Epoch 34, Batch 635, LR 0.000005 Loss 4.497598, Accuracy 89.430%\n",
      "Epoch 34, Batch 636, LR 0.000005 Loss 4.497403, Accuracy 89.430%\n",
      "Epoch 34, Batch 637, LR 0.000005 Loss 4.497146, Accuracy 89.426%\n",
      "Epoch 34, Batch 638, LR 0.000005 Loss 4.498160, Accuracy 89.423%\n",
      "Epoch 34, Batch 639, LR 0.000005 Loss 4.498541, Accuracy 89.422%\n",
      "Epoch 34, Batch 640, LR 0.000005 Loss 4.499466, Accuracy 89.415%\n",
      "Epoch 34, Batch 641, LR 0.000005 Loss 4.498696, Accuracy 89.412%\n",
      "Epoch 34, Batch 642, LR 0.000005 Loss 4.498951, Accuracy 89.408%\n",
      "Epoch 34, Batch 643, LR 0.000005 Loss 4.498834, Accuracy 89.411%\n",
      "Epoch 34, Batch 644, LR 0.000005 Loss 4.498627, Accuracy 89.407%\n",
      "Epoch 34, Batch 645, LR 0.000005 Loss 4.498684, Accuracy 89.413%\n",
      "Epoch 34, Batch 646, LR 0.000005 Loss 4.497980, Accuracy 89.414%\n",
      "Epoch 34, Batch 647, LR 0.000005 Loss 4.498167, Accuracy 89.418%\n",
      "Epoch 34, Batch 648, LR 0.000005 Loss 4.497450, Accuracy 89.419%\n",
      "Epoch 34, Batch 649, LR 0.000005 Loss 4.496816, Accuracy 89.428%\n",
      "Epoch 34, Batch 650, LR 0.000005 Loss 4.496780, Accuracy 89.428%\n",
      "Epoch 34, Batch 651, LR 0.000005 Loss 4.496581, Accuracy 89.427%\n",
      "Epoch 34, Batch 652, LR 0.000005 Loss 4.496886, Accuracy 89.422%\n",
      "Epoch 34, Batch 653, LR 0.000005 Loss 4.495725, Accuracy 89.427%\n",
      "Epoch 34, Batch 654, LR 0.000005 Loss 4.495553, Accuracy 89.432%\n",
      "Epoch 34, Batch 655, LR 0.000005 Loss 4.495933, Accuracy 89.432%\n",
      "Epoch 34, Batch 656, LR 0.000005 Loss 4.496406, Accuracy 89.432%\n",
      "Epoch 34, Batch 657, LR 0.000005 Loss 4.496309, Accuracy 89.435%\n",
      "Epoch 34, Batch 658, LR 0.000005 Loss 4.496226, Accuracy 89.440%\n",
      "Epoch 34, Batch 659, LR 0.000005 Loss 4.495740, Accuracy 89.445%\n",
      "Epoch 34, Batch 660, LR 0.000005 Loss 4.495544, Accuracy 89.447%\n",
      "Epoch 34, Batch 661, LR 0.000005 Loss 4.495238, Accuracy 89.451%\n",
      "Epoch 34, Batch 662, LR 0.000005 Loss 4.495534, Accuracy 89.451%\n",
      "Epoch 34, Batch 663, LR 0.000005 Loss 4.495360, Accuracy 89.450%\n",
      "Epoch 34, Batch 664, LR 0.000005 Loss 4.495486, Accuracy 89.452%\n",
      "Epoch 34, Batch 665, LR 0.000005 Loss 4.495122, Accuracy 89.458%\n",
      "Epoch 34, Batch 666, LR 0.000005 Loss 4.494518, Accuracy 89.461%\n",
      "Epoch 34, Batch 667, LR 0.000005 Loss 4.494596, Accuracy 89.461%\n",
      "Epoch 34, Batch 668, LR 0.000005 Loss 4.495479, Accuracy 89.461%\n",
      "Epoch 34, Batch 669, LR 0.000005 Loss 4.495948, Accuracy 89.461%\n",
      "Epoch 34, Batch 670, LR 0.000005 Loss 4.496802, Accuracy 89.458%\n",
      "Epoch 34, Batch 671, LR 0.000005 Loss 4.497079, Accuracy 89.455%\n",
      "Epoch 34, Batch 672, LR 0.000005 Loss 4.497612, Accuracy 89.451%\n",
      "Epoch 34, Batch 673, LR 0.000005 Loss 4.497492, Accuracy 89.450%\n",
      "Epoch 34, Batch 674, LR 0.000005 Loss 4.497334, Accuracy 89.453%\n",
      "Epoch 34, Batch 675, LR 0.000005 Loss 4.497661, Accuracy 89.455%\n",
      "Epoch 34, Batch 676, LR 0.000005 Loss 4.497528, Accuracy 89.454%\n",
      "Epoch 34, Batch 677, LR 0.000005 Loss 4.497385, Accuracy 89.456%\n",
      "Epoch 34, Batch 678, LR 0.000005 Loss 4.496886, Accuracy 89.453%\n",
      "Epoch 34, Batch 679, LR 0.000005 Loss 4.497208, Accuracy 89.456%\n",
      "Epoch 34, Batch 680, LR 0.000005 Loss 4.497579, Accuracy 89.453%\n",
      "Epoch 34, Batch 681, LR 0.000005 Loss 4.497074, Accuracy 89.458%\n",
      "Epoch 34, Batch 682, LR 0.000005 Loss 4.498144, Accuracy 89.459%\n",
      "Epoch 34, Batch 683, LR 0.000005 Loss 4.498170, Accuracy 89.461%\n",
      "Epoch 34, Batch 684, LR 0.000005 Loss 4.497799, Accuracy 89.462%\n",
      "Epoch 34, Batch 685, LR 0.000005 Loss 4.497915, Accuracy 89.467%\n",
      "Epoch 34, Batch 686, LR 0.000005 Loss 4.497127, Accuracy 89.471%\n",
      "Epoch 34, Batch 687, LR 0.000005 Loss 4.498298, Accuracy 89.466%\n",
      "Epoch 34, Batch 688, LR 0.000005 Loss 4.497561, Accuracy 89.464%\n",
      "Epoch 34, Batch 689, LR 0.000005 Loss 4.497245, Accuracy 89.471%\n",
      "Epoch 34, Batch 690, LR 0.000005 Loss 4.498136, Accuracy 89.467%\n",
      "Epoch 34, Batch 691, LR 0.000005 Loss 4.498441, Accuracy 89.464%\n",
      "Epoch 34, Batch 692, LR 0.000005 Loss 4.498425, Accuracy 89.469%\n",
      "Epoch 34, Batch 693, LR 0.000005 Loss 4.497888, Accuracy 89.469%\n",
      "Epoch 34, Batch 694, LR 0.000005 Loss 4.498239, Accuracy 89.468%\n",
      "Epoch 34, Batch 695, LR 0.000005 Loss 4.498354, Accuracy 89.468%\n",
      "Epoch 34, Batch 696, LR 0.000005 Loss 4.498503, Accuracy 89.465%\n",
      "Epoch 34, Batch 697, LR 0.000005 Loss 4.498281, Accuracy 89.466%\n",
      "Epoch 34, Batch 698, LR 0.000005 Loss 4.497364, Accuracy 89.472%\n",
      "Epoch 34, Batch 699, LR 0.000005 Loss 4.497363, Accuracy 89.472%\n",
      "Epoch 34, Batch 700, LR 0.000005 Loss 4.496533, Accuracy 89.480%\n",
      "Epoch 34, Batch 701, LR 0.000005 Loss 4.495709, Accuracy 89.482%\n",
      "Epoch 34, Batch 702, LR 0.000005 Loss 4.496498, Accuracy 89.480%\n",
      "Epoch 34, Batch 703, LR 0.000005 Loss 4.497147, Accuracy 89.476%\n",
      "Epoch 34, Batch 704, LR 0.000005 Loss 4.497760, Accuracy 89.474%\n",
      "Epoch 34, Batch 705, LR 0.000005 Loss 4.497129, Accuracy 89.474%\n",
      "Epoch 34, Batch 706, LR 0.000005 Loss 4.498740, Accuracy 89.463%\n",
      "Epoch 34, Batch 707, LR 0.000005 Loss 4.497898, Accuracy 89.467%\n",
      "Epoch 34, Batch 708, LR 0.000005 Loss 4.498170, Accuracy 89.464%\n",
      "Epoch 34, Batch 709, LR 0.000005 Loss 4.498219, Accuracy 89.465%\n",
      "Epoch 34, Batch 710, LR 0.000005 Loss 4.498151, Accuracy 89.464%\n",
      "Epoch 34, Batch 711, LR 0.000005 Loss 4.498296, Accuracy 89.462%\n",
      "Epoch 34, Batch 712, LR 0.000005 Loss 4.497820, Accuracy 89.462%\n",
      "Epoch 34, Batch 713, LR 0.000005 Loss 4.497763, Accuracy 89.464%\n",
      "Epoch 34, Batch 714, LR 0.000005 Loss 4.497686, Accuracy 89.460%\n",
      "Epoch 34, Batch 715, LR 0.000005 Loss 4.498752, Accuracy 89.456%\n",
      "Epoch 34, Batch 716, LR 0.000005 Loss 4.499044, Accuracy 89.456%\n",
      "Epoch 34, Batch 717, LR 0.000005 Loss 4.499663, Accuracy 89.458%\n",
      "Epoch 34, Batch 718, LR 0.000005 Loss 4.499486, Accuracy 89.460%\n",
      "Epoch 34, Batch 719, LR 0.000005 Loss 4.500118, Accuracy 89.459%\n",
      "Epoch 34, Batch 720, LR 0.000005 Loss 4.500000, Accuracy 89.459%\n",
      "Epoch 34, Batch 721, LR 0.000005 Loss 4.500828, Accuracy 89.455%\n",
      "Epoch 34, Batch 722, LR 0.000005 Loss 4.500576, Accuracy 89.457%\n",
      "Epoch 34, Batch 723, LR 0.000005 Loss 4.500136, Accuracy 89.458%\n",
      "Epoch 34, Batch 724, LR 0.000005 Loss 4.500096, Accuracy 89.456%\n",
      "Epoch 34, Batch 725, LR 0.000005 Loss 4.499525, Accuracy 89.456%\n",
      "Epoch 34, Batch 726, LR 0.000005 Loss 4.499450, Accuracy 89.454%\n",
      "Epoch 34, Batch 727, LR 0.000005 Loss 4.498412, Accuracy 89.457%\n",
      "Epoch 34, Batch 728, LR 0.000005 Loss 4.499090, Accuracy 89.449%\n",
      "Epoch 34, Batch 729, LR 0.000005 Loss 4.498910, Accuracy 89.447%\n",
      "Epoch 34, Batch 730, LR 0.000005 Loss 4.499449, Accuracy 89.445%\n",
      "Epoch 34, Batch 731, LR 0.000005 Loss 4.498857, Accuracy 89.447%\n",
      "Epoch 34, Batch 732, LR 0.000005 Loss 4.498566, Accuracy 89.450%\n",
      "Epoch 34, Batch 733, LR 0.000005 Loss 4.497337, Accuracy 89.459%\n",
      "Epoch 34, Batch 734, LR 0.000005 Loss 4.497902, Accuracy 89.457%\n",
      "Epoch 34, Batch 735, LR 0.000005 Loss 4.498013, Accuracy 89.457%\n",
      "Epoch 34, Batch 736, LR 0.000005 Loss 4.497767, Accuracy 89.455%\n",
      "Epoch 34, Batch 737, LR 0.000005 Loss 4.496774, Accuracy 89.457%\n",
      "Epoch 34, Batch 738, LR 0.000005 Loss 4.496224, Accuracy 89.456%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 739, LR 0.000005 Loss 4.495598, Accuracy 89.458%\n",
      "Epoch 34, Batch 740, LR 0.000005 Loss 4.495515, Accuracy 89.453%\n",
      "Epoch 34, Batch 741, LR 0.000005 Loss 4.495789, Accuracy 89.453%\n",
      "Epoch 34, Batch 742, LR 0.000005 Loss 4.495978, Accuracy 89.446%\n",
      "Epoch 34, Batch 743, LR 0.000005 Loss 4.494900, Accuracy 89.452%\n",
      "Epoch 34, Batch 744, LR 0.000005 Loss 4.495576, Accuracy 89.448%\n",
      "Epoch 34, Batch 745, LR 0.000005 Loss 4.496341, Accuracy 89.438%\n",
      "Epoch 34, Batch 746, LR 0.000005 Loss 4.495981, Accuracy 89.443%\n",
      "Epoch 34, Batch 747, LR 0.000005 Loss 4.496555, Accuracy 89.444%\n",
      "Epoch 34, Batch 748, LR 0.000005 Loss 4.496420, Accuracy 89.447%\n",
      "Epoch 34, Batch 749, LR 0.000005 Loss 4.497898, Accuracy 89.436%\n",
      "Epoch 34, Batch 750, LR 0.000005 Loss 4.498416, Accuracy 89.430%\n",
      "Epoch 34, Batch 751, LR 0.000005 Loss 4.498033, Accuracy 89.431%\n",
      "Epoch 34, Batch 752, LR 0.000005 Loss 4.497791, Accuracy 89.429%\n",
      "Epoch 34, Batch 753, LR 0.000005 Loss 4.498055, Accuracy 89.427%\n",
      "Epoch 34, Batch 754, LR 0.000005 Loss 4.498338, Accuracy 89.425%\n",
      "Epoch 34, Batch 755, LR 0.000005 Loss 4.497998, Accuracy 89.427%\n",
      "Epoch 34, Batch 756, LR 0.000005 Loss 4.497596, Accuracy 89.429%\n",
      "Epoch 34, Batch 757, LR 0.000005 Loss 4.496782, Accuracy 89.430%\n",
      "Epoch 34, Batch 758, LR 0.000005 Loss 4.497076, Accuracy 89.426%\n",
      "Epoch 34, Batch 759, LR 0.000005 Loss 4.497197, Accuracy 89.433%\n",
      "Epoch 34, Batch 760, LR 0.000005 Loss 4.497806, Accuracy 89.431%\n",
      "Epoch 34, Batch 761, LR 0.000005 Loss 4.498201, Accuracy 89.431%\n",
      "Epoch 34, Batch 762, LR 0.000005 Loss 4.497961, Accuracy 89.434%\n",
      "Epoch 34, Batch 763, LR 0.000005 Loss 4.498041, Accuracy 89.436%\n",
      "Epoch 34, Batch 764, LR 0.000005 Loss 4.498138, Accuracy 89.440%\n",
      "Epoch 34, Batch 765, LR 0.000005 Loss 4.498503, Accuracy 89.440%\n",
      "Epoch 34, Batch 766, LR 0.000005 Loss 4.497863, Accuracy 89.444%\n",
      "Epoch 34, Batch 767, LR 0.000005 Loss 4.498560, Accuracy 89.441%\n",
      "Epoch 34, Batch 768, LR 0.000005 Loss 4.498457, Accuracy 89.441%\n",
      "Epoch 34, Batch 769, LR 0.000005 Loss 4.498927, Accuracy 89.442%\n",
      "Epoch 34, Batch 770, LR 0.000005 Loss 4.498805, Accuracy 89.439%\n",
      "Epoch 34, Batch 771, LR 0.000005 Loss 4.499351, Accuracy 89.433%\n",
      "Epoch 34, Batch 772, LR 0.000005 Loss 4.499705, Accuracy 89.428%\n",
      "Epoch 34, Batch 773, LR 0.000005 Loss 4.498552, Accuracy 89.432%\n",
      "Epoch 34, Batch 774, LR 0.000005 Loss 4.498333, Accuracy 89.435%\n",
      "Epoch 34, Batch 775, LR 0.000005 Loss 4.497810, Accuracy 89.438%\n",
      "Epoch 34, Batch 776, LR 0.000005 Loss 4.497240, Accuracy 89.441%\n",
      "Epoch 34, Batch 777, LR 0.000005 Loss 4.499054, Accuracy 89.436%\n",
      "Epoch 34, Batch 778, LR 0.000005 Loss 4.499215, Accuracy 89.436%\n",
      "Epoch 34, Batch 779, LR 0.000005 Loss 4.499097, Accuracy 89.439%\n",
      "Epoch 34, Batch 780, LR 0.000005 Loss 4.499086, Accuracy 89.439%\n",
      "Epoch 34, Batch 781, LR 0.000005 Loss 4.499386, Accuracy 89.438%\n",
      "Epoch 34, Batch 782, LR 0.000005 Loss 4.499274, Accuracy 89.442%\n",
      "Epoch 34, Batch 783, LR 0.000005 Loss 4.498833, Accuracy 89.443%\n",
      "Epoch 34, Batch 784, LR 0.000005 Loss 4.499205, Accuracy 89.440%\n",
      "Epoch 34, Batch 785, LR 0.000005 Loss 4.499462, Accuracy 89.441%\n",
      "Epoch 34, Batch 786, LR 0.000005 Loss 4.500458, Accuracy 89.436%\n",
      "Epoch 34, Batch 787, LR 0.000005 Loss 4.501562, Accuracy 89.436%\n",
      "Epoch 34, Batch 788, LR 0.000005 Loss 4.502218, Accuracy 89.430%\n",
      "Epoch 34, Batch 789, LR 0.000005 Loss 4.502722, Accuracy 89.430%\n",
      "Epoch 34, Batch 790, LR 0.000005 Loss 4.502634, Accuracy 89.430%\n",
      "Epoch 34, Batch 791, LR 0.000005 Loss 4.502780, Accuracy 89.427%\n",
      "Epoch 34, Batch 792, LR 0.000005 Loss 4.502095, Accuracy 89.430%\n",
      "Epoch 34, Batch 793, LR 0.000005 Loss 4.501267, Accuracy 89.434%\n",
      "Epoch 34, Batch 794, LR 0.000005 Loss 4.501773, Accuracy 89.427%\n",
      "Epoch 34, Batch 795, LR 0.000005 Loss 4.501511, Accuracy 89.429%\n",
      "Epoch 34, Batch 796, LR 0.000005 Loss 4.501877, Accuracy 89.425%\n",
      "Epoch 34, Batch 797, LR 0.000005 Loss 4.501083, Accuracy 89.427%\n",
      "Epoch 34, Batch 798, LR 0.000005 Loss 4.501891, Accuracy 89.426%\n",
      "Epoch 34, Batch 799, LR 0.000005 Loss 4.502019, Accuracy 89.427%\n",
      "Epoch 34, Batch 800, LR 0.000005 Loss 4.502257, Accuracy 89.423%\n",
      "Epoch 34, Batch 801, LR 0.000005 Loss 4.502444, Accuracy 89.423%\n",
      "Epoch 34, Batch 802, LR 0.000005 Loss 4.501931, Accuracy 89.425%\n",
      "Epoch 34, Batch 803, LR 0.000005 Loss 4.501651, Accuracy 89.422%\n",
      "Epoch 34, Batch 804, LR 0.000005 Loss 4.501018, Accuracy 89.429%\n",
      "Epoch 34, Batch 805, LR 0.000005 Loss 4.500865, Accuracy 89.428%\n",
      "Epoch 34, Batch 806, LR 0.000005 Loss 4.500557, Accuracy 89.430%\n",
      "Epoch 34, Batch 807, LR 0.000005 Loss 4.500770, Accuracy 89.427%\n",
      "Epoch 34, Batch 808, LR 0.000005 Loss 4.500720, Accuracy 89.428%\n",
      "Epoch 34, Batch 809, LR 0.000005 Loss 4.500571, Accuracy 89.428%\n",
      "Epoch 34, Batch 810, LR 0.000005 Loss 4.500461, Accuracy 89.431%\n",
      "Epoch 34, Batch 811, LR 0.000005 Loss 4.500271, Accuracy 89.434%\n",
      "Epoch 34, Batch 812, LR 0.000005 Loss 4.500619, Accuracy 89.435%\n",
      "Epoch 34, Batch 813, LR 0.000005 Loss 4.499044, Accuracy 89.442%\n",
      "Epoch 34, Batch 814, LR 0.000005 Loss 4.499079, Accuracy 89.443%\n",
      "Epoch 34, Batch 815, LR 0.000005 Loss 4.499179, Accuracy 89.445%\n",
      "Epoch 34, Batch 816, LR 0.000005 Loss 4.498984, Accuracy 89.446%\n",
      "Epoch 34, Batch 817, LR 0.000005 Loss 4.499649, Accuracy 89.449%\n",
      "Epoch 34, Batch 818, LR 0.000005 Loss 4.500024, Accuracy 89.450%\n",
      "Epoch 34, Batch 819, LR 0.000005 Loss 4.499760, Accuracy 89.451%\n",
      "Epoch 34, Batch 820, LR 0.000005 Loss 4.499894, Accuracy 89.450%\n",
      "Epoch 34, Batch 821, LR 0.000005 Loss 4.499735, Accuracy 89.453%\n",
      "Epoch 34, Batch 822, LR 0.000005 Loss 4.499804, Accuracy 89.453%\n",
      "Epoch 34, Batch 823, LR 0.000005 Loss 4.498812, Accuracy 89.455%\n",
      "Epoch 34, Batch 824, LR 0.000005 Loss 4.498066, Accuracy 89.457%\n",
      "Epoch 34, Batch 825, LR 0.000005 Loss 4.498101, Accuracy 89.460%\n",
      "Epoch 34, Batch 826, LR 0.000005 Loss 4.499789, Accuracy 89.456%\n",
      "Epoch 34, Batch 827, LR 0.000005 Loss 4.500131, Accuracy 89.457%\n",
      "Epoch 34, Batch 828, LR 0.000005 Loss 4.500567, Accuracy 89.459%\n",
      "Epoch 34, Batch 829, LR 0.000005 Loss 4.500299, Accuracy 89.457%\n",
      "Epoch 34, Batch 830, LR 0.000005 Loss 4.500731, Accuracy 89.452%\n",
      "Epoch 34, Batch 831, LR 0.000005 Loss 4.500423, Accuracy 89.454%\n",
      "Epoch 34, Batch 832, LR 0.000005 Loss 4.499373, Accuracy 89.455%\n",
      "Epoch 34, Batch 833, LR 0.000005 Loss 4.498118, Accuracy 89.461%\n",
      "Epoch 34, Batch 834, LR 0.000005 Loss 4.497591, Accuracy 89.463%\n",
      "Epoch 34, Batch 835, LR 0.000005 Loss 4.498099, Accuracy 89.462%\n",
      "Epoch 34, Batch 836, LR 0.000005 Loss 4.498167, Accuracy 89.462%\n",
      "Epoch 34, Batch 837, LR 0.000005 Loss 4.497315, Accuracy 89.469%\n",
      "Epoch 34, Batch 838, LR 0.000005 Loss 4.498197, Accuracy 89.464%\n",
      "Epoch 34, Batch 839, LR 0.000005 Loss 4.497723, Accuracy 89.469%\n",
      "Epoch 34, Batch 840, LR 0.000005 Loss 4.498456, Accuracy 89.465%\n",
      "Epoch 34, Batch 841, LR 0.000005 Loss 4.498037, Accuracy 89.467%\n",
      "Epoch 34, Batch 842, LR 0.000005 Loss 4.497096, Accuracy 89.470%\n",
      "Epoch 34, Batch 843, LR 0.000005 Loss 4.498184, Accuracy 89.467%\n",
      "Epoch 34, Batch 844, LR 0.000005 Loss 4.497211, Accuracy 89.470%\n",
      "Epoch 34, Batch 845, LR 0.000005 Loss 4.497972, Accuracy 89.467%\n",
      "Epoch 34, Batch 846, LR 0.000005 Loss 4.497907, Accuracy 89.469%\n",
      "Epoch 34, Batch 847, LR 0.000005 Loss 4.497548, Accuracy 89.469%\n",
      "Epoch 34, Batch 848, LR 0.000005 Loss 4.498413, Accuracy 89.471%\n",
      "Epoch 34, Batch 849, LR 0.000005 Loss 4.498126, Accuracy 89.470%\n",
      "Epoch 34, Batch 850, LR 0.000005 Loss 4.498173, Accuracy 89.467%\n",
      "Epoch 34, Batch 851, LR 0.000005 Loss 4.497311, Accuracy 89.469%\n",
      "Epoch 34, Batch 852, LR 0.000005 Loss 4.497747, Accuracy 89.464%\n",
      "Epoch 34, Batch 853, LR 0.000005 Loss 4.497397, Accuracy 89.466%\n",
      "Epoch 34, Batch 854, LR 0.000005 Loss 4.497366, Accuracy 89.466%\n",
      "Epoch 34, Batch 855, LR 0.000005 Loss 4.498555, Accuracy 89.464%\n",
      "Epoch 34, Batch 856, LR 0.000005 Loss 4.497839, Accuracy 89.466%\n",
      "Epoch 34, Batch 857, LR 0.000005 Loss 4.497004, Accuracy 89.466%\n",
      "Epoch 34, Batch 858, LR 0.000005 Loss 4.497309, Accuracy 89.467%\n",
      "Epoch 34, Batch 859, LR 0.000005 Loss 4.497058, Accuracy 89.466%\n",
      "Epoch 34, Batch 860, LR 0.000005 Loss 4.497400, Accuracy 89.468%\n",
      "Epoch 34, Batch 861, LR 0.000005 Loss 4.497849, Accuracy 89.467%\n",
      "Epoch 34, Batch 862, LR 0.000005 Loss 4.498136, Accuracy 89.467%\n",
      "Epoch 34, Batch 863, LR 0.000005 Loss 4.498758, Accuracy 89.464%\n",
      "Epoch 34, Batch 864, LR 0.000005 Loss 4.498530, Accuracy 89.463%\n",
      "Epoch 34, Batch 865, LR 0.000005 Loss 4.498506, Accuracy 89.461%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 866, LR 0.000005 Loss 4.498392, Accuracy 89.459%\n",
      "Epoch 34, Batch 867, LR 0.000005 Loss 4.498435, Accuracy 89.459%\n",
      "Epoch 34, Batch 868, LR 0.000005 Loss 4.498698, Accuracy 89.459%\n",
      "Epoch 34, Batch 869, LR 0.000004 Loss 4.497707, Accuracy 89.463%\n",
      "Epoch 34, Batch 870, LR 0.000004 Loss 4.498050, Accuracy 89.460%\n",
      "Epoch 34, Batch 871, LR 0.000004 Loss 4.497991, Accuracy 89.460%\n",
      "Epoch 34, Batch 872, LR 0.000004 Loss 4.498675, Accuracy 89.456%\n",
      "Epoch 34, Batch 873, LR 0.000004 Loss 4.498536, Accuracy 89.459%\n",
      "Epoch 34, Batch 874, LR 0.000004 Loss 4.497678, Accuracy 89.462%\n",
      "Epoch 34, Batch 875, LR 0.000004 Loss 4.498290, Accuracy 89.463%\n",
      "Epoch 34, Batch 876, LR 0.000004 Loss 4.498429, Accuracy 89.462%\n",
      "Epoch 34, Batch 877, LR 0.000004 Loss 4.498019, Accuracy 89.465%\n",
      "Epoch 34, Batch 878, LR 0.000004 Loss 4.498836, Accuracy 89.460%\n",
      "Epoch 34, Batch 879, LR 0.000004 Loss 4.498539, Accuracy 89.463%\n",
      "Epoch 34, Batch 880, LR 0.000004 Loss 4.499017, Accuracy 89.459%\n",
      "Epoch 34, Batch 881, LR 0.000004 Loss 4.498470, Accuracy 89.457%\n",
      "Epoch 34, Batch 882, LR 0.000004 Loss 4.498559, Accuracy 89.456%\n",
      "Epoch 34, Batch 883, LR 0.000004 Loss 4.498696, Accuracy 89.454%\n",
      "Epoch 34, Batch 884, LR 0.000004 Loss 4.498307, Accuracy 89.458%\n",
      "Epoch 34, Batch 885, LR 0.000004 Loss 4.498008, Accuracy 89.458%\n",
      "Epoch 34, Batch 886, LR 0.000004 Loss 4.498103, Accuracy 89.460%\n",
      "Epoch 34, Batch 887, LR 0.000004 Loss 4.498401, Accuracy 89.462%\n",
      "Epoch 34, Batch 888, LR 0.000004 Loss 4.498141, Accuracy 89.470%\n",
      "Epoch 34, Batch 889, LR 0.000004 Loss 4.498611, Accuracy 89.466%\n",
      "Epoch 34, Batch 890, LR 0.000004 Loss 4.498677, Accuracy 89.463%\n",
      "Epoch 34, Batch 891, LR 0.000004 Loss 4.498487, Accuracy 89.464%\n",
      "Epoch 34, Batch 892, LR 0.000004 Loss 4.499578, Accuracy 89.460%\n",
      "Epoch 34, Batch 893, LR 0.000004 Loss 4.499233, Accuracy 89.461%\n",
      "Epoch 34, Batch 894, LR 0.000004 Loss 4.499697, Accuracy 89.462%\n",
      "Epoch 34, Batch 895, LR 0.000004 Loss 4.500140, Accuracy 89.461%\n",
      "Epoch 34, Batch 896, LR 0.000004 Loss 4.499974, Accuracy 89.459%\n",
      "Epoch 34, Batch 897, LR 0.000004 Loss 4.499748, Accuracy 89.462%\n",
      "Epoch 34, Batch 898, LR 0.000004 Loss 4.499782, Accuracy 89.463%\n",
      "Epoch 34, Batch 899, LR 0.000004 Loss 4.499840, Accuracy 89.461%\n",
      "Epoch 34, Batch 900, LR 0.000004 Loss 4.499549, Accuracy 89.466%\n",
      "Epoch 34, Batch 901, LR 0.000004 Loss 4.499161, Accuracy 89.464%\n",
      "Epoch 34, Batch 902, LR 0.000004 Loss 4.498583, Accuracy 89.468%\n",
      "Epoch 34, Batch 903, LR 0.000004 Loss 4.497907, Accuracy 89.473%\n",
      "Epoch 34, Batch 904, LR 0.000004 Loss 4.497730, Accuracy 89.478%\n",
      "Epoch 34, Batch 905, LR 0.000004 Loss 4.497386, Accuracy 89.479%\n",
      "Epoch 34, Batch 906, LR 0.000004 Loss 4.498106, Accuracy 89.479%\n",
      "Epoch 34, Batch 907, LR 0.000004 Loss 4.497851, Accuracy 89.482%\n",
      "Epoch 34, Batch 908, LR 0.000004 Loss 4.496195, Accuracy 89.488%\n",
      "Epoch 34, Batch 909, LR 0.000004 Loss 4.495711, Accuracy 89.491%\n",
      "Epoch 34, Batch 910, LR 0.000004 Loss 4.496126, Accuracy 89.489%\n",
      "Epoch 34, Batch 911, LR 0.000004 Loss 4.496060, Accuracy 89.491%\n",
      "Epoch 34, Batch 912, LR 0.000004 Loss 4.495977, Accuracy 89.489%\n",
      "Epoch 34, Batch 913, LR 0.000004 Loss 4.496892, Accuracy 89.484%\n",
      "Epoch 34, Batch 914, LR 0.000004 Loss 4.496158, Accuracy 89.486%\n",
      "Epoch 34, Batch 915, LR 0.000004 Loss 4.496450, Accuracy 89.487%\n",
      "Epoch 34, Batch 916, LR 0.000004 Loss 4.495517, Accuracy 89.490%\n",
      "Epoch 34, Batch 917, LR 0.000004 Loss 4.495182, Accuracy 89.491%\n",
      "Epoch 34, Batch 918, LR 0.000004 Loss 4.494789, Accuracy 89.492%\n",
      "Epoch 34, Batch 919, LR 0.000004 Loss 4.495035, Accuracy 89.492%\n",
      "Epoch 34, Batch 920, LR 0.000004 Loss 4.495228, Accuracy 89.490%\n",
      "Epoch 34, Batch 921, LR 0.000004 Loss 4.494724, Accuracy 89.491%\n",
      "Epoch 34, Batch 922, LR 0.000004 Loss 4.495032, Accuracy 89.491%\n",
      "Epoch 34, Batch 923, LR 0.000004 Loss 4.495125, Accuracy 89.488%\n",
      "Epoch 34, Batch 924, LR 0.000004 Loss 4.494480, Accuracy 89.492%\n",
      "Epoch 34, Batch 925, LR 0.000004 Loss 4.493901, Accuracy 89.495%\n",
      "Epoch 34, Batch 926, LR 0.000004 Loss 4.494155, Accuracy 89.497%\n",
      "Epoch 34, Batch 927, LR 0.000004 Loss 4.494399, Accuracy 89.498%\n",
      "Epoch 34, Batch 928, LR 0.000004 Loss 4.494324, Accuracy 89.497%\n",
      "Epoch 34, Batch 929, LR 0.000004 Loss 4.494607, Accuracy 89.498%\n",
      "Epoch 34, Batch 930, LR 0.000004 Loss 4.494426, Accuracy 89.498%\n",
      "Epoch 34, Batch 931, LR 0.000004 Loss 4.493932, Accuracy 89.499%\n",
      "Epoch 34, Batch 932, LR 0.000004 Loss 4.494207, Accuracy 89.498%\n",
      "Epoch 34, Batch 933, LR 0.000004 Loss 4.493700, Accuracy 89.498%\n",
      "Epoch 34, Batch 934, LR 0.000004 Loss 4.492968, Accuracy 89.502%\n",
      "Epoch 34, Batch 935, LR 0.000004 Loss 4.492863, Accuracy 89.504%\n",
      "Epoch 34, Batch 936, LR 0.000004 Loss 4.492646, Accuracy 89.503%\n",
      "Epoch 34, Batch 937, LR 0.000004 Loss 4.492569, Accuracy 89.500%\n",
      "Epoch 34, Batch 938, LR 0.000004 Loss 4.491540, Accuracy 89.503%\n",
      "Epoch 34, Batch 939, LR 0.000004 Loss 4.491223, Accuracy 89.505%\n",
      "Epoch 34, Batch 940, LR 0.000004 Loss 4.492043, Accuracy 89.501%\n",
      "Epoch 34, Batch 941, LR 0.000004 Loss 4.491348, Accuracy 89.502%\n",
      "Epoch 34, Batch 942, LR 0.000004 Loss 4.491329, Accuracy 89.503%\n",
      "Epoch 34, Batch 943, LR 0.000004 Loss 4.491913, Accuracy 89.503%\n",
      "Epoch 34, Batch 944, LR 0.000004 Loss 4.491209, Accuracy 89.509%\n",
      "Epoch 34, Batch 945, LR 0.000004 Loss 4.490929, Accuracy 89.512%\n",
      "Epoch 34, Batch 946, LR 0.000004 Loss 4.491329, Accuracy 89.510%\n",
      "Epoch 34, Batch 947, LR 0.000004 Loss 4.491855, Accuracy 89.506%\n",
      "Epoch 34, Batch 948, LR 0.000004 Loss 4.491819, Accuracy 89.506%\n",
      "Epoch 34, Batch 949, LR 0.000004 Loss 4.492214, Accuracy 89.500%\n",
      "Epoch 34, Batch 950, LR 0.000004 Loss 4.492662, Accuracy 89.499%\n",
      "Epoch 34, Batch 951, LR 0.000004 Loss 4.492753, Accuracy 89.496%\n",
      "Epoch 34, Batch 952, LR 0.000004 Loss 4.493179, Accuracy 89.495%\n",
      "Epoch 34, Batch 953, LR 0.000004 Loss 4.493560, Accuracy 89.495%\n",
      "Epoch 34, Batch 954, LR 0.000004 Loss 4.492996, Accuracy 89.498%\n",
      "Epoch 34, Batch 955, LR 0.000004 Loss 4.493003, Accuracy 89.499%\n",
      "Epoch 34, Batch 956, LR 0.000004 Loss 4.493150, Accuracy 89.500%\n",
      "Epoch 34, Batch 957, LR 0.000004 Loss 4.492753, Accuracy 89.503%\n",
      "Epoch 34, Batch 958, LR 0.000004 Loss 4.492421, Accuracy 89.504%\n",
      "Epoch 34, Batch 959, LR 0.000004 Loss 4.492422, Accuracy 89.507%\n",
      "Epoch 34, Batch 960, LR 0.000004 Loss 4.492429, Accuracy 89.504%\n",
      "Epoch 34, Batch 961, LR 0.000004 Loss 4.492364, Accuracy 89.504%\n",
      "Epoch 34, Batch 962, LR 0.000004 Loss 4.492333, Accuracy 89.503%\n",
      "Epoch 34, Batch 963, LR 0.000004 Loss 4.492567, Accuracy 89.501%\n",
      "Epoch 34, Batch 964, LR 0.000004 Loss 4.492811, Accuracy 89.500%\n",
      "Epoch 34, Batch 965, LR 0.000004 Loss 4.492580, Accuracy 89.499%\n",
      "Epoch 34, Batch 966, LR 0.000004 Loss 4.493016, Accuracy 89.497%\n",
      "Epoch 34, Batch 967, LR 0.000004 Loss 4.492818, Accuracy 89.496%\n",
      "Epoch 34, Batch 968, LR 0.000004 Loss 4.492978, Accuracy 89.496%\n",
      "Epoch 34, Batch 969, LR 0.000004 Loss 4.493361, Accuracy 89.495%\n",
      "Epoch 34, Batch 970, LR 0.000004 Loss 4.493390, Accuracy 89.493%\n",
      "Epoch 34, Batch 971, LR 0.000004 Loss 4.493098, Accuracy 89.497%\n",
      "Epoch 34, Batch 972, LR 0.000004 Loss 4.493150, Accuracy 89.496%\n",
      "Epoch 34, Batch 973, LR 0.000004 Loss 4.492662, Accuracy 89.498%\n",
      "Epoch 34, Batch 974, LR 0.000004 Loss 4.491915, Accuracy 89.505%\n",
      "Epoch 34, Batch 975, LR 0.000004 Loss 4.492316, Accuracy 89.503%\n",
      "Epoch 34, Batch 976, LR 0.000004 Loss 4.491141, Accuracy 89.508%\n",
      "Epoch 34, Batch 977, LR 0.000004 Loss 4.491182, Accuracy 89.506%\n",
      "Epoch 34, Batch 978, LR 0.000004 Loss 4.490780, Accuracy 89.511%\n",
      "Epoch 34, Batch 979, LR 0.000004 Loss 4.490708, Accuracy 89.513%\n",
      "Epoch 34, Batch 980, LR 0.000004 Loss 4.490090, Accuracy 89.518%\n",
      "Epoch 34, Batch 981, LR 0.000004 Loss 4.490056, Accuracy 89.516%\n",
      "Epoch 34, Batch 982, LR 0.000004 Loss 4.489529, Accuracy 89.515%\n",
      "Epoch 34, Batch 983, LR 0.000004 Loss 4.489454, Accuracy 89.517%\n",
      "Epoch 34, Batch 984, LR 0.000004 Loss 4.489772, Accuracy 89.516%\n",
      "Epoch 34, Batch 985, LR 0.000004 Loss 4.490094, Accuracy 89.515%\n",
      "Epoch 34, Batch 986, LR 0.000004 Loss 4.490075, Accuracy 89.512%\n",
      "Epoch 34, Batch 987, LR 0.000004 Loss 4.489934, Accuracy 89.513%\n",
      "Epoch 34, Batch 988, LR 0.000004 Loss 4.490630, Accuracy 89.509%\n",
      "Epoch 34, Batch 989, LR 0.000004 Loss 4.490630, Accuracy 89.506%\n",
      "Epoch 34, Batch 990, LR 0.000004 Loss 4.490431, Accuracy 89.508%\n",
      "Epoch 34, Batch 991, LR 0.000004 Loss 4.490805, Accuracy 89.505%\n",
      "Epoch 34, Batch 992, LR 0.000004 Loss 4.490247, Accuracy 89.507%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 993, LR 0.000004 Loss 4.490358, Accuracy 89.507%\n",
      "Epoch 34, Batch 994, LR 0.000004 Loss 4.490271, Accuracy 89.507%\n",
      "Epoch 34, Batch 995, LR 0.000004 Loss 4.490037, Accuracy 89.508%\n",
      "Epoch 34, Batch 996, LR 0.000004 Loss 4.489280, Accuracy 89.514%\n",
      "Epoch 34, Batch 997, LR 0.000004 Loss 4.488956, Accuracy 89.514%\n",
      "Epoch 34, Batch 998, LR 0.000004 Loss 4.489882, Accuracy 89.509%\n",
      "Epoch 34, Batch 999, LR 0.000004 Loss 4.490395, Accuracy 89.507%\n",
      "Epoch 34, Batch 1000, LR 0.000004 Loss 4.489867, Accuracy 89.513%\n",
      "Epoch 34, Batch 1001, LR 0.000004 Loss 4.490163, Accuracy 89.514%\n",
      "Epoch 34, Batch 1002, LR 0.000004 Loss 4.491103, Accuracy 89.508%\n",
      "Epoch 34, Batch 1003, LR 0.000004 Loss 4.491038, Accuracy 89.508%\n",
      "Epoch 34, Batch 1004, LR 0.000004 Loss 4.491735, Accuracy 89.504%\n",
      "Epoch 34, Batch 1005, LR 0.000004 Loss 4.491567, Accuracy 89.506%\n",
      "Epoch 34, Batch 1006, LR 0.000004 Loss 4.491051, Accuracy 89.508%\n",
      "Epoch 34, Batch 1007, LR 0.000004 Loss 4.491571, Accuracy 89.503%\n",
      "Epoch 34, Batch 1008, LR 0.000004 Loss 4.492397, Accuracy 89.502%\n",
      "Epoch 34, Batch 1009, LR 0.000004 Loss 4.491881, Accuracy 89.505%\n",
      "Epoch 34, Batch 1010, LR 0.000004 Loss 4.492409, Accuracy 89.500%\n",
      "Epoch 34, Batch 1011, LR 0.000004 Loss 4.492492, Accuracy 89.500%\n",
      "Epoch 34, Batch 1012, LR 0.000004 Loss 4.492403, Accuracy 89.500%\n",
      "Epoch 34, Batch 1013, LR 0.000004 Loss 4.492091, Accuracy 89.501%\n",
      "Epoch 34, Batch 1014, LR 0.000004 Loss 4.491306, Accuracy 89.506%\n",
      "Epoch 34, Batch 1015, LR 0.000004 Loss 4.491188, Accuracy 89.507%\n",
      "Epoch 34, Batch 1016, LR 0.000004 Loss 4.490375, Accuracy 89.510%\n",
      "Epoch 34, Batch 1017, LR 0.000004 Loss 4.490068, Accuracy 89.511%\n",
      "Epoch 34, Batch 1018, LR 0.000004 Loss 4.489664, Accuracy 89.511%\n",
      "Epoch 34, Batch 1019, LR 0.000004 Loss 4.489320, Accuracy 89.511%\n",
      "Epoch 34, Batch 1020, LR 0.000004 Loss 4.488955, Accuracy 89.513%\n",
      "Epoch 34, Batch 1021, LR 0.000004 Loss 4.488819, Accuracy 89.514%\n",
      "Epoch 34, Batch 1022, LR 0.000004 Loss 4.489026, Accuracy 89.517%\n",
      "Epoch 34, Batch 1023, LR 0.000004 Loss 4.489850, Accuracy 89.514%\n",
      "Epoch 34, Batch 1024, LR 0.000004 Loss 4.489606, Accuracy 89.513%\n",
      "Epoch 34, Batch 1025, LR 0.000004 Loss 4.488894, Accuracy 89.516%\n",
      "Epoch 34, Batch 1026, LR 0.000004 Loss 4.489263, Accuracy 89.516%\n",
      "Epoch 34, Batch 1027, LR 0.000004 Loss 4.489711, Accuracy 89.517%\n",
      "Epoch 34, Batch 1028, LR 0.000004 Loss 4.489594, Accuracy 89.516%\n",
      "Epoch 34, Batch 1029, LR 0.000004 Loss 4.489677, Accuracy 89.518%\n",
      "Epoch 34, Batch 1030, LR 0.000004 Loss 4.489665, Accuracy 89.515%\n",
      "Epoch 34, Batch 1031, LR 0.000004 Loss 4.489512, Accuracy 89.516%\n",
      "Epoch 34, Batch 1032, LR 0.000004 Loss 4.490179, Accuracy 89.509%\n",
      "Epoch 34, Batch 1033, LR 0.000004 Loss 4.490163, Accuracy 89.507%\n",
      "Epoch 34, Batch 1034, LR 0.000004 Loss 4.490836, Accuracy 89.502%\n",
      "Epoch 34, Batch 1035, LR 0.000004 Loss 4.490868, Accuracy 89.504%\n",
      "Epoch 34, Batch 1036, LR 0.000004 Loss 4.490549, Accuracy 89.505%\n",
      "Epoch 34, Batch 1037, LR 0.000004 Loss 4.490692, Accuracy 89.505%\n",
      "Epoch 34, Batch 1038, LR 0.000004 Loss 4.489947, Accuracy 89.510%\n",
      "Epoch 34, Batch 1039, LR 0.000004 Loss 4.490231, Accuracy 89.506%\n",
      "Epoch 34, Batch 1040, LR 0.000004 Loss 4.489634, Accuracy 89.507%\n",
      "Epoch 34, Batch 1041, LR 0.000004 Loss 4.489684, Accuracy 89.507%\n",
      "Epoch 34, Batch 1042, LR 0.000004 Loss 4.489536, Accuracy 89.506%\n",
      "Epoch 34, Batch 1043, LR 0.000004 Loss 4.490456, Accuracy 89.504%\n",
      "Epoch 34, Batch 1044, LR 0.000004 Loss 4.490599, Accuracy 89.504%\n",
      "Epoch 34, Batch 1045, LR 0.000004 Loss 4.490189, Accuracy 89.504%\n",
      "Epoch 34, Batch 1046, LR 0.000004 Loss 4.490312, Accuracy 89.505%\n",
      "Epoch 34, Batch 1047, LR 0.000004 Loss 4.490729, Accuracy 89.504%\n",
      "Epoch 34, Loss (train set) 4.490729, Accuracy (train set) 89.504%\n",
      "Epoch 34, Accuracy (validation set) 72.222%\n",
      "Epoch 34, EER (test set) 5.373%\n",
      "Epoch 35, Batch 1, LR 0.000004 Loss 4.071630, Accuracy 92.969%\n",
      "Epoch 35, Batch 2, LR 0.000004 Loss 4.543427, Accuracy 91.016%\n",
      "Epoch 35, Batch 3, LR 0.000004 Loss 4.421878, Accuracy 90.365%\n",
      "Epoch 35, Batch 4, LR 0.000004 Loss 4.574172, Accuracy 90.039%\n",
      "Epoch 35, Batch 5, LR 0.000004 Loss 4.637506, Accuracy 89.531%\n",
      "Epoch 35, Batch 6, LR 0.000004 Loss 4.586203, Accuracy 89.193%\n",
      "Epoch 35, Batch 7, LR 0.000004 Loss 4.566243, Accuracy 89.062%\n",
      "Epoch 35, Batch 8, LR 0.000004 Loss 4.478494, Accuracy 88.867%\n",
      "Epoch 35, Batch 9, LR 0.000004 Loss 4.534786, Accuracy 88.542%\n",
      "Epoch 35, Batch 10, LR 0.000004 Loss 4.523475, Accuracy 88.828%\n",
      "Epoch 35, Batch 11, LR 0.000004 Loss 4.483456, Accuracy 89.062%\n",
      "Epoch 35, Batch 12, LR 0.000004 Loss 4.480832, Accuracy 89.193%\n",
      "Epoch 35, Batch 13, LR 0.000004 Loss 4.471682, Accuracy 89.183%\n",
      "Epoch 35, Batch 14, LR 0.000004 Loss 4.519058, Accuracy 88.951%\n",
      "Epoch 35, Batch 15, LR 0.000004 Loss 4.486914, Accuracy 89.115%\n",
      "Epoch 35, Batch 16, LR 0.000004 Loss 4.426256, Accuracy 89.355%\n",
      "Epoch 35, Batch 17, LR 0.000004 Loss 4.439848, Accuracy 89.430%\n",
      "Epoch 35, Batch 18, LR 0.000004 Loss 4.455025, Accuracy 89.410%\n",
      "Epoch 35, Batch 19, LR 0.000004 Loss 4.502154, Accuracy 89.391%\n",
      "Epoch 35, Batch 20, LR 0.000004 Loss 4.500701, Accuracy 89.414%\n",
      "Epoch 35, Batch 21, LR 0.000004 Loss 4.491930, Accuracy 89.509%\n",
      "Epoch 35, Batch 22, LR 0.000004 Loss 4.497892, Accuracy 89.489%\n",
      "Epoch 35, Batch 23, LR 0.000004 Loss 4.487837, Accuracy 89.674%\n",
      "Epoch 35, Batch 24, LR 0.000004 Loss 4.490592, Accuracy 89.648%\n",
      "Epoch 35, Batch 25, LR 0.000004 Loss 4.501050, Accuracy 89.688%\n",
      "Epoch 35, Batch 26, LR 0.000004 Loss 4.495646, Accuracy 89.663%\n",
      "Epoch 35, Batch 27, LR 0.000004 Loss 4.495504, Accuracy 89.699%\n",
      "Epoch 35, Batch 28, LR 0.000004 Loss 4.491838, Accuracy 89.732%\n",
      "Epoch 35, Batch 29, LR 0.000004 Loss 4.456594, Accuracy 89.844%\n",
      "Epoch 35, Batch 30, LR 0.000004 Loss 4.496097, Accuracy 89.688%\n",
      "Epoch 35, Batch 31, LR 0.000004 Loss 4.472827, Accuracy 89.693%\n",
      "Epoch 35, Batch 32, LR 0.000004 Loss 4.475638, Accuracy 89.722%\n",
      "Epoch 35, Batch 33, LR 0.000004 Loss 4.495109, Accuracy 89.607%\n",
      "Epoch 35, Batch 34, LR 0.000004 Loss 4.501460, Accuracy 89.614%\n",
      "Epoch 35, Batch 35, LR 0.000004 Loss 4.511284, Accuracy 89.576%\n",
      "Epoch 35, Batch 36, LR 0.000004 Loss 4.503401, Accuracy 89.627%\n",
      "Epoch 35, Batch 37, LR 0.000004 Loss 4.505868, Accuracy 89.611%\n",
      "Epoch 35, Batch 38, LR 0.000004 Loss 4.518431, Accuracy 89.433%\n",
      "Epoch 35, Batch 39, LR 0.000004 Loss 4.505071, Accuracy 89.463%\n",
      "Epoch 35, Batch 40, LR 0.000004 Loss 4.486204, Accuracy 89.512%\n",
      "Epoch 35, Batch 41, LR 0.000004 Loss 4.485887, Accuracy 89.444%\n",
      "Epoch 35, Batch 42, LR 0.000004 Loss 4.466910, Accuracy 89.509%\n",
      "Epoch 35, Batch 43, LR 0.000004 Loss 4.460011, Accuracy 89.535%\n",
      "Epoch 35, Batch 44, LR 0.000004 Loss 4.472467, Accuracy 89.453%\n",
      "Epoch 35, Batch 45, LR 0.000004 Loss 4.470124, Accuracy 89.462%\n",
      "Epoch 35, Batch 46, LR 0.000004 Loss 4.458216, Accuracy 89.419%\n",
      "Epoch 35, Batch 47, LR 0.000004 Loss 4.465900, Accuracy 89.362%\n",
      "Epoch 35, Batch 48, LR 0.000004 Loss 4.474720, Accuracy 89.323%\n",
      "Epoch 35, Batch 49, LR 0.000004 Loss 4.475192, Accuracy 89.413%\n",
      "Epoch 35, Batch 50, LR 0.000004 Loss 4.475522, Accuracy 89.422%\n",
      "Epoch 35, Batch 51, LR 0.000004 Loss 4.489656, Accuracy 89.369%\n",
      "Epoch 35, Batch 52, LR 0.000004 Loss 4.484967, Accuracy 89.438%\n",
      "Epoch 35, Batch 53, LR 0.000004 Loss 4.490060, Accuracy 89.387%\n",
      "Epoch 35, Batch 54, LR 0.000004 Loss 4.489128, Accuracy 89.439%\n",
      "Epoch 35, Batch 55, LR 0.000004 Loss 4.486883, Accuracy 89.460%\n",
      "Epoch 35, Batch 56, LR 0.000004 Loss 4.487841, Accuracy 89.481%\n",
      "Epoch 35, Batch 57, LR 0.000004 Loss 4.483625, Accuracy 89.487%\n",
      "Epoch 35, Batch 58, LR 0.000004 Loss 4.486317, Accuracy 89.507%\n",
      "Epoch 35, Batch 59, LR 0.000004 Loss 4.484884, Accuracy 89.486%\n",
      "Epoch 35, Batch 60, LR 0.000004 Loss 4.492164, Accuracy 89.479%\n",
      "Epoch 35, Batch 61, LR 0.000004 Loss 4.495935, Accuracy 89.472%\n",
      "Epoch 35, Batch 62, LR 0.000004 Loss 4.485699, Accuracy 89.554%\n",
      "Epoch 35, Batch 63, LR 0.000004 Loss 4.490476, Accuracy 89.509%\n",
      "Epoch 35, Batch 64, LR 0.000004 Loss 4.492163, Accuracy 89.478%\n",
      "Epoch 35, Batch 65, LR 0.000004 Loss 4.486386, Accuracy 89.555%\n",
      "Epoch 35, Batch 66, LR 0.000004 Loss 4.481438, Accuracy 89.524%\n",
      "Epoch 35, Batch 67, LR 0.000004 Loss 4.480239, Accuracy 89.552%\n",
      "Epoch 35, Batch 68, LR 0.000004 Loss 4.480149, Accuracy 89.580%\n",
      "Epoch 35, Batch 69, LR 0.000004 Loss 4.483956, Accuracy 89.583%\n",
      "Epoch 35, Batch 70, LR 0.000004 Loss 4.477749, Accuracy 89.609%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 71, LR 0.000004 Loss 4.481759, Accuracy 89.591%\n",
      "Epoch 35, Batch 72, LR 0.000004 Loss 4.479424, Accuracy 89.605%\n",
      "Epoch 35, Batch 73, LR 0.000004 Loss 4.474535, Accuracy 89.608%\n",
      "Epoch 35, Batch 74, LR 0.000004 Loss 4.480613, Accuracy 89.601%\n",
      "Epoch 35, Batch 75, LR 0.000004 Loss 4.480586, Accuracy 89.594%\n",
      "Epoch 35, Batch 76, LR 0.000004 Loss 4.483669, Accuracy 89.597%\n",
      "Epoch 35, Batch 77, LR 0.000004 Loss 4.480792, Accuracy 89.560%\n",
      "Epoch 35, Batch 78, LR 0.000004 Loss 4.479332, Accuracy 89.593%\n",
      "Epoch 35, Batch 79, LR 0.000004 Loss 4.471716, Accuracy 89.606%\n",
      "Epoch 35, Batch 80, LR 0.000004 Loss 4.467817, Accuracy 89.648%\n",
      "Epoch 35, Batch 81, LR 0.000004 Loss 4.476784, Accuracy 89.583%\n",
      "Epoch 35, Batch 82, LR 0.000004 Loss 4.482039, Accuracy 89.577%\n",
      "Epoch 35, Batch 83, LR 0.000004 Loss 4.496310, Accuracy 89.486%\n",
      "Epoch 35, Batch 84, LR 0.000004 Loss 4.496281, Accuracy 89.453%\n",
      "Epoch 35, Batch 85, LR 0.000004 Loss 4.500235, Accuracy 89.458%\n",
      "Epoch 35, Batch 86, LR 0.000004 Loss 4.493937, Accuracy 89.489%\n",
      "Epoch 35, Batch 87, LR 0.000004 Loss 4.485801, Accuracy 89.494%\n",
      "Epoch 35, Batch 88, LR 0.000004 Loss 4.485425, Accuracy 89.480%\n",
      "Epoch 35, Batch 89, LR 0.000004 Loss 4.482756, Accuracy 89.501%\n",
      "Epoch 35, Batch 90, LR 0.000004 Loss 4.482543, Accuracy 89.479%\n",
      "Epoch 35, Batch 91, LR 0.000004 Loss 4.477850, Accuracy 89.492%\n",
      "Epoch 35, Batch 92, LR 0.000004 Loss 4.477322, Accuracy 89.487%\n",
      "Epoch 35, Batch 93, LR 0.000004 Loss 4.476867, Accuracy 89.483%\n",
      "Epoch 35, Batch 94, LR 0.000004 Loss 4.475736, Accuracy 89.445%\n",
      "Epoch 35, Batch 95, LR 0.000004 Loss 4.476422, Accuracy 89.449%\n",
      "Epoch 35, Batch 96, LR 0.000004 Loss 4.474122, Accuracy 89.486%\n",
      "Epoch 35, Batch 97, LR 0.000004 Loss 4.473935, Accuracy 89.465%\n",
      "Epoch 35, Batch 98, LR 0.000004 Loss 4.483641, Accuracy 89.445%\n",
      "Epoch 35, Batch 99, LR 0.000004 Loss 4.482429, Accuracy 89.457%\n",
      "Epoch 35, Batch 100, LR 0.000004 Loss 4.483725, Accuracy 89.453%\n",
      "Epoch 35, Batch 101, LR 0.000004 Loss 4.481912, Accuracy 89.465%\n",
      "Epoch 35, Batch 102, LR 0.000004 Loss 4.481091, Accuracy 89.438%\n",
      "Epoch 35, Batch 103, LR 0.000004 Loss 4.474486, Accuracy 89.449%\n",
      "Epoch 35, Batch 104, LR 0.000004 Loss 4.478881, Accuracy 89.431%\n",
      "Epoch 35, Batch 105, LR 0.000004 Loss 4.487904, Accuracy 89.405%\n",
      "Epoch 35, Batch 106, LR 0.000004 Loss 4.484028, Accuracy 89.409%\n",
      "Epoch 35, Batch 107, LR 0.000004 Loss 4.487095, Accuracy 89.428%\n",
      "Epoch 35, Batch 108, LR 0.000004 Loss 4.486027, Accuracy 89.417%\n",
      "Epoch 35, Batch 109, LR 0.000004 Loss 4.490414, Accuracy 89.407%\n",
      "Epoch 35, Batch 110, LR 0.000004 Loss 4.490184, Accuracy 89.425%\n",
      "Epoch 35, Batch 111, LR 0.000004 Loss 4.490456, Accuracy 89.450%\n",
      "Epoch 35, Batch 112, LR 0.000004 Loss 4.493901, Accuracy 89.453%\n",
      "Epoch 35, Batch 113, LR 0.000004 Loss 4.492675, Accuracy 89.450%\n",
      "Epoch 35, Batch 114, LR 0.000004 Loss 4.490933, Accuracy 89.446%\n",
      "Epoch 35, Batch 115, LR 0.000004 Loss 4.487092, Accuracy 89.457%\n",
      "Epoch 35, Batch 116, LR 0.000004 Loss 4.493424, Accuracy 89.406%\n",
      "Epoch 35, Batch 117, LR 0.000004 Loss 4.489264, Accuracy 89.403%\n",
      "Epoch 35, Batch 118, LR 0.000004 Loss 4.485236, Accuracy 89.413%\n",
      "Epoch 35, Batch 119, LR 0.000004 Loss 4.486202, Accuracy 89.404%\n",
      "Epoch 35, Batch 120, LR 0.000004 Loss 4.478579, Accuracy 89.440%\n",
      "Epoch 35, Batch 121, LR 0.000004 Loss 4.482781, Accuracy 89.418%\n",
      "Epoch 35, Batch 122, LR 0.000004 Loss 4.487446, Accuracy 89.357%\n",
      "Epoch 35, Batch 123, LR 0.000004 Loss 4.481553, Accuracy 89.386%\n",
      "Epoch 35, Batch 124, LR 0.000004 Loss 4.475076, Accuracy 89.428%\n",
      "Epoch 35, Batch 125, LR 0.000004 Loss 4.473079, Accuracy 89.438%\n",
      "Epoch 35, Batch 126, LR 0.000004 Loss 4.470508, Accuracy 89.447%\n",
      "Epoch 35, Batch 127, LR 0.000004 Loss 4.471016, Accuracy 89.450%\n",
      "Epoch 35, Batch 128, LR 0.000004 Loss 4.464860, Accuracy 89.465%\n",
      "Epoch 35, Batch 129, LR 0.000004 Loss 4.465008, Accuracy 89.474%\n",
      "Epoch 35, Batch 130, LR 0.000004 Loss 4.466519, Accuracy 89.477%\n",
      "Epoch 35, Batch 131, LR 0.000004 Loss 4.460148, Accuracy 89.516%\n",
      "Epoch 35, Batch 132, LR 0.000004 Loss 4.461253, Accuracy 89.524%\n",
      "Epoch 35, Batch 133, LR 0.000004 Loss 4.462924, Accuracy 89.497%\n",
      "Epoch 35, Batch 134, LR 0.000004 Loss 4.458860, Accuracy 89.517%\n",
      "Epoch 35, Batch 135, LR 0.000004 Loss 4.460435, Accuracy 89.502%\n",
      "Epoch 35, Batch 136, LR 0.000004 Loss 4.458760, Accuracy 89.516%\n",
      "Epoch 35, Batch 137, LR 0.000004 Loss 4.456097, Accuracy 89.513%\n",
      "Epoch 35, Batch 138, LR 0.000004 Loss 4.457343, Accuracy 89.481%\n",
      "Epoch 35, Batch 139, LR 0.000004 Loss 4.454952, Accuracy 89.478%\n",
      "Epoch 35, Batch 140, LR 0.000004 Loss 4.455093, Accuracy 89.492%\n",
      "Epoch 35, Batch 141, LR 0.000004 Loss 4.457551, Accuracy 89.467%\n",
      "Epoch 35, Batch 142, LR 0.000004 Loss 4.458787, Accuracy 89.475%\n",
      "Epoch 35, Batch 143, LR 0.000004 Loss 4.456467, Accuracy 89.478%\n",
      "Epoch 35, Batch 144, LR 0.000004 Loss 4.453677, Accuracy 89.491%\n",
      "Epoch 35, Batch 145, LR 0.000004 Loss 4.457008, Accuracy 89.461%\n",
      "Epoch 35, Batch 146, LR 0.000004 Loss 4.460394, Accuracy 89.464%\n",
      "Epoch 35, Batch 147, LR 0.000004 Loss 4.459419, Accuracy 89.456%\n",
      "Epoch 35, Batch 148, LR 0.000004 Loss 4.457495, Accuracy 89.464%\n",
      "Epoch 35, Batch 149, LR 0.000004 Loss 4.459435, Accuracy 89.466%\n",
      "Epoch 35, Batch 150, LR 0.000004 Loss 4.460784, Accuracy 89.474%\n",
      "Epoch 35, Batch 151, LR 0.000004 Loss 4.458737, Accuracy 89.487%\n",
      "Epoch 35, Batch 152, LR 0.000004 Loss 4.459614, Accuracy 89.499%\n",
      "Epoch 35, Batch 153, LR 0.000004 Loss 4.456966, Accuracy 89.517%\n",
      "Epoch 35, Batch 154, LR 0.000004 Loss 4.452725, Accuracy 89.524%\n",
      "Epoch 35, Batch 155, LR 0.000004 Loss 4.453835, Accuracy 89.541%\n",
      "Epoch 35, Batch 156, LR 0.000004 Loss 4.452054, Accuracy 89.553%\n",
      "Epoch 35, Batch 157, LR 0.000004 Loss 4.454150, Accuracy 89.555%\n",
      "Epoch 35, Batch 158, LR 0.000004 Loss 4.459234, Accuracy 89.512%\n",
      "Epoch 35, Batch 159, LR 0.000004 Loss 4.458923, Accuracy 89.505%\n",
      "Epoch 35, Batch 160, LR 0.000004 Loss 4.457917, Accuracy 89.492%\n",
      "Epoch 35, Batch 161, LR 0.000004 Loss 4.458875, Accuracy 89.494%\n",
      "Epoch 35, Batch 162, LR 0.000004 Loss 4.456292, Accuracy 89.521%\n",
      "Epoch 35, Batch 163, LR 0.000004 Loss 4.458215, Accuracy 89.508%\n",
      "Epoch 35, Batch 164, LR 0.000004 Loss 4.458682, Accuracy 89.510%\n",
      "Epoch 35, Batch 165, LR 0.000004 Loss 4.455643, Accuracy 89.522%\n",
      "Epoch 35, Batch 166, LR 0.000004 Loss 4.458176, Accuracy 89.500%\n",
      "Epoch 35, Batch 167, LR 0.000004 Loss 4.454722, Accuracy 89.516%\n",
      "Epoch 35, Batch 168, LR 0.000004 Loss 4.456838, Accuracy 89.486%\n",
      "Epoch 35, Batch 169, LR 0.000004 Loss 4.456708, Accuracy 89.469%\n",
      "Epoch 35, Batch 170, LR 0.000004 Loss 4.460647, Accuracy 89.444%\n",
      "Epoch 35, Batch 171, LR 0.000004 Loss 4.462181, Accuracy 89.442%\n",
      "Epoch 35, Batch 172, LR 0.000004 Loss 4.461175, Accuracy 89.467%\n",
      "Epoch 35, Batch 173, LR 0.000004 Loss 4.461078, Accuracy 89.460%\n",
      "Epoch 35, Batch 174, LR 0.000004 Loss 4.460268, Accuracy 89.467%\n",
      "Epoch 35, Batch 175, LR 0.000004 Loss 4.459091, Accuracy 89.478%\n",
      "Epoch 35, Batch 176, LR 0.000004 Loss 4.457438, Accuracy 89.493%\n",
      "Epoch 35, Batch 177, LR 0.000004 Loss 4.459176, Accuracy 89.486%\n",
      "Epoch 35, Batch 178, LR 0.000004 Loss 4.459416, Accuracy 89.488%\n",
      "Epoch 35, Batch 179, LR 0.000004 Loss 4.462185, Accuracy 89.481%\n",
      "Epoch 35, Batch 180, LR 0.000004 Loss 4.462927, Accuracy 89.484%\n",
      "Epoch 35, Batch 181, LR 0.000004 Loss 4.461614, Accuracy 89.481%\n",
      "Epoch 35, Batch 182, LR 0.000004 Loss 4.462936, Accuracy 89.479%\n",
      "Epoch 35, Batch 183, LR 0.000004 Loss 4.461373, Accuracy 89.502%\n",
      "Epoch 35, Batch 184, LR 0.000004 Loss 4.460899, Accuracy 89.496%\n",
      "Epoch 35, Batch 185, LR 0.000004 Loss 4.461038, Accuracy 89.502%\n",
      "Epoch 35, Batch 186, LR 0.000004 Loss 4.462668, Accuracy 89.512%\n",
      "Epoch 35, Batch 187, LR 0.000004 Loss 4.466985, Accuracy 89.489%\n",
      "Epoch 35, Batch 188, LR 0.000004 Loss 4.469812, Accuracy 89.470%\n",
      "Epoch 35, Batch 189, LR 0.000004 Loss 4.467409, Accuracy 89.484%\n",
      "Epoch 35, Batch 190, LR 0.000004 Loss 4.467316, Accuracy 89.490%\n",
      "Epoch 35, Batch 191, LR 0.000004 Loss 4.463570, Accuracy 89.488%\n",
      "Epoch 35, Batch 192, LR 0.000004 Loss 4.463278, Accuracy 89.490%\n",
      "Epoch 35, Batch 193, LR 0.000004 Loss 4.464954, Accuracy 89.488%\n",
      "Epoch 35, Batch 194, LR 0.000004 Loss 4.466367, Accuracy 89.461%\n",
      "Epoch 35, Batch 195, LR 0.000004 Loss 4.465216, Accuracy 89.463%\n",
      "Epoch 35, Batch 196, LR 0.000004 Loss 4.466918, Accuracy 89.469%\n",
      "Epoch 35, Batch 197, LR 0.000004 Loss 4.465631, Accuracy 89.463%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 198, LR 0.000004 Loss 4.466472, Accuracy 89.457%\n",
      "Epoch 35, Batch 199, LR 0.000004 Loss 4.468833, Accuracy 89.443%\n",
      "Epoch 35, Batch 200, LR 0.000004 Loss 4.466923, Accuracy 89.441%\n",
      "Epoch 35, Batch 201, LR 0.000004 Loss 4.462389, Accuracy 89.451%\n",
      "Epoch 35, Batch 202, LR 0.000004 Loss 4.464878, Accuracy 89.445%\n",
      "Epoch 35, Batch 203, LR 0.000004 Loss 4.465047, Accuracy 89.440%\n",
      "Epoch 35, Batch 204, LR 0.000004 Loss 4.460869, Accuracy 89.453%\n",
      "Epoch 35, Batch 205, LR 0.000004 Loss 4.458376, Accuracy 89.463%\n",
      "Epoch 35, Batch 206, LR 0.000004 Loss 4.455649, Accuracy 89.483%\n",
      "Epoch 35, Batch 207, LR 0.000004 Loss 4.455848, Accuracy 89.478%\n",
      "Epoch 35, Batch 208, LR 0.000004 Loss 4.452760, Accuracy 89.487%\n",
      "Epoch 35, Batch 209, LR 0.000004 Loss 4.449649, Accuracy 89.496%\n",
      "Epoch 35, Batch 210, LR 0.000004 Loss 4.451022, Accuracy 89.475%\n",
      "Epoch 35, Batch 211, LR 0.000004 Loss 4.448356, Accuracy 89.485%\n",
      "Epoch 35, Batch 212, LR 0.000004 Loss 4.449104, Accuracy 89.479%\n",
      "Epoch 35, Batch 213, LR 0.000004 Loss 4.446422, Accuracy 89.492%\n",
      "Epoch 35, Batch 214, LR 0.000004 Loss 4.442697, Accuracy 89.497%\n",
      "Epoch 35, Batch 215, LR 0.000004 Loss 4.444204, Accuracy 89.499%\n",
      "Epoch 35, Batch 216, LR 0.000004 Loss 4.443339, Accuracy 89.507%\n",
      "Epoch 35, Batch 217, LR 0.000004 Loss 4.444873, Accuracy 89.495%\n",
      "Epoch 35, Batch 218, LR 0.000004 Loss 4.445926, Accuracy 89.485%\n",
      "Epoch 35, Batch 219, LR 0.000004 Loss 4.446928, Accuracy 89.480%\n",
      "Epoch 35, Batch 220, LR 0.000004 Loss 4.448870, Accuracy 89.467%\n",
      "Epoch 35, Batch 221, LR 0.000004 Loss 4.448148, Accuracy 89.462%\n",
      "Epoch 35, Batch 222, LR 0.000004 Loss 4.448746, Accuracy 89.467%\n",
      "Epoch 35, Batch 223, LR 0.000004 Loss 4.452051, Accuracy 89.451%\n",
      "Epoch 35, Batch 224, LR 0.000004 Loss 4.452704, Accuracy 89.443%\n",
      "Epoch 35, Batch 225, LR 0.000004 Loss 4.451492, Accuracy 89.444%\n",
      "Epoch 35, Batch 226, LR 0.000004 Loss 4.449942, Accuracy 89.446%\n",
      "Epoch 35, Batch 227, LR 0.000004 Loss 4.450459, Accuracy 89.451%\n",
      "Epoch 35, Batch 228, LR 0.000004 Loss 4.446287, Accuracy 89.474%\n",
      "Epoch 35, Batch 229, LR 0.000004 Loss 4.449575, Accuracy 89.465%\n",
      "Epoch 35, Batch 230, LR 0.000004 Loss 4.451175, Accuracy 89.446%\n",
      "Epoch 35, Batch 231, LR 0.000004 Loss 4.451509, Accuracy 89.438%\n",
      "Epoch 35, Batch 232, LR 0.000004 Loss 4.452255, Accuracy 89.446%\n",
      "Epoch 35, Batch 233, LR 0.000004 Loss 4.454377, Accuracy 89.462%\n",
      "Epoch 35, Batch 234, LR 0.000004 Loss 4.453787, Accuracy 89.473%\n",
      "Epoch 35, Batch 235, LR 0.000004 Loss 4.452314, Accuracy 89.478%\n",
      "Epoch 35, Batch 236, LR 0.000004 Loss 4.456117, Accuracy 89.450%\n",
      "Epoch 35, Batch 237, LR 0.000004 Loss 4.454018, Accuracy 89.461%\n",
      "Epoch 35, Batch 238, LR 0.000004 Loss 4.457464, Accuracy 89.463%\n",
      "Epoch 35, Batch 239, LR 0.000004 Loss 4.456446, Accuracy 89.465%\n",
      "Epoch 35, Batch 240, LR 0.000004 Loss 4.454085, Accuracy 89.479%\n",
      "Epoch 35, Batch 241, LR 0.000004 Loss 4.452451, Accuracy 89.487%\n",
      "Epoch 35, Batch 242, LR 0.000004 Loss 4.454869, Accuracy 89.463%\n",
      "Epoch 35, Batch 243, LR 0.000004 Loss 4.459395, Accuracy 89.432%\n",
      "Epoch 35, Batch 244, LR 0.000004 Loss 4.461658, Accuracy 89.415%\n",
      "Epoch 35, Batch 245, LR 0.000004 Loss 4.460851, Accuracy 89.410%\n",
      "Epoch 35, Batch 246, LR 0.000004 Loss 4.462430, Accuracy 89.399%\n",
      "Epoch 35, Batch 247, LR 0.000004 Loss 4.463509, Accuracy 89.407%\n",
      "Epoch 35, Batch 248, LR 0.000004 Loss 4.463619, Accuracy 89.403%\n",
      "Epoch 35, Batch 249, LR 0.000004 Loss 4.466422, Accuracy 89.389%\n",
      "Epoch 35, Batch 250, LR 0.000004 Loss 4.465042, Accuracy 89.394%\n",
      "Epoch 35, Batch 251, LR 0.000004 Loss 4.464099, Accuracy 89.389%\n",
      "Epoch 35, Batch 252, LR 0.000004 Loss 4.463461, Accuracy 89.394%\n",
      "Epoch 35, Batch 253, LR 0.000004 Loss 4.465007, Accuracy 89.393%\n",
      "Epoch 35, Batch 254, LR 0.000004 Loss 4.468305, Accuracy 89.385%\n",
      "Epoch 35, Batch 255, LR 0.000004 Loss 4.468192, Accuracy 89.403%\n",
      "Epoch 35, Batch 256, LR 0.000004 Loss 4.467836, Accuracy 89.404%\n",
      "Epoch 35, Batch 257, LR 0.000004 Loss 4.464883, Accuracy 89.415%\n",
      "Epoch 35, Batch 258, LR 0.000004 Loss 4.462852, Accuracy 89.408%\n",
      "Epoch 35, Batch 259, LR 0.000004 Loss 4.464317, Accuracy 89.397%\n",
      "Epoch 35, Batch 260, LR 0.000004 Loss 4.463016, Accuracy 89.411%\n",
      "Epoch 35, Batch 261, LR 0.000004 Loss 4.463806, Accuracy 89.407%\n",
      "Epoch 35, Batch 262, LR 0.000004 Loss 4.464838, Accuracy 89.414%\n",
      "Epoch 35, Batch 263, LR 0.000004 Loss 4.465648, Accuracy 89.413%\n",
      "Epoch 35, Batch 264, LR 0.000004 Loss 4.467303, Accuracy 89.418%\n",
      "Epoch 35, Batch 265, LR 0.000004 Loss 4.467913, Accuracy 89.407%\n",
      "Epoch 35, Batch 266, LR 0.000004 Loss 4.467570, Accuracy 89.400%\n",
      "Epoch 35, Batch 267, LR 0.000004 Loss 4.470002, Accuracy 89.405%\n",
      "Epoch 35, Batch 268, LR 0.000004 Loss 4.470493, Accuracy 89.401%\n",
      "Epoch 35, Batch 269, LR 0.000004 Loss 4.470202, Accuracy 89.399%\n",
      "Epoch 35, Batch 270, LR 0.000004 Loss 4.469084, Accuracy 89.401%\n",
      "Epoch 35, Batch 271, LR 0.000004 Loss 4.471346, Accuracy 89.391%\n",
      "Epoch 35, Batch 272, LR 0.000004 Loss 4.469683, Accuracy 89.404%\n",
      "Epoch 35, Batch 273, LR 0.000004 Loss 4.470379, Accuracy 89.397%\n",
      "Epoch 35, Batch 274, LR 0.000004 Loss 4.473218, Accuracy 89.393%\n",
      "Epoch 35, Batch 275, LR 0.000004 Loss 4.473445, Accuracy 89.386%\n",
      "Epoch 35, Batch 276, LR 0.000004 Loss 4.473610, Accuracy 89.377%\n",
      "Epoch 35, Batch 277, LR 0.000004 Loss 4.474208, Accuracy 89.381%\n",
      "Epoch 35, Batch 278, LR 0.000004 Loss 4.473709, Accuracy 89.386%\n",
      "Epoch 35, Batch 279, LR 0.000004 Loss 4.471578, Accuracy 89.396%\n",
      "Epoch 35, Batch 280, LR 0.000004 Loss 4.470955, Accuracy 89.392%\n",
      "Epoch 35, Batch 281, LR 0.000004 Loss 4.468645, Accuracy 89.396%\n",
      "Epoch 35, Batch 282, LR 0.000004 Loss 4.466023, Accuracy 89.414%\n",
      "Epoch 35, Batch 283, LR 0.000004 Loss 4.466813, Accuracy 89.427%\n",
      "Epoch 35, Batch 284, LR 0.000004 Loss 4.467168, Accuracy 89.426%\n",
      "Epoch 35, Batch 285, LR 0.000004 Loss 4.467013, Accuracy 89.424%\n",
      "Epoch 35, Batch 286, LR 0.000004 Loss 4.466070, Accuracy 89.423%\n",
      "Epoch 35, Batch 287, LR 0.000004 Loss 4.464619, Accuracy 89.427%\n",
      "Epoch 35, Batch 288, LR 0.000004 Loss 4.464689, Accuracy 89.434%\n",
      "Epoch 35, Batch 289, LR 0.000004 Loss 4.466642, Accuracy 89.433%\n",
      "Epoch 35, Batch 290, LR 0.000004 Loss 4.466725, Accuracy 89.437%\n",
      "Epoch 35, Batch 291, LR 0.000004 Loss 4.466831, Accuracy 89.433%\n",
      "Epoch 35, Batch 292, LR 0.000004 Loss 4.469110, Accuracy 89.421%\n",
      "Epoch 35, Batch 293, LR 0.000004 Loss 4.470626, Accuracy 89.414%\n",
      "Epoch 35, Batch 294, LR 0.000004 Loss 4.468986, Accuracy 89.413%\n",
      "Epoch 35, Batch 295, LR 0.000004 Loss 4.470880, Accuracy 89.399%\n",
      "Epoch 35, Batch 296, LR 0.000004 Loss 4.472774, Accuracy 89.398%\n",
      "Epoch 35, Batch 297, LR 0.000004 Loss 4.472358, Accuracy 89.407%\n",
      "Epoch 35, Batch 298, LR 0.000004 Loss 4.468951, Accuracy 89.414%\n",
      "Epoch 35, Batch 299, LR 0.000004 Loss 4.468836, Accuracy 89.420%\n",
      "Epoch 35, Batch 300, LR 0.000004 Loss 4.469261, Accuracy 89.430%\n",
      "Epoch 35, Batch 301, LR 0.000004 Loss 4.468038, Accuracy 89.436%\n",
      "Epoch 35, Batch 302, LR 0.000004 Loss 4.465325, Accuracy 89.443%\n",
      "Epoch 35, Batch 303, LR 0.000004 Loss 4.463363, Accuracy 89.452%\n",
      "Epoch 35, Batch 304, LR 0.000004 Loss 4.461203, Accuracy 89.458%\n",
      "Epoch 35, Batch 305, LR 0.000004 Loss 4.462775, Accuracy 89.454%\n",
      "Epoch 35, Batch 306, LR 0.000004 Loss 4.462960, Accuracy 89.445%\n",
      "Epoch 35, Batch 307, LR 0.000004 Loss 4.461711, Accuracy 89.462%\n",
      "Epoch 35, Batch 308, LR 0.000004 Loss 4.461614, Accuracy 89.453%\n",
      "Epoch 35, Batch 309, LR 0.000004 Loss 4.461413, Accuracy 89.459%\n",
      "Epoch 35, Batch 310, LR 0.000004 Loss 4.461178, Accuracy 89.456%\n",
      "Epoch 35, Batch 311, LR 0.000004 Loss 4.460821, Accuracy 89.449%\n",
      "Epoch 35, Batch 312, LR 0.000004 Loss 4.459504, Accuracy 89.461%\n",
      "Epoch 35, Batch 313, LR 0.000004 Loss 4.458173, Accuracy 89.464%\n",
      "Epoch 35, Batch 314, LR 0.000004 Loss 4.458262, Accuracy 89.471%\n",
      "Epoch 35, Batch 315, LR 0.000004 Loss 4.456942, Accuracy 89.474%\n",
      "Epoch 35, Batch 316, LR 0.000004 Loss 4.456584, Accuracy 89.480%\n",
      "Epoch 35, Batch 317, LR 0.000004 Loss 4.457061, Accuracy 89.477%\n",
      "Epoch 35, Batch 318, LR 0.000004 Loss 4.457613, Accuracy 89.480%\n",
      "Epoch 35, Batch 319, LR 0.000004 Loss 4.459354, Accuracy 89.462%\n",
      "Epoch 35, Batch 320, LR 0.000004 Loss 4.459175, Accuracy 89.463%\n",
      "Epoch 35, Batch 321, LR 0.000004 Loss 4.459713, Accuracy 89.464%\n",
      "Epoch 35, Batch 322, LR 0.000004 Loss 4.460612, Accuracy 89.458%\n",
      "Epoch 35, Batch 323, LR 0.000004 Loss 4.458602, Accuracy 89.466%\n",
      "Epoch 35, Batch 324, LR 0.000004 Loss 4.458029, Accuracy 89.465%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 325, LR 0.000004 Loss 4.458200, Accuracy 89.476%\n",
      "Epoch 35, Batch 326, LR 0.000004 Loss 4.457053, Accuracy 89.477%\n",
      "Epoch 35, Batch 327, LR 0.000004 Loss 4.456874, Accuracy 89.473%\n",
      "Epoch 35, Batch 328, LR 0.000004 Loss 4.454439, Accuracy 89.479%\n",
      "Epoch 35, Batch 329, LR 0.000004 Loss 4.456196, Accuracy 89.485%\n",
      "Epoch 35, Batch 330, LR 0.000004 Loss 4.454088, Accuracy 89.489%\n",
      "Epoch 35, Batch 331, LR 0.000004 Loss 4.451640, Accuracy 89.494%\n",
      "Epoch 35, Batch 332, LR 0.000004 Loss 4.449304, Accuracy 89.498%\n",
      "Epoch 35, Batch 333, LR 0.000004 Loss 4.448905, Accuracy 89.508%\n",
      "Epoch 35, Batch 334, LR 0.000004 Loss 4.448446, Accuracy 89.509%\n",
      "Epoch 35, Batch 335, LR 0.000004 Loss 4.450736, Accuracy 89.492%\n",
      "Epoch 35, Batch 336, LR 0.000004 Loss 4.450638, Accuracy 89.497%\n",
      "Epoch 35, Batch 337, LR 0.000004 Loss 4.452052, Accuracy 89.491%\n",
      "Epoch 35, Batch 338, LR 0.000004 Loss 4.451985, Accuracy 89.495%\n",
      "Epoch 35, Batch 339, LR 0.000004 Loss 4.451640, Accuracy 89.505%\n",
      "Epoch 35, Batch 340, LR 0.000004 Loss 4.451835, Accuracy 89.501%\n",
      "Epoch 35, Batch 341, LR 0.000004 Loss 4.453163, Accuracy 89.496%\n",
      "Epoch 35, Batch 342, LR 0.000004 Loss 4.453155, Accuracy 89.490%\n",
      "Epoch 35, Batch 343, LR 0.000004 Loss 4.454033, Accuracy 89.502%\n",
      "Epoch 35, Batch 344, LR 0.000004 Loss 4.455007, Accuracy 89.489%\n",
      "Epoch 35, Batch 345, LR 0.000004 Loss 4.452910, Accuracy 89.493%\n",
      "Epoch 35, Batch 346, LR 0.000004 Loss 4.451884, Accuracy 89.496%\n",
      "Epoch 35, Batch 347, LR 0.000004 Loss 4.451501, Accuracy 89.493%\n",
      "Epoch 35, Batch 348, LR 0.000004 Loss 4.454566, Accuracy 89.487%\n",
      "Epoch 35, Batch 349, LR 0.000004 Loss 4.454449, Accuracy 89.483%\n",
      "Epoch 35, Batch 350, LR 0.000004 Loss 4.454463, Accuracy 89.482%\n",
      "Epoch 35, Batch 351, LR 0.000004 Loss 4.453341, Accuracy 89.483%\n",
      "Epoch 35, Batch 352, LR 0.000004 Loss 4.452155, Accuracy 89.500%\n",
      "Epoch 35, Batch 353, LR 0.000004 Loss 4.454125, Accuracy 89.483%\n",
      "Epoch 35, Batch 354, LR 0.000004 Loss 4.453658, Accuracy 89.486%\n",
      "Epoch 35, Batch 355, LR 0.000004 Loss 4.454509, Accuracy 89.476%\n",
      "Epoch 35, Batch 356, LR 0.000004 Loss 4.452999, Accuracy 89.477%\n",
      "Epoch 35, Batch 357, LR 0.000004 Loss 4.452930, Accuracy 89.478%\n",
      "Epoch 35, Batch 358, LR 0.000004 Loss 4.452887, Accuracy 89.468%\n",
      "Epoch 35, Batch 359, LR 0.000004 Loss 4.452908, Accuracy 89.463%\n",
      "Epoch 35, Batch 360, LR 0.000004 Loss 4.453011, Accuracy 89.462%\n",
      "Epoch 35, Batch 361, LR 0.000004 Loss 4.452142, Accuracy 89.461%\n",
      "Epoch 35, Batch 362, LR 0.000004 Loss 4.451022, Accuracy 89.464%\n",
      "Epoch 35, Batch 363, LR 0.000004 Loss 4.450749, Accuracy 89.456%\n",
      "Epoch 35, Batch 364, LR 0.000004 Loss 4.450479, Accuracy 89.462%\n",
      "Epoch 35, Batch 365, LR 0.000004 Loss 4.447887, Accuracy 89.473%\n",
      "Epoch 35, Batch 366, LR 0.000004 Loss 4.447633, Accuracy 89.470%\n",
      "Epoch 35, Batch 367, LR 0.000004 Loss 4.447422, Accuracy 89.475%\n",
      "Epoch 35, Batch 368, LR 0.000004 Loss 4.447460, Accuracy 89.483%\n",
      "Epoch 35, Batch 369, LR 0.000004 Loss 4.447943, Accuracy 89.471%\n",
      "Epoch 35, Batch 370, LR 0.000004 Loss 4.447467, Accuracy 89.474%\n",
      "Epoch 35, Batch 371, LR 0.000004 Loss 4.446551, Accuracy 89.477%\n",
      "Epoch 35, Batch 372, LR 0.000004 Loss 4.445299, Accuracy 89.480%\n",
      "Epoch 35, Batch 373, LR 0.000004 Loss 4.445203, Accuracy 89.483%\n",
      "Epoch 35, Batch 374, LR 0.000004 Loss 4.446070, Accuracy 89.482%\n",
      "Epoch 35, Batch 375, LR 0.000004 Loss 4.447375, Accuracy 89.469%\n",
      "Epoch 35, Batch 376, LR 0.000004 Loss 4.448286, Accuracy 89.470%\n",
      "Epoch 35, Batch 377, LR 0.000004 Loss 4.450964, Accuracy 89.456%\n",
      "Epoch 35, Batch 378, LR 0.000004 Loss 4.451129, Accuracy 89.463%\n",
      "Epoch 35, Batch 379, LR 0.000004 Loss 4.451366, Accuracy 89.464%\n",
      "Epoch 35, Batch 380, LR 0.000004 Loss 4.451087, Accuracy 89.463%\n",
      "Epoch 35, Batch 381, LR 0.000004 Loss 4.450831, Accuracy 89.466%\n",
      "Epoch 35, Batch 382, LR 0.000004 Loss 4.452477, Accuracy 89.469%\n",
      "Epoch 35, Batch 383, LR 0.000004 Loss 4.452124, Accuracy 89.475%\n",
      "Epoch 35, Batch 384, LR 0.000004 Loss 4.452057, Accuracy 89.476%\n",
      "Epoch 35, Batch 385, LR 0.000004 Loss 4.452427, Accuracy 89.478%\n",
      "Epoch 35, Batch 386, LR 0.000004 Loss 4.453544, Accuracy 89.471%\n",
      "Epoch 35, Batch 387, LR 0.000004 Loss 4.452342, Accuracy 89.476%\n",
      "Epoch 35, Batch 388, LR 0.000004 Loss 4.450315, Accuracy 89.491%\n",
      "Epoch 35, Batch 389, LR 0.000004 Loss 4.450308, Accuracy 89.498%\n",
      "Epoch 35, Batch 390, LR 0.000004 Loss 4.451469, Accuracy 89.491%\n",
      "Epoch 35, Batch 391, LR 0.000004 Loss 4.450783, Accuracy 89.496%\n",
      "Epoch 35, Batch 392, LR 0.000004 Loss 4.449809, Accuracy 89.503%\n",
      "Epoch 35, Batch 393, LR 0.000004 Loss 4.450806, Accuracy 89.500%\n",
      "Epoch 35, Batch 394, LR 0.000004 Loss 4.449472, Accuracy 89.505%\n",
      "Epoch 35, Batch 395, LR 0.000004 Loss 4.447151, Accuracy 89.519%\n",
      "Epoch 35, Batch 396, LR 0.000004 Loss 4.445660, Accuracy 89.520%\n",
      "Epoch 35, Batch 397, LR 0.000004 Loss 4.445948, Accuracy 89.525%\n",
      "Epoch 35, Batch 398, LR 0.000004 Loss 4.448265, Accuracy 89.514%\n",
      "Epoch 35, Batch 399, LR 0.000004 Loss 4.447753, Accuracy 89.509%\n",
      "Epoch 35, Batch 400, LR 0.000004 Loss 4.446312, Accuracy 89.523%\n",
      "Epoch 35, Batch 401, LR 0.000004 Loss 4.446637, Accuracy 89.526%\n",
      "Epoch 35, Batch 402, LR 0.000004 Loss 4.446627, Accuracy 89.527%\n",
      "Epoch 35, Batch 403, LR 0.000004 Loss 4.445217, Accuracy 89.534%\n",
      "Epoch 35, Batch 404, LR 0.000004 Loss 4.445945, Accuracy 89.525%\n",
      "Epoch 35, Batch 405, LR 0.000004 Loss 4.445722, Accuracy 89.533%\n",
      "Epoch 35, Batch 406, LR 0.000004 Loss 4.445572, Accuracy 89.530%\n",
      "Epoch 35, Batch 407, LR 0.000004 Loss 4.444234, Accuracy 89.535%\n",
      "Epoch 35, Batch 408, LR 0.000004 Loss 4.443850, Accuracy 89.539%\n",
      "Epoch 35, Batch 409, LR 0.000004 Loss 4.442536, Accuracy 89.542%\n",
      "Epoch 35, Batch 410, LR 0.000004 Loss 4.442846, Accuracy 89.546%\n",
      "Epoch 35, Batch 411, LR 0.000004 Loss 4.440108, Accuracy 89.553%\n",
      "Epoch 35, Batch 412, LR 0.000004 Loss 4.441147, Accuracy 89.546%\n",
      "Epoch 35, Batch 413, LR 0.000004 Loss 4.441062, Accuracy 89.547%\n",
      "Epoch 35, Batch 414, LR 0.000004 Loss 4.442392, Accuracy 89.536%\n",
      "Epoch 35, Batch 415, LR 0.000004 Loss 4.441761, Accuracy 89.533%\n",
      "Epoch 35, Batch 416, LR 0.000004 Loss 4.442114, Accuracy 89.532%\n",
      "Epoch 35, Batch 417, LR 0.000004 Loss 4.443270, Accuracy 89.516%\n",
      "Epoch 35, Batch 418, LR 0.000004 Loss 4.442459, Accuracy 89.524%\n",
      "Epoch 35, Batch 419, LR 0.000004 Loss 4.442379, Accuracy 89.519%\n",
      "Epoch 35, Batch 420, LR 0.000004 Loss 4.441200, Accuracy 89.526%\n",
      "Epoch 35, Batch 421, LR 0.000004 Loss 4.440520, Accuracy 89.534%\n",
      "Epoch 35, Batch 422, LR 0.000004 Loss 4.439092, Accuracy 89.542%\n",
      "Epoch 35, Batch 423, LR 0.000004 Loss 4.439730, Accuracy 89.541%\n",
      "Epoch 35, Batch 424, LR 0.000004 Loss 4.439654, Accuracy 89.542%\n",
      "Epoch 35, Batch 425, LR 0.000004 Loss 4.438950, Accuracy 89.542%\n",
      "Epoch 35, Batch 426, LR 0.000004 Loss 4.438867, Accuracy 89.550%\n",
      "Epoch 35, Batch 427, LR 0.000004 Loss 4.437774, Accuracy 89.560%\n",
      "Epoch 35, Batch 428, LR 0.000004 Loss 4.438886, Accuracy 89.564%\n",
      "Epoch 35, Batch 429, LR 0.000004 Loss 4.439507, Accuracy 89.574%\n",
      "Epoch 35, Batch 430, LR 0.000004 Loss 4.438502, Accuracy 89.580%\n",
      "Epoch 35, Batch 431, LR 0.000004 Loss 4.437221, Accuracy 89.590%\n",
      "Epoch 35, Batch 432, LR 0.000004 Loss 4.436940, Accuracy 89.600%\n",
      "Epoch 35, Batch 433, LR 0.000004 Loss 4.437079, Accuracy 89.604%\n",
      "Epoch 35, Batch 434, LR 0.000004 Loss 4.437125, Accuracy 89.601%\n",
      "Epoch 35, Batch 435, LR 0.000004 Loss 4.436298, Accuracy 89.601%\n",
      "Epoch 35, Batch 436, LR 0.000004 Loss 4.437669, Accuracy 89.596%\n",
      "Epoch 35, Batch 437, LR 0.000004 Loss 4.437635, Accuracy 89.606%\n",
      "Epoch 35, Batch 438, LR 0.000004 Loss 4.437321, Accuracy 89.614%\n",
      "Epoch 35, Batch 439, LR 0.000004 Loss 4.435978, Accuracy 89.620%\n",
      "Epoch 35, Batch 440, LR 0.000004 Loss 4.438295, Accuracy 89.609%\n",
      "Epoch 35, Batch 441, LR 0.000004 Loss 4.438975, Accuracy 89.612%\n",
      "Epoch 35, Batch 442, LR 0.000004 Loss 4.439769, Accuracy 89.605%\n",
      "Epoch 35, Batch 443, LR 0.000004 Loss 4.439180, Accuracy 89.607%\n",
      "Epoch 35, Batch 444, LR 0.000004 Loss 4.438697, Accuracy 89.613%\n",
      "Epoch 35, Batch 445, LR 0.000004 Loss 4.439263, Accuracy 89.612%\n",
      "Epoch 35, Batch 446, LR 0.000004 Loss 4.437637, Accuracy 89.623%\n",
      "Epoch 35, Batch 447, LR 0.000004 Loss 4.438021, Accuracy 89.618%\n",
      "Epoch 35, Batch 448, LR 0.000004 Loss 4.439436, Accuracy 89.608%\n",
      "Epoch 35, Batch 449, LR 0.000004 Loss 4.439875, Accuracy 89.618%\n",
      "Epoch 35, Batch 450, LR 0.000004 Loss 4.440429, Accuracy 89.618%\n",
      "Epoch 35, Batch 451, LR 0.000004 Loss 4.439721, Accuracy 89.624%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 452, LR 0.000004 Loss 4.440917, Accuracy 89.624%\n",
      "Epoch 35, Batch 453, LR 0.000004 Loss 4.440927, Accuracy 89.626%\n",
      "Epoch 35, Batch 454, LR 0.000004 Loss 4.440755, Accuracy 89.627%\n",
      "Epoch 35, Batch 455, LR 0.000004 Loss 4.440045, Accuracy 89.627%\n",
      "Epoch 35, Batch 456, LR 0.000004 Loss 4.439752, Accuracy 89.624%\n",
      "Epoch 35, Batch 457, LR 0.000004 Loss 4.439293, Accuracy 89.627%\n",
      "Epoch 35, Batch 458, LR 0.000004 Loss 4.439814, Accuracy 89.622%\n",
      "Epoch 35, Batch 459, LR 0.000004 Loss 4.439430, Accuracy 89.617%\n",
      "Epoch 35, Batch 460, LR 0.000004 Loss 4.439911, Accuracy 89.620%\n",
      "Epoch 35, Batch 461, LR 0.000004 Loss 4.440193, Accuracy 89.613%\n",
      "Epoch 35, Batch 462, LR 0.000004 Loss 4.439497, Accuracy 89.612%\n",
      "Epoch 35, Batch 463, LR 0.000004 Loss 4.439691, Accuracy 89.608%\n",
      "Epoch 35, Batch 464, LR 0.000004 Loss 4.441859, Accuracy 89.591%\n",
      "Epoch 35, Batch 465, LR 0.000004 Loss 4.439651, Accuracy 89.600%\n",
      "Epoch 35, Batch 466, LR 0.000004 Loss 4.439433, Accuracy 89.601%\n",
      "Epoch 35, Batch 467, LR 0.000004 Loss 4.438489, Accuracy 89.601%\n",
      "Epoch 35, Batch 468, LR 0.000004 Loss 4.439656, Accuracy 89.597%\n",
      "Epoch 35, Batch 469, LR 0.000004 Loss 4.438252, Accuracy 89.604%\n",
      "Epoch 35, Batch 470, LR 0.000003 Loss 4.439513, Accuracy 89.598%\n",
      "Epoch 35, Batch 471, LR 0.000003 Loss 4.440161, Accuracy 89.593%\n",
      "Epoch 35, Batch 472, LR 0.000003 Loss 4.439904, Accuracy 89.592%\n",
      "Epoch 35, Batch 473, LR 0.000003 Loss 4.440912, Accuracy 89.579%\n",
      "Epoch 35, Batch 474, LR 0.000003 Loss 4.441204, Accuracy 89.577%\n",
      "Epoch 35, Batch 475, LR 0.000003 Loss 4.441104, Accuracy 89.577%\n",
      "Epoch 35, Batch 476, LR 0.000003 Loss 4.440433, Accuracy 89.578%\n",
      "Epoch 35, Batch 477, LR 0.000003 Loss 4.439910, Accuracy 89.580%\n",
      "Epoch 35, Batch 478, LR 0.000003 Loss 4.440527, Accuracy 89.576%\n",
      "Epoch 35, Batch 479, LR 0.000003 Loss 4.440046, Accuracy 89.580%\n",
      "Epoch 35, Batch 480, LR 0.000003 Loss 4.442366, Accuracy 89.567%\n",
      "Epoch 35, Batch 481, LR 0.000003 Loss 4.442243, Accuracy 89.569%\n",
      "Epoch 35, Batch 482, LR 0.000003 Loss 4.442072, Accuracy 89.570%\n",
      "Epoch 35, Batch 483, LR 0.000003 Loss 4.442602, Accuracy 89.567%\n",
      "Epoch 35, Batch 484, LR 0.000003 Loss 4.441786, Accuracy 89.574%\n",
      "Epoch 35, Batch 485, LR 0.000003 Loss 4.441903, Accuracy 89.578%\n",
      "Epoch 35, Batch 486, LR 0.000003 Loss 4.441679, Accuracy 89.572%\n",
      "Epoch 35, Batch 487, LR 0.000003 Loss 4.442582, Accuracy 89.566%\n",
      "Epoch 35, Batch 488, LR 0.000003 Loss 4.444747, Accuracy 89.557%\n",
      "Epoch 35, Batch 489, LR 0.000003 Loss 4.444481, Accuracy 89.561%\n",
      "Epoch 35, Batch 490, LR 0.000003 Loss 4.446947, Accuracy 89.547%\n",
      "Epoch 35, Batch 491, LR 0.000003 Loss 4.446376, Accuracy 89.546%\n",
      "Epoch 35, Batch 492, LR 0.000003 Loss 4.446634, Accuracy 89.548%\n",
      "Epoch 35, Batch 493, LR 0.000003 Loss 4.447272, Accuracy 89.552%\n",
      "Epoch 35, Batch 494, LR 0.000003 Loss 4.447300, Accuracy 89.546%\n",
      "Epoch 35, Batch 495, LR 0.000003 Loss 4.446952, Accuracy 89.552%\n",
      "Epoch 35, Batch 496, LR 0.000003 Loss 4.446495, Accuracy 89.543%\n",
      "Epoch 35, Batch 497, LR 0.000003 Loss 4.446423, Accuracy 89.539%\n",
      "Epoch 35, Batch 498, LR 0.000003 Loss 4.447054, Accuracy 89.536%\n",
      "Epoch 35, Batch 499, LR 0.000003 Loss 4.447460, Accuracy 89.534%\n",
      "Epoch 35, Batch 500, LR 0.000003 Loss 4.445928, Accuracy 89.542%\n",
      "Epoch 35, Batch 501, LR 0.000003 Loss 4.445954, Accuracy 89.543%\n",
      "Epoch 35, Batch 502, LR 0.000003 Loss 4.446062, Accuracy 89.547%\n",
      "Epoch 35, Batch 503, LR 0.000003 Loss 4.447268, Accuracy 89.544%\n",
      "Epoch 35, Batch 504, LR 0.000003 Loss 4.447913, Accuracy 89.537%\n",
      "Epoch 35, Batch 505, LR 0.000003 Loss 4.447886, Accuracy 89.539%\n",
      "Epoch 35, Batch 506, LR 0.000003 Loss 4.446560, Accuracy 89.540%\n",
      "Epoch 35, Batch 507, LR 0.000003 Loss 4.447441, Accuracy 89.534%\n",
      "Epoch 35, Batch 508, LR 0.000003 Loss 4.447121, Accuracy 89.538%\n",
      "Epoch 35, Batch 509, LR 0.000003 Loss 4.447432, Accuracy 89.541%\n",
      "Epoch 35, Batch 510, LR 0.000003 Loss 4.446900, Accuracy 89.550%\n",
      "Epoch 35, Batch 511, LR 0.000003 Loss 4.446623, Accuracy 89.547%\n",
      "Epoch 35, Batch 512, LR 0.000003 Loss 4.447021, Accuracy 89.555%\n",
      "Epoch 35, Batch 513, LR 0.000003 Loss 4.446409, Accuracy 89.559%\n",
      "Epoch 35, Batch 514, LR 0.000003 Loss 4.446086, Accuracy 89.567%\n",
      "Epoch 35, Batch 515, LR 0.000003 Loss 4.445145, Accuracy 89.569%\n",
      "Epoch 35, Batch 516, LR 0.000003 Loss 4.443731, Accuracy 89.571%\n",
      "Epoch 35, Batch 517, LR 0.000003 Loss 4.444297, Accuracy 89.569%\n",
      "Epoch 35, Batch 518, LR 0.000003 Loss 4.444137, Accuracy 89.569%\n",
      "Epoch 35, Batch 519, LR 0.000003 Loss 4.444239, Accuracy 89.564%\n",
      "Epoch 35, Batch 520, LR 0.000003 Loss 4.444528, Accuracy 89.564%\n",
      "Epoch 35, Batch 521, LR 0.000003 Loss 4.445400, Accuracy 89.563%\n",
      "Epoch 35, Batch 522, LR 0.000003 Loss 4.446762, Accuracy 89.564%\n",
      "Epoch 35, Batch 523, LR 0.000003 Loss 4.445896, Accuracy 89.563%\n",
      "Epoch 35, Batch 524, LR 0.000003 Loss 4.445778, Accuracy 89.571%\n",
      "Epoch 35, Batch 525, LR 0.000003 Loss 4.445778, Accuracy 89.568%\n",
      "Epoch 35, Batch 526, LR 0.000003 Loss 4.445205, Accuracy 89.572%\n",
      "Epoch 35, Batch 527, LR 0.000003 Loss 4.446428, Accuracy 89.572%\n",
      "Epoch 35, Batch 528, LR 0.000003 Loss 4.447583, Accuracy 89.567%\n",
      "Epoch 35, Batch 529, LR 0.000003 Loss 4.447322, Accuracy 89.573%\n",
      "Epoch 35, Batch 530, LR 0.000003 Loss 4.447929, Accuracy 89.565%\n",
      "Epoch 35, Batch 531, LR 0.000003 Loss 4.448965, Accuracy 89.567%\n",
      "Epoch 35, Batch 532, LR 0.000003 Loss 4.449944, Accuracy 89.563%\n",
      "Epoch 35, Batch 533, LR 0.000003 Loss 4.451334, Accuracy 89.559%\n",
      "Epoch 35, Batch 534, LR 0.000003 Loss 4.451002, Accuracy 89.560%\n",
      "Epoch 35, Batch 535, LR 0.000003 Loss 4.451969, Accuracy 89.553%\n",
      "Epoch 35, Batch 536, LR 0.000003 Loss 4.451899, Accuracy 89.549%\n",
      "Epoch 35, Batch 537, LR 0.000003 Loss 4.452823, Accuracy 89.547%\n",
      "Epoch 35, Batch 538, LR 0.000003 Loss 4.450841, Accuracy 89.559%\n",
      "Epoch 35, Batch 539, LR 0.000003 Loss 4.451223, Accuracy 89.557%\n",
      "Epoch 35, Batch 540, LR 0.000003 Loss 4.451082, Accuracy 89.556%\n",
      "Epoch 35, Batch 541, LR 0.000003 Loss 4.453038, Accuracy 89.543%\n",
      "Epoch 35, Batch 542, LR 0.000003 Loss 4.452545, Accuracy 89.550%\n",
      "Epoch 35, Batch 543, LR 0.000003 Loss 4.453313, Accuracy 89.544%\n",
      "Epoch 35, Batch 544, LR 0.000003 Loss 4.454095, Accuracy 89.538%\n",
      "Epoch 35, Batch 545, LR 0.000003 Loss 4.452873, Accuracy 89.538%\n",
      "Epoch 35, Batch 546, LR 0.000003 Loss 4.453430, Accuracy 89.532%\n",
      "Epoch 35, Batch 547, LR 0.000003 Loss 4.452885, Accuracy 89.532%\n",
      "Epoch 35, Batch 548, LR 0.000003 Loss 4.452088, Accuracy 89.529%\n",
      "Epoch 35, Batch 549, LR 0.000003 Loss 4.450429, Accuracy 89.536%\n",
      "Epoch 35, Batch 550, LR 0.000003 Loss 4.449875, Accuracy 89.531%\n",
      "Epoch 35, Batch 551, LR 0.000003 Loss 4.450099, Accuracy 89.533%\n",
      "Epoch 35, Batch 552, LR 0.000003 Loss 4.451543, Accuracy 89.524%\n",
      "Epoch 35, Batch 553, LR 0.000003 Loss 4.450991, Accuracy 89.527%\n",
      "Epoch 35, Batch 554, LR 0.000003 Loss 4.451652, Accuracy 89.522%\n",
      "Epoch 35, Batch 555, LR 0.000003 Loss 4.451433, Accuracy 89.521%\n",
      "Epoch 35, Batch 556, LR 0.000003 Loss 4.452380, Accuracy 89.516%\n",
      "Epoch 35, Batch 557, LR 0.000003 Loss 4.451761, Accuracy 89.518%\n",
      "Epoch 35, Batch 558, LR 0.000003 Loss 4.452076, Accuracy 89.513%\n",
      "Epoch 35, Batch 559, LR 0.000003 Loss 4.451419, Accuracy 89.511%\n",
      "Epoch 35, Batch 560, LR 0.000003 Loss 4.452283, Accuracy 89.506%\n",
      "Epoch 35, Batch 561, LR 0.000003 Loss 4.452134, Accuracy 89.503%\n",
      "Epoch 35, Batch 562, LR 0.000003 Loss 4.451605, Accuracy 89.507%\n",
      "Epoch 35, Batch 563, LR 0.000003 Loss 4.451650, Accuracy 89.508%\n",
      "Epoch 35, Batch 564, LR 0.000003 Loss 4.451121, Accuracy 89.513%\n",
      "Epoch 35, Batch 565, LR 0.000003 Loss 4.451578, Accuracy 89.515%\n",
      "Epoch 35, Batch 566, LR 0.000003 Loss 4.450404, Accuracy 89.522%\n",
      "Epoch 35, Batch 567, LR 0.000003 Loss 4.449661, Accuracy 89.523%\n",
      "Epoch 35, Batch 568, LR 0.000003 Loss 4.448804, Accuracy 89.525%\n",
      "Epoch 35, Batch 569, LR 0.000003 Loss 4.449231, Accuracy 89.528%\n",
      "Epoch 35, Batch 570, LR 0.000003 Loss 4.448910, Accuracy 89.531%\n",
      "Epoch 35, Batch 571, LR 0.000003 Loss 4.449062, Accuracy 89.530%\n",
      "Epoch 35, Batch 572, LR 0.000003 Loss 4.449989, Accuracy 89.527%\n",
      "Epoch 35, Batch 573, LR 0.000003 Loss 4.450195, Accuracy 89.522%\n",
      "Epoch 35, Batch 574, LR 0.000003 Loss 4.450126, Accuracy 89.521%\n",
      "Epoch 35, Batch 575, LR 0.000003 Loss 4.451337, Accuracy 89.516%\n",
      "Epoch 35, Batch 576, LR 0.000003 Loss 4.451333, Accuracy 89.517%\n",
      "Epoch 35, Batch 577, LR 0.000003 Loss 4.451776, Accuracy 89.522%\n",
      "Epoch 35, Batch 578, LR 0.000003 Loss 4.450160, Accuracy 89.526%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 579, LR 0.000003 Loss 4.449774, Accuracy 89.525%\n",
      "Epoch 35, Batch 580, LR 0.000003 Loss 4.449929, Accuracy 89.522%\n",
      "Epoch 35, Batch 581, LR 0.000003 Loss 4.449919, Accuracy 89.517%\n",
      "Epoch 35, Batch 582, LR 0.000003 Loss 4.451376, Accuracy 89.515%\n",
      "Epoch 35, Batch 583, LR 0.000003 Loss 4.450481, Accuracy 89.517%\n",
      "Epoch 35, Batch 584, LR 0.000003 Loss 4.451950, Accuracy 89.515%\n",
      "Epoch 35, Batch 585, LR 0.000003 Loss 4.450891, Accuracy 89.517%\n",
      "Epoch 35, Batch 586, LR 0.000003 Loss 4.449943, Accuracy 89.525%\n",
      "Epoch 35, Batch 587, LR 0.000003 Loss 4.449878, Accuracy 89.524%\n",
      "Epoch 35, Batch 588, LR 0.000003 Loss 4.450045, Accuracy 89.522%\n",
      "Epoch 35, Batch 589, LR 0.000003 Loss 4.449994, Accuracy 89.521%\n",
      "Epoch 35, Batch 590, LR 0.000003 Loss 4.450385, Accuracy 89.525%\n",
      "Epoch 35, Batch 591, LR 0.000003 Loss 4.450635, Accuracy 89.521%\n",
      "Epoch 35, Batch 592, LR 0.000003 Loss 4.450373, Accuracy 89.523%\n",
      "Epoch 35, Batch 593, LR 0.000003 Loss 4.450854, Accuracy 89.525%\n",
      "Epoch 35, Batch 594, LR 0.000003 Loss 4.450683, Accuracy 89.522%\n",
      "Epoch 35, Batch 595, LR 0.000003 Loss 4.451214, Accuracy 89.523%\n",
      "Epoch 35, Batch 596, LR 0.000003 Loss 4.451293, Accuracy 89.523%\n",
      "Epoch 35, Batch 597, LR 0.000003 Loss 4.450914, Accuracy 89.519%\n",
      "Epoch 35, Batch 598, LR 0.000003 Loss 4.450225, Accuracy 89.526%\n",
      "Epoch 35, Batch 599, LR 0.000003 Loss 4.449211, Accuracy 89.533%\n",
      "Epoch 35, Batch 600, LR 0.000003 Loss 4.449286, Accuracy 89.531%\n",
      "Epoch 35, Batch 601, LR 0.000003 Loss 4.449264, Accuracy 89.529%\n",
      "Epoch 35, Batch 602, LR 0.000003 Loss 4.450465, Accuracy 89.522%\n",
      "Epoch 35, Batch 603, LR 0.000003 Loss 4.449616, Accuracy 89.532%\n",
      "Epoch 35, Batch 604, LR 0.000003 Loss 4.448362, Accuracy 89.532%\n",
      "Epoch 35, Batch 605, LR 0.000003 Loss 4.448563, Accuracy 89.534%\n",
      "Epoch 35, Batch 606, LR 0.000003 Loss 4.447476, Accuracy 89.542%\n",
      "Epoch 35, Batch 607, LR 0.000003 Loss 4.447851, Accuracy 89.541%\n",
      "Epoch 35, Batch 608, LR 0.000003 Loss 4.448091, Accuracy 89.546%\n",
      "Epoch 35, Batch 609, LR 0.000003 Loss 4.446612, Accuracy 89.553%\n",
      "Epoch 35, Batch 610, LR 0.000003 Loss 4.446845, Accuracy 89.554%\n",
      "Epoch 35, Batch 611, LR 0.000003 Loss 4.446770, Accuracy 89.550%\n",
      "Epoch 35, Batch 612, LR 0.000003 Loss 4.446142, Accuracy 89.551%\n",
      "Epoch 35, Batch 613, LR 0.000003 Loss 4.446107, Accuracy 89.553%\n",
      "Epoch 35, Batch 614, LR 0.000003 Loss 4.445755, Accuracy 89.552%\n",
      "Epoch 35, Batch 615, LR 0.000003 Loss 4.445658, Accuracy 89.554%\n",
      "Epoch 35, Batch 616, LR 0.000003 Loss 4.446230, Accuracy 89.550%\n",
      "Epoch 35, Batch 617, LR 0.000003 Loss 4.447632, Accuracy 89.546%\n",
      "Epoch 35, Batch 618, LR 0.000003 Loss 4.447531, Accuracy 89.549%\n",
      "Epoch 35, Batch 619, LR 0.000003 Loss 4.447278, Accuracy 89.546%\n",
      "Epoch 35, Batch 620, LR 0.000003 Loss 4.447497, Accuracy 89.544%\n",
      "Epoch 35, Batch 621, LR 0.000003 Loss 4.447520, Accuracy 89.541%\n",
      "Epoch 35, Batch 622, LR 0.000003 Loss 4.447759, Accuracy 89.528%\n",
      "Epoch 35, Batch 623, LR 0.000003 Loss 4.447856, Accuracy 89.520%\n",
      "Epoch 35, Batch 624, LR 0.000003 Loss 4.446314, Accuracy 89.529%\n",
      "Epoch 35, Batch 625, LR 0.000003 Loss 4.445402, Accuracy 89.534%\n",
      "Epoch 35, Batch 626, LR 0.000003 Loss 4.446973, Accuracy 89.523%\n",
      "Epoch 35, Batch 627, LR 0.000003 Loss 4.447552, Accuracy 89.522%\n",
      "Epoch 35, Batch 628, LR 0.000003 Loss 4.447773, Accuracy 89.523%\n",
      "Epoch 35, Batch 629, LR 0.000003 Loss 4.447363, Accuracy 89.530%\n",
      "Epoch 35, Batch 630, LR 0.000003 Loss 4.447080, Accuracy 89.531%\n",
      "Epoch 35, Batch 631, LR 0.000003 Loss 4.447505, Accuracy 89.532%\n",
      "Epoch 35, Batch 632, LR 0.000003 Loss 4.448623, Accuracy 89.526%\n",
      "Epoch 35, Batch 633, LR 0.000003 Loss 4.448265, Accuracy 89.528%\n",
      "Epoch 35, Batch 634, LR 0.000003 Loss 4.448301, Accuracy 89.531%\n",
      "Epoch 35, Batch 635, LR 0.000003 Loss 4.448907, Accuracy 89.532%\n",
      "Epoch 35, Batch 636, LR 0.000003 Loss 4.448045, Accuracy 89.533%\n",
      "Epoch 35, Batch 637, LR 0.000003 Loss 4.447667, Accuracy 89.535%\n",
      "Epoch 35, Batch 638, LR 0.000003 Loss 4.447126, Accuracy 89.535%\n",
      "Epoch 35, Batch 639, LR 0.000003 Loss 4.445894, Accuracy 89.545%\n",
      "Epoch 35, Batch 640, LR 0.000003 Loss 4.445412, Accuracy 89.547%\n",
      "Epoch 35, Batch 641, LR 0.000003 Loss 4.445494, Accuracy 89.543%\n",
      "Epoch 35, Batch 642, LR 0.000003 Loss 4.445167, Accuracy 89.548%\n",
      "Epoch 35, Batch 643, LR 0.000003 Loss 4.445827, Accuracy 89.544%\n",
      "Epoch 35, Batch 644, LR 0.000003 Loss 4.446264, Accuracy 89.542%\n",
      "Epoch 35, Batch 645, LR 0.000003 Loss 4.446277, Accuracy 89.545%\n",
      "Epoch 35, Batch 646, LR 0.000003 Loss 4.447121, Accuracy 89.541%\n",
      "Epoch 35, Batch 647, LR 0.000003 Loss 4.448122, Accuracy 89.536%\n",
      "Epoch 35, Batch 648, LR 0.000003 Loss 4.447127, Accuracy 89.539%\n",
      "Epoch 35, Batch 649, LR 0.000003 Loss 4.446675, Accuracy 89.537%\n",
      "Epoch 35, Batch 650, LR 0.000003 Loss 4.445938, Accuracy 89.541%\n",
      "Epoch 35, Batch 651, LR 0.000003 Loss 4.447021, Accuracy 89.531%\n",
      "Epoch 35, Batch 652, LR 0.000003 Loss 4.447885, Accuracy 89.530%\n",
      "Epoch 35, Batch 653, LR 0.000003 Loss 4.447551, Accuracy 89.530%\n",
      "Epoch 35, Batch 654, LR 0.000003 Loss 4.447588, Accuracy 89.531%\n",
      "Epoch 35, Batch 655, LR 0.000003 Loss 4.446643, Accuracy 89.532%\n",
      "Epoch 35, Batch 656, LR 0.000003 Loss 4.446690, Accuracy 89.534%\n",
      "Epoch 35, Batch 657, LR 0.000003 Loss 4.446120, Accuracy 89.538%\n",
      "Epoch 35, Batch 658, LR 0.000003 Loss 4.446410, Accuracy 89.539%\n",
      "Epoch 35, Batch 659, LR 0.000003 Loss 4.446179, Accuracy 89.540%\n",
      "Epoch 35, Batch 660, LR 0.000003 Loss 4.446668, Accuracy 89.537%\n",
      "Epoch 35, Batch 661, LR 0.000003 Loss 4.446836, Accuracy 89.535%\n",
      "Epoch 35, Batch 662, LR 0.000003 Loss 4.447370, Accuracy 89.535%\n",
      "Epoch 35, Batch 663, LR 0.000003 Loss 4.447426, Accuracy 89.530%\n",
      "Epoch 35, Batch 664, LR 0.000003 Loss 4.446896, Accuracy 89.534%\n",
      "Epoch 35, Batch 665, LR 0.000003 Loss 4.447668, Accuracy 89.528%\n",
      "Epoch 35, Batch 666, LR 0.000003 Loss 4.447407, Accuracy 89.532%\n",
      "Epoch 35, Batch 667, LR 0.000003 Loss 4.446797, Accuracy 89.539%\n",
      "Epoch 35, Batch 668, LR 0.000003 Loss 4.446734, Accuracy 89.543%\n",
      "Epoch 35, Batch 669, LR 0.000003 Loss 4.446844, Accuracy 89.541%\n",
      "Epoch 35, Batch 670, LR 0.000003 Loss 4.447396, Accuracy 89.536%\n",
      "Epoch 35, Batch 671, LR 0.000003 Loss 4.447775, Accuracy 89.533%\n",
      "Epoch 35, Batch 672, LR 0.000003 Loss 4.448226, Accuracy 89.530%\n",
      "Epoch 35, Batch 673, LR 0.000003 Loss 4.448358, Accuracy 89.531%\n",
      "Epoch 35, Batch 674, LR 0.000003 Loss 4.448366, Accuracy 89.527%\n",
      "Epoch 35, Batch 675, LR 0.000003 Loss 4.447412, Accuracy 89.535%\n",
      "Epoch 35, Batch 676, LR 0.000003 Loss 4.448153, Accuracy 89.533%\n",
      "Epoch 35, Batch 677, LR 0.000003 Loss 4.448566, Accuracy 89.531%\n",
      "Epoch 35, Batch 678, LR 0.000003 Loss 4.447451, Accuracy 89.535%\n",
      "Epoch 35, Batch 679, LR 0.000003 Loss 4.447165, Accuracy 89.537%\n",
      "Epoch 35, Batch 680, LR 0.000003 Loss 4.446731, Accuracy 89.538%\n",
      "Epoch 35, Batch 681, LR 0.000003 Loss 4.446630, Accuracy 89.539%\n",
      "Epoch 35, Batch 682, LR 0.000003 Loss 4.446758, Accuracy 89.542%\n",
      "Epoch 35, Batch 683, LR 0.000003 Loss 4.447374, Accuracy 89.536%\n",
      "Epoch 35, Batch 684, LR 0.000003 Loss 4.447167, Accuracy 89.540%\n",
      "Epoch 35, Batch 685, LR 0.000003 Loss 4.446479, Accuracy 89.540%\n",
      "Epoch 35, Batch 686, LR 0.000003 Loss 4.446708, Accuracy 89.537%\n",
      "Epoch 35, Batch 687, LR 0.000003 Loss 4.446653, Accuracy 89.538%\n",
      "Epoch 35, Batch 688, LR 0.000003 Loss 4.445822, Accuracy 89.539%\n",
      "Epoch 35, Batch 689, LR 0.000003 Loss 4.446305, Accuracy 89.535%\n",
      "Epoch 35, Batch 690, LR 0.000003 Loss 4.446417, Accuracy 89.536%\n",
      "Epoch 35, Batch 691, LR 0.000003 Loss 4.447516, Accuracy 89.531%\n",
      "Epoch 35, Batch 692, LR 0.000003 Loss 4.447827, Accuracy 89.528%\n",
      "Epoch 35, Batch 693, LR 0.000003 Loss 4.447900, Accuracy 89.538%\n",
      "Epoch 35, Batch 694, LR 0.000003 Loss 4.447747, Accuracy 89.539%\n",
      "Epoch 35, Batch 695, LR 0.000003 Loss 4.447885, Accuracy 89.538%\n",
      "Epoch 35, Batch 696, LR 0.000003 Loss 4.447453, Accuracy 89.534%\n",
      "Epoch 35, Batch 697, LR 0.000003 Loss 4.447746, Accuracy 89.529%\n",
      "Epoch 35, Batch 698, LR 0.000003 Loss 4.447725, Accuracy 89.529%\n",
      "Epoch 35, Batch 699, LR 0.000003 Loss 4.448050, Accuracy 89.532%\n",
      "Epoch 35, Batch 700, LR 0.000003 Loss 4.448268, Accuracy 89.533%\n",
      "Epoch 35, Batch 701, LR 0.000003 Loss 4.448276, Accuracy 89.528%\n",
      "Epoch 35, Batch 702, LR 0.000003 Loss 4.448489, Accuracy 89.528%\n",
      "Epoch 35, Batch 703, LR 0.000003 Loss 4.448093, Accuracy 89.529%\n",
      "Epoch 35, Batch 704, LR 0.000003 Loss 4.448057, Accuracy 89.531%\n",
      "Epoch 35, Batch 705, LR 0.000003 Loss 4.448505, Accuracy 89.530%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 706, LR 0.000003 Loss 4.448965, Accuracy 89.529%\n",
      "Epoch 35, Batch 707, LR 0.000003 Loss 4.449067, Accuracy 89.530%\n",
      "Epoch 35, Batch 708, LR 0.000003 Loss 4.449474, Accuracy 89.524%\n",
      "Epoch 35, Batch 709, LR 0.000003 Loss 4.449274, Accuracy 89.523%\n",
      "Epoch 35, Batch 710, LR 0.000003 Loss 4.448566, Accuracy 89.528%\n",
      "Epoch 35, Batch 711, LR 0.000003 Loss 4.448171, Accuracy 89.528%\n",
      "Epoch 35, Batch 712, LR 0.000003 Loss 4.447720, Accuracy 89.534%\n",
      "Epoch 35, Batch 713, LR 0.000003 Loss 4.446817, Accuracy 89.539%\n",
      "Epoch 35, Batch 714, LR 0.000003 Loss 4.447421, Accuracy 89.541%\n",
      "Epoch 35, Batch 715, LR 0.000003 Loss 4.447996, Accuracy 89.540%\n",
      "Epoch 35, Batch 716, LR 0.000003 Loss 4.448156, Accuracy 89.538%\n",
      "Epoch 35, Batch 717, LR 0.000003 Loss 4.448163, Accuracy 89.538%\n",
      "Epoch 35, Batch 718, LR 0.000003 Loss 4.448590, Accuracy 89.536%\n",
      "Epoch 35, Batch 719, LR 0.000003 Loss 4.448594, Accuracy 89.538%\n",
      "Epoch 35, Batch 720, LR 0.000003 Loss 4.449289, Accuracy 89.536%\n",
      "Epoch 35, Batch 721, LR 0.000003 Loss 4.448693, Accuracy 89.543%\n",
      "Epoch 35, Batch 722, LR 0.000003 Loss 4.448303, Accuracy 89.545%\n",
      "Epoch 35, Batch 723, LR 0.000003 Loss 4.448096, Accuracy 89.544%\n",
      "Epoch 35, Batch 724, LR 0.000003 Loss 4.447890, Accuracy 89.548%\n",
      "Epoch 35, Batch 725, LR 0.000003 Loss 4.447838, Accuracy 89.546%\n",
      "Epoch 35, Batch 726, LR 0.000003 Loss 4.447214, Accuracy 89.550%\n",
      "Epoch 35, Batch 727, LR 0.000003 Loss 4.446884, Accuracy 89.549%\n",
      "Epoch 35, Batch 728, LR 0.000003 Loss 4.446963, Accuracy 89.551%\n",
      "Epoch 35, Batch 729, LR 0.000003 Loss 4.447195, Accuracy 89.548%\n",
      "Epoch 35, Batch 730, LR 0.000003 Loss 4.446146, Accuracy 89.555%\n",
      "Epoch 35, Batch 731, LR 0.000003 Loss 4.445883, Accuracy 89.561%\n",
      "Epoch 35, Batch 732, LR 0.000003 Loss 4.445959, Accuracy 89.558%\n",
      "Epoch 35, Batch 733, LR 0.000003 Loss 4.445837, Accuracy 89.559%\n",
      "Epoch 35, Batch 734, LR 0.000003 Loss 4.445534, Accuracy 89.564%\n",
      "Epoch 35, Batch 735, LR 0.000003 Loss 4.445875, Accuracy 89.561%\n",
      "Epoch 35, Batch 736, LR 0.000003 Loss 4.445510, Accuracy 89.559%\n",
      "Epoch 35, Batch 737, LR 0.000003 Loss 4.445988, Accuracy 89.552%\n",
      "Epoch 35, Batch 738, LR 0.000003 Loss 4.445316, Accuracy 89.556%\n",
      "Epoch 35, Batch 739, LR 0.000003 Loss 4.445355, Accuracy 89.554%\n",
      "Epoch 35, Batch 740, LR 0.000003 Loss 4.445855, Accuracy 89.552%\n",
      "Epoch 35, Batch 741, LR 0.000003 Loss 4.446069, Accuracy 89.551%\n",
      "Epoch 35, Batch 742, LR 0.000003 Loss 4.445602, Accuracy 89.551%\n",
      "Epoch 35, Batch 743, LR 0.000003 Loss 4.445822, Accuracy 89.550%\n",
      "Epoch 35, Batch 744, LR 0.000003 Loss 4.445517, Accuracy 89.555%\n",
      "Epoch 35, Batch 745, LR 0.000003 Loss 4.446113, Accuracy 89.551%\n",
      "Epoch 35, Batch 746, LR 0.000003 Loss 4.446743, Accuracy 89.545%\n",
      "Epoch 35, Batch 747, LR 0.000003 Loss 4.446865, Accuracy 89.539%\n",
      "Epoch 35, Batch 748, LR 0.000003 Loss 4.446392, Accuracy 89.541%\n",
      "Epoch 35, Batch 749, LR 0.000003 Loss 4.446058, Accuracy 89.540%\n",
      "Epoch 35, Batch 750, LR 0.000003 Loss 4.447275, Accuracy 89.533%\n",
      "Epoch 35, Batch 751, LR 0.000003 Loss 4.445968, Accuracy 89.542%\n",
      "Epoch 35, Batch 752, LR 0.000003 Loss 4.446791, Accuracy 89.537%\n",
      "Epoch 35, Batch 753, LR 0.000003 Loss 4.446683, Accuracy 89.538%\n",
      "Epoch 35, Batch 754, LR 0.000003 Loss 4.446089, Accuracy 89.543%\n",
      "Epoch 35, Batch 755, LR 0.000003 Loss 4.446001, Accuracy 89.543%\n",
      "Epoch 35, Batch 756, LR 0.000003 Loss 4.445903, Accuracy 89.541%\n",
      "Epoch 35, Batch 757, LR 0.000003 Loss 4.447653, Accuracy 89.540%\n",
      "Epoch 35, Batch 758, LR 0.000003 Loss 4.447519, Accuracy 89.543%\n",
      "Epoch 35, Batch 759, LR 0.000003 Loss 4.447933, Accuracy 89.545%\n",
      "Epoch 35, Batch 760, LR 0.000003 Loss 4.447821, Accuracy 89.547%\n",
      "Epoch 35, Batch 761, LR 0.000003 Loss 4.447455, Accuracy 89.548%\n",
      "Epoch 35, Batch 762, LR 0.000003 Loss 4.447496, Accuracy 89.545%\n",
      "Epoch 35, Batch 763, LR 0.000003 Loss 4.447221, Accuracy 89.550%\n",
      "Epoch 35, Batch 764, LR 0.000003 Loss 4.447742, Accuracy 89.547%\n",
      "Epoch 35, Batch 765, LR 0.000003 Loss 4.448806, Accuracy 89.546%\n",
      "Epoch 35, Batch 766, LR 0.000003 Loss 4.448695, Accuracy 89.550%\n",
      "Epoch 35, Batch 767, LR 0.000003 Loss 4.449085, Accuracy 89.548%\n",
      "Epoch 35, Batch 768, LR 0.000003 Loss 4.449931, Accuracy 89.547%\n",
      "Epoch 35, Batch 769, LR 0.000003 Loss 4.449291, Accuracy 89.547%\n",
      "Epoch 35, Batch 770, LR 0.000003 Loss 4.448921, Accuracy 89.551%\n",
      "Epoch 35, Batch 771, LR 0.000003 Loss 4.448319, Accuracy 89.553%\n",
      "Epoch 35, Batch 772, LR 0.000003 Loss 4.449157, Accuracy 89.547%\n",
      "Epoch 35, Batch 773, LR 0.000003 Loss 4.448698, Accuracy 89.550%\n",
      "Epoch 35, Batch 774, LR 0.000003 Loss 4.448761, Accuracy 89.549%\n",
      "Epoch 35, Batch 775, LR 0.000003 Loss 4.449563, Accuracy 89.546%\n",
      "Epoch 35, Batch 776, LR 0.000003 Loss 4.449745, Accuracy 89.545%\n",
      "Epoch 35, Batch 777, LR 0.000003 Loss 4.450132, Accuracy 89.544%\n",
      "Epoch 35, Batch 778, LR 0.000003 Loss 4.449557, Accuracy 89.547%\n",
      "Epoch 35, Batch 779, LR 0.000003 Loss 4.449786, Accuracy 89.544%\n",
      "Epoch 35, Batch 780, LR 0.000003 Loss 4.450335, Accuracy 89.542%\n",
      "Epoch 35, Batch 781, LR 0.000003 Loss 4.450467, Accuracy 89.542%\n",
      "Epoch 35, Batch 782, LR 0.000003 Loss 4.450869, Accuracy 89.545%\n",
      "Epoch 35, Batch 783, LR 0.000003 Loss 4.452445, Accuracy 89.535%\n",
      "Epoch 35, Batch 784, LR 0.000003 Loss 4.452826, Accuracy 89.534%\n",
      "Epoch 35, Batch 785, LR 0.000003 Loss 4.454180, Accuracy 89.528%\n",
      "Epoch 35, Batch 786, LR 0.000003 Loss 4.454019, Accuracy 89.530%\n",
      "Epoch 35, Batch 787, LR 0.000003 Loss 4.453715, Accuracy 89.529%\n",
      "Epoch 35, Batch 788, LR 0.000003 Loss 4.454333, Accuracy 89.528%\n",
      "Epoch 35, Batch 789, LR 0.000003 Loss 4.454593, Accuracy 89.526%\n",
      "Epoch 35, Batch 790, LR 0.000003 Loss 4.455340, Accuracy 89.522%\n",
      "Epoch 35, Batch 791, LR 0.000003 Loss 4.455564, Accuracy 89.523%\n",
      "Epoch 35, Batch 792, LR 0.000003 Loss 4.455768, Accuracy 89.526%\n",
      "Epoch 35, Batch 793, LR 0.000003 Loss 4.455245, Accuracy 89.528%\n",
      "Epoch 35, Batch 794, LR 0.000003 Loss 4.454352, Accuracy 89.531%\n",
      "Epoch 35, Batch 795, LR 0.000003 Loss 4.455230, Accuracy 89.526%\n",
      "Epoch 35, Batch 796, LR 0.000003 Loss 4.455454, Accuracy 89.525%\n",
      "Epoch 35, Batch 797, LR 0.000003 Loss 4.456525, Accuracy 89.522%\n",
      "Epoch 35, Batch 798, LR 0.000003 Loss 4.457254, Accuracy 89.519%\n",
      "Epoch 35, Batch 799, LR 0.000003 Loss 4.457753, Accuracy 89.517%\n",
      "Epoch 35, Batch 800, LR 0.000003 Loss 4.457237, Accuracy 89.517%\n",
      "Epoch 35, Batch 801, LR 0.000003 Loss 4.456484, Accuracy 89.519%\n",
      "Epoch 35, Batch 802, LR 0.000003 Loss 4.456101, Accuracy 89.520%\n",
      "Epoch 35, Batch 803, LR 0.000003 Loss 4.456456, Accuracy 89.518%\n",
      "Epoch 35, Batch 804, LR 0.000003 Loss 4.455726, Accuracy 89.521%\n",
      "Epoch 35, Batch 805, LR 0.000003 Loss 4.456059, Accuracy 89.523%\n",
      "Epoch 35, Batch 806, LR 0.000003 Loss 4.455901, Accuracy 89.525%\n",
      "Epoch 35, Batch 807, LR 0.000003 Loss 4.456719, Accuracy 89.521%\n",
      "Epoch 35, Batch 808, LR 0.000003 Loss 4.456375, Accuracy 89.521%\n",
      "Epoch 35, Batch 809, LR 0.000003 Loss 4.456641, Accuracy 89.516%\n",
      "Epoch 35, Batch 810, LR 0.000003 Loss 4.456042, Accuracy 89.517%\n",
      "Epoch 35, Batch 811, LR 0.000003 Loss 4.456776, Accuracy 89.513%\n",
      "Epoch 35, Batch 812, LR 0.000003 Loss 4.456855, Accuracy 89.515%\n",
      "Epoch 35, Batch 813, LR 0.000003 Loss 4.456842, Accuracy 89.514%\n",
      "Epoch 35, Batch 814, LR 0.000003 Loss 4.457058, Accuracy 89.516%\n",
      "Epoch 35, Batch 815, LR 0.000003 Loss 4.457920, Accuracy 89.514%\n",
      "Epoch 35, Batch 816, LR 0.000003 Loss 4.457407, Accuracy 89.516%\n",
      "Epoch 35, Batch 817, LR 0.000003 Loss 4.457336, Accuracy 89.519%\n",
      "Epoch 35, Batch 818, LR 0.000003 Loss 4.456801, Accuracy 89.521%\n",
      "Epoch 35, Batch 819, LR 0.000003 Loss 4.456519, Accuracy 89.521%\n",
      "Epoch 35, Batch 820, LR 0.000003 Loss 4.456811, Accuracy 89.519%\n",
      "Epoch 35, Batch 821, LR 0.000003 Loss 4.456519, Accuracy 89.521%\n",
      "Epoch 35, Batch 822, LR 0.000003 Loss 4.456310, Accuracy 89.523%\n",
      "Epoch 35, Batch 823, LR 0.000003 Loss 4.456456, Accuracy 89.522%\n",
      "Epoch 35, Batch 824, LR 0.000003 Loss 4.456534, Accuracy 89.519%\n",
      "Epoch 35, Batch 825, LR 0.000003 Loss 4.456637, Accuracy 89.521%\n",
      "Epoch 35, Batch 826, LR 0.000003 Loss 4.456560, Accuracy 89.522%\n",
      "Epoch 35, Batch 827, LR 0.000003 Loss 4.456654, Accuracy 89.524%\n",
      "Epoch 35, Batch 828, LR 0.000003 Loss 4.456041, Accuracy 89.529%\n",
      "Epoch 35, Batch 829, LR 0.000003 Loss 4.456215, Accuracy 89.529%\n",
      "Epoch 35, Batch 830, LR 0.000003 Loss 4.455888, Accuracy 89.529%\n",
      "Epoch 35, Batch 831, LR 0.000003 Loss 4.456199, Accuracy 89.530%\n",
      "Epoch 35, Batch 832, LR 0.000003 Loss 4.456039, Accuracy 89.532%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 833, LR 0.000003 Loss 4.455599, Accuracy 89.532%\n",
      "Epoch 35, Batch 834, LR 0.000003 Loss 4.454630, Accuracy 89.539%\n",
      "Epoch 35, Batch 835, LR 0.000003 Loss 4.454825, Accuracy 89.536%\n",
      "Epoch 35, Batch 836, LR 0.000003 Loss 4.454352, Accuracy 89.538%\n",
      "Epoch 35, Batch 837, LR 0.000003 Loss 4.455227, Accuracy 89.533%\n",
      "Epoch 35, Batch 838, LR 0.000003 Loss 4.455022, Accuracy 89.531%\n",
      "Epoch 35, Batch 839, LR 0.000003 Loss 4.455276, Accuracy 89.531%\n",
      "Epoch 35, Batch 840, LR 0.000003 Loss 4.454947, Accuracy 89.529%\n",
      "Epoch 35, Batch 841, LR 0.000003 Loss 4.455219, Accuracy 89.527%\n",
      "Epoch 35, Batch 842, LR 0.000003 Loss 4.455591, Accuracy 89.524%\n",
      "Epoch 35, Batch 843, LR 0.000003 Loss 4.455087, Accuracy 89.527%\n",
      "Epoch 35, Batch 844, LR 0.000003 Loss 4.456259, Accuracy 89.520%\n",
      "Epoch 35, Batch 845, LR 0.000003 Loss 4.455319, Accuracy 89.523%\n",
      "Epoch 35, Batch 846, LR 0.000003 Loss 4.455304, Accuracy 89.521%\n",
      "Epoch 35, Batch 847, LR 0.000003 Loss 4.455072, Accuracy 89.523%\n",
      "Epoch 35, Batch 848, LR 0.000003 Loss 4.454773, Accuracy 89.524%\n",
      "Epoch 35, Batch 849, LR 0.000003 Loss 4.454381, Accuracy 89.525%\n",
      "Epoch 35, Batch 850, LR 0.000003 Loss 4.454478, Accuracy 89.527%\n",
      "Epoch 35, Batch 851, LR 0.000003 Loss 4.454368, Accuracy 89.531%\n",
      "Epoch 35, Batch 852, LR 0.000003 Loss 4.454113, Accuracy 89.532%\n",
      "Epoch 35, Batch 853, LR 0.000003 Loss 4.454312, Accuracy 89.530%\n",
      "Epoch 35, Batch 854, LR 0.000003 Loss 4.455591, Accuracy 89.528%\n",
      "Epoch 35, Batch 855, LR 0.000003 Loss 4.455327, Accuracy 89.534%\n",
      "Epoch 35, Batch 856, LR 0.000003 Loss 4.455177, Accuracy 89.535%\n",
      "Epoch 35, Batch 857, LR 0.000003 Loss 4.454961, Accuracy 89.537%\n",
      "Epoch 35, Batch 858, LR 0.000003 Loss 4.454799, Accuracy 89.537%\n",
      "Epoch 35, Batch 859, LR 0.000003 Loss 4.454227, Accuracy 89.542%\n",
      "Epoch 35, Batch 860, LR 0.000003 Loss 4.454765, Accuracy 89.540%\n",
      "Epoch 35, Batch 861, LR 0.000003 Loss 4.454345, Accuracy 89.541%\n",
      "Epoch 35, Batch 862, LR 0.000003 Loss 4.454220, Accuracy 89.542%\n",
      "Epoch 35, Batch 863, LR 0.000003 Loss 4.454432, Accuracy 89.541%\n",
      "Epoch 35, Batch 864, LR 0.000003 Loss 4.454704, Accuracy 89.544%\n",
      "Epoch 35, Batch 865, LR 0.000003 Loss 4.454875, Accuracy 89.548%\n",
      "Epoch 35, Batch 866, LR 0.000003 Loss 4.453522, Accuracy 89.554%\n",
      "Epoch 35, Batch 867, LR 0.000003 Loss 4.454387, Accuracy 89.550%\n",
      "Epoch 35, Batch 868, LR 0.000003 Loss 4.454628, Accuracy 89.551%\n",
      "Epoch 35, Batch 869, LR 0.000003 Loss 4.454681, Accuracy 89.552%\n",
      "Epoch 35, Batch 870, LR 0.000003 Loss 4.454482, Accuracy 89.552%\n",
      "Epoch 35, Batch 871, LR 0.000003 Loss 4.454047, Accuracy 89.552%\n",
      "Epoch 35, Batch 872, LR 0.000003 Loss 4.453433, Accuracy 89.551%\n",
      "Epoch 35, Batch 873, LR 0.000003 Loss 4.453709, Accuracy 89.548%\n",
      "Epoch 35, Batch 874, LR 0.000003 Loss 4.454131, Accuracy 89.546%\n",
      "Epoch 35, Batch 875, LR 0.000003 Loss 4.454946, Accuracy 89.543%\n",
      "Epoch 35, Batch 876, LR 0.000003 Loss 4.454848, Accuracy 89.541%\n",
      "Epoch 35, Batch 877, LR 0.000003 Loss 4.455176, Accuracy 89.537%\n",
      "Epoch 35, Batch 878, LR 0.000003 Loss 4.456301, Accuracy 89.536%\n",
      "Epoch 35, Batch 879, LR 0.000003 Loss 4.456159, Accuracy 89.539%\n",
      "Epoch 35, Batch 880, LR 0.000003 Loss 4.455777, Accuracy 89.542%\n",
      "Epoch 35, Batch 881, LR 0.000003 Loss 4.455134, Accuracy 89.543%\n",
      "Epoch 35, Batch 882, LR 0.000003 Loss 4.455713, Accuracy 89.538%\n",
      "Epoch 35, Batch 883, LR 0.000003 Loss 4.455390, Accuracy 89.539%\n",
      "Epoch 35, Batch 884, LR 0.000003 Loss 4.455144, Accuracy 89.542%\n",
      "Epoch 35, Batch 885, LR 0.000003 Loss 4.455543, Accuracy 89.543%\n",
      "Epoch 35, Batch 886, LR 0.000003 Loss 4.455446, Accuracy 89.545%\n",
      "Epoch 35, Batch 887, LR 0.000003 Loss 4.454951, Accuracy 89.548%\n",
      "Epoch 35, Batch 888, LR 0.000003 Loss 4.455600, Accuracy 89.546%\n",
      "Epoch 35, Batch 889, LR 0.000003 Loss 4.455571, Accuracy 89.547%\n",
      "Epoch 35, Batch 890, LR 0.000003 Loss 4.455065, Accuracy 89.549%\n",
      "Epoch 35, Batch 891, LR 0.000003 Loss 4.455798, Accuracy 89.546%\n",
      "Epoch 35, Batch 892, LR 0.000003 Loss 4.455816, Accuracy 89.548%\n",
      "Epoch 35, Batch 893, LR 0.000003 Loss 4.455836, Accuracy 89.551%\n",
      "Epoch 35, Batch 894, LR 0.000003 Loss 4.456122, Accuracy 89.550%\n",
      "Epoch 35, Batch 895, LR 0.000003 Loss 4.456244, Accuracy 89.548%\n",
      "Epoch 35, Batch 896, LR 0.000003 Loss 4.456032, Accuracy 89.548%\n",
      "Epoch 35, Batch 897, LR 0.000003 Loss 4.455882, Accuracy 89.550%\n",
      "Epoch 35, Batch 898, LR 0.000003 Loss 4.455333, Accuracy 89.553%\n",
      "Epoch 35, Batch 899, LR 0.000003 Loss 4.454808, Accuracy 89.555%\n",
      "Epoch 35, Batch 900, LR 0.000003 Loss 4.455582, Accuracy 89.556%\n",
      "Epoch 35, Batch 901, LR 0.000003 Loss 4.455062, Accuracy 89.556%\n",
      "Epoch 35, Batch 902, LR 0.000003 Loss 4.454762, Accuracy 89.557%\n",
      "Epoch 35, Batch 903, LR 0.000003 Loss 4.455060, Accuracy 89.553%\n",
      "Epoch 35, Batch 904, LR 0.000003 Loss 4.454840, Accuracy 89.557%\n",
      "Epoch 35, Batch 905, LR 0.000003 Loss 4.455193, Accuracy 89.559%\n",
      "Epoch 35, Batch 906, LR 0.000003 Loss 4.454645, Accuracy 89.563%\n",
      "Epoch 35, Batch 907, LR 0.000003 Loss 4.455404, Accuracy 89.563%\n",
      "Epoch 35, Batch 908, LR 0.000003 Loss 4.454985, Accuracy 89.565%\n",
      "Epoch 35, Batch 909, LR 0.000003 Loss 4.454771, Accuracy 89.568%\n",
      "Epoch 35, Batch 910, LR 0.000003 Loss 4.455112, Accuracy 89.564%\n",
      "Epoch 35, Batch 911, LR 0.000003 Loss 4.454569, Accuracy 89.567%\n",
      "Epoch 35, Batch 912, LR 0.000003 Loss 4.454043, Accuracy 89.567%\n",
      "Epoch 35, Batch 913, LR 0.000003 Loss 4.454823, Accuracy 89.562%\n",
      "Epoch 35, Batch 914, LR 0.000003 Loss 4.454592, Accuracy 89.561%\n",
      "Epoch 35, Batch 915, LR 0.000003 Loss 4.454032, Accuracy 89.564%\n",
      "Epoch 35, Batch 916, LR 0.000003 Loss 4.453692, Accuracy 89.568%\n",
      "Epoch 35, Batch 917, LR 0.000003 Loss 4.454012, Accuracy 89.567%\n",
      "Epoch 35, Batch 918, LR 0.000003 Loss 4.453173, Accuracy 89.569%\n",
      "Epoch 35, Batch 919, LR 0.000003 Loss 4.454184, Accuracy 89.563%\n",
      "Epoch 35, Batch 920, LR 0.000003 Loss 4.453998, Accuracy 89.564%\n",
      "Epoch 35, Batch 921, LR 0.000003 Loss 4.454552, Accuracy 89.563%\n",
      "Epoch 35, Batch 922, LR 0.000003 Loss 4.454043, Accuracy 89.563%\n",
      "Epoch 35, Batch 923, LR 0.000003 Loss 4.455326, Accuracy 89.557%\n",
      "Epoch 35, Batch 924, LR 0.000003 Loss 4.456467, Accuracy 89.553%\n",
      "Epoch 35, Batch 925, LR 0.000003 Loss 4.457750, Accuracy 89.544%\n",
      "Epoch 35, Batch 926, LR 0.000003 Loss 4.456890, Accuracy 89.547%\n",
      "Epoch 35, Batch 927, LR 0.000003 Loss 4.456103, Accuracy 89.550%\n",
      "Epoch 35, Batch 928, LR 0.000003 Loss 4.456515, Accuracy 89.551%\n",
      "Epoch 35, Batch 929, LR 0.000003 Loss 4.456344, Accuracy 89.555%\n",
      "Epoch 35, Batch 930, LR 0.000003 Loss 4.456287, Accuracy 89.557%\n",
      "Epoch 35, Batch 931, LR 0.000003 Loss 4.456687, Accuracy 89.558%\n",
      "Epoch 35, Batch 932, LR 0.000003 Loss 4.456564, Accuracy 89.562%\n",
      "Epoch 35, Batch 933, LR 0.000003 Loss 4.456739, Accuracy 89.563%\n",
      "Epoch 35, Batch 934, LR 0.000003 Loss 4.457109, Accuracy 89.561%\n",
      "Epoch 35, Batch 935, LR 0.000003 Loss 4.457782, Accuracy 89.560%\n",
      "Epoch 35, Batch 936, LR 0.000003 Loss 4.457278, Accuracy 89.562%\n",
      "Epoch 35, Batch 937, LR 0.000003 Loss 4.457294, Accuracy 89.563%\n",
      "Epoch 35, Batch 938, LR 0.000003 Loss 4.457842, Accuracy 89.562%\n",
      "Epoch 35, Batch 939, LR 0.000003 Loss 4.458240, Accuracy 89.561%\n",
      "Epoch 35, Batch 940, LR 0.000003 Loss 4.458066, Accuracy 89.564%\n",
      "Epoch 35, Batch 941, LR 0.000003 Loss 4.458653, Accuracy 89.557%\n",
      "Epoch 35, Batch 942, LR 0.000003 Loss 4.458690, Accuracy 89.559%\n",
      "Epoch 35, Batch 943, LR 0.000003 Loss 4.459097, Accuracy 89.555%\n",
      "Epoch 35, Batch 944, LR 0.000003 Loss 4.458860, Accuracy 89.555%\n",
      "Epoch 35, Batch 945, LR 0.000003 Loss 4.458734, Accuracy 89.557%\n",
      "Epoch 35, Batch 946, LR 0.000003 Loss 4.459192, Accuracy 89.554%\n",
      "Epoch 35, Batch 947, LR 0.000003 Loss 4.459473, Accuracy 89.555%\n",
      "Epoch 35, Batch 948, LR 0.000003 Loss 4.459402, Accuracy 89.555%\n",
      "Epoch 35, Batch 949, LR 0.000003 Loss 4.459536, Accuracy 89.553%\n",
      "Epoch 35, Batch 950, LR 0.000003 Loss 4.459138, Accuracy 89.554%\n",
      "Epoch 35, Batch 951, LR 0.000003 Loss 4.459368, Accuracy 89.552%\n",
      "Epoch 35, Batch 952, LR 0.000003 Loss 4.458397, Accuracy 89.557%\n",
      "Epoch 35, Batch 953, LR 0.000003 Loss 4.458410, Accuracy 89.554%\n",
      "Epoch 35, Batch 954, LR 0.000003 Loss 4.458447, Accuracy 89.555%\n",
      "Epoch 35, Batch 955, LR 0.000003 Loss 4.458653, Accuracy 89.549%\n",
      "Epoch 35, Batch 956, LR 0.000003 Loss 4.459375, Accuracy 89.548%\n",
      "Epoch 35, Batch 957, LR 0.000003 Loss 4.459035, Accuracy 89.551%\n",
      "Epoch 35, Batch 958, LR 0.000003 Loss 4.458907, Accuracy 89.553%\n",
      "Epoch 35, Batch 959, LR 0.000003 Loss 4.459632, Accuracy 89.548%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 960, LR 0.000003 Loss 4.460625, Accuracy 89.541%\n",
      "Epoch 35, Batch 961, LR 0.000003 Loss 4.460620, Accuracy 89.541%\n",
      "Epoch 35, Batch 962, LR 0.000003 Loss 4.459863, Accuracy 89.547%\n",
      "Epoch 35, Batch 963, LR 0.000003 Loss 4.459817, Accuracy 89.548%\n",
      "Epoch 35, Batch 964, LR 0.000003 Loss 4.460040, Accuracy 89.549%\n",
      "Epoch 35, Batch 965, LR 0.000003 Loss 4.459667, Accuracy 89.551%\n",
      "Epoch 35, Batch 966, LR 0.000003 Loss 4.459730, Accuracy 89.553%\n",
      "Epoch 35, Batch 967, LR 0.000003 Loss 4.459388, Accuracy 89.554%\n",
      "Epoch 35, Batch 968, LR 0.000003 Loss 4.459537, Accuracy 89.553%\n",
      "Epoch 35, Batch 969, LR 0.000003 Loss 4.459800, Accuracy 89.550%\n",
      "Epoch 35, Batch 970, LR 0.000003 Loss 4.459785, Accuracy 89.551%\n",
      "Epoch 35, Batch 971, LR 0.000003 Loss 4.460209, Accuracy 89.549%\n",
      "Epoch 35, Batch 972, LR 0.000003 Loss 4.460908, Accuracy 89.548%\n",
      "Epoch 35, Batch 973, LR 0.000003 Loss 4.461286, Accuracy 89.546%\n",
      "Epoch 35, Batch 974, LR 0.000003 Loss 4.462800, Accuracy 89.538%\n",
      "Epoch 35, Batch 975, LR 0.000003 Loss 4.463344, Accuracy 89.534%\n",
      "Epoch 35, Batch 976, LR 0.000003 Loss 4.462062, Accuracy 89.540%\n",
      "Epoch 35, Batch 977, LR 0.000003 Loss 4.461753, Accuracy 89.542%\n",
      "Epoch 35, Batch 978, LR 0.000003 Loss 4.462353, Accuracy 89.542%\n",
      "Epoch 35, Batch 979, LR 0.000003 Loss 4.462234, Accuracy 89.538%\n",
      "Epoch 35, Batch 980, LR 0.000003 Loss 4.461929, Accuracy 89.542%\n",
      "Epoch 35, Batch 981, LR 0.000003 Loss 4.462078, Accuracy 89.540%\n",
      "Epoch 35, Batch 982, LR 0.000003 Loss 4.461642, Accuracy 89.543%\n",
      "Epoch 35, Batch 983, LR 0.000003 Loss 4.461343, Accuracy 89.544%\n",
      "Epoch 35, Batch 984, LR 0.000003 Loss 4.460782, Accuracy 89.547%\n",
      "Epoch 35, Batch 985, LR 0.000003 Loss 4.460207, Accuracy 89.550%\n",
      "Epoch 35, Batch 986, LR 0.000003 Loss 4.460880, Accuracy 89.547%\n",
      "Epoch 35, Batch 987, LR 0.000003 Loss 4.460719, Accuracy 89.548%\n",
      "Epoch 35, Batch 988, LR 0.000003 Loss 4.460968, Accuracy 89.546%\n",
      "Epoch 35, Batch 989, LR 0.000003 Loss 4.461607, Accuracy 89.542%\n",
      "Epoch 35, Batch 990, LR 0.000003 Loss 4.462243, Accuracy 89.542%\n",
      "Epoch 35, Batch 991, LR 0.000003 Loss 4.462329, Accuracy 89.539%\n",
      "Epoch 35, Batch 992, LR 0.000003 Loss 4.462511, Accuracy 89.537%\n",
      "Epoch 35, Batch 993, LR 0.000003 Loss 4.462580, Accuracy 89.536%\n",
      "Epoch 35, Batch 994, LR 0.000003 Loss 4.462462, Accuracy 89.535%\n",
      "Epoch 35, Batch 995, LR 0.000003 Loss 4.462448, Accuracy 89.538%\n",
      "Epoch 35, Batch 996, LR 0.000003 Loss 4.462702, Accuracy 89.542%\n",
      "Epoch 35, Batch 997, LR 0.000003 Loss 4.462923, Accuracy 89.540%\n",
      "Epoch 35, Batch 998, LR 0.000003 Loss 4.462870, Accuracy 89.538%\n",
      "Epoch 35, Batch 999, LR 0.000003 Loss 4.462043, Accuracy 89.538%\n",
      "Epoch 35, Batch 1000, LR 0.000003 Loss 4.462210, Accuracy 89.537%\n",
      "Epoch 35, Batch 1001, LR 0.000003 Loss 4.462067, Accuracy 89.536%\n",
      "Epoch 35, Batch 1002, LR 0.000003 Loss 4.462120, Accuracy 89.539%\n",
      "Epoch 35, Batch 1003, LR 0.000003 Loss 4.462579, Accuracy 89.538%\n",
      "Epoch 35, Batch 1004, LR 0.000003 Loss 4.462429, Accuracy 89.540%\n",
      "Epoch 35, Batch 1005, LR 0.000003 Loss 4.462513, Accuracy 89.540%\n",
      "Epoch 35, Batch 1006, LR 0.000003 Loss 4.462159, Accuracy 89.541%\n",
      "Epoch 35, Batch 1007, LR 0.000003 Loss 4.462568, Accuracy 89.540%\n",
      "Epoch 35, Batch 1008, LR 0.000003 Loss 4.462398, Accuracy 89.540%\n",
      "Epoch 35, Batch 1009, LR 0.000003 Loss 4.463002, Accuracy 89.536%\n",
      "Epoch 35, Batch 1010, LR 0.000003 Loss 4.463091, Accuracy 89.534%\n",
      "Epoch 35, Batch 1011, LR 0.000003 Loss 4.463114, Accuracy 89.532%\n",
      "Epoch 35, Batch 1012, LR 0.000003 Loss 4.463542, Accuracy 89.529%\n",
      "Epoch 35, Batch 1013, LR 0.000003 Loss 4.464635, Accuracy 89.523%\n",
      "Epoch 35, Batch 1014, LR 0.000003 Loss 4.464761, Accuracy 89.519%\n",
      "Epoch 35, Batch 1015, LR 0.000003 Loss 4.464954, Accuracy 89.517%\n",
      "Epoch 35, Batch 1016, LR 0.000003 Loss 4.464397, Accuracy 89.519%\n",
      "Epoch 35, Batch 1017, LR 0.000003 Loss 4.464120, Accuracy 89.517%\n",
      "Epoch 35, Batch 1018, LR 0.000003 Loss 4.463583, Accuracy 89.517%\n",
      "Epoch 35, Batch 1019, LR 0.000003 Loss 4.463490, Accuracy 89.516%\n",
      "Epoch 35, Batch 1020, LR 0.000003 Loss 4.462906, Accuracy 89.519%\n",
      "Epoch 35, Batch 1021, LR 0.000003 Loss 4.462843, Accuracy 89.523%\n",
      "Epoch 35, Batch 1022, LR 0.000003 Loss 4.462103, Accuracy 89.524%\n",
      "Epoch 35, Batch 1023, LR 0.000003 Loss 4.462736, Accuracy 89.519%\n",
      "Epoch 35, Batch 1024, LR 0.000003 Loss 4.462415, Accuracy 89.523%\n",
      "Epoch 35, Batch 1025, LR 0.000003 Loss 4.462681, Accuracy 89.523%\n",
      "Epoch 35, Batch 1026, LR 0.000003 Loss 4.463069, Accuracy 89.517%\n",
      "Epoch 35, Batch 1027, LR 0.000003 Loss 4.462699, Accuracy 89.520%\n",
      "Epoch 35, Batch 1028, LR 0.000003 Loss 4.463525, Accuracy 89.515%\n",
      "Epoch 35, Batch 1029, LR 0.000003 Loss 4.463701, Accuracy 89.510%\n",
      "Epoch 35, Batch 1030, LR 0.000003 Loss 4.463065, Accuracy 89.511%\n",
      "Epoch 35, Batch 1031, LR 0.000003 Loss 4.463186, Accuracy 89.511%\n",
      "Epoch 35, Batch 1032, LR 0.000003 Loss 4.463121, Accuracy 89.514%\n",
      "Epoch 35, Batch 1033, LR 0.000003 Loss 4.463336, Accuracy 89.514%\n",
      "Epoch 35, Batch 1034, LR 0.000003 Loss 4.462806, Accuracy 89.517%\n",
      "Epoch 35, Batch 1035, LR 0.000003 Loss 4.463093, Accuracy 89.515%\n",
      "Epoch 35, Batch 1036, LR 0.000003 Loss 4.462707, Accuracy 89.513%\n",
      "Epoch 35, Batch 1037, LR 0.000003 Loss 4.462863, Accuracy 89.511%\n",
      "Epoch 35, Batch 1038, LR 0.000003 Loss 4.462730, Accuracy 89.511%\n",
      "Epoch 35, Batch 1039, LR 0.000003 Loss 4.462940, Accuracy 89.508%\n",
      "Epoch 35, Batch 1040, LR 0.000003 Loss 4.463085, Accuracy 89.507%\n",
      "Epoch 35, Batch 1041, LR 0.000003 Loss 4.462504, Accuracy 89.511%\n",
      "Epoch 35, Batch 1042, LR 0.000003 Loss 4.462321, Accuracy 89.515%\n",
      "Epoch 35, Batch 1043, LR 0.000003 Loss 4.462315, Accuracy 89.517%\n",
      "Epoch 35, Batch 1044, LR 0.000003 Loss 4.461984, Accuracy 89.521%\n",
      "Epoch 35, Batch 1045, LR 0.000003 Loss 4.462169, Accuracy 89.518%\n",
      "Epoch 35, Batch 1046, LR 0.000003 Loss 4.462264, Accuracy 89.516%\n",
      "Epoch 35, Batch 1047, LR 0.000003 Loss 4.461868, Accuracy 89.519%\n",
      "Epoch 35, Loss (train set) 4.461868, Accuracy (train set) 89.519%\n",
      "Epoch 36, Batch 1, LR 0.000003 Loss 4.341098, Accuracy 92.188%\n",
      "Epoch 36, Batch 2, LR 0.000003 Loss 4.763424, Accuracy 91.406%\n",
      "Epoch 36, Batch 3, LR 0.000003 Loss 4.549798, Accuracy 91.146%\n",
      "Epoch 36, Batch 4, LR 0.000003 Loss 4.535376, Accuracy 91.406%\n",
      "Epoch 36, Batch 5, LR 0.000003 Loss 4.556020, Accuracy 90.781%\n",
      "Epoch 36, Batch 6, LR 0.000003 Loss 4.585778, Accuracy 90.625%\n",
      "Epoch 36, Batch 7, LR 0.000003 Loss 4.533603, Accuracy 90.625%\n",
      "Epoch 36, Batch 8, LR 0.000003 Loss 4.497341, Accuracy 91.016%\n",
      "Epoch 36, Batch 9, LR 0.000003 Loss 4.529823, Accuracy 90.712%\n",
      "Epoch 36, Batch 10, LR 0.000003 Loss 4.537063, Accuracy 90.469%\n",
      "Epoch 36, Batch 11, LR 0.000003 Loss 4.517992, Accuracy 90.625%\n",
      "Epoch 36, Batch 12, LR 0.000003 Loss 4.535314, Accuracy 90.560%\n",
      "Epoch 36, Batch 13, LR 0.000003 Loss 4.512336, Accuracy 90.144%\n",
      "Epoch 36, Batch 14, LR 0.000003 Loss 4.480961, Accuracy 90.067%\n",
      "Epoch 36, Batch 15, LR 0.000003 Loss 4.503710, Accuracy 90.000%\n",
      "Epoch 36, Batch 16, LR 0.000003 Loss 4.537442, Accuracy 89.600%\n",
      "Epoch 36, Batch 17, LR 0.000003 Loss 4.546140, Accuracy 89.568%\n",
      "Epoch 36, Batch 18, LR 0.000003 Loss 4.467863, Accuracy 89.800%\n",
      "Epoch 36, Batch 19, LR 0.000003 Loss 4.468296, Accuracy 89.638%\n",
      "Epoch 36, Batch 20, LR 0.000003 Loss 4.429783, Accuracy 89.883%\n",
      "Epoch 36, Batch 21, LR 0.000003 Loss 4.421733, Accuracy 89.918%\n",
      "Epoch 36, Batch 22, LR 0.000003 Loss 4.436208, Accuracy 89.808%\n",
      "Epoch 36, Batch 23, LR 0.000003 Loss 4.441951, Accuracy 89.708%\n",
      "Epoch 36, Batch 24, LR 0.000003 Loss 4.454236, Accuracy 89.714%\n",
      "Epoch 36, Batch 25, LR 0.000003 Loss 4.445228, Accuracy 89.781%\n",
      "Epoch 36, Batch 26, LR 0.000003 Loss 4.445671, Accuracy 89.694%\n",
      "Epoch 36, Batch 27, LR 0.000003 Loss 4.423596, Accuracy 89.728%\n",
      "Epoch 36, Batch 28, LR 0.000003 Loss 4.400855, Accuracy 89.760%\n",
      "Epoch 36, Batch 29, LR 0.000003 Loss 4.392963, Accuracy 89.817%\n",
      "Epoch 36, Batch 30, LR 0.000003 Loss 4.362991, Accuracy 89.974%\n",
      "Epoch 36, Batch 31, LR 0.000003 Loss 4.382838, Accuracy 89.844%\n",
      "Epoch 36, Batch 32, LR 0.000003 Loss 4.386011, Accuracy 89.893%\n",
      "Epoch 36, Batch 33, LR 0.000003 Loss 4.399181, Accuracy 89.844%\n",
      "Epoch 36, Batch 34, LR 0.000003 Loss 4.377564, Accuracy 89.959%\n",
      "Epoch 36, Batch 35, LR 0.000003 Loss 4.358616, Accuracy 90.022%\n",
      "Epoch 36, Batch 36, LR 0.000003 Loss 4.363472, Accuracy 89.974%\n",
      "Epoch 36, Batch 37, LR 0.000003 Loss 4.349738, Accuracy 90.139%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 38, LR 0.000003 Loss 4.368552, Accuracy 90.090%\n",
      "Epoch 36, Batch 39, LR 0.000003 Loss 4.371266, Accuracy 90.044%\n",
      "Epoch 36, Batch 40, LR 0.000003 Loss 4.364555, Accuracy 90.098%\n",
      "Epoch 36, Batch 41, LR 0.000003 Loss 4.378355, Accuracy 90.034%\n",
      "Epoch 36, Batch 42, LR 0.000003 Loss 4.386849, Accuracy 90.067%\n",
      "Epoch 36, Batch 43, LR 0.000003 Loss 4.391247, Accuracy 90.098%\n",
      "Epoch 36, Batch 44, LR 0.000003 Loss 4.401383, Accuracy 89.968%\n",
      "Epoch 36, Batch 45, LR 0.000003 Loss 4.391538, Accuracy 90.035%\n",
      "Epoch 36, Batch 46, LR 0.000003 Loss 4.394090, Accuracy 90.014%\n",
      "Epoch 36, Batch 47, LR 0.000003 Loss 4.395894, Accuracy 89.977%\n",
      "Epoch 36, Batch 48, LR 0.000003 Loss 4.395354, Accuracy 90.023%\n",
      "Epoch 36, Batch 49, LR 0.000003 Loss 4.393705, Accuracy 90.051%\n",
      "Epoch 36, Batch 50, LR 0.000003 Loss 4.410957, Accuracy 89.953%\n",
      "Epoch 36, Batch 51, LR 0.000003 Loss 4.422614, Accuracy 89.828%\n",
      "Epoch 36, Batch 52, LR 0.000003 Loss 4.429275, Accuracy 89.814%\n",
      "Epoch 36, Batch 53, LR 0.000003 Loss 4.411295, Accuracy 89.888%\n",
      "Epoch 36, Batch 54, LR 0.000003 Loss 4.419343, Accuracy 89.815%\n",
      "Epoch 36, Batch 55, LR 0.000003 Loss 4.414974, Accuracy 89.872%\n",
      "Epoch 36, Batch 56, LR 0.000003 Loss 4.406536, Accuracy 89.886%\n",
      "Epoch 36, Batch 57, LR 0.000003 Loss 4.404175, Accuracy 89.899%\n",
      "Epoch 36, Batch 58, LR 0.000003 Loss 4.411027, Accuracy 89.790%\n",
      "Epoch 36, Batch 59, LR 0.000003 Loss 4.417702, Accuracy 89.804%\n",
      "Epoch 36, Batch 60, LR 0.000003 Loss 4.429761, Accuracy 89.740%\n",
      "Epoch 36, Batch 61, LR 0.000003 Loss 4.424641, Accuracy 89.793%\n",
      "Epoch 36, Batch 62, LR 0.000003 Loss 4.418042, Accuracy 89.831%\n",
      "Epoch 36, Batch 63, LR 0.000003 Loss 4.420080, Accuracy 89.881%\n",
      "Epoch 36, Batch 64, LR 0.000003 Loss 4.413416, Accuracy 89.844%\n",
      "Epoch 36, Batch 65, LR 0.000003 Loss 4.418880, Accuracy 89.844%\n",
      "Epoch 36, Batch 66, LR 0.000003 Loss 4.413988, Accuracy 89.844%\n",
      "Epoch 36, Batch 67, LR 0.000003 Loss 4.422097, Accuracy 89.867%\n",
      "Epoch 36, Batch 68, LR 0.000003 Loss 4.415015, Accuracy 89.913%\n",
      "Epoch 36, Batch 69, LR 0.000003 Loss 4.412950, Accuracy 89.900%\n",
      "Epoch 36, Batch 70, LR 0.000003 Loss 4.409313, Accuracy 89.933%\n",
      "Epoch 36, Batch 71, LR 0.000003 Loss 4.398347, Accuracy 89.954%\n",
      "Epoch 36, Batch 72, LR 0.000003 Loss 4.411657, Accuracy 89.941%\n",
      "Epoch 36, Batch 73, LR 0.000003 Loss 4.410657, Accuracy 89.951%\n",
      "Epoch 36, Batch 74, LR 0.000003 Loss 4.402154, Accuracy 89.960%\n",
      "Epoch 36, Batch 75, LR 0.000003 Loss 4.403134, Accuracy 89.906%\n",
      "Epoch 36, Batch 76, LR 0.000003 Loss 4.409352, Accuracy 89.875%\n",
      "Epoch 36, Batch 77, LR 0.000003 Loss 4.405978, Accuracy 89.864%\n",
      "Epoch 36, Batch 78, LR 0.000003 Loss 4.400504, Accuracy 89.904%\n",
      "Epoch 36, Batch 79, LR 0.000003 Loss 4.407923, Accuracy 89.844%\n",
      "Epoch 36, Batch 80, LR 0.000003 Loss 4.408270, Accuracy 89.824%\n",
      "Epoch 36, Batch 81, LR 0.000003 Loss 4.413402, Accuracy 89.786%\n",
      "Epoch 36, Batch 82, LR 0.000003 Loss 4.403984, Accuracy 89.844%\n",
      "Epoch 36, Batch 83, LR 0.000003 Loss 4.411311, Accuracy 89.797%\n",
      "Epoch 36, Batch 84, LR 0.000003 Loss 4.412281, Accuracy 89.816%\n",
      "Epoch 36, Batch 85, LR 0.000003 Loss 4.411364, Accuracy 89.816%\n",
      "Epoch 36, Batch 86, LR 0.000003 Loss 4.405090, Accuracy 89.844%\n",
      "Epoch 36, Batch 87, LR 0.000003 Loss 4.393705, Accuracy 89.871%\n",
      "Epoch 36, Batch 88, LR 0.000003 Loss 4.401322, Accuracy 89.844%\n",
      "Epoch 36, Batch 89, LR 0.000003 Loss 4.401919, Accuracy 89.826%\n",
      "Epoch 36, Batch 90, LR 0.000003 Loss 4.401956, Accuracy 89.835%\n",
      "Epoch 36, Batch 91, LR 0.000003 Loss 4.400222, Accuracy 89.835%\n",
      "Epoch 36, Batch 92, LR 0.000003 Loss 4.407539, Accuracy 89.818%\n",
      "Epoch 36, Batch 93, LR 0.000003 Loss 4.408921, Accuracy 89.768%\n",
      "Epoch 36, Batch 94, LR 0.000003 Loss 4.408985, Accuracy 89.769%\n",
      "Epoch 36, Batch 95, LR 0.000003 Loss 4.410747, Accuracy 89.745%\n",
      "Epoch 36, Batch 96, LR 0.000003 Loss 4.405884, Accuracy 89.771%\n",
      "Epoch 36, Batch 97, LR 0.000003 Loss 4.411020, Accuracy 89.755%\n",
      "Epoch 36, Batch 98, LR 0.000003 Loss 4.414920, Accuracy 89.756%\n",
      "Epoch 36, Batch 99, LR 0.000003 Loss 4.422185, Accuracy 89.717%\n",
      "Epoch 36, Batch 100, LR 0.000003 Loss 4.420317, Accuracy 89.734%\n",
      "Epoch 36, Batch 101, LR 0.000003 Loss 4.423585, Accuracy 89.735%\n",
      "Epoch 36, Batch 102, LR 0.000003 Loss 4.425746, Accuracy 89.721%\n",
      "Epoch 36, Batch 103, LR 0.000003 Loss 4.423855, Accuracy 89.768%\n",
      "Epoch 36, Batch 104, LR 0.000003 Loss 4.422701, Accuracy 89.761%\n",
      "Epoch 36, Batch 105, LR 0.000003 Loss 4.422064, Accuracy 89.784%\n",
      "Epoch 36, Batch 106, LR 0.000003 Loss 4.415952, Accuracy 89.822%\n",
      "Epoch 36, Batch 107, LR 0.000003 Loss 4.419294, Accuracy 89.800%\n",
      "Epoch 36, Batch 108, LR 0.000003 Loss 4.417746, Accuracy 89.829%\n",
      "Epoch 36, Batch 109, LR 0.000003 Loss 4.413055, Accuracy 89.865%\n",
      "Epoch 36, Batch 110, LR 0.000003 Loss 4.410869, Accuracy 89.844%\n",
      "Epoch 36, Batch 111, LR 0.000003 Loss 4.409466, Accuracy 89.830%\n",
      "Epoch 36, Batch 112, LR 0.000003 Loss 4.410846, Accuracy 89.823%\n",
      "Epoch 36, Batch 113, LR 0.000003 Loss 4.409116, Accuracy 89.816%\n",
      "Epoch 36, Batch 114, LR 0.000003 Loss 4.404934, Accuracy 89.844%\n",
      "Epoch 36, Batch 115, LR 0.000003 Loss 4.402248, Accuracy 89.844%\n",
      "Epoch 36, Batch 116, LR 0.000003 Loss 4.398752, Accuracy 89.864%\n",
      "Epoch 36, Batch 117, LR 0.000003 Loss 4.393135, Accuracy 89.857%\n",
      "Epoch 36, Batch 118, LR 0.000003 Loss 4.394418, Accuracy 89.844%\n",
      "Epoch 36, Batch 119, LR 0.000003 Loss 4.393292, Accuracy 89.870%\n",
      "Epoch 36, Batch 120, LR 0.000003 Loss 4.395758, Accuracy 89.850%\n",
      "Epoch 36, Batch 121, LR 0.000003 Loss 4.393834, Accuracy 89.831%\n",
      "Epoch 36, Batch 122, LR 0.000003 Loss 4.401107, Accuracy 89.786%\n",
      "Epoch 36, Batch 123, LR 0.000003 Loss 4.398111, Accuracy 89.812%\n",
      "Epoch 36, Batch 124, LR 0.000003 Loss 4.398976, Accuracy 89.812%\n",
      "Epoch 36, Batch 125, LR 0.000003 Loss 4.394485, Accuracy 89.831%\n",
      "Epoch 36, Batch 126, LR 0.000003 Loss 4.395784, Accuracy 89.819%\n",
      "Epoch 36, Batch 127, LR 0.000003 Loss 4.402972, Accuracy 89.770%\n",
      "Epoch 36, Batch 128, LR 0.000003 Loss 4.401789, Accuracy 89.777%\n",
      "Epoch 36, Batch 129, LR 0.000003 Loss 4.403549, Accuracy 89.759%\n",
      "Epoch 36, Batch 130, LR 0.000003 Loss 4.398986, Accuracy 89.784%\n",
      "Epoch 36, Batch 131, LR 0.000003 Loss 4.399960, Accuracy 89.784%\n",
      "Epoch 36, Batch 132, LR 0.000003 Loss 4.404221, Accuracy 89.761%\n",
      "Epoch 36, Batch 133, LR 0.000003 Loss 4.413790, Accuracy 89.726%\n",
      "Epoch 36, Batch 134, LR 0.000003 Loss 4.411823, Accuracy 89.750%\n",
      "Epoch 36, Batch 135, LR 0.000003 Loss 4.412176, Accuracy 89.728%\n",
      "Epoch 36, Batch 136, LR 0.000003 Loss 4.413987, Accuracy 89.735%\n",
      "Epoch 36, Batch 137, LR 0.000003 Loss 4.416513, Accuracy 89.701%\n",
      "Epoch 36, Batch 138, LR 0.000003 Loss 4.415840, Accuracy 89.697%\n",
      "Epoch 36, Batch 139, LR 0.000003 Loss 4.416021, Accuracy 89.714%\n",
      "Epoch 36, Batch 140, LR 0.000003 Loss 4.416459, Accuracy 89.710%\n",
      "Epoch 36, Batch 141, LR 0.000003 Loss 4.411732, Accuracy 89.733%\n",
      "Epoch 36, Batch 142, LR 0.000003 Loss 4.409457, Accuracy 89.767%\n",
      "Epoch 36, Batch 143, LR 0.000003 Loss 4.406886, Accuracy 89.784%\n",
      "Epoch 36, Batch 144, LR 0.000003 Loss 4.408952, Accuracy 89.779%\n",
      "Epoch 36, Batch 145, LR 0.000003 Loss 4.411564, Accuracy 89.731%\n",
      "Epoch 36, Batch 146, LR 0.000003 Loss 4.413066, Accuracy 89.726%\n",
      "Epoch 36, Batch 147, LR 0.000003 Loss 4.412849, Accuracy 89.727%\n",
      "Epoch 36, Batch 148, LR 0.000003 Loss 4.416764, Accuracy 89.712%\n",
      "Epoch 36, Batch 149, LR 0.000003 Loss 4.416751, Accuracy 89.713%\n",
      "Epoch 36, Batch 150, LR 0.000003 Loss 4.419626, Accuracy 89.714%\n",
      "Epoch 36, Batch 151, LR 0.000003 Loss 4.419734, Accuracy 89.720%\n",
      "Epoch 36, Batch 152, LR 0.000003 Loss 4.419122, Accuracy 89.731%\n",
      "Epoch 36, Batch 153, LR 0.000003 Loss 4.419059, Accuracy 89.731%\n",
      "Epoch 36, Batch 154, LR 0.000003 Loss 4.417069, Accuracy 89.747%\n",
      "Epoch 36, Batch 155, LR 0.000003 Loss 4.415617, Accuracy 89.743%\n",
      "Epoch 36, Batch 156, LR 0.000003 Loss 4.415899, Accuracy 89.729%\n",
      "Epoch 36, Batch 157, LR 0.000003 Loss 4.419833, Accuracy 89.714%\n",
      "Epoch 36, Batch 158, LR 0.000003 Loss 4.421952, Accuracy 89.725%\n",
      "Epoch 36, Batch 159, LR 0.000003 Loss 4.422203, Accuracy 89.731%\n",
      "Epoch 36, Batch 160, LR 0.000003 Loss 4.426884, Accuracy 89.717%\n",
      "Epoch 36, Batch 161, LR 0.000003 Loss 4.423292, Accuracy 89.727%\n",
      "Epoch 36, Batch 162, LR 0.000003 Loss 4.422607, Accuracy 89.747%\n",
      "Epoch 36, Batch 163, LR 0.000003 Loss 4.421138, Accuracy 89.757%\n",
      "Epoch 36, Batch 164, LR 0.000003 Loss 4.418256, Accuracy 89.782%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 165, LR 0.000003 Loss 4.419888, Accuracy 89.796%\n",
      "Epoch 36, Batch 166, LR 0.000003 Loss 4.423745, Accuracy 89.764%\n",
      "Epoch 36, Batch 167, LR 0.000003 Loss 4.426091, Accuracy 89.755%\n",
      "Epoch 36, Batch 168, LR 0.000002 Loss 4.427243, Accuracy 89.765%\n",
      "Epoch 36, Batch 169, LR 0.000002 Loss 4.426960, Accuracy 89.770%\n",
      "Epoch 36, Batch 170, LR 0.000002 Loss 4.424859, Accuracy 89.789%\n",
      "Epoch 36, Batch 171, LR 0.000002 Loss 4.418928, Accuracy 89.821%\n",
      "Epoch 36, Batch 172, LR 0.000002 Loss 4.424997, Accuracy 89.771%\n",
      "Epoch 36, Batch 173, LR 0.000002 Loss 4.426474, Accuracy 89.785%\n",
      "Epoch 36, Batch 174, LR 0.000002 Loss 4.422634, Accuracy 89.803%\n",
      "Epoch 36, Batch 175, LR 0.000002 Loss 4.424088, Accuracy 89.804%\n",
      "Epoch 36, Batch 176, LR 0.000002 Loss 4.427593, Accuracy 89.768%\n",
      "Epoch 36, Batch 177, LR 0.000002 Loss 4.430045, Accuracy 89.760%\n",
      "Epoch 36, Batch 178, LR 0.000002 Loss 4.434791, Accuracy 89.743%\n",
      "Epoch 36, Batch 179, LR 0.000002 Loss 4.435227, Accuracy 89.735%\n",
      "Epoch 36, Batch 180, LR 0.000002 Loss 4.438709, Accuracy 89.727%\n",
      "Epoch 36, Batch 181, LR 0.000002 Loss 4.433228, Accuracy 89.740%\n",
      "Epoch 36, Batch 182, LR 0.000002 Loss 4.431040, Accuracy 89.741%\n",
      "Epoch 36, Batch 183, LR 0.000002 Loss 4.432910, Accuracy 89.741%\n",
      "Epoch 36, Batch 184, LR 0.000002 Loss 4.428889, Accuracy 89.763%\n",
      "Epoch 36, Batch 185, LR 0.000002 Loss 4.432639, Accuracy 89.759%\n",
      "Epoch 36, Batch 186, LR 0.000002 Loss 4.429747, Accuracy 89.772%\n",
      "Epoch 36, Batch 187, LR 0.000002 Loss 4.430121, Accuracy 89.773%\n",
      "Epoch 36, Batch 188, LR 0.000002 Loss 4.430475, Accuracy 89.744%\n",
      "Epoch 36, Batch 189, LR 0.000002 Loss 4.428711, Accuracy 89.749%\n",
      "Epoch 36, Batch 190, LR 0.000002 Loss 4.425623, Accuracy 89.762%\n",
      "Epoch 36, Batch 191, LR 0.000002 Loss 4.422108, Accuracy 89.778%\n",
      "Epoch 36, Batch 192, LR 0.000002 Loss 4.424518, Accuracy 89.775%\n",
      "Epoch 36, Batch 193, LR 0.000002 Loss 4.424810, Accuracy 89.755%\n",
      "Epoch 36, Batch 194, LR 0.000002 Loss 4.420975, Accuracy 89.767%\n",
      "Epoch 36, Batch 195, LR 0.000002 Loss 4.421451, Accuracy 89.744%\n",
      "Epoch 36, Batch 196, LR 0.000002 Loss 4.421887, Accuracy 89.748%\n",
      "Epoch 36, Batch 197, LR 0.000002 Loss 4.422761, Accuracy 89.729%\n",
      "Epoch 36, Batch 198, LR 0.000002 Loss 4.421323, Accuracy 89.729%\n",
      "Epoch 36, Batch 199, LR 0.000002 Loss 4.421402, Accuracy 89.722%\n",
      "Epoch 36, Batch 200, LR 0.000002 Loss 4.417941, Accuracy 89.734%\n",
      "Epoch 36, Batch 201, LR 0.000002 Loss 4.419955, Accuracy 89.727%\n",
      "Epoch 36, Batch 202, LR 0.000002 Loss 4.421465, Accuracy 89.728%\n",
      "Epoch 36, Batch 203, LR 0.000002 Loss 4.418895, Accuracy 89.736%\n",
      "Epoch 36, Batch 204, LR 0.000002 Loss 4.417942, Accuracy 89.748%\n",
      "Epoch 36, Batch 205, LR 0.000002 Loss 4.420755, Accuracy 89.726%\n",
      "Epoch 36, Batch 206, LR 0.000002 Loss 4.421337, Accuracy 89.726%\n",
      "Epoch 36, Batch 207, LR 0.000002 Loss 4.418150, Accuracy 89.738%\n",
      "Epoch 36, Batch 208, LR 0.000002 Loss 4.418249, Accuracy 89.750%\n",
      "Epoch 36, Batch 209, LR 0.000002 Loss 4.424395, Accuracy 89.717%\n",
      "Epoch 36, Batch 210, LR 0.000002 Loss 4.423910, Accuracy 89.725%\n",
      "Epoch 36, Batch 211, LR 0.000002 Loss 4.424748, Accuracy 89.725%\n",
      "Epoch 36, Batch 212, LR 0.000002 Loss 4.422529, Accuracy 89.737%\n",
      "Epoch 36, Batch 213, LR 0.000002 Loss 4.422790, Accuracy 89.752%\n",
      "Epoch 36, Batch 214, LR 0.000002 Loss 4.422985, Accuracy 89.760%\n",
      "Epoch 36, Batch 215, LR 0.000002 Loss 4.418385, Accuracy 89.775%\n",
      "Epoch 36, Batch 216, LR 0.000002 Loss 4.421719, Accuracy 89.779%\n",
      "Epoch 36, Batch 217, LR 0.000002 Loss 4.421260, Accuracy 89.775%\n",
      "Epoch 36, Batch 218, LR 0.000002 Loss 4.423764, Accuracy 89.765%\n",
      "Epoch 36, Batch 219, LR 0.000002 Loss 4.424167, Accuracy 89.755%\n",
      "Epoch 36, Batch 220, LR 0.000002 Loss 4.422332, Accuracy 89.766%\n",
      "Epoch 36, Batch 221, LR 0.000002 Loss 4.424957, Accuracy 89.748%\n",
      "Epoch 36, Batch 222, LR 0.000002 Loss 4.422353, Accuracy 89.759%\n",
      "Epoch 36, Batch 223, LR 0.000002 Loss 4.422419, Accuracy 89.746%\n",
      "Epoch 36, Batch 224, LR 0.000002 Loss 4.423320, Accuracy 89.736%\n",
      "Epoch 36, Batch 225, LR 0.000002 Loss 4.422680, Accuracy 89.733%\n",
      "Epoch 36, Batch 226, LR 0.000002 Loss 4.426165, Accuracy 89.712%\n",
      "Epoch 36, Batch 227, LR 0.000002 Loss 4.425482, Accuracy 89.720%\n",
      "Epoch 36, Batch 228, LR 0.000002 Loss 4.422918, Accuracy 89.731%\n",
      "Epoch 36, Batch 229, LR 0.000002 Loss 4.422036, Accuracy 89.738%\n",
      "Epoch 36, Batch 230, LR 0.000002 Loss 4.422925, Accuracy 89.738%\n",
      "Epoch 36, Batch 231, LR 0.000002 Loss 4.421684, Accuracy 89.752%\n",
      "Epoch 36, Batch 232, LR 0.000002 Loss 4.421107, Accuracy 89.756%\n",
      "Epoch 36, Batch 233, LR 0.000002 Loss 4.418611, Accuracy 89.763%\n",
      "Epoch 36, Batch 234, LR 0.000002 Loss 4.416121, Accuracy 89.780%\n",
      "Epoch 36, Batch 235, LR 0.000002 Loss 4.412899, Accuracy 89.787%\n",
      "Epoch 36, Batch 236, LR 0.000002 Loss 4.413810, Accuracy 89.787%\n",
      "Epoch 36, Batch 237, LR 0.000002 Loss 4.417335, Accuracy 89.775%\n",
      "Epoch 36, Batch 238, LR 0.000002 Loss 4.419097, Accuracy 89.772%\n",
      "Epoch 36, Batch 239, LR 0.000002 Loss 4.416440, Accuracy 89.785%\n",
      "Epoch 36, Batch 240, LR 0.000002 Loss 4.418709, Accuracy 89.759%\n",
      "Epoch 36, Batch 241, LR 0.000002 Loss 4.417388, Accuracy 89.772%\n",
      "Epoch 36, Batch 242, LR 0.000002 Loss 4.419131, Accuracy 89.773%\n",
      "Epoch 36, Batch 243, LR 0.000002 Loss 4.420118, Accuracy 89.773%\n",
      "Epoch 36, Batch 244, LR 0.000002 Loss 4.419034, Accuracy 89.786%\n",
      "Epoch 36, Batch 245, LR 0.000002 Loss 4.416561, Accuracy 89.812%\n",
      "Epoch 36, Batch 246, LR 0.000002 Loss 4.417085, Accuracy 89.815%\n",
      "Epoch 36, Batch 247, LR 0.000002 Loss 4.414335, Accuracy 89.834%\n",
      "Epoch 36, Batch 248, LR 0.000002 Loss 4.411915, Accuracy 89.844%\n",
      "Epoch 36, Batch 249, LR 0.000002 Loss 4.411939, Accuracy 89.837%\n",
      "Epoch 36, Batch 250, LR 0.000002 Loss 4.411596, Accuracy 89.834%\n",
      "Epoch 36, Batch 251, LR 0.000002 Loss 4.412102, Accuracy 89.838%\n",
      "Epoch 36, Batch 252, LR 0.000002 Loss 4.413226, Accuracy 89.834%\n",
      "Epoch 36, Batch 253, LR 0.000002 Loss 4.414353, Accuracy 89.828%\n",
      "Epoch 36, Batch 254, LR 0.000002 Loss 4.416384, Accuracy 89.816%\n",
      "Epoch 36, Batch 255, LR 0.000002 Loss 4.416476, Accuracy 89.810%\n",
      "Epoch 36, Batch 256, LR 0.000002 Loss 4.413599, Accuracy 89.825%\n",
      "Epoch 36, Batch 257, LR 0.000002 Loss 4.414066, Accuracy 89.813%\n",
      "Epoch 36, Batch 258, LR 0.000002 Loss 4.416792, Accuracy 89.789%\n",
      "Epoch 36, Batch 259, LR 0.000002 Loss 4.412585, Accuracy 89.814%\n",
      "Epoch 36, Batch 260, LR 0.000002 Loss 4.414984, Accuracy 89.808%\n",
      "Epoch 36, Batch 261, LR 0.000002 Loss 4.411362, Accuracy 89.823%\n",
      "Epoch 36, Batch 262, LR 0.000002 Loss 4.410410, Accuracy 89.829%\n",
      "Epoch 36, Batch 263, LR 0.000002 Loss 4.413152, Accuracy 89.811%\n",
      "Epoch 36, Batch 264, LR 0.000002 Loss 4.412095, Accuracy 89.805%\n",
      "Epoch 36, Batch 265, LR 0.000002 Loss 4.411837, Accuracy 89.800%\n",
      "Epoch 36, Batch 266, LR 0.000002 Loss 4.411735, Accuracy 89.806%\n",
      "Epoch 36, Batch 267, LR 0.000002 Loss 4.413779, Accuracy 89.800%\n",
      "Epoch 36, Batch 268, LR 0.000002 Loss 4.412800, Accuracy 89.803%\n",
      "Epoch 36, Batch 269, LR 0.000002 Loss 4.414805, Accuracy 89.794%\n",
      "Epoch 36, Batch 270, LR 0.000002 Loss 4.414818, Accuracy 89.783%\n",
      "Epoch 36, Batch 271, LR 0.000002 Loss 4.413502, Accuracy 89.789%\n",
      "Epoch 36, Batch 272, LR 0.000002 Loss 4.411008, Accuracy 89.804%\n",
      "Epoch 36, Batch 273, LR 0.000002 Loss 4.409917, Accuracy 89.812%\n",
      "Epoch 36, Batch 274, LR 0.000002 Loss 4.411798, Accuracy 89.795%\n",
      "Epoch 36, Batch 275, LR 0.000002 Loss 4.411620, Accuracy 89.795%\n",
      "Epoch 36, Batch 276, LR 0.000002 Loss 4.414171, Accuracy 89.793%\n",
      "Epoch 36, Batch 277, LR 0.000002 Loss 4.416293, Accuracy 89.793%\n",
      "Epoch 36, Batch 278, LR 0.000002 Loss 4.413045, Accuracy 89.810%\n",
      "Epoch 36, Batch 279, LR 0.000002 Loss 4.411940, Accuracy 89.819%\n",
      "Epoch 36, Batch 280, LR 0.000002 Loss 4.413580, Accuracy 89.816%\n",
      "Epoch 36, Batch 281, LR 0.000002 Loss 4.417463, Accuracy 89.796%\n",
      "Epoch 36, Batch 282, LR 0.000002 Loss 4.416667, Accuracy 89.797%\n",
      "Epoch 36, Batch 283, LR 0.000002 Loss 4.418770, Accuracy 89.786%\n",
      "Epoch 36, Batch 284, LR 0.000002 Loss 4.417867, Accuracy 89.802%\n",
      "Epoch 36, Batch 285, LR 0.000002 Loss 4.418741, Accuracy 89.794%\n",
      "Epoch 36, Batch 286, LR 0.000002 Loss 4.418471, Accuracy 89.789%\n",
      "Epoch 36, Batch 287, LR 0.000002 Loss 4.421244, Accuracy 89.765%\n",
      "Epoch 36, Batch 288, LR 0.000002 Loss 4.423065, Accuracy 89.757%\n",
      "Epoch 36, Batch 289, LR 0.000002 Loss 4.420997, Accuracy 89.768%\n",
      "Epoch 36, Batch 290, LR 0.000002 Loss 4.421474, Accuracy 89.760%\n",
      "Epoch 36, Batch 291, LR 0.000002 Loss 4.419154, Accuracy 89.774%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 292, LR 0.000002 Loss 4.419956, Accuracy 89.766%\n",
      "Epoch 36, Batch 293, LR 0.000002 Loss 4.423411, Accuracy 89.753%\n",
      "Epoch 36, Batch 294, LR 0.000002 Loss 4.421368, Accuracy 89.753%\n",
      "Epoch 36, Batch 295, LR 0.000002 Loss 4.420269, Accuracy 89.743%\n",
      "Epoch 36, Batch 296, LR 0.000002 Loss 4.424001, Accuracy 89.725%\n",
      "Epoch 36, Batch 297, LR 0.000002 Loss 4.421923, Accuracy 89.736%\n",
      "Epoch 36, Batch 298, LR 0.000002 Loss 4.423738, Accuracy 89.734%\n",
      "Epoch 36, Batch 299, LR 0.000002 Loss 4.425266, Accuracy 89.734%\n",
      "Epoch 36, Batch 300, LR 0.000002 Loss 4.423513, Accuracy 89.750%\n",
      "Epoch 36, Batch 301, LR 0.000002 Loss 4.422407, Accuracy 89.761%\n",
      "Epoch 36, Batch 302, LR 0.000002 Loss 4.424209, Accuracy 89.753%\n",
      "Epoch 36, Batch 303, LR 0.000002 Loss 4.425150, Accuracy 89.743%\n",
      "Epoch 36, Batch 304, LR 0.000002 Loss 4.426677, Accuracy 89.738%\n",
      "Epoch 36, Batch 305, LR 0.000002 Loss 4.428303, Accuracy 89.736%\n",
      "Epoch 36, Batch 306, LR 0.000002 Loss 4.426977, Accuracy 89.729%\n",
      "Epoch 36, Batch 307, LR 0.000002 Loss 4.427735, Accuracy 89.737%\n",
      "Epoch 36, Batch 308, LR 0.000002 Loss 4.428551, Accuracy 89.725%\n",
      "Epoch 36, Batch 309, LR 0.000002 Loss 4.428453, Accuracy 89.733%\n",
      "Epoch 36, Batch 310, LR 0.000002 Loss 4.428531, Accuracy 89.723%\n",
      "Epoch 36, Batch 311, LR 0.000002 Loss 4.430577, Accuracy 89.718%\n",
      "Epoch 36, Batch 312, LR 0.000002 Loss 4.428995, Accuracy 89.729%\n",
      "Epoch 36, Batch 313, LR 0.000002 Loss 4.428117, Accuracy 89.736%\n",
      "Epoch 36, Batch 314, LR 0.000002 Loss 4.427894, Accuracy 89.734%\n",
      "Epoch 36, Batch 315, LR 0.000002 Loss 4.427150, Accuracy 89.742%\n",
      "Epoch 36, Batch 316, LR 0.000002 Loss 4.428822, Accuracy 89.740%\n",
      "Epoch 36, Batch 317, LR 0.000002 Loss 4.431567, Accuracy 89.733%\n",
      "Epoch 36, Batch 318, LR 0.000002 Loss 4.430619, Accuracy 89.738%\n",
      "Epoch 36, Batch 319, LR 0.000002 Loss 4.432163, Accuracy 89.726%\n",
      "Epoch 36, Batch 320, LR 0.000002 Loss 4.433617, Accuracy 89.717%\n",
      "Epoch 36, Batch 321, LR 0.000002 Loss 4.431291, Accuracy 89.720%\n",
      "Epoch 36, Batch 322, LR 0.000002 Loss 4.431818, Accuracy 89.715%\n",
      "Epoch 36, Batch 323, LR 0.000002 Loss 4.430148, Accuracy 89.728%\n",
      "Epoch 36, Batch 324, LR 0.000002 Loss 4.428863, Accuracy 89.740%\n",
      "Epoch 36, Batch 325, LR 0.000002 Loss 4.430331, Accuracy 89.731%\n",
      "Epoch 36, Batch 326, LR 0.000002 Loss 4.431322, Accuracy 89.724%\n",
      "Epoch 36, Batch 327, LR 0.000002 Loss 4.430857, Accuracy 89.717%\n",
      "Epoch 36, Batch 328, LR 0.000002 Loss 4.431080, Accuracy 89.727%\n",
      "Epoch 36, Batch 329, LR 0.000002 Loss 4.430262, Accuracy 89.742%\n",
      "Epoch 36, Batch 330, LR 0.000002 Loss 4.429548, Accuracy 89.742%\n",
      "Epoch 36, Batch 331, LR 0.000002 Loss 4.428420, Accuracy 89.747%\n",
      "Epoch 36, Batch 332, LR 0.000002 Loss 4.427210, Accuracy 89.754%\n",
      "Epoch 36, Batch 333, LR 0.000002 Loss 4.427159, Accuracy 89.750%\n",
      "Epoch 36, Batch 334, LR 0.000002 Loss 4.425650, Accuracy 89.764%\n",
      "Epoch 36, Batch 335, LR 0.000002 Loss 4.428056, Accuracy 89.753%\n",
      "Epoch 36, Batch 336, LR 0.000002 Loss 4.426904, Accuracy 89.751%\n",
      "Epoch 36, Batch 337, LR 0.000002 Loss 4.426400, Accuracy 89.756%\n",
      "Epoch 36, Batch 338, LR 0.000002 Loss 4.427434, Accuracy 89.754%\n",
      "Epoch 36, Batch 339, LR 0.000002 Loss 4.427984, Accuracy 89.761%\n",
      "Epoch 36, Batch 340, LR 0.000002 Loss 4.427939, Accuracy 89.756%\n",
      "Epoch 36, Batch 341, LR 0.000002 Loss 4.427801, Accuracy 89.757%\n",
      "Epoch 36, Batch 342, LR 0.000002 Loss 4.428226, Accuracy 89.759%\n",
      "Epoch 36, Batch 343, LR 0.000002 Loss 4.429455, Accuracy 89.755%\n",
      "Epoch 36, Batch 344, LR 0.000002 Loss 4.428936, Accuracy 89.755%\n",
      "Epoch 36, Batch 345, LR 0.000002 Loss 4.430344, Accuracy 89.744%\n",
      "Epoch 36, Batch 346, LR 0.000002 Loss 4.430046, Accuracy 89.742%\n",
      "Epoch 36, Batch 347, LR 0.000002 Loss 4.432762, Accuracy 89.724%\n",
      "Epoch 36, Batch 348, LR 0.000002 Loss 4.432902, Accuracy 89.723%\n",
      "Epoch 36, Batch 349, LR 0.000002 Loss 4.435082, Accuracy 89.712%\n",
      "Epoch 36, Batch 350, LR 0.000002 Loss 4.434560, Accuracy 89.714%\n",
      "Epoch 36, Batch 351, LR 0.000002 Loss 4.434797, Accuracy 89.717%\n",
      "Epoch 36, Batch 352, LR 0.000002 Loss 4.433668, Accuracy 89.715%\n",
      "Epoch 36, Batch 353, LR 0.000002 Loss 4.438585, Accuracy 89.684%\n",
      "Epoch 36, Batch 354, LR 0.000002 Loss 4.439124, Accuracy 89.685%\n",
      "Epoch 36, Batch 355, LR 0.000002 Loss 4.441393, Accuracy 89.670%\n",
      "Epoch 36, Batch 356, LR 0.000002 Loss 4.441693, Accuracy 89.670%\n",
      "Epoch 36, Batch 357, LR 0.000002 Loss 4.441562, Accuracy 89.673%\n",
      "Epoch 36, Batch 358, LR 0.000002 Loss 4.441410, Accuracy 89.669%\n",
      "Epoch 36, Batch 359, LR 0.000002 Loss 4.441233, Accuracy 89.678%\n",
      "Epoch 36, Batch 360, LR 0.000002 Loss 4.441614, Accuracy 89.681%\n",
      "Epoch 36, Batch 361, LR 0.000002 Loss 4.441224, Accuracy 89.697%\n",
      "Epoch 36, Batch 362, LR 0.000002 Loss 4.437980, Accuracy 89.706%\n",
      "Epoch 36, Batch 363, LR 0.000002 Loss 4.436423, Accuracy 89.719%\n",
      "Epoch 36, Batch 364, LR 0.000002 Loss 4.434215, Accuracy 89.732%\n",
      "Epoch 36, Batch 365, LR 0.000002 Loss 4.434638, Accuracy 89.730%\n",
      "Epoch 36, Batch 366, LR 0.000002 Loss 4.433943, Accuracy 89.731%\n",
      "Epoch 36, Batch 367, LR 0.000002 Loss 4.435054, Accuracy 89.725%\n",
      "Epoch 36, Batch 368, LR 0.000002 Loss 4.436923, Accuracy 89.704%\n",
      "Epoch 36, Batch 369, LR 0.000002 Loss 4.436177, Accuracy 89.704%\n",
      "Epoch 36, Batch 370, LR 0.000002 Loss 4.436660, Accuracy 89.696%\n",
      "Epoch 36, Batch 371, LR 0.000002 Loss 4.433888, Accuracy 89.707%\n",
      "Epoch 36, Batch 372, LR 0.000002 Loss 4.435061, Accuracy 89.707%\n",
      "Epoch 36, Batch 373, LR 0.000002 Loss 4.434411, Accuracy 89.712%\n",
      "Epoch 36, Batch 374, LR 0.000002 Loss 4.435222, Accuracy 89.700%\n",
      "Epoch 36, Batch 375, LR 0.000002 Loss 4.435882, Accuracy 89.694%\n",
      "Epoch 36, Batch 376, LR 0.000002 Loss 4.434665, Accuracy 89.700%\n",
      "Epoch 36, Batch 377, LR 0.000002 Loss 4.435061, Accuracy 89.711%\n",
      "Epoch 36, Batch 378, LR 0.000002 Loss 4.436971, Accuracy 89.699%\n",
      "Epoch 36, Batch 379, LR 0.000002 Loss 4.436906, Accuracy 89.699%\n",
      "Epoch 36, Batch 380, LR 0.000002 Loss 4.434716, Accuracy 89.708%\n",
      "Epoch 36, Batch 381, LR 0.000002 Loss 4.433628, Accuracy 89.713%\n",
      "Epoch 36, Batch 382, LR 0.000002 Loss 4.434373, Accuracy 89.713%\n",
      "Epoch 36, Batch 383, LR 0.000002 Loss 4.434065, Accuracy 89.715%\n",
      "Epoch 36, Batch 384, LR 0.000002 Loss 4.434859, Accuracy 89.709%\n",
      "Epoch 36, Batch 385, LR 0.000002 Loss 4.434169, Accuracy 89.710%\n",
      "Epoch 36, Batch 386, LR 0.000002 Loss 4.433178, Accuracy 89.714%\n",
      "Epoch 36, Batch 387, LR 0.000002 Loss 4.431502, Accuracy 89.717%\n",
      "Epoch 36, Batch 388, LR 0.000002 Loss 4.430056, Accuracy 89.721%\n",
      "Epoch 36, Batch 389, LR 0.000002 Loss 4.433304, Accuracy 89.713%\n",
      "Epoch 36, Batch 390, LR 0.000002 Loss 4.432976, Accuracy 89.708%\n",
      "Epoch 36, Batch 391, LR 0.000002 Loss 4.432197, Accuracy 89.706%\n",
      "Epoch 36, Batch 392, LR 0.000002 Loss 4.431341, Accuracy 89.710%\n",
      "Epoch 36, Batch 393, LR 0.000002 Loss 4.431315, Accuracy 89.711%\n",
      "Epoch 36, Batch 394, LR 0.000002 Loss 4.429497, Accuracy 89.719%\n",
      "Epoch 36, Batch 395, LR 0.000002 Loss 4.428611, Accuracy 89.725%\n",
      "Epoch 36, Batch 396, LR 0.000002 Loss 4.429380, Accuracy 89.721%\n",
      "Epoch 36, Batch 397, LR 0.000002 Loss 4.430497, Accuracy 89.718%\n",
      "Epoch 36, Batch 398, LR 0.000002 Loss 4.428965, Accuracy 89.724%\n",
      "Epoch 36, Batch 399, LR 0.000002 Loss 4.429025, Accuracy 89.722%\n",
      "Epoch 36, Batch 400, LR 0.000002 Loss 4.429651, Accuracy 89.717%\n",
      "Epoch 36, Batch 401, LR 0.000002 Loss 4.431253, Accuracy 89.707%\n",
      "Epoch 36, Batch 402, LR 0.000002 Loss 4.429706, Accuracy 89.710%\n",
      "Epoch 36, Batch 403, LR 0.000002 Loss 4.429820, Accuracy 89.702%\n",
      "Epoch 36, Batch 404, LR 0.000002 Loss 4.429986, Accuracy 89.706%\n",
      "Epoch 36, Batch 405, LR 0.000002 Loss 4.429235, Accuracy 89.705%\n",
      "Epoch 36, Batch 406, LR 0.000002 Loss 4.428292, Accuracy 89.717%\n",
      "Epoch 36, Batch 407, LR 0.000002 Loss 4.429947, Accuracy 89.702%\n",
      "Epoch 36, Batch 408, LR 0.000002 Loss 4.432594, Accuracy 89.689%\n",
      "Epoch 36, Batch 409, LR 0.000002 Loss 4.431768, Accuracy 89.691%\n",
      "Epoch 36, Batch 410, LR 0.000002 Loss 4.432951, Accuracy 89.688%\n",
      "Epoch 36, Batch 411, LR 0.000002 Loss 4.433415, Accuracy 89.682%\n",
      "Epoch 36, Batch 412, LR 0.000002 Loss 4.433235, Accuracy 89.675%\n",
      "Epoch 36, Batch 413, LR 0.000002 Loss 4.434999, Accuracy 89.660%\n",
      "Epoch 36, Batch 414, LR 0.000002 Loss 4.435546, Accuracy 89.659%\n",
      "Epoch 36, Batch 415, LR 0.000002 Loss 4.435551, Accuracy 89.657%\n",
      "Epoch 36, Batch 416, LR 0.000002 Loss 4.434056, Accuracy 89.665%\n",
      "Epoch 36, Batch 417, LR 0.000002 Loss 4.434885, Accuracy 89.658%\n",
      "Epoch 36, Batch 418, LR 0.000002 Loss 4.437122, Accuracy 89.648%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 419, LR 0.000002 Loss 4.436123, Accuracy 89.657%\n",
      "Epoch 36, Batch 420, LR 0.000002 Loss 4.437825, Accuracy 89.650%\n",
      "Epoch 36, Batch 421, LR 0.000002 Loss 4.439312, Accuracy 89.641%\n",
      "Epoch 36, Batch 422, LR 0.000002 Loss 4.439785, Accuracy 89.635%\n",
      "Epoch 36, Batch 423, LR 0.000002 Loss 4.440497, Accuracy 89.641%\n",
      "Epoch 36, Batch 424, LR 0.000002 Loss 4.441259, Accuracy 89.639%\n",
      "Epoch 36, Batch 425, LR 0.000002 Loss 4.442420, Accuracy 89.636%\n",
      "Epoch 36, Batch 426, LR 0.000002 Loss 4.442197, Accuracy 89.640%\n",
      "Epoch 36, Batch 427, LR 0.000002 Loss 4.440399, Accuracy 89.648%\n",
      "Epoch 36, Batch 428, LR 0.000002 Loss 4.440282, Accuracy 89.647%\n",
      "Epoch 36, Batch 429, LR 0.000002 Loss 4.440211, Accuracy 89.647%\n",
      "Epoch 36, Batch 430, LR 0.000002 Loss 4.438344, Accuracy 89.649%\n",
      "Epoch 36, Batch 431, LR 0.000002 Loss 4.436628, Accuracy 89.662%\n",
      "Epoch 36, Batch 432, LR 0.000002 Loss 4.434711, Accuracy 89.676%\n",
      "Epoch 36, Batch 433, LR 0.000002 Loss 4.433095, Accuracy 89.678%\n",
      "Epoch 36, Batch 434, LR 0.000002 Loss 4.435955, Accuracy 89.666%\n",
      "Epoch 36, Batch 435, LR 0.000002 Loss 4.435109, Accuracy 89.668%\n",
      "Epoch 36, Batch 436, LR 0.000002 Loss 4.437370, Accuracy 89.659%\n",
      "Epoch 36, Batch 437, LR 0.000002 Loss 4.436871, Accuracy 89.660%\n",
      "Epoch 36, Batch 438, LR 0.000002 Loss 4.436054, Accuracy 89.662%\n",
      "Epoch 36, Batch 439, LR 0.000002 Loss 4.436500, Accuracy 89.657%\n",
      "Epoch 36, Batch 440, LR 0.000002 Loss 4.436901, Accuracy 89.656%\n",
      "Epoch 36, Batch 441, LR 0.000002 Loss 4.437438, Accuracy 89.656%\n",
      "Epoch 36, Batch 442, LR 0.000002 Loss 4.437605, Accuracy 89.660%\n",
      "Epoch 36, Batch 443, LR 0.000002 Loss 4.438064, Accuracy 89.666%\n",
      "Epoch 36, Batch 444, LR 0.000002 Loss 4.436704, Accuracy 89.675%\n",
      "Epoch 36, Batch 445, LR 0.000002 Loss 4.435157, Accuracy 89.684%\n",
      "Epoch 36, Batch 446, LR 0.000002 Loss 4.435720, Accuracy 89.681%\n",
      "Epoch 36, Batch 447, LR 0.000002 Loss 4.434621, Accuracy 89.683%\n",
      "Epoch 36, Batch 448, LR 0.000002 Loss 4.434550, Accuracy 89.687%\n",
      "Epoch 36, Batch 449, LR 0.000002 Loss 4.436041, Accuracy 89.680%\n",
      "Epoch 36, Batch 450, LR 0.000002 Loss 4.435734, Accuracy 89.674%\n",
      "Epoch 36, Batch 451, LR 0.000002 Loss 4.435504, Accuracy 89.679%\n",
      "Epoch 36, Batch 452, LR 0.000002 Loss 4.435907, Accuracy 89.676%\n",
      "Epoch 36, Batch 453, LR 0.000002 Loss 4.435804, Accuracy 89.678%\n",
      "Epoch 36, Batch 454, LR 0.000002 Loss 4.434747, Accuracy 89.685%\n",
      "Epoch 36, Batch 455, LR 0.000002 Loss 4.435405, Accuracy 89.686%\n",
      "Epoch 36, Batch 456, LR 0.000002 Loss 4.434644, Accuracy 89.696%\n",
      "Epoch 36, Batch 457, LR 0.000002 Loss 4.433355, Accuracy 89.698%\n",
      "Epoch 36, Batch 458, LR 0.000002 Loss 4.431894, Accuracy 89.702%\n",
      "Epoch 36, Batch 459, LR 0.000002 Loss 4.433386, Accuracy 89.697%\n",
      "Epoch 36, Batch 460, LR 0.000002 Loss 4.432718, Accuracy 89.701%\n",
      "Epoch 36, Batch 461, LR 0.000002 Loss 4.432100, Accuracy 89.713%\n",
      "Epoch 36, Batch 462, LR 0.000002 Loss 4.432397, Accuracy 89.710%\n",
      "Epoch 36, Batch 463, LR 0.000002 Loss 4.432084, Accuracy 89.709%\n",
      "Epoch 36, Batch 464, LR 0.000002 Loss 4.432949, Accuracy 89.696%\n",
      "Epoch 36, Batch 465, LR 0.000002 Loss 4.432642, Accuracy 89.699%\n",
      "Epoch 36, Batch 466, LR 0.000002 Loss 4.433780, Accuracy 89.696%\n",
      "Epoch 36, Batch 467, LR 0.000002 Loss 4.434709, Accuracy 89.692%\n",
      "Epoch 36, Batch 468, LR 0.000002 Loss 4.434377, Accuracy 89.690%\n",
      "Epoch 36, Batch 469, LR 0.000002 Loss 4.433938, Accuracy 89.690%\n",
      "Epoch 36, Batch 470, LR 0.000002 Loss 4.434050, Accuracy 89.689%\n",
      "Epoch 36, Batch 471, LR 0.000002 Loss 4.434442, Accuracy 89.685%\n",
      "Epoch 36, Batch 472, LR 0.000002 Loss 4.435395, Accuracy 89.687%\n",
      "Epoch 36, Batch 473, LR 0.000002 Loss 4.435749, Accuracy 89.690%\n",
      "Epoch 36, Batch 474, LR 0.000002 Loss 4.435499, Accuracy 89.695%\n",
      "Epoch 36, Batch 475, LR 0.000002 Loss 4.435464, Accuracy 89.702%\n",
      "Epoch 36, Batch 476, LR 0.000002 Loss 4.435805, Accuracy 89.699%\n",
      "Epoch 36, Batch 477, LR 0.000002 Loss 4.434832, Accuracy 89.701%\n",
      "Epoch 36, Batch 478, LR 0.000002 Loss 4.433607, Accuracy 89.705%\n",
      "Epoch 36, Batch 479, LR 0.000002 Loss 4.433986, Accuracy 89.702%\n",
      "Epoch 36, Batch 480, LR 0.000002 Loss 4.435049, Accuracy 89.707%\n",
      "Epoch 36, Batch 481, LR 0.000002 Loss 4.436415, Accuracy 89.707%\n",
      "Epoch 36, Batch 482, LR 0.000002 Loss 4.436394, Accuracy 89.712%\n",
      "Epoch 36, Batch 483, LR 0.000002 Loss 4.436549, Accuracy 89.708%\n",
      "Epoch 36, Batch 484, LR 0.000002 Loss 4.435587, Accuracy 89.713%\n",
      "Epoch 36, Batch 485, LR 0.000002 Loss 4.436678, Accuracy 89.712%\n",
      "Epoch 36, Batch 486, LR 0.000002 Loss 4.437278, Accuracy 89.717%\n",
      "Epoch 36, Batch 487, LR 0.000002 Loss 4.437184, Accuracy 89.719%\n",
      "Epoch 36, Batch 488, LR 0.000002 Loss 4.437527, Accuracy 89.722%\n",
      "Epoch 36, Batch 489, LR 0.000002 Loss 4.437940, Accuracy 89.719%\n",
      "Epoch 36, Batch 490, LR 0.000002 Loss 4.437118, Accuracy 89.727%\n",
      "Epoch 36, Batch 491, LR 0.000002 Loss 4.436523, Accuracy 89.723%\n",
      "Epoch 36, Batch 492, LR 0.000002 Loss 4.435221, Accuracy 89.731%\n",
      "Epoch 36, Batch 493, LR 0.000002 Loss 4.434146, Accuracy 89.731%\n",
      "Epoch 36, Batch 494, LR 0.000002 Loss 4.434006, Accuracy 89.730%\n",
      "Epoch 36, Batch 495, LR 0.000002 Loss 4.434494, Accuracy 89.724%\n",
      "Epoch 36, Batch 496, LR 0.000002 Loss 4.434824, Accuracy 89.716%\n",
      "Epoch 36, Batch 497, LR 0.000002 Loss 4.435025, Accuracy 89.709%\n",
      "Epoch 36, Batch 498, LR 0.000002 Loss 4.435928, Accuracy 89.707%\n",
      "Epoch 36, Batch 499, LR 0.000002 Loss 4.435683, Accuracy 89.708%\n",
      "Epoch 36, Batch 500, LR 0.000002 Loss 4.436659, Accuracy 89.705%\n",
      "Epoch 36, Batch 501, LR 0.000002 Loss 4.435266, Accuracy 89.705%\n",
      "Epoch 36, Batch 502, LR 0.000002 Loss 4.434386, Accuracy 89.711%\n",
      "Epoch 36, Batch 503, LR 0.000002 Loss 4.434821, Accuracy 89.704%\n",
      "Epoch 36, Batch 504, LR 0.000002 Loss 4.433872, Accuracy 89.712%\n",
      "Epoch 36, Batch 505, LR 0.000002 Loss 4.434630, Accuracy 89.709%\n",
      "Epoch 36, Batch 506, LR 0.000002 Loss 4.435812, Accuracy 89.708%\n",
      "Epoch 36, Batch 507, LR 0.000002 Loss 4.434614, Accuracy 89.704%\n",
      "Epoch 36, Batch 508, LR 0.000002 Loss 4.434984, Accuracy 89.708%\n",
      "Epoch 36, Batch 509, LR 0.000002 Loss 4.436556, Accuracy 89.701%\n",
      "Epoch 36, Batch 510, LR 0.000002 Loss 4.435463, Accuracy 89.707%\n",
      "Epoch 36, Batch 511, LR 0.000002 Loss 4.434224, Accuracy 89.720%\n",
      "Epoch 36, Batch 512, LR 0.000002 Loss 4.436280, Accuracy 89.713%\n",
      "Epoch 36, Batch 513, LR 0.000002 Loss 4.434915, Accuracy 89.719%\n",
      "Epoch 36, Batch 514, LR 0.000002 Loss 4.435114, Accuracy 89.713%\n",
      "Epoch 36, Batch 515, LR 0.000002 Loss 4.434027, Accuracy 89.722%\n",
      "Epoch 36, Batch 516, LR 0.000002 Loss 4.434189, Accuracy 89.720%\n",
      "Epoch 36, Batch 517, LR 0.000002 Loss 4.434529, Accuracy 89.721%\n",
      "Epoch 36, Batch 518, LR 0.000002 Loss 4.434839, Accuracy 89.725%\n",
      "Epoch 36, Batch 519, LR 0.000002 Loss 4.435894, Accuracy 89.722%\n",
      "Epoch 36, Batch 520, LR 0.000002 Loss 4.435317, Accuracy 89.724%\n",
      "Epoch 36, Batch 521, LR 0.000002 Loss 4.435276, Accuracy 89.722%\n",
      "Epoch 36, Batch 522, LR 0.000002 Loss 4.435041, Accuracy 89.721%\n",
      "Epoch 36, Batch 523, LR 0.000002 Loss 4.434618, Accuracy 89.718%\n",
      "Epoch 36, Batch 524, LR 0.000002 Loss 4.434048, Accuracy 89.721%\n",
      "Epoch 36, Batch 525, LR 0.000002 Loss 4.432389, Accuracy 89.726%\n",
      "Epoch 36, Batch 526, LR 0.000002 Loss 4.432732, Accuracy 89.723%\n",
      "Epoch 36, Batch 527, LR 0.000002 Loss 4.433187, Accuracy 89.721%\n",
      "Epoch 36, Batch 528, LR 0.000002 Loss 4.432960, Accuracy 89.721%\n",
      "Epoch 36, Batch 529, LR 0.000002 Loss 4.434046, Accuracy 89.712%\n",
      "Epoch 36, Batch 530, LR 0.000002 Loss 4.434196, Accuracy 89.720%\n",
      "Epoch 36, Batch 531, LR 0.000002 Loss 4.435049, Accuracy 89.710%\n",
      "Epoch 36, Batch 532, LR 0.000002 Loss 4.435650, Accuracy 89.709%\n",
      "Epoch 36, Batch 533, LR 0.000002 Loss 4.437595, Accuracy 89.697%\n",
      "Epoch 36, Batch 534, LR 0.000002 Loss 4.437591, Accuracy 89.692%\n",
      "Epoch 36, Batch 535, LR 0.000002 Loss 4.438349, Accuracy 89.686%\n",
      "Epoch 36, Batch 536, LR 0.000002 Loss 4.439098, Accuracy 89.683%\n",
      "Epoch 36, Batch 537, LR 0.000002 Loss 4.438617, Accuracy 89.688%\n",
      "Epoch 36, Batch 538, LR 0.000002 Loss 4.439671, Accuracy 89.688%\n",
      "Epoch 36, Batch 539, LR 0.000002 Loss 4.438155, Accuracy 89.697%\n",
      "Epoch 36, Batch 540, LR 0.000002 Loss 4.437583, Accuracy 89.696%\n",
      "Epoch 36, Batch 541, LR 0.000002 Loss 4.437433, Accuracy 89.695%\n",
      "Epoch 36, Batch 542, LR 0.000002 Loss 4.437725, Accuracy 89.694%\n",
      "Epoch 36, Batch 543, LR 0.000002 Loss 4.436278, Accuracy 89.700%\n",
      "Epoch 36, Batch 544, LR 0.000002 Loss 4.436691, Accuracy 89.700%\n",
      "Epoch 36, Batch 545, LR 0.000002 Loss 4.436071, Accuracy 89.702%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 546, LR 0.000002 Loss 4.436831, Accuracy 89.696%\n",
      "Epoch 36, Batch 547, LR 0.000002 Loss 4.435912, Accuracy 89.699%\n",
      "Epoch 36, Batch 548, LR 0.000002 Loss 4.435268, Accuracy 89.703%\n",
      "Epoch 36, Batch 549, LR 0.000002 Loss 4.436811, Accuracy 89.696%\n",
      "Epoch 36, Batch 550, LR 0.000002 Loss 4.437533, Accuracy 89.690%\n",
      "Epoch 36, Batch 551, LR 0.000002 Loss 4.437536, Accuracy 89.693%\n",
      "Epoch 36, Batch 552, LR 0.000002 Loss 4.437962, Accuracy 89.694%\n",
      "Epoch 36, Batch 553, LR 0.000002 Loss 4.437241, Accuracy 89.701%\n",
      "Epoch 36, Batch 554, LR 0.000002 Loss 4.437758, Accuracy 89.701%\n",
      "Epoch 36, Batch 555, LR 0.000002 Loss 4.438017, Accuracy 89.703%\n",
      "Epoch 36, Batch 556, LR 0.000002 Loss 4.438233, Accuracy 89.702%\n",
      "Epoch 36, Batch 557, LR 0.000002 Loss 4.437182, Accuracy 89.711%\n",
      "Epoch 36, Batch 558, LR 0.000002 Loss 4.435877, Accuracy 89.712%\n",
      "Epoch 36, Batch 559, LR 0.000002 Loss 4.435642, Accuracy 89.714%\n",
      "Epoch 36, Batch 560, LR 0.000002 Loss 4.436542, Accuracy 89.708%\n",
      "Epoch 36, Batch 561, LR 0.000002 Loss 4.436375, Accuracy 89.713%\n",
      "Epoch 36, Batch 562, LR 0.000002 Loss 4.436860, Accuracy 89.713%\n",
      "Epoch 36, Batch 563, LR 0.000002 Loss 4.436239, Accuracy 89.719%\n",
      "Epoch 36, Batch 564, LR 0.000002 Loss 4.435177, Accuracy 89.723%\n",
      "Epoch 36, Batch 565, LR 0.000002 Loss 4.434430, Accuracy 89.725%\n",
      "Epoch 36, Batch 566, LR 0.000002 Loss 4.435004, Accuracy 89.728%\n",
      "Epoch 36, Batch 567, LR 0.000002 Loss 4.434057, Accuracy 89.731%\n",
      "Epoch 36, Batch 568, LR 0.000002 Loss 4.434244, Accuracy 89.734%\n",
      "Epoch 36, Batch 569, LR 0.000002 Loss 4.434305, Accuracy 89.739%\n",
      "Epoch 36, Batch 570, LR 0.000002 Loss 4.435625, Accuracy 89.731%\n",
      "Epoch 36, Batch 571, LR 0.000002 Loss 4.435609, Accuracy 89.730%\n",
      "Epoch 36, Batch 572, LR 0.000002 Loss 4.436056, Accuracy 89.729%\n",
      "Epoch 36, Batch 573, LR 0.000002 Loss 4.436579, Accuracy 89.725%\n",
      "Epoch 36, Batch 574, LR 0.000002 Loss 4.435973, Accuracy 89.729%\n",
      "Epoch 36, Batch 575, LR 0.000002 Loss 4.435459, Accuracy 89.731%\n",
      "Epoch 36, Batch 576, LR 0.000002 Loss 4.435478, Accuracy 89.727%\n",
      "Epoch 36, Batch 577, LR 0.000002 Loss 4.436548, Accuracy 89.723%\n",
      "Epoch 36, Batch 578, LR 0.000002 Loss 4.436012, Accuracy 89.729%\n",
      "Epoch 36, Batch 579, LR 0.000002 Loss 4.436857, Accuracy 89.725%\n",
      "Epoch 36, Batch 580, LR 0.000002 Loss 4.435475, Accuracy 89.728%\n",
      "Epoch 36, Batch 581, LR 0.000002 Loss 4.435174, Accuracy 89.728%\n",
      "Epoch 36, Batch 582, LR 0.000002 Loss 4.434745, Accuracy 89.727%\n",
      "Epoch 36, Batch 583, LR 0.000002 Loss 4.434907, Accuracy 89.729%\n",
      "Epoch 36, Batch 584, LR 0.000002 Loss 4.434911, Accuracy 89.733%\n",
      "Epoch 36, Batch 585, LR 0.000002 Loss 4.434425, Accuracy 89.737%\n",
      "Epoch 36, Batch 586, LR 0.000002 Loss 4.434494, Accuracy 89.736%\n",
      "Epoch 36, Batch 587, LR 0.000002 Loss 4.433993, Accuracy 89.743%\n",
      "Epoch 36, Batch 588, LR 0.000002 Loss 4.433285, Accuracy 89.745%\n",
      "Epoch 36, Batch 589, LR 0.000002 Loss 4.433706, Accuracy 89.744%\n",
      "Epoch 36, Batch 590, LR 0.000002 Loss 4.433592, Accuracy 89.747%\n",
      "Epoch 36, Batch 591, LR 0.000002 Loss 4.433688, Accuracy 89.747%\n",
      "Epoch 36, Batch 592, LR 0.000002 Loss 4.433182, Accuracy 89.753%\n",
      "Epoch 36, Batch 593, LR 0.000002 Loss 4.432839, Accuracy 89.754%\n",
      "Epoch 36, Batch 594, LR 0.000002 Loss 4.433137, Accuracy 89.754%\n",
      "Epoch 36, Batch 595, LR 0.000002 Loss 4.432422, Accuracy 89.758%\n",
      "Epoch 36, Batch 596, LR 0.000002 Loss 4.432999, Accuracy 89.752%\n",
      "Epoch 36, Batch 597, LR 0.000002 Loss 4.432795, Accuracy 89.751%\n",
      "Epoch 36, Batch 598, LR 0.000002 Loss 4.432437, Accuracy 89.751%\n",
      "Epoch 36, Batch 599, LR 0.000002 Loss 4.432608, Accuracy 89.752%\n",
      "Epoch 36, Batch 600, LR 0.000002 Loss 4.433979, Accuracy 89.743%\n",
      "Epoch 36, Batch 601, LR 0.000002 Loss 4.433078, Accuracy 89.749%\n",
      "Epoch 36, Batch 602, LR 0.000002 Loss 4.434381, Accuracy 89.740%\n",
      "Epoch 36, Batch 603, LR 0.000002 Loss 4.434313, Accuracy 89.745%\n",
      "Epoch 36, Batch 604, LR 0.000002 Loss 4.435334, Accuracy 89.740%\n",
      "Epoch 36, Batch 605, LR 0.000002 Loss 4.435121, Accuracy 89.746%\n",
      "Epoch 36, Batch 606, LR 0.000002 Loss 4.435377, Accuracy 89.747%\n",
      "Epoch 36, Batch 607, LR 0.000002 Loss 4.436403, Accuracy 89.743%\n",
      "Epoch 36, Batch 608, LR 0.000002 Loss 4.436071, Accuracy 89.745%\n",
      "Epoch 36, Batch 609, LR 0.000002 Loss 4.435384, Accuracy 89.753%\n",
      "Epoch 36, Batch 610, LR 0.000002 Loss 4.436841, Accuracy 89.741%\n",
      "Epoch 36, Batch 611, LR 0.000002 Loss 4.437572, Accuracy 89.740%\n",
      "Epoch 36, Batch 612, LR 0.000002 Loss 4.436993, Accuracy 89.744%\n",
      "Epoch 36, Batch 613, LR 0.000002 Loss 4.438234, Accuracy 89.743%\n",
      "Epoch 36, Batch 614, LR 0.000002 Loss 4.437166, Accuracy 89.748%\n",
      "Epoch 36, Batch 615, LR 0.000002 Loss 4.438239, Accuracy 89.746%\n",
      "Epoch 36, Batch 616, LR 0.000002 Loss 4.438157, Accuracy 89.746%\n",
      "Epoch 36, Batch 617, LR 0.000002 Loss 4.438961, Accuracy 89.742%\n",
      "Epoch 36, Batch 618, LR 0.000002 Loss 4.439744, Accuracy 89.739%\n",
      "Epoch 36, Batch 619, LR 0.000002 Loss 4.440396, Accuracy 89.740%\n",
      "Epoch 36, Batch 620, LR 0.000002 Loss 4.439176, Accuracy 89.745%\n",
      "Epoch 36, Batch 621, LR 0.000002 Loss 4.439850, Accuracy 89.744%\n",
      "Epoch 36, Batch 622, LR 0.000002 Loss 4.439539, Accuracy 89.750%\n",
      "Epoch 36, Batch 623, LR 0.000002 Loss 4.440085, Accuracy 89.747%\n",
      "Epoch 36, Batch 624, LR 0.000002 Loss 4.439870, Accuracy 89.747%\n",
      "Epoch 36, Batch 625, LR 0.000002 Loss 4.439353, Accuracy 89.754%\n",
      "Epoch 36, Batch 626, LR 0.000002 Loss 4.439675, Accuracy 89.749%\n",
      "Epoch 36, Batch 627, LR 0.000002 Loss 4.440271, Accuracy 89.752%\n",
      "Epoch 36, Batch 628, LR 0.000002 Loss 4.442493, Accuracy 89.747%\n",
      "Epoch 36, Batch 629, LR 0.000002 Loss 4.443297, Accuracy 89.742%\n",
      "Epoch 36, Batch 630, LR 0.000002 Loss 4.443053, Accuracy 89.740%\n",
      "Epoch 36, Batch 631, LR 0.000002 Loss 4.443851, Accuracy 89.729%\n",
      "Epoch 36, Batch 632, LR 0.000002 Loss 4.443712, Accuracy 89.731%\n",
      "Epoch 36, Batch 633, LR 0.000002 Loss 4.443199, Accuracy 89.733%\n",
      "Epoch 36, Batch 634, LR 0.000002 Loss 4.442644, Accuracy 89.738%\n",
      "Epoch 36, Batch 635, LR 0.000002 Loss 4.443588, Accuracy 89.737%\n",
      "Epoch 36, Batch 636, LR 0.000002 Loss 4.444869, Accuracy 89.733%\n",
      "Epoch 36, Batch 637, LR 0.000002 Loss 4.444638, Accuracy 89.732%\n",
      "Epoch 36, Batch 638, LR 0.000002 Loss 4.445763, Accuracy 89.731%\n",
      "Epoch 36, Batch 639, LR 0.000002 Loss 4.446494, Accuracy 89.728%\n",
      "Epoch 36, Batch 640, LR 0.000002 Loss 4.446404, Accuracy 89.727%\n",
      "Epoch 36, Batch 641, LR 0.000002 Loss 4.446287, Accuracy 89.728%\n",
      "Epoch 36, Batch 642, LR 0.000002 Loss 4.446406, Accuracy 89.726%\n",
      "Epoch 36, Batch 643, LR 0.000002 Loss 4.446372, Accuracy 89.728%\n",
      "Epoch 36, Batch 644, LR 0.000002 Loss 4.445720, Accuracy 89.733%\n",
      "Epoch 36, Batch 645, LR 0.000002 Loss 4.445701, Accuracy 89.732%\n",
      "Epoch 36, Batch 646, LR 0.000002 Loss 4.445192, Accuracy 89.732%\n",
      "Epoch 36, Batch 647, LR 0.000002 Loss 4.443937, Accuracy 89.740%\n",
      "Epoch 36, Batch 648, LR 0.000002 Loss 4.444937, Accuracy 89.736%\n",
      "Epoch 36, Batch 649, LR 0.000002 Loss 4.444578, Accuracy 89.738%\n",
      "Epoch 36, Batch 650, LR 0.000002 Loss 4.444638, Accuracy 89.736%\n",
      "Epoch 36, Batch 651, LR 0.000002 Loss 4.444376, Accuracy 89.730%\n",
      "Epoch 36, Batch 652, LR 0.000002 Loss 4.445464, Accuracy 89.726%\n",
      "Epoch 36, Batch 653, LR 0.000002 Loss 4.446042, Accuracy 89.723%\n",
      "Epoch 36, Batch 654, LR 0.000002 Loss 4.445638, Accuracy 89.723%\n",
      "Epoch 36, Batch 655, LR 0.000002 Loss 4.445662, Accuracy 89.722%\n",
      "Epoch 36, Batch 656, LR 0.000002 Loss 4.446123, Accuracy 89.716%\n",
      "Epoch 36, Batch 657, LR 0.000002 Loss 4.445597, Accuracy 89.718%\n",
      "Epoch 36, Batch 658, LR 0.000002 Loss 4.445395, Accuracy 89.719%\n",
      "Epoch 36, Batch 659, LR 0.000002 Loss 4.445986, Accuracy 89.719%\n",
      "Epoch 36, Batch 660, LR 0.000002 Loss 4.446355, Accuracy 89.721%\n",
      "Epoch 36, Batch 661, LR 0.000002 Loss 4.446621, Accuracy 89.720%\n",
      "Epoch 36, Batch 662, LR 0.000002 Loss 4.445928, Accuracy 89.722%\n",
      "Epoch 36, Batch 663, LR 0.000002 Loss 4.446135, Accuracy 89.713%\n",
      "Epoch 36, Batch 664, LR 0.000002 Loss 4.447201, Accuracy 89.710%\n",
      "Epoch 36, Batch 665, LR 0.000002 Loss 4.447068, Accuracy 89.706%\n",
      "Epoch 36, Batch 666, LR 0.000002 Loss 4.446120, Accuracy 89.717%\n",
      "Epoch 36, Batch 667, LR 0.000002 Loss 4.446626, Accuracy 89.715%\n",
      "Epoch 36, Batch 668, LR 0.000002 Loss 4.446070, Accuracy 89.710%\n",
      "Epoch 36, Batch 669, LR 0.000002 Loss 4.446141, Accuracy 89.711%\n",
      "Epoch 36, Batch 670, LR 0.000002 Loss 4.445399, Accuracy 89.714%\n",
      "Epoch 36, Batch 671, LR 0.000002 Loss 4.446257, Accuracy 89.711%\n",
      "Epoch 36, Batch 672, LR 0.000002 Loss 4.445579, Accuracy 89.716%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 673, LR 0.000002 Loss 4.445505, Accuracy 89.717%\n",
      "Epoch 36, Batch 674, LR 0.000002 Loss 4.445858, Accuracy 89.720%\n",
      "Epoch 36, Batch 675, LR 0.000002 Loss 4.445504, Accuracy 89.721%\n",
      "Epoch 36, Batch 676, LR 0.000002 Loss 4.445753, Accuracy 89.725%\n",
      "Epoch 36, Batch 677, LR 0.000002 Loss 4.446446, Accuracy 89.719%\n",
      "Epoch 36, Batch 678, LR 0.000002 Loss 4.447260, Accuracy 89.712%\n",
      "Epoch 36, Batch 679, LR 0.000002 Loss 4.446814, Accuracy 89.709%\n",
      "Epoch 36, Batch 680, LR 0.000002 Loss 4.445926, Accuracy 89.709%\n",
      "Epoch 36, Batch 681, LR 0.000002 Loss 4.444448, Accuracy 89.715%\n",
      "Epoch 36, Batch 682, LR 0.000002 Loss 4.443251, Accuracy 89.723%\n",
      "Epoch 36, Batch 683, LR 0.000002 Loss 4.443725, Accuracy 89.725%\n",
      "Epoch 36, Batch 684, LR 0.000002 Loss 4.443444, Accuracy 89.727%\n",
      "Epoch 36, Batch 685, LR 0.000002 Loss 4.443221, Accuracy 89.730%\n",
      "Epoch 36, Batch 686, LR 0.000002 Loss 4.442928, Accuracy 89.731%\n",
      "Epoch 36, Batch 687, LR 0.000002 Loss 4.442914, Accuracy 89.730%\n",
      "Epoch 36, Batch 688, LR 0.000002 Loss 4.443017, Accuracy 89.732%\n",
      "Epoch 36, Batch 689, LR 0.000002 Loss 4.442097, Accuracy 89.736%\n",
      "Epoch 36, Batch 690, LR 0.000002 Loss 4.442376, Accuracy 89.736%\n",
      "Epoch 36, Batch 691, LR 0.000002 Loss 4.443007, Accuracy 89.737%\n",
      "Epoch 36, Batch 692, LR 0.000002 Loss 4.441719, Accuracy 89.746%\n",
      "Epoch 36, Batch 693, LR 0.000002 Loss 4.441640, Accuracy 89.742%\n",
      "Epoch 36, Batch 694, LR 0.000002 Loss 4.441556, Accuracy 89.742%\n",
      "Epoch 36, Batch 695, LR 0.000002 Loss 4.441431, Accuracy 89.747%\n",
      "Epoch 36, Batch 696, LR 0.000002 Loss 4.441634, Accuracy 89.745%\n",
      "Epoch 36, Batch 697, LR 0.000002 Loss 4.442537, Accuracy 89.744%\n",
      "Epoch 36, Batch 698, LR 0.000002 Loss 4.441803, Accuracy 89.747%\n",
      "Epoch 36, Batch 699, LR 0.000002 Loss 4.441749, Accuracy 89.747%\n",
      "Epoch 36, Batch 700, LR 0.000002 Loss 4.443138, Accuracy 89.740%\n",
      "Epoch 36, Batch 701, LR 0.000002 Loss 4.443626, Accuracy 89.737%\n",
      "Epoch 36, Batch 702, LR 0.000002 Loss 4.444988, Accuracy 89.732%\n",
      "Epoch 36, Batch 703, LR 0.000002 Loss 4.444725, Accuracy 89.735%\n",
      "Epoch 36, Batch 704, LR 0.000002 Loss 4.443749, Accuracy 89.737%\n",
      "Epoch 36, Batch 705, LR 0.000002 Loss 4.443117, Accuracy 89.738%\n",
      "Epoch 36, Batch 706, LR 0.000002 Loss 4.442945, Accuracy 89.738%\n",
      "Epoch 36, Batch 707, LR 0.000002 Loss 4.443190, Accuracy 89.738%\n",
      "Epoch 36, Batch 708, LR 0.000002 Loss 4.443262, Accuracy 89.736%\n",
      "Epoch 36, Batch 709, LR 0.000002 Loss 4.443792, Accuracy 89.730%\n",
      "Epoch 36, Batch 710, LR 0.000002 Loss 4.442361, Accuracy 89.736%\n",
      "Epoch 36, Batch 711, LR 0.000002 Loss 4.442545, Accuracy 89.737%\n",
      "Epoch 36, Batch 712, LR 0.000002 Loss 4.442353, Accuracy 89.743%\n",
      "Epoch 36, Batch 713, LR 0.000002 Loss 4.441912, Accuracy 89.744%\n",
      "Epoch 36, Batch 714, LR 0.000002 Loss 4.442254, Accuracy 89.745%\n",
      "Epoch 36, Batch 715, LR 0.000002 Loss 4.442395, Accuracy 89.747%\n",
      "Epoch 36, Batch 716, LR 0.000002 Loss 4.442385, Accuracy 89.748%\n",
      "Epoch 36, Batch 717, LR 0.000002 Loss 4.441941, Accuracy 89.752%\n",
      "Epoch 36, Batch 718, LR 0.000002 Loss 4.441630, Accuracy 89.753%\n",
      "Epoch 36, Batch 719, LR 0.000002 Loss 4.442379, Accuracy 89.751%\n",
      "Epoch 36, Batch 720, LR 0.000002 Loss 4.442517, Accuracy 89.748%\n",
      "Epoch 36, Batch 721, LR 0.000002 Loss 4.441876, Accuracy 89.747%\n",
      "Epoch 36, Batch 722, LR 0.000002 Loss 4.442453, Accuracy 89.745%\n",
      "Epoch 36, Batch 723, LR 0.000002 Loss 4.441968, Accuracy 89.742%\n",
      "Epoch 36, Batch 724, LR 0.000002 Loss 4.442527, Accuracy 89.746%\n",
      "Epoch 36, Batch 725, LR 0.000002 Loss 4.443419, Accuracy 89.746%\n",
      "Epoch 36, Batch 726, LR 0.000002 Loss 4.442510, Accuracy 89.747%\n",
      "Epoch 36, Batch 727, LR 0.000002 Loss 4.442189, Accuracy 89.748%\n",
      "Epoch 36, Batch 728, LR 0.000002 Loss 4.442825, Accuracy 89.745%\n",
      "Epoch 36, Batch 729, LR 0.000002 Loss 4.442214, Accuracy 89.751%\n",
      "Epoch 36, Batch 730, LR 0.000002 Loss 4.442361, Accuracy 89.750%\n",
      "Epoch 36, Batch 731, LR 0.000002 Loss 4.443226, Accuracy 89.746%\n",
      "Epoch 36, Batch 732, LR 0.000002 Loss 4.443905, Accuracy 89.741%\n",
      "Epoch 36, Batch 733, LR 0.000002 Loss 4.443169, Accuracy 89.746%\n",
      "Epoch 36, Batch 734, LR 0.000002 Loss 4.442748, Accuracy 89.751%\n",
      "Epoch 36, Batch 735, LR 0.000002 Loss 4.442879, Accuracy 89.747%\n",
      "Epoch 36, Batch 736, LR 0.000002 Loss 4.443425, Accuracy 89.746%\n",
      "Epoch 36, Batch 737, LR 0.000002 Loss 4.443318, Accuracy 89.747%\n",
      "Epoch 36, Batch 738, LR 0.000002 Loss 4.442652, Accuracy 89.753%\n",
      "Epoch 36, Batch 739, LR 0.000002 Loss 4.442992, Accuracy 89.752%\n",
      "Epoch 36, Batch 740, LR 0.000002 Loss 4.442748, Accuracy 89.753%\n",
      "Epoch 36, Batch 741, LR 0.000002 Loss 4.442658, Accuracy 89.753%\n",
      "Epoch 36, Batch 742, LR 0.000002 Loss 4.442757, Accuracy 89.752%\n",
      "Epoch 36, Batch 743, LR 0.000002 Loss 4.443612, Accuracy 89.746%\n",
      "Epoch 36, Batch 744, LR 0.000002 Loss 4.443218, Accuracy 89.751%\n",
      "Epoch 36, Batch 745, LR 0.000002 Loss 4.442887, Accuracy 89.747%\n",
      "Epoch 36, Batch 746, LR 0.000002 Loss 4.442853, Accuracy 89.744%\n",
      "Epoch 36, Batch 747, LR 0.000002 Loss 4.442685, Accuracy 89.749%\n",
      "Epoch 36, Batch 748, LR 0.000002 Loss 4.442957, Accuracy 89.743%\n",
      "Epoch 36, Batch 749, LR 0.000002 Loss 4.443383, Accuracy 89.740%\n",
      "Epoch 36, Batch 750, LR 0.000002 Loss 4.443297, Accuracy 89.737%\n",
      "Epoch 36, Batch 751, LR 0.000002 Loss 4.444483, Accuracy 89.729%\n",
      "Epoch 36, Batch 752, LR 0.000002 Loss 4.445128, Accuracy 89.726%\n",
      "Epoch 36, Batch 753, LR 0.000002 Loss 4.445920, Accuracy 89.727%\n",
      "Epoch 36, Batch 754, LR 0.000002 Loss 4.445182, Accuracy 89.729%\n",
      "Epoch 36, Batch 755, LR 0.000002 Loss 4.445155, Accuracy 89.726%\n",
      "Epoch 36, Batch 756, LR 0.000002 Loss 4.444703, Accuracy 89.729%\n",
      "Epoch 36, Batch 757, LR 0.000002 Loss 4.444631, Accuracy 89.731%\n",
      "Epoch 36, Batch 758, LR 0.000002 Loss 4.445788, Accuracy 89.727%\n",
      "Epoch 36, Batch 759, LR 0.000002 Loss 4.445633, Accuracy 89.723%\n",
      "Epoch 36, Batch 760, LR 0.000002 Loss 4.445755, Accuracy 89.726%\n",
      "Epoch 36, Batch 761, LR 0.000002 Loss 4.446007, Accuracy 89.725%\n",
      "Epoch 36, Batch 762, LR 0.000002 Loss 4.446486, Accuracy 89.730%\n",
      "Epoch 36, Batch 763, LR 0.000002 Loss 4.446034, Accuracy 89.732%\n",
      "Epoch 36, Batch 764, LR 0.000002 Loss 4.445924, Accuracy 89.734%\n",
      "Epoch 36, Batch 765, LR 0.000002 Loss 4.446577, Accuracy 89.733%\n",
      "Epoch 36, Batch 766, LR 0.000002 Loss 4.446581, Accuracy 89.735%\n",
      "Epoch 36, Batch 767, LR 0.000002 Loss 4.446924, Accuracy 89.738%\n",
      "Epoch 36, Batch 768, LR 0.000002 Loss 4.446729, Accuracy 89.739%\n",
      "Epoch 36, Batch 769, LR 0.000002 Loss 4.446235, Accuracy 89.742%\n",
      "Epoch 36, Batch 770, LR 0.000002 Loss 4.446213, Accuracy 89.743%\n",
      "Epoch 36, Batch 771, LR 0.000002 Loss 4.446460, Accuracy 89.740%\n",
      "Epoch 36, Batch 772, LR 0.000002 Loss 4.445756, Accuracy 89.746%\n",
      "Epoch 36, Batch 773, LR 0.000002 Loss 4.445351, Accuracy 89.748%\n",
      "Epoch 36, Batch 774, LR 0.000002 Loss 4.446083, Accuracy 89.745%\n",
      "Epoch 36, Batch 775, LR 0.000002 Loss 4.445918, Accuracy 89.741%\n",
      "Epoch 36, Batch 776, LR 0.000002 Loss 4.444157, Accuracy 89.751%\n",
      "Epoch 36, Batch 777, LR 0.000002 Loss 4.445322, Accuracy 89.746%\n",
      "Epoch 36, Batch 778, LR 0.000002 Loss 4.445885, Accuracy 89.739%\n",
      "Epoch 36, Batch 779, LR 0.000002 Loss 4.446141, Accuracy 89.736%\n",
      "Epoch 36, Batch 780, LR 0.000002 Loss 4.446668, Accuracy 89.735%\n",
      "Epoch 36, Batch 781, LR 0.000002 Loss 4.445964, Accuracy 89.737%\n",
      "Epoch 36, Batch 782, LR 0.000002 Loss 4.446443, Accuracy 89.733%\n",
      "Epoch 36, Batch 783, LR 0.000002 Loss 4.446443, Accuracy 89.730%\n",
      "Epoch 36, Batch 784, LR 0.000002 Loss 4.445640, Accuracy 89.734%\n",
      "Epoch 36, Batch 785, LR 0.000002 Loss 4.445840, Accuracy 89.737%\n",
      "Epoch 36, Batch 786, LR 0.000002 Loss 4.445575, Accuracy 89.736%\n",
      "Epoch 36, Batch 787, LR 0.000002 Loss 4.445327, Accuracy 89.739%\n",
      "Epoch 36, Batch 788, LR 0.000002 Loss 4.446507, Accuracy 89.736%\n",
      "Epoch 36, Batch 789, LR 0.000002 Loss 4.446312, Accuracy 89.734%\n",
      "Epoch 36, Batch 790, LR 0.000002 Loss 4.446263, Accuracy 89.735%\n",
      "Epoch 36, Batch 791, LR 0.000002 Loss 4.445762, Accuracy 89.739%\n",
      "Epoch 36, Batch 792, LR 0.000002 Loss 4.445670, Accuracy 89.739%\n",
      "Epoch 36, Batch 793, LR 0.000002 Loss 4.445480, Accuracy 89.737%\n",
      "Epoch 36, Batch 794, LR 0.000002 Loss 4.445745, Accuracy 89.731%\n",
      "Epoch 36, Batch 795, LR 0.000002 Loss 4.444997, Accuracy 89.736%\n",
      "Epoch 36, Batch 796, LR 0.000002 Loss 4.444995, Accuracy 89.735%\n",
      "Epoch 36, Batch 797, LR 0.000002 Loss 4.445658, Accuracy 89.731%\n",
      "Epoch 36, Batch 798, LR 0.000002 Loss 4.446195, Accuracy 89.730%\n",
      "Epoch 36, Batch 799, LR 0.000002 Loss 4.446168, Accuracy 89.732%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 800, LR 0.000002 Loss 4.446445, Accuracy 89.731%\n",
      "Epoch 36, Batch 801, LR 0.000002 Loss 4.446416, Accuracy 89.728%\n",
      "Epoch 36, Batch 802, LR 0.000002 Loss 4.446782, Accuracy 89.726%\n",
      "Epoch 36, Batch 803, LR 0.000002 Loss 4.446233, Accuracy 89.731%\n",
      "Epoch 36, Batch 804, LR 0.000002 Loss 4.446006, Accuracy 89.731%\n",
      "Epoch 36, Batch 805, LR 0.000002 Loss 4.445554, Accuracy 89.736%\n",
      "Epoch 36, Batch 806, LR 0.000002 Loss 4.444671, Accuracy 89.738%\n",
      "Epoch 36, Batch 807, LR 0.000002 Loss 4.443743, Accuracy 89.744%\n",
      "Epoch 36, Batch 808, LR 0.000002 Loss 4.443141, Accuracy 89.746%\n",
      "Epoch 36, Batch 809, LR 0.000002 Loss 4.442996, Accuracy 89.748%\n",
      "Epoch 36, Batch 810, LR 0.000002 Loss 4.442774, Accuracy 89.751%\n",
      "Epoch 36, Batch 811, LR 0.000002 Loss 4.442514, Accuracy 89.754%\n",
      "Epoch 36, Batch 812, LR 0.000002 Loss 4.443460, Accuracy 89.753%\n",
      "Epoch 36, Batch 813, LR 0.000002 Loss 4.443164, Accuracy 89.752%\n",
      "Epoch 36, Batch 814, LR 0.000002 Loss 4.442666, Accuracy 89.756%\n",
      "Epoch 36, Batch 815, LR 0.000002 Loss 4.441984, Accuracy 89.755%\n",
      "Epoch 36, Batch 816, LR 0.000002 Loss 4.441451, Accuracy 89.757%\n",
      "Epoch 36, Batch 817, LR 0.000002 Loss 4.442371, Accuracy 89.750%\n",
      "Epoch 36, Batch 818, LR 0.000002 Loss 4.442635, Accuracy 89.749%\n",
      "Epoch 36, Batch 819, LR 0.000002 Loss 4.443761, Accuracy 89.745%\n",
      "Epoch 36, Batch 820, LR 0.000002 Loss 4.444160, Accuracy 89.737%\n",
      "Epoch 36, Batch 821, LR 0.000002 Loss 4.444151, Accuracy 89.735%\n",
      "Epoch 36, Batch 822, LR 0.000002 Loss 4.444768, Accuracy 89.730%\n",
      "Epoch 36, Batch 823, LR 0.000002 Loss 4.444550, Accuracy 89.729%\n",
      "Epoch 36, Batch 824, LR 0.000002 Loss 4.445310, Accuracy 89.723%\n",
      "Epoch 36, Batch 825, LR 0.000002 Loss 4.444635, Accuracy 89.724%\n",
      "Epoch 36, Batch 826, LR 0.000002 Loss 4.444863, Accuracy 89.721%\n",
      "Epoch 36, Batch 827, LR 0.000002 Loss 4.445116, Accuracy 89.718%\n",
      "Epoch 36, Batch 828, LR 0.000002 Loss 4.445209, Accuracy 89.719%\n",
      "Epoch 36, Batch 829, LR 0.000002 Loss 4.445543, Accuracy 89.715%\n",
      "Epoch 36, Batch 830, LR 0.000002 Loss 4.445842, Accuracy 89.710%\n",
      "Epoch 36, Batch 831, LR 0.000002 Loss 4.445793, Accuracy 89.713%\n",
      "Epoch 36, Batch 832, LR 0.000002 Loss 4.445749, Accuracy 89.714%\n",
      "Epoch 36, Batch 833, LR 0.000002 Loss 4.446436, Accuracy 89.712%\n",
      "Epoch 36, Batch 834, LR 0.000002 Loss 4.445899, Accuracy 89.711%\n",
      "Epoch 36, Batch 835, LR 0.000002 Loss 4.446219, Accuracy 89.712%\n",
      "Epoch 36, Batch 836, LR 0.000002 Loss 4.446171, Accuracy 89.710%\n",
      "Epoch 36, Batch 837, LR 0.000002 Loss 4.446519, Accuracy 89.707%\n",
      "Epoch 36, Batch 838, LR 0.000002 Loss 4.446532, Accuracy 89.706%\n",
      "Epoch 36, Batch 839, LR 0.000002 Loss 4.447036, Accuracy 89.702%\n",
      "Epoch 36, Batch 840, LR 0.000002 Loss 4.447727, Accuracy 89.701%\n",
      "Epoch 36, Batch 841, LR 0.000002 Loss 4.446921, Accuracy 89.705%\n",
      "Epoch 36, Batch 842, LR 0.000002 Loss 4.446852, Accuracy 89.706%\n",
      "Epoch 36, Batch 843, LR 0.000002 Loss 4.446631, Accuracy 89.706%\n",
      "Epoch 36, Batch 844, LR 0.000002 Loss 4.446889, Accuracy 89.705%\n",
      "Epoch 36, Batch 845, LR 0.000002 Loss 4.446859, Accuracy 89.705%\n",
      "Epoch 36, Batch 846, LR 0.000002 Loss 4.446655, Accuracy 89.708%\n",
      "Epoch 36, Batch 847, LR 0.000002 Loss 4.446549, Accuracy 89.706%\n",
      "Epoch 36, Batch 848, LR 0.000002 Loss 4.445732, Accuracy 89.710%\n",
      "Epoch 36, Batch 849, LR 0.000002 Loss 4.445678, Accuracy 89.713%\n",
      "Epoch 36, Batch 850, LR 0.000002 Loss 4.445711, Accuracy 89.716%\n",
      "Epoch 36, Batch 851, LR 0.000002 Loss 4.446631, Accuracy 89.710%\n",
      "Epoch 36, Batch 852, LR 0.000002 Loss 4.445673, Accuracy 89.712%\n",
      "Epoch 36, Batch 853, LR 0.000002 Loss 4.444291, Accuracy 89.718%\n",
      "Epoch 36, Batch 854, LR 0.000002 Loss 4.444715, Accuracy 89.716%\n",
      "Epoch 36, Batch 855, LR 0.000002 Loss 4.445485, Accuracy 89.715%\n",
      "Epoch 36, Batch 856, LR 0.000002 Loss 4.445764, Accuracy 89.714%\n",
      "Epoch 36, Batch 857, LR 0.000002 Loss 4.447255, Accuracy 89.712%\n",
      "Epoch 36, Batch 858, LR 0.000002 Loss 4.447168, Accuracy 89.715%\n",
      "Epoch 36, Batch 859, LR 0.000002 Loss 4.447533, Accuracy 89.714%\n",
      "Epoch 36, Batch 860, LR 0.000002 Loss 4.447086, Accuracy 89.717%\n",
      "Epoch 36, Batch 861, LR 0.000002 Loss 4.447106, Accuracy 89.715%\n",
      "Epoch 36, Batch 862, LR 0.000002 Loss 4.447332, Accuracy 89.713%\n",
      "Epoch 36, Batch 863, LR 0.000002 Loss 4.446957, Accuracy 89.717%\n",
      "Epoch 36, Batch 864, LR 0.000002 Loss 4.447830, Accuracy 89.713%\n",
      "Epoch 36, Batch 865, LR 0.000002 Loss 4.447495, Accuracy 89.715%\n",
      "Epoch 36, Batch 866, LR 0.000002 Loss 4.447624, Accuracy 89.714%\n",
      "Epoch 36, Batch 867, LR 0.000002 Loss 4.447164, Accuracy 89.712%\n",
      "Epoch 36, Batch 868, LR 0.000002 Loss 4.446241, Accuracy 89.715%\n",
      "Epoch 36, Batch 869, LR 0.000002 Loss 4.447459, Accuracy 89.707%\n",
      "Epoch 36, Batch 870, LR 0.000002 Loss 4.448354, Accuracy 89.700%\n",
      "Epoch 36, Batch 871, LR 0.000002 Loss 4.447850, Accuracy 89.701%\n",
      "Epoch 36, Batch 872, LR 0.000002 Loss 4.447556, Accuracy 89.704%\n",
      "Epoch 36, Batch 873, LR 0.000002 Loss 4.447208, Accuracy 89.709%\n",
      "Epoch 36, Batch 874, LR 0.000002 Loss 4.447416, Accuracy 89.708%\n",
      "Epoch 36, Batch 875, LR 0.000002 Loss 4.446895, Accuracy 89.710%\n",
      "Epoch 36, Batch 876, LR 0.000002 Loss 4.446692, Accuracy 89.709%\n",
      "Epoch 36, Batch 877, LR 0.000002 Loss 4.446798, Accuracy 89.706%\n",
      "Epoch 36, Batch 878, LR 0.000002 Loss 4.445684, Accuracy 89.712%\n",
      "Epoch 36, Batch 879, LR 0.000002 Loss 4.445411, Accuracy 89.710%\n",
      "Epoch 36, Batch 880, LR 0.000002 Loss 4.445687, Accuracy 89.711%\n",
      "Epoch 36, Batch 881, LR 0.000002 Loss 4.445762, Accuracy 89.713%\n",
      "Epoch 36, Batch 882, LR 0.000002 Loss 4.445882, Accuracy 89.712%\n",
      "Epoch 36, Batch 883, LR 0.000002 Loss 4.446531, Accuracy 89.708%\n",
      "Epoch 36, Batch 884, LR 0.000002 Loss 4.446272, Accuracy 89.708%\n",
      "Epoch 36, Batch 885, LR 0.000002 Loss 4.446492, Accuracy 89.704%\n",
      "Epoch 36, Batch 886, LR 0.000002 Loss 4.446260, Accuracy 89.704%\n",
      "Epoch 36, Batch 887, LR 0.000002 Loss 4.446265, Accuracy 89.700%\n",
      "Epoch 36, Batch 888, LR 0.000002 Loss 4.446766, Accuracy 89.695%\n",
      "Epoch 36, Batch 889, LR 0.000002 Loss 4.445882, Accuracy 89.701%\n",
      "Epoch 36, Batch 890, LR 0.000002 Loss 4.446137, Accuracy 89.700%\n",
      "Epoch 36, Batch 891, LR 0.000002 Loss 4.445796, Accuracy 89.705%\n",
      "Epoch 36, Batch 892, LR 0.000002 Loss 4.444917, Accuracy 89.708%\n",
      "Epoch 36, Batch 893, LR 0.000002 Loss 4.443943, Accuracy 89.711%\n",
      "Epoch 36, Batch 894, LR 0.000002 Loss 4.444187, Accuracy 89.711%\n",
      "Epoch 36, Batch 895, LR 0.000002 Loss 4.444115, Accuracy 89.714%\n",
      "Epoch 36, Batch 896, LR 0.000002 Loss 4.443816, Accuracy 89.715%\n",
      "Epoch 36, Batch 897, LR 0.000002 Loss 4.443423, Accuracy 89.717%\n",
      "Epoch 36, Batch 898, LR 0.000002 Loss 4.443663, Accuracy 89.715%\n",
      "Epoch 36, Batch 899, LR 0.000002 Loss 4.444411, Accuracy 89.713%\n",
      "Epoch 36, Batch 900, LR 0.000002 Loss 4.444198, Accuracy 89.719%\n",
      "Epoch 36, Batch 901, LR 0.000002 Loss 4.443866, Accuracy 89.722%\n",
      "Epoch 36, Batch 902, LR 0.000002 Loss 4.444381, Accuracy 89.721%\n",
      "Epoch 36, Batch 903, LR 0.000002 Loss 4.444314, Accuracy 89.723%\n",
      "Epoch 36, Batch 904, LR 0.000002 Loss 4.444484, Accuracy 89.720%\n",
      "Epoch 36, Batch 905, LR 0.000002 Loss 4.444833, Accuracy 89.716%\n",
      "Epoch 36, Batch 906, LR 0.000002 Loss 4.444353, Accuracy 89.718%\n",
      "Epoch 36, Batch 907, LR 0.000002 Loss 4.443925, Accuracy 89.721%\n",
      "Epoch 36, Batch 908, LR 0.000002 Loss 4.443330, Accuracy 89.724%\n",
      "Epoch 36, Batch 909, LR 0.000002 Loss 4.442513, Accuracy 89.729%\n",
      "Epoch 36, Batch 910, LR 0.000002 Loss 4.443251, Accuracy 89.728%\n",
      "Epoch 36, Batch 911, LR 0.000002 Loss 4.443142, Accuracy 89.725%\n",
      "Epoch 36, Batch 912, LR 0.000002 Loss 4.442585, Accuracy 89.727%\n",
      "Epoch 36, Batch 913, LR 0.000002 Loss 4.441911, Accuracy 89.733%\n",
      "Epoch 36, Batch 914, LR 0.000002 Loss 4.441195, Accuracy 89.734%\n",
      "Epoch 36, Batch 915, LR 0.000002 Loss 4.441276, Accuracy 89.735%\n",
      "Epoch 36, Batch 916, LR 0.000002 Loss 4.441686, Accuracy 89.733%\n",
      "Epoch 36, Batch 917, LR 0.000002 Loss 4.441998, Accuracy 89.733%\n",
      "Epoch 36, Batch 918, LR 0.000002 Loss 4.441780, Accuracy 89.734%\n",
      "Epoch 36, Batch 919, LR 0.000002 Loss 4.442410, Accuracy 89.731%\n",
      "Epoch 36, Batch 920, LR 0.000002 Loss 4.443106, Accuracy 89.725%\n",
      "Epoch 36, Batch 921, LR 0.000002 Loss 4.442751, Accuracy 89.726%\n",
      "Epoch 36, Batch 922, LR 0.000002 Loss 4.442653, Accuracy 89.731%\n",
      "Epoch 36, Batch 923, LR 0.000002 Loss 4.442529, Accuracy 89.730%\n",
      "Epoch 36, Batch 924, LR 0.000002 Loss 4.443715, Accuracy 89.724%\n",
      "Epoch 36, Batch 925, LR 0.000002 Loss 4.443062, Accuracy 89.727%\n",
      "Epoch 36, Batch 926, LR 0.000002 Loss 4.442585, Accuracy 89.732%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 927, LR 0.000002 Loss 4.442566, Accuracy 89.734%\n",
      "Epoch 36, Batch 928, LR 0.000002 Loss 4.441780, Accuracy 89.742%\n",
      "Epoch 36, Batch 929, LR 0.000002 Loss 4.441402, Accuracy 89.744%\n",
      "Epoch 36, Batch 930, LR 0.000002 Loss 4.441523, Accuracy 89.745%\n",
      "Epoch 36, Batch 931, LR 0.000002 Loss 4.441360, Accuracy 89.748%\n",
      "Epoch 36, Batch 932, LR 0.000002 Loss 4.441793, Accuracy 89.747%\n",
      "Epoch 36, Batch 933, LR 0.000002 Loss 4.441654, Accuracy 89.749%\n",
      "Epoch 36, Batch 934, LR 0.000002 Loss 4.441210, Accuracy 89.748%\n",
      "Epoch 36, Batch 935, LR 0.000002 Loss 4.440579, Accuracy 89.750%\n",
      "Epoch 36, Batch 936, LR 0.000002 Loss 4.440324, Accuracy 89.754%\n",
      "Epoch 36, Batch 937, LR 0.000002 Loss 4.440328, Accuracy 89.755%\n",
      "Epoch 36, Batch 938, LR 0.000002 Loss 4.440420, Accuracy 89.752%\n",
      "Epoch 36, Batch 939, LR 0.000002 Loss 4.440021, Accuracy 89.756%\n",
      "Epoch 36, Batch 940, LR 0.000002 Loss 4.440364, Accuracy 89.757%\n",
      "Epoch 36, Batch 941, LR 0.000002 Loss 4.440364, Accuracy 89.756%\n",
      "Epoch 36, Batch 942, LR 0.000002 Loss 4.440514, Accuracy 89.753%\n",
      "Epoch 36, Batch 943, LR 0.000002 Loss 4.439710, Accuracy 89.755%\n",
      "Epoch 36, Batch 944, LR 0.000002 Loss 4.439644, Accuracy 89.754%\n",
      "Epoch 36, Batch 945, LR 0.000002 Loss 4.440091, Accuracy 89.754%\n",
      "Epoch 36, Batch 946, LR 0.000002 Loss 4.439673, Accuracy 89.757%\n",
      "Epoch 36, Batch 947, LR 0.000002 Loss 4.439132, Accuracy 89.758%\n",
      "Epoch 36, Batch 948, LR 0.000002 Loss 4.439739, Accuracy 89.759%\n",
      "Epoch 36, Batch 949, LR 0.000002 Loss 4.440040, Accuracy 89.755%\n",
      "Epoch 36, Batch 950, LR 0.000002 Loss 4.440007, Accuracy 89.757%\n",
      "Epoch 36, Batch 951, LR 0.000002 Loss 4.440154, Accuracy 89.755%\n",
      "Epoch 36, Batch 952, LR 0.000002 Loss 4.440491, Accuracy 89.756%\n",
      "Epoch 36, Batch 953, LR 0.000002 Loss 4.440400, Accuracy 89.756%\n",
      "Epoch 36, Batch 954, LR 0.000002 Loss 4.440084, Accuracy 89.758%\n",
      "Epoch 36, Batch 955, LR 0.000002 Loss 4.439661, Accuracy 89.761%\n",
      "Epoch 36, Batch 956, LR 0.000002 Loss 4.439924, Accuracy 89.763%\n",
      "Epoch 36, Batch 957, LR 0.000002 Loss 4.440136, Accuracy 89.759%\n",
      "Epoch 36, Batch 958, LR 0.000002 Loss 4.439351, Accuracy 89.763%\n",
      "Epoch 36, Batch 959, LR 0.000002 Loss 4.440048, Accuracy 89.760%\n",
      "Epoch 36, Batch 960, LR 0.000002 Loss 4.439820, Accuracy 89.760%\n",
      "Epoch 36, Batch 961, LR 0.000002 Loss 4.439445, Accuracy 89.762%\n",
      "Epoch 36, Batch 962, LR 0.000002 Loss 4.439612, Accuracy 89.760%\n",
      "Epoch 36, Batch 963, LR 0.000002 Loss 4.439662, Accuracy 89.757%\n",
      "Epoch 36, Batch 964, LR 0.000002 Loss 4.439200, Accuracy 89.760%\n",
      "Epoch 36, Batch 965, LR 0.000002 Loss 4.439503, Accuracy 89.756%\n",
      "Epoch 36, Batch 966, LR 0.000002 Loss 4.439051, Accuracy 89.760%\n",
      "Epoch 36, Batch 967, LR 0.000002 Loss 4.439198, Accuracy 89.759%\n",
      "Epoch 36, Batch 968, LR 0.000002 Loss 4.439805, Accuracy 89.753%\n",
      "Epoch 36, Batch 969, LR 0.000002 Loss 4.440011, Accuracy 89.753%\n",
      "Epoch 36, Batch 970, LR 0.000002 Loss 4.439691, Accuracy 89.752%\n",
      "Epoch 36, Batch 971, LR 0.000002 Loss 4.439644, Accuracy 89.754%\n",
      "Epoch 36, Batch 972, LR 0.000002 Loss 4.440397, Accuracy 89.747%\n",
      "Epoch 36, Batch 973, LR 0.000002 Loss 4.440301, Accuracy 89.747%\n",
      "Epoch 36, Batch 974, LR 0.000002 Loss 4.440171, Accuracy 89.746%\n",
      "Epoch 36, Batch 975, LR 0.000002 Loss 4.440063, Accuracy 89.748%\n",
      "Epoch 36, Batch 976, LR 0.000002 Loss 4.440439, Accuracy 89.746%\n",
      "Epoch 36, Batch 977, LR 0.000002 Loss 4.439602, Accuracy 89.750%\n",
      "Epoch 36, Batch 978, LR 0.000002 Loss 4.439722, Accuracy 89.750%\n",
      "Epoch 36, Batch 979, LR 0.000002 Loss 4.440199, Accuracy 89.742%\n",
      "Epoch 36, Batch 980, LR 0.000002 Loss 4.440136, Accuracy 89.743%\n",
      "Epoch 36, Batch 981, LR 0.000002 Loss 4.440248, Accuracy 89.741%\n",
      "Epoch 36, Batch 982, LR 0.000002 Loss 4.440160, Accuracy 89.741%\n",
      "Epoch 36, Batch 983, LR 0.000002 Loss 4.439899, Accuracy 89.744%\n",
      "Epoch 36, Batch 984, LR 0.000002 Loss 4.439261, Accuracy 89.749%\n",
      "Epoch 36, Batch 985, LR 0.000002 Loss 4.439447, Accuracy 89.751%\n",
      "Epoch 36, Batch 986, LR 0.000002 Loss 4.439695, Accuracy 89.752%\n",
      "Epoch 36, Batch 987, LR 0.000002 Loss 4.438690, Accuracy 89.756%\n",
      "Epoch 36, Batch 988, LR 0.000002 Loss 4.438928, Accuracy 89.754%\n",
      "Epoch 36, Batch 989, LR 0.000002 Loss 4.439075, Accuracy 89.754%\n",
      "Epoch 36, Batch 990, LR 0.000002 Loss 4.439629, Accuracy 89.751%\n",
      "Epoch 36, Batch 991, LR 0.000002 Loss 4.439488, Accuracy 89.752%\n",
      "Epoch 36, Batch 992, LR 0.000002 Loss 4.439464, Accuracy 89.753%\n",
      "Epoch 36, Batch 993, LR 0.000002 Loss 4.439270, Accuracy 89.754%\n",
      "Epoch 36, Batch 994, LR 0.000002 Loss 4.438836, Accuracy 89.753%\n",
      "Epoch 36, Batch 995, LR 0.000002 Loss 4.438673, Accuracy 89.753%\n",
      "Epoch 36, Batch 996, LR 0.000002 Loss 4.438547, Accuracy 89.755%\n",
      "Epoch 36, Batch 997, LR 0.000002 Loss 4.438220, Accuracy 89.754%\n",
      "Epoch 36, Batch 998, LR 0.000002 Loss 4.437205, Accuracy 89.756%\n",
      "Epoch 36, Batch 999, LR 0.000002 Loss 4.437396, Accuracy 89.757%\n",
      "Epoch 36, Batch 1000, LR 0.000002 Loss 4.438118, Accuracy 89.755%\n",
      "Epoch 36, Batch 1001, LR 0.000002 Loss 4.437671, Accuracy 89.759%\n",
      "Epoch 36, Batch 1002, LR 0.000002 Loss 4.437375, Accuracy 89.762%\n",
      "Epoch 36, Batch 1003, LR 0.000002 Loss 4.437381, Accuracy 89.761%\n",
      "Epoch 36, Batch 1004, LR 0.000002 Loss 4.437082, Accuracy 89.759%\n",
      "Epoch 36, Batch 1005, LR 0.000002 Loss 4.436922, Accuracy 89.759%\n",
      "Epoch 36, Batch 1006, LR 0.000002 Loss 4.437221, Accuracy 89.755%\n",
      "Epoch 36, Batch 1007, LR 0.000002 Loss 4.437998, Accuracy 89.754%\n",
      "Epoch 36, Batch 1008, LR 0.000002 Loss 4.437507, Accuracy 89.755%\n",
      "Epoch 36, Batch 1009, LR 0.000002 Loss 4.436501, Accuracy 89.759%\n",
      "Epoch 36, Batch 1010, LR 0.000002 Loss 4.436944, Accuracy 89.758%\n",
      "Epoch 36, Batch 1011, LR 0.000002 Loss 4.437263, Accuracy 89.759%\n",
      "Epoch 36, Batch 1012, LR 0.000002 Loss 4.437114, Accuracy 89.759%\n",
      "Epoch 36, Batch 1013, LR 0.000002 Loss 4.437349, Accuracy 89.754%\n",
      "Epoch 36, Batch 1014, LR 0.000002 Loss 4.437435, Accuracy 89.753%\n",
      "Epoch 36, Batch 1015, LR 0.000002 Loss 4.437189, Accuracy 89.754%\n",
      "Epoch 36, Batch 1016, LR 0.000002 Loss 4.436759, Accuracy 89.752%\n",
      "Epoch 36, Batch 1017, LR 0.000002 Loss 4.436551, Accuracy 89.752%\n",
      "Epoch 36, Batch 1018, LR 0.000002 Loss 4.435833, Accuracy 89.755%\n",
      "Epoch 36, Batch 1019, LR 0.000002 Loss 4.436214, Accuracy 89.755%\n",
      "Epoch 36, Batch 1020, LR 0.000002 Loss 4.435565, Accuracy 89.756%\n",
      "Epoch 36, Batch 1021, LR 0.000002 Loss 4.434854, Accuracy 89.760%\n",
      "Epoch 36, Batch 1022, LR 0.000002 Loss 4.434832, Accuracy 89.760%\n",
      "Epoch 36, Batch 1023, LR 0.000002 Loss 4.434717, Accuracy 89.760%\n",
      "Epoch 36, Batch 1024, LR 0.000002 Loss 4.435126, Accuracy 89.761%\n",
      "Epoch 36, Batch 1025, LR 0.000002 Loss 4.435118, Accuracy 89.761%\n",
      "Epoch 36, Batch 1026, LR 0.000002 Loss 4.435256, Accuracy 89.761%\n",
      "Epoch 36, Batch 1027, LR 0.000002 Loss 4.434661, Accuracy 89.765%\n",
      "Epoch 36, Batch 1028, LR 0.000002 Loss 4.434452, Accuracy 89.768%\n",
      "Epoch 36, Batch 1029, LR 0.000002 Loss 4.434534, Accuracy 89.764%\n",
      "Epoch 36, Batch 1030, LR 0.000002 Loss 4.435011, Accuracy 89.763%\n",
      "Epoch 36, Batch 1031, LR 0.000002 Loss 4.435750, Accuracy 89.759%\n",
      "Epoch 36, Batch 1032, LR 0.000002 Loss 4.436172, Accuracy 89.756%\n",
      "Epoch 36, Batch 1033, LR 0.000002 Loss 4.436095, Accuracy 89.757%\n",
      "Epoch 36, Batch 1034, LR 0.000002 Loss 4.435074, Accuracy 89.764%\n",
      "Epoch 36, Batch 1035, LR 0.000002 Loss 4.435325, Accuracy 89.758%\n",
      "Epoch 36, Batch 1036, LR 0.000002 Loss 4.435768, Accuracy 89.758%\n",
      "Epoch 36, Batch 1037, LR 0.000002 Loss 4.435504, Accuracy 89.759%\n",
      "Epoch 36, Batch 1038, LR 0.000002 Loss 4.435234, Accuracy 89.762%\n",
      "Epoch 36, Batch 1039, LR 0.000002 Loss 4.435034, Accuracy 89.761%\n",
      "Epoch 36, Batch 1040, LR 0.000002 Loss 4.434746, Accuracy 89.762%\n",
      "Epoch 36, Batch 1041, LR 0.000002 Loss 4.434375, Accuracy 89.760%\n",
      "Epoch 36, Batch 1042, LR 0.000002 Loss 4.434647, Accuracy 89.758%\n",
      "Epoch 36, Batch 1043, LR 0.000002 Loss 4.434547, Accuracy 89.755%\n",
      "Epoch 36, Batch 1044, LR 0.000002 Loss 4.434662, Accuracy 89.756%\n",
      "Epoch 36, Batch 1045, LR 0.000002 Loss 4.434545, Accuracy 89.754%\n",
      "Epoch 36, Batch 1046, LR 0.000002 Loss 4.434146, Accuracy 89.756%\n",
      "Epoch 36, Batch 1047, LR 0.000002 Loss 4.434359, Accuracy 89.757%\n",
      "Epoch 36, Loss (train set) 4.434359, Accuracy (train set) 89.757%\n",
      "Epoch 37, Batch 1, LR 0.000002 Loss 4.259688, Accuracy 91.406%\n",
      "Epoch 37, Batch 2, LR 0.000002 Loss 4.346080, Accuracy 89.453%\n",
      "Epoch 37, Batch 3, LR 0.000002 Loss 4.416156, Accuracy 88.542%\n",
      "Epoch 37, Batch 4, LR 0.000002 Loss 4.527584, Accuracy 88.477%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 5, LR 0.000002 Loss 4.528235, Accuracy 88.125%\n",
      "Epoch 37, Batch 6, LR 0.000002 Loss 4.472279, Accuracy 88.411%\n",
      "Epoch 37, Batch 7, LR 0.000002 Loss 4.447815, Accuracy 89.174%\n",
      "Epoch 37, Batch 8, LR 0.000002 Loss 4.467423, Accuracy 88.965%\n",
      "Epoch 37, Batch 9, LR 0.000002 Loss 4.429464, Accuracy 89.236%\n",
      "Epoch 37, Batch 10, LR 0.000002 Loss 4.373147, Accuracy 89.531%\n",
      "Epoch 37, Batch 11, LR 0.000002 Loss 4.362290, Accuracy 89.489%\n",
      "Epoch 37, Batch 12, LR 0.000002 Loss 4.345479, Accuracy 89.779%\n",
      "Epoch 37, Batch 13, LR 0.000002 Loss 4.363867, Accuracy 89.904%\n",
      "Epoch 37, Batch 14, LR 0.000002 Loss 4.378490, Accuracy 89.676%\n",
      "Epoch 37, Batch 15, LR 0.000002 Loss 4.386295, Accuracy 89.688%\n",
      "Epoch 37, Batch 16, LR 0.000002 Loss 4.371156, Accuracy 89.746%\n",
      "Epoch 37, Batch 17, LR 0.000002 Loss 4.357972, Accuracy 89.844%\n",
      "Epoch 37, Batch 18, LR 0.000002 Loss 4.349829, Accuracy 89.800%\n",
      "Epoch 37, Batch 19, LR 0.000002 Loss 4.342738, Accuracy 89.638%\n",
      "Epoch 37, Batch 20, LR 0.000002 Loss 4.334040, Accuracy 89.805%\n",
      "Epoch 37, Batch 21, LR 0.000002 Loss 4.321195, Accuracy 89.955%\n",
      "Epoch 37, Batch 22, LR 0.000002 Loss 4.298081, Accuracy 89.986%\n",
      "Epoch 37, Batch 23, LR 0.000002 Loss 4.318021, Accuracy 89.912%\n",
      "Epoch 37, Batch 24, LR 0.000002 Loss 4.342831, Accuracy 89.779%\n",
      "Epoch 37, Batch 25, LR 0.000002 Loss 4.368588, Accuracy 89.750%\n",
      "Epoch 37, Batch 26, LR 0.000002 Loss 4.374024, Accuracy 89.724%\n",
      "Epoch 37, Batch 27, LR 0.000002 Loss 4.384065, Accuracy 89.612%\n",
      "Epoch 37, Batch 28, LR 0.000002 Loss 4.401644, Accuracy 89.425%\n",
      "Epoch 37, Batch 29, LR 0.000002 Loss 4.408092, Accuracy 89.413%\n",
      "Epoch 37, Batch 30, LR 0.000002 Loss 4.403634, Accuracy 89.453%\n",
      "Epoch 37, Batch 31, LR 0.000002 Loss 4.422009, Accuracy 89.340%\n",
      "Epoch 37, Batch 32, LR 0.000002 Loss 4.417667, Accuracy 89.404%\n",
      "Epoch 37, Batch 33, LR 0.000001 Loss 4.403267, Accuracy 89.512%\n",
      "Epoch 37, Batch 34, LR 0.000001 Loss 4.423369, Accuracy 89.430%\n",
      "Epoch 37, Batch 35, LR 0.000001 Loss 4.411005, Accuracy 89.464%\n",
      "Epoch 37, Batch 36, LR 0.000001 Loss 4.400850, Accuracy 89.475%\n",
      "Epoch 37, Batch 37, LR 0.000001 Loss 4.414006, Accuracy 89.400%\n",
      "Epoch 37, Batch 38, LR 0.000001 Loss 4.410021, Accuracy 89.412%\n",
      "Epoch 37, Batch 39, LR 0.000001 Loss 4.395948, Accuracy 89.403%\n",
      "Epoch 37, Batch 40, LR 0.000001 Loss 4.390514, Accuracy 89.473%\n",
      "Epoch 37, Batch 41, LR 0.000001 Loss 4.389004, Accuracy 89.558%\n",
      "Epoch 37, Batch 42, LR 0.000001 Loss 4.393455, Accuracy 89.621%\n",
      "Epoch 37, Batch 43, LR 0.000001 Loss 4.387702, Accuracy 89.735%\n",
      "Epoch 37, Batch 44, LR 0.000001 Loss 4.396776, Accuracy 89.719%\n",
      "Epoch 37, Batch 45, LR 0.000001 Loss 4.417833, Accuracy 89.583%\n",
      "Epoch 37, Batch 46, LR 0.000001 Loss 4.415985, Accuracy 89.487%\n",
      "Epoch 37, Batch 47, LR 0.000001 Loss 4.424021, Accuracy 89.528%\n",
      "Epoch 37, Batch 48, LR 0.000001 Loss 4.448564, Accuracy 89.486%\n",
      "Epoch 37, Batch 49, LR 0.000001 Loss 4.438795, Accuracy 89.557%\n",
      "Epoch 37, Batch 50, LR 0.000001 Loss 4.423113, Accuracy 89.578%\n",
      "Epoch 37, Batch 51, LR 0.000001 Loss 4.425189, Accuracy 89.522%\n",
      "Epoch 37, Batch 52, LR 0.000001 Loss 4.441065, Accuracy 89.423%\n",
      "Epoch 37, Batch 53, LR 0.000001 Loss 4.431621, Accuracy 89.446%\n",
      "Epoch 37, Batch 54, LR 0.000001 Loss 4.435914, Accuracy 89.439%\n",
      "Epoch 37, Batch 55, LR 0.000001 Loss 4.436027, Accuracy 89.474%\n",
      "Epoch 37, Batch 56, LR 0.000001 Loss 4.442547, Accuracy 89.439%\n",
      "Epoch 37, Batch 57, LR 0.000001 Loss 4.440809, Accuracy 89.474%\n",
      "Epoch 37, Batch 58, LR 0.000001 Loss 4.432381, Accuracy 89.547%\n",
      "Epoch 37, Batch 59, LR 0.000001 Loss 4.424321, Accuracy 89.592%\n",
      "Epoch 37, Batch 60, LR 0.000001 Loss 4.428805, Accuracy 89.609%\n",
      "Epoch 37, Batch 61, LR 0.000001 Loss 4.437662, Accuracy 89.575%\n",
      "Epoch 37, Batch 62, LR 0.000001 Loss 4.435934, Accuracy 89.567%\n",
      "Epoch 37, Batch 63, LR 0.000001 Loss 4.447733, Accuracy 89.559%\n",
      "Epoch 37, Batch 64, LR 0.000001 Loss 4.441358, Accuracy 89.575%\n",
      "Epoch 37, Batch 65, LR 0.000001 Loss 4.445702, Accuracy 89.603%\n",
      "Epoch 37, Batch 66, LR 0.000001 Loss 4.444043, Accuracy 89.607%\n",
      "Epoch 37, Batch 67, LR 0.000001 Loss 4.448557, Accuracy 89.599%\n",
      "Epoch 37, Batch 68, LR 0.000001 Loss 4.443200, Accuracy 89.660%\n",
      "Epoch 37, Batch 69, LR 0.000001 Loss 4.443062, Accuracy 89.697%\n",
      "Epoch 37, Batch 70, LR 0.000001 Loss 4.442095, Accuracy 89.654%\n",
      "Epoch 37, Batch 71, LR 0.000001 Loss 4.443627, Accuracy 89.635%\n",
      "Epoch 37, Batch 72, LR 0.000001 Loss 4.443358, Accuracy 89.605%\n",
      "Epoch 37, Batch 73, LR 0.000001 Loss 4.450076, Accuracy 89.608%\n",
      "Epoch 37, Batch 74, LR 0.000001 Loss 4.448597, Accuracy 89.601%\n",
      "Epoch 37, Batch 75, LR 0.000001 Loss 4.450242, Accuracy 89.573%\n",
      "Epoch 37, Batch 76, LR 0.000001 Loss 4.458405, Accuracy 89.587%\n",
      "Epoch 37, Batch 77, LR 0.000001 Loss 4.451733, Accuracy 89.600%\n",
      "Epoch 37, Batch 78, LR 0.000001 Loss 4.462122, Accuracy 89.533%\n",
      "Epoch 37, Batch 79, LR 0.000001 Loss 4.460592, Accuracy 89.537%\n",
      "Epoch 37, Batch 80, LR 0.000001 Loss 4.453062, Accuracy 89.590%\n",
      "Epoch 37, Batch 81, LR 0.000001 Loss 4.447863, Accuracy 89.593%\n",
      "Epoch 37, Batch 82, LR 0.000001 Loss 4.444495, Accuracy 89.625%\n",
      "Epoch 37, Batch 83, LR 0.000001 Loss 4.445083, Accuracy 89.618%\n",
      "Epoch 37, Batch 84, LR 0.000001 Loss 4.436295, Accuracy 89.648%\n",
      "Epoch 37, Batch 85, LR 0.000001 Loss 4.445112, Accuracy 89.623%\n",
      "Epoch 37, Batch 86, LR 0.000001 Loss 4.452893, Accuracy 89.608%\n",
      "Epoch 37, Batch 87, LR 0.000001 Loss 4.452333, Accuracy 89.619%\n",
      "Epoch 37, Batch 88, LR 0.000001 Loss 4.452468, Accuracy 89.613%\n",
      "Epoch 37, Batch 89, LR 0.000001 Loss 4.451514, Accuracy 89.651%\n",
      "Epoch 37, Batch 90, LR 0.000001 Loss 4.447764, Accuracy 89.679%\n",
      "Epoch 37, Batch 91, LR 0.000001 Loss 4.446842, Accuracy 89.681%\n",
      "Epoch 37, Batch 92, LR 0.000001 Loss 4.436999, Accuracy 89.725%\n",
      "Epoch 37, Batch 93, LR 0.000001 Loss 4.433265, Accuracy 89.760%\n",
      "Epoch 37, Batch 94, LR 0.000001 Loss 4.426440, Accuracy 89.811%\n",
      "Epoch 37, Batch 95, LR 0.000001 Loss 4.419348, Accuracy 89.819%\n",
      "Epoch 37, Batch 96, LR 0.000001 Loss 4.423721, Accuracy 89.836%\n",
      "Epoch 37, Batch 97, LR 0.000001 Loss 4.425024, Accuracy 89.795%\n",
      "Epoch 37, Batch 98, LR 0.000001 Loss 4.418074, Accuracy 89.820%\n",
      "Epoch 37, Batch 99, LR 0.000001 Loss 4.411975, Accuracy 89.820%\n",
      "Epoch 37, Batch 100, LR 0.000001 Loss 4.410623, Accuracy 89.836%\n",
      "Epoch 37, Batch 101, LR 0.000001 Loss 4.409700, Accuracy 89.828%\n",
      "Epoch 37, Batch 102, LR 0.000001 Loss 4.404047, Accuracy 89.867%\n",
      "Epoch 37, Batch 103, LR 0.000001 Loss 4.409856, Accuracy 89.867%\n",
      "Epoch 37, Batch 104, LR 0.000001 Loss 4.402222, Accuracy 89.911%\n",
      "Epoch 37, Batch 105, LR 0.000001 Loss 4.405427, Accuracy 89.881%\n",
      "Epoch 37, Batch 106, LR 0.000001 Loss 4.404183, Accuracy 89.888%\n",
      "Epoch 37, Batch 107, LR 0.000001 Loss 4.408615, Accuracy 89.873%\n",
      "Epoch 37, Batch 108, LR 0.000001 Loss 4.408709, Accuracy 89.865%\n",
      "Epoch 37, Batch 109, LR 0.000001 Loss 4.413732, Accuracy 89.829%\n",
      "Epoch 37, Batch 110, LR 0.000001 Loss 4.412992, Accuracy 89.837%\n",
      "Epoch 37, Batch 111, LR 0.000001 Loss 4.417703, Accuracy 89.802%\n",
      "Epoch 37, Batch 112, LR 0.000001 Loss 4.418257, Accuracy 89.816%\n",
      "Epoch 37, Batch 113, LR 0.000001 Loss 4.427105, Accuracy 89.761%\n",
      "Epoch 37, Batch 114, LR 0.000001 Loss 4.419391, Accuracy 89.803%\n",
      "Epoch 37, Batch 115, LR 0.000001 Loss 4.424446, Accuracy 89.803%\n",
      "Epoch 37, Batch 116, LR 0.000001 Loss 4.424345, Accuracy 89.810%\n",
      "Epoch 37, Batch 117, LR 0.000001 Loss 4.428021, Accuracy 89.790%\n",
      "Epoch 37, Batch 118, LR 0.000001 Loss 4.422557, Accuracy 89.824%\n",
      "Epoch 37, Batch 119, LR 0.000001 Loss 4.424283, Accuracy 89.798%\n",
      "Epoch 37, Batch 120, LR 0.000001 Loss 4.424317, Accuracy 89.811%\n",
      "Epoch 37, Batch 121, LR 0.000001 Loss 4.421935, Accuracy 89.818%\n",
      "Epoch 37, Batch 122, LR 0.000001 Loss 4.423835, Accuracy 89.799%\n",
      "Epoch 37, Batch 123, LR 0.000001 Loss 4.423949, Accuracy 89.774%\n",
      "Epoch 37, Batch 124, LR 0.000001 Loss 4.424492, Accuracy 89.800%\n",
      "Epoch 37, Batch 125, LR 0.000001 Loss 4.421075, Accuracy 89.819%\n",
      "Epoch 37, Batch 126, LR 0.000001 Loss 4.422821, Accuracy 89.807%\n",
      "Epoch 37, Batch 127, LR 0.000001 Loss 4.423748, Accuracy 89.825%\n",
      "Epoch 37, Batch 128, LR 0.000001 Loss 4.423610, Accuracy 89.807%\n",
      "Epoch 37, Batch 129, LR 0.000001 Loss 4.421359, Accuracy 89.820%\n",
      "Epoch 37, Batch 130, LR 0.000001 Loss 4.427594, Accuracy 89.808%\n",
      "Epoch 37, Batch 131, LR 0.000001 Loss 4.434619, Accuracy 89.778%\n",
      "Epoch 37, Batch 132, LR 0.000001 Loss 4.435139, Accuracy 89.761%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 133, LR 0.000001 Loss 4.437901, Accuracy 89.750%\n",
      "Epoch 37, Batch 134, LR 0.000001 Loss 4.443969, Accuracy 89.727%\n",
      "Epoch 37, Batch 135, LR 0.000001 Loss 4.444458, Accuracy 89.734%\n",
      "Epoch 37, Batch 136, LR 0.000001 Loss 4.446980, Accuracy 89.723%\n",
      "Epoch 37, Batch 137, LR 0.000001 Loss 4.449715, Accuracy 89.713%\n",
      "Epoch 37, Batch 138, LR 0.000001 Loss 4.447679, Accuracy 89.691%\n",
      "Epoch 37, Batch 139, LR 0.000001 Loss 4.460422, Accuracy 89.630%\n",
      "Epoch 37, Batch 140, LR 0.000001 Loss 4.454200, Accuracy 89.643%\n",
      "Epoch 37, Batch 141, LR 0.000001 Loss 4.454298, Accuracy 89.639%\n",
      "Epoch 37, Batch 142, LR 0.000001 Loss 4.459861, Accuracy 89.591%\n",
      "Epoch 37, Batch 143, LR 0.000001 Loss 4.460923, Accuracy 89.592%\n",
      "Epoch 37, Batch 144, LR 0.000001 Loss 4.459494, Accuracy 89.610%\n",
      "Epoch 37, Batch 145, LR 0.000001 Loss 4.458916, Accuracy 89.612%\n",
      "Epoch 37, Batch 146, LR 0.000001 Loss 4.461077, Accuracy 89.624%\n",
      "Epoch 37, Batch 147, LR 0.000001 Loss 4.466305, Accuracy 89.599%\n",
      "Epoch 37, Batch 148, LR 0.000001 Loss 4.464402, Accuracy 89.606%\n",
      "Epoch 37, Batch 149, LR 0.000001 Loss 4.461945, Accuracy 89.613%\n",
      "Epoch 37, Batch 150, LR 0.000001 Loss 4.457394, Accuracy 89.651%\n",
      "Epoch 37, Batch 151, LR 0.000001 Loss 4.455206, Accuracy 89.678%\n",
      "Epoch 37, Batch 152, LR 0.000001 Loss 4.455536, Accuracy 89.695%\n",
      "Epoch 37, Batch 153, LR 0.000001 Loss 4.454676, Accuracy 89.691%\n",
      "Epoch 37, Batch 154, LR 0.000001 Loss 4.451414, Accuracy 89.712%\n",
      "Epoch 37, Batch 155, LR 0.000001 Loss 4.449447, Accuracy 89.703%\n",
      "Epoch 37, Batch 156, LR 0.000001 Loss 4.447670, Accuracy 89.729%\n",
      "Epoch 37, Batch 157, LR 0.000001 Loss 4.448663, Accuracy 89.734%\n",
      "Epoch 37, Batch 158, LR 0.000001 Loss 4.445061, Accuracy 89.745%\n",
      "Epoch 37, Batch 159, LR 0.000001 Loss 4.437974, Accuracy 89.780%\n",
      "Epoch 37, Batch 160, LR 0.000001 Loss 4.439174, Accuracy 89.775%\n",
      "Epoch 37, Batch 161, LR 0.000001 Loss 4.438181, Accuracy 89.781%\n",
      "Epoch 37, Batch 162, LR 0.000001 Loss 4.440038, Accuracy 89.747%\n",
      "Epoch 37, Batch 163, LR 0.000001 Loss 4.439994, Accuracy 89.753%\n",
      "Epoch 37, Batch 164, LR 0.000001 Loss 4.441760, Accuracy 89.748%\n",
      "Epoch 37, Batch 165, LR 0.000001 Loss 4.443691, Accuracy 89.744%\n",
      "Epoch 37, Batch 166, LR 0.000001 Loss 4.448612, Accuracy 89.731%\n",
      "Epoch 37, Batch 167, LR 0.000001 Loss 4.453092, Accuracy 89.717%\n",
      "Epoch 37, Batch 168, LR 0.000001 Loss 4.448254, Accuracy 89.737%\n",
      "Epoch 37, Batch 169, LR 0.000001 Loss 4.449866, Accuracy 89.728%\n",
      "Epoch 37, Batch 170, LR 0.000001 Loss 4.450206, Accuracy 89.724%\n",
      "Epoch 37, Batch 171, LR 0.000001 Loss 4.457356, Accuracy 89.688%\n",
      "Epoch 37, Batch 172, LR 0.000001 Loss 4.454955, Accuracy 89.703%\n",
      "Epoch 37, Batch 173, LR 0.000001 Loss 4.456444, Accuracy 89.663%\n",
      "Epoch 37, Batch 174, LR 0.000001 Loss 4.451507, Accuracy 89.691%\n",
      "Epoch 37, Batch 175, LR 0.000001 Loss 4.449628, Accuracy 89.679%\n",
      "Epoch 37, Batch 176, LR 0.000001 Loss 4.447054, Accuracy 89.697%\n",
      "Epoch 37, Batch 177, LR 0.000001 Loss 4.444093, Accuracy 89.720%\n",
      "Epoch 37, Batch 178, LR 0.000001 Loss 4.444115, Accuracy 89.712%\n",
      "Epoch 37, Batch 179, LR 0.000001 Loss 4.445913, Accuracy 89.704%\n",
      "Epoch 37, Batch 180, LR 0.000001 Loss 4.445680, Accuracy 89.709%\n",
      "Epoch 37, Batch 181, LR 0.000001 Loss 4.447102, Accuracy 89.680%\n",
      "Epoch 37, Batch 182, LR 0.000001 Loss 4.448320, Accuracy 89.689%\n",
      "Epoch 37, Batch 183, LR 0.000001 Loss 4.446669, Accuracy 89.699%\n",
      "Epoch 37, Batch 184, LR 0.000001 Loss 4.448708, Accuracy 89.665%\n",
      "Epoch 37, Batch 185, LR 0.000001 Loss 4.448549, Accuracy 89.658%\n",
      "Epoch 37, Batch 186, LR 0.000001 Loss 4.445953, Accuracy 89.672%\n",
      "Epoch 37, Batch 187, LR 0.000001 Loss 4.447794, Accuracy 89.664%\n",
      "Epoch 37, Batch 188, LR 0.000001 Loss 4.445514, Accuracy 89.678%\n",
      "Epoch 37, Batch 189, LR 0.000001 Loss 4.448805, Accuracy 89.674%\n",
      "Epoch 37, Batch 190, LR 0.000001 Loss 4.451815, Accuracy 89.646%\n",
      "Epoch 37, Batch 191, LR 0.000001 Loss 4.453559, Accuracy 89.643%\n",
      "Epoch 37, Batch 192, LR 0.000001 Loss 4.455109, Accuracy 89.636%\n",
      "Epoch 37, Batch 193, LR 0.000001 Loss 4.452215, Accuracy 89.653%\n",
      "Epoch 37, Batch 194, LR 0.000001 Loss 4.447492, Accuracy 89.675%\n",
      "Epoch 37, Batch 195, LR 0.000001 Loss 4.441680, Accuracy 89.708%\n",
      "Epoch 37, Batch 196, LR 0.000001 Loss 4.437929, Accuracy 89.740%\n",
      "Epoch 37, Batch 197, LR 0.000001 Loss 4.441006, Accuracy 89.737%\n",
      "Epoch 37, Batch 198, LR 0.000001 Loss 4.439789, Accuracy 89.729%\n",
      "Epoch 37, Batch 199, LR 0.000001 Loss 4.441348, Accuracy 89.714%\n",
      "Epoch 37, Batch 200, LR 0.000001 Loss 4.442868, Accuracy 89.699%\n",
      "Epoch 37, Batch 201, LR 0.000001 Loss 4.444997, Accuracy 89.688%\n",
      "Epoch 37, Batch 202, LR 0.000001 Loss 4.442497, Accuracy 89.697%\n",
      "Epoch 37, Batch 203, LR 0.000001 Loss 4.439354, Accuracy 89.698%\n",
      "Epoch 37, Batch 204, LR 0.000001 Loss 4.436838, Accuracy 89.714%\n",
      "Epoch 37, Batch 205, LR 0.000001 Loss 4.432827, Accuracy 89.714%\n",
      "Epoch 37, Batch 206, LR 0.000001 Loss 4.434380, Accuracy 89.692%\n",
      "Epoch 37, Batch 207, LR 0.000001 Loss 4.430980, Accuracy 89.697%\n",
      "Epoch 37, Batch 208, LR 0.000001 Loss 4.431548, Accuracy 89.697%\n",
      "Epoch 37, Batch 209, LR 0.000001 Loss 4.432110, Accuracy 89.690%\n",
      "Epoch 37, Batch 210, LR 0.000001 Loss 4.430609, Accuracy 89.710%\n",
      "Epoch 37, Batch 211, LR 0.000001 Loss 4.432181, Accuracy 89.707%\n",
      "Epoch 37, Batch 212, LR 0.000001 Loss 4.433335, Accuracy 89.707%\n",
      "Epoch 37, Batch 213, LR 0.000001 Loss 4.434971, Accuracy 89.693%\n",
      "Epoch 37, Batch 214, LR 0.000001 Loss 4.433654, Accuracy 89.705%\n",
      "Epoch 37, Batch 215, LR 0.000001 Loss 4.434257, Accuracy 89.702%\n",
      "Epoch 37, Batch 216, LR 0.000001 Loss 4.434615, Accuracy 89.699%\n",
      "Epoch 37, Batch 217, LR 0.000001 Loss 4.432886, Accuracy 89.707%\n",
      "Epoch 37, Batch 218, LR 0.000001 Loss 4.431923, Accuracy 89.704%\n",
      "Epoch 37, Batch 219, LR 0.000001 Loss 4.430236, Accuracy 89.708%\n",
      "Epoch 37, Batch 220, LR 0.000001 Loss 4.430277, Accuracy 89.702%\n",
      "Epoch 37, Batch 221, LR 0.000001 Loss 4.429626, Accuracy 89.699%\n",
      "Epoch 37, Batch 222, LR 0.000001 Loss 4.427836, Accuracy 89.714%\n",
      "Epoch 37, Batch 223, LR 0.000001 Loss 4.429176, Accuracy 89.707%\n",
      "Epoch 37, Batch 224, LR 0.000001 Loss 4.427181, Accuracy 89.718%\n",
      "Epoch 37, Batch 225, LR 0.000001 Loss 4.426542, Accuracy 89.729%\n",
      "Epoch 37, Batch 226, LR 0.000001 Loss 4.427706, Accuracy 89.740%\n",
      "Epoch 37, Batch 227, LR 0.000001 Loss 4.427466, Accuracy 89.754%\n",
      "Epoch 37, Batch 228, LR 0.000001 Loss 4.424999, Accuracy 89.748%\n",
      "Epoch 37, Batch 229, LR 0.000001 Loss 4.425905, Accuracy 89.741%\n",
      "Epoch 37, Batch 230, LR 0.000001 Loss 4.426661, Accuracy 89.732%\n",
      "Epoch 37, Batch 231, LR 0.000001 Loss 4.427169, Accuracy 89.725%\n",
      "Epoch 37, Batch 232, LR 0.000001 Loss 4.427716, Accuracy 89.723%\n",
      "Epoch 37, Batch 233, LR 0.000001 Loss 4.427962, Accuracy 89.730%\n",
      "Epoch 37, Batch 234, LR 0.000001 Loss 4.428071, Accuracy 89.730%\n",
      "Epoch 37, Batch 235, LR 0.000001 Loss 4.430660, Accuracy 89.714%\n",
      "Epoch 37, Batch 236, LR 0.000001 Loss 4.431849, Accuracy 89.715%\n",
      "Epoch 37, Batch 237, LR 0.000001 Loss 4.430870, Accuracy 89.705%\n",
      "Epoch 37, Batch 238, LR 0.000001 Loss 4.433021, Accuracy 89.696%\n",
      "Epoch 37, Batch 239, LR 0.000001 Loss 4.429073, Accuracy 89.710%\n",
      "Epoch 37, Batch 240, LR 0.000001 Loss 4.430519, Accuracy 89.697%\n",
      "Epoch 37, Batch 241, LR 0.000001 Loss 4.429861, Accuracy 89.701%\n",
      "Epoch 37, Batch 242, LR 0.000001 Loss 4.435317, Accuracy 89.676%\n",
      "Epoch 37, Batch 243, LR 0.000001 Loss 4.435630, Accuracy 89.680%\n",
      "Epoch 37, Batch 244, LR 0.000001 Loss 4.436260, Accuracy 89.684%\n",
      "Epoch 37, Batch 245, LR 0.000001 Loss 4.437620, Accuracy 89.681%\n",
      "Epoch 37, Batch 246, LR 0.000001 Loss 4.435415, Accuracy 89.685%\n",
      "Epoch 37, Batch 247, LR 0.000001 Loss 4.434990, Accuracy 89.692%\n",
      "Epoch 37, Batch 248, LR 0.000001 Loss 4.436142, Accuracy 89.689%\n",
      "Epoch 37, Batch 249, LR 0.000001 Loss 4.435949, Accuracy 89.690%\n",
      "Epoch 37, Batch 250, LR 0.000001 Loss 4.435279, Accuracy 89.684%\n",
      "Epoch 37, Batch 251, LR 0.000001 Loss 4.436752, Accuracy 89.682%\n",
      "Epoch 37, Batch 252, LR 0.000001 Loss 4.436587, Accuracy 89.695%\n",
      "Epoch 37, Batch 253, LR 0.000001 Loss 4.436658, Accuracy 89.683%\n",
      "Epoch 37, Batch 254, LR 0.000001 Loss 4.435573, Accuracy 89.687%\n",
      "Epoch 37, Batch 255, LR 0.000001 Loss 4.436484, Accuracy 89.694%\n",
      "Epoch 37, Batch 256, LR 0.000001 Loss 4.437825, Accuracy 89.697%\n",
      "Epoch 37, Batch 257, LR 0.000001 Loss 4.436678, Accuracy 89.701%\n",
      "Epoch 37, Batch 258, LR 0.000001 Loss 4.437186, Accuracy 89.698%\n",
      "Epoch 37, Batch 259, LR 0.000001 Loss 4.436532, Accuracy 89.702%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 260, LR 0.000001 Loss 4.436241, Accuracy 89.700%\n",
      "Epoch 37, Batch 261, LR 0.000001 Loss 4.436210, Accuracy 89.715%\n",
      "Epoch 37, Batch 262, LR 0.000001 Loss 4.434925, Accuracy 89.730%\n",
      "Epoch 37, Batch 263, LR 0.000001 Loss 4.437079, Accuracy 89.722%\n",
      "Epoch 37, Batch 264, LR 0.000001 Loss 4.434552, Accuracy 89.731%\n",
      "Epoch 37, Batch 265, LR 0.000001 Loss 4.434987, Accuracy 89.729%\n",
      "Epoch 37, Batch 266, LR 0.000001 Loss 4.433484, Accuracy 89.747%\n",
      "Epoch 37, Batch 267, LR 0.000001 Loss 4.430830, Accuracy 89.747%\n",
      "Epoch 37, Batch 268, LR 0.000001 Loss 4.428556, Accuracy 89.756%\n",
      "Epoch 37, Batch 269, LR 0.000001 Loss 4.427682, Accuracy 89.771%\n",
      "Epoch 37, Batch 270, LR 0.000001 Loss 4.429693, Accuracy 89.771%\n",
      "Epoch 37, Batch 271, LR 0.000001 Loss 4.429668, Accuracy 89.775%\n",
      "Epoch 37, Batch 272, LR 0.000001 Loss 4.427715, Accuracy 89.775%\n",
      "Epoch 37, Batch 273, LR 0.000001 Loss 4.427554, Accuracy 89.764%\n",
      "Epoch 37, Batch 274, LR 0.000001 Loss 4.426555, Accuracy 89.772%\n",
      "Epoch 37, Batch 275, LR 0.000001 Loss 4.425625, Accuracy 89.778%\n",
      "Epoch 37, Batch 276, LR 0.000001 Loss 4.424446, Accuracy 89.790%\n",
      "Epoch 37, Batch 277, LR 0.000001 Loss 4.425383, Accuracy 89.796%\n",
      "Epoch 37, Batch 278, LR 0.000001 Loss 4.427636, Accuracy 89.799%\n",
      "Epoch 37, Batch 279, LR 0.000001 Loss 4.425846, Accuracy 89.802%\n",
      "Epoch 37, Batch 280, LR 0.000001 Loss 4.427755, Accuracy 89.788%\n",
      "Epoch 37, Batch 281, LR 0.000001 Loss 4.429779, Accuracy 89.783%\n",
      "Epoch 37, Batch 282, LR 0.000001 Loss 4.428635, Accuracy 89.780%\n",
      "Epoch 37, Batch 283, LR 0.000001 Loss 4.427495, Accuracy 89.780%\n",
      "Epoch 37, Batch 284, LR 0.000001 Loss 4.430732, Accuracy 89.764%\n",
      "Epoch 37, Batch 285, LR 0.000001 Loss 4.431425, Accuracy 89.764%\n",
      "Epoch 37, Batch 286, LR 0.000001 Loss 4.430641, Accuracy 89.773%\n",
      "Epoch 37, Batch 287, LR 0.000001 Loss 4.430676, Accuracy 89.778%\n",
      "Epoch 37, Batch 288, LR 0.000001 Loss 4.432139, Accuracy 89.768%\n",
      "Epoch 37, Batch 289, LR 0.000001 Loss 4.433099, Accuracy 89.760%\n",
      "Epoch 37, Batch 290, LR 0.000001 Loss 4.431438, Accuracy 89.768%\n",
      "Epoch 37, Batch 291, LR 0.000001 Loss 4.431061, Accuracy 89.785%\n",
      "Epoch 37, Batch 292, LR 0.000001 Loss 4.432353, Accuracy 89.785%\n",
      "Epoch 37, Batch 293, LR 0.000001 Loss 4.433527, Accuracy 89.772%\n",
      "Epoch 37, Batch 294, LR 0.000001 Loss 4.433241, Accuracy 89.772%\n",
      "Epoch 37, Batch 295, LR 0.000001 Loss 4.432869, Accuracy 89.767%\n",
      "Epoch 37, Batch 296, LR 0.000001 Loss 4.432516, Accuracy 89.778%\n",
      "Epoch 37, Batch 297, LR 0.000001 Loss 4.433616, Accuracy 89.775%\n",
      "Epoch 37, Batch 298, LR 0.000001 Loss 4.435307, Accuracy 89.765%\n",
      "Epoch 37, Batch 299, LR 0.000001 Loss 4.435010, Accuracy 89.760%\n",
      "Epoch 37, Batch 300, LR 0.000001 Loss 4.434911, Accuracy 89.766%\n",
      "Epoch 37, Batch 301, LR 0.000001 Loss 4.436346, Accuracy 89.766%\n",
      "Epoch 37, Batch 302, LR 0.000001 Loss 4.435671, Accuracy 89.779%\n",
      "Epoch 37, Batch 303, LR 0.000001 Loss 4.434620, Accuracy 89.784%\n",
      "Epoch 37, Batch 304, LR 0.000001 Loss 4.434692, Accuracy 89.774%\n",
      "Epoch 37, Batch 305, LR 0.000001 Loss 4.434964, Accuracy 89.772%\n",
      "Epoch 37, Batch 306, LR 0.000001 Loss 4.433511, Accuracy 89.782%\n",
      "Epoch 37, Batch 307, LR 0.000001 Loss 4.433097, Accuracy 89.785%\n",
      "Epoch 37, Batch 308, LR 0.000001 Loss 4.429151, Accuracy 89.806%\n",
      "Epoch 37, Batch 309, LR 0.000001 Loss 4.431278, Accuracy 89.793%\n",
      "Epoch 37, Batch 310, LR 0.000001 Loss 4.432885, Accuracy 89.796%\n",
      "Epoch 37, Batch 311, LR 0.000001 Loss 4.433883, Accuracy 89.801%\n",
      "Epoch 37, Batch 312, LR 0.000001 Loss 4.434453, Accuracy 89.801%\n",
      "Epoch 37, Batch 313, LR 0.000001 Loss 4.432021, Accuracy 89.809%\n",
      "Epoch 37, Batch 314, LR 0.000001 Loss 4.433071, Accuracy 89.801%\n",
      "Epoch 37, Batch 315, LR 0.000001 Loss 4.430706, Accuracy 89.807%\n",
      "Epoch 37, Batch 316, LR 0.000001 Loss 4.432222, Accuracy 89.799%\n",
      "Epoch 37, Batch 317, LR 0.000001 Loss 4.432629, Accuracy 89.790%\n",
      "Epoch 37, Batch 318, LR 0.000001 Loss 4.433911, Accuracy 89.782%\n",
      "Epoch 37, Batch 319, LR 0.000001 Loss 4.436657, Accuracy 89.763%\n",
      "Epoch 37, Batch 320, LR 0.000001 Loss 4.437477, Accuracy 89.766%\n",
      "Epoch 37, Batch 321, LR 0.000001 Loss 4.438192, Accuracy 89.759%\n",
      "Epoch 37, Batch 322, LR 0.000001 Loss 4.438127, Accuracy 89.754%\n",
      "Epoch 37, Batch 323, LR 0.000001 Loss 4.437861, Accuracy 89.764%\n",
      "Epoch 37, Batch 324, LR 0.000001 Loss 4.436218, Accuracy 89.764%\n",
      "Epoch 37, Batch 325, LR 0.000001 Loss 4.437686, Accuracy 89.767%\n",
      "Epoch 37, Batch 326, LR 0.000001 Loss 4.438397, Accuracy 89.755%\n",
      "Epoch 37, Batch 327, LR 0.000001 Loss 4.437437, Accuracy 89.767%\n",
      "Epoch 37, Batch 328, LR 0.000001 Loss 4.435795, Accuracy 89.772%\n",
      "Epoch 37, Batch 329, LR 0.000001 Loss 4.436472, Accuracy 89.784%\n",
      "Epoch 37, Batch 330, LR 0.000001 Loss 4.435375, Accuracy 89.796%\n",
      "Epoch 37, Batch 331, LR 0.000001 Loss 4.435413, Accuracy 89.797%\n",
      "Epoch 37, Batch 332, LR 0.000001 Loss 4.435657, Accuracy 89.790%\n",
      "Epoch 37, Batch 333, LR 0.000001 Loss 4.437992, Accuracy 89.778%\n",
      "Epoch 37, Batch 334, LR 0.000001 Loss 4.438558, Accuracy 89.767%\n",
      "Epoch 37, Batch 335, LR 0.000001 Loss 4.439869, Accuracy 89.757%\n",
      "Epoch 37, Batch 336, LR 0.000001 Loss 4.440845, Accuracy 89.760%\n",
      "Epoch 37, Batch 337, LR 0.000001 Loss 4.441086, Accuracy 89.756%\n",
      "Epoch 37, Batch 338, LR 0.000001 Loss 4.440337, Accuracy 89.767%\n",
      "Epoch 37, Batch 339, LR 0.000001 Loss 4.440760, Accuracy 89.763%\n",
      "Epoch 37, Batch 340, LR 0.000001 Loss 4.443510, Accuracy 89.752%\n",
      "Epoch 37, Batch 341, LR 0.000001 Loss 4.441703, Accuracy 89.754%\n",
      "Epoch 37, Batch 342, LR 0.000001 Loss 4.441012, Accuracy 89.757%\n",
      "Epoch 37, Batch 343, LR 0.000001 Loss 4.439882, Accuracy 89.764%\n",
      "Epoch 37, Batch 344, LR 0.000001 Loss 4.440308, Accuracy 89.764%\n",
      "Epoch 37, Batch 345, LR 0.000001 Loss 4.441204, Accuracy 89.771%\n",
      "Epoch 37, Batch 346, LR 0.000001 Loss 4.441420, Accuracy 89.778%\n",
      "Epoch 37, Batch 347, LR 0.000001 Loss 4.440621, Accuracy 89.774%\n",
      "Epoch 37, Batch 348, LR 0.000001 Loss 4.440348, Accuracy 89.765%\n",
      "Epoch 37, Batch 349, LR 0.000001 Loss 4.440520, Accuracy 89.765%\n",
      "Epoch 37, Batch 350, LR 0.000001 Loss 4.440958, Accuracy 89.759%\n",
      "Epoch 37, Batch 351, LR 0.000001 Loss 4.441834, Accuracy 89.759%\n",
      "Epoch 37, Batch 352, LR 0.000001 Loss 4.443100, Accuracy 89.748%\n",
      "Epoch 37, Batch 353, LR 0.000001 Loss 4.442411, Accuracy 89.753%\n",
      "Epoch 37, Batch 354, LR 0.000001 Loss 4.441900, Accuracy 89.751%\n",
      "Epoch 37, Batch 355, LR 0.000001 Loss 4.440627, Accuracy 89.760%\n",
      "Epoch 37, Batch 356, LR 0.000001 Loss 4.441117, Accuracy 89.763%\n",
      "Epoch 37, Batch 357, LR 0.000001 Loss 4.441502, Accuracy 89.756%\n",
      "Epoch 37, Batch 358, LR 0.000001 Loss 4.442723, Accuracy 89.737%\n",
      "Epoch 37, Batch 359, LR 0.000001 Loss 4.442281, Accuracy 89.739%\n",
      "Epoch 37, Batch 360, LR 0.000001 Loss 4.442786, Accuracy 89.735%\n",
      "Epoch 37, Batch 361, LR 0.000001 Loss 4.441583, Accuracy 89.744%\n",
      "Epoch 37, Batch 362, LR 0.000001 Loss 4.440631, Accuracy 89.736%\n",
      "Epoch 37, Batch 363, LR 0.000001 Loss 4.440473, Accuracy 89.730%\n",
      "Epoch 37, Batch 364, LR 0.000001 Loss 4.439378, Accuracy 89.732%\n",
      "Epoch 37, Batch 365, LR 0.000001 Loss 4.439856, Accuracy 89.730%\n",
      "Epoch 37, Batch 366, LR 0.000001 Loss 4.439243, Accuracy 89.737%\n",
      "Epoch 37, Batch 367, LR 0.000001 Loss 4.435991, Accuracy 89.759%\n",
      "Epoch 37, Batch 368, LR 0.000001 Loss 4.436896, Accuracy 89.763%\n",
      "Epoch 37, Batch 369, LR 0.000001 Loss 4.435055, Accuracy 89.774%\n",
      "Epoch 37, Batch 370, LR 0.000001 Loss 4.433529, Accuracy 89.780%\n",
      "Epoch 37, Batch 371, LR 0.000001 Loss 4.433660, Accuracy 89.770%\n",
      "Epoch 37, Batch 372, LR 0.000001 Loss 4.434789, Accuracy 89.762%\n",
      "Epoch 37, Batch 373, LR 0.000001 Loss 4.436212, Accuracy 89.754%\n",
      "Epoch 37, Batch 374, LR 0.000001 Loss 4.436587, Accuracy 89.752%\n",
      "Epoch 37, Batch 375, LR 0.000001 Loss 4.437075, Accuracy 89.746%\n",
      "Epoch 37, Batch 376, LR 0.000001 Loss 4.434948, Accuracy 89.759%\n",
      "Epoch 37, Batch 377, LR 0.000001 Loss 4.434112, Accuracy 89.763%\n",
      "Epoch 37, Batch 378, LR 0.000001 Loss 4.436166, Accuracy 89.757%\n",
      "Epoch 37, Batch 379, LR 0.000001 Loss 4.437623, Accuracy 89.757%\n",
      "Epoch 37, Batch 380, LR 0.000001 Loss 4.439143, Accuracy 89.745%\n",
      "Epoch 37, Batch 381, LR 0.000001 Loss 4.438738, Accuracy 89.751%\n",
      "Epoch 37, Batch 382, LR 0.000001 Loss 4.439705, Accuracy 89.744%\n",
      "Epoch 37, Batch 383, LR 0.000001 Loss 4.440128, Accuracy 89.756%\n",
      "Epoch 37, Batch 384, LR 0.000001 Loss 4.440242, Accuracy 89.756%\n",
      "Epoch 37, Batch 385, LR 0.000001 Loss 4.438531, Accuracy 89.759%\n",
      "Epoch 37, Batch 386, LR 0.000001 Loss 4.437653, Accuracy 89.761%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 387, LR 0.000001 Loss 4.438506, Accuracy 89.761%\n",
      "Epoch 37, Batch 388, LR 0.000001 Loss 4.436197, Accuracy 89.773%\n",
      "Epoch 37, Batch 389, LR 0.000001 Loss 4.437317, Accuracy 89.769%\n",
      "Epoch 37, Batch 390, LR 0.000001 Loss 4.439180, Accuracy 89.762%\n",
      "Epoch 37, Batch 391, LR 0.000001 Loss 4.438820, Accuracy 89.770%\n",
      "Epoch 37, Batch 392, LR 0.000001 Loss 4.438384, Accuracy 89.776%\n",
      "Epoch 37, Batch 393, LR 0.000001 Loss 4.437003, Accuracy 89.784%\n",
      "Epoch 37, Batch 394, LR 0.000001 Loss 4.437169, Accuracy 89.786%\n",
      "Epoch 37, Batch 395, LR 0.000001 Loss 4.438467, Accuracy 89.784%\n",
      "Epoch 37, Batch 396, LR 0.000001 Loss 4.437871, Accuracy 89.792%\n",
      "Epoch 37, Batch 397, LR 0.000001 Loss 4.438001, Accuracy 89.797%\n",
      "Epoch 37, Batch 398, LR 0.000001 Loss 4.438069, Accuracy 89.797%\n",
      "Epoch 37, Batch 399, LR 0.000001 Loss 4.437582, Accuracy 89.801%\n",
      "Epoch 37, Batch 400, LR 0.000001 Loss 4.436073, Accuracy 89.811%\n",
      "Epoch 37, Batch 401, LR 0.000001 Loss 4.434418, Accuracy 89.811%\n",
      "Epoch 37, Batch 402, LR 0.000001 Loss 4.433769, Accuracy 89.813%\n",
      "Epoch 37, Batch 403, LR 0.000001 Loss 4.434131, Accuracy 89.813%\n",
      "Epoch 37, Batch 404, LR 0.000001 Loss 4.432579, Accuracy 89.817%\n",
      "Epoch 37, Batch 405, LR 0.000001 Loss 4.431130, Accuracy 89.821%\n",
      "Epoch 37, Batch 406, LR 0.000001 Loss 4.432627, Accuracy 89.817%\n",
      "Epoch 37, Batch 407, LR 0.000001 Loss 4.433935, Accuracy 89.796%\n",
      "Epoch 37, Batch 408, LR 0.000001 Loss 4.432127, Accuracy 89.798%\n",
      "Epoch 37, Batch 409, LR 0.000001 Loss 4.432036, Accuracy 89.788%\n",
      "Epoch 37, Batch 410, LR 0.000001 Loss 4.433936, Accuracy 89.781%\n",
      "Epoch 37, Batch 411, LR 0.000001 Loss 4.434506, Accuracy 89.779%\n",
      "Epoch 37, Batch 412, LR 0.000001 Loss 4.435726, Accuracy 89.772%\n",
      "Epoch 37, Batch 413, LR 0.000001 Loss 4.436759, Accuracy 89.779%\n",
      "Epoch 37, Batch 414, LR 0.000001 Loss 4.435196, Accuracy 89.781%\n",
      "Epoch 37, Batch 415, LR 0.000001 Loss 4.435992, Accuracy 89.780%\n",
      "Epoch 37, Batch 416, LR 0.000001 Loss 4.438105, Accuracy 89.761%\n",
      "Epoch 37, Batch 417, LR 0.000001 Loss 4.439404, Accuracy 89.763%\n",
      "Epoch 37, Batch 418, LR 0.000001 Loss 4.437818, Accuracy 89.762%\n",
      "Epoch 37, Batch 419, LR 0.000001 Loss 4.437424, Accuracy 89.767%\n",
      "Epoch 37, Batch 420, LR 0.000001 Loss 4.439804, Accuracy 89.762%\n",
      "Epoch 37, Batch 421, LR 0.000001 Loss 4.439156, Accuracy 89.775%\n",
      "Epoch 37, Batch 422, LR 0.000001 Loss 4.438337, Accuracy 89.786%\n",
      "Epoch 37, Batch 423, LR 0.000001 Loss 4.437759, Accuracy 89.786%\n",
      "Epoch 37, Batch 424, LR 0.000001 Loss 4.437816, Accuracy 89.785%\n",
      "Epoch 37, Batch 425, LR 0.000001 Loss 4.437354, Accuracy 89.787%\n",
      "Epoch 37, Batch 426, LR 0.000001 Loss 4.436875, Accuracy 89.791%\n",
      "Epoch 37, Batch 427, LR 0.000001 Loss 4.437957, Accuracy 89.783%\n",
      "Epoch 37, Batch 428, LR 0.000001 Loss 4.437206, Accuracy 89.791%\n",
      "Epoch 37, Batch 429, LR 0.000001 Loss 4.436307, Accuracy 89.791%\n",
      "Epoch 37, Batch 430, LR 0.000001 Loss 4.436040, Accuracy 89.797%\n",
      "Epoch 37, Batch 431, LR 0.000001 Loss 4.436811, Accuracy 89.789%\n",
      "Epoch 37, Batch 432, LR 0.000001 Loss 4.435184, Accuracy 89.788%\n",
      "Epoch 37, Batch 433, LR 0.000001 Loss 4.434497, Accuracy 89.793%\n",
      "Epoch 37, Batch 434, LR 0.000001 Loss 4.435647, Accuracy 89.792%\n",
      "Epoch 37, Batch 435, LR 0.000001 Loss 4.436553, Accuracy 89.786%\n",
      "Epoch 37, Batch 436, LR 0.000001 Loss 4.435820, Accuracy 89.786%\n",
      "Epoch 37, Batch 437, LR 0.000001 Loss 4.434818, Accuracy 89.787%\n",
      "Epoch 37, Batch 438, LR 0.000001 Loss 4.436515, Accuracy 89.780%\n",
      "Epoch 37, Batch 439, LR 0.000001 Loss 4.435025, Accuracy 89.785%\n",
      "Epoch 37, Batch 440, LR 0.000001 Loss 4.435043, Accuracy 89.787%\n",
      "Epoch 37, Batch 441, LR 0.000001 Loss 4.435216, Accuracy 89.776%\n",
      "Epoch 37, Batch 442, LR 0.000001 Loss 4.433929, Accuracy 89.773%\n",
      "Epoch 37, Batch 443, LR 0.000001 Loss 4.433931, Accuracy 89.771%\n",
      "Epoch 37, Batch 444, LR 0.000001 Loss 4.433940, Accuracy 89.770%\n",
      "Epoch 37, Batch 445, LR 0.000001 Loss 4.433298, Accuracy 89.761%\n",
      "Epoch 37, Batch 446, LR 0.000001 Loss 4.432078, Accuracy 89.763%\n",
      "Epoch 37, Batch 447, LR 0.000001 Loss 4.431809, Accuracy 89.767%\n",
      "Epoch 37, Batch 448, LR 0.000001 Loss 4.430031, Accuracy 89.777%\n",
      "Epoch 37, Batch 449, LR 0.000001 Loss 4.430825, Accuracy 89.769%\n",
      "Epoch 37, Batch 450, LR 0.000001 Loss 4.428867, Accuracy 89.778%\n",
      "Epoch 37, Batch 451, LR 0.000001 Loss 4.428264, Accuracy 89.774%\n",
      "Epoch 37, Batch 452, LR 0.000001 Loss 4.426077, Accuracy 89.788%\n",
      "Epoch 37, Batch 453, LR 0.000001 Loss 4.426111, Accuracy 89.790%\n",
      "Epoch 37, Batch 454, LR 0.000001 Loss 4.425403, Accuracy 89.796%\n",
      "Epoch 37, Batch 455, LR 0.000001 Loss 4.424652, Accuracy 89.792%\n",
      "Epoch 37, Batch 456, LR 0.000001 Loss 4.424775, Accuracy 89.796%\n",
      "Epoch 37, Batch 457, LR 0.000001 Loss 4.423813, Accuracy 89.796%\n",
      "Epoch 37, Batch 458, LR 0.000001 Loss 4.423830, Accuracy 89.794%\n",
      "Epoch 37, Batch 459, LR 0.000001 Loss 4.423397, Accuracy 89.794%\n",
      "Epoch 37, Batch 460, LR 0.000001 Loss 4.423034, Accuracy 89.791%\n",
      "Epoch 37, Batch 461, LR 0.000001 Loss 4.422526, Accuracy 89.791%\n",
      "Epoch 37, Batch 462, LR 0.000001 Loss 4.423163, Accuracy 89.788%\n",
      "Epoch 37, Batch 463, LR 0.000001 Loss 4.424213, Accuracy 89.778%\n",
      "Epoch 37, Batch 464, LR 0.000001 Loss 4.423452, Accuracy 89.780%\n",
      "Epoch 37, Batch 465, LR 0.000001 Loss 4.423397, Accuracy 89.775%\n",
      "Epoch 37, Batch 466, LR 0.000001 Loss 4.424060, Accuracy 89.765%\n",
      "Epoch 37, Batch 467, LR 0.000001 Loss 4.423519, Accuracy 89.765%\n",
      "Epoch 37, Batch 468, LR 0.000001 Loss 4.424554, Accuracy 89.757%\n",
      "Epoch 37, Batch 469, LR 0.000001 Loss 4.425429, Accuracy 89.760%\n",
      "Epoch 37, Batch 470, LR 0.000001 Loss 4.424921, Accuracy 89.766%\n",
      "Epoch 37, Batch 471, LR 0.000001 Loss 4.425787, Accuracy 89.759%\n",
      "Epoch 37, Batch 472, LR 0.000001 Loss 4.426074, Accuracy 89.761%\n",
      "Epoch 37, Batch 473, LR 0.000001 Loss 4.424950, Accuracy 89.764%\n",
      "Epoch 37, Batch 474, LR 0.000001 Loss 4.425484, Accuracy 89.761%\n",
      "Epoch 37, Batch 475, LR 0.000001 Loss 4.426037, Accuracy 89.760%\n",
      "Epoch 37, Batch 476, LR 0.000001 Loss 4.426496, Accuracy 89.758%\n",
      "Epoch 37, Batch 477, LR 0.000001 Loss 4.425581, Accuracy 89.763%\n",
      "Epoch 37, Batch 478, LR 0.000001 Loss 4.425429, Accuracy 89.770%\n",
      "Epoch 37, Batch 479, LR 0.000001 Loss 4.424101, Accuracy 89.775%\n",
      "Epoch 37, Batch 480, LR 0.000001 Loss 4.424378, Accuracy 89.771%\n",
      "Epoch 37, Batch 481, LR 0.000001 Loss 4.423094, Accuracy 89.774%\n",
      "Epoch 37, Batch 482, LR 0.000001 Loss 4.423453, Accuracy 89.771%\n",
      "Epoch 37, Batch 483, LR 0.000001 Loss 4.422359, Accuracy 89.769%\n",
      "Epoch 37, Batch 484, LR 0.000001 Loss 4.421979, Accuracy 89.774%\n",
      "Epoch 37, Batch 485, LR 0.000001 Loss 4.422696, Accuracy 89.768%\n",
      "Epoch 37, Batch 486, LR 0.000001 Loss 4.422978, Accuracy 89.767%\n",
      "Epoch 37, Batch 487, LR 0.000001 Loss 4.422560, Accuracy 89.772%\n",
      "Epoch 37, Batch 488, LR 0.000001 Loss 4.424267, Accuracy 89.757%\n",
      "Epoch 37, Batch 489, LR 0.000001 Loss 4.423629, Accuracy 89.764%\n",
      "Epoch 37, Batch 490, LR 0.000001 Loss 4.423603, Accuracy 89.759%\n",
      "Epoch 37, Batch 491, LR 0.000001 Loss 4.423190, Accuracy 89.761%\n",
      "Epoch 37, Batch 492, LR 0.000001 Loss 4.421318, Accuracy 89.769%\n",
      "Epoch 37, Batch 493, LR 0.000001 Loss 4.422218, Accuracy 89.763%\n",
      "Epoch 37, Batch 494, LR 0.000001 Loss 4.421930, Accuracy 89.762%\n",
      "Epoch 37, Batch 495, LR 0.000001 Loss 4.422336, Accuracy 89.755%\n",
      "Epoch 37, Batch 496, LR 0.000001 Loss 4.421998, Accuracy 89.759%\n",
      "Epoch 37, Batch 497, LR 0.000001 Loss 4.421912, Accuracy 89.767%\n",
      "Epoch 37, Batch 498, LR 0.000001 Loss 4.421282, Accuracy 89.775%\n",
      "Epoch 37, Batch 499, LR 0.000001 Loss 4.423005, Accuracy 89.772%\n",
      "Epoch 37, Batch 500, LR 0.000001 Loss 4.423183, Accuracy 89.773%\n",
      "Epoch 37, Batch 501, LR 0.000001 Loss 4.422345, Accuracy 89.780%\n",
      "Epoch 37, Batch 502, LR 0.000001 Loss 4.423158, Accuracy 89.780%\n",
      "Epoch 37, Batch 503, LR 0.000001 Loss 4.423253, Accuracy 89.782%\n",
      "Epoch 37, Batch 504, LR 0.000001 Loss 4.422405, Accuracy 89.788%\n",
      "Epoch 37, Batch 505, LR 0.000001 Loss 4.421967, Accuracy 89.790%\n",
      "Epoch 37, Batch 506, LR 0.000001 Loss 4.421411, Accuracy 89.796%\n",
      "Epoch 37, Batch 507, LR 0.000001 Loss 4.422443, Accuracy 89.796%\n",
      "Epoch 37, Batch 508, LR 0.000001 Loss 4.422406, Accuracy 89.793%\n",
      "Epoch 37, Batch 509, LR 0.000001 Loss 4.420925, Accuracy 89.801%\n",
      "Epoch 37, Batch 510, LR 0.000001 Loss 4.420939, Accuracy 89.801%\n",
      "Epoch 37, Batch 511, LR 0.000001 Loss 4.421488, Accuracy 89.801%\n",
      "Epoch 37, Batch 512, LR 0.000001 Loss 4.420372, Accuracy 89.804%\n",
      "Epoch 37, Batch 513, LR 0.000001 Loss 4.419936, Accuracy 89.798%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 514, LR 0.000001 Loss 4.419740, Accuracy 89.804%\n",
      "Epoch 37, Batch 515, LR 0.000001 Loss 4.418642, Accuracy 89.813%\n",
      "Epoch 37, Batch 516, LR 0.000001 Loss 4.419339, Accuracy 89.815%\n",
      "Epoch 37, Batch 517, LR 0.000001 Loss 4.418725, Accuracy 89.815%\n",
      "Epoch 37, Batch 518, LR 0.000001 Loss 4.417824, Accuracy 89.817%\n",
      "Epoch 37, Batch 519, LR 0.000001 Loss 4.417823, Accuracy 89.817%\n",
      "Epoch 37, Batch 520, LR 0.000001 Loss 4.417421, Accuracy 89.826%\n",
      "Epoch 37, Batch 521, LR 0.000001 Loss 4.416244, Accuracy 89.833%\n",
      "Epoch 37, Batch 522, LR 0.000001 Loss 4.415471, Accuracy 89.835%\n",
      "Epoch 37, Batch 523, LR 0.000001 Loss 4.415341, Accuracy 89.835%\n",
      "Epoch 37, Batch 524, LR 0.000001 Loss 4.414905, Accuracy 89.835%\n",
      "Epoch 37, Batch 525, LR 0.000001 Loss 4.416127, Accuracy 89.823%\n",
      "Epoch 37, Batch 526, LR 0.000001 Loss 4.418729, Accuracy 89.817%\n",
      "Epoch 37, Batch 527, LR 0.000001 Loss 4.418511, Accuracy 89.819%\n",
      "Epoch 37, Batch 528, LR 0.000001 Loss 4.418089, Accuracy 89.817%\n",
      "Epoch 37, Batch 529, LR 0.000001 Loss 4.419279, Accuracy 89.811%\n",
      "Epoch 37, Batch 530, LR 0.000001 Loss 4.419581, Accuracy 89.807%\n",
      "Epoch 37, Batch 531, LR 0.000001 Loss 4.418982, Accuracy 89.811%\n",
      "Epoch 37, Batch 532, LR 0.000001 Loss 4.418633, Accuracy 89.809%\n",
      "Epoch 37, Batch 533, LR 0.000001 Loss 4.419608, Accuracy 89.809%\n",
      "Epoch 37, Batch 534, LR 0.000001 Loss 4.420365, Accuracy 89.803%\n",
      "Epoch 37, Batch 535, LR 0.000001 Loss 4.420599, Accuracy 89.806%\n",
      "Epoch 37, Batch 536, LR 0.000001 Loss 4.420744, Accuracy 89.810%\n",
      "Epoch 37, Batch 537, LR 0.000001 Loss 4.421632, Accuracy 89.803%\n",
      "Epoch 37, Batch 538, LR 0.000001 Loss 4.422651, Accuracy 89.802%\n",
      "Epoch 37, Batch 539, LR 0.000001 Loss 4.424006, Accuracy 89.796%\n",
      "Epoch 37, Batch 540, LR 0.000001 Loss 4.423388, Accuracy 89.797%\n",
      "Epoch 37, Batch 541, LR 0.000001 Loss 4.422950, Accuracy 89.793%\n",
      "Epoch 37, Batch 542, LR 0.000001 Loss 4.422745, Accuracy 89.798%\n",
      "Epoch 37, Batch 543, LR 0.000001 Loss 4.423384, Accuracy 89.792%\n",
      "Epoch 37, Batch 544, LR 0.000001 Loss 4.422438, Accuracy 89.789%\n",
      "Epoch 37, Batch 545, LR 0.000001 Loss 4.423911, Accuracy 89.775%\n",
      "Epoch 37, Batch 546, LR 0.000001 Loss 4.424092, Accuracy 89.784%\n",
      "Epoch 37, Batch 547, LR 0.000001 Loss 4.424118, Accuracy 89.788%\n",
      "Epoch 37, Batch 548, LR 0.000001 Loss 4.424802, Accuracy 89.785%\n",
      "Epoch 37, Batch 549, LR 0.000001 Loss 4.423960, Accuracy 89.788%\n",
      "Epoch 37, Batch 550, LR 0.000001 Loss 4.425548, Accuracy 89.786%\n",
      "Epoch 37, Batch 551, LR 0.000001 Loss 4.426114, Accuracy 89.780%\n",
      "Epoch 37, Batch 552, LR 0.000001 Loss 4.426456, Accuracy 89.774%\n",
      "Epoch 37, Batch 553, LR 0.000001 Loss 4.426919, Accuracy 89.767%\n",
      "Epoch 37, Batch 554, LR 0.000001 Loss 4.428267, Accuracy 89.756%\n",
      "Epoch 37, Batch 555, LR 0.000001 Loss 4.428519, Accuracy 89.756%\n",
      "Epoch 37, Batch 556, LR 0.000001 Loss 4.428265, Accuracy 89.759%\n",
      "Epoch 37, Batch 557, LR 0.000001 Loss 4.428409, Accuracy 89.757%\n",
      "Epoch 37, Batch 558, LR 0.000001 Loss 4.428081, Accuracy 89.763%\n",
      "Epoch 37, Batch 559, LR 0.000001 Loss 4.429466, Accuracy 89.760%\n",
      "Epoch 37, Batch 560, LR 0.000001 Loss 4.429431, Accuracy 89.757%\n",
      "Epoch 37, Batch 561, LR 0.000001 Loss 4.430291, Accuracy 89.749%\n",
      "Epoch 37, Batch 562, LR 0.000001 Loss 4.430973, Accuracy 89.744%\n",
      "Epoch 37, Batch 563, LR 0.000001 Loss 4.431002, Accuracy 89.747%\n",
      "Epoch 37, Batch 564, LR 0.000001 Loss 4.430694, Accuracy 89.740%\n",
      "Epoch 37, Batch 565, LR 0.000001 Loss 4.432659, Accuracy 89.728%\n",
      "Epoch 37, Batch 566, LR 0.000001 Loss 4.431986, Accuracy 89.729%\n",
      "Epoch 37, Batch 567, LR 0.000001 Loss 4.433075, Accuracy 89.724%\n",
      "Epoch 37, Batch 568, LR 0.000001 Loss 4.432834, Accuracy 89.724%\n",
      "Epoch 37, Batch 569, LR 0.000001 Loss 4.433323, Accuracy 89.717%\n",
      "Epoch 37, Batch 570, LR 0.000001 Loss 4.432635, Accuracy 89.719%\n",
      "Epoch 37, Batch 571, LR 0.000001 Loss 4.431048, Accuracy 89.725%\n",
      "Epoch 37, Batch 572, LR 0.000001 Loss 4.432015, Accuracy 89.721%\n",
      "Epoch 37, Batch 573, LR 0.000001 Loss 4.431914, Accuracy 89.722%\n",
      "Epoch 37, Batch 574, LR 0.000001 Loss 4.430483, Accuracy 89.731%\n",
      "Epoch 37, Batch 575, LR 0.000001 Loss 4.430377, Accuracy 89.732%\n",
      "Epoch 37, Batch 576, LR 0.000001 Loss 4.431221, Accuracy 89.722%\n",
      "Epoch 37, Batch 577, LR 0.000001 Loss 4.432545, Accuracy 89.718%\n",
      "Epoch 37, Batch 578, LR 0.000001 Loss 4.431072, Accuracy 89.723%\n",
      "Epoch 37, Batch 579, LR 0.000001 Loss 4.432518, Accuracy 89.726%\n",
      "Epoch 37, Batch 580, LR 0.000001 Loss 4.431755, Accuracy 89.731%\n",
      "Epoch 37, Batch 581, LR 0.000001 Loss 4.433393, Accuracy 89.716%\n",
      "Epoch 37, Batch 582, LR 0.000001 Loss 4.433316, Accuracy 89.719%\n",
      "Epoch 37, Batch 583, LR 0.000001 Loss 4.433543, Accuracy 89.724%\n",
      "Epoch 37, Batch 584, LR 0.000001 Loss 4.434583, Accuracy 89.721%\n",
      "Epoch 37, Batch 585, LR 0.000001 Loss 4.433435, Accuracy 89.729%\n",
      "Epoch 37, Batch 586, LR 0.000001 Loss 4.433231, Accuracy 89.733%\n",
      "Epoch 37, Batch 587, LR 0.000001 Loss 4.433396, Accuracy 89.731%\n",
      "Epoch 37, Batch 588, LR 0.000001 Loss 4.433165, Accuracy 89.733%\n",
      "Epoch 37, Batch 589, LR 0.000001 Loss 4.434446, Accuracy 89.731%\n",
      "Epoch 37, Batch 590, LR 0.000001 Loss 4.435420, Accuracy 89.731%\n",
      "Epoch 37, Batch 591, LR 0.000001 Loss 4.435505, Accuracy 89.733%\n",
      "Epoch 37, Batch 592, LR 0.000001 Loss 4.436478, Accuracy 89.733%\n",
      "Epoch 37, Batch 593, LR 0.000001 Loss 4.436884, Accuracy 89.732%\n",
      "Epoch 37, Batch 594, LR 0.000001 Loss 4.438664, Accuracy 89.725%\n",
      "Epoch 37, Batch 595, LR 0.000001 Loss 4.437584, Accuracy 89.727%\n",
      "Epoch 37, Batch 596, LR 0.000001 Loss 4.437326, Accuracy 89.721%\n",
      "Epoch 37, Batch 597, LR 0.000001 Loss 4.438716, Accuracy 89.718%\n",
      "Epoch 37, Batch 598, LR 0.000001 Loss 4.437398, Accuracy 89.727%\n",
      "Epoch 37, Batch 599, LR 0.000001 Loss 4.437383, Accuracy 89.730%\n",
      "Epoch 37, Batch 600, LR 0.000001 Loss 4.437619, Accuracy 89.728%\n",
      "Epoch 37, Batch 601, LR 0.000001 Loss 4.437283, Accuracy 89.728%\n",
      "Epoch 37, Batch 602, LR 0.000001 Loss 4.435708, Accuracy 89.735%\n",
      "Epoch 37, Batch 603, LR 0.000001 Loss 4.435004, Accuracy 89.739%\n",
      "Epoch 37, Batch 604, LR 0.000001 Loss 4.435363, Accuracy 89.729%\n",
      "Epoch 37, Batch 605, LR 0.000001 Loss 4.435359, Accuracy 89.729%\n",
      "Epoch 37, Batch 606, LR 0.000001 Loss 4.435357, Accuracy 89.729%\n",
      "Epoch 37, Batch 607, LR 0.000001 Loss 4.434366, Accuracy 89.734%\n",
      "Epoch 37, Batch 608, LR 0.000001 Loss 4.435088, Accuracy 89.738%\n",
      "Epoch 37, Batch 609, LR 0.000001 Loss 4.435283, Accuracy 89.740%\n",
      "Epoch 37, Batch 610, LR 0.000001 Loss 4.434683, Accuracy 89.739%\n",
      "Epoch 37, Batch 611, LR 0.000001 Loss 4.434973, Accuracy 89.734%\n",
      "Epoch 37, Batch 612, LR 0.000001 Loss 4.434337, Accuracy 89.734%\n",
      "Epoch 37, Batch 613, LR 0.000001 Loss 4.433849, Accuracy 89.741%\n",
      "Epoch 37, Batch 614, LR 0.000001 Loss 4.433597, Accuracy 89.742%\n",
      "Epoch 37, Batch 615, LR 0.000001 Loss 4.434346, Accuracy 89.741%\n",
      "Epoch 37, Batch 616, LR 0.000001 Loss 4.434061, Accuracy 89.740%\n",
      "Epoch 37, Batch 617, LR 0.000001 Loss 4.434607, Accuracy 89.744%\n",
      "Epoch 37, Batch 618, LR 0.000001 Loss 4.434008, Accuracy 89.744%\n",
      "Epoch 37, Batch 619, LR 0.000001 Loss 4.434618, Accuracy 89.744%\n",
      "Epoch 37, Batch 620, LR 0.000001 Loss 4.434622, Accuracy 89.738%\n",
      "Epoch 37, Batch 621, LR 0.000001 Loss 4.434336, Accuracy 89.743%\n",
      "Epoch 37, Batch 622, LR 0.000001 Loss 4.435667, Accuracy 89.737%\n",
      "Epoch 37, Batch 623, LR 0.000001 Loss 4.436226, Accuracy 89.733%\n",
      "Epoch 37, Batch 624, LR 0.000001 Loss 4.435410, Accuracy 89.732%\n",
      "Epoch 37, Batch 625, LR 0.000001 Loss 4.435940, Accuracy 89.734%\n",
      "Epoch 37, Batch 626, LR 0.000001 Loss 4.436277, Accuracy 89.734%\n",
      "Epoch 37, Batch 627, LR 0.000001 Loss 4.435937, Accuracy 89.742%\n",
      "Epoch 37, Batch 628, LR 0.000001 Loss 4.436403, Accuracy 89.733%\n",
      "Epoch 37, Batch 629, LR 0.000001 Loss 4.435796, Accuracy 89.734%\n",
      "Epoch 37, Batch 630, LR 0.000001 Loss 4.435614, Accuracy 89.732%\n",
      "Epoch 37, Batch 631, LR 0.000001 Loss 4.436331, Accuracy 89.734%\n",
      "Epoch 37, Batch 632, LR 0.000001 Loss 4.437041, Accuracy 89.734%\n",
      "Epoch 37, Batch 633, LR 0.000001 Loss 4.437732, Accuracy 89.728%\n",
      "Epoch 37, Batch 634, LR 0.000001 Loss 4.438527, Accuracy 89.723%\n",
      "Epoch 37, Batch 635, LR 0.000001 Loss 4.438401, Accuracy 89.722%\n",
      "Epoch 37, Batch 636, LR 0.000001 Loss 4.438446, Accuracy 89.718%\n",
      "Epoch 37, Batch 637, LR 0.000001 Loss 4.437213, Accuracy 89.726%\n",
      "Epoch 37, Batch 638, LR 0.000001 Loss 4.436723, Accuracy 89.730%\n",
      "Epoch 37, Batch 639, LR 0.000001 Loss 4.435932, Accuracy 89.730%\n",
      "Epoch 37, Batch 640, LR 0.000001 Loss 4.436611, Accuracy 89.727%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 641, LR 0.000001 Loss 4.436018, Accuracy 89.730%\n",
      "Epoch 37, Batch 642, LR 0.000001 Loss 4.435636, Accuracy 89.731%\n",
      "Epoch 37, Batch 643, LR 0.000001 Loss 4.435003, Accuracy 89.734%\n",
      "Epoch 37, Batch 644, LR 0.000001 Loss 4.434846, Accuracy 89.732%\n",
      "Epoch 37, Batch 645, LR 0.000001 Loss 4.434943, Accuracy 89.731%\n",
      "Epoch 37, Batch 646, LR 0.000001 Loss 4.434401, Accuracy 89.729%\n",
      "Epoch 37, Batch 647, LR 0.000001 Loss 4.434941, Accuracy 89.722%\n",
      "Epoch 37, Batch 648, LR 0.000001 Loss 4.434965, Accuracy 89.718%\n",
      "Epoch 37, Batch 649, LR 0.000001 Loss 4.435877, Accuracy 89.713%\n",
      "Epoch 37, Batch 650, LR 0.000001 Loss 4.436324, Accuracy 89.708%\n",
      "Epoch 37, Batch 651, LR 0.000001 Loss 4.436110, Accuracy 89.709%\n",
      "Epoch 37, Batch 652, LR 0.000001 Loss 4.435654, Accuracy 89.714%\n",
      "Epoch 37, Batch 653, LR 0.000001 Loss 4.435684, Accuracy 89.715%\n",
      "Epoch 37, Batch 654, LR 0.000001 Loss 4.436278, Accuracy 89.716%\n",
      "Epoch 37, Batch 655, LR 0.000001 Loss 4.436266, Accuracy 89.716%\n",
      "Epoch 37, Batch 656, LR 0.000001 Loss 4.435671, Accuracy 89.720%\n",
      "Epoch 37, Batch 657, LR 0.000001 Loss 4.434581, Accuracy 89.726%\n",
      "Epoch 37, Batch 658, LR 0.000001 Loss 4.433071, Accuracy 89.731%\n",
      "Epoch 37, Batch 659, LR 0.000001 Loss 4.433413, Accuracy 89.730%\n",
      "Epoch 37, Batch 660, LR 0.000001 Loss 4.433758, Accuracy 89.734%\n",
      "Epoch 37, Batch 661, LR 0.000001 Loss 4.434545, Accuracy 89.730%\n",
      "Epoch 37, Batch 662, LR 0.000001 Loss 4.434174, Accuracy 89.732%\n",
      "Epoch 37, Batch 663, LR 0.000001 Loss 4.433677, Accuracy 89.731%\n",
      "Epoch 37, Batch 664, LR 0.000001 Loss 4.433408, Accuracy 89.733%\n",
      "Epoch 37, Batch 665, LR 0.000001 Loss 4.432875, Accuracy 89.731%\n",
      "Epoch 37, Batch 666, LR 0.000001 Loss 4.432507, Accuracy 89.736%\n",
      "Epoch 37, Batch 667, LR 0.000001 Loss 4.433983, Accuracy 89.732%\n",
      "Epoch 37, Batch 668, LR 0.000001 Loss 4.433618, Accuracy 89.734%\n",
      "Epoch 37, Batch 669, LR 0.000001 Loss 4.434498, Accuracy 89.732%\n",
      "Epoch 37, Batch 670, LR 0.000001 Loss 4.434186, Accuracy 89.732%\n",
      "Epoch 37, Batch 671, LR 0.000001 Loss 4.433534, Accuracy 89.733%\n",
      "Epoch 37, Batch 672, LR 0.000001 Loss 4.433179, Accuracy 89.734%\n",
      "Epoch 37, Batch 673, LR 0.000001 Loss 4.432261, Accuracy 89.737%\n",
      "Epoch 37, Batch 674, LR 0.000001 Loss 4.432953, Accuracy 89.734%\n",
      "Epoch 37, Batch 675, LR 0.000001 Loss 4.432850, Accuracy 89.729%\n",
      "Epoch 37, Batch 676, LR 0.000001 Loss 4.434410, Accuracy 89.726%\n",
      "Epoch 37, Batch 677, LR 0.000001 Loss 4.433483, Accuracy 89.728%\n",
      "Epoch 37, Batch 678, LR 0.000001 Loss 4.433006, Accuracy 89.726%\n",
      "Epoch 37, Batch 679, LR 0.000001 Loss 4.433056, Accuracy 89.724%\n",
      "Epoch 37, Batch 680, LR 0.000001 Loss 4.432898, Accuracy 89.727%\n",
      "Epoch 37, Batch 681, LR 0.000001 Loss 4.433016, Accuracy 89.726%\n",
      "Epoch 37, Batch 682, LR 0.000001 Loss 4.434027, Accuracy 89.720%\n",
      "Epoch 37, Batch 683, LR 0.000001 Loss 4.434787, Accuracy 89.718%\n",
      "Epoch 37, Batch 684, LR 0.000001 Loss 4.434212, Accuracy 89.720%\n",
      "Epoch 37, Batch 685, LR 0.000001 Loss 4.433768, Accuracy 89.724%\n",
      "Epoch 37, Batch 686, LR 0.000001 Loss 4.434292, Accuracy 89.723%\n",
      "Epoch 37, Batch 687, LR 0.000001 Loss 4.434721, Accuracy 89.721%\n",
      "Epoch 37, Batch 688, LR 0.000001 Loss 4.434990, Accuracy 89.727%\n",
      "Epoch 37, Batch 689, LR 0.000001 Loss 4.435879, Accuracy 89.721%\n",
      "Epoch 37, Batch 690, LR 0.000001 Loss 4.436303, Accuracy 89.723%\n",
      "Epoch 37, Batch 691, LR 0.000001 Loss 4.435691, Accuracy 89.722%\n",
      "Epoch 37, Batch 692, LR 0.000001 Loss 4.437310, Accuracy 89.709%\n",
      "Epoch 37, Batch 693, LR 0.000001 Loss 4.438250, Accuracy 89.705%\n",
      "Epoch 37, Batch 694, LR 0.000001 Loss 4.438401, Accuracy 89.702%\n",
      "Epoch 37, Batch 695, LR 0.000001 Loss 4.438796, Accuracy 89.699%\n",
      "Epoch 37, Batch 696, LR 0.000001 Loss 4.439565, Accuracy 89.689%\n",
      "Epoch 37, Batch 697, LR 0.000001 Loss 4.439031, Accuracy 89.687%\n",
      "Epoch 37, Batch 698, LR 0.000001 Loss 4.438735, Accuracy 89.687%\n",
      "Epoch 37, Batch 699, LR 0.000001 Loss 4.437795, Accuracy 89.691%\n",
      "Epoch 37, Batch 700, LR 0.000001 Loss 4.436469, Accuracy 89.695%\n",
      "Epoch 37, Batch 701, LR 0.000001 Loss 4.437237, Accuracy 89.693%\n",
      "Epoch 37, Batch 702, LR 0.000001 Loss 4.436678, Accuracy 89.696%\n",
      "Epoch 37, Batch 703, LR 0.000001 Loss 4.437960, Accuracy 89.689%\n",
      "Epoch 37, Batch 704, LR 0.000001 Loss 4.438442, Accuracy 89.687%\n",
      "Epoch 37, Batch 705, LR 0.000001 Loss 4.438236, Accuracy 89.689%\n",
      "Epoch 37, Batch 706, LR 0.000001 Loss 4.437932, Accuracy 89.687%\n",
      "Epoch 37, Batch 707, LR 0.000001 Loss 4.438620, Accuracy 89.681%\n",
      "Epoch 37, Batch 708, LR 0.000001 Loss 4.437453, Accuracy 89.687%\n",
      "Epoch 37, Batch 709, LR 0.000001 Loss 4.437451, Accuracy 89.687%\n",
      "Epoch 37, Batch 710, LR 0.000001 Loss 4.436726, Accuracy 89.694%\n",
      "Epoch 37, Batch 711, LR 0.000001 Loss 4.436682, Accuracy 89.695%\n",
      "Epoch 37, Batch 712, LR 0.000001 Loss 4.436320, Accuracy 89.697%\n",
      "Epoch 37, Batch 713, LR 0.000001 Loss 4.436118, Accuracy 89.697%\n",
      "Epoch 37, Batch 714, LR 0.000001 Loss 4.436274, Accuracy 89.695%\n",
      "Epoch 37, Batch 715, LR 0.000001 Loss 4.435335, Accuracy 89.700%\n",
      "Epoch 37, Batch 716, LR 0.000001 Loss 4.436385, Accuracy 89.693%\n",
      "Epoch 37, Batch 717, LR 0.000001 Loss 4.436042, Accuracy 89.697%\n",
      "Epoch 37, Batch 718, LR 0.000001 Loss 4.437514, Accuracy 89.691%\n",
      "Epoch 37, Batch 719, LR 0.000001 Loss 4.437683, Accuracy 89.691%\n",
      "Epoch 37, Batch 720, LR 0.000001 Loss 4.437633, Accuracy 89.692%\n",
      "Epoch 37, Batch 721, LR 0.000001 Loss 4.437442, Accuracy 89.700%\n",
      "Epoch 37, Batch 722, LR 0.000001 Loss 4.437326, Accuracy 89.703%\n",
      "Epoch 37, Batch 723, LR 0.000001 Loss 4.436426, Accuracy 89.711%\n",
      "Epoch 37, Batch 724, LR 0.000001 Loss 4.436557, Accuracy 89.712%\n",
      "Epoch 37, Batch 725, LR 0.000001 Loss 4.437258, Accuracy 89.714%\n",
      "Epoch 37, Batch 726, LR 0.000001 Loss 4.437688, Accuracy 89.711%\n",
      "Epoch 37, Batch 727, LR 0.000001 Loss 4.437846, Accuracy 89.709%\n",
      "Epoch 37, Batch 728, LR 0.000001 Loss 4.438205, Accuracy 89.704%\n",
      "Epoch 37, Batch 729, LR 0.000001 Loss 4.438764, Accuracy 89.703%\n",
      "Epoch 37, Batch 730, LR 0.000001 Loss 4.439279, Accuracy 89.701%\n",
      "Epoch 37, Batch 731, LR 0.000001 Loss 4.439904, Accuracy 89.699%\n",
      "Epoch 37, Batch 732, LR 0.000001 Loss 4.439660, Accuracy 89.705%\n",
      "Epoch 37, Batch 733, LR 0.000001 Loss 4.440244, Accuracy 89.706%\n",
      "Epoch 37, Batch 734, LR 0.000001 Loss 4.439585, Accuracy 89.712%\n",
      "Epoch 37, Batch 735, LR 0.000001 Loss 4.438826, Accuracy 89.718%\n",
      "Epoch 37, Batch 736, LR 0.000001 Loss 4.438510, Accuracy 89.717%\n",
      "Epoch 37, Batch 737, LR 0.000001 Loss 4.438827, Accuracy 89.714%\n",
      "Epoch 37, Batch 738, LR 0.000001 Loss 4.438858, Accuracy 89.711%\n",
      "Epoch 37, Batch 739, LR 0.000001 Loss 4.438075, Accuracy 89.712%\n",
      "Epoch 37, Batch 740, LR 0.000001 Loss 4.438262, Accuracy 89.714%\n",
      "Epoch 37, Batch 741, LR 0.000001 Loss 4.438982, Accuracy 89.708%\n",
      "Epoch 37, Batch 742, LR 0.000001 Loss 4.439537, Accuracy 89.707%\n",
      "Epoch 37, Batch 743, LR 0.000001 Loss 4.439375, Accuracy 89.708%\n",
      "Epoch 37, Batch 744, LR 0.000001 Loss 4.438933, Accuracy 89.712%\n",
      "Epoch 37, Batch 745, LR 0.000001 Loss 4.438965, Accuracy 89.715%\n",
      "Epoch 37, Batch 746, LR 0.000001 Loss 4.438283, Accuracy 89.720%\n",
      "Epoch 37, Batch 747, LR 0.000001 Loss 4.437900, Accuracy 89.726%\n",
      "Epoch 37, Batch 748, LR 0.000001 Loss 4.437268, Accuracy 89.730%\n",
      "Epoch 37, Batch 749, LR 0.000001 Loss 4.436396, Accuracy 89.735%\n",
      "Epoch 37, Batch 750, LR 0.000001 Loss 4.437124, Accuracy 89.729%\n",
      "Epoch 37, Batch 751, LR 0.000001 Loss 4.436770, Accuracy 89.732%\n",
      "Epoch 37, Batch 752, LR 0.000001 Loss 4.436245, Accuracy 89.734%\n",
      "Epoch 37, Batch 753, LR 0.000001 Loss 4.435568, Accuracy 89.738%\n",
      "Epoch 37, Batch 754, LR 0.000001 Loss 4.435496, Accuracy 89.737%\n",
      "Epoch 37, Batch 755, LR 0.000001 Loss 4.435230, Accuracy 89.733%\n",
      "Epoch 37, Batch 756, LR 0.000001 Loss 4.434445, Accuracy 89.738%\n",
      "Epoch 37, Batch 757, LR 0.000001 Loss 4.433498, Accuracy 89.747%\n",
      "Epoch 37, Batch 758, LR 0.000001 Loss 4.433201, Accuracy 89.748%\n",
      "Epoch 37, Batch 759, LR 0.000001 Loss 4.432751, Accuracy 89.749%\n",
      "Epoch 37, Batch 760, LR 0.000001 Loss 4.433482, Accuracy 89.745%\n",
      "Epoch 37, Batch 761, LR 0.000001 Loss 4.433347, Accuracy 89.745%\n",
      "Epoch 37, Batch 762, LR 0.000001 Loss 4.434470, Accuracy 89.742%\n",
      "Epoch 37, Batch 763, LR 0.000001 Loss 4.434759, Accuracy 89.740%\n",
      "Epoch 37, Batch 764, LR 0.000001 Loss 4.433545, Accuracy 89.745%\n",
      "Epoch 37, Batch 765, LR 0.000001 Loss 4.432995, Accuracy 89.745%\n",
      "Epoch 37, Batch 766, LR 0.000001 Loss 4.432510, Accuracy 89.748%\n",
      "Epoch 37, Batch 767, LR 0.000001 Loss 4.432166, Accuracy 89.751%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 768, LR 0.000001 Loss 4.432486, Accuracy 89.753%\n",
      "Epoch 37, Batch 769, LR 0.000001 Loss 4.432785, Accuracy 89.755%\n",
      "Epoch 37, Batch 770, LR 0.000001 Loss 4.433175, Accuracy 89.753%\n",
      "Epoch 37, Batch 771, LR 0.000001 Loss 4.433389, Accuracy 89.756%\n",
      "Epoch 37, Batch 772, LR 0.000001 Loss 4.432793, Accuracy 89.760%\n",
      "Epoch 37, Batch 773, LR 0.000001 Loss 4.432405, Accuracy 89.765%\n",
      "Epoch 37, Batch 774, LR 0.000001 Loss 4.431283, Accuracy 89.772%\n",
      "Epoch 37, Batch 775, LR 0.000001 Loss 4.430783, Accuracy 89.776%\n",
      "Epoch 37, Batch 776, LR 0.000001 Loss 4.430217, Accuracy 89.777%\n",
      "Epoch 37, Batch 777, LR 0.000001 Loss 4.431325, Accuracy 89.771%\n",
      "Epoch 37, Batch 778, LR 0.000001 Loss 4.431009, Accuracy 89.775%\n",
      "Epoch 37, Batch 779, LR 0.000001 Loss 4.431127, Accuracy 89.779%\n",
      "Epoch 37, Batch 780, LR 0.000001 Loss 4.430355, Accuracy 89.782%\n",
      "Epoch 37, Batch 781, LR 0.000001 Loss 4.430179, Accuracy 89.781%\n",
      "Epoch 37, Batch 782, LR 0.000001 Loss 4.431001, Accuracy 89.776%\n",
      "Epoch 37, Batch 783, LR 0.000001 Loss 4.431550, Accuracy 89.773%\n",
      "Epoch 37, Batch 784, LR 0.000001 Loss 4.432551, Accuracy 89.764%\n",
      "Epoch 37, Batch 785, LR 0.000001 Loss 4.432015, Accuracy 89.769%\n",
      "Epoch 37, Batch 786, LR 0.000001 Loss 4.431784, Accuracy 89.762%\n",
      "Epoch 37, Batch 787, LR 0.000001 Loss 4.431945, Accuracy 89.758%\n",
      "Epoch 37, Batch 788, LR 0.000001 Loss 4.431201, Accuracy 89.761%\n",
      "Epoch 37, Batch 789, LR 0.000001 Loss 4.430772, Accuracy 89.764%\n",
      "Epoch 37, Batch 790, LR 0.000001 Loss 4.431739, Accuracy 89.760%\n",
      "Epoch 37, Batch 791, LR 0.000001 Loss 4.433162, Accuracy 89.759%\n",
      "Epoch 37, Batch 792, LR 0.000001 Loss 4.433263, Accuracy 89.756%\n",
      "Epoch 37, Batch 793, LR 0.000001 Loss 4.433194, Accuracy 89.756%\n",
      "Epoch 37, Batch 794, LR 0.000001 Loss 4.432422, Accuracy 89.757%\n",
      "Epoch 37, Batch 795, LR 0.000001 Loss 4.433447, Accuracy 89.751%\n",
      "Epoch 37, Batch 796, LR 0.000001 Loss 4.433064, Accuracy 89.752%\n",
      "Epoch 37, Batch 797, LR 0.000001 Loss 4.433396, Accuracy 89.751%\n",
      "Epoch 37, Batch 798, LR 0.000001 Loss 4.433251, Accuracy 89.747%\n",
      "Epoch 37, Batch 799, LR 0.000001 Loss 4.433939, Accuracy 89.745%\n",
      "Epoch 37, Batch 800, LR 0.000001 Loss 4.433533, Accuracy 89.750%\n",
      "Epoch 37, Batch 801, LR 0.000001 Loss 4.433217, Accuracy 89.753%\n",
      "Epoch 37, Batch 802, LR 0.000001 Loss 4.432511, Accuracy 89.759%\n",
      "Epoch 37, Batch 803, LR 0.000001 Loss 4.433089, Accuracy 89.756%\n",
      "Epoch 37, Batch 804, LR 0.000001 Loss 4.433268, Accuracy 89.755%\n",
      "Epoch 37, Batch 805, LR 0.000001 Loss 4.432632, Accuracy 89.756%\n",
      "Epoch 37, Batch 806, LR 0.000001 Loss 4.432478, Accuracy 89.756%\n",
      "Epoch 37, Batch 807, LR 0.000001 Loss 4.432345, Accuracy 89.759%\n",
      "Epoch 37, Batch 808, LR 0.000001 Loss 4.433187, Accuracy 89.752%\n",
      "Epoch 37, Batch 809, LR 0.000001 Loss 4.433479, Accuracy 89.746%\n",
      "Epoch 37, Batch 810, LR 0.000001 Loss 4.432910, Accuracy 89.747%\n",
      "Epoch 37, Batch 811, LR 0.000001 Loss 4.433181, Accuracy 89.746%\n",
      "Epoch 37, Batch 812, LR 0.000001 Loss 4.433598, Accuracy 89.748%\n",
      "Epoch 37, Batch 813, LR 0.000001 Loss 4.433852, Accuracy 89.743%\n",
      "Epoch 37, Batch 814, LR 0.000001 Loss 4.434270, Accuracy 89.741%\n",
      "Epoch 37, Batch 815, LR 0.000001 Loss 4.433776, Accuracy 89.745%\n",
      "Epoch 37, Batch 816, LR 0.000001 Loss 4.433191, Accuracy 89.749%\n",
      "Epoch 37, Batch 817, LR 0.000001 Loss 4.432833, Accuracy 89.753%\n",
      "Epoch 37, Batch 818, LR 0.000001 Loss 4.432975, Accuracy 89.752%\n",
      "Epoch 37, Batch 819, LR 0.000001 Loss 4.432181, Accuracy 89.757%\n",
      "Epoch 37, Batch 820, LR 0.000001 Loss 4.433194, Accuracy 89.752%\n",
      "Epoch 37, Batch 821, LR 0.000001 Loss 4.432702, Accuracy 89.754%\n",
      "Epoch 37, Batch 822, LR 0.000001 Loss 4.433227, Accuracy 89.752%\n",
      "Epoch 37, Batch 823, LR 0.000001 Loss 4.432831, Accuracy 89.750%\n",
      "Epoch 37, Batch 824, LR 0.000001 Loss 4.431897, Accuracy 89.751%\n",
      "Epoch 37, Batch 825, LR 0.000001 Loss 4.431897, Accuracy 89.750%\n",
      "Epoch 37, Batch 826, LR 0.000001 Loss 4.432118, Accuracy 89.746%\n",
      "Epoch 37, Batch 827, LR 0.000001 Loss 4.432232, Accuracy 89.745%\n",
      "Epoch 37, Batch 828, LR 0.000001 Loss 4.432395, Accuracy 89.744%\n",
      "Epoch 37, Batch 829, LR 0.000001 Loss 4.432152, Accuracy 89.745%\n",
      "Epoch 37, Batch 830, LR 0.000001 Loss 4.431641, Accuracy 89.750%\n",
      "Epoch 37, Batch 831, LR 0.000001 Loss 4.431549, Accuracy 89.755%\n",
      "Epoch 37, Batch 832, LR 0.000001 Loss 4.430593, Accuracy 89.759%\n",
      "Epoch 37, Batch 833, LR 0.000001 Loss 4.429717, Accuracy 89.760%\n",
      "Epoch 37, Batch 834, LR 0.000001 Loss 4.428312, Accuracy 89.760%\n",
      "Epoch 37, Batch 835, LR 0.000001 Loss 4.428363, Accuracy 89.764%\n",
      "Epoch 37, Batch 836, LR 0.000001 Loss 4.429040, Accuracy 89.764%\n",
      "Epoch 37, Batch 837, LR 0.000001 Loss 4.428960, Accuracy 89.763%\n",
      "Epoch 37, Batch 838, LR 0.000001 Loss 4.429236, Accuracy 89.763%\n",
      "Epoch 37, Batch 839, LR 0.000001 Loss 4.429654, Accuracy 89.761%\n",
      "Epoch 37, Batch 840, LR 0.000001 Loss 4.430445, Accuracy 89.759%\n",
      "Epoch 37, Batch 841, LR 0.000001 Loss 4.431015, Accuracy 89.755%\n",
      "Epoch 37, Batch 842, LR 0.000001 Loss 4.430897, Accuracy 89.756%\n",
      "Epoch 37, Batch 843, LR 0.000001 Loss 4.431512, Accuracy 89.756%\n",
      "Epoch 37, Batch 844, LR 0.000001 Loss 4.432002, Accuracy 89.754%\n",
      "Epoch 37, Batch 845, LR 0.000001 Loss 4.431972, Accuracy 89.752%\n",
      "Epoch 37, Batch 846, LR 0.000001 Loss 4.431708, Accuracy 89.751%\n",
      "Epoch 37, Batch 847, LR 0.000001 Loss 4.431239, Accuracy 89.752%\n",
      "Epoch 37, Batch 848, LR 0.000001 Loss 4.430872, Accuracy 89.753%\n",
      "Epoch 37, Batch 849, LR 0.000001 Loss 4.430428, Accuracy 89.754%\n",
      "Epoch 37, Batch 850, LR 0.000001 Loss 4.430876, Accuracy 89.749%\n",
      "Epoch 37, Batch 851, LR 0.000001 Loss 4.430837, Accuracy 89.751%\n",
      "Epoch 37, Batch 852, LR 0.000001 Loss 4.430018, Accuracy 89.756%\n",
      "Epoch 37, Batch 853, LR 0.000001 Loss 4.429997, Accuracy 89.753%\n",
      "Epoch 37, Batch 854, LR 0.000001 Loss 4.429331, Accuracy 89.755%\n",
      "Epoch 37, Batch 855, LR 0.000001 Loss 4.429557, Accuracy 89.756%\n",
      "Epoch 37, Batch 856, LR 0.000001 Loss 4.429049, Accuracy 89.761%\n",
      "Epoch 37, Batch 857, LR 0.000001 Loss 4.429436, Accuracy 89.757%\n",
      "Epoch 37, Batch 858, LR 0.000001 Loss 4.428648, Accuracy 89.761%\n",
      "Epoch 37, Batch 859, LR 0.000001 Loss 4.428805, Accuracy 89.763%\n",
      "Epoch 37, Batch 860, LR 0.000001 Loss 4.428749, Accuracy 89.760%\n",
      "Epoch 37, Batch 861, LR 0.000001 Loss 4.427649, Accuracy 89.766%\n",
      "Epoch 37, Batch 862, LR 0.000001 Loss 4.428272, Accuracy 89.761%\n",
      "Epoch 37, Batch 863, LR 0.000001 Loss 4.428345, Accuracy 89.762%\n",
      "Epoch 37, Batch 864, LR 0.000001 Loss 4.427712, Accuracy 89.767%\n",
      "Epoch 37, Batch 865, LR 0.000001 Loss 4.427543, Accuracy 89.768%\n",
      "Epoch 37, Batch 866, LR 0.000001 Loss 4.427111, Accuracy 89.767%\n",
      "Epoch 37, Batch 867, LR 0.000001 Loss 4.427023, Accuracy 89.767%\n",
      "Epoch 37, Batch 868, LR 0.000001 Loss 4.426337, Accuracy 89.767%\n",
      "Epoch 37, Batch 869, LR 0.000001 Loss 4.426007, Accuracy 89.770%\n",
      "Epoch 37, Batch 870, LR 0.000001 Loss 4.425354, Accuracy 89.771%\n",
      "Epoch 37, Batch 871, LR 0.000001 Loss 4.424859, Accuracy 89.770%\n",
      "Epoch 37, Batch 872, LR 0.000001 Loss 4.425465, Accuracy 89.766%\n",
      "Epoch 37, Batch 873, LR 0.000001 Loss 4.425544, Accuracy 89.766%\n",
      "Epoch 37, Batch 874, LR 0.000001 Loss 4.425434, Accuracy 89.764%\n",
      "Epoch 37, Batch 875, LR 0.000001 Loss 4.425346, Accuracy 89.759%\n",
      "Epoch 37, Batch 876, LR 0.000001 Loss 4.425285, Accuracy 89.763%\n",
      "Epoch 37, Batch 877, LR 0.000001 Loss 4.424704, Accuracy 89.767%\n",
      "Epoch 37, Batch 878, LR 0.000001 Loss 4.425800, Accuracy 89.760%\n",
      "Epoch 37, Batch 879, LR 0.000001 Loss 4.425210, Accuracy 89.760%\n",
      "Epoch 37, Batch 880, LR 0.000001 Loss 4.424111, Accuracy 89.764%\n",
      "Epoch 37, Batch 881, LR 0.000001 Loss 4.424524, Accuracy 89.758%\n",
      "Epoch 37, Batch 882, LR 0.000001 Loss 4.425035, Accuracy 89.758%\n",
      "Epoch 37, Batch 883, LR 0.000001 Loss 4.425029, Accuracy 89.761%\n",
      "Epoch 37, Batch 884, LR 0.000001 Loss 4.425161, Accuracy 89.761%\n",
      "Epoch 37, Batch 885, LR 0.000001 Loss 4.425643, Accuracy 89.761%\n",
      "Epoch 37, Batch 886, LR 0.000001 Loss 4.425758, Accuracy 89.760%\n",
      "Epoch 37, Batch 887, LR 0.000001 Loss 4.425973, Accuracy 89.756%\n",
      "Epoch 37, Batch 888, LR 0.000001 Loss 4.426730, Accuracy 89.753%\n",
      "Epoch 37, Batch 889, LR 0.000001 Loss 4.427205, Accuracy 89.752%\n",
      "Epoch 37, Batch 890, LR 0.000001 Loss 4.427003, Accuracy 89.754%\n",
      "Epoch 37, Batch 891, LR 0.000001 Loss 4.426058, Accuracy 89.758%\n",
      "Epoch 37, Batch 892, LR 0.000001 Loss 4.426354, Accuracy 89.758%\n",
      "Epoch 37, Batch 893, LR 0.000001 Loss 4.426444, Accuracy 89.760%\n",
      "Epoch 37, Batch 894, LR 0.000001 Loss 4.426365, Accuracy 89.762%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 895, LR 0.000001 Loss 4.425739, Accuracy 89.767%\n",
      "Epoch 37, Batch 896, LR 0.000001 Loss 4.426582, Accuracy 89.767%\n",
      "Epoch 37, Batch 897, LR 0.000001 Loss 4.425871, Accuracy 89.767%\n",
      "Epoch 37, Batch 898, LR 0.000001 Loss 4.426157, Accuracy 89.765%\n",
      "Epoch 37, Batch 899, LR 0.000001 Loss 4.426093, Accuracy 89.766%\n",
      "Epoch 37, Batch 900, LR 0.000001 Loss 4.426009, Accuracy 89.765%\n",
      "Epoch 37, Batch 901, LR 0.000001 Loss 4.425867, Accuracy 89.767%\n",
      "Epoch 37, Batch 902, LR 0.000001 Loss 4.426353, Accuracy 89.765%\n",
      "Epoch 37, Batch 903, LR 0.000001 Loss 4.427143, Accuracy 89.762%\n",
      "Epoch 37, Batch 904, LR 0.000001 Loss 4.427136, Accuracy 89.762%\n",
      "Epoch 37, Batch 905, LR 0.000001 Loss 4.427093, Accuracy 89.763%\n",
      "Epoch 37, Batch 906, LR 0.000001 Loss 4.426683, Accuracy 89.764%\n",
      "Epoch 37, Batch 907, LR 0.000001 Loss 4.425371, Accuracy 89.771%\n",
      "Epoch 37, Batch 908, LR 0.000001 Loss 4.425246, Accuracy 89.769%\n",
      "Epoch 37, Batch 909, LR 0.000001 Loss 4.425133, Accuracy 89.767%\n",
      "Epoch 37, Batch 910, LR 0.000001 Loss 4.425469, Accuracy 89.762%\n",
      "Epoch 37, Batch 911, LR 0.000001 Loss 4.425940, Accuracy 89.761%\n",
      "Epoch 37, Batch 912, LR 0.000001 Loss 4.426114, Accuracy 89.763%\n",
      "Epoch 37, Batch 913, LR 0.000001 Loss 4.425709, Accuracy 89.762%\n",
      "Epoch 37, Batch 914, LR 0.000001 Loss 4.425044, Accuracy 89.769%\n",
      "Epoch 37, Batch 915, LR 0.000001 Loss 4.425239, Accuracy 89.769%\n",
      "Epoch 37, Batch 916, LR 0.000001 Loss 4.425782, Accuracy 89.767%\n",
      "Epoch 37, Batch 917, LR 0.000001 Loss 4.425728, Accuracy 89.765%\n",
      "Epoch 37, Batch 918, LR 0.000001 Loss 4.425876, Accuracy 89.767%\n",
      "Epoch 37, Batch 919, LR 0.000001 Loss 4.425933, Accuracy 89.765%\n",
      "Epoch 37, Batch 920, LR 0.000001 Loss 4.426290, Accuracy 89.764%\n",
      "Epoch 37, Batch 921, LR 0.000001 Loss 4.426647, Accuracy 89.761%\n",
      "Epoch 37, Batch 922, LR 0.000001 Loss 4.426157, Accuracy 89.761%\n",
      "Epoch 37, Batch 923, LR 0.000001 Loss 4.426375, Accuracy 89.760%\n",
      "Epoch 37, Batch 924, LR 0.000001 Loss 4.426879, Accuracy 89.758%\n",
      "Epoch 37, Batch 925, LR 0.000001 Loss 4.427249, Accuracy 89.759%\n",
      "Epoch 37, Batch 926, LR 0.000001 Loss 4.426484, Accuracy 89.761%\n",
      "Epoch 37, Batch 927, LR 0.000001 Loss 4.426314, Accuracy 89.761%\n",
      "Epoch 37, Batch 928, LR 0.000001 Loss 4.426768, Accuracy 89.757%\n",
      "Epoch 37, Batch 929, LR 0.000001 Loss 4.427171, Accuracy 89.755%\n",
      "Epoch 37, Batch 930, LR 0.000001 Loss 4.427737, Accuracy 89.751%\n",
      "Epoch 37, Batch 931, LR 0.000001 Loss 4.427792, Accuracy 89.753%\n",
      "Epoch 37, Batch 932, LR 0.000001 Loss 4.427954, Accuracy 89.747%\n",
      "Epoch 37, Batch 933, LR 0.000001 Loss 4.428371, Accuracy 89.747%\n",
      "Epoch 37, Batch 934, LR 0.000001 Loss 4.428385, Accuracy 89.743%\n",
      "Epoch 37, Batch 935, LR 0.000001 Loss 4.427760, Accuracy 89.747%\n",
      "Epoch 37, Batch 936, LR 0.000001 Loss 4.429006, Accuracy 89.742%\n",
      "Epoch 37, Batch 937, LR 0.000001 Loss 4.429671, Accuracy 89.740%\n",
      "Epoch 37, Batch 938, LR 0.000001 Loss 4.429551, Accuracy 89.743%\n",
      "Epoch 37, Batch 939, LR 0.000001 Loss 4.429670, Accuracy 89.739%\n",
      "Epoch 37, Batch 940, LR 0.000001 Loss 4.430124, Accuracy 89.739%\n",
      "Epoch 37, Batch 941, LR 0.000001 Loss 4.430773, Accuracy 89.732%\n",
      "Epoch 37, Batch 942, LR 0.000001 Loss 4.430617, Accuracy 89.736%\n",
      "Epoch 37, Batch 943, LR 0.000001 Loss 4.430889, Accuracy 89.734%\n",
      "Epoch 37, Batch 944, LR 0.000001 Loss 4.431239, Accuracy 89.735%\n",
      "Epoch 37, Batch 945, LR 0.000001 Loss 4.431488, Accuracy 89.735%\n",
      "Epoch 37, Batch 946, LR 0.000001 Loss 4.430655, Accuracy 89.737%\n",
      "Epoch 37, Batch 947, LR 0.000001 Loss 4.429933, Accuracy 89.743%\n",
      "Epoch 37, Batch 948, LR 0.000001 Loss 4.429923, Accuracy 89.747%\n",
      "Epoch 37, Batch 949, LR 0.000001 Loss 4.430463, Accuracy 89.746%\n",
      "Epoch 37, Batch 950, LR 0.000001 Loss 4.430077, Accuracy 89.749%\n",
      "Epoch 37, Batch 951, LR 0.000001 Loss 4.429704, Accuracy 89.750%\n",
      "Epoch 37, Batch 952, LR 0.000001 Loss 4.429485, Accuracy 89.754%\n",
      "Epoch 37, Batch 953, LR 0.000001 Loss 4.429409, Accuracy 89.754%\n",
      "Epoch 37, Batch 954, LR 0.000001 Loss 4.429312, Accuracy 89.756%\n",
      "Epoch 37, Batch 955, LR 0.000001 Loss 4.430615, Accuracy 89.751%\n",
      "Epoch 37, Batch 956, LR 0.000001 Loss 4.430587, Accuracy 89.753%\n",
      "Epoch 37, Batch 957, LR 0.000001 Loss 4.430613, Accuracy 89.749%\n",
      "Epoch 37, Batch 958, LR 0.000001 Loss 4.431328, Accuracy 89.744%\n",
      "Epoch 37, Batch 959, LR 0.000001 Loss 4.431173, Accuracy 89.745%\n",
      "Epoch 37, Batch 960, LR 0.000001 Loss 4.431698, Accuracy 89.741%\n",
      "Epoch 37, Batch 961, LR 0.000001 Loss 4.431715, Accuracy 89.742%\n",
      "Epoch 37, Batch 962, LR 0.000001 Loss 4.431940, Accuracy 89.742%\n",
      "Epoch 37, Batch 963, LR 0.000001 Loss 4.432201, Accuracy 89.745%\n",
      "Epoch 37, Batch 964, LR 0.000001 Loss 4.431742, Accuracy 89.744%\n",
      "Epoch 37, Batch 965, LR 0.000001 Loss 4.432372, Accuracy 89.740%\n",
      "Epoch 37, Batch 966, LR 0.000001 Loss 4.433252, Accuracy 89.731%\n",
      "Epoch 37, Batch 967, LR 0.000001 Loss 4.432739, Accuracy 89.736%\n",
      "Epoch 37, Batch 968, LR 0.000001 Loss 4.431783, Accuracy 89.742%\n",
      "Epoch 37, Batch 969, LR 0.000001 Loss 4.432455, Accuracy 89.738%\n",
      "Epoch 37, Batch 970, LR 0.000001 Loss 4.432655, Accuracy 89.738%\n",
      "Epoch 37, Batch 971, LR 0.000001 Loss 4.433046, Accuracy 89.737%\n",
      "Epoch 37, Batch 972, LR 0.000001 Loss 4.433457, Accuracy 89.733%\n",
      "Epoch 37, Batch 973, LR 0.000001 Loss 4.433809, Accuracy 89.732%\n",
      "Epoch 37, Batch 974, LR 0.000001 Loss 4.434142, Accuracy 89.731%\n",
      "Epoch 37, Batch 975, LR 0.000001 Loss 4.433713, Accuracy 89.733%\n",
      "Epoch 37, Batch 976, LR 0.000001 Loss 4.432876, Accuracy 89.732%\n",
      "Epoch 37, Batch 977, LR 0.000001 Loss 4.433034, Accuracy 89.733%\n",
      "Epoch 37, Batch 978, LR 0.000001 Loss 4.432641, Accuracy 89.734%\n",
      "Epoch 37, Batch 979, LR 0.000001 Loss 4.432888, Accuracy 89.730%\n",
      "Epoch 37, Batch 980, LR 0.000001 Loss 4.432733, Accuracy 89.730%\n",
      "Epoch 37, Batch 981, LR 0.000001 Loss 4.432834, Accuracy 89.729%\n",
      "Epoch 37, Batch 982, LR 0.000001 Loss 4.432817, Accuracy 89.728%\n",
      "Epoch 37, Batch 983, LR 0.000001 Loss 4.433125, Accuracy 89.725%\n",
      "Epoch 37, Batch 984, LR 0.000001 Loss 4.432857, Accuracy 89.729%\n",
      "Epoch 37, Batch 985, LR 0.000001 Loss 4.433031, Accuracy 89.728%\n",
      "Epoch 37, Batch 986, LR 0.000001 Loss 4.432093, Accuracy 89.731%\n",
      "Epoch 37, Batch 987, LR 0.000001 Loss 4.431647, Accuracy 89.734%\n",
      "Epoch 37, Batch 988, LR 0.000001 Loss 4.431125, Accuracy 89.738%\n",
      "Epoch 37, Batch 989, LR 0.000001 Loss 4.430040, Accuracy 89.743%\n",
      "Epoch 37, Batch 990, LR 0.000001 Loss 4.430157, Accuracy 89.744%\n",
      "Epoch 37, Batch 991, LR 0.000001 Loss 4.429115, Accuracy 89.744%\n",
      "Epoch 37, Batch 992, LR 0.000001 Loss 4.430029, Accuracy 89.739%\n",
      "Epoch 37, Batch 993, LR 0.000001 Loss 4.429668, Accuracy 89.741%\n",
      "Epoch 37, Batch 994, LR 0.000001 Loss 4.428927, Accuracy 89.742%\n",
      "Epoch 37, Batch 995, LR 0.000001 Loss 4.429332, Accuracy 89.742%\n",
      "Epoch 37, Batch 996, LR 0.000001 Loss 4.429440, Accuracy 89.740%\n",
      "Epoch 37, Batch 997, LR 0.000001 Loss 4.428912, Accuracy 89.740%\n",
      "Epoch 37, Batch 998, LR 0.000001 Loss 4.429109, Accuracy 89.740%\n",
      "Epoch 37, Batch 999, LR 0.000001 Loss 4.428630, Accuracy 89.744%\n",
      "Epoch 37, Batch 1000, LR 0.000001 Loss 4.428951, Accuracy 89.741%\n",
      "Epoch 37, Batch 1001, LR 0.000001 Loss 4.428843, Accuracy 89.742%\n",
      "Epoch 37, Batch 1002, LR 0.000001 Loss 4.429159, Accuracy 89.736%\n",
      "Epoch 37, Batch 1003, LR 0.000001 Loss 4.428901, Accuracy 89.739%\n",
      "Epoch 37, Batch 1004, LR 0.000001 Loss 4.428771, Accuracy 89.739%\n",
      "Epoch 37, Batch 1005, LR 0.000001 Loss 4.428654, Accuracy 89.741%\n",
      "Epoch 37, Batch 1006, LR 0.000001 Loss 4.427557, Accuracy 89.741%\n",
      "Epoch 37, Batch 1007, LR 0.000001 Loss 4.427629, Accuracy 89.745%\n",
      "Epoch 37, Batch 1008, LR 0.000001 Loss 4.427489, Accuracy 89.748%\n",
      "Epoch 37, Batch 1009, LR 0.000001 Loss 4.426797, Accuracy 89.750%\n",
      "Epoch 37, Batch 1010, LR 0.000001 Loss 4.426935, Accuracy 89.752%\n",
      "Epoch 37, Batch 1011, LR 0.000001 Loss 4.426270, Accuracy 89.755%\n",
      "Epoch 37, Batch 1012, LR 0.000001 Loss 4.425962, Accuracy 89.756%\n",
      "Epoch 37, Batch 1013, LR 0.000001 Loss 4.426208, Accuracy 89.753%\n",
      "Epoch 37, Batch 1014, LR 0.000001 Loss 4.426249, Accuracy 89.754%\n",
      "Epoch 37, Batch 1015, LR 0.000001 Loss 4.426606, Accuracy 89.752%\n",
      "Epoch 37, Batch 1016, LR 0.000001 Loss 4.426339, Accuracy 89.753%\n",
      "Epoch 37, Batch 1017, LR 0.000001 Loss 4.426616, Accuracy 89.749%\n",
      "Epoch 37, Batch 1018, LR 0.000001 Loss 4.426891, Accuracy 89.748%\n",
      "Epoch 37, Batch 1019, LR 0.000001 Loss 4.426532, Accuracy 89.749%\n",
      "Epoch 37, Batch 1020, LR 0.000001 Loss 4.426599, Accuracy 89.747%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 1021, LR 0.000001 Loss 4.426247, Accuracy 89.751%\n",
      "Epoch 37, Batch 1022, LR 0.000001 Loss 4.425780, Accuracy 89.752%\n",
      "Epoch 37, Batch 1023, LR 0.000001 Loss 4.426259, Accuracy 89.750%\n",
      "Epoch 37, Batch 1024, LR 0.000001 Loss 4.426598, Accuracy 89.749%\n",
      "Epoch 37, Batch 1025, LR 0.000001 Loss 4.426400, Accuracy 89.750%\n",
      "Epoch 37, Batch 1026, LR 0.000001 Loss 4.426481, Accuracy 89.748%\n",
      "Epoch 37, Batch 1027, LR 0.000001 Loss 4.426801, Accuracy 89.747%\n",
      "Epoch 37, Batch 1028, LR 0.000001 Loss 4.426951, Accuracy 89.742%\n",
      "Epoch 37, Batch 1029, LR 0.000001 Loss 4.427236, Accuracy 89.741%\n",
      "Epoch 37, Batch 1030, LR 0.000001 Loss 4.427595, Accuracy 89.741%\n",
      "Epoch 37, Batch 1031, LR 0.000001 Loss 4.427417, Accuracy 89.741%\n",
      "Epoch 37, Batch 1032, LR 0.000001 Loss 4.427267, Accuracy 89.741%\n",
      "Epoch 37, Batch 1033, LR 0.000001 Loss 4.427014, Accuracy 89.740%\n",
      "Epoch 37, Batch 1034, LR 0.000001 Loss 4.426540, Accuracy 89.743%\n",
      "Epoch 37, Batch 1035, LR 0.000001 Loss 4.426231, Accuracy 89.745%\n",
      "Epoch 37, Batch 1036, LR 0.000001 Loss 4.426631, Accuracy 89.740%\n",
      "Epoch 37, Batch 1037, LR 0.000001 Loss 4.427573, Accuracy 89.734%\n",
      "Epoch 37, Batch 1038, LR 0.000001 Loss 4.427461, Accuracy 89.735%\n",
      "Epoch 37, Batch 1039, LR 0.000001 Loss 4.427872, Accuracy 89.732%\n",
      "Epoch 37, Batch 1040, LR 0.000001 Loss 4.427823, Accuracy 89.733%\n",
      "Epoch 37, Batch 1041, LR 0.000001 Loss 4.428086, Accuracy 89.732%\n",
      "Epoch 37, Batch 1042, LR 0.000001 Loss 4.428094, Accuracy 89.731%\n",
      "Epoch 37, Batch 1043, LR 0.000001 Loss 4.428859, Accuracy 89.728%\n",
      "Epoch 37, Batch 1044, LR 0.000001 Loss 4.429011, Accuracy 89.730%\n",
      "Epoch 37, Batch 1045, LR 0.000001 Loss 4.429937, Accuracy 89.723%\n",
      "Epoch 37, Batch 1046, LR 0.000001 Loss 4.429647, Accuracy 89.726%\n",
      "Epoch 37, Batch 1047, LR 0.000001 Loss 4.429863, Accuracy 89.726%\n",
      "Epoch 37, Loss (train set) 4.429863, Accuracy (train set) 89.726%\n",
      "Epoch 38, Batch 1, LR 0.000001 Loss 4.998216, Accuracy 87.500%\n",
      "Epoch 38, Batch 2, LR 0.000001 Loss 4.438524, Accuracy 90.625%\n",
      "Epoch 38, Batch 3, LR 0.000001 Loss 4.147452, Accuracy 91.146%\n",
      "Epoch 38, Batch 4, LR 0.000001 Loss 4.283921, Accuracy 90.039%\n",
      "Epoch 38, Batch 5, LR 0.000001 Loss 4.291781, Accuracy 90.156%\n",
      "Epoch 38, Batch 6, LR 0.000001 Loss 4.363962, Accuracy 89.583%\n",
      "Epoch 38, Batch 7, LR 0.000001 Loss 4.346864, Accuracy 89.397%\n",
      "Epoch 38, Batch 8, LR 0.000001 Loss 4.394044, Accuracy 89.258%\n",
      "Epoch 38, Batch 9, LR 0.000001 Loss 4.391441, Accuracy 89.236%\n",
      "Epoch 38, Batch 10, LR 0.000001 Loss 4.349605, Accuracy 89.766%\n",
      "Epoch 38, Batch 11, LR 0.000001 Loss 4.395875, Accuracy 89.631%\n",
      "Epoch 38, Batch 12, LR 0.000001 Loss 4.360077, Accuracy 89.844%\n",
      "Epoch 38, Batch 13, LR 0.000001 Loss 4.378033, Accuracy 89.724%\n",
      "Epoch 38, Batch 14, LR 0.000001 Loss 4.374342, Accuracy 89.732%\n",
      "Epoch 38, Batch 15, LR 0.000001 Loss 4.463171, Accuracy 89.427%\n",
      "Epoch 38, Batch 16, LR 0.000001 Loss 4.464444, Accuracy 89.648%\n",
      "Epoch 38, Batch 17, LR 0.000001 Loss 4.460073, Accuracy 89.706%\n",
      "Epoch 38, Batch 18, LR 0.000001 Loss 4.514389, Accuracy 89.757%\n",
      "Epoch 38, Batch 19, LR 0.000001 Loss 4.515038, Accuracy 89.844%\n",
      "Epoch 38, Batch 20, LR 0.000001 Loss 4.520276, Accuracy 89.883%\n",
      "Epoch 38, Batch 21, LR 0.000001 Loss 4.533879, Accuracy 89.844%\n",
      "Epoch 38, Batch 22, LR 0.000001 Loss 4.537255, Accuracy 89.702%\n",
      "Epoch 38, Batch 23, LR 0.000001 Loss 4.555192, Accuracy 89.674%\n",
      "Epoch 38, Batch 24, LR 0.000001 Loss 4.557521, Accuracy 89.746%\n",
      "Epoch 38, Batch 25, LR 0.000001 Loss 4.552777, Accuracy 89.688%\n",
      "Epoch 38, Batch 26, LR 0.000001 Loss 4.558723, Accuracy 89.573%\n",
      "Epoch 38, Batch 27, LR 0.000001 Loss 4.579942, Accuracy 89.381%\n",
      "Epoch 38, Batch 28, LR 0.000001 Loss 4.566603, Accuracy 89.314%\n",
      "Epoch 38, Batch 29, LR 0.000001 Loss 4.553908, Accuracy 89.332%\n",
      "Epoch 38, Batch 30, LR 0.000001 Loss 4.540328, Accuracy 89.375%\n",
      "Epoch 38, Batch 31, LR 0.000001 Loss 4.528063, Accuracy 89.315%\n",
      "Epoch 38, Batch 32, LR 0.000001 Loss 4.541860, Accuracy 89.233%\n",
      "Epoch 38, Batch 33, LR 0.000001 Loss 4.518387, Accuracy 89.394%\n",
      "Epoch 38, Batch 34, LR 0.000001 Loss 4.519434, Accuracy 89.361%\n",
      "Epoch 38, Batch 35, LR 0.000001 Loss 4.529372, Accuracy 89.353%\n",
      "Epoch 38, Batch 36, LR 0.000001 Loss 4.532442, Accuracy 89.345%\n",
      "Epoch 38, Batch 37, LR 0.000001 Loss 4.537806, Accuracy 89.210%\n",
      "Epoch 38, Batch 38, LR 0.000001 Loss 4.523984, Accuracy 89.268%\n",
      "Epoch 38, Batch 39, LR 0.000001 Loss 4.501260, Accuracy 89.363%\n",
      "Epoch 38, Batch 40, LR 0.000001 Loss 4.497615, Accuracy 89.414%\n",
      "Epoch 38, Batch 41, LR 0.000001 Loss 4.489974, Accuracy 89.520%\n",
      "Epoch 38, Batch 42, LR 0.000001 Loss 4.485579, Accuracy 89.509%\n",
      "Epoch 38, Batch 43, LR 0.000001 Loss 4.472211, Accuracy 89.680%\n",
      "Epoch 38, Batch 44, LR 0.000001 Loss 4.461625, Accuracy 89.737%\n",
      "Epoch 38, Batch 45, LR 0.000001 Loss 4.460476, Accuracy 89.809%\n",
      "Epoch 38, Batch 46, LR 0.000001 Loss 4.460913, Accuracy 89.810%\n",
      "Epoch 38, Batch 47, LR 0.000001 Loss 4.465290, Accuracy 89.777%\n",
      "Epoch 38, Batch 48, LR 0.000001 Loss 4.465136, Accuracy 89.811%\n",
      "Epoch 38, Batch 49, LR 0.000001 Loss 4.458327, Accuracy 89.844%\n",
      "Epoch 38, Batch 50, LR 0.000001 Loss 4.460376, Accuracy 89.797%\n",
      "Epoch 38, Batch 51, LR 0.000001 Loss 4.466213, Accuracy 89.767%\n",
      "Epoch 38, Batch 52, LR 0.000001 Loss 4.457838, Accuracy 89.859%\n",
      "Epoch 38, Batch 53, LR 0.000001 Loss 4.465819, Accuracy 89.844%\n",
      "Epoch 38, Batch 54, LR 0.000001 Loss 4.464481, Accuracy 89.786%\n",
      "Epoch 38, Batch 55, LR 0.000001 Loss 4.471626, Accuracy 89.787%\n",
      "Epoch 38, Batch 56, LR 0.000001 Loss 4.485357, Accuracy 89.746%\n",
      "Epoch 38, Batch 57, LR 0.000001 Loss 4.493377, Accuracy 89.762%\n",
      "Epoch 38, Batch 58, LR 0.000001 Loss 4.492431, Accuracy 89.776%\n",
      "Epoch 38, Batch 59, LR 0.000001 Loss 4.478447, Accuracy 89.857%\n",
      "Epoch 38, Batch 60, LR 0.000001 Loss 4.471566, Accuracy 89.922%\n",
      "Epoch 38, Batch 61, LR 0.000001 Loss 4.466805, Accuracy 89.882%\n",
      "Epoch 38, Batch 62, LR 0.000001 Loss 4.472483, Accuracy 89.882%\n",
      "Epoch 38, Batch 63, LR 0.000001 Loss 4.468863, Accuracy 89.955%\n",
      "Epoch 38, Batch 64, LR 0.000001 Loss 4.477617, Accuracy 89.917%\n",
      "Epoch 38, Batch 65, LR 0.000001 Loss 4.483700, Accuracy 89.928%\n",
      "Epoch 38, Batch 66, LR 0.000001 Loss 4.493790, Accuracy 89.915%\n",
      "Epoch 38, Batch 67, LR 0.000001 Loss 4.497073, Accuracy 89.914%\n",
      "Epoch 38, Batch 68, LR 0.000001 Loss 4.494723, Accuracy 89.890%\n",
      "Epoch 38, Batch 69, LR 0.000001 Loss 4.494634, Accuracy 89.900%\n",
      "Epoch 38, Batch 70, LR 0.000001 Loss 4.493453, Accuracy 89.888%\n",
      "Epoch 38, Batch 71, LR 0.000001 Loss 4.489726, Accuracy 89.921%\n",
      "Epoch 38, Batch 72, LR 0.000001 Loss 4.486642, Accuracy 89.952%\n",
      "Epoch 38, Batch 73, LR 0.000001 Loss 4.485751, Accuracy 89.929%\n",
      "Epoch 38, Batch 74, LR 0.000001 Loss 4.485308, Accuracy 89.960%\n",
      "Epoch 38, Batch 75, LR 0.000001 Loss 4.475314, Accuracy 90.010%\n",
      "Epoch 38, Batch 76, LR 0.000001 Loss 4.478506, Accuracy 90.008%\n",
      "Epoch 38, Batch 77, LR 0.000001 Loss 4.470918, Accuracy 90.006%\n",
      "Epoch 38, Batch 78, LR 0.000001 Loss 4.460569, Accuracy 90.014%\n",
      "Epoch 38, Batch 79, LR 0.000001 Loss 4.460381, Accuracy 90.022%\n",
      "Epoch 38, Batch 80, LR 0.000001 Loss 4.455465, Accuracy 90.039%\n",
      "Epoch 38, Batch 81, LR 0.000001 Loss 4.453738, Accuracy 90.046%\n",
      "Epoch 38, Batch 82, LR 0.000001 Loss 4.456735, Accuracy 90.063%\n",
      "Epoch 38, Batch 83, LR 0.000001 Loss 4.448556, Accuracy 90.079%\n",
      "Epoch 38, Batch 84, LR 0.000001 Loss 4.447917, Accuracy 90.095%\n",
      "Epoch 38, Batch 85, LR 0.000001 Loss 4.449155, Accuracy 90.101%\n",
      "Epoch 38, Batch 86, LR 0.000001 Loss 4.457218, Accuracy 90.035%\n",
      "Epoch 38, Batch 87, LR 0.000001 Loss 4.455703, Accuracy 90.014%\n",
      "Epoch 38, Batch 88, LR 0.000001 Loss 4.462925, Accuracy 89.977%\n",
      "Epoch 38, Batch 89, LR 0.000001 Loss 4.456322, Accuracy 90.019%\n",
      "Epoch 38, Batch 90, LR 0.000001 Loss 4.454418, Accuracy 90.052%\n",
      "Epoch 38, Batch 91, LR 0.000001 Loss 4.451726, Accuracy 90.084%\n",
      "Epoch 38, Batch 92, LR 0.000001 Loss 4.459072, Accuracy 90.082%\n",
      "Epoch 38, Batch 93, LR 0.000001 Loss 4.459523, Accuracy 90.087%\n",
      "Epoch 38, Batch 94, LR 0.000001 Loss 4.459278, Accuracy 90.093%\n",
      "Epoch 38, Batch 95, LR 0.000001 Loss 4.466472, Accuracy 90.082%\n",
      "Epoch 38, Batch 96, LR 0.000001 Loss 4.462160, Accuracy 90.096%\n",
      "Epoch 38, Batch 97, LR 0.000001 Loss 4.462990, Accuracy 90.126%\n",
      "Epoch 38, Batch 98, LR 0.000001 Loss 4.466589, Accuracy 90.051%\n",
      "Epoch 38, Batch 99, LR 0.000001 Loss 4.468352, Accuracy 90.073%\n",
      "Epoch 38, Batch 100, LR 0.000001 Loss 4.470125, Accuracy 90.055%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 101, LR 0.000001 Loss 4.467213, Accuracy 90.076%\n",
      "Epoch 38, Batch 102, LR 0.000001 Loss 4.473969, Accuracy 90.074%\n",
      "Epoch 38, Batch 103, LR 0.000001 Loss 4.466918, Accuracy 90.109%\n",
      "Epoch 38, Batch 104, LR 0.000001 Loss 4.470917, Accuracy 90.069%\n",
      "Epoch 38, Batch 105, LR 0.000001 Loss 4.479826, Accuracy 90.045%\n",
      "Epoch 38, Batch 106, LR 0.000001 Loss 4.488011, Accuracy 90.006%\n",
      "Epoch 38, Batch 107, LR 0.000001 Loss 4.489877, Accuracy 90.019%\n",
      "Epoch 38, Batch 108, LR 0.000001 Loss 4.490683, Accuracy 90.025%\n",
      "Epoch 38, Batch 109, LR 0.000001 Loss 4.498810, Accuracy 89.930%\n",
      "Epoch 38, Batch 110, LR 0.000001 Loss 4.499304, Accuracy 89.908%\n",
      "Epoch 38, Batch 111, LR 0.000001 Loss 4.496086, Accuracy 89.921%\n",
      "Epoch 38, Batch 112, LR 0.000001 Loss 4.500984, Accuracy 89.893%\n",
      "Epoch 38, Batch 113, LR 0.000001 Loss 4.501609, Accuracy 89.934%\n",
      "Epoch 38, Batch 114, LR 0.000001 Loss 4.503240, Accuracy 89.905%\n",
      "Epoch 38, Batch 115, LR 0.000001 Loss 4.495814, Accuracy 89.932%\n",
      "Epoch 38, Batch 116, LR 0.000001 Loss 4.496809, Accuracy 89.904%\n",
      "Epoch 38, Batch 117, LR 0.000001 Loss 4.491838, Accuracy 89.917%\n",
      "Epoch 38, Batch 118, LR 0.000001 Loss 4.488952, Accuracy 89.930%\n",
      "Epoch 38, Batch 119, LR 0.000001 Loss 4.492074, Accuracy 89.916%\n",
      "Epoch 38, Batch 120, LR 0.000001 Loss 4.490211, Accuracy 89.922%\n",
      "Epoch 38, Batch 121, LR 0.000001 Loss 4.487758, Accuracy 89.934%\n",
      "Epoch 38, Batch 122, LR 0.000001 Loss 4.489499, Accuracy 89.940%\n",
      "Epoch 38, Batch 123, LR 0.000001 Loss 4.489485, Accuracy 89.971%\n",
      "Epoch 38, Batch 124, LR 0.000001 Loss 4.483001, Accuracy 89.982%\n",
      "Epoch 38, Batch 125, LR 0.000001 Loss 4.486161, Accuracy 89.969%\n",
      "Epoch 38, Batch 126, LR 0.000001 Loss 4.489073, Accuracy 89.968%\n",
      "Epoch 38, Batch 127, LR 0.000001 Loss 4.484822, Accuracy 89.979%\n",
      "Epoch 38, Batch 128, LR 0.000001 Loss 4.490487, Accuracy 89.954%\n",
      "Epoch 38, Batch 129, LR 0.000001 Loss 4.484962, Accuracy 89.977%\n",
      "Epoch 38, Batch 130, LR 0.000001 Loss 4.485118, Accuracy 89.988%\n",
      "Epoch 38, Batch 131, LR 0.000001 Loss 4.485587, Accuracy 89.987%\n",
      "Epoch 38, Batch 132, LR 0.000001 Loss 4.488250, Accuracy 89.974%\n",
      "Epoch 38, Batch 133, LR 0.000001 Loss 4.486982, Accuracy 89.967%\n",
      "Epoch 38, Batch 134, LR 0.000001 Loss 4.479404, Accuracy 89.984%\n",
      "Epoch 38, Batch 135, LR 0.000001 Loss 4.476338, Accuracy 89.983%\n",
      "Epoch 38, Batch 136, LR 0.000001 Loss 4.476152, Accuracy 89.993%\n",
      "Epoch 38, Batch 137, LR 0.000001 Loss 4.476750, Accuracy 89.986%\n",
      "Epoch 38, Batch 138, LR 0.000001 Loss 4.478095, Accuracy 89.985%\n",
      "Epoch 38, Batch 139, LR 0.000001 Loss 4.475696, Accuracy 89.990%\n",
      "Epoch 38, Batch 140, LR 0.000001 Loss 4.477977, Accuracy 90.006%\n",
      "Epoch 38, Batch 141, LR 0.000001 Loss 4.472419, Accuracy 90.016%\n",
      "Epoch 38, Batch 142, LR 0.000001 Loss 4.469499, Accuracy 90.058%\n",
      "Epoch 38, Batch 143, LR 0.000001 Loss 4.468592, Accuracy 90.057%\n",
      "Epoch 38, Batch 144, LR 0.000001 Loss 4.470760, Accuracy 90.023%\n",
      "Epoch 38, Batch 145, LR 0.000001 Loss 4.469102, Accuracy 90.005%\n",
      "Epoch 38, Batch 146, LR 0.000001 Loss 4.474006, Accuracy 89.994%\n",
      "Epoch 38, Batch 147, LR 0.000001 Loss 4.474335, Accuracy 89.987%\n",
      "Epoch 38, Batch 148, LR 0.000001 Loss 4.475284, Accuracy 89.981%\n",
      "Epoch 38, Batch 149, LR 0.000001 Loss 4.473037, Accuracy 90.006%\n",
      "Epoch 38, Batch 150, LR 0.000001 Loss 4.477423, Accuracy 90.000%\n",
      "Epoch 38, Batch 151, LR 0.000001 Loss 4.481696, Accuracy 89.968%\n",
      "Epoch 38, Batch 152, LR 0.000001 Loss 4.481124, Accuracy 89.988%\n",
      "Epoch 38, Batch 153, LR 0.000001 Loss 4.485123, Accuracy 89.966%\n",
      "Epoch 38, Batch 154, LR 0.000001 Loss 4.481056, Accuracy 89.986%\n",
      "Epoch 38, Batch 155, LR 0.000001 Loss 4.480285, Accuracy 89.975%\n",
      "Epoch 38, Batch 156, LR 0.000001 Loss 4.481163, Accuracy 89.974%\n",
      "Epoch 38, Batch 157, LR 0.000001 Loss 4.475439, Accuracy 89.993%\n",
      "Epoch 38, Batch 158, LR 0.000001 Loss 4.476974, Accuracy 89.972%\n",
      "Epoch 38, Batch 159, LR 0.000001 Loss 4.480994, Accuracy 89.947%\n",
      "Epoch 38, Batch 160, LR 0.000001 Loss 4.483272, Accuracy 89.946%\n",
      "Epoch 38, Batch 161, LR 0.000001 Loss 4.481008, Accuracy 89.946%\n",
      "Epoch 38, Batch 162, LR 0.000001 Loss 4.478123, Accuracy 89.964%\n",
      "Epoch 38, Batch 163, LR 0.000001 Loss 4.475748, Accuracy 89.964%\n",
      "Epoch 38, Batch 164, LR 0.000001 Loss 4.474337, Accuracy 89.953%\n",
      "Epoch 38, Batch 165, LR 0.000001 Loss 4.470235, Accuracy 89.981%\n",
      "Epoch 38, Batch 166, LR 0.000001 Loss 4.470000, Accuracy 89.976%\n",
      "Epoch 38, Batch 167, LR 0.000001 Loss 4.473079, Accuracy 89.979%\n",
      "Epoch 38, Batch 168, LR 0.000001 Loss 4.468093, Accuracy 90.011%\n",
      "Epoch 38, Batch 169, LR 0.000001 Loss 4.467152, Accuracy 90.010%\n",
      "Epoch 38, Batch 170, LR 0.000001 Loss 4.467560, Accuracy 90.023%\n",
      "Epoch 38, Batch 171, LR 0.000001 Loss 4.465507, Accuracy 90.036%\n",
      "Epoch 38, Batch 172, LR 0.000001 Loss 4.462397, Accuracy 90.062%\n",
      "Epoch 38, Batch 173, LR 0.000001 Loss 4.467987, Accuracy 90.047%\n",
      "Epoch 38, Batch 174, LR 0.000001 Loss 4.463202, Accuracy 90.059%\n",
      "Epoch 38, Batch 175, LR 0.000001 Loss 4.462273, Accuracy 90.062%\n",
      "Epoch 38, Batch 176, LR 0.000001 Loss 4.458530, Accuracy 90.066%\n",
      "Epoch 38, Batch 177, LR 0.000001 Loss 4.460046, Accuracy 90.051%\n",
      "Epoch 38, Batch 178, LR 0.000001 Loss 4.459803, Accuracy 90.054%\n",
      "Epoch 38, Batch 179, LR 0.000001 Loss 4.459629, Accuracy 90.053%\n",
      "Epoch 38, Batch 180, LR 0.000001 Loss 4.457314, Accuracy 90.078%\n",
      "Epoch 38, Batch 181, LR 0.000001 Loss 4.457025, Accuracy 90.094%\n",
      "Epoch 38, Batch 182, LR 0.000001 Loss 4.456389, Accuracy 90.084%\n",
      "Epoch 38, Batch 183, LR 0.000001 Loss 4.451862, Accuracy 90.096%\n",
      "Epoch 38, Batch 184, LR 0.000001 Loss 4.454134, Accuracy 90.099%\n",
      "Epoch 38, Batch 185, LR 0.000001 Loss 4.452964, Accuracy 90.101%\n",
      "Epoch 38, Batch 186, LR 0.000001 Loss 4.450643, Accuracy 90.117%\n",
      "Epoch 38, Batch 187, LR 0.000001 Loss 4.447533, Accuracy 90.124%\n",
      "Epoch 38, Batch 188, LR 0.000001 Loss 4.444248, Accuracy 90.135%\n",
      "Epoch 38, Batch 189, LR 0.000001 Loss 4.445315, Accuracy 90.117%\n",
      "Epoch 38, Batch 190, LR 0.000001 Loss 4.443619, Accuracy 90.119%\n",
      "Epoch 38, Batch 191, LR 0.000001 Loss 4.441435, Accuracy 90.126%\n",
      "Epoch 38, Batch 192, LR 0.000001 Loss 4.442105, Accuracy 90.120%\n",
      "Epoch 38, Batch 193, LR 0.000001 Loss 4.442351, Accuracy 90.111%\n",
      "Epoch 38, Batch 194, LR 0.000001 Loss 4.436583, Accuracy 90.126%\n",
      "Epoch 38, Batch 195, LR 0.000001 Loss 4.437478, Accuracy 90.116%\n",
      "Epoch 38, Batch 196, LR 0.000001 Loss 4.436773, Accuracy 90.111%\n",
      "Epoch 38, Batch 197, LR 0.000001 Loss 4.438583, Accuracy 90.102%\n",
      "Epoch 38, Batch 198, LR 0.000001 Loss 4.440351, Accuracy 90.080%\n",
      "Epoch 38, Batch 199, LR 0.000001 Loss 4.442093, Accuracy 90.064%\n",
      "Epoch 38, Batch 200, LR 0.000001 Loss 4.444250, Accuracy 90.043%\n",
      "Epoch 38, Batch 201, LR 0.000001 Loss 4.445335, Accuracy 90.023%\n",
      "Epoch 38, Batch 202, LR 0.000001 Loss 4.448509, Accuracy 90.006%\n",
      "Epoch 38, Batch 203, LR 0.000001 Loss 4.448591, Accuracy 90.021%\n",
      "Epoch 38, Batch 204, LR 0.000001 Loss 4.445880, Accuracy 90.028%\n",
      "Epoch 38, Batch 205, LR 0.000001 Loss 4.449043, Accuracy 90.004%\n",
      "Epoch 38, Batch 206, LR 0.000001 Loss 4.453119, Accuracy 89.976%\n",
      "Epoch 38, Batch 207, LR 0.000001 Loss 4.458817, Accuracy 89.953%\n",
      "Epoch 38, Batch 208, LR 0.000001 Loss 4.461616, Accuracy 89.923%\n",
      "Epoch 38, Batch 209, LR 0.000001 Loss 4.461483, Accuracy 89.915%\n",
      "Epoch 38, Batch 210, LR 0.000001 Loss 4.459918, Accuracy 89.933%\n",
      "Epoch 38, Batch 211, LR 0.000001 Loss 4.459456, Accuracy 89.940%\n",
      "Epoch 38, Batch 212, LR 0.000001 Loss 4.459541, Accuracy 89.940%\n",
      "Epoch 38, Batch 213, LR 0.000001 Loss 4.457384, Accuracy 89.946%\n",
      "Epoch 38, Batch 214, LR 0.000001 Loss 4.458808, Accuracy 89.939%\n",
      "Epoch 38, Batch 215, LR 0.000001 Loss 4.457257, Accuracy 89.942%\n",
      "Epoch 38, Batch 216, LR 0.000001 Loss 4.456626, Accuracy 89.949%\n",
      "Epoch 38, Batch 217, LR 0.000001 Loss 4.456733, Accuracy 89.941%\n",
      "Epoch 38, Batch 218, LR 0.000001 Loss 4.455416, Accuracy 89.955%\n",
      "Epoch 38, Batch 219, LR 0.000001 Loss 4.454554, Accuracy 89.961%\n",
      "Epoch 38, Batch 220, LR 0.000001 Loss 4.454217, Accuracy 89.972%\n",
      "Epoch 38, Batch 221, LR 0.000001 Loss 4.450890, Accuracy 89.985%\n",
      "Epoch 38, Batch 222, LR 0.000001 Loss 4.450163, Accuracy 90.002%\n",
      "Epoch 38, Batch 223, LR 0.000001 Loss 4.448624, Accuracy 90.008%\n",
      "Epoch 38, Batch 224, LR 0.000001 Loss 4.447921, Accuracy 90.022%\n",
      "Epoch 38, Batch 225, LR 0.000001 Loss 4.445601, Accuracy 90.024%\n",
      "Epoch 38, Batch 226, LR 0.000001 Loss 4.444308, Accuracy 90.020%\n",
      "Epoch 38, Batch 227, LR 0.000001 Loss 4.444105, Accuracy 90.012%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 228, LR 0.000001 Loss 4.445448, Accuracy 89.988%\n",
      "Epoch 38, Batch 229, LR 0.000001 Loss 4.447221, Accuracy 89.987%\n",
      "Epoch 38, Batch 230, LR 0.000001 Loss 4.446779, Accuracy 90.000%\n",
      "Epoch 38, Batch 231, LR 0.000001 Loss 4.449999, Accuracy 89.982%\n",
      "Epoch 38, Batch 232, LR 0.000001 Loss 4.449100, Accuracy 89.978%\n",
      "Epoch 38, Batch 233, LR 0.000001 Loss 4.447819, Accuracy 89.981%\n",
      "Epoch 38, Batch 234, LR 0.000001 Loss 4.447800, Accuracy 89.984%\n",
      "Epoch 38, Batch 235, LR 0.000001 Loss 4.446030, Accuracy 89.983%\n",
      "Epoch 38, Batch 236, LR 0.000001 Loss 4.446547, Accuracy 89.976%\n",
      "Epoch 38, Batch 237, LR 0.000001 Loss 4.445372, Accuracy 89.976%\n",
      "Epoch 38, Batch 238, LR 0.000001 Loss 4.447950, Accuracy 89.965%\n",
      "Epoch 38, Batch 239, LR 0.000001 Loss 4.446264, Accuracy 89.958%\n",
      "Epoch 38, Batch 240, LR 0.000001 Loss 4.446665, Accuracy 89.954%\n",
      "Epoch 38, Batch 241, LR 0.000001 Loss 4.447155, Accuracy 89.947%\n",
      "Epoch 38, Batch 242, LR 0.000001 Loss 4.443984, Accuracy 89.957%\n",
      "Epoch 38, Batch 243, LR 0.000001 Loss 4.446212, Accuracy 89.943%\n",
      "Epoch 38, Batch 244, LR 0.000001 Loss 4.445408, Accuracy 89.946%\n",
      "Epoch 38, Batch 245, LR 0.000001 Loss 4.447389, Accuracy 89.952%\n",
      "Epoch 38, Batch 246, LR 0.000001 Loss 4.448446, Accuracy 89.945%\n",
      "Epoch 38, Batch 247, LR 0.000001 Loss 4.451206, Accuracy 89.939%\n",
      "Epoch 38, Batch 248, LR 0.000001 Loss 4.452362, Accuracy 89.929%\n",
      "Epoch 38, Batch 249, LR 0.000001 Loss 4.450019, Accuracy 89.944%\n",
      "Epoch 38, Batch 250, LR 0.000001 Loss 4.450795, Accuracy 89.941%\n",
      "Epoch 38, Batch 251, LR 0.000001 Loss 4.451684, Accuracy 89.934%\n",
      "Epoch 38, Batch 252, LR 0.000001 Loss 4.447115, Accuracy 89.949%\n",
      "Epoch 38, Batch 253, LR 0.000001 Loss 4.446095, Accuracy 89.946%\n",
      "Epoch 38, Batch 254, LR 0.000001 Loss 4.446481, Accuracy 89.942%\n",
      "Epoch 38, Batch 255, LR 0.000001 Loss 4.448023, Accuracy 89.948%\n",
      "Epoch 38, Batch 256, LR 0.000001 Loss 4.447101, Accuracy 89.963%\n",
      "Epoch 38, Batch 257, LR 0.000001 Loss 4.446522, Accuracy 89.968%\n",
      "Epoch 38, Batch 258, LR 0.000001 Loss 4.449156, Accuracy 89.950%\n",
      "Epoch 38, Batch 259, LR 0.000001 Loss 4.447417, Accuracy 89.967%\n",
      "Epoch 38, Batch 260, LR 0.000001 Loss 4.447513, Accuracy 89.967%\n",
      "Epoch 38, Batch 261, LR 0.000001 Loss 4.449550, Accuracy 89.955%\n",
      "Epoch 38, Batch 262, LR 0.000001 Loss 4.447514, Accuracy 89.966%\n",
      "Epoch 38, Batch 263, LR 0.000001 Loss 4.448141, Accuracy 89.954%\n",
      "Epoch 38, Batch 264, LR 0.000001 Loss 4.447512, Accuracy 89.953%\n",
      "Epoch 38, Batch 265, LR 0.000001 Loss 4.443367, Accuracy 89.973%\n",
      "Epoch 38, Batch 266, LR 0.000001 Loss 4.442668, Accuracy 89.961%\n",
      "Epoch 38, Batch 267, LR 0.000001 Loss 4.444924, Accuracy 89.952%\n",
      "Epoch 38, Batch 268, LR 0.000001 Loss 4.443607, Accuracy 89.957%\n",
      "Epoch 38, Batch 269, LR 0.000001 Loss 4.442121, Accuracy 89.957%\n",
      "Epoch 38, Batch 270, LR 0.000001 Loss 4.442698, Accuracy 89.957%\n",
      "Epoch 38, Batch 271, LR 0.000001 Loss 4.440781, Accuracy 89.962%\n",
      "Epoch 38, Batch 272, LR 0.000001 Loss 4.442733, Accuracy 89.956%\n",
      "Epoch 38, Batch 273, LR 0.000001 Loss 4.441779, Accuracy 89.947%\n",
      "Epoch 38, Batch 274, LR 0.000001 Loss 4.441897, Accuracy 89.932%\n",
      "Epoch 38, Batch 275, LR 0.000001 Loss 4.443080, Accuracy 89.915%\n",
      "Epoch 38, Batch 276, LR 0.000001 Loss 4.442103, Accuracy 89.929%\n",
      "Epoch 38, Batch 277, LR 0.000001 Loss 4.443357, Accuracy 89.914%\n",
      "Epoch 38, Batch 278, LR 0.000001 Loss 4.444599, Accuracy 89.894%\n",
      "Epoch 38, Batch 279, LR 0.000001 Loss 4.442812, Accuracy 89.903%\n",
      "Epoch 38, Batch 280, LR 0.000001 Loss 4.443334, Accuracy 89.902%\n",
      "Epoch 38, Batch 281, LR 0.000001 Loss 4.443697, Accuracy 89.897%\n",
      "Epoch 38, Batch 282, LR 0.000001 Loss 4.446148, Accuracy 89.891%\n",
      "Epoch 38, Batch 283, LR 0.000001 Loss 4.449993, Accuracy 89.869%\n",
      "Epoch 38, Batch 284, LR 0.000001 Loss 4.452360, Accuracy 89.860%\n",
      "Epoch 38, Batch 285, LR 0.000001 Loss 4.454584, Accuracy 89.855%\n",
      "Epoch 38, Batch 286, LR 0.000001 Loss 4.453095, Accuracy 89.863%\n",
      "Epoch 38, Batch 287, LR 0.000001 Loss 4.453895, Accuracy 89.855%\n",
      "Epoch 38, Batch 288, LR 0.000001 Loss 4.453247, Accuracy 89.841%\n",
      "Epoch 38, Batch 289, LR 0.000001 Loss 4.451922, Accuracy 89.849%\n",
      "Epoch 38, Batch 290, LR 0.000001 Loss 4.451980, Accuracy 89.844%\n",
      "Epoch 38, Batch 291, LR 0.000001 Loss 4.452785, Accuracy 89.846%\n",
      "Epoch 38, Batch 292, LR 0.000001 Loss 4.454440, Accuracy 89.846%\n",
      "Epoch 38, Batch 293, LR 0.000001 Loss 4.455475, Accuracy 89.849%\n",
      "Epoch 38, Batch 294, LR 0.000001 Loss 4.456742, Accuracy 89.852%\n",
      "Epoch 38, Batch 295, LR 0.000001 Loss 4.455556, Accuracy 89.862%\n",
      "Epoch 38, Batch 296, LR 0.000001 Loss 4.454876, Accuracy 89.865%\n",
      "Epoch 38, Batch 297, LR 0.000001 Loss 4.454873, Accuracy 89.862%\n",
      "Epoch 38, Batch 298, LR 0.000001 Loss 4.453388, Accuracy 89.867%\n",
      "Epoch 38, Batch 299, LR 0.000001 Loss 4.453629, Accuracy 89.867%\n",
      "Epoch 38, Batch 300, LR 0.000001 Loss 4.454343, Accuracy 89.857%\n",
      "Epoch 38, Batch 301, LR 0.000001 Loss 4.453068, Accuracy 89.859%\n",
      "Epoch 38, Batch 302, LR 0.000001 Loss 4.456124, Accuracy 89.844%\n",
      "Epoch 38, Batch 303, LR 0.000001 Loss 4.455789, Accuracy 89.857%\n",
      "Epoch 38, Batch 304, LR 0.000000 Loss 4.456642, Accuracy 89.857%\n",
      "Epoch 38, Batch 305, LR 0.000000 Loss 4.456072, Accuracy 89.857%\n",
      "Epoch 38, Batch 306, LR 0.000000 Loss 4.455255, Accuracy 89.854%\n",
      "Epoch 38, Batch 307, LR 0.000000 Loss 4.455606, Accuracy 89.856%\n",
      "Epoch 38, Batch 308, LR 0.000000 Loss 4.454610, Accuracy 89.859%\n",
      "Epoch 38, Batch 309, LR 0.000000 Loss 4.457005, Accuracy 89.856%\n",
      "Epoch 38, Batch 310, LR 0.000000 Loss 4.457743, Accuracy 89.844%\n",
      "Epoch 38, Batch 311, LR 0.000000 Loss 4.456992, Accuracy 89.844%\n",
      "Epoch 38, Batch 312, LR 0.000000 Loss 4.456505, Accuracy 89.844%\n",
      "Epoch 38, Batch 313, LR 0.000000 Loss 4.455371, Accuracy 89.841%\n",
      "Epoch 38, Batch 314, LR 0.000000 Loss 4.453207, Accuracy 89.841%\n",
      "Epoch 38, Batch 315, LR 0.000000 Loss 4.453382, Accuracy 89.831%\n",
      "Epoch 38, Batch 316, LR 0.000000 Loss 4.453446, Accuracy 89.836%\n",
      "Epoch 38, Batch 317, LR 0.000000 Loss 4.455237, Accuracy 89.826%\n",
      "Epoch 38, Batch 318, LR 0.000000 Loss 4.454346, Accuracy 89.829%\n",
      "Epoch 38, Batch 319, LR 0.000000 Loss 4.452242, Accuracy 89.832%\n",
      "Epoch 38, Batch 320, LR 0.000000 Loss 4.451609, Accuracy 89.834%\n",
      "Epoch 38, Batch 321, LR 0.000000 Loss 4.450067, Accuracy 89.844%\n",
      "Epoch 38, Batch 322, LR 0.000000 Loss 4.450605, Accuracy 89.844%\n",
      "Epoch 38, Batch 323, LR 0.000000 Loss 4.449730, Accuracy 89.851%\n",
      "Epoch 38, Batch 324, LR 0.000000 Loss 4.450070, Accuracy 89.841%\n",
      "Epoch 38, Batch 325, LR 0.000000 Loss 4.449855, Accuracy 89.837%\n",
      "Epoch 38, Batch 326, LR 0.000000 Loss 4.448421, Accuracy 89.851%\n",
      "Epoch 38, Batch 327, LR 0.000000 Loss 4.448191, Accuracy 89.856%\n",
      "Epoch 38, Batch 328, LR 0.000000 Loss 4.445447, Accuracy 89.860%\n",
      "Epoch 38, Batch 329, LR 0.000000 Loss 4.445449, Accuracy 89.856%\n",
      "Epoch 38, Batch 330, LR 0.000000 Loss 4.445907, Accuracy 89.846%\n",
      "Epoch 38, Batch 331, LR 0.000000 Loss 4.445317, Accuracy 89.846%\n",
      "Epoch 38, Batch 332, LR 0.000000 Loss 4.446171, Accuracy 89.839%\n",
      "Epoch 38, Batch 333, LR 0.000000 Loss 4.447480, Accuracy 89.832%\n",
      "Epoch 38, Batch 334, LR 0.000000 Loss 4.446828, Accuracy 89.834%\n",
      "Epoch 38, Batch 335, LR 0.000000 Loss 4.447071, Accuracy 89.832%\n",
      "Epoch 38, Batch 336, LR 0.000000 Loss 4.445885, Accuracy 89.844%\n",
      "Epoch 38, Batch 337, LR 0.000000 Loss 4.446167, Accuracy 89.855%\n",
      "Epoch 38, Batch 338, LR 0.000000 Loss 4.446360, Accuracy 89.851%\n",
      "Epoch 38, Batch 339, LR 0.000000 Loss 4.449064, Accuracy 89.832%\n",
      "Epoch 38, Batch 340, LR 0.000000 Loss 4.450735, Accuracy 89.825%\n",
      "Epoch 38, Batch 341, LR 0.000000 Loss 4.451988, Accuracy 89.828%\n",
      "Epoch 38, Batch 342, LR 0.000000 Loss 4.452656, Accuracy 89.821%\n",
      "Epoch 38, Batch 343, LR 0.000000 Loss 4.453067, Accuracy 89.823%\n",
      "Epoch 38, Batch 344, LR 0.000000 Loss 4.452279, Accuracy 89.821%\n",
      "Epoch 38, Batch 345, LR 0.000000 Loss 4.452913, Accuracy 89.826%\n",
      "Epoch 38, Batch 346, LR 0.000000 Loss 4.453869, Accuracy 89.826%\n",
      "Epoch 38, Batch 347, LR 0.000000 Loss 4.453814, Accuracy 89.837%\n",
      "Epoch 38, Batch 348, LR 0.000000 Loss 4.454465, Accuracy 89.835%\n",
      "Epoch 38, Batch 349, LR 0.000000 Loss 4.453593, Accuracy 89.842%\n",
      "Epoch 38, Batch 350, LR 0.000000 Loss 4.453555, Accuracy 89.850%\n",
      "Epoch 38, Batch 351, LR 0.000000 Loss 4.453762, Accuracy 89.846%\n",
      "Epoch 38, Batch 352, LR 0.000000 Loss 4.453360, Accuracy 89.844%\n",
      "Epoch 38, Batch 353, LR 0.000000 Loss 4.455030, Accuracy 89.846%\n",
      "Epoch 38, Batch 354, LR 0.000000 Loss 4.453259, Accuracy 89.861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 355, LR 0.000000 Loss 4.453737, Accuracy 89.861%\n",
      "Epoch 38, Batch 356, LR 0.000000 Loss 4.454082, Accuracy 89.857%\n",
      "Epoch 38, Batch 357, LR 0.000000 Loss 4.453714, Accuracy 89.861%\n",
      "Epoch 38, Batch 358, LR 0.000000 Loss 4.451767, Accuracy 89.876%\n",
      "Epoch 38, Batch 359, LR 0.000000 Loss 4.451967, Accuracy 89.876%\n",
      "Epoch 38, Batch 360, LR 0.000000 Loss 4.452501, Accuracy 89.876%\n",
      "Epoch 38, Batch 361, LR 0.000000 Loss 4.452318, Accuracy 89.876%\n",
      "Epoch 38, Batch 362, LR 0.000000 Loss 4.452196, Accuracy 89.878%\n",
      "Epoch 38, Batch 363, LR 0.000000 Loss 4.452318, Accuracy 89.872%\n",
      "Epoch 38, Batch 364, LR 0.000000 Loss 4.451668, Accuracy 89.867%\n",
      "Epoch 38, Batch 365, LR 0.000000 Loss 4.450325, Accuracy 89.872%\n",
      "Epoch 38, Batch 366, LR 0.000000 Loss 4.450453, Accuracy 89.874%\n",
      "Epoch 38, Batch 367, LR 0.000000 Loss 4.449313, Accuracy 89.880%\n",
      "Epoch 38, Batch 368, LR 0.000000 Loss 4.448127, Accuracy 89.880%\n",
      "Epoch 38, Batch 369, LR 0.000000 Loss 4.449025, Accuracy 89.871%\n",
      "Epoch 38, Batch 370, LR 0.000000 Loss 4.448560, Accuracy 89.871%\n",
      "Epoch 38, Batch 371, LR 0.000000 Loss 4.447064, Accuracy 89.890%\n",
      "Epoch 38, Batch 372, LR 0.000000 Loss 4.446981, Accuracy 89.892%\n",
      "Epoch 38, Batch 373, LR 0.000000 Loss 4.448802, Accuracy 89.881%\n",
      "Epoch 38, Batch 374, LR 0.000000 Loss 4.448587, Accuracy 89.881%\n",
      "Epoch 38, Batch 375, LR 0.000000 Loss 4.449660, Accuracy 89.875%\n",
      "Epoch 38, Batch 376, LR 0.000000 Loss 4.446552, Accuracy 89.883%\n",
      "Epoch 38, Batch 377, LR 0.000000 Loss 4.445966, Accuracy 89.891%\n",
      "Epoch 38, Batch 378, LR 0.000000 Loss 4.446840, Accuracy 89.891%\n",
      "Epoch 38, Batch 379, LR 0.000000 Loss 4.447497, Accuracy 89.897%\n",
      "Epoch 38, Batch 380, LR 0.000000 Loss 4.444877, Accuracy 89.910%\n",
      "Epoch 38, Batch 381, LR 0.000000 Loss 4.444714, Accuracy 89.907%\n",
      "Epoch 38, Batch 382, LR 0.000000 Loss 4.444168, Accuracy 89.909%\n",
      "Epoch 38, Batch 383, LR 0.000000 Loss 4.444804, Accuracy 89.909%\n",
      "Epoch 38, Batch 384, LR 0.000000 Loss 4.444444, Accuracy 89.913%\n",
      "Epoch 38, Batch 385, LR 0.000000 Loss 4.444761, Accuracy 89.907%\n",
      "Epoch 38, Batch 386, LR 0.000000 Loss 4.442257, Accuracy 89.911%\n",
      "Epoch 38, Batch 387, LR 0.000000 Loss 4.441886, Accuracy 89.904%\n",
      "Epoch 38, Batch 388, LR 0.000000 Loss 4.442078, Accuracy 89.908%\n",
      "Epoch 38, Batch 389, LR 0.000000 Loss 4.439769, Accuracy 89.920%\n",
      "Epoch 38, Batch 390, LR 0.000000 Loss 4.439322, Accuracy 89.926%\n",
      "Epoch 38, Batch 391, LR 0.000000 Loss 4.436734, Accuracy 89.930%\n",
      "Epoch 38, Batch 392, LR 0.000000 Loss 4.437410, Accuracy 89.929%\n",
      "Epoch 38, Batch 393, LR 0.000000 Loss 4.437465, Accuracy 89.925%\n",
      "Epoch 38, Batch 394, LR 0.000000 Loss 4.435822, Accuracy 89.937%\n",
      "Epoch 38, Batch 395, LR 0.000000 Loss 4.438549, Accuracy 89.915%\n",
      "Epoch 38, Batch 396, LR 0.000000 Loss 4.439209, Accuracy 89.921%\n",
      "Epoch 38, Batch 397, LR 0.000000 Loss 4.440011, Accuracy 89.920%\n",
      "Epoch 38, Batch 398, LR 0.000000 Loss 4.438825, Accuracy 89.924%\n",
      "Epoch 38, Batch 399, LR 0.000000 Loss 4.438597, Accuracy 89.910%\n",
      "Epoch 38, Batch 400, LR 0.000000 Loss 4.439800, Accuracy 89.898%\n",
      "Epoch 38, Batch 401, LR 0.000000 Loss 4.437337, Accuracy 89.916%\n",
      "Epoch 38, Batch 402, LR 0.000000 Loss 4.434994, Accuracy 89.927%\n",
      "Epoch 38, Batch 403, LR 0.000000 Loss 4.434397, Accuracy 89.933%\n",
      "Epoch 38, Batch 404, LR 0.000000 Loss 4.432833, Accuracy 89.948%\n",
      "Epoch 38, Batch 405, LR 0.000000 Loss 4.431869, Accuracy 89.950%\n",
      "Epoch 38, Batch 406, LR 0.000000 Loss 4.430519, Accuracy 89.955%\n",
      "Epoch 38, Batch 407, LR 0.000000 Loss 4.431144, Accuracy 89.957%\n",
      "Epoch 38, Batch 408, LR 0.000000 Loss 4.431153, Accuracy 89.957%\n",
      "Epoch 38, Batch 409, LR 0.000000 Loss 4.431964, Accuracy 89.960%\n",
      "Epoch 38, Batch 410, LR 0.000000 Loss 4.432322, Accuracy 89.956%\n",
      "Epoch 38, Batch 411, LR 0.000000 Loss 4.432283, Accuracy 89.956%\n",
      "Epoch 38, Batch 412, LR 0.000000 Loss 4.429516, Accuracy 89.963%\n",
      "Epoch 38, Batch 413, LR 0.000000 Loss 4.431415, Accuracy 89.955%\n",
      "Epoch 38, Batch 414, LR 0.000000 Loss 4.432199, Accuracy 89.942%\n",
      "Epoch 38, Batch 415, LR 0.000000 Loss 4.430110, Accuracy 89.953%\n",
      "Epoch 38, Batch 416, LR 0.000000 Loss 4.428384, Accuracy 89.958%\n",
      "Epoch 38, Batch 417, LR 0.000000 Loss 4.428273, Accuracy 89.964%\n",
      "Epoch 38, Batch 418, LR 0.000000 Loss 4.428481, Accuracy 89.958%\n",
      "Epoch 38, Batch 419, LR 0.000000 Loss 4.427971, Accuracy 89.946%\n",
      "Epoch 38, Batch 420, LR 0.000000 Loss 4.429019, Accuracy 89.942%\n",
      "Epoch 38, Batch 421, LR 0.000000 Loss 4.430210, Accuracy 89.942%\n",
      "Epoch 38, Batch 422, LR 0.000000 Loss 4.431595, Accuracy 89.936%\n",
      "Epoch 38, Batch 423, LR 0.000000 Loss 4.430477, Accuracy 89.942%\n",
      "Epoch 38, Batch 424, LR 0.000000 Loss 4.428606, Accuracy 89.949%\n",
      "Epoch 38, Batch 425, LR 0.000000 Loss 4.428499, Accuracy 89.943%\n",
      "Epoch 38, Batch 426, LR 0.000000 Loss 4.428405, Accuracy 89.941%\n",
      "Epoch 38, Batch 427, LR 0.000000 Loss 4.428090, Accuracy 89.948%\n",
      "Epoch 38, Batch 428, LR 0.000000 Loss 4.427552, Accuracy 89.951%\n",
      "Epoch 38, Batch 429, LR 0.000000 Loss 4.426800, Accuracy 89.958%\n",
      "Epoch 38, Batch 430, LR 0.000000 Loss 4.427727, Accuracy 89.949%\n",
      "Epoch 38, Batch 431, LR 0.000000 Loss 4.427601, Accuracy 89.953%\n",
      "Epoch 38, Batch 432, LR 0.000000 Loss 4.428097, Accuracy 89.950%\n",
      "Epoch 38, Batch 433, LR 0.000000 Loss 4.426772, Accuracy 89.961%\n",
      "Epoch 38, Batch 434, LR 0.000000 Loss 4.427149, Accuracy 89.959%\n",
      "Epoch 38, Batch 435, LR 0.000000 Loss 4.425842, Accuracy 89.966%\n",
      "Epoch 38, Batch 436, LR 0.000000 Loss 4.425345, Accuracy 89.967%\n",
      "Epoch 38, Batch 437, LR 0.000000 Loss 4.424409, Accuracy 89.967%\n",
      "Epoch 38, Batch 438, LR 0.000000 Loss 4.423830, Accuracy 89.974%\n",
      "Epoch 38, Batch 439, LR 0.000000 Loss 4.422972, Accuracy 89.970%\n",
      "Epoch 38, Batch 440, LR 0.000000 Loss 4.422052, Accuracy 89.979%\n",
      "Epoch 38, Batch 441, LR 0.000000 Loss 4.421847, Accuracy 89.977%\n",
      "Epoch 38, Batch 442, LR 0.000000 Loss 4.420533, Accuracy 89.978%\n",
      "Epoch 38, Batch 443, LR 0.000000 Loss 4.419179, Accuracy 89.985%\n",
      "Epoch 38, Batch 444, LR 0.000000 Loss 4.418484, Accuracy 89.986%\n",
      "Epoch 38, Batch 445, LR 0.000000 Loss 4.419013, Accuracy 89.981%\n",
      "Epoch 38, Batch 446, LR 0.000000 Loss 4.418093, Accuracy 89.989%\n",
      "Epoch 38, Batch 447, LR 0.000000 Loss 4.417915, Accuracy 89.982%\n",
      "Epoch 38, Batch 448, LR 0.000000 Loss 4.418746, Accuracy 89.978%\n",
      "Epoch 38, Batch 449, LR 0.000000 Loss 4.418931, Accuracy 89.979%\n",
      "Epoch 38, Batch 450, LR 0.000000 Loss 4.418765, Accuracy 89.972%\n",
      "Epoch 38, Batch 451, LR 0.000000 Loss 4.420027, Accuracy 89.968%\n",
      "Epoch 38, Batch 452, LR 0.000000 Loss 4.420801, Accuracy 89.958%\n",
      "Epoch 38, Batch 453, LR 0.000000 Loss 4.419564, Accuracy 89.966%\n",
      "Epoch 38, Batch 454, LR 0.000000 Loss 4.417971, Accuracy 89.975%\n",
      "Epoch 38, Batch 455, LR 0.000000 Loss 4.416284, Accuracy 89.981%\n",
      "Epoch 38, Batch 456, LR 0.000000 Loss 4.416390, Accuracy 89.981%\n",
      "Epoch 38, Batch 457, LR 0.000000 Loss 4.417754, Accuracy 89.972%\n",
      "Epoch 38, Batch 458, LR 0.000000 Loss 4.418901, Accuracy 89.972%\n",
      "Epoch 38, Batch 459, LR 0.000000 Loss 4.419399, Accuracy 89.971%\n",
      "Epoch 38, Batch 460, LR 0.000000 Loss 4.419593, Accuracy 89.980%\n",
      "Epoch 38, Batch 461, LR 0.000000 Loss 4.419781, Accuracy 89.979%\n",
      "Epoch 38, Batch 462, LR 0.000000 Loss 4.419476, Accuracy 89.981%\n",
      "Epoch 38, Batch 463, LR 0.000000 Loss 4.420483, Accuracy 89.980%\n",
      "Epoch 38, Batch 464, LR 0.000000 Loss 4.421236, Accuracy 89.985%\n",
      "Epoch 38, Batch 465, LR 0.000000 Loss 4.421313, Accuracy 89.976%\n",
      "Epoch 38, Batch 466, LR 0.000000 Loss 4.422495, Accuracy 89.964%\n",
      "Epoch 38, Batch 467, LR 0.000000 Loss 4.420814, Accuracy 89.964%\n",
      "Epoch 38, Batch 468, LR 0.000000 Loss 4.421259, Accuracy 89.964%\n",
      "Epoch 38, Batch 469, LR 0.000000 Loss 4.423033, Accuracy 89.949%\n",
      "Epoch 38, Batch 470, LR 0.000000 Loss 4.423018, Accuracy 89.947%\n",
      "Epoch 38, Batch 471, LR 0.000000 Loss 4.424687, Accuracy 89.938%\n",
      "Epoch 38, Batch 472, LR 0.000000 Loss 4.425194, Accuracy 89.940%\n",
      "Epoch 38, Batch 473, LR 0.000000 Loss 4.423236, Accuracy 89.951%\n",
      "Epoch 38, Batch 474, LR 0.000000 Loss 4.423520, Accuracy 89.954%\n",
      "Epoch 38, Batch 475, LR 0.000000 Loss 4.422486, Accuracy 89.961%\n",
      "Epoch 38, Batch 476, LR 0.000000 Loss 4.422278, Accuracy 89.964%\n",
      "Epoch 38, Batch 477, LR 0.000000 Loss 4.423284, Accuracy 89.962%\n",
      "Epoch 38, Batch 478, LR 0.000000 Loss 4.423578, Accuracy 89.960%\n",
      "Epoch 38, Batch 479, LR 0.000000 Loss 4.424013, Accuracy 89.951%\n",
      "Epoch 38, Batch 480, LR 0.000000 Loss 4.423850, Accuracy 89.956%\n",
      "Epoch 38, Batch 481, LR 0.000000 Loss 4.423440, Accuracy 89.956%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 482, LR 0.000000 Loss 4.424261, Accuracy 89.944%\n",
      "Epoch 38, Batch 483, LR 0.000000 Loss 4.424725, Accuracy 89.946%\n",
      "Epoch 38, Batch 484, LR 0.000000 Loss 4.425023, Accuracy 89.945%\n",
      "Epoch 38, Batch 485, LR 0.000000 Loss 4.424121, Accuracy 89.950%\n",
      "Epoch 38, Batch 486, LR 0.000000 Loss 4.424101, Accuracy 89.950%\n",
      "Epoch 38, Batch 487, LR 0.000000 Loss 4.425402, Accuracy 89.948%\n",
      "Epoch 38, Batch 488, LR 0.000000 Loss 4.423232, Accuracy 89.956%\n",
      "Epoch 38, Batch 489, LR 0.000000 Loss 4.423493, Accuracy 89.956%\n",
      "Epoch 38, Batch 490, LR 0.000000 Loss 4.423586, Accuracy 89.955%\n",
      "Epoch 38, Batch 491, LR 0.000000 Loss 4.424183, Accuracy 89.949%\n",
      "Epoch 38, Batch 492, LR 0.000000 Loss 4.424491, Accuracy 89.941%\n",
      "Epoch 38, Batch 493, LR 0.000000 Loss 4.424914, Accuracy 89.940%\n",
      "Epoch 38, Batch 494, LR 0.000000 Loss 4.423674, Accuracy 89.948%\n",
      "Epoch 38, Batch 495, LR 0.000000 Loss 4.423049, Accuracy 89.954%\n",
      "Epoch 38, Batch 496, LR 0.000000 Loss 4.422328, Accuracy 89.957%\n",
      "Epoch 38, Batch 497, LR 0.000000 Loss 4.422248, Accuracy 89.960%\n",
      "Epoch 38, Batch 498, LR 0.000000 Loss 4.423113, Accuracy 89.954%\n",
      "Epoch 38, Batch 499, LR 0.000000 Loss 4.422873, Accuracy 89.956%\n",
      "Epoch 38, Batch 500, LR 0.000000 Loss 4.421668, Accuracy 89.966%\n",
      "Epoch 38, Batch 501, LR 0.000000 Loss 4.422573, Accuracy 89.961%\n",
      "Epoch 38, Batch 502, LR 0.000000 Loss 4.422060, Accuracy 89.959%\n",
      "Epoch 38, Batch 503, LR 0.000000 Loss 4.420959, Accuracy 89.956%\n",
      "Epoch 38, Batch 504, LR 0.000000 Loss 4.420084, Accuracy 89.960%\n",
      "Epoch 38, Batch 505, LR 0.000000 Loss 4.420166, Accuracy 89.961%\n",
      "Epoch 38, Batch 506, LR 0.000000 Loss 4.421145, Accuracy 89.958%\n",
      "Epoch 38, Batch 507, LR 0.000000 Loss 4.421261, Accuracy 89.955%\n",
      "Epoch 38, Batch 508, LR 0.000000 Loss 4.422087, Accuracy 89.953%\n",
      "Epoch 38, Batch 509, LR 0.000000 Loss 4.422504, Accuracy 89.951%\n",
      "Epoch 38, Batch 510, LR 0.000000 Loss 4.423399, Accuracy 89.949%\n",
      "Epoch 38, Batch 511, LR 0.000000 Loss 4.422417, Accuracy 89.954%\n",
      "Epoch 38, Batch 512, LR 0.000000 Loss 4.422401, Accuracy 89.955%\n",
      "Epoch 38, Batch 513, LR 0.000000 Loss 4.423234, Accuracy 89.953%\n",
      "Epoch 38, Batch 514, LR 0.000000 Loss 4.422894, Accuracy 89.955%\n",
      "Epoch 38, Batch 515, LR 0.000000 Loss 4.423525, Accuracy 89.947%\n",
      "Epoch 38, Batch 516, LR 0.000000 Loss 4.422813, Accuracy 89.953%\n",
      "Epoch 38, Batch 517, LR 0.000000 Loss 4.423625, Accuracy 89.948%\n",
      "Epoch 38, Batch 518, LR 0.000000 Loss 4.422773, Accuracy 89.951%\n",
      "Epoch 38, Batch 519, LR 0.000000 Loss 4.423358, Accuracy 89.946%\n",
      "Epoch 38, Batch 520, LR 0.000000 Loss 4.423252, Accuracy 89.946%\n",
      "Epoch 38, Batch 521, LR 0.000000 Loss 4.424537, Accuracy 89.947%\n",
      "Epoch 38, Batch 522, LR 0.000000 Loss 4.423927, Accuracy 89.949%\n",
      "Epoch 38, Batch 523, LR 0.000000 Loss 4.423896, Accuracy 89.947%\n",
      "Epoch 38, Batch 524, LR 0.000000 Loss 4.423638, Accuracy 89.942%\n",
      "Epoch 38, Batch 525, LR 0.000000 Loss 4.423957, Accuracy 89.935%\n",
      "Epoch 38, Batch 526, LR 0.000000 Loss 4.422440, Accuracy 89.942%\n",
      "Epoch 38, Batch 527, LR 0.000000 Loss 4.422342, Accuracy 89.943%\n",
      "Epoch 38, Batch 528, LR 0.000000 Loss 4.422263, Accuracy 89.950%\n",
      "Epoch 38, Batch 529, LR 0.000000 Loss 4.421673, Accuracy 89.953%\n",
      "Epoch 38, Batch 530, LR 0.000000 Loss 4.422829, Accuracy 89.945%\n",
      "Epoch 38, Batch 531, LR 0.000000 Loss 4.422742, Accuracy 89.945%\n",
      "Epoch 38, Batch 532, LR 0.000000 Loss 4.421935, Accuracy 89.947%\n",
      "Epoch 38, Batch 533, LR 0.000000 Loss 4.422637, Accuracy 89.940%\n",
      "Epoch 38, Batch 534, LR 0.000000 Loss 4.422512, Accuracy 89.945%\n",
      "Epoch 38, Batch 535, LR 0.000000 Loss 4.423923, Accuracy 89.933%\n",
      "Epoch 38, Batch 536, LR 0.000000 Loss 4.424160, Accuracy 89.936%\n",
      "Epoch 38, Batch 537, LR 0.000000 Loss 4.424759, Accuracy 89.935%\n",
      "Epoch 38, Batch 538, LR 0.000000 Loss 4.424611, Accuracy 89.937%\n",
      "Epoch 38, Batch 539, LR 0.000000 Loss 4.424766, Accuracy 89.932%\n",
      "Epoch 38, Batch 540, LR 0.000000 Loss 4.425004, Accuracy 89.929%\n",
      "Epoch 38, Batch 541, LR 0.000000 Loss 4.424159, Accuracy 89.932%\n",
      "Epoch 38, Batch 542, LR 0.000000 Loss 4.423515, Accuracy 89.932%\n",
      "Epoch 38, Batch 543, LR 0.000000 Loss 4.422884, Accuracy 89.937%\n",
      "Epoch 38, Batch 544, LR 0.000000 Loss 4.422385, Accuracy 89.941%\n",
      "Epoch 38, Batch 545, LR 0.000000 Loss 4.423638, Accuracy 89.931%\n",
      "Epoch 38, Batch 546, LR 0.000000 Loss 4.422652, Accuracy 89.935%\n",
      "Epoch 38, Batch 547, LR 0.000000 Loss 4.423852, Accuracy 89.928%\n",
      "Epoch 38, Batch 548, LR 0.000000 Loss 4.422828, Accuracy 89.934%\n",
      "Epoch 38, Batch 549, LR 0.000000 Loss 4.422328, Accuracy 89.929%\n",
      "Epoch 38, Batch 550, LR 0.000000 Loss 4.422916, Accuracy 89.930%\n",
      "Epoch 38, Batch 551, LR 0.000000 Loss 4.421748, Accuracy 89.933%\n",
      "Epoch 38, Batch 552, LR 0.000000 Loss 4.421336, Accuracy 89.937%\n",
      "Epoch 38, Batch 553, LR 0.000000 Loss 4.421978, Accuracy 89.934%\n",
      "Epoch 38, Batch 554, LR 0.000000 Loss 4.421815, Accuracy 89.934%\n",
      "Epoch 38, Batch 555, LR 0.000000 Loss 4.422121, Accuracy 89.934%\n",
      "Epoch 38, Batch 556, LR 0.000000 Loss 4.421645, Accuracy 89.932%\n",
      "Epoch 38, Batch 557, LR 0.000000 Loss 4.420693, Accuracy 89.935%\n",
      "Epoch 38, Batch 558, LR 0.000000 Loss 4.420561, Accuracy 89.935%\n",
      "Epoch 38, Batch 559, LR 0.000000 Loss 4.419828, Accuracy 89.940%\n",
      "Epoch 38, Batch 560, LR 0.000000 Loss 4.419078, Accuracy 89.944%\n",
      "Epoch 38, Batch 561, LR 0.000000 Loss 4.419116, Accuracy 89.937%\n",
      "Epoch 38, Batch 562, LR 0.000000 Loss 4.419853, Accuracy 89.933%\n",
      "Epoch 38, Batch 563, LR 0.000000 Loss 4.419723, Accuracy 89.928%\n",
      "Epoch 38, Batch 564, LR 0.000000 Loss 4.420845, Accuracy 89.923%\n",
      "Epoch 38, Batch 565, LR 0.000000 Loss 4.419681, Accuracy 89.927%\n",
      "Epoch 38, Batch 566, LR 0.000000 Loss 4.420219, Accuracy 89.924%\n",
      "Epoch 38, Batch 567, LR 0.000000 Loss 4.418927, Accuracy 89.932%\n",
      "Epoch 38, Batch 568, LR 0.000000 Loss 4.418569, Accuracy 89.928%\n",
      "Epoch 38, Batch 569, LR 0.000000 Loss 4.417908, Accuracy 89.932%\n",
      "Epoch 38, Batch 570, LR 0.000000 Loss 4.417292, Accuracy 89.931%\n",
      "Epoch 38, Batch 571, LR 0.000000 Loss 4.417578, Accuracy 89.934%\n",
      "Epoch 38, Batch 572, LR 0.000000 Loss 4.416970, Accuracy 89.927%\n",
      "Epoch 38, Batch 573, LR 0.000000 Loss 4.416114, Accuracy 89.932%\n",
      "Epoch 38, Batch 574, LR 0.000000 Loss 4.417069, Accuracy 89.927%\n",
      "Epoch 38, Batch 575, LR 0.000000 Loss 4.416889, Accuracy 89.936%\n",
      "Epoch 38, Batch 576, LR 0.000000 Loss 4.415487, Accuracy 89.943%\n",
      "Epoch 38, Batch 577, LR 0.000000 Loss 4.415171, Accuracy 89.940%\n",
      "Epoch 38, Batch 578, LR 0.000000 Loss 4.414551, Accuracy 89.946%\n",
      "Epoch 38, Batch 579, LR 0.000000 Loss 4.414627, Accuracy 89.953%\n",
      "Epoch 38, Batch 580, LR 0.000000 Loss 4.414172, Accuracy 89.957%\n",
      "Epoch 38, Batch 581, LR 0.000000 Loss 4.415679, Accuracy 89.949%\n",
      "Epoch 38, Batch 582, LR 0.000000 Loss 4.416104, Accuracy 89.947%\n",
      "Epoch 38, Batch 583, LR 0.000000 Loss 4.416260, Accuracy 89.947%\n",
      "Epoch 38, Batch 584, LR 0.000000 Loss 4.416464, Accuracy 89.940%\n",
      "Epoch 38, Batch 585, LR 0.000000 Loss 4.417064, Accuracy 89.936%\n",
      "Epoch 38, Batch 586, LR 0.000000 Loss 4.415690, Accuracy 89.941%\n",
      "Epoch 38, Batch 587, LR 0.000000 Loss 4.415582, Accuracy 89.937%\n",
      "Epoch 38, Batch 588, LR 0.000000 Loss 4.415745, Accuracy 89.933%\n",
      "Epoch 38, Batch 589, LR 0.000000 Loss 4.415802, Accuracy 89.935%\n",
      "Epoch 38, Batch 590, LR 0.000000 Loss 4.415480, Accuracy 89.936%\n",
      "Epoch 38, Batch 591, LR 0.000000 Loss 4.414279, Accuracy 89.939%\n",
      "Epoch 38, Batch 592, LR 0.000000 Loss 4.415262, Accuracy 89.936%\n",
      "Epoch 38, Batch 593, LR 0.000000 Loss 4.414617, Accuracy 89.937%\n",
      "Epoch 38, Batch 594, LR 0.000000 Loss 4.414455, Accuracy 89.938%\n",
      "Epoch 38, Batch 595, LR 0.000000 Loss 4.415173, Accuracy 89.940%\n",
      "Epoch 38, Batch 596, LR 0.000000 Loss 4.415098, Accuracy 89.946%\n",
      "Epoch 38, Batch 597, LR 0.000000 Loss 4.414649, Accuracy 89.951%\n",
      "Epoch 38, Batch 598, LR 0.000000 Loss 4.416075, Accuracy 89.951%\n",
      "Epoch 38, Batch 599, LR 0.000000 Loss 4.417560, Accuracy 89.947%\n",
      "Epoch 38, Batch 600, LR 0.000000 Loss 4.417756, Accuracy 89.943%\n",
      "Epoch 38, Batch 601, LR 0.000000 Loss 4.416889, Accuracy 89.941%\n",
      "Epoch 38, Batch 602, LR 0.000000 Loss 4.417996, Accuracy 89.928%\n",
      "Epoch 38, Batch 603, LR 0.000000 Loss 4.417500, Accuracy 89.929%\n",
      "Epoch 38, Batch 604, LR 0.000000 Loss 4.417668, Accuracy 89.928%\n",
      "Epoch 38, Batch 605, LR 0.000000 Loss 4.418323, Accuracy 89.924%\n",
      "Epoch 38, Batch 606, LR 0.000000 Loss 4.419187, Accuracy 89.928%\n",
      "Epoch 38, Batch 607, LR 0.000000 Loss 4.419796, Accuracy 89.920%\n",
      "Epoch 38, Batch 608, LR 0.000000 Loss 4.419628, Accuracy 89.911%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 609, LR 0.000000 Loss 4.418892, Accuracy 89.913%\n",
      "Epoch 38, Batch 610, LR 0.000000 Loss 4.419878, Accuracy 89.913%\n",
      "Epoch 38, Batch 611, LR 0.000000 Loss 4.418839, Accuracy 89.918%\n",
      "Epoch 38, Batch 612, LR 0.000000 Loss 4.418208, Accuracy 89.920%\n",
      "Epoch 38, Batch 613, LR 0.000000 Loss 4.417269, Accuracy 89.919%\n",
      "Epoch 38, Batch 614, LR 0.000000 Loss 4.418115, Accuracy 89.912%\n",
      "Epoch 38, Batch 615, LR 0.000000 Loss 4.418169, Accuracy 89.912%\n",
      "Epoch 38, Batch 616, LR 0.000000 Loss 4.417460, Accuracy 89.912%\n",
      "Epoch 38, Batch 617, LR 0.000000 Loss 4.417133, Accuracy 89.912%\n",
      "Epoch 38, Batch 618, LR 0.000000 Loss 4.419055, Accuracy 89.903%\n",
      "Epoch 38, Batch 619, LR 0.000000 Loss 4.419650, Accuracy 89.899%\n",
      "Epoch 38, Batch 620, LR 0.000000 Loss 4.419354, Accuracy 89.897%\n",
      "Epoch 38, Batch 621, LR 0.000000 Loss 4.418953, Accuracy 89.898%\n",
      "Epoch 38, Batch 622, LR 0.000000 Loss 4.418318, Accuracy 89.899%\n",
      "Epoch 38, Batch 623, LR 0.000000 Loss 4.418534, Accuracy 89.896%\n",
      "Epoch 38, Batch 624, LR 0.000000 Loss 4.418348, Accuracy 89.891%\n",
      "Epoch 38, Batch 625, LR 0.000000 Loss 4.418703, Accuracy 89.890%\n",
      "Epoch 38, Batch 626, LR 0.000000 Loss 4.419731, Accuracy 89.890%\n",
      "Epoch 38, Batch 627, LR 0.000000 Loss 4.419257, Accuracy 89.890%\n",
      "Epoch 38, Batch 628, LR 0.000000 Loss 4.418824, Accuracy 89.894%\n",
      "Epoch 38, Batch 629, LR 0.000000 Loss 4.418596, Accuracy 89.897%\n",
      "Epoch 38, Batch 630, LR 0.000000 Loss 4.418334, Accuracy 89.897%\n",
      "Epoch 38, Batch 631, LR 0.000000 Loss 4.418640, Accuracy 89.896%\n",
      "Epoch 38, Batch 632, LR 0.000000 Loss 4.418552, Accuracy 89.899%\n",
      "Epoch 38, Batch 633, LR 0.000000 Loss 4.418368, Accuracy 89.892%\n",
      "Epoch 38, Batch 634, LR 0.000000 Loss 4.419099, Accuracy 89.897%\n",
      "Epoch 38, Batch 635, LR 0.000000 Loss 4.420060, Accuracy 89.897%\n",
      "Epoch 38, Batch 636, LR 0.000000 Loss 4.420268, Accuracy 89.895%\n",
      "Epoch 38, Batch 637, LR 0.000000 Loss 4.420781, Accuracy 89.893%\n",
      "Epoch 38, Batch 638, LR 0.000000 Loss 4.420055, Accuracy 89.896%\n",
      "Epoch 38, Batch 639, LR 0.000000 Loss 4.420692, Accuracy 89.899%\n",
      "Epoch 38, Batch 640, LR 0.000000 Loss 4.420138, Accuracy 89.897%\n",
      "Epoch 38, Batch 641, LR 0.000000 Loss 4.420104, Accuracy 89.899%\n",
      "Epoch 38, Batch 642, LR 0.000000 Loss 4.420682, Accuracy 89.899%\n",
      "Epoch 38, Batch 643, LR 0.000000 Loss 4.420302, Accuracy 89.906%\n",
      "Epoch 38, Batch 644, LR 0.000000 Loss 4.420798, Accuracy 89.903%\n",
      "Epoch 38, Batch 645, LR 0.000000 Loss 4.420766, Accuracy 89.903%\n",
      "Epoch 38, Batch 646, LR 0.000000 Loss 4.420174, Accuracy 89.903%\n",
      "Epoch 38, Batch 647, LR 0.000000 Loss 4.420350, Accuracy 89.902%\n",
      "Epoch 38, Batch 648, LR 0.000000 Loss 4.419603, Accuracy 89.900%\n",
      "Epoch 38, Batch 649, LR 0.000000 Loss 4.419042, Accuracy 89.902%\n",
      "Epoch 38, Batch 650, LR 0.000000 Loss 4.419119, Accuracy 89.901%\n",
      "Epoch 38, Batch 651, LR 0.000000 Loss 4.418815, Accuracy 89.906%\n",
      "Epoch 38, Batch 652, LR 0.000000 Loss 4.418972, Accuracy 89.907%\n",
      "Epoch 38, Batch 653, LR 0.000000 Loss 4.419644, Accuracy 89.899%\n",
      "Epoch 38, Batch 654, LR 0.000000 Loss 4.420436, Accuracy 89.894%\n",
      "Epoch 38, Batch 655, LR 0.000000 Loss 4.419976, Accuracy 89.895%\n",
      "Epoch 38, Batch 656, LR 0.000000 Loss 4.420278, Accuracy 89.896%\n",
      "Epoch 38, Batch 657, LR 0.000000 Loss 4.419963, Accuracy 89.897%\n",
      "Epoch 38, Batch 658, LR 0.000000 Loss 4.419489, Accuracy 89.902%\n",
      "Epoch 38, Batch 659, LR 0.000000 Loss 4.420461, Accuracy 89.894%\n",
      "Epoch 38, Batch 660, LR 0.000000 Loss 4.420221, Accuracy 89.898%\n",
      "Epoch 38, Batch 661, LR 0.000000 Loss 4.419222, Accuracy 89.902%\n",
      "Epoch 38, Batch 662, LR 0.000000 Loss 4.419130, Accuracy 89.903%\n",
      "Epoch 38, Batch 663, LR 0.000000 Loss 4.417973, Accuracy 89.907%\n",
      "Epoch 38, Batch 664, LR 0.000000 Loss 4.418127, Accuracy 89.904%\n",
      "Epoch 38, Batch 665, LR 0.000000 Loss 4.419409, Accuracy 89.900%\n",
      "Epoch 38, Batch 666, LR 0.000000 Loss 4.418705, Accuracy 89.904%\n",
      "Epoch 38, Batch 667, LR 0.000000 Loss 4.417563, Accuracy 89.902%\n",
      "Epoch 38, Batch 668, LR 0.000000 Loss 4.419097, Accuracy 89.896%\n",
      "Epoch 38, Batch 669, LR 0.000000 Loss 4.418503, Accuracy 89.900%\n",
      "Epoch 38, Batch 670, LR 0.000000 Loss 4.418829, Accuracy 89.899%\n",
      "Epoch 38, Batch 671, LR 0.000000 Loss 4.418912, Accuracy 89.897%\n",
      "Epoch 38, Batch 672, LR 0.000000 Loss 4.419260, Accuracy 89.890%\n",
      "Epoch 38, Batch 673, LR 0.000000 Loss 4.420353, Accuracy 89.886%\n",
      "Epoch 38, Batch 674, LR 0.000000 Loss 4.419792, Accuracy 89.889%\n",
      "Epoch 38, Batch 675, LR 0.000000 Loss 4.419500, Accuracy 89.890%\n",
      "Epoch 38, Batch 676, LR 0.000000 Loss 4.419510, Accuracy 89.890%\n",
      "Epoch 38, Batch 677, LR 0.000000 Loss 4.419732, Accuracy 89.888%\n",
      "Epoch 38, Batch 678, LR 0.000000 Loss 4.419518, Accuracy 89.889%\n",
      "Epoch 38, Batch 679, LR 0.000000 Loss 4.418893, Accuracy 89.891%\n",
      "Epoch 38, Batch 680, LR 0.000000 Loss 4.418638, Accuracy 89.895%\n",
      "Epoch 38, Batch 681, LR 0.000000 Loss 4.418546, Accuracy 89.898%\n",
      "Epoch 38, Batch 682, LR 0.000000 Loss 4.418307, Accuracy 89.896%\n",
      "Epoch 38, Batch 683, LR 0.000000 Loss 4.419571, Accuracy 89.890%\n",
      "Epoch 38, Batch 684, LR 0.000000 Loss 4.418240, Accuracy 89.893%\n",
      "Epoch 38, Batch 685, LR 0.000000 Loss 4.418454, Accuracy 89.894%\n",
      "Epoch 38, Batch 686, LR 0.000000 Loss 4.418763, Accuracy 89.892%\n",
      "Epoch 38, Batch 687, LR 0.000000 Loss 4.417427, Accuracy 89.896%\n",
      "Epoch 38, Batch 688, LR 0.000000 Loss 4.416781, Accuracy 89.898%\n",
      "Epoch 38, Batch 689, LR 0.000000 Loss 4.417649, Accuracy 89.899%\n",
      "Epoch 38, Batch 690, LR 0.000000 Loss 4.416320, Accuracy 89.903%\n",
      "Epoch 38, Batch 691, LR 0.000000 Loss 4.416793, Accuracy 89.899%\n",
      "Epoch 38, Batch 692, LR 0.000000 Loss 4.416636, Accuracy 89.897%\n",
      "Epoch 38, Batch 693, LR 0.000000 Loss 4.415320, Accuracy 89.900%\n",
      "Epoch 38, Batch 694, LR 0.000000 Loss 4.415148, Accuracy 89.902%\n",
      "Epoch 38, Batch 695, LR 0.000000 Loss 4.415765, Accuracy 89.897%\n",
      "Epoch 38, Batch 696, LR 0.000000 Loss 4.416221, Accuracy 89.891%\n",
      "Epoch 38, Batch 697, LR 0.000000 Loss 4.415715, Accuracy 89.894%\n",
      "Epoch 38, Batch 698, LR 0.000000 Loss 4.415827, Accuracy 89.897%\n",
      "Epoch 38, Batch 699, LR 0.000000 Loss 4.415565, Accuracy 89.891%\n",
      "Epoch 38, Batch 700, LR 0.000000 Loss 4.415829, Accuracy 89.886%\n",
      "Epoch 38, Batch 701, LR 0.000000 Loss 4.415008, Accuracy 89.893%\n",
      "Epoch 38, Batch 702, LR 0.000000 Loss 4.415223, Accuracy 89.894%\n",
      "Epoch 38, Batch 703, LR 0.000000 Loss 4.415177, Accuracy 89.892%\n",
      "Epoch 38, Batch 704, LR 0.000000 Loss 4.415422, Accuracy 89.887%\n",
      "Epoch 38, Batch 705, LR 0.000000 Loss 4.415650, Accuracy 89.883%\n",
      "Epoch 38, Batch 706, LR 0.000000 Loss 4.416446, Accuracy 89.877%\n",
      "Epoch 38, Batch 707, LR 0.000000 Loss 4.417243, Accuracy 89.876%\n",
      "Epoch 38, Batch 708, LR 0.000000 Loss 4.418509, Accuracy 89.870%\n",
      "Epoch 38, Batch 709, LR 0.000000 Loss 4.419157, Accuracy 89.866%\n",
      "Epoch 38, Batch 710, LR 0.000000 Loss 4.420148, Accuracy 89.862%\n",
      "Epoch 38, Batch 711, LR 0.000000 Loss 4.420492, Accuracy 89.861%\n",
      "Epoch 38, Batch 712, LR 0.000000 Loss 4.420382, Accuracy 89.858%\n",
      "Epoch 38, Batch 713, LR 0.000000 Loss 4.420862, Accuracy 89.856%\n",
      "Epoch 38, Batch 714, LR 0.000000 Loss 4.421021, Accuracy 89.854%\n",
      "Epoch 38, Batch 715, LR 0.000000 Loss 4.421086, Accuracy 89.851%\n",
      "Epoch 38, Batch 716, LR 0.000000 Loss 4.421759, Accuracy 89.849%\n",
      "Epoch 38, Batch 717, LR 0.000000 Loss 4.421034, Accuracy 89.850%\n",
      "Epoch 38, Batch 718, LR 0.000000 Loss 4.421116, Accuracy 89.850%\n",
      "Epoch 38, Batch 719, LR 0.000000 Loss 4.420410, Accuracy 89.852%\n",
      "Epoch 38, Batch 720, LR 0.000000 Loss 4.421084, Accuracy 89.850%\n",
      "Epoch 38, Batch 721, LR 0.000000 Loss 4.420871, Accuracy 89.847%\n",
      "Epoch 38, Batch 722, LR 0.000000 Loss 4.420225, Accuracy 89.850%\n",
      "Epoch 38, Batch 723, LR 0.000000 Loss 4.420131, Accuracy 89.850%\n",
      "Epoch 38, Batch 724, LR 0.000000 Loss 4.421154, Accuracy 89.847%\n",
      "Epoch 38, Batch 725, LR 0.000000 Loss 4.421122, Accuracy 89.846%\n",
      "Epoch 38, Batch 726, LR 0.000000 Loss 4.420705, Accuracy 89.849%\n",
      "Epoch 38, Batch 727, LR 0.000000 Loss 4.420368, Accuracy 89.852%\n",
      "Epoch 38, Batch 728, LR 0.000000 Loss 4.419705, Accuracy 89.854%\n",
      "Epoch 38, Batch 729, LR 0.000000 Loss 4.419903, Accuracy 89.853%\n",
      "Epoch 38, Batch 730, LR 0.000000 Loss 4.419409, Accuracy 89.860%\n",
      "Epoch 38, Batch 731, LR 0.000000 Loss 4.418237, Accuracy 89.866%\n",
      "Epoch 38, Batch 732, LR 0.000000 Loss 4.419058, Accuracy 89.862%\n",
      "Epoch 38, Batch 733, LR 0.000000 Loss 4.418346, Accuracy 89.864%\n",
      "Epoch 38, Batch 734, LR 0.000000 Loss 4.418491, Accuracy 89.861%\n",
      "Epoch 38, Batch 735, LR 0.000000 Loss 4.417299, Accuracy 89.861%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 736, LR 0.000000 Loss 4.417701, Accuracy 89.862%\n",
      "Epoch 38, Batch 737, LR 0.000000 Loss 4.418179, Accuracy 89.862%\n",
      "Epoch 38, Batch 738, LR 0.000000 Loss 4.419281, Accuracy 89.859%\n",
      "Epoch 38, Batch 739, LR 0.000000 Loss 4.419832, Accuracy 89.855%\n",
      "Epoch 38, Batch 740, LR 0.000000 Loss 4.419859, Accuracy 89.857%\n",
      "Epoch 38, Batch 741, LR 0.000000 Loss 4.419294, Accuracy 89.859%\n",
      "Epoch 38, Batch 742, LR 0.000000 Loss 4.418361, Accuracy 89.858%\n",
      "Epoch 38, Batch 743, LR 0.000000 Loss 4.418970, Accuracy 89.857%\n",
      "Epoch 38, Batch 744, LR 0.000000 Loss 4.419992, Accuracy 89.852%\n",
      "Epoch 38, Batch 745, LR 0.000000 Loss 4.419372, Accuracy 89.853%\n",
      "Epoch 38, Batch 746, LR 0.000000 Loss 4.419640, Accuracy 89.848%\n",
      "Epoch 38, Batch 747, LR 0.000000 Loss 4.419149, Accuracy 89.851%\n",
      "Epoch 38, Batch 748, LR 0.000000 Loss 4.418693, Accuracy 89.853%\n",
      "Epoch 38, Batch 749, LR 0.000000 Loss 4.418905, Accuracy 89.855%\n",
      "Epoch 38, Batch 750, LR 0.000000 Loss 4.418731, Accuracy 89.857%\n",
      "Epoch 38, Batch 751, LR 0.000000 Loss 4.419230, Accuracy 89.854%\n",
      "Epoch 38, Batch 752, LR 0.000000 Loss 4.419500, Accuracy 89.848%\n",
      "Epoch 38, Batch 753, LR 0.000000 Loss 4.419591, Accuracy 89.849%\n",
      "Epoch 38, Batch 754, LR 0.000000 Loss 4.420565, Accuracy 89.850%\n",
      "Epoch 38, Batch 755, LR 0.000000 Loss 4.420796, Accuracy 89.851%\n",
      "Epoch 38, Batch 756, LR 0.000000 Loss 4.420286, Accuracy 89.851%\n",
      "Epoch 38, Batch 757, LR 0.000000 Loss 4.420274, Accuracy 89.851%\n",
      "Epoch 38, Batch 758, LR 0.000000 Loss 4.420577, Accuracy 89.851%\n",
      "Epoch 38, Batch 759, LR 0.000000 Loss 4.421066, Accuracy 89.847%\n",
      "Epoch 38, Batch 760, LR 0.000000 Loss 4.421118, Accuracy 89.846%\n",
      "Epoch 38, Batch 761, LR 0.000000 Loss 4.421317, Accuracy 89.845%\n",
      "Epoch 38, Batch 762, LR 0.000000 Loss 4.420560, Accuracy 89.852%\n",
      "Epoch 38, Batch 763, LR 0.000000 Loss 4.420659, Accuracy 89.851%\n",
      "Epoch 38, Batch 764, LR 0.000000 Loss 4.420478, Accuracy 89.854%\n",
      "Epoch 38, Batch 765, LR 0.000000 Loss 4.420718, Accuracy 89.854%\n",
      "Epoch 38, Batch 766, LR 0.000000 Loss 4.419324, Accuracy 89.862%\n",
      "Epoch 38, Batch 767, LR 0.000000 Loss 4.420796, Accuracy 89.858%\n",
      "Epoch 38, Batch 768, LR 0.000000 Loss 4.420323, Accuracy 89.863%\n",
      "Epoch 38, Batch 769, LR 0.000000 Loss 4.419653, Accuracy 89.862%\n",
      "Epoch 38, Batch 770, LR 0.000000 Loss 4.419840, Accuracy 89.862%\n",
      "Epoch 38, Batch 771, LR 0.000000 Loss 4.419458, Accuracy 89.865%\n",
      "Epoch 38, Batch 772, LR 0.000000 Loss 4.420100, Accuracy 89.868%\n",
      "Epoch 38, Batch 773, LR 0.000000 Loss 4.421124, Accuracy 89.858%\n",
      "Epoch 38, Batch 774, LR 0.000000 Loss 4.420733, Accuracy 89.864%\n",
      "Epoch 38, Batch 775, LR 0.000000 Loss 4.419807, Accuracy 89.868%\n",
      "Epoch 38, Batch 776, LR 0.000000 Loss 4.420148, Accuracy 89.868%\n",
      "Epoch 38, Batch 777, LR 0.000000 Loss 4.420428, Accuracy 89.872%\n",
      "Epoch 38, Batch 778, LR 0.000000 Loss 4.420438, Accuracy 89.873%\n",
      "Epoch 38, Batch 779, LR 0.000000 Loss 4.421707, Accuracy 89.867%\n",
      "Epoch 38, Batch 780, LR 0.000000 Loss 4.421866, Accuracy 89.867%\n",
      "Epoch 38, Batch 781, LR 0.000000 Loss 4.421987, Accuracy 89.865%\n",
      "Epoch 38, Batch 782, LR 0.000000 Loss 4.421958, Accuracy 89.864%\n",
      "Epoch 38, Batch 783, LR 0.000000 Loss 4.422359, Accuracy 89.866%\n",
      "Epoch 38, Batch 784, LR 0.000000 Loss 4.421160, Accuracy 89.872%\n",
      "Epoch 38, Batch 785, LR 0.000000 Loss 4.420696, Accuracy 89.872%\n",
      "Epoch 38, Batch 786, LR 0.000000 Loss 4.420504, Accuracy 89.874%\n",
      "Epoch 38, Batch 787, LR 0.000000 Loss 4.421081, Accuracy 89.874%\n",
      "Epoch 38, Batch 788, LR 0.000000 Loss 4.420570, Accuracy 89.876%\n",
      "Epoch 38, Batch 789, LR 0.000000 Loss 4.421000, Accuracy 89.872%\n",
      "Epoch 38, Batch 790, LR 0.000000 Loss 4.420707, Accuracy 89.872%\n",
      "Epoch 38, Batch 791, LR 0.000000 Loss 4.420315, Accuracy 89.871%\n",
      "Epoch 38, Batch 792, LR 0.000000 Loss 4.420004, Accuracy 89.877%\n",
      "Epoch 38, Batch 793, LR 0.000000 Loss 4.420895, Accuracy 89.880%\n",
      "Epoch 38, Batch 794, LR 0.000000 Loss 4.420293, Accuracy 89.886%\n",
      "Epoch 38, Batch 795, LR 0.000000 Loss 4.420362, Accuracy 89.885%\n",
      "Epoch 38, Batch 796, LR 0.000000 Loss 4.420624, Accuracy 89.883%\n",
      "Epoch 38, Batch 797, LR 0.000000 Loss 4.420513, Accuracy 89.880%\n",
      "Epoch 38, Batch 798, LR 0.000000 Loss 4.420946, Accuracy 89.878%\n",
      "Epoch 38, Batch 799, LR 0.000000 Loss 4.420861, Accuracy 89.876%\n",
      "Epoch 38, Batch 800, LR 0.000000 Loss 4.420563, Accuracy 89.879%\n",
      "Epoch 38, Batch 801, LR 0.000000 Loss 4.420563, Accuracy 89.878%\n",
      "Epoch 38, Batch 802, LR 0.000000 Loss 4.420836, Accuracy 89.880%\n",
      "Epoch 38, Batch 803, LR 0.000000 Loss 4.421344, Accuracy 89.876%\n",
      "Epoch 38, Batch 804, LR 0.000000 Loss 4.420753, Accuracy 89.877%\n",
      "Epoch 38, Batch 805, LR 0.000000 Loss 4.421011, Accuracy 89.872%\n",
      "Epoch 38, Batch 806, LR 0.000000 Loss 4.420082, Accuracy 89.877%\n",
      "Epoch 38, Batch 807, LR 0.000000 Loss 4.419885, Accuracy 89.880%\n",
      "Epoch 38, Batch 808, LR 0.000000 Loss 4.420056, Accuracy 89.875%\n",
      "Epoch 38, Batch 809, LR 0.000000 Loss 4.420525, Accuracy 89.874%\n",
      "Epoch 38, Batch 810, LR 0.000000 Loss 4.420363, Accuracy 89.876%\n",
      "Epoch 38, Batch 811, LR 0.000000 Loss 4.420644, Accuracy 89.875%\n",
      "Epoch 38, Batch 812, LR 0.000000 Loss 4.419324, Accuracy 89.882%\n",
      "Epoch 38, Batch 813, LR 0.000000 Loss 4.419120, Accuracy 89.884%\n",
      "Epoch 38, Batch 814, LR 0.000000 Loss 4.418486, Accuracy 89.885%\n",
      "Epoch 38, Batch 815, LR 0.000000 Loss 4.418318, Accuracy 89.886%\n",
      "Epoch 38, Batch 816, LR 0.000000 Loss 4.418608, Accuracy 89.885%\n",
      "Epoch 38, Batch 817, LR 0.000000 Loss 4.417898, Accuracy 89.887%\n",
      "Epoch 38, Batch 818, LR 0.000000 Loss 4.418156, Accuracy 89.892%\n",
      "Epoch 38, Batch 819, LR 0.000000 Loss 4.418139, Accuracy 89.890%\n",
      "Epoch 38, Batch 820, LR 0.000000 Loss 4.418984, Accuracy 89.888%\n",
      "Epoch 38, Batch 821, LR 0.000000 Loss 4.419589, Accuracy 89.883%\n",
      "Epoch 38, Batch 822, LR 0.000000 Loss 4.419668, Accuracy 89.880%\n",
      "Epoch 38, Batch 823, LR 0.000000 Loss 4.418969, Accuracy 89.886%\n",
      "Epoch 38, Batch 824, LR 0.000000 Loss 4.419495, Accuracy 89.889%\n",
      "Epoch 38, Batch 825, LR 0.000000 Loss 4.419076, Accuracy 89.890%\n",
      "Epoch 38, Batch 826, LR 0.000000 Loss 4.418626, Accuracy 89.895%\n",
      "Epoch 38, Batch 827, LR 0.000000 Loss 4.418597, Accuracy 89.892%\n",
      "Epoch 38, Batch 828, LR 0.000000 Loss 4.418782, Accuracy 89.893%\n",
      "Epoch 38, Batch 829, LR 0.000000 Loss 4.418393, Accuracy 89.892%\n",
      "Epoch 38, Batch 830, LR 0.000000 Loss 4.418579, Accuracy 89.894%\n",
      "Epoch 38, Batch 831, LR 0.000000 Loss 4.417996, Accuracy 89.897%\n",
      "Epoch 38, Batch 832, LR 0.000000 Loss 4.417050, Accuracy 89.900%\n",
      "Epoch 38, Batch 833, LR 0.000000 Loss 4.417135, Accuracy 89.902%\n",
      "Epoch 38, Batch 834, LR 0.000000 Loss 4.417042, Accuracy 89.901%\n",
      "Epoch 38, Batch 835, LR 0.000000 Loss 4.417117, Accuracy 89.901%\n",
      "Epoch 38, Batch 836, LR 0.000000 Loss 4.417266, Accuracy 89.898%\n",
      "Epoch 38, Batch 837, LR 0.000000 Loss 4.416897, Accuracy 89.904%\n",
      "Epoch 38, Batch 838, LR 0.000000 Loss 4.417177, Accuracy 89.902%\n",
      "Epoch 38, Batch 839, LR 0.000000 Loss 4.417524, Accuracy 89.898%\n",
      "Epoch 38, Batch 840, LR 0.000000 Loss 4.417678, Accuracy 89.899%\n",
      "Epoch 38, Batch 841, LR 0.000000 Loss 4.416928, Accuracy 89.900%\n",
      "Epoch 38, Batch 842, LR 0.000000 Loss 4.417128, Accuracy 89.902%\n",
      "Epoch 38, Batch 843, LR 0.000000 Loss 4.416462, Accuracy 89.906%\n",
      "Epoch 38, Batch 844, LR 0.000000 Loss 4.416279, Accuracy 89.901%\n",
      "Epoch 38, Batch 845, LR 0.000000 Loss 4.415987, Accuracy 89.905%\n",
      "Epoch 38, Batch 846, LR 0.000000 Loss 4.415496, Accuracy 89.907%\n",
      "Epoch 38, Batch 847, LR 0.000000 Loss 4.414553, Accuracy 89.910%\n",
      "Epoch 38, Batch 848, LR 0.000000 Loss 4.414276, Accuracy 89.910%\n",
      "Epoch 38, Batch 849, LR 0.000000 Loss 4.414748, Accuracy 89.909%\n",
      "Epoch 38, Batch 850, LR 0.000000 Loss 4.414362, Accuracy 89.909%\n",
      "Epoch 38, Batch 851, LR 0.000000 Loss 4.414440, Accuracy 89.909%\n",
      "Epoch 38, Batch 852, LR 0.000000 Loss 4.413814, Accuracy 89.913%\n",
      "Epoch 38, Batch 853, LR 0.000000 Loss 4.413553, Accuracy 89.913%\n",
      "Epoch 38, Batch 854, LR 0.000000 Loss 4.414149, Accuracy 89.914%\n",
      "Epoch 38, Batch 855, LR 0.000000 Loss 4.415035, Accuracy 89.916%\n",
      "Epoch 38, Batch 856, LR 0.000000 Loss 4.414545, Accuracy 89.920%\n",
      "Epoch 38, Batch 857, LR 0.000000 Loss 4.415060, Accuracy 89.918%\n",
      "Epoch 38, Batch 858, LR 0.000000 Loss 4.415484, Accuracy 89.916%\n",
      "Epoch 38, Batch 859, LR 0.000000 Loss 4.415794, Accuracy 89.917%\n",
      "Epoch 38, Batch 860, LR 0.000000 Loss 4.416302, Accuracy 89.913%\n",
      "Epoch 38, Batch 861, LR 0.000000 Loss 4.416515, Accuracy 89.909%\n",
      "Epoch 38, Batch 862, LR 0.000000 Loss 4.416506, Accuracy 89.905%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 863, LR 0.000000 Loss 4.416464, Accuracy 89.906%\n",
      "Epoch 38, Batch 864, LR 0.000000 Loss 4.416360, Accuracy 89.907%\n",
      "Epoch 38, Batch 865, LR 0.000000 Loss 4.416267, Accuracy 89.909%\n",
      "Epoch 38, Batch 866, LR 0.000000 Loss 4.415354, Accuracy 89.911%\n",
      "Epoch 38, Batch 867, LR 0.000000 Loss 4.415843, Accuracy 89.911%\n",
      "Epoch 38, Batch 868, LR 0.000000 Loss 4.416387, Accuracy 89.912%\n",
      "Epoch 38, Batch 869, LR 0.000000 Loss 4.415985, Accuracy 89.913%\n",
      "Epoch 38, Batch 870, LR 0.000000 Loss 4.416011, Accuracy 89.913%\n",
      "Epoch 38, Batch 871, LR 0.000000 Loss 4.415859, Accuracy 89.916%\n",
      "Epoch 38, Batch 872, LR 0.000000 Loss 4.415389, Accuracy 89.917%\n",
      "Epoch 38, Batch 873, LR 0.000000 Loss 4.415834, Accuracy 89.916%\n",
      "Epoch 38, Batch 874, LR 0.000000 Loss 4.415653, Accuracy 89.920%\n",
      "Epoch 38, Batch 875, LR 0.000000 Loss 4.416242, Accuracy 89.921%\n",
      "Epoch 38, Batch 876, LR 0.000000 Loss 4.416618, Accuracy 89.921%\n",
      "Epoch 38, Batch 877, LR 0.000000 Loss 4.416818, Accuracy 89.923%\n",
      "Epoch 38, Batch 878, LR 0.000000 Loss 4.417238, Accuracy 89.923%\n",
      "Epoch 38, Batch 879, LR 0.000000 Loss 4.417655, Accuracy 89.921%\n",
      "Epoch 38, Batch 880, LR 0.000000 Loss 4.417497, Accuracy 89.923%\n",
      "Epoch 38, Batch 881, LR 0.000000 Loss 4.417466, Accuracy 89.928%\n",
      "Epoch 38, Batch 882, LR 0.000000 Loss 4.418029, Accuracy 89.923%\n",
      "Epoch 38, Batch 883, LR 0.000000 Loss 4.417741, Accuracy 89.922%\n",
      "Epoch 38, Batch 884, LR 0.000000 Loss 4.418585, Accuracy 89.919%\n",
      "Epoch 38, Batch 885, LR 0.000000 Loss 4.418041, Accuracy 89.918%\n",
      "Epoch 38, Batch 886, LR 0.000000 Loss 4.418311, Accuracy 89.920%\n",
      "Epoch 38, Batch 887, LR 0.000000 Loss 4.418557, Accuracy 89.922%\n",
      "Epoch 38, Batch 888, LR 0.000000 Loss 4.417906, Accuracy 89.924%\n",
      "Epoch 38, Batch 889, LR 0.000000 Loss 4.418602, Accuracy 89.923%\n",
      "Epoch 38, Batch 890, LR 0.000000 Loss 4.418467, Accuracy 89.924%\n",
      "Epoch 38, Batch 891, LR 0.000000 Loss 4.418739, Accuracy 89.924%\n",
      "Epoch 38, Batch 892, LR 0.000000 Loss 4.418628, Accuracy 89.924%\n",
      "Epoch 38, Batch 893, LR 0.000000 Loss 4.418503, Accuracy 89.924%\n",
      "Epoch 38, Batch 894, LR 0.000000 Loss 4.418667, Accuracy 89.927%\n",
      "Epoch 38, Batch 895, LR 0.000000 Loss 4.418758, Accuracy 89.921%\n",
      "Epoch 38, Batch 896, LR 0.000000 Loss 4.418603, Accuracy 89.920%\n",
      "Epoch 38, Batch 897, LR 0.000000 Loss 4.418450, Accuracy 89.921%\n",
      "Epoch 38, Batch 898, LR 0.000000 Loss 4.418475, Accuracy 89.922%\n",
      "Epoch 38, Batch 899, LR 0.000000 Loss 4.418551, Accuracy 89.921%\n",
      "Epoch 38, Batch 900, LR 0.000000 Loss 4.418740, Accuracy 89.919%\n",
      "Epoch 38, Batch 901, LR 0.000000 Loss 4.418642, Accuracy 89.919%\n",
      "Epoch 38, Batch 902, LR 0.000000 Loss 4.418082, Accuracy 89.919%\n",
      "Epoch 38, Batch 903, LR 0.000000 Loss 4.418212, Accuracy 89.920%\n",
      "Epoch 38, Batch 904, LR 0.000000 Loss 4.418141, Accuracy 89.919%\n",
      "Epoch 38, Batch 905, LR 0.000000 Loss 4.417649, Accuracy 89.923%\n",
      "Epoch 38, Batch 906, LR 0.000000 Loss 4.417153, Accuracy 89.924%\n",
      "Epoch 38, Batch 907, LR 0.000000 Loss 4.418139, Accuracy 89.916%\n",
      "Epoch 38, Batch 908, LR 0.000000 Loss 4.418029, Accuracy 89.918%\n",
      "Epoch 38, Batch 909, LR 0.000000 Loss 4.418227, Accuracy 89.917%\n",
      "Epoch 38, Batch 910, LR 0.000000 Loss 4.418663, Accuracy 89.916%\n",
      "Epoch 38, Batch 911, LR 0.000000 Loss 4.419016, Accuracy 89.916%\n",
      "Epoch 38, Batch 912, LR 0.000000 Loss 4.418760, Accuracy 89.917%\n",
      "Epoch 38, Batch 913, LR 0.000000 Loss 4.418510, Accuracy 89.922%\n",
      "Epoch 38, Batch 914, LR 0.000000 Loss 4.418838, Accuracy 89.920%\n",
      "Epoch 38, Batch 915, LR 0.000000 Loss 4.419013, Accuracy 89.918%\n",
      "Epoch 38, Batch 916, LR 0.000000 Loss 4.419064, Accuracy 89.915%\n",
      "Epoch 38, Batch 917, LR 0.000000 Loss 4.419411, Accuracy 89.914%\n",
      "Epoch 38, Batch 918, LR 0.000000 Loss 4.418712, Accuracy 89.916%\n",
      "Epoch 38, Batch 919, LR 0.000000 Loss 4.418218, Accuracy 89.920%\n",
      "Epoch 38, Batch 920, LR 0.000000 Loss 4.418503, Accuracy 89.919%\n",
      "Epoch 38, Batch 921, LR 0.000000 Loss 4.418394, Accuracy 89.916%\n",
      "Epoch 38, Batch 922, LR 0.000000 Loss 4.418397, Accuracy 89.917%\n",
      "Epoch 38, Batch 923, LR 0.000000 Loss 4.418877, Accuracy 89.915%\n",
      "Epoch 38, Batch 924, LR 0.000000 Loss 4.418656, Accuracy 89.918%\n",
      "Epoch 38, Batch 925, LR 0.000000 Loss 4.418149, Accuracy 89.921%\n",
      "Epoch 38, Batch 926, LR 0.000000 Loss 4.418330, Accuracy 89.923%\n",
      "Epoch 38, Batch 927, LR 0.000000 Loss 4.417896, Accuracy 89.925%\n",
      "Epoch 38, Batch 928, LR 0.000000 Loss 4.417802, Accuracy 89.925%\n",
      "Epoch 38, Batch 929, LR 0.000000 Loss 4.417672, Accuracy 89.926%\n",
      "Epoch 38, Batch 930, LR 0.000000 Loss 4.417227, Accuracy 89.926%\n",
      "Epoch 38, Batch 931, LR 0.000000 Loss 4.416127, Accuracy 89.929%\n",
      "Epoch 38, Batch 932, LR 0.000000 Loss 4.416225, Accuracy 89.925%\n",
      "Epoch 38, Batch 933, LR 0.000000 Loss 4.415751, Accuracy 89.925%\n",
      "Epoch 38, Batch 934, LR 0.000000 Loss 4.414687, Accuracy 89.932%\n",
      "Epoch 38, Batch 935, LR 0.000000 Loss 4.414781, Accuracy 89.932%\n",
      "Epoch 38, Batch 936, LR 0.000000 Loss 4.415464, Accuracy 89.930%\n",
      "Epoch 38, Batch 937, LR 0.000000 Loss 4.415523, Accuracy 89.929%\n",
      "Epoch 38, Batch 938, LR 0.000000 Loss 4.415745, Accuracy 89.926%\n",
      "Epoch 38, Batch 939, LR 0.000000 Loss 4.416180, Accuracy 89.922%\n",
      "Epoch 38, Batch 940, LR 0.000000 Loss 4.416609, Accuracy 89.921%\n",
      "Epoch 38, Batch 941, LR 0.000000 Loss 4.417233, Accuracy 89.916%\n",
      "Epoch 38, Batch 942, LR 0.000000 Loss 4.417707, Accuracy 89.916%\n",
      "Epoch 38, Batch 943, LR 0.000000 Loss 4.417804, Accuracy 89.912%\n",
      "Epoch 38, Batch 944, LR 0.000000 Loss 4.418396, Accuracy 89.907%\n",
      "Epoch 38, Batch 945, LR 0.000000 Loss 4.418878, Accuracy 89.910%\n",
      "Epoch 38, Batch 946, LR 0.000000 Loss 4.417938, Accuracy 89.915%\n",
      "Epoch 38, Batch 947, LR 0.000000 Loss 4.417568, Accuracy 89.916%\n",
      "Epoch 38, Batch 948, LR 0.000000 Loss 4.417190, Accuracy 89.917%\n",
      "Epoch 38, Batch 949, LR 0.000000 Loss 4.417348, Accuracy 89.917%\n",
      "Epoch 38, Batch 950, LR 0.000000 Loss 4.416952, Accuracy 89.924%\n",
      "Epoch 38, Batch 951, LR 0.000000 Loss 4.416458, Accuracy 89.926%\n",
      "Epoch 38, Batch 952, LR 0.000000 Loss 4.416998, Accuracy 89.918%\n",
      "Epoch 38, Batch 953, LR 0.000000 Loss 4.417816, Accuracy 89.913%\n",
      "Epoch 38, Batch 954, LR 0.000000 Loss 4.417163, Accuracy 89.916%\n",
      "Epoch 38, Batch 955, LR 0.000000 Loss 4.416547, Accuracy 89.918%\n",
      "Epoch 38, Batch 956, LR 0.000000 Loss 4.416127, Accuracy 89.921%\n",
      "Epoch 38, Batch 957, LR 0.000000 Loss 4.415998, Accuracy 89.920%\n",
      "Epoch 38, Batch 958, LR 0.000000 Loss 4.416276, Accuracy 89.923%\n",
      "Epoch 38, Batch 959, LR 0.000000 Loss 4.415616, Accuracy 89.925%\n",
      "Epoch 38, Batch 960, LR 0.000000 Loss 4.415219, Accuracy 89.931%\n",
      "Epoch 38, Batch 961, LR 0.000000 Loss 4.415232, Accuracy 89.932%\n",
      "Epoch 38, Batch 962, LR 0.000000 Loss 4.415052, Accuracy 89.933%\n",
      "Epoch 38, Batch 963, LR 0.000000 Loss 4.414993, Accuracy 89.935%\n",
      "Epoch 38, Batch 964, LR 0.000000 Loss 4.415109, Accuracy 89.935%\n",
      "Epoch 38, Batch 965, LR 0.000000 Loss 4.415075, Accuracy 89.932%\n",
      "Epoch 38, Batch 966, LR 0.000000 Loss 4.415454, Accuracy 89.929%\n",
      "Epoch 38, Batch 967, LR 0.000000 Loss 4.416076, Accuracy 89.925%\n",
      "Epoch 38, Batch 968, LR 0.000000 Loss 4.416275, Accuracy 89.924%\n",
      "Epoch 38, Batch 969, LR 0.000000 Loss 4.416831, Accuracy 89.920%\n",
      "Epoch 38, Batch 970, LR 0.000000 Loss 4.416840, Accuracy 89.918%\n",
      "Epoch 38, Batch 971, LR 0.000000 Loss 4.417501, Accuracy 89.915%\n",
      "Epoch 38, Batch 972, LR 0.000000 Loss 4.417460, Accuracy 89.914%\n",
      "Epoch 38, Batch 973, LR 0.000000 Loss 4.417160, Accuracy 89.917%\n",
      "Epoch 38, Batch 974, LR 0.000000 Loss 4.418952, Accuracy 89.910%\n",
      "Epoch 38, Batch 975, LR 0.000000 Loss 4.419023, Accuracy 89.906%\n",
      "Epoch 38, Batch 976, LR 0.000000 Loss 4.418938, Accuracy 89.905%\n",
      "Epoch 38, Batch 977, LR 0.000000 Loss 4.419048, Accuracy 89.902%\n",
      "Epoch 38, Batch 978, LR 0.000000 Loss 4.418593, Accuracy 89.904%\n",
      "Epoch 38, Batch 979, LR 0.000000 Loss 4.419886, Accuracy 89.899%\n",
      "Epoch 38, Batch 980, LR 0.000000 Loss 4.420080, Accuracy 89.898%\n",
      "Epoch 38, Batch 981, LR 0.000000 Loss 4.420399, Accuracy 89.897%\n",
      "Epoch 38, Batch 982, LR 0.000000 Loss 4.420139, Accuracy 89.899%\n",
      "Epoch 38, Batch 983, LR 0.000000 Loss 4.420578, Accuracy 89.899%\n",
      "Epoch 38, Batch 984, LR 0.000000 Loss 4.420407, Accuracy 89.902%\n",
      "Epoch 38, Batch 985, LR 0.000000 Loss 4.420213, Accuracy 89.901%\n",
      "Epoch 38, Batch 986, LR 0.000000 Loss 4.420568, Accuracy 89.899%\n",
      "Epoch 38, Batch 987, LR 0.000000 Loss 4.420538, Accuracy 89.898%\n",
      "Epoch 38, Batch 988, LR 0.000000 Loss 4.420953, Accuracy 89.898%\n",
      "Epoch 38, Batch 989, LR 0.000000 Loss 4.420861, Accuracy 89.897%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 990, LR 0.000000 Loss 4.421298, Accuracy 89.897%\n",
      "Epoch 38, Batch 991, LR 0.000000 Loss 4.420945, Accuracy 89.899%\n",
      "Epoch 38, Batch 992, LR 0.000000 Loss 4.421048, Accuracy 89.899%\n",
      "Epoch 38, Batch 993, LR 0.000000 Loss 4.420749, Accuracy 89.903%\n",
      "Epoch 38, Batch 994, LR 0.000000 Loss 4.420689, Accuracy 89.905%\n",
      "Epoch 38, Batch 995, LR 0.000000 Loss 4.420962, Accuracy 89.901%\n",
      "Epoch 38, Batch 996, LR 0.000000 Loss 4.421189, Accuracy 89.902%\n",
      "Epoch 38, Batch 997, LR 0.000000 Loss 4.421346, Accuracy 89.903%\n",
      "Epoch 38, Batch 998, LR 0.000000 Loss 4.421615, Accuracy 89.904%\n",
      "Epoch 38, Batch 999, LR 0.000000 Loss 4.421937, Accuracy 89.906%\n",
      "Epoch 38, Batch 1000, LR 0.000000 Loss 4.422175, Accuracy 89.909%\n",
      "Epoch 38, Batch 1001, LR 0.000000 Loss 4.422311, Accuracy 89.913%\n",
      "Epoch 38, Batch 1002, LR 0.000000 Loss 4.422398, Accuracy 89.912%\n",
      "Epoch 38, Batch 1003, LR 0.000000 Loss 4.422811, Accuracy 89.909%\n",
      "Epoch 38, Batch 1004, LR 0.000000 Loss 4.423144, Accuracy 89.906%\n",
      "Epoch 38, Batch 1005, LR 0.000000 Loss 4.422754, Accuracy 89.907%\n",
      "Epoch 38, Batch 1006, LR 0.000000 Loss 4.422156, Accuracy 89.910%\n",
      "Epoch 38, Batch 1007, LR 0.000000 Loss 4.421851, Accuracy 89.911%\n",
      "Epoch 38, Batch 1008, LR 0.000000 Loss 4.422295, Accuracy 89.911%\n",
      "Epoch 38, Batch 1009, LR 0.000000 Loss 4.421587, Accuracy 89.912%\n",
      "Epoch 38, Batch 1010, LR 0.000000 Loss 4.421752, Accuracy 89.913%\n",
      "Epoch 38, Batch 1011, LR 0.000000 Loss 4.421852, Accuracy 89.912%\n",
      "Epoch 38, Batch 1012, LR 0.000000 Loss 4.422243, Accuracy 89.911%\n",
      "Epoch 38, Batch 1013, LR 0.000000 Loss 4.422193, Accuracy 89.916%\n",
      "Epoch 38, Batch 1014, LR 0.000000 Loss 4.421992, Accuracy 89.917%\n",
      "Epoch 38, Batch 1015, LR 0.000000 Loss 4.421309, Accuracy 89.920%\n",
      "Epoch 38, Batch 1016, LR 0.000000 Loss 4.420421, Accuracy 89.924%\n",
      "Epoch 38, Batch 1017, LR 0.000000 Loss 4.420780, Accuracy 89.924%\n",
      "Epoch 38, Batch 1018, LR 0.000000 Loss 4.420350, Accuracy 89.928%\n",
      "Epoch 38, Batch 1019, LR 0.000000 Loss 4.420655, Accuracy 89.927%\n",
      "Epoch 38, Batch 1020, LR 0.000000 Loss 4.420292, Accuracy 89.928%\n",
      "Epoch 38, Batch 1021, LR 0.000000 Loss 4.419953, Accuracy 89.929%\n",
      "Epoch 38, Batch 1022, LR 0.000000 Loss 4.419565, Accuracy 89.929%\n",
      "Epoch 38, Batch 1023, LR 0.000000 Loss 4.419523, Accuracy 89.929%\n",
      "Epoch 38, Batch 1024, LR 0.000000 Loss 4.419224, Accuracy 89.929%\n",
      "Epoch 38, Batch 1025, LR 0.000000 Loss 4.419283, Accuracy 89.926%\n",
      "Epoch 38, Batch 1026, LR 0.000000 Loss 4.419136, Accuracy 89.924%\n",
      "Epoch 38, Batch 1027, LR 0.000000 Loss 4.419355, Accuracy 89.922%\n",
      "Epoch 38, Batch 1028, LR 0.000000 Loss 4.419297, Accuracy 89.921%\n",
      "Epoch 38, Batch 1029, LR 0.000000 Loss 4.418697, Accuracy 89.921%\n",
      "Epoch 38, Batch 1030, LR 0.000000 Loss 4.417971, Accuracy 89.923%\n",
      "Epoch 38, Batch 1031, LR 0.000000 Loss 4.417920, Accuracy 89.920%\n",
      "Epoch 38, Batch 1032, LR 0.000000 Loss 4.417945, Accuracy 89.922%\n",
      "Epoch 38, Batch 1033, LR 0.000000 Loss 4.417775, Accuracy 89.923%\n",
      "Epoch 38, Batch 1034, LR 0.000000 Loss 4.417763, Accuracy 89.922%\n",
      "Epoch 38, Batch 1035, LR 0.000000 Loss 4.418244, Accuracy 89.917%\n",
      "Epoch 38, Batch 1036, LR 0.000000 Loss 4.417816, Accuracy 89.918%\n",
      "Epoch 38, Batch 1037, LR 0.000000 Loss 4.418435, Accuracy 89.914%\n",
      "Epoch 38, Batch 1038, LR 0.000000 Loss 4.418746, Accuracy 89.909%\n",
      "Epoch 38, Batch 1039, LR 0.000000 Loss 4.418734, Accuracy 89.909%\n",
      "Epoch 38, Batch 1040, LR 0.000000 Loss 4.418643, Accuracy 89.911%\n",
      "Epoch 38, Batch 1041, LR 0.000000 Loss 4.418318, Accuracy 89.914%\n",
      "Epoch 38, Batch 1042, LR 0.000000 Loss 4.418259, Accuracy 89.913%\n",
      "Epoch 38, Batch 1043, LR 0.000000 Loss 4.418537, Accuracy 89.911%\n",
      "Epoch 38, Batch 1044, LR 0.000000 Loss 4.419113, Accuracy 89.911%\n",
      "Epoch 38, Batch 1045, LR 0.000000 Loss 4.418925, Accuracy 89.912%\n",
      "Epoch 38, Batch 1046, LR 0.000000 Loss 4.418571, Accuracy 89.910%\n",
      "Epoch 38, Batch 1047, LR 0.000000 Loss 4.418860, Accuracy 89.907%\n",
      "Epoch 38, Loss (train set) 4.418860, Accuracy (train set) 89.907%\n",
      "Epoch 39, Batch 1, LR 0.000000 Loss 3.956323, Accuracy 89.062%\n",
      "Epoch 39, Batch 2, LR 0.000000 Loss 4.116436, Accuracy 91.016%\n",
      "Epoch 39, Batch 3, LR 0.000000 Loss 4.359930, Accuracy 91.146%\n",
      "Epoch 39, Batch 4, LR 0.000000 Loss 4.613260, Accuracy 89.453%\n",
      "Epoch 39, Batch 5, LR 0.000000 Loss 4.567670, Accuracy 89.531%\n",
      "Epoch 39, Batch 6, LR 0.000000 Loss 4.439141, Accuracy 89.714%\n",
      "Epoch 39, Batch 7, LR 0.000000 Loss 4.510170, Accuracy 89.509%\n",
      "Epoch 39, Batch 8, LR 0.000000 Loss 4.614702, Accuracy 88.965%\n",
      "Epoch 39, Batch 9, LR 0.000000 Loss 4.596366, Accuracy 88.889%\n",
      "Epoch 39, Batch 10, LR 0.000000 Loss 4.535570, Accuracy 89.219%\n",
      "Epoch 39, Batch 11, LR 0.000000 Loss 4.437347, Accuracy 89.489%\n",
      "Epoch 39, Batch 12, LR 0.000000 Loss 4.482479, Accuracy 89.128%\n",
      "Epoch 39, Batch 13, LR 0.000000 Loss 4.489214, Accuracy 89.123%\n",
      "Epoch 39, Batch 14, LR 0.000000 Loss 4.479124, Accuracy 89.286%\n",
      "Epoch 39, Batch 15, LR 0.000000 Loss 4.484735, Accuracy 89.323%\n",
      "Epoch 39, Batch 16, LR 0.000000 Loss 4.479490, Accuracy 89.307%\n",
      "Epoch 39, Batch 17, LR 0.000000 Loss 4.458442, Accuracy 89.292%\n",
      "Epoch 39, Batch 18, LR 0.000000 Loss 4.463177, Accuracy 89.540%\n",
      "Epoch 39, Batch 19, LR 0.000000 Loss 4.491469, Accuracy 89.515%\n",
      "Epoch 39, Batch 20, LR 0.000000 Loss 4.506639, Accuracy 89.648%\n",
      "Epoch 39, Batch 21, LR 0.000000 Loss 4.491360, Accuracy 89.844%\n",
      "Epoch 39, Batch 22, LR 0.000000 Loss 4.474600, Accuracy 89.950%\n",
      "Epoch 39, Batch 23, LR 0.000000 Loss 4.494654, Accuracy 89.844%\n",
      "Epoch 39, Batch 24, LR 0.000000 Loss 4.503183, Accuracy 89.779%\n",
      "Epoch 39, Batch 25, LR 0.000000 Loss 4.503178, Accuracy 89.844%\n",
      "Epoch 39, Batch 26, LR 0.000000 Loss 4.524982, Accuracy 89.784%\n",
      "Epoch 39, Batch 27, LR 0.000000 Loss 4.510707, Accuracy 89.815%\n",
      "Epoch 39, Batch 28, LR 0.000000 Loss 4.479758, Accuracy 89.900%\n",
      "Epoch 39, Batch 29, LR 0.000000 Loss 4.488124, Accuracy 89.898%\n",
      "Epoch 39, Batch 30, LR 0.000000 Loss 4.482494, Accuracy 89.870%\n",
      "Epoch 39, Batch 31, LR 0.000000 Loss 4.479610, Accuracy 89.869%\n",
      "Epoch 39, Batch 32, LR 0.000000 Loss 4.495305, Accuracy 89.795%\n",
      "Epoch 39, Batch 33, LR 0.000000 Loss 4.478109, Accuracy 89.891%\n",
      "Epoch 39, Batch 34, LR 0.000000 Loss 4.461621, Accuracy 90.028%\n",
      "Epoch 39, Batch 35, LR 0.000000 Loss 4.452792, Accuracy 90.022%\n",
      "Epoch 39, Batch 36, LR 0.000000 Loss 4.451983, Accuracy 90.061%\n",
      "Epoch 39, Batch 37, LR 0.000000 Loss 4.440452, Accuracy 90.055%\n",
      "Epoch 39, Batch 38, LR 0.000000 Loss 4.453914, Accuracy 89.988%\n",
      "Epoch 39, Batch 39, LR 0.000000 Loss 4.453362, Accuracy 90.064%\n",
      "Epoch 39, Batch 40, LR 0.000000 Loss 4.434515, Accuracy 90.156%\n",
      "Epoch 39, Batch 41, LR 0.000000 Loss 4.429270, Accuracy 90.130%\n",
      "Epoch 39, Batch 42, LR 0.000000 Loss 4.423834, Accuracy 90.123%\n",
      "Epoch 39, Batch 43, LR 0.000000 Loss 4.422462, Accuracy 90.062%\n",
      "Epoch 39, Batch 44, LR 0.000000 Loss 4.418911, Accuracy 90.181%\n",
      "Epoch 39, Batch 45, LR 0.000000 Loss 4.439627, Accuracy 90.035%\n",
      "Epoch 39, Batch 46, LR 0.000000 Loss 4.423542, Accuracy 90.014%\n",
      "Epoch 39, Batch 47, LR 0.000000 Loss 4.415556, Accuracy 89.993%\n",
      "Epoch 39, Batch 48, LR 0.000000 Loss 4.410190, Accuracy 90.023%\n",
      "Epoch 39, Batch 49, LR 0.000000 Loss 4.405704, Accuracy 90.083%\n",
      "Epoch 39, Batch 50, LR 0.000000 Loss 4.412038, Accuracy 90.062%\n",
      "Epoch 39, Batch 51, LR 0.000000 Loss 4.405156, Accuracy 90.119%\n",
      "Epoch 39, Batch 52, LR 0.000000 Loss 4.404008, Accuracy 90.054%\n",
      "Epoch 39, Batch 53, LR 0.000000 Loss 4.398848, Accuracy 90.094%\n",
      "Epoch 39, Batch 54, LR 0.000000 Loss 4.400505, Accuracy 90.090%\n",
      "Epoch 39, Batch 55, LR 0.000000 Loss 4.402172, Accuracy 90.043%\n",
      "Epoch 39, Batch 56, LR 0.000000 Loss 4.426170, Accuracy 89.914%\n",
      "Epoch 39, Batch 57, LR 0.000000 Loss 4.424889, Accuracy 89.885%\n",
      "Epoch 39, Batch 58, LR 0.000000 Loss 4.432128, Accuracy 89.871%\n",
      "Epoch 39, Batch 59, LR 0.000000 Loss 4.436867, Accuracy 89.870%\n",
      "Epoch 39, Batch 60, LR 0.000000 Loss 4.428743, Accuracy 89.870%\n",
      "Epoch 39, Batch 61, LR 0.000000 Loss 4.427262, Accuracy 89.908%\n",
      "Epoch 39, Batch 62, LR 0.000000 Loss 4.427443, Accuracy 89.856%\n",
      "Epoch 39, Batch 63, LR 0.000000 Loss 4.433664, Accuracy 89.819%\n",
      "Epoch 39, Batch 64, LR 0.000000 Loss 4.443482, Accuracy 89.746%\n",
      "Epoch 39, Batch 65, LR 0.000000 Loss 4.438178, Accuracy 89.736%\n",
      "Epoch 39, Batch 66, LR 0.000000 Loss 4.428477, Accuracy 89.737%\n",
      "Epoch 39, Batch 67, LR 0.000000 Loss 4.426056, Accuracy 89.762%\n",
      "Epoch 39, Batch 68, LR 0.000000 Loss 4.431145, Accuracy 89.717%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 69, LR 0.000000 Loss 4.434052, Accuracy 89.742%\n",
      "Epoch 39, Batch 70, LR 0.000000 Loss 4.437938, Accuracy 89.654%\n",
      "Epoch 39, Batch 71, LR 0.000000 Loss 4.434240, Accuracy 89.690%\n",
      "Epoch 39, Batch 72, LR 0.000000 Loss 4.435418, Accuracy 89.670%\n",
      "Epoch 39, Batch 73, LR 0.000000 Loss 4.442708, Accuracy 89.587%\n",
      "Epoch 39, Batch 74, LR 0.000000 Loss 4.438410, Accuracy 89.590%\n",
      "Epoch 39, Batch 75, LR 0.000000 Loss 4.441344, Accuracy 89.583%\n",
      "Epoch 39, Batch 76, LR 0.000000 Loss 4.438002, Accuracy 89.638%\n",
      "Epoch 39, Batch 77, LR 0.000000 Loss 4.436081, Accuracy 89.671%\n",
      "Epoch 39, Batch 78, LR 0.000000 Loss 4.426089, Accuracy 89.714%\n",
      "Epoch 39, Batch 79, LR 0.000000 Loss 4.435911, Accuracy 89.606%\n",
      "Epoch 39, Batch 80, LR 0.000000 Loss 4.442341, Accuracy 89.600%\n",
      "Epoch 39, Batch 81, LR 0.000000 Loss 4.441736, Accuracy 89.603%\n",
      "Epoch 39, Batch 82, LR 0.000000 Loss 4.444033, Accuracy 89.596%\n",
      "Epoch 39, Batch 83, LR 0.000000 Loss 4.437655, Accuracy 89.608%\n",
      "Epoch 39, Batch 84, LR 0.000000 Loss 4.435737, Accuracy 89.593%\n",
      "Epoch 39, Batch 85, LR 0.000000 Loss 4.448116, Accuracy 89.550%\n",
      "Epoch 39, Batch 86, LR 0.000000 Loss 4.447017, Accuracy 89.544%\n",
      "Epoch 39, Batch 87, LR 0.000000 Loss 4.450376, Accuracy 89.538%\n",
      "Epoch 39, Batch 88, LR 0.000000 Loss 4.454904, Accuracy 89.551%\n",
      "Epoch 39, Batch 89, LR 0.000000 Loss 4.456454, Accuracy 89.554%\n",
      "Epoch 39, Batch 90, LR 0.000000 Loss 4.451888, Accuracy 89.557%\n",
      "Epoch 39, Batch 91, LR 0.000000 Loss 4.457144, Accuracy 89.552%\n",
      "Epoch 39, Batch 92, LR 0.000000 Loss 4.451097, Accuracy 89.614%\n",
      "Epoch 39, Batch 93, LR 0.000000 Loss 4.453295, Accuracy 89.625%\n",
      "Epoch 39, Batch 94, LR 0.000000 Loss 4.456947, Accuracy 89.619%\n",
      "Epoch 39, Batch 95, LR 0.000000 Loss 4.461396, Accuracy 89.622%\n",
      "Epoch 39, Batch 96, LR 0.000000 Loss 4.463675, Accuracy 89.657%\n",
      "Epoch 39, Batch 97, LR 0.000000 Loss 4.474415, Accuracy 89.618%\n",
      "Epoch 39, Batch 98, LR 0.000000 Loss 4.479091, Accuracy 89.613%\n",
      "Epoch 39, Batch 99, LR 0.000000 Loss 4.471717, Accuracy 89.639%\n",
      "Epoch 39, Batch 100, LR 0.000000 Loss 4.477610, Accuracy 89.625%\n",
      "Epoch 39, Batch 101, LR 0.000000 Loss 4.472597, Accuracy 89.674%\n",
      "Epoch 39, Batch 102, LR 0.000000 Loss 4.468056, Accuracy 89.675%\n",
      "Epoch 39, Batch 103, LR 0.000000 Loss 4.466666, Accuracy 89.677%\n",
      "Epoch 39, Batch 104, LR 0.000000 Loss 4.468147, Accuracy 89.694%\n",
      "Epoch 39, Batch 105, LR 0.000000 Loss 4.474239, Accuracy 89.688%\n",
      "Epoch 39, Batch 106, LR 0.000000 Loss 4.473682, Accuracy 89.682%\n",
      "Epoch 39, Batch 107, LR 0.000000 Loss 4.464924, Accuracy 89.727%\n",
      "Epoch 39, Batch 108, LR 0.000000 Loss 4.464239, Accuracy 89.779%\n",
      "Epoch 39, Batch 109, LR 0.000000 Loss 4.466623, Accuracy 89.815%\n",
      "Epoch 39, Batch 110, LR 0.000000 Loss 4.467930, Accuracy 89.822%\n",
      "Epoch 39, Batch 111, LR 0.000000 Loss 4.460414, Accuracy 89.858%\n",
      "Epoch 39, Batch 112, LR 0.000000 Loss 4.455838, Accuracy 89.872%\n",
      "Epoch 39, Batch 113, LR 0.000000 Loss 4.452912, Accuracy 89.885%\n",
      "Epoch 39, Batch 114, LR 0.000000 Loss 4.450367, Accuracy 89.864%\n",
      "Epoch 39, Batch 115, LR 0.000000 Loss 4.445861, Accuracy 89.871%\n",
      "Epoch 39, Batch 116, LR 0.000000 Loss 4.445931, Accuracy 89.877%\n",
      "Epoch 39, Batch 117, LR 0.000000 Loss 4.444355, Accuracy 89.884%\n",
      "Epoch 39, Batch 118, LR 0.000000 Loss 4.447809, Accuracy 89.870%\n",
      "Epoch 39, Batch 119, LR 0.000000 Loss 4.444076, Accuracy 89.903%\n",
      "Epoch 39, Batch 120, LR 0.000000 Loss 4.449230, Accuracy 89.876%\n",
      "Epoch 39, Batch 121, LR 0.000000 Loss 4.447014, Accuracy 89.921%\n",
      "Epoch 39, Batch 122, LR 0.000000 Loss 4.440156, Accuracy 89.959%\n",
      "Epoch 39, Batch 123, LR 0.000000 Loss 4.434850, Accuracy 89.983%\n",
      "Epoch 39, Batch 124, LR 0.000000 Loss 4.429546, Accuracy 90.020%\n",
      "Epoch 39, Batch 125, LR 0.000000 Loss 4.427990, Accuracy 90.025%\n",
      "Epoch 39, Batch 126, LR 0.000000 Loss 4.431154, Accuracy 89.999%\n",
      "Epoch 39, Batch 127, LR 0.000000 Loss 4.435383, Accuracy 89.985%\n",
      "Epoch 39, Batch 128, LR 0.000000 Loss 4.432745, Accuracy 90.015%\n",
      "Epoch 39, Batch 129, LR 0.000000 Loss 4.427147, Accuracy 90.025%\n",
      "Epoch 39, Batch 130, LR 0.000000 Loss 4.429212, Accuracy 90.018%\n",
      "Epoch 39, Batch 131, LR 0.000000 Loss 4.431032, Accuracy 90.011%\n",
      "Epoch 39, Batch 132, LR 0.000000 Loss 4.430629, Accuracy 90.021%\n",
      "Epoch 39, Batch 133, LR 0.000000 Loss 4.430486, Accuracy 90.032%\n",
      "Epoch 39, Batch 134, LR 0.000000 Loss 4.428392, Accuracy 90.054%\n",
      "Epoch 39, Batch 135, LR 0.000000 Loss 4.425519, Accuracy 90.046%\n",
      "Epoch 39, Batch 136, LR 0.000000 Loss 4.427710, Accuracy 90.074%\n",
      "Epoch 39, Batch 137, LR 0.000000 Loss 4.427648, Accuracy 90.066%\n",
      "Epoch 39, Batch 138, LR 0.000000 Loss 4.424138, Accuracy 90.076%\n",
      "Epoch 39, Batch 139, LR 0.000000 Loss 4.421155, Accuracy 90.091%\n",
      "Epoch 39, Batch 140, LR 0.000000 Loss 4.423212, Accuracy 90.078%\n",
      "Epoch 39, Batch 141, LR 0.000000 Loss 4.423035, Accuracy 90.076%\n",
      "Epoch 39, Batch 142, LR 0.000000 Loss 4.424772, Accuracy 90.042%\n",
      "Epoch 39, Batch 143, LR 0.000000 Loss 4.424943, Accuracy 90.035%\n",
      "Epoch 39, Batch 144, LR 0.000000 Loss 4.425920, Accuracy 90.007%\n",
      "Epoch 39, Batch 145, LR 0.000000 Loss 4.425484, Accuracy 90.027%\n",
      "Epoch 39, Batch 146, LR 0.000000 Loss 4.424611, Accuracy 90.026%\n",
      "Epoch 39, Batch 147, LR 0.000000 Loss 4.424210, Accuracy 90.046%\n",
      "Epoch 39, Batch 148, LR 0.000000 Loss 4.422073, Accuracy 90.050%\n",
      "Epoch 39, Batch 149, LR 0.000000 Loss 4.423271, Accuracy 90.043%\n",
      "Epoch 39, Batch 150, LR 0.000000 Loss 4.417636, Accuracy 90.057%\n",
      "Epoch 39, Batch 151, LR 0.000000 Loss 4.422438, Accuracy 90.014%\n",
      "Epoch 39, Batch 152, LR 0.000000 Loss 4.419598, Accuracy 90.024%\n",
      "Epoch 39, Batch 153, LR 0.000000 Loss 4.419403, Accuracy 90.022%\n",
      "Epoch 39, Batch 154, LR 0.000000 Loss 4.421005, Accuracy 90.016%\n",
      "Epoch 39, Batch 155, LR 0.000000 Loss 4.418207, Accuracy 90.020%\n",
      "Epoch 39, Batch 156, LR 0.000000 Loss 4.420404, Accuracy 90.014%\n",
      "Epoch 39, Batch 157, LR 0.000000 Loss 4.423998, Accuracy 89.983%\n",
      "Epoch 39, Batch 158, LR 0.000000 Loss 4.419164, Accuracy 89.997%\n",
      "Epoch 39, Batch 159, LR 0.000000 Loss 4.416409, Accuracy 90.016%\n",
      "Epoch 39, Batch 160, LR 0.000000 Loss 4.420841, Accuracy 90.000%\n",
      "Epoch 39, Batch 161, LR 0.000000 Loss 4.418875, Accuracy 90.018%\n",
      "Epoch 39, Batch 162, LR 0.000000 Loss 4.415929, Accuracy 90.046%\n",
      "Epoch 39, Batch 163, LR 0.000000 Loss 4.424293, Accuracy 89.992%\n",
      "Epoch 39, Batch 164, LR 0.000000 Loss 4.427896, Accuracy 89.977%\n",
      "Epoch 39, Batch 165, LR 0.000000 Loss 4.431284, Accuracy 89.962%\n",
      "Epoch 39, Batch 166, LR 0.000000 Loss 4.431942, Accuracy 89.976%\n",
      "Epoch 39, Batch 167, LR 0.000000 Loss 4.430158, Accuracy 89.970%\n",
      "Epoch 39, Batch 168, LR 0.000000 Loss 4.429026, Accuracy 89.969%\n",
      "Epoch 39, Batch 169, LR 0.000000 Loss 4.427557, Accuracy 89.964%\n",
      "Epoch 39, Batch 170, LR 0.000000 Loss 4.426758, Accuracy 89.959%\n",
      "Epoch 39, Batch 171, LR 0.000000 Loss 4.424973, Accuracy 89.995%\n",
      "Epoch 39, Batch 172, LR 0.000000 Loss 4.420907, Accuracy 90.025%\n",
      "Epoch 39, Batch 173, LR 0.000000 Loss 4.418051, Accuracy 90.029%\n",
      "Epoch 39, Batch 174, LR 0.000000 Loss 4.418620, Accuracy 90.028%\n",
      "Epoch 39, Batch 175, LR 0.000000 Loss 4.419939, Accuracy 90.022%\n",
      "Epoch 39, Batch 176, LR 0.000000 Loss 4.419801, Accuracy 90.008%\n",
      "Epoch 39, Batch 177, LR 0.000000 Loss 4.424671, Accuracy 89.989%\n",
      "Epoch 39, Batch 178, LR 0.000000 Loss 4.423734, Accuracy 89.997%\n",
      "Epoch 39, Batch 179, LR 0.000000 Loss 4.424012, Accuracy 89.988%\n",
      "Epoch 39, Batch 180, LR 0.000000 Loss 4.425126, Accuracy 89.961%\n",
      "Epoch 39, Batch 181, LR 0.000000 Loss 4.424524, Accuracy 89.956%\n",
      "Epoch 39, Batch 182, LR 0.000000 Loss 4.428272, Accuracy 89.942%\n",
      "Epoch 39, Batch 183, LR 0.000000 Loss 4.425890, Accuracy 89.950%\n",
      "Epoch 39, Batch 184, LR 0.000000 Loss 4.425305, Accuracy 89.963%\n",
      "Epoch 39, Batch 185, LR 0.000000 Loss 4.428076, Accuracy 89.970%\n",
      "Epoch 39, Batch 186, LR 0.000000 Loss 4.422826, Accuracy 89.982%\n",
      "Epoch 39, Batch 187, LR 0.000000 Loss 4.425090, Accuracy 89.973%\n",
      "Epoch 39, Batch 188, LR 0.000000 Loss 4.419992, Accuracy 89.998%\n",
      "Epoch 39, Batch 189, LR 0.000000 Loss 4.422909, Accuracy 89.968%\n",
      "Epoch 39, Batch 190, LR 0.000000 Loss 4.422885, Accuracy 89.967%\n",
      "Epoch 39, Batch 191, LR 0.000000 Loss 4.425282, Accuracy 89.946%\n",
      "Epoch 39, Batch 192, LR 0.000000 Loss 4.427336, Accuracy 89.925%\n",
      "Epoch 39, Batch 193, LR 0.000000 Loss 4.426469, Accuracy 89.933%\n",
      "Epoch 39, Batch 194, LR 0.000000 Loss 4.427662, Accuracy 89.924%\n",
      "Epoch 39, Batch 195, LR 0.000000 Loss 4.426664, Accuracy 89.924%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 196, LR 0.000000 Loss 4.422923, Accuracy 89.939%\n",
      "Epoch 39, Batch 197, LR 0.000000 Loss 4.425387, Accuracy 89.911%\n",
      "Epoch 39, Batch 198, LR 0.000000 Loss 4.424234, Accuracy 89.907%\n",
      "Epoch 39, Batch 199, LR 0.000000 Loss 4.425373, Accuracy 89.899%\n",
      "Epoch 39, Batch 200, LR 0.000000 Loss 4.425020, Accuracy 89.898%\n",
      "Epoch 39, Batch 201, LR 0.000000 Loss 4.424734, Accuracy 89.883%\n",
      "Epoch 39, Batch 202, LR 0.000000 Loss 4.424743, Accuracy 89.886%\n",
      "Epoch 39, Batch 203, LR 0.000000 Loss 4.426046, Accuracy 89.882%\n",
      "Epoch 39, Batch 204, LR 0.000000 Loss 4.423287, Accuracy 89.894%\n",
      "Epoch 39, Batch 205, LR 0.000000 Loss 4.424040, Accuracy 89.893%\n",
      "Epoch 39, Batch 206, LR 0.000000 Loss 4.423174, Accuracy 89.904%\n",
      "Epoch 39, Batch 207, LR 0.000000 Loss 4.422488, Accuracy 89.897%\n",
      "Epoch 39, Batch 208, LR 0.000000 Loss 4.419926, Accuracy 89.904%\n",
      "Epoch 39, Batch 209, LR 0.000000 Loss 4.418170, Accuracy 89.907%\n",
      "Epoch 39, Batch 210, LR 0.000000 Loss 4.422867, Accuracy 89.892%\n",
      "Epoch 39, Batch 211, LR 0.000000 Loss 4.423344, Accuracy 89.884%\n",
      "Epoch 39, Batch 212, LR 0.000000 Loss 4.420915, Accuracy 89.906%\n",
      "Epoch 39, Batch 213, LR 0.000000 Loss 4.420063, Accuracy 89.921%\n",
      "Epoch 39, Batch 214, LR 0.000000 Loss 4.424433, Accuracy 89.906%\n",
      "Epoch 39, Batch 215, LR 0.000000 Loss 4.421222, Accuracy 89.931%\n",
      "Epoch 39, Batch 216, LR 0.000000 Loss 4.418973, Accuracy 89.941%\n",
      "Epoch 39, Batch 217, LR 0.000000 Loss 4.418126, Accuracy 89.948%\n",
      "Epoch 39, Batch 218, LR 0.000000 Loss 4.418553, Accuracy 89.933%\n",
      "Epoch 39, Batch 219, LR 0.000000 Loss 4.417869, Accuracy 89.929%\n",
      "Epoch 39, Batch 220, LR 0.000000 Loss 4.415557, Accuracy 89.950%\n",
      "Epoch 39, Batch 221, LR 0.000000 Loss 4.416764, Accuracy 89.950%\n",
      "Epoch 39, Batch 222, LR 0.000000 Loss 4.416282, Accuracy 89.949%\n",
      "Epoch 39, Batch 223, LR 0.000000 Loss 4.415424, Accuracy 89.952%\n",
      "Epoch 39, Batch 224, LR 0.000000 Loss 4.414663, Accuracy 89.962%\n",
      "Epoch 39, Batch 225, LR 0.000000 Loss 4.416699, Accuracy 89.944%\n",
      "Epoch 39, Batch 226, LR 0.000000 Loss 4.419210, Accuracy 89.947%\n",
      "Epoch 39, Batch 227, LR 0.000000 Loss 4.419162, Accuracy 89.954%\n",
      "Epoch 39, Batch 228, LR 0.000000 Loss 4.419639, Accuracy 89.960%\n",
      "Epoch 39, Batch 229, LR 0.000000 Loss 4.418888, Accuracy 89.956%\n",
      "Epoch 39, Batch 230, LR 0.000000 Loss 4.418671, Accuracy 89.956%\n",
      "Epoch 39, Batch 231, LR 0.000000 Loss 4.414500, Accuracy 89.979%\n",
      "Epoch 39, Batch 232, LR 0.000000 Loss 4.415849, Accuracy 89.975%\n",
      "Epoch 39, Batch 233, LR 0.000000 Loss 4.419979, Accuracy 89.954%\n",
      "Epoch 39, Batch 234, LR 0.000000 Loss 4.418098, Accuracy 89.967%\n",
      "Epoch 39, Batch 235, LR 0.000000 Loss 4.418254, Accuracy 89.977%\n",
      "Epoch 39, Batch 236, LR 0.000000 Loss 4.418589, Accuracy 89.970%\n",
      "Epoch 39, Batch 237, LR 0.000000 Loss 4.418941, Accuracy 89.962%\n",
      "Epoch 39, Batch 238, LR 0.000000 Loss 4.420336, Accuracy 89.955%\n",
      "Epoch 39, Batch 239, LR 0.000000 Loss 4.420867, Accuracy 89.952%\n",
      "Epoch 39, Batch 240, LR 0.000000 Loss 4.421062, Accuracy 89.941%\n",
      "Epoch 39, Batch 241, LR 0.000000 Loss 4.424057, Accuracy 89.928%\n",
      "Epoch 39, Batch 242, LR 0.000000 Loss 4.423501, Accuracy 89.928%\n",
      "Epoch 39, Batch 243, LR 0.000000 Loss 4.421851, Accuracy 89.918%\n",
      "Epoch 39, Batch 244, LR 0.000000 Loss 4.424832, Accuracy 89.905%\n",
      "Epoch 39, Batch 245, LR 0.000000 Loss 4.425342, Accuracy 89.901%\n",
      "Epoch 39, Batch 246, LR 0.000000 Loss 4.427884, Accuracy 89.907%\n",
      "Epoch 39, Batch 247, LR 0.000000 Loss 4.428044, Accuracy 89.913%\n",
      "Epoch 39, Batch 248, LR 0.000000 Loss 4.430038, Accuracy 89.910%\n",
      "Epoch 39, Batch 249, LR 0.000000 Loss 4.432123, Accuracy 89.881%\n",
      "Epoch 39, Batch 250, LR 0.000000 Loss 4.431177, Accuracy 89.891%\n",
      "Epoch 39, Batch 251, LR 0.000000 Loss 4.430752, Accuracy 89.887%\n",
      "Epoch 39, Batch 252, LR 0.000000 Loss 4.430724, Accuracy 89.893%\n",
      "Epoch 39, Batch 253, LR 0.000000 Loss 4.434329, Accuracy 89.862%\n",
      "Epoch 39, Batch 254, LR 0.000000 Loss 4.430559, Accuracy 89.871%\n",
      "Epoch 39, Batch 255, LR 0.000000 Loss 4.429744, Accuracy 89.877%\n",
      "Epoch 39, Batch 256, LR 0.000000 Loss 4.430190, Accuracy 89.874%\n",
      "Epoch 39, Batch 257, LR 0.000000 Loss 4.428398, Accuracy 89.877%\n",
      "Epoch 39, Batch 258, LR 0.000000 Loss 4.427844, Accuracy 89.871%\n",
      "Epoch 39, Batch 259, LR 0.000000 Loss 4.429638, Accuracy 89.874%\n",
      "Epoch 39, Batch 260, LR 0.000000 Loss 4.429680, Accuracy 89.880%\n",
      "Epoch 39, Batch 261, LR 0.000000 Loss 4.431934, Accuracy 89.874%\n",
      "Epoch 39, Batch 262, LR 0.000000 Loss 4.430582, Accuracy 89.883%\n",
      "Epoch 39, Batch 263, LR 0.000000 Loss 4.429363, Accuracy 89.879%\n",
      "Epoch 39, Batch 264, LR 0.000000 Loss 4.429029, Accuracy 89.870%\n",
      "Epoch 39, Batch 265, LR 0.000000 Loss 4.430324, Accuracy 89.864%\n",
      "Epoch 39, Batch 266, LR 0.000000 Loss 4.430751, Accuracy 89.870%\n",
      "Epoch 39, Batch 267, LR 0.000000 Loss 4.431803, Accuracy 89.855%\n",
      "Epoch 39, Batch 268, LR 0.000000 Loss 4.432316, Accuracy 89.852%\n",
      "Epoch 39, Batch 269, LR 0.000000 Loss 4.429475, Accuracy 89.855%\n",
      "Epoch 39, Batch 270, LR 0.000000 Loss 4.430970, Accuracy 89.850%\n",
      "Epoch 39, Batch 271, LR 0.000000 Loss 4.433376, Accuracy 89.832%\n",
      "Epoch 39, Batch 272, LR 0.000000 Loss 4.435150, Accuracy 89.824%\n",
      "Epoch 39, Batch 273, LR 0.000000 Loss 4.435549, Accuracy 89.832%\n",
      "Epoch 39, Batch 274, LR 0.000000 Loss 4.435035, Accuracy 89.835%\n",
      "Epoch 39, Batch 275, LR 0.000000 Loss 4.434347, Accuracy 89.847%\n",
      "Epoch 39, Batch 276, LR 0.000000 Loss 4.434982, Accuracy 89.838%\n",
      "Epoch 39, Batch 277, LR 0.000000 Loss 4.438063, Accuracy 89.810%\n",
      "Epoch 39, Batch 278, LR 0.000000 Loss 4.434718, Accuracy 89.821%\n",
      "Epoch 39, Batch 279, LR 0.000000 Loss 4.435947, Accuracy 89.807%\n",
      "Epoch 39, Batch 280, LR 0.000000 Loss 4.434840, Accuracy 89.807%\n",
      "Epoch 39, Batch 281, LR 0.000000 Loss 4.433785, Accuracy 89.808%\n",
      "Epoch 39, Batch 282, LR 0.000000 Loss 4.433185, Accuracy 89.811%\n",
      "Epoch 39, Batch 283, LR 0.000000 Loss 4.433963, Accuracy 89.802%\n",
      "Epoch 39, Batch 284, LR 0.000000 Loss 4.432677, Accuracy 89.813%\n",
      "Epoch 39, Batch 285, LR 0.000000 Loss 4.434419, Accuracy 89.811%\n",
      "Epoch 39, Batch 286, LR 0.000000 Loss 4.434471, Accuracy 89.819%\n",
      "Epoch 39, Batch 287, LR 0.000000 Loss 4.434225, Accuracy 89.827%\n",
      "Epoch 39, Batch 288, LR 0.000000 Loss 4.435650, Accuracy 89.819%\n",
      "Epoch 39, Batch 289, LR 0.000000 Loss 4.435447, Accuracy 89.830%\n",
      "Epoch 39, Batch 290, LR 0.000000 Loss 4.435710, Accuracy 89.833%\n",
      "Epoch 39, Batch 291, LR 0.000000 Loss 4.436685, Accuracy 89.833%\n",
      "Epoch 39, Batch 292, LR 0.000000 Loss 4.436986, Accuracy 89.838%\n",
      "Epoch 39, Batch 293, LR 0.000000 Loss 4.435497, Accuracy 89.838%\n",
      "Epoch 39, Batch 294, LR 0.000000 Loss 4.433801, Accuracy 89.846%\n",
      "Epoch 39, Batch 295, LR 0.000000 Loss 4.436366, Accuracy 89.844%\n",
      "Epoch 39, Batch 296, LR 0.000000 Loss 4.438117, Accuracy 89.841%\n",
      "Epoch 39, Batch 297, LR 0.000000 Loss 4.437793, Accuracy 89.844%\n",
      "Epoch 39, Batch 298, LR 0.000000 Loss 4.435699, Accuracy 89.862%\n",
      "Epoch 39, Batch 299, LR 0.000000 Loss 4.438291, Accuracy 89.839%\n",
      "Epoch 39, Batch 300, LR 0.000000 Loss 4.437820, Accuracy 89.852%\n",
      "Epoch 39, Batch 301, LR 0.000000 Loss 4.437503, Accuracy 89.852%\n",
      "Epoch 39, Batch 302, LR 0.000000 Loss 4.438769, Accuracy 89.844%\n",
      "Epoch 39, Batch 303, LR 0.000000 Loss 4.438543, Accuracy 89.841%\n",
      "Epoch 39, Batch 304, LR 0.000000 Loss 4.438702, Accuracy 89.841%\n",
      "Epoch 39, Batch 305, LR 0.000000 Loss 4.439445, Accuracy 89.834%\n",
      "Epoch 39, Batch 306, LR 0.000000 Loss 4.439249, Accuracy 89.841%\n",
      "Epoch 39, Batch 307, LR 0.000000 Loss 4.443981, Accuracy 89.813%\n",
      "Epoch 39, Batch 308, LR 0.000000 Loss 4.445339, Accuracy 89.813%\n",
      "Epoch 39, Batch 309, LR 0.000000 Loss 4.446518, Accuracy 89.796%\n",
      "Epoch 39, Batch 310, LR 0.000000 Loss 4.446255, Accuracy 89.796%\n",
      "Epoch 39, Batch 311, LR 0.000000 Loss 4.448204, Accuracy 89.786%\n",
      "Epoch 39, Batch 312, LR 0.000000 Loss 4.446353, Accuracy 89.796%\n",
      "Epoch 39, Batch 313, LR 0.000000 Loss 4.446424, Accuracy 89.794%\n",
      "Epoch 39, Batch 314, LR 0.000000 Loss 4.444243, Accuracy 89.799%\n",
      "Epoch 39, Batch 315, LR 0.000000 Loss 4.444284, Accuracy 89.804%\n",
      "Epoch 39, Batch 316, LR 0.000000 Loss 4.446228, Accuracy 89.784%\n",
      "Epoch 39, Batch 317, LR 0.000000 Loss 4.446153, Accuracy 89.785%\n",
      "Epoch 39, Batch 318, LR 0.000000 Loss 4.445924, Accuracy 89.790%\n",
      "Epoch 39, Batch 319, LR 0.000000 Loss 4.446502, Accuracy 89.787%\n",
      "Epoch 39, Batch 320, LR 0.000000 Loss 4.447032, Accuracy 89.788%\n",
      "Epoch 39, Batch 321, LR 0.000000 Loss 4.445450, Accuracy 89.795%\n",
      "Epoch 39, Batch 322, LR 0.000000 Loss 4.445863, Accuracy 89.790%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 323, LR 0.000000 Loss 4.447038, Accuracy 89.800%\n",
      "Epoch 39, Batch 324, LR 0.000000 Loss 4.445449, Accuracy 89.805%\n",
      "Epoch 39, Batch 325, LR 0.000000 Loss 4.445758, Accuracy 89.803%\n",
      "Epoch 39, Batch 326, LR 0.000000 Loss 4.444648, Accuracy 89.808%\n",
      "Epoch 39, Batch 327, LR 0.000000 Loss 4.444877, Accuracy 89.806%\n",
      "Epoch 39, Batch 328, LR 0.000000 Loss 4.443953, Accuracy 89.808%\n",
      "Epoch 39, Batch 329, LR 0.000000 Loss 4.445404, Accuracy 89.799%\n",
      "Epoch 39, Batch 330, LR 0.000000 Loss 4.444682, Accuracy 89.796%\n",
      "Epoch 39, Batch 331, LR 0.000000 Loss 4.443393, Accuracy 89.804%\n",
      "Epoch 39, Batch 332, LR 0.000000 Loss 4.445071, Accuracy 89.799%\n",
      "Epoch 39, Batch 333, LR 0.000000 Loss 4.443238, Accuracy 89.809%\n",
      "Epoch 39, Batch 334, LR 0.000000 Loss 4.442891, Accuracy 89.809%\n",
      "Epoch 39, Batch 335, LR 0.000000 Loss 4.442770, Accuracy 89.806%\n",
      "Epoch 39, Batch 336, LR 0.000000 Loss 4.439426, Accuracy 89.820%\n",
      "Epoch 39, Batch 337, LR 0.000000 Loss 4.437892, Accuracy 89.830%\n",
      "Epoch 39, Batch 338, LR 0.000000 Loss 4.440824, Accuracy 89.821%\n",
      "Epoch 39, Batch 339, LR 0.000000 Loss 4.441656, Accuracy 89.821%\n",
      "Epoch 39, Batch 340, LR 0.000000 Loss 4.441189, Accuracy 89.823%\n",
      "Epoch 39, Batch 341, LR 0.000000 Loss 4.438968, Accuracy 89.832%\n",
      "Epoch 39, Batch 342, LR 0.000000 Loss 4.439015, Accuracy 89.837%\n",
      "Epoch 39, Batch 343, LR 0.000000 Loss 4.437227, Accuracy 89.851%\n",
      "Epoch 39, Batch 344, LR 0.000000 Loss 4.438998, Accuracy 89.846%\n",
      "Epoch 39, Batch 345, LR 0.000000 Loss 4.438614, Accuracy 89.853%\n",
      "Epoch 39, Batch 346, LR 0.000000 Loss 4.439745, Accuracy 89.846%\n",
      "Epoch 39, Batch 347, LR 0.000000 Loss 4.442106, Accuracy 89.844%\n",
      "Epoch 39, Batch 348, LR 0.000000 Loss 4.440790, Accuracy 89.853%\n",
      "Epoch 39, Batch 349, LR 0.000000 Loss 4.440463, Accuracy 89.857%\n",
      "Epoch 39, Batch 350, LR 0.000000 Loss 4.439257, Accuracy 89.868%\n",
      "Epoch 39, Batch 351, LR 0.000000 Loss 4.439413, Accuracy 89.866%\n",
      "Epoch 39, Batch 352, LR 0.000000 Loss 4.439992, Accuracy 89.864%\n",
      "Epoch 39, Batch 353, LR 0.000000 Loss 4.440328, Accuracy 89.864%\n",
      "Epoch 39, Batch 354, LR 0.000000 Loss 4.440107, Accuracy 89.872%\n",
      "Epoch 39, Batch 355, LR 0.000000 Loss 4.440569, Accuracy 89.872%\n",
      "Epoch 39, Batch 356, LR 0.000000 Loss 4.437689, Accuracy 89.888%\n",
      "Epoch 39, Batch 357, LR 0.000000 Loss 4.435076, Accuracy 89.898%\n",
      "Epoch 39, Batch 358, LR 0.000000 Loss 4.433970, Accuracy 89.909%\n",
      "Epoch 39, Batch 359, LR 0.000000 Loss 4.435939, Accuracy 89.892%\n",
      "Epoch 39, Batch 360, LR 0.000000 Loss 4.435269, Accuracy 89.894%\n",
      "Epoch 39, Batch 361, LR 0.000000 Loss 4.434507, Accuracy 89.887%\n",
      "Epoch 39, Batch 362, LR 0.000000 Loss 4.432766, Accuracy 89.900%\n",
      "Epoch 39, Batch 363, LR 0.000000 Loss 4.433234, Accuracy 89.898%\n",
      "Epoch 39, Batch 364, LR 0.000000 Loss 4.433455, Accuracy 89.902%\n",
      "Epoch 39, Batch 365, LR 0.000000 Loss 4.432166, Accuracy 89.910%\n",
      "Epoch 39, Batch 366, LR 0.000000 Loss 4.432151, Accuracy 89.916%\n",
      "Epoch 39, Batch 367, LR 0.000000 Loss 4.430985, Accuracy 89.910%\n",
      "Epoch 39, Batch 368, LR 0.000000 Loss 4.430070, Accuracy 89.903%\n",
      "Epoch 39, Batch 369, LR 0.000000 Loss 4.428799, Accuracy 89.909%\n",
      "Epoch 39, Batch 370, LR 0.000000 Loss 4.428593, Accuracy 89.903%\n",
      "Epoch 39, Batch 371, LR 0.000000 Loss 4.428444, Accuracy 89.899%\n",
      "Epoch 39, Batch 372, LR 0.000000 Loss 4.425696, Accuracy 89.909%\n",
      "Epoch 39, Batch 373, LR 0.000000 Loss 4.426515, Accuracy 89.896%\n",
      "Epoch 39, Batch 374, LR 0.000000 Loss 4.427625, Accuracy 89.894%\n",
      "Epoch 39, Batch 375, LR 0.000000 Loss 4.428233, Accuracy 89.890%\n",
      "Epoch 39, Batch 376, LR 0.000000 Loss 4.427352, Accuracy 89.885%\n",
      "Epoch 39, Batch 377, LR 0.000000 Loss 4.425810, Accuracy 89.900%\n",
      "Epoch 39, Batch 378, LR 0.000000 Loss 4.424008, Accuracy 89.906%\n",
      "Epoch 39, Batch 379, LR 0.000000 Loss 4.424863, Accuracy 89.893%\n",
      "Epoch 39, Batch 380, LR 0.000000 Loss 4.425205, Accuracy 89.889%\n",
      "Epoch 39, Batch 381, LR 0.000000 Loss 4.423692, Accuracy 89.897%\n",
      "Epoch 39, Batch 382, LR 0.000000 Loss 4.423891, Accuracy 89.895%\n",
      "Epoch 39, Batch 383, LR 0.000000 Loss 4.423732, Accuracy 89.895%\n",
      "Epoch 39, Batch 384, LR 0.000000 Loss 4.424157, Accuracy 89.891%\n",
      "Epoch 39, Batch 385, LR 0.000000 Loss 4.424431, Accuracy 89.884%\n",
      "Epoch 39, Batch 386, LR 0.000000 Loss 4.425014, Accuracy 89.880%\n",
      "Epoch 39, Batch 387, LR 0.000000 Loss 4.424491, Accuracy 89.882%\n",
      "Epoch 39, Batch 388, LR 0.000000 Loss 4.424595, Accuracy 89.878%\n",
      "Epoch 39, Batch 389, LR 0.000000 Loss 4.425463, Accuracy 89.872%\n",
      "Epoch 39, Batch 390, LR 0.000000 Loss 4.424976, Accuracy 89.874%\n",
      "Epoch 39, Batch 391, LR 0.000000 Loss 4.424752, Accuracy 89.866%\n",
      "Epoch 39, Batch 392, LR 0.000000 Loss 4.424521, Accuracy 89.860%\n",
      "Epoch 39, Batch 393, LR 0.000000 Loss 4.421782, Accuracy 89.872%\n",
      "Epoch 39, Batch 394, LR 0.000000 Loss 4.420591, Accuracy 89.875%\n",
      "Epoch 39, Batch 395, LR 0.000000 Loss 4.419943, Accuracy 89.885%\n",
      "Epoch 39, Batch 396, LR 0.000000 Loss 4.420411, Accuracy 89.881%\n",
      "Epoch 39, Batch 397, LR 0.000000 Loss 4.418090, Accuracy 89.889%\n",
      "Epoch 39, Batch 398, LR 0.000000 Loss 4.419524, Accuracy 89.881%\n",
      "Epoch 39, Batch 399, LR 0.000000 Loss 4.420139, Accuracy 89.865%\n",
      "Epoch 39, Batch 400, LR 0.000000 Loss 4.421788, Accuracy 89.863%\n",
      "Epoch 39, Batch 401, LR 0.000000 Loss 4.420159, Accuracy 89.865%\n",
      "Epoch 39, Batch 402, LR 0.000000 Loss 4.419479, Accuracy 89.867%\n",
      "Epoch 39, Batch 403, LR 0.000000 Loss 4.419834, Accuracy 89.867%\n",
      "Epoch 39, Batch 404, LR 0.000000 Loss 4.419598, Accuracy 89.867%\n",
      "Epoch 39, Batch 405, LR 0.000000 Loss 4.419685, Accuracy 89.875%\n",
      "Epoch 39, Batch 406, LR 0.000000 Loss 4.419440, Accuracy 89.873%\n",
      "Epoch 39, Batch 407, LR 0.000000 Loss 4.419076, Accuracy 89.878%\n",
      "Epoch 39, Batch 408, LR 0.000000 Loss 4.418681, Accuracy 89.878%\n",
      "Epoch 39, Batch 409, LR 0.000000 Loss 4.419610, Accuracy 89.878%\n",
      "Epoch 39, Batch 410, LR 0.000000 Loss 4.418507, Accuracy 89.878%\n",
      "Epoch 39, Batch 411, LR 0.000000 Loss 4.417989, Accuracy 89.884%\n",
      "Epoch 39, Batch 412, LR 0.000000 Loss 4.419852, Accuracy 89.874%\n",
      "Epoch 39, Batch 413, LR 0.000000 Loss 4.419208, Accuracy 89.878%\n",
      "Epoch 39, Batch 414, LR 0.000000 Loss 4.418877, Accuracy 89.880%\n",
      "Epoch 39, Batch 415, LR 0.000000 Loss 4.419404, Accuracy 89.885%\n",
      "Epoch 39, Batch 416, LR 0.000000 Loss 4.418706, Accuracy 89.885%\n",
      "Epoch 39, Batch 417, LR 0.000000 Loss 4.419384, Accuracy 89.887%\n",
      "Epoch 39, Batch 418, LR 0.000000 Loss 4.419958, Accuracy 89.877%\n",
      "Epoch 39, Batch 419, LR 0.000000 Loss 4.420088, Accuracy 89.885%\n",
      "Epoch 39, Batch 420, LR 0.000000 Loss 4.418935, Accuracy 89.883%\n",
      "Epoch 39, Batch 421, LR 0.000000 Loss 4.418987, Accuracy 89.886%\n",
      "Epoch 39, Batch 422, LR 0.000000 Loss 4.419888, Accuracy 89.881%\n",
      "Epoch 39, Batch 423, LR 0.000000 Loss 4.419393, Accuracy 89.884%\n",
      "Epoch 39, Batch 424, LR 0.000000 Loss 4.418560, Accuracy 89.884%\n",
      "Epoch 39, Batch 425, LR 0.000000 Loss 4.418789, Accuracy 89.879%\n",
      "Epoch 39, Batch 426, LR 0.000000 Loss 4.419514, Accuracy 89.869%\n",
      "Epoch 39, Batch 427, LR 0.000000 Loss 4.419570, Accuracy 89.871%\n",
      "Epoch 39, Batch 428, LR 0.000000 Loss 4.419973, Accuracy 89.866%\n",
      "Epoch 39, Batch 429, LR 0.000000 Loss 4.420881, Accuracy 89.858%\n",
      "Epoch 39, Batch 430, LR 0.000000 Loss 4.420339, Accuracy 89.860%\n",
      "Epoch 39, Batch 431, LR 0.000000 Loss 4.419207, Accuracy 89.860%\n",
      "Epoch 39, Batch 432, LR 0.000000 Loss 4.420086, Accuracy 89.855%\n",
      "Epoch 39, Batch 433, LR 0.000000 Loss 4.420613, Accuracy 89.849%\n",
      "Epoch 39, Batch 434, LR 0.000000 Loss 4.420404, Accuracy 89.855%\n",
      "Epoch 39, Batch 435, LR 0.000000 Loss 4.420641, Accuracy 89.849%\n",
      "Epoch 39, Batch 436, LR 0.000000 Loss 4.420234, Accuracy 89.858%\n",
      "Epoch 39, Batch 437, LR 0.000000 Loss 4.419730, Accuracy 89.862%\n",
      "Epoch 39, Batch 438, LR 0.000000 Loss 4.419466, Accuracy 89.865%\n",
      "Epoch 39, Batch 439, LR 0.000000 Loss 4.418908, Accuracy 89.870%\n",
      "Epoch 39, Batch 440, LR 0.000000 Loss 4.419030, Accuracy 89.869%\n",
      "Epoch 39, Batch 441, LR 0.000000 Loss 4.419510, Accuracy 89.870%\n",
      "Epoch 39, Batch 442, LR 0.000000 Loss 4.421063, Accuracy 89.861%\n",
      "Epoch 39, Batch 443, LR 0.000000 Loss 4.420544, Accuracy 89.868%\n",
      "Epoch 39, Batch 444, LR 0.000000 Loss 4.421675, Accuracy 89.849%\n",
      "Epoch 39, Batch 445, LR 0.000000 Loss 4.421224, Accuracy 89.854%\n",
      "Epoch 39, Batch 446, LR 0.000000 Loss 4.420639, Accuracy 89.858%\n",
      "Epoch 39, Batch 447, LR 0.000000 Loss 4.420291, Accuracy 89.849%\n",
      "Epoch 39, Batch 448, LR 0.000000 Loss 4.419770, Accuracy 89.849%\n",
      "Epoch 39, Batch 449, LR 0.000000 Loss 4.419500, Accuracy 89.849%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 450, LR 0.000000 Loss 4.419842, Accuracy 89.847%\n",
      "Epoch 39, Batch 451, LR 0.000000 Loss 4.419098, Accuracy 89.852%\n",
      "Epoch 39, Batch 452, LR 0.000000 Loss 4.417892, Accuracy 89.858%\n",
      "Epoch 39, Batch 453, LR 0.000000 Loss 4.418728, Accuracy 89.845%\n",
      "Epoch 39, Batch 454, LR 0.000000 Loss 4.419250, Accuracy 89.842%\n",
      "Epoch 39, Batch 455, LR 0.000000 Loss 4.418429, Accuracy 89.849%\n",
      "Epoch 39, Batch 456, LR 0.000000 Loss 4.417258, Accuracy 89.854%\n",
      "Epoch 39, Batch 457, LR 0.000000 Loss 4.417798, Accuracy 89.847%\n",
      "Epoch 39, Batch 458, LR 0.000000 Loss 4.418270, Accuracy 89.849%\n",
      "Epoch 39, Batch 459, LR 0.000000 Loss 4.417710, Accuracy 89.852%\n",
      "Epoch 39, Batch 460, LR 0.000000 Loss 4.418782, Accuracy 89.839%\n",
      "Epoch 39, Batch 461, LR 0.000000 Loss 4.418801, Accuracy 89.834%\n",
      "Epoch 39, Batch 462, LR 0.000000 Loss 4.418835, Accuracy 89.829%\n",
      "Epoch 39, Batch 463, LR 0.000000 Loss 4.420508, Accuracy 89.818%\n",
      "Epoch 39, Batch 464, LR 0.000000 Loss 4.420483, Accuracy 89.817%\n",
      "Epoch 39, Batch 465, LR 0.000000 Loss 4.420011, Accuracy 89.814%\n",
      "Epoch 39, Batch 466, LR 0.000000 Loss 4.419339, Accuracy 89.822%\n",
      "Epoch 39, Batch 467, LR 0.000000 Loss 4.420708, Accuracy 89.814%\n",
      "Epoch 39, Batch 468, LR 0.000000 Loss 4.420693, Accuracy 89.812%\n",
      "Epoch 39, Batch 469, LR 0.000000 Loss 4.421363, Accuracy 89.812%\n",
      "Epoch 39, Batch 470, LR 0.000000 Loss 4.420083, Accuracy 89.819%\n",
      "Epoch 39, Batch 471, LR 0.000000 Loss 4.419717, Accuracy 89.819%\n",
      "Epoch 39, Batch 472, LR 0.000000 Loss 4.420385, Accuracy 89.819%\n",
      "Epoch 39, Batch 473, LR 0.000000 Loss 4.422350, Accuracy 89.807%\n",
      "Epoch 39, Batch 474, LR 0.000000 Loss 4.421820, Accuracy 89.806%\n",
      "Epoch 39, Batch 475, LR 0.000000 Loss 4.421642, Accuracy 89.811%\n",
      "Epoch 39, Batch 476, LR 0.000000 Loss 4.422448, Accuracy 89.804%\n",
      "Epoch 39, Batch 477, LR 0.000000 Loss 4.422891, Accuracy 89.808%\n",
      "Epoch 39, Batch 478, LR 0.000000 Loss 4.423728, Accuracy 89.805%\n",
      "Epoch 39, Batch 479, LR 0.000000 Loss 4.424865, Accuracy 89.800%\n",
      "Epoch 39, Batch 480, LR 0.000000 Loss 4.424582, Accuracy 89.806%\n",
      "Epoch 39, Batch 481, LR 0.000000 Loss 4.423876, Accuracy 89.811%\n",
      "Epoch 39, Batch 482, LR 0.000000 Loss 4.423282, Accuracy 89.810%\n",
      "Epoch 39, Batch 483, LR 0.000000 Loss 4.424534, Accuracy 89.811%\n",
      "Epoch 39, Batch 484, LR 0.000000 Loss 4.424534, Accuracy 89.811%\n",
      "Epoch 39, Batch 485, LR 0.000000 Loss 4.425011, Accuracy 89.812%\n",
      "Epoch 39, Batch 486, LR 0.000000 Loss 4.424608, Accuracy 89.813%\n",
      "Epoch 39, Batch 487, LR 0.000000 Loss 4.424839, Accuracy 89.812%\n",
      "Epoch 39, Batch 488, LR 0.000000 Loss 4.423529, Accuracy 89.813%\n",
      "Epoch 39, Batch 489, LR 0.000000 Loss 4.423529, Accuracy 89.818%\n",
      "Epoch 39, Batch 490, LR 0.000000 Loss 4.421013, Accuracy 89.826%\n",
      "Epoch 39, Batch 491, LR 0.000000 Loss 4.420852, Accuracy 89.818%\n",
      "Epoch 39, Batch 492, LR 0.000000 Loss 4.420941, Accuracy 89.818%\n",
      "Epoch 39, Batch 493, LR 0.000000 Loss 4.420987, Accuracy 89.809%\n",
      "Epoch 39, Batch 494, LR 0.000000 Loss 4.421018, Accuracy 89.806%\n",
      "Epoch 39, Batch 495, LR 0.000000 Loss 4.421900, Accuracy 89.800%\n",
      "Epoch 39, Batch 496, LR 0.000000 Loss 4.423255, Accuracy 89.793%\n",
      "Epoch 39, Batch 497, LR 0.000000 Loss 4.422523, Accuracy 89.793%\n",
      "Epoch 39, Batch 498, LR 0.000000 Loss 4.422864, Accuracy 89.786%\n",
      "Epoch 39, Batch 499, LR 0.000000 Loss 4.421300, Accuracy 89.794%\n",
      "Epoch 39, Batch 500, LR 0.000000 Loss 4.421043, Accuracy 89.800%\n",
      "Epoch 39, Batch 501, LR 0.000000 Loss 4.421630, Accuracy 89.799%\n",
      "Epoch 39, Batch 502, LR 0.000000 Loss 4.421410, Accuracy 89.799%\n",
      "Epoch 39, Batch 503, LR 0.000000 Loss 4.421081, Accuracy 89.799%\n",
      "Epoch 39, Batch 504, LR 0.000000 Loss 4.420640, Accuracy 89.803%\n",
      "Epoch 39, Batch 505, LR 0.000000 Loss 4.421095, Accuracy 89.802%\n",
      "Epoch 39, Batch 506, LR 0.000000 Loss 4.420561, Accuracy 89.810%\n",
      "Epoch 39, Batch 507, LR 0.000000 Loss 4.420486, Accuracy 89.808%\n",
      "Epoch 39, Batch 508, LR 0.000000 Loss 4.421605, Accuracy 89.801%\n",
      "Epoch 39, Batch 509, LR 0.000000 Loss 4.422363, Accuracy 89.796%\n",
      "Epoch 39, Batch 510, LR 0.000000 Loss 4.422475, Accuracy 89.798%\n",
      "Epoch 39, Batch 511, LR 0.000000 Loss 4.422244, Accuracy 89.799%\n",
      "Epoch 39, Batch 512, LR 0.000000 Loss 4.420184, Accuracy 89.806%\n",
      "Epoch 39, Batch 513, LR 0.000000 Loss 4.420131, Accuracy 89.806%\n",
      "Epoch 39, Batch 514, LR 0.000000 Loss 4.418800, Accuracy 89.810%\n",
      "Epoch 39, Batch 515, LR 0.000000 Loss 4.418149, Accuracy 89.809%\n",
      "Epoch 39, Batch 516, LR 0.000000 Loss 4.418460, Accuracy 89.815%\n",
      "Epoch 39, Batch 517, LR 0.000000 Loss 4.419053, Accuracy 89.815%\n",
      "Epoch 39, Batch 518, LR 0.000000 Loss 4.418717, Accuracy 89.817%\n",
      "Epoch 39, Batch 519, LR 0.000000 Loss 4.418550, Accuracy 89.821%\n",
      "Epoch 39, Batch 520, LR 0.000000 Loss 4.418016, Accuracy 89.821%\n",
      "Epoch 39, Batch 521, LR 0.000000 Loss 4.417565, Accuracy 89.824%\n",
      "Epoch 39, Batch 522, LR 0.000000 Loss 4.418674, Accuracy 89.812%\n",
      "Epoch 39, Batch 523, LR 0.000000 Loss 4.418212, Accuracy 89.808%\n",
      "Epoch 39, Batch 524, LR 0.000000 Loss 4.418431, Accuracy 89.805%\n",
      "Epoch 39, Batch 525, LR 0.000000 Loss 4.417682, Accuracy 89.810%\n",
      "Epoch 39, Batch 526, LR 0.000000 Loss 4.417457, Accuracy 89.814%\n",
      "Epoch 39, Batch 527, LR 0.000000 Loss 4.417770, Accuracy 89.807%\n",
      "Epoch 39, Batch 528, LR 0.000000 Loss 4.417852, Accuracy 89.810%\n",
      "Epoch 39, Batch 529, LR 0.000000 Loss 4.418359, Accuracy 89.807%\n",
      "Epoch 39, Batch 530, LR 0.000000 Loss 4.419637, Accuracy 89.804%\n",
      "Epoch 39, Batch 531, LR 0.000000 Loss 4.418927, Accuracy 89.811%\n",
      "Epoch 39, Batch 532, LR 0.000000 Loss 4.417244, Accuracy 89.817%\n",
      "Epoch 39, Batch 533, LR 0.000000 Loss 4.419783, Accuracy 89.803%\n",
      "Epoch 39, Batch 534, LR 0.000000 Loss 4.420588, Accuracy 89.803%\n",
      "Epoch 39, Batch 535, LR 0.000000 Loss 4.420103, Accuracy 89.806%\n",
      "Epoch 39, Batch 536, LR 0.000000 Loss 4.421019, Accuracy 89.800%\n",
      "Epoch 39, Batch 537, LR 0.000000 Loss 4.420141, Accuracy 89.806%\n",
      "Epoch 39, Batch 538, LR 0.000000 Loss 4.419429, Accuracy 89.807%\n",
      "Epoch 39, Batch 539, LR 0.000000 Loss 4.418400, Accuracy 89.813%\n",
      "Epoch 39, Batch 540, LR 0.000000 Loss 4.418739, Accuracy 89.808%\n",
      "Epoch 39, Batch 541, LR 0.000000 Loss 4.417832, Accuracy 89.809%\n",
      "Epoch 39, Batch 542, LR 0.000000 Loss 4.418631, Accuracy 89.811%\n",
      "Epoch 39, Batch 543, LR 0.000000 Loss 4.417995, Accuracy 89.812%\n",
      "Epoch 39, Batch 544, LR 0.000000 Loss 4.417560, Accuracy 89.812%\n",
      "Epoch 39, Batch 545, LR 0.000000 Loss 4.417863, Accuracy 89.815%\n",
      "Epoch 39, Batch 546, LR 0.000000 Loss 4.418262, Accuracy 89.811%\n",
      "Epoch 39, Batch 547, LR 0.000000 Loss 4.417819, Accuracy 89.814%\n",
      "Epoch 39, Batch 548, LR 0.000000 Loss 4.417538, Accuracy 89.818%\n",
      "Epoch 39, Batch 549, LR 0.000000 Loss 4.417631, Accuracy 89.814%\n",
      "Epoch 39, Batch 550, LR 0.000000 Loss 4.417119, Accuracy 89.814%\n",
      "Epoch 39, Batch 551, LR 0.000000 Loss 4.415830, Accuracy 89.820%\n",
      "Epoch 39, Batch 552, LR 0.000000 Loss 4.416111, Accuracy 89.814%\n",
      "Epoch 39, Batch 553, LR 0.000000 Loss 4.415821, Accuracy 89.808%\n",
      "Epoch 39, Batch 554, LR 0.000000 Loss 4.415733, Accuracy 89.806%\n",
      "Epoch 39, Batch 555, LR 0.000000 Loss 4.415077, Accuracy 89.804%\n",
      "Epoch 39, Batch 556, LR 0.000000 Loss 4.414011, Accuracy 89.806%\n",
      "Epoch 39, Batch 557, LR 0.000000 Loss 4.413897, Accuracy 89.806%\n",
      "Epoch 39, Batch 558, LR 0.000000 Loss 4.412710, Accuracy 89.812%\n",
      "Epoch 39, Batch 559, LR 0.000000 Loss 4.412799, Accuracy 89.806%\n",
      "Epoch 39, Batch 560, LR 0.000000 Loss 4.413966, Accuracy 89.802%\n",
      "Epoch 39, Batch 561, LR 0.000000 Loss 4.413400, Accuracy 89.803%\n",
      "Epoch 39, Batch 562, LR 0.000000 Loss 4.411941, Accuracy 89.809%\n",
      "Epoch 39, Batch 563, LR 0.000000 Loss 4.412165, Accuracy 89.802%\n",
      "Epoch 39, Batch 564, LR 0.000000 Loss 4.412291, Accuracy 89.805%\n",
      "Epoch 39, Batch 565, LR 0.000000 Loss 4.413647, Accuracy 89.794%\n",
      "Epoch 39, Batch 566, LR 0.000000 Loss 4.412485, Accuracy 89.802%\n",
      "Epoch 39, Batch 567, LR 0.000000 Loss 4.412730, Accuracy 89.797%\n",
      "Epoch 39, Batch 568, LR 0.000000 Loss 4.412613, Accuracy 89.794%\n",
      "Epoch 39, Batch 569, LR 0.000000 Loss 4.412793, Accuracy 89.793%\n",
      "Epoch 39, Batch 570, LR 0.000000 Loss 4.414225, Accuracy 89.783%\n",
      "Epoch 39, Batch 571, LR 0.000000 Loss 4.413989, Accuracy 89.784%\n",
      "Epoch 39, Batch 572, LR 0.000000 Loss 4.414681, Accuracy 89.780%\n",
      "Epoch 39, Batch 573, LR 0.000000 Loss 4.415618, Accuracy 89.777%\n",
      "Epoch 39, Batch 574, LR 0.000000 Loss 4.416119, Accuracy 89.773%\n",
      "Epoch 39, Batch 575, LR 0.000000 Loss 4.415031, Accuracy 89.783%\n",
      "Epoch 39, Batch 576, LR 0.000000 Loss 4.416032, Accuracy 89.776%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 577, LR 0.000000 Loss 4.416210, Accuracy 89.775%\n",
      "Epoch 39, Batch 578, LR 0.000000 Loss 4.415891, Accuracy 89.775%\n",
      "Epoch 39, Batch 579, LR 0.000000 Loss 4.416327, Accuracy 89.767%\n",
      "Epoch 39, Batch 580, LR 0.000000 Loss 4.417270, Accuracy 89.760%\n",
      "Epoch 39, Batch 581, LR 0.000000 Loss 4.417053, Accuracy 89.763%\n",
      "Epoch 39, Batch 582, LR 0.000000 Loss 4.417246, Accuracy 89.763%\n",
      "Epoch 39, Batch 583, LR 0.000000 Loss 4.417509, Accuracy 89.763%\n",
      "Epoch 39, Batch 584, LR 0.000000 Loss 4.418205, Accuracy 89.769%\n",
      "Epoch 39, Batch 585, LR 0.000000 Loss 4.417903, Accuracy 89.769%\n",
      "Epoch 39, Batch 586, LR 0.000000 Loss 4.417781, Accuracy 89.769%\n",
      "Epoch 39, Batch 587, LR 0.000000 Loss 4.418081, Accuracy 89.768%\n",
      "Epoch 39, Batch 588, LR 0.000000 Loss 4.417426, Accuracy 89.771%\n",
      "Epoch 39, Batch 589, LR 0.000000 Loss 4.417340, Accuracy 89.775%\n",
      "Epoch 39, Batch 590, LR 0.000000 Loss 4.416454, Accuracy 89.779%\n",
      "Epoch 39, Batch 591, LR 0.000000 Loss 4.415673, Accuracy 89.780%\n",
      "Epoch 39, Batch 592, LR 0.000000 Loss 4.414975, Accuracy 89.778%\n",
      "Epoch 39, Batch 593, LR 0.000000 Loss 4.415670, Accuracy 89.781%\n",
      "Epoch 39, Batch 594, LR 0.000000 Loss 4.415053, Accuracy 89.787%\n",
      "Epoch 39, Batch 595, LR 0.000000 Loss 4.414358, Accuracy 89.789%\n",
      "Epoch 39, Batch 596, LR 0.000000 Loss 4.413597, Accuracy 89.795%\n",
      "Epoch 39, Batch 597, LR 0.000000 Loss 4.414315, Accuracy 89.795%\n",
      "Epoch 39, Batch 598, LR 0.000000 Loss 4.412697, Accuracy 89.801%\n",
      "Epoch 39, Batch 599, LR 0.000000 Loss 4.410935, Accuracy 89.809%\n",
      "Epoch 39, Batch 600, LR 0.000000 Loss 4.410529, Accuracy 89.809%\n",
      "Epoch 39, Batch 601, LR 0.000000 Loss 4.409786, Accuracy 89.811%\n",
      "Epoch 39, Batch 602, LR 0.000000 Loss 4.409900, Accuracy 89.809%\n",
      "Epoch 39, Batch 603, LR 0.000000 Loss 4.409154, Accuracy 89.811%\n",
      "Epoch 39, Batch 604, LR 0.000000 Loss 4.408881, Accuracy 89.813%\n",
      "Epoch 39, Batch 605, LR 0.000000 Loss 4.410079, Accuracy 89.802%\n",
      "Epoch 39, Batch 606, LR 0.000000 Loss 4.409514, Accuracy 89.802%\n",
      "Epoch 39, Batch 607, LR 0.000000 Loss 4.409224, Accuracy 89.797%\n",
      "Epoch 39, Batch 608, LR 0.000000 Loss 4.408647, Accuracy 89.797%\n",
      "Epoch 39, Batch 609, LR 0.000000 Loss 4.408535, Accuracy 89.798%\n",
      "Epoch 39, Batch 610, LR 0.000000 Loss 4.409048, Accuracy 89.801%\n",
      "Epoch 39, Batch 611, LR 0.000000 Loss 4.409495, Accuracy 89.803%\n",
      "Epoch 39, Batch 612, LR 0.000000 Loss 4.409124, Accuracy 89.803%\n",
      "Epoch 39, Batch 613, LR 0.000000 Loss 4.410808, Accuracy 89.797%\n",
      "Epoch 39, Batch 614, LR 0.000000 Loss 4.410951, Accuracy 89.795%\n",
      "Epoch 39, Batch 615, LR 0.000000 Loss 4.410968, Accuracy 89.797%\n",
      "Epoch 39, Batch 616, LR 0.000000 Loss 4.409659, Accuracy 89.804%\n",
      "Epoch 39, Batch 617, LR 0.000000 Loss 4.410227, Accuracy 89.804%\n",
      "Epoch 39, Batch 618, LR 0.000000 Loss 4.409935, Accuracy 89.812%\n",
      "Epoch 39, Batch 619, LR 0.000000 Loss 4.410804, Accuracy 89.806%\n",
      "Epoch 39, Batch 620, LR 0.000000 Loss 4.412447, Accuracy 89.798%\n",
      "Epoch 39, Batch 621, LR 0.000000 Loss 4.413160, Accuracy 89.788%\n",
      "Epoch 39, Batch 622, LR 0.000000 Loss 4.413136, Accuracy 89.791%\n",
      "Epoch 39, Batch 623, LR 0.000000 Loss 4.412369, Accuracy 89.789%\n",
      "Epoch 39, Batch 624, LR 0.000000 Loss 4.412931, Accuracy 89.782%\n",
      "Epoch 39, Batch 625, LR 0.000000 Loss 4.412007, Accuracy 89.784%\n",
      "Epoch 39, Batch 626, LR 0.000000 Loss 4.412500, Accuracy 89.779%\n",
      "Epoch 39, Batch 627, LR 0.000000 Loss 4.412676, Accuracy 89.785%\n",
      "Epoch 39, Batch 628, LR 0.000000 Loss 4.412346, Accuracy 89.789%\n",
      "Epoch 39, Batch 629, LR 0.000000 Loss 4.411235, Accuracy 89.792%\n",
      "Epoch 39, Batch 630, LR 0.000000 Loss 4.410671, Accuracy 89.793%\n",
      "Epoch 39, Batch 631, LR 0.000000 Loss 4.410060, Accuracy 89.793%\n",
      "Epoch 39, Batch 632, LR 0.000000 Loss 4.410235, Accuracy 89.796%\n",
      "Epoch 39, Batch 633, LR 0.000000 Loss 4.411185, Accuracy 89.792%\n",
      "Epoch 39, Batch 634, LR 0.000000 Loss 4.411262, Accuracy 89.788%\n",
      "Epoch 39, Batch 635, LR 0.000000 Loss 4.412221, Accuracy 89.787%\n",
      "Epoch 39, Batch 636, LR 0.000000 Loss 4.411093, Accuracy 89.795%\n",
      "Epoch 39, Batch 637, LR 0.000000 Loss 4.411327, Accuracy 89.790%\n",
      "Epoch 39, Batch 638, LR 0.000000 Loss 4.409685, Accuracy 89.795%\n",
      "Epoch 39, Batch 639, LR 0.000000 Loss 4.408971, Accuracy 89.802%\n",
      "Epoch 39, Batch 640, LR 0.000000 Loss 4.407598, Accuracy 89.811%\n",
      "Epoch 39, Batch 641, LR 0.000000 Loss 4.406624, Accuracy 89.811%\n",
      "Epoch 39, Batch 642, LR 0.000000 Loss 4.406542, Accuracy 89.816%\n",
      "Epoch 39, Batch 643, LR 0.000000 Loss 4.406136, Accuracy 89.817%\n",
      "Epoch 39, Batch 644, LR 0.000000 Loss 4.406000, Accuracy 89.816%\n",
      "Epoch 39, Batch 645, LR 0.000000 Loss 4.406283, Accuracy 89.812%\n",
      "Epoch 39, Batch 646, LR 0.000000 Loss 4.407832, Accuracy 89.804%\n",
      "Epoch 39, Batch 647, LR 0.000000 Loss 4.407048, Accuracy 89.806%\n",
      "Epoch 39, Batch 648, LR 0.000000 Loss 4.405797, Accuracy 89.812%\n",
      "Epoch 39, Batch 649, LR 0.000000 Loss 4.405499, Accuracy 89.815%\n",
      "Epoch 39, Batch 650, LR 0.000000 Loss 4.405723, Accuracy 89.810%\n",
      "Epoch 39, Batch 651, LR 0.000000 Loss 4.406551, Accuracy 89.809%\n",
      "Epoch 39, Batch 652, LR 0.000000 Loss 4.406389, Accuracy 89.809%\n",
      "Epoch 39, Batch 653, LR 0.000000 Loss 4.406278, Accuracy 89.811%\n",
      "Epoch 39, Batch 654, LR 0.000000 Loss 4.407145, Accuracy 89.811%\n",
      "Epoch 39, Batch 655, LR 0.000000 Loss 4.406750, Accuracy 89.813%\n",
      "Epoch 39, Batch 656, LR 0.000000 Loss 4.406797, Accuracy 89.814%\n",
      "Epoch 39, Batch 657, LR 0.000000 Loss 4.407905, Accuracy 89.805%\n",
      "Epoch 39, Batch 658, LR 0.000000 Loss 4.407668, Accuracy 89.805%\n",
      "Epoch 39, Batch 659, LR 0.000000 Loss 4.408228, Accuracy 89.799%\n",
      "Epoch 39, Batch 660, LR 0.000000 Loss 4.407811, Accuracy 89.805%\n",
      "Epoch 39, Batch 661, LR 0.000000 Loss 4.407086, Accuracy 89.809%\n",
      "Epoch 39, Batch 662, LR 0.000000 Loss 4.408191, Accuracy 89.808%\n",
      "Epoch 39, Batch 663, LR 0.000000 Loss 4.409565, Accuracy 89.803%\n",
      "Epoch 39, Batch 664, LR 0.000000 Loss 4.409397, Accuracy 89.807%\n",
      "Epoch 39, Batch 665, LR 0.000000 Loss 4.409415, Accuracy 89.806%\n",
      "Epoch 39, Batch 666, LR 0.000000 Loss 4.408372, Accuracy 89.811%\n",
      "Epoch 39, Batch 667, LR 0.000000 Loss 4.409365, Accuracy 89.797%\n",
      "Epoch 39, Batch 668, LR 0.000000 Loss 4.409068, Accuracy 89.799%\n",
      "Epoch 39, Batch 669, LR 0.000000 Loss 4.408523, Accuracy 89.796%\n",
      "Epoch 39, Batch 670, LR 0.000000 Loss 4.408664, Accuracy 89.791%\n",
      "Epoch 39, Batch 671, LR 0.000000 Loss 4.409458, Accuracy 89.789%\n",
      "Epoch 39, Batch 672, LR 0.000000 Loss 4.409850, Accuracy 89.783%\n",
      "Epoch 39, Batch 673, LR 0.000000 Loss 4.409780, Accuracy 89.786%\n",
      "Epoch 39, Batch 674, LR 0.000000 Loss 4.409784, Accuracy 89.781%\n",
      "Epoch 39, Batch 675, LR 0.000000 Loss 4.410781, Accuracy 89.777%\n",
      "Epoch 39, Batch 676, LR 0.000000 Loss 4.409621, Accuracy 89.782%\n",
      "Epoch 39, Batch 677, LR 0.000000 Loss 4.409441, Accuracy 89.781%\n",
      "Epoch 39, Batch 678, LR 0.000000 Loss 4.410999, Accuracy 89.773%\n",
      "Epoch 39, Batch 679, LR 0.000000 Loss 4.411434, Accuracy 89.774%\n",
      "Epoch 39, Batch 680, LR 0.000000 Loss 4.411771, Accuracy 89.771%\n",
      "Epoch 39, Batch 681, LR 0.000000 Loss 4.410445, Accuracy 89.775%\n",
      "Epoch 39, Batch 682, LR 0.000000 Loss 4.410409, Accuracy 89.775%\n",
      "Epoch 39, Batch 683, LR 0.000000 Loss 4.409453, Accuracy 89.777%\n",
      "Epoch 39, Batch 684, LR 0.000000 Loss 4.409550, Accuracy 89.784%\n",
      "Epoch 39, Batch 685, LR 0.000000 Loss 4.408891, Accuracy 89.787%\n",
      "Epoch 39, Batch 686, LR 0.000000 Loss 4.409143, Accuracy 89.785%\n",
      "Epoch 39, Batch 687, LR 0.000000 Loss 4.409255, Accuracy 89.783%\n",
      "Epoch 39, Batch 688, LR 0.000000 Loss 4.408615, Accuracy 89.788%\n",
      "Epoch 39, Batch 689, LR 0.000000 Loss 4.409747, Accuracy 89.789%\n",
      "Epoch 39, Batch 690, LR 0.000000 Loss 4.408029, Accuracy 89.791%\n",
      "Epoch 39, Batch 691, LR 0.000000 Loss 4.407615, Accuracy 89.786%\n",
      "Epoch 39, Batch 692, LR 0.000000 Loss 4.408364, Accuracy 89.783%\n",
      "Epoch 39, Batch 693, LR 0.000000 Loss 4.408613, Accuracy 89.783%\n",
      "Epoch 39, Batch 694, LR 0.000000 Loss 4.408803, Accuracy 89.781%\n",
      "Epoch 39, Batch 695, LR 0.000000 Loss 4.408604, Accuracy 89.785%\n",
      "Epoch 39, Batch 696, LR 0.000000 Loss 4.409041, Accuracy 89.782%\n",
      "Epoch 39, Batch 697, LR 0.000000 Loss 4.408859, Accuracy 89.780%\n",
      "Epoch 39, Batch 698, LR 0.000000 Loss 4.408644, Accuracy 89.781%\n",
      "Epoch 39, Batch 699, LR 0.000000 Loss 4.408273, Accuracy 89.790%\n",
      "Epoch 39, Batch 700, LR 0.000000 Loss 4.410203, Accuracy 89.782%\n",
      "Epoch 39, Batch 701, LR 0.000000 Loss 4.409500, Accuracy 89.786%\n",
      "Epoch 39, Batch 702, LR 0.000000 Loss 4.409903, Accuracy 89.781%\n",
      "Epoch 39, Batch 703, LR 0.000000 Loss 4.410716, Accuracy 89.775%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 704, LR 0.000000 Loss 4.410673, Accuracy 89.777%\n",
      "Epoch 39, Batch 705, LR 0.000000 Loss 4.410902, Accuracy 89.774%\n",
      "Epoch 39, Batch 706, LR 0.000000 Loss 4.411125, Accuracy 89.773%\n",
      "Epoch 39, Batch 707, LR 0.000000 Loss 4.410370, Accuracy 89.775%\n",
      "Epoch 39, Batch 708, LR 0.000000 Loss 4.410235, Accuracy 89.772%\n",
      "Epoch 39, Batch 709, LR 0.000000 Loss 4.410618, Accuracy 89.772%\n",
      "Epoch 39, Batch 710, LR 0.000000 Loss 4.410115, Accuracy 89.773%\n",
      "Epoch 39, Batch 711, LR 0.000000 Loss 4.410553, Accuracy 89.770%\n",
      "Epoch 39, Batch 712, LR 0.000000 Loss 4.410520, Accuracy 89.775%\n",
      "Epoch 39, Batch 713, LR 0.000000 Loss 4.410289, Accuracy 89.775%\n",
      "Epoch 39, Batch 714, LR 0.000000 Loss 4.410108, Accuracy 89.774%\n",
      "Epoch 39, Batch 715, LR 0.000000 Loss 4.409809, Accuracy 89.774%\n",
      "Epoch 39, Batch 716, LR 0.000000 Loss 4.410156, Accuracy 89.773%\n",
      "Epoch 39, Batch 717, LR 0.000000 Loss 4.410488, Accuracy 89.772%\n",
      "Epoch 39, Batch 718, LR 0.000000 Loss 4.410993, Accuracy 89.773%\n",
      "Epoch 39, Batch 719, LR 0.000000 Loss 4.410793, Accuracy 89.772%\n",
      "Epoch 39, Batch 720, LR 0.000000 Loss 4.410259, Accuracy 89.778%\n",
      "Epoch 39, Batch 721, LR 0.000000 Loss 4.411268, Accuracy 89.772%\n",
      "Epoch 39, Batch 722, LR 0.000000 Loss 4.411894, Accuracy 89.771%\n",
      "Epoch 39, Batch 723, LR 0.000000 Loss 4.412329, Accuracy 89.768%\n",
      "Epoch 39, Batch 724, LR 0.000000 Loss 4.412500, Accuracy 89.767%\n",
      "Epoch 39, Batch 725, LR 0.000000 Loss 4.413337, Accuracy 89.764%\n",
      "Epoch 39, Batch 726, LR 0.000000 Loss 4.413242, Accuracy 89.765%\n",
      "Epoch 39, Batch 727, LR 0.000000 Loss 4.412353, Accuracy 89.763%\n",
      "Epoch 39, Batch 728, LR 0.000000 Loss 4.412923, Accuracy 89.759%\n",
      "Epoch 39, Batch 729, LR 0.000000 Loss 4.412284, Accuracy 89.763%\n",
      "Epoch 39, Batch 730, LR 0.000000 Loss 4.411764, Accuracy 89.763%\n",
      "Epoch 39, Batch 731, LR 0.000000 Loss 4.411660, Accuracy 89.771%\n",
      "Epoch 39, Batch 732, LR 0.000000 Loss 4.412827, Accuracy 89.767%\n",
      "Epoch 39, Batch 733, LR 0.000000 Loss 4.411662, Accuracy 89.770%\n",
      "Epoch 39, Batch 734, LR 0.000000 Loss 4.410950, Accuracy 89.772%\n",
      "Epoch 39, Batch 735, LR 0.000000 Loss 4.411028, Accuracy 89.774%\n",
      "Epoch 39, Batch 736, LR 0.000000 Loss 4.411134, Accuracy 89.771%\n",
      "Epoch 39, Batch 737, LR 0.000000 Loss 4.410902, Accuracy 89.775%\n",
      "Epoch 39, Batch 738, LR 0.000000 Loss 4.411390, Accuracy 89.774%\n",
      "Epoch 39, Batch 739, LR 0.000000 Loss 4.411137, Accuracy 89.778%\n",
      "Epoch 39, Batch 740, LR 0.000000 Loss 4.411687, Accuracy 89.780%\n",
      "Epoch 39, Batch 741, LR 0.000000 Loss 4.410576, Accuracy 89.785%\n",
      "Epoch 39, Batch 742, LR 0.000000 Loss 4.411253, Accuracy 89.781%\n",
      "Epoch 39, Batch 743, LR 0.000000 Loss 4.410659, Accuracy 89.782%\n",
      "Epoch 39, Batch 744, LR 0.000000 Loss 4.410777, Accuracy 89.780%\n",
      "Epoch 39, Batch 745, LR 0.000000 Loss 4.410475, Accuracy 89.778%\n",
      "Epoch 39, Batch 746, LR 0.000000 Loss 4.409531, Accuracy 89.780%\n",
      "Epoch 39, Batch 747, LR 0.000000 Loss 4.409067, Accuracy 89.783%\n",
      "Epoch 39, Batch 748, LR 0.000000 Loss 4.409549, Accuracy 89.780%\n",
      "Epoch 39, Batch 749, LR 0.000000 Loss 4.409236, Accuracy 89.779%\n",
      "Epoch 39, Batch 750, LR 0.000000 Loss 4.409194, Accuracy 89.778%\n",
      "Epoch 39, Batch 751, LR 0.000000 Loss 4.409865, Accuracy 89.774%\n",
      "Epoch 39, Batch 752, LR 0.000000 Loss 4.410183, Accuracy 89.771%\n",
      "Epoch 39, Batch 753, LR 0.000000 Loss 4.410959, Accuracy 89.765%\n",
      "Epoch 39, Batch 754, LR 0.000000 Loss 4.409958, Accuracy 89.767%\n",
      "Epoch 39, Batch 755, LR 0.000000 Loss 4.410059, Accuracy 89.763%\n",
      "Epoch 39, Batch 756, LR 0.000000 Loss 4.409939, Accuracy 89.762%\n",
      "Epoch 39, Batch 757, LR 0.000000 Loss 4.410406, Accuracy 89.762%\n",
      "Epoch 39, Batch 758, LR 0.000000 Loss 4.410367, Accuracy 89.767%\n",
      "Epoch 39, Batch 759, LR 0.000000 Loss 4.411070, Accuracy 89.766%\n",
      "Epoch 39, Batch 760, LR 0.000000 Loss 4.411302, Accuracy 89.767%\n",
      "Epoch 39, Batch 761, LR 0.000000 Loss 4.410839, Accuracy 89.770%\n",
      "Epoch 39, Batch 762, LR 0.000000 Loss 4.411253, Accuracy 89.766%\n",
      "Epoch 39, Batch 763, LR 0.000000 Loss 4.409683, Accuracy 89.773%\n",
      "Epoch 39, Batch 764, LR 0.000000 Loss 4.409413, Accuracy 89.771%\n",
      "Epoch 39, Batch 765, LR 0.000000 Loss 4.409813, Accuracy 89.768%\n",
      "Epoch 39, Batch 766, LR 0.000000 Loss 4.410430, Accuracy 89.768%\n",
      "Epoch 39, Batch 767, LR 0.000000 Loss 4.410166, Accuracy 89.770%\n",
      "Epoch 39, Batch 768, LR 0.000000 Loss 4.408594, Accuracy 89.777%\n",
      "Epoch 39, Batch 769, LR 0.000000 Loss 4.409920, Accuracy 89.771%\n",
      "Epoch 39, Batch 770, LR 0.000000 Loss 4.409224, Accuracy 89.776%\n",
      "Epoch 39, Batch 771, LR 0.000000 Loss 4.410305, Accuracy 89.776%\n",
      "Epoch 39, Batch 772, LR 0.000000 Loss 4.410375, Accuracy 89.773%\n",
      "Epoch 39, Batch 773, LR 0.000000 Loss 4.410352, Accuracy 89.771%\n",
      "Epoch 39, Batch 774, LR 0.000000 Loss 4.410341, Accuracy 89.774%\n",
      "Epoch 39, Batch 775, LR 0.000000 Loss 4.410005, Accuracy 89.775%\n",
      "Epoch 39, Batch 776, LR 0.000000 Loss 4.410366, Accuracy 89.776%\n",
      "Epoch 39, Batch 777, LR 0.000000 Loss 4.410496, Accuracy 89.777%\n",
      "Epoch 39, Batch 778, LR 0.000000 Loss 4.409539, Accuracy 89.779%\n",
      "Epoch 39, Batch 779, LR 0.000000 Loss 4.409916, Accuracy 89.775%\n",
      "Epoch 39, Batch 780, LR 0.000000 Loss 4.410336, Accuracy 89.771%\n",
      "Epoch 39, Batch 781, LR 0.000000 Loss 4.411218, Accuracy 89.766%\n",
      "Epoch 39, Batch 782, LR 0.000000 Loss 4.411206, Accuracy 89.766%\n",
      "Epoch 39, Batch 783, LR 0.000000 Loss 4.411778, Accuracy 89.767%\n",
      "Epoch 39, Batch 784, LR 0.000000 Loss 4.411697, Accuracy 89.766%\n",
      "Epoch 39, Batch 785, LR 0.000000 Loss 4.411507, Accuracy 89.770%\n",
      "Epoch 39, Batch 786, LR 0.000000 Loss 4.412604, Accuracy 89.767%\n",
      "Epoch 39, Batch 787, LR 0.000000 Loss 4.413212, Accuracy 89.764%\n",
      "Epoch 39, Batch 788, LR 0.000000 Loss 4.412976, Accuracy 89.769%\n",
      "Epoch 39, Batch 789, LR 0.000000 Loss 4.412425, Accuracy 89.769%\n",
      "Epoch 39, Batch 790, LR 0.000000 Loss 4.412677, Accuracy 89.771%\n",
      "Epoch 39, Batch 791, LR 0.000000 Loss 4.413187, Accuracy 89.764%\n",
      "Epoch 39, Batch 792, LR 0.000000 Loss 4.413280, Accuracy 89.765%\n",
      "Epoch 39, Batch 793, LR 0.000000 Loss 4.413166, Accuracy 89.764%\n",
      "Epoch 39, Batch 794, LR 0.000000 Loss 4.413181, Accuracy 89.763%\n",
      "Epoch 39, Batch 795, LR 0.000000 Loss 4.413121, Accuracy 89.765%\n",
      "Epoch 39, Batch 796, LR 0.000000 Loss 4.413424, Accuracy 89.766%\n",
      "Epoch 39, Batch 797, LR 0.000000 Loss 4.413483, Accuracy 89.764%\n",
      "Epoch 39, Batch 798, LR 0.000000 Loss 4.412923, Accuracy 89.767%\n",
      "Epoch 39, Batch 799, LR 0.000000 Loss 4.413384, Accuracy 89.767%\n",
      "Epoch 39, Batch 800, LR 0.000000 Loss 4.412070, Accuracy 89.771%\n",
      "Epoch 39, Batch 801, LR 0.000000 Loss 4.412064, Accuracy 89.773%\n",
      "Epoch 39, Batch 802, LR 0.000000 Loss 4.411338, Accuracy 89.775%\n",
      "Epoch 39, Batch 803, LR 0.000000 Loss 4.410466, Accuracy 89.777%\n",
      "Epoch 39, Batch 804, LR 0.000000 Loss 4.410594, Accuracy 89.775%\n",
      "Epoch 39, Batch 805, LR 0.000000 Loss 4.410475, Accuracy 89.775%\n",
      "Epoch 39, Batch 806, LR 0.000000 Loss 4.410050, Accuracy 89.774%\n",
      "Epoch 39, Batch 807, LR 0.000000 Loss 4.410872, Accuracy 89.770%\n",
      "Epoch 39, Batch 808, LR 0.000000 Loss 4.410781, Accuracy 89.769%\n",
      "Epoch 39, Batch 809, LR 0.000000 Loss 4.410453, Accuracy 89.768%\n",
      "Epoch 39, Batch 810, LR 0.000000 Loss 4.409878, Accuracy 89.770%\n",
      "Epoch 39, Batch 811, LR 0.000000 Loss 4.409708, Accuracy 89.772%\n",
      "Epoch 39, Batch 812, LR 0.000000 Loss 4.409297, Accuracy 89.771%\n",
      "Epoch 39, Batch 813, LR 0.000000 Loss 4.408661, Accuracy 89.775%\n",
      "Epoch 39, Batch 814, LR 0.000000 Loss 4.408910, Accuracy 89.772%\n",
      "Epoch 39, Batch 815, LR 0.000000 Loss 4.408530, Accuracy 89.773%\n",
      "Epoch 39, Batch 816, LR 0.000000 Loss 4.409322, Accuracy 89.772%\n",
      "Epoch 39, Batch 817, LR 0.000000 Loss 4.409926, Accuracy 89.766%\n",
      "Epoch 39, Batch 818, LR 0.000000 Loss 4.409686, Accuracy 89.768%\n",
      "Epoch 39, Batch 819, LR 0.000000 Loss 4.408799, Accuracy 89.774%\n",
      "Epoch 39, Batch 820, LR 0.000000 Loss 4.409303, Accuracy 89.773%\n",
      "Epoch 39, Batch 821, LR 0.000000 Loss 4.410157, Accuracy 89.768%\n",
      "Epoch 39, Batch 822, LR 0.000000 Loss 4.410160, Accuracy 89.765%\n",
      "Epoch 39, Batch 823, LR 0.000000 Loss 4.410475, Accuracy 89.763%\n",
      "Epoch 39, Batch 824, LR 0.000000 Loss 4.411989, Accuracy 89.757%\n",
      "Epoch 39, Batch 825, LR 0.000000 Loss 4.412636, Accuracy 89.754%\n",
      "Epoch 39, Batch 826, LR 0.000000 Loss 4.413192, Accuracy 89.750%\n",
      "Epoch 39, Batch 827, LR 0.000000 Loss 4.413077, Accuracy 89.752%\n",
      "Epoch 39, Batch 828, LR 0.000000 Loss 4.414380, Accuracy 89.747%\n",
      "Epoch 39, Batch 829, LR 0.000000 Loss 4.414010, Accuracy 89.750%\n",
      "Epoch 39, Batch 830, LR 0.000000 Loss 4.414234, Accuracy 89.748%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 831, LR 0.000000 Loss 4.414121, Accuracy 89.745%\n",
      "Epoch 39, Batch 832, LR 0.000000 Loss 4.414298, Accuracy 89.743%\n",
      "Epoch 39, Batch 833, LR 0.000000 Loss 4.413580, Accuracy 89.751%\n",
      "Epoch 39, Batch 834, LR 0.000000 Loss 4.414109, Accuracy 89.749%\n",
      "Epoch 39, Batch 835, LR 0.000000 Loss 4.413701, Accuracy 89.751%\n",
      "Epoch 39, Batch 836, LR 0.000000 Loss 4.413613, Accuracy 89.753%\n",
      "Epoch 39, Batch 837, LR 0.000000 Loss 4.414387, Accuracy 89.745%\n",
      "Epoch 39, Batch 838, LR 0.000000 Loss 4.414995, Accuracy 89.742%\n",
      "Epoch 39, Batch 839, LR 0.000000 Loss 4.415776, Accuracy 89.737%\n",
      "Epoch 39, Batch 840, LR 0.000000 Loss 4.415708, Accuracy 89.736%\n",
      "Epoch 39, Batch 841, LR 0.000000 Loss 4.415336, Accuracy 89.734%\n",
      "Epoch 39, Batch 842, LR 0.000000 Loss 4.415527, Accuracy 89.735%\n",
      "Epoch 39, Batch 843, LR 0.000000 Loss 4.414594, Accuracy 89.739%\n",
      "Epoch 39, Batch 844, LR 0.000000 Loss 4.415088, Accuracy 89.739%\n",
      "Epoch 39, Batch 845, LR 0.000000 Loss 4.415240, Accuracy 89.738%\n",
      "Epoch 39, Batch 846, LR 0.000000 Loss 4.415988, Accuracy 89.736%\n",
      "Epoch 39, Batch 847, LR 0.000000 Loss 4.415521, Accuracy 89.740%\n",
      "Epoch 39, Batch 848, LR 0.000000 Loss 4.415799, Accuracy 89.741%\n",
      "Epoch 39, Batch 849, LR 0.000000 Loss 4.415512, Accuracy 89.742%\n",
      "Epoch 39, Batch 850, LR 0.000000 Loss 4.415272, Accuracy 89.744%\n",
      "Epoch 39, Batch 851, LR 0.000000 Loss 4.416110, Accuracy 89.736%\n",
      "Epoch 39, Batch 852, LR 0.000000 Loss 4.414935, Accuracy 89.741%\n",
      "Epoch 39, Batch 853, LR 0.000000 Loss 4.414985, Accuracy 89.743%\n",
      "Epoch 39, Batch 854, LR 0.000000 Loss 4.414738, Accuracy 89.739%\n",
      "Epoch 39, Batch 855, LR 0.000000 Loss 4.414960, Accuracy 89.740%\n",
      "Epoch 39, Batch 856, LR 0.000000 Loss 4.414398, Accuracy 89.743%\n",
      "Epoch 39, Batch 857, LR 0.000000 Loss 4.413985, Accuracy 89.743%\n",
      "Epoch 39, Batch 858, LR 0.000000 Loss 4.414336, Accuracy 89.741%\n",
      "Epoch 39, Batch 859, LR 0.000000 Loss 4.414387, Accuracy 89.742%\n",
      "Epoch 39, Batch 860, LR 0.000000 Loss 4.415247, Accuracy 89.741%\n",
      "Epoch 39, Batch 861, LR 0.000000 Loss 4.415697, Accuracy 89.737%\n",
      "Epoch 39, Batch 862, LR 0.000000 Loss 4.415183, Accuracy 89.740%\n",
      "Epoch 39, Batch 863, LR 0.000000 Loss 4.414119, Accuracy 89.742%\n",
      "Epoch 39, Batch 864, LR 0.000000 Loss 4.413820, Accuracy 89.744%\n",
      "Epoch 39, Batch 865, LR 0.000000 Loss 4.413410, Accuracy 89.746%\n",
      "Epoch 39, Batch 866, LR 0.000000 Loss 4.412744, Accuracy 89.748%\n",
      "Epoch 39, Batch 867, LR 0.000000 Loss 4.412145, Accuracy 89.751%\n",
      "Epoch 39, Batch 868, LR 0.000000 Loss 4.412500, Accuracy 89.753%\n",
      "Epoch 39, Batch 869, LR 0.000000 Loss 4.411648, Accuracy 89.754%\n",
      "Epoch 39, Batch 870, LR 0.000000 Loss 4.412043, Accuracy 89.753%\n",
      "Epoch 39, Batch 871, LR 0.000000 Loss 4.411499, Accuracy 89.752%\n",
      "Epoch 39, Batch 872, LR 0.000000 Loss 4.411590, Accuracy 89.756%\n",
      "Epoch 39, Batch 873, LR 0.000000 Loss 4.411247, Accuracy 89.754%\n",
      "Epoch 39, Batch 874, LR 0.000000 Loss 4.410804, Accuracy 89.754%\n",
      "Epoch 39, Batch 875, LR 0.000000 Loss 4.410682, Accuracy 89.750%\n",
      "Epoch 39, Batch 876, LR 0.000000 Loss 4.411251, Accuracy 89.747%\n",
      "Epoch 39, Batch 877, LR 0.000000 Loss 4.411552, Accuracy 89.744%\n",
      "Epoch 39, Batch 878, LR 0.000000 Loss 4.411288, Accuracy 89.743%\n",
      "Epoch 39, Batch 879, LR 0.000000 Loss 4.411602, Accuracy 89.741%\n",
      "Epoch 39, Batch 880, LR 0.000000 Loss 4.411625, Accuracy 89.738%\n",
      "Epoch 39, Batch 881, LR 0.000000 Loss 4.412485, Accuracy 89.731%\n",
      "Epoch 39, Batch 882, LR 0.000000 Loss 4.412795, Accuracy 89.731%\n",
      "Epoch 39, Batch 883, LR 0.000000 Loss 4.413281, Accuracy 89.730%\n",
      "Epoch 39, Batch 884, LR 0.000000 Loss 4.413022, Accuracy 89.731%\n",
      "Epoch 39, Batch 885, LR 0.000000 Loss 4.412756, Accuracy 89.730%\n",
      "Epoch 39, Batch 886, LR 0.000000 Loss 4.411470, Accuracy 89.734%\n",
      "Epoch 39, Batch 887, LR 0.000000 Loss 4.410517, Accuracy 89.736%\n",
      "Epoch 39, Batch 888, LR 0.000000 Loss 4.411145, Accuracy 89.735%\n",
      "Epoch 39, Batch 889, LR 0.000000 Loss 4.411410, Accuracy 89.730%\n",
      "Epoch 39, Batch 890, LR 0.000000 Loss 4.411844, Accuracy 89.725%\n",
      "Epoch 39, Batch 891, LR 0.000000 Loss 4.412874, Accuracy 89.725%\n",
      "Epoch 39, Batch 892, LR 0.000000 Loss 4.412505, Accuracy 89.725%\n",
      "Epoch 39, Batch 893, LR 0.000000 Loss 4.412443, Accuracy 89.726%\n",
      "Epoch 39, Batch 894, LR 0.000000 Loss 4.412372, Accuracy 89.727%\n",
      "Epoch 39, Batch 895, LR 0.000000 Loss 4.412476, Accuracy 89.727%\n",
      "Epoch 39, Batch 896, LR 0.000000 Loss 4.412636, Accuracy 89.724%\n",
      "Epoch 39, Batch 897, LR 0.000000 Loss 4.412836, Accuracy 89.720%\n",
      "Epoch 39, Batch 898, LR 0.000000 Loss 4.413020, Accuracy 89.720%\n",
      "Epoch 39, Batch 899, LR 0.000000 Loss 4.413608, Accuracy 89.718%\n",
      "Epoch 39, Batch 900, LR 0.000000 Loss 4.412454, Accuracy 89.721%\n",
      "Epoch 39, Batch 901, LR 0.000000 Loss 4.412961, Accuracy 89.723%\n",
      "Epoch 39, Batch 902, LR 0.000000 Loss 4.413016, Accuracy 89.720%\n",
      "Epoch 39, Batch 903, LR 0.000000 Loss 4.413487, Accuracy 89.717%\n",
      "Epoch 39, Batch 904, LR 0.000000 Loss 4.413401, Accuracy 89.719%\n",
      "Epoch 39, Batch 905, LR 0.000000 Loss 4.413809, Accuracy 89.720%\n",
      "Epoch 39, Batch 906, LR 0.000000 Loss 4.414389, Accuracy 89.720%\n",
      "Epoch 39, Batch 907, LR 0.000000 Loss 4.414816, Accuracy 89.721%\n",
      "Epoch 39, Batch 908, LR 0.000000 Loss 4.415164, Accuracy 89.722%\n",
      "Epoch 39, Batch 909, LR 0.000000 Loss 4.415089, Accuracy 89.720%\n",
      "Epoch 39, Batch 910, LR 0.000000 Loss 4.414523, Accuracy 89.719%\n",
      "Epoch 39, Batch 911, LR 0.000000 Loss 4.415247, Accuracy 89.715%\n",
      "Epoch 39, Batch 912, LR 0.000000 Loss 4.414972, Accuracy 89.715%\n",
      "Epoch 39, Batch 913, LR 0.000000 Loss 4.415385, Accuracy 89.715%\n",
      "Epoch 39, Batch 914, LR 0.000000 Loss 4.414795, Accuracy 89.715%\n",
      "Epoch 39, Batch 915, LR 0.000000 Loss 4.414024, Accuracy 89.718%\n",
      "Epoch 39, Batch 916, LR 0.000000 Loss 4.413271, Accuracy 89.721%\n",
      "Epoch 39, Batch 917, LR 0.000000 Loss 4.413157, Accuracy 89.724%\n",
      "Epoch 39, Batch 918, LR 0.000000 Loss 4.412898, Accuracy 89.729%\n",
      "Epoch 39, Batch 919, LR 0.000000 Loss 4.413025, Accuracy 89.729%\n",
      "Epoch 39, Batch 920, LR 0.000000 Loss 4.413662, Accuracy 89.730%\n",
      "Epoch 39, Batch 921, LR 0.000000 Loss 4.413773, Accuracy 89.732%\n",
      "Epoch 39, Batch 922, LR 0.000000 Loss 4.413968, Accuracy 89.729%\n",
      "Epoch 39, Batch 923, LR 0.000000 Loss 4.413997, Accuracy 89.729%\n",
      "Epoch 39, Batch 924, LR 0.000000 Loss 4.413646, Accuracy 89.732%\n",
      "Epoch 39, Batch 925, LR 0.000000 Loss 4.413894, Accuracy 89.732%\n",
      "Epoch 39, Batch 926, LR 0.000000 Loss 4.413982, Accuracy 89.732%\n",
      "Epoch 39, Batch 927, LR 0.000000 Loss 4.414662, Accuracy 89.731%\n",
      "Epoch 39, Batch 928, LR 0.000000 Loss 4.414922, Accuracy 89.728%\n",
      "Epoch 39, Batch 929, LR 0.000000 Loss 4.415203, Accuracy 89.724%\n",
      "Epoch 39, Batch 930, LR 0.000000 Loss 4.415315, Accuracy 89.724%\n",
      "Epoch 39, Batch 931, LR 0.000000 Loss 4.415725, Accuracy 89.721%\n",
      "Epoch 39, Batch 932, LR 0.000000 Loss 4.415568, Accuracy 89.721%\n",
      "Epoch 39, Batch 933, LR 0.000000 Loss 4.415842, Accuracy 89.721%\n",
      "Epoch 39, Batch 934, LR 0.000000 Loss 4.415640, Accuracy 89.723%\n",
      "Epoch 39, Batch 935, LR 0.000000 Loss 4.415597, Accuracy 89.723%\n",
      "Epoch 39, Batch 936, LR 0.000000 Loss 4.415934, Accuracy 89.721%\n",
      "Epoch 39, Batch 937, LR 0.000000 Loss 4.415850, Accuracy 89.725%\n",
      "Epoch 39, Batch 938, LR 0.000000 Loss 4.415725, Accuracy 89.723%\n",
      "Epoch 39, Batch 939, LR 0.000000 Loss 4.415839, Accuracy 89.720%\n",
      "Epoch 39, Batch 940, LR 0.000000 Loss 4.415976, Accuracy 89.717%\n",
      "Epoch 39, Batch 941, LR 0.000000 Loss 4.415983, Accuracy 89.717%\n",
      "Epoch 39, Batch 942, LR 0.000000 Loss 4.415912, Accuracy 89.718%\n",
      "Epoch 39, Batch 943, LR 0.000000 Loss 4.416635, Accuracy 89.712%\n",
      "Epoch 39, Batch 944, LR 0.000000 Loss 4.416471, Accuracy 89.714%\n",
      "Epoch 39, Batch 945, LR 0.000000 Loss 4.416557, Accuracy 89.711%\n",
      "Epoch 39, Batch 946, LR 0.000000 Loss 4.417138, Accuracy 89.710%\n",
      "Epoch 39, Batch 947, LR 0.000000 Loss 4.416858, Accuracy 89.711%\n",
      "Epoch 39, Batch 948, LR 0.000000 Loss 4.415863, Accuracy 89.715%\n",
      "Epoch 39, Batch 949, LR 0.000000 Loss 4.415820, Accuracy 89.715%\n",
      "Epoch 39, Batch 950, LR 0.000000 Loss 4.415434, Accuracy 89.718%\n",
      "Epoch 39, Batch 951, LR 0.000000 Loss 4.415662, Accuracy 89.712%\n",
      "Epoch 39, Batch 952, LR 0.000000 Loss 4.415436, Accuracy 89.710%\n",
      "Epoch 39, Batch 953, LR 0.000000 Loss 4.415538, Accuracy 89.710%\n",
      "Epoch 39, Batch 954, LR 0.000000 Loss 4.415987, Accuracy 89.707%\n",
      "Epoch 39, Batch 955, LR 0.000000 Loss 4.416190, Accuracy 89.704%\n",
      "Epoch 39, Batch 956, LR 0.000000 Loss 4.415632, Accuracy 89.706%\n",
      "Epoch 39, Batch 957, LR 0.000000 Loss 4.415378, Accuracy 89.710%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 958, LR 0.000000 Loss 4.415315, Accuracy 89.708%\n",
      "Epoch 39, Batch 959, LR 0.000000 Loss 4.415762, Accuracy 89.709%\n",
      "Epoch 39, Batch 960, LR 0.000000 Loss 4.415128, Accuracy 89.713%\n",
      "Epoch 39, Batch 961, LR 0.000000 Loss 4.415222, Accuracy 89.709%\n",
      "Epoch 39, Batch 962, LR 0.000000 Loss 4.415371, Accuracy 89.708%\n",
      "Epoch 39, Batch 963, LR 0.000000 Loss 4.414906, Accuracy 89.712%\n",
      "Epoch 39, Batch 964, LR 0.000000 Loss 4.414703, Accuracy 89.712%\n",
      "Epoch 39, Batch 965, LR 0.000000 Loss 4.414999, Accuracy 89.708%\n",
      "Epoch 39, Batch 966, LR 0.000000 Loss 4.415051, Accuracy 89.707%\n",
      "Epoch 39, Batch 967, LR 0.000000 Loss 4.414675, Accuracy 89.707%\n",
      "Epoch 39, Batch 968, LR 0.000000 Loss 4.415120, Accuracy 89.703%\n",
      "Epoch 39, Batch 969, LR 0.000000 Loss 4.415516, Accuracy 89.699%\n",
      "Epoch 39, Batch 970, LR 0.000000 Loss 4.415427, Accuracy 89.700%\n",
      "Epoch 39, Batch 971, LR 0.000000 Loss 4.415843, Accuracy 89.696%\n",
      "Epoch 39, Batch 972, LR 0.000000 Loss 4.416312, Accuracy 89.694%\n",
      "Epoch 39, Batch 973, LR 0.000000 Loss 4.416411, Accuracy 89.692%\n",
      "Epoch 39, Batch 974, LR 0.000000 Loss 4.416011, Accuracy 89.694%\n",
      "Epoch 39, Batch 975, LR 0.000000 Loss 4.416249, Accuracy 89.694%\n",
      "Epoch 39, Batch 976, LR 0.000000 Loss 4.416632, Accuracy 89.691%\n",
      "Epoch 39, Batch 977, LR 0.000000 Loss 4.416748, Accuracy 89.689%\n",
      "Epoch 39, Batch 978, LR 0.000000 Loss 4.416599, Accuracy 89.692%\n",
      "Epoch 39, Batch 979, LR 0.000000 Loss 4.416356, Accuracy 89.694%\n",
      "Epoch 39, Batch 980, LR 0.000000 Loss 4.416604, Accuracy 89.687%\n",
      "Epoch 39, Batch 981, LR 0.000000 Loss 4.416127, Accuracy 89.688%\n",
      "Epoch 39, Batch 982, LR 0.000000 Loss 4.415898, Accuracy 89.687%\n",
      "Epoch 39, Batch 983, LR 0.000000 Loss 4.415956, Accuracy 89.685%\n",
      "Epoch 39, Batch 984, LR 0.000000 Loss 4.415145, Accuracy 89.691%\n",
      "Epoch 39, Batch 985, LR 0.000000 Loss 4.415345, Accuracy 89.691%\n",
      "Epoch 39, Batch 986, LR 0.000000 Loss 4.414831, Accuracy 89.692%\n",
      "Epoch 39, Batch 987, LR 0.000000 Loss 4.415077, Accuracy 89.689%\n",
      "Epoch 39, Batch 988, LR 0.000000 Loss 4.415757, Accuracy 89.688%\n",
      "Epoch 39, Batch 989, LR 0.000000 Loss 4.415703, Accuracy 89.687%\n",
      "Epoch 39, Batch 990, LR 0.000000 Loss 4.415340, Accuracy 89.689%\n",
      "Epoch 39, Batch 991, LR 0.000000 Loss 4.415052, Accuracy 89.692%\n",
      "Epoch 39, Batch 992, LR 0.000000 Loss 4.414326, Accuracy 89.694%\n",
      "Epoch 39, Batch 993, LR 0.000000 Loss 4.415279, Accuracy 89.690%\n",
      "Epoch 39, Batch 994, LR 0.000000 Loss 4.416270, Accuracy 89.685%\n",
      "Epoch 39, Batch 995, LR 0.000000 Loss 4.416203, Accuracy 89.688%\n",
      "Epoch 39, Batch 996, LR 0.000000 Loss 4.415796, Accuracy 89.689%\n",
      "Epoch 39, Batch 997, LR 0.000000 Loss 4.415053, Accuracy 89.692%\n",
      "Epoch 39, Batch 998, LR 0.000000 Loss 4.415328, Accuracy 89.690%\n",
      "Epoch 39, Batch 999, LR 0.000000 Loss 4.415001, Accuracy 89.694%\n",
      "Epoch 39, Batch 1000, LR 0.000000 Loss 4.414923, Accuracy 89.694%\n",
      "Epoch 39, Batch 1001, LR 0.000000 Loss 4.414855, Accuracy 89.695%\n",
      "Epoch 39, Batch 1002, LR 0.000000 Loss 4.414687, Accuracy 89.696%\n",
      "Epoch 39, Batch 1003, LR 0.000000 Loss 4.414297, Accuracy 89.701%\n",
      "Epoch 39, Batch 1004, LR 0.000000 Loss 4.414815, Accuracy 89.701%\n",
      "Epoch 39, Batch 1005, LR 0.000000 Loss 4.415248, Accuracy 89.698%\n",
      "Epoch 39, Batch 1006, LR 0.000000 Loss 4.415193, Accuracy 89.702%\n",
      "Epoch 39, Batch 1007, LR 0.000000 Loss 4.415116, Accuracy 89.702%\n",
      "Epoch 39, Batch 1008, LR 0.000000 Loss 4.415602, Accuracy 89.697%\n",
      "Epoch 39, Batch 1009, LR 0.000000 Loss 4.415741, Accuracy 89.697%\n",
      "Epoch 39, Batch 1010, LR 0.000000 Loss 4.416278, Accuracy 89.695%\n",
      "Epoch 39, Batch 1011, LR 0.000000 Loss 4.415930, Accuracy 89.698%\n",
      "Epoch 39, Batch 1012, LR 0.000000 Loss 4.415428, Accuracy 89.699%\n",
      "Epoch 39, Batch 1013, LR 0.000000 Loss 4.414806, Accuracy 89.702%\n",
      "Epoch 39, Batch 1014, LR 0.000000 Loss 4.414541, Accuracy 89.705%\n",
      "Epoch 39, Batch 1015, LR 0.000000 Loss 4.414873, Accuracy 89.704%\n",
      "Epoch 39, Batch 1016, LR 0.000000 Loss 4.414753, Accuracy 89.707%\n",
      "Epoch 39, Batch 1017, LR 0.000000 Loss 4.414914, Accuracy 89.709%\n",
      "Epoch 39, Batch 1018, LR 0.000000 Loss 4.415526, Accuracy 89.707%\n",
      "Epoch 39, Batch 1019, LR 0.000000 Loss 4.415469, Accuracy 89.707%\n",
      "Epoch 39, Batch 1020, LR 0.000000 Loss 4.415352, Accuracy 89.705%\n",
      "Epoch 39, Batch 1021, LR 0.000000 Loss 4.415368, Accuracy 89.706%\n",
      "Epoch 39, Batch 1022, LR 0.000000 Loss 4.414766, Accuracy 89.708%\n",
      "Epoch 39, Batch 1023, LR 0.000000 Loss 4.415224, Accuracy 89.708%\n",
      "Epoch 39, Batch 1024, LR 0.000000 Loss 4.415042, Accuracy 89.707%\n",
      "Epoch 39, Batch 1025, LR 0.000000 Loss 4.413866, Accuracy 89.713%\n",
      "Epoch 39, Batch 1026, LR 0.000000 Loss 4.413324, Accuracy 89.718%\n",
      "Epoch 39, Batch 1027, LR 0.000000 Loss 4.414054, Accuracy 89.712%\n",
      "Epoch 39, Batch 1028, LR 0.000000 Loss 4.414556, Accuracy 89.708%\n",
      "Epoch 39, Batch 1029, LR 0.000000 Loss 4.413931, Accuracy 89.710%\n",
      "Epoch 39, Batch 1030, LR 0.000000 Loss 4.413961, Accuracy 89.708%\n",
      "Epoch 39, Batch 1031, LR 0.000000 Loss 4.413523, Accuracy 89.710%\n",
      "Epoch 39, Batch 1032, LR 0.000000 Loss 4.414772, Accuracy 89.703%\n",
      "Epoch 39, Batch 1033, LR 0.000000 Loss 4.414897, Accuracy 89.702%\n",
      "Epoch 39, Batch 1034, LR 0.000000 Loss 4.414654, Accuracy 89.705%\n",
      "Epoch 39, Batch 1035, LR 0.000000 Loss 4.414079, Accuracy 89.708%\n",
      "Epoch 39, Batch 1036, LR 0.000000 Loss 4.414247, Accuracy 89.707%\n",
      "Epoch 39, Batch 1037, LR 0.000000 Loss 4.414180, Accuracy 89.705%\n",
      "Epoch 39, Batch 1038, LR 0.000000 Loss 4.414392, Accuracy 89.706%\n",
      "Epoch 39, Batch 1039, LR 0.000000 Loss 4.415105, Accuracy 89.702%\n",
      "Epoch 39, Batch 1040, LR 0.000000 Loss 4.415200, Accuracy 89.703%\n",
      "Epoch 39, Batch 1041, LR 0.000000 Loss 4.414913, Accuracy 89.703%\n",
      "Epoch 39, Batch 1042, LR 0.000000 Loss 4.415641, Accuracy 89.701%\n",
      "Epoch 39, Batch 1043, LR 0.000000 Loss 4.415096, Accuracy 89.702%\n",
      "Epoch 39, Batch 1044, LR 0.000000 Loss 4.415039, Accuracy 89.708%\n",
      "Epoch 39, Batch 1045, LR 0.000000 Loss 4.415304, Accuracy 89.704%\n",
      "Epoch 39, Batch 1046, LR 0.000000 Loss 4.415283, Accuracy 89.708%\n",
      "Epoch 39, Batch 1047, LR 0.000000 Loss 4.415599, Accuracy 89.704%\n",
      "Epoch 39, Loss (train set) 4.415599, Accuracy (train set) 89.704%\n",
      "Epoch 39, Accuracy (validation set) 72.532%\n",
      "Epoch 39, EER (test set) 5.398%\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "checkpoint_flag = False\n",
    "\n",
    "if checkpoint_flag:\n",
    "    start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab6_models/lab6_model_0004.pth')\n",
    "    start_epoch = start_epoch + 1\n",
    "\n",
    "# Train model\n",
    "for num_epoch in range(start_epoch, max_epoch):\n",
    "    train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "    \n",
    "    print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "    if (num_epoch + 1)%val_interval == 0:\n",
    "        _, val_top1 = test_network(val_loader, main_model)                               # compute accuracy for validation set\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "        feats = extract_features(main_model, test_loader)                                # compute enroll/test speaker embeddings\n",
    "        all_scores, all_labels, all_trials = compute_scores(feats, verif_protocol_lines) # compute scores between enroll and test speaker models\n",
    "        tar_scores, imp_scores = tar_imp_hists(all_scores, all_labels)                   # devide the scores into target and impostor scores\n",
    "        EER, _ = get_eer(tar_scores, imp_scores)                                         # compute EER, %\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, EER (test set) {:2.3f}%\".format(num_epoch, EER))\n",
    "        \n",
    "        saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab6_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f235a",
   "metadata": {},
   "source": [
    "**3. Дополнительные задания по обучению дикторской модели на основе трансформерной архитектуры**\n",
    "\n",
    "В рамках настоящего пункта предлагается выбрать и решить одно из дополнительных заданий на выбор:\n",
    "\n",
    "1. Переведя параметр load_weight кодировщика Whisper tiny в режим False, попробуйте обучить трансформерную нейронную сеть без предварительно загруженных в неё весов кодировщика. Сравните полученное качество модели с тем, что было в пункте 2. При проведении исследования разрешается менять любые настройки процедуры обучения, в частности, параметр сходимости.\n",
    "\n",
    "2. Переведя параметр freeze_feats кодировщика Whisper tiny в режим True, попробуйте обучить трансформерную нейронную сеть с «замороженным» кодировщиком. Сравните полученное качество модели с тем, что было в пункте 2. При проведении исследования разрешается менять любые настройки процедуры обучения, в частности, параметр сходимости.\n",
    "\n",
    "3. В настройках процедуры обучения рассогласуйте параметр, отвечающий за максимальное число фреймов, используемых на этапе обучения, валидации и тестирования, например, выставив его равным 400 на этапе обучения и 1000 на этапах валидации и тестирования. Попробуйте обучить трансформерную нейронную сеть для выбранных настроек. Сравните полученное качество модели с тем, что было в пункте 2.\n",
    "\n",
    "4. Попробуйте заменить кодировщика Whisper tiny на Whisper модель больших размеров base/small/medium/large и повторите процедуру обучения из пункта 2. Сравните полученное качество модели с тем, что было в пункте 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4189778",
   "metadata": {},
   "source": [
    "**4. Контрольные вопросы**\n",
    "\n",
    "1. Что такое трансформерные нейросетевые модели и для какой задачи они были изначально предложены?\n",
    "\n",
    "2. Назовите основные отличия трансформерных нейросетевых архитектур от свёрточных?\n",
    "\n",
    "3. Как устроена архитектура Whisper и для решения каких задач она была изначально разработана?\n",
    "\n",
    "4. Из каких основных нейросетевых слоёв состоит архитектура Whisper?\n",
    "\n",
    "5. Какие разновидности архитектуры Whisper существуют и чем они отличаются друг от друга?\n",
    "\n",
    "6. Что такое предобученная нейросетевая модель и для каких целей она используется?\n",
    "\n",
    "7. В чём состоит основное отличие в политике изменения параметра сходимости на этапе обучения при наличии преобученной модели от ситуации, когда она отсутствует? \n",
    "\n",
    "8. Назовите примеры трансформерных нейросетевых претрейнов для обработки аудиосигналов и кратко опишите, как они были получены?\n",
    "\n",
    "9. Как реализовать блок построения дикторских моделей на основе архитектуры Whisper?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d110ea",
   "metadata": {},
   "source": [
    "**5. Список литературы**\n",
    "\n",
    "1. Bai Z., Zhang X.-L., Chen J. Speaker recognition based on deep learning: an overview // \tarXiv:2012.00931 [eess.AS] ([ссылка](https://arxiv.org/pdf/2012.00931.pdf)).\n",
    "\n",
    "2. Hansen J.H.L., Hasan T. Speaker recognition by machines and humans: a tutorial review // IEEE Signal Processing Magazine, 2015. V. 32. № 6. P. 74–99 ([ссылка](https://www.researchgate.net/publication/282940395_Speaker_Recognition_by_Machines_and_Humans_A_tutorial_review)).\n",
    "\n",
    "3. Vaswani A. et al. Attention is all you need // arXiv:1706.03762 [cs.CL] ([ссылка](https://arxiv.org/pdf/1706.03762)).\n",
    "\n",
    "4. Radford A. et al. Robust speech recognition via large-scale weak supervision // arXiv:2212.04356 [eess.AS] ([ссылка](https://arxiv.org/pdf/2212.04356))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
