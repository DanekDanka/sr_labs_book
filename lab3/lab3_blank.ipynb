{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9199231",
   "metadata": {},
   "source": [
    "**Лабораторный практикум по курсу «Распознавание диктора», Университет ИТМО, 2021**\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91dd7db",
   "metadata": {},
   "source": [
    "**Лабораторная работа №3. Построение дикторских моделей и их сравнение**\n",
    "\n",
    "**Цель работы:** изучение процедуры построения дикторских моделей с использованием глубоких нейросетевых архитектур.\n",
    "\n",
    "**Краткое описание:** в рамках настоящей лабораторной работы предлагается изучить и реализовать схему построения дикторских моделей с использованием глубокой нейросетевой архитектуры, построенной на основе ResNet-блоков. Процедуры обучения и тестирования предлагается рассмотреть по отношению к задаче идентификации на закрытом множестве, то есть для ситуации, когда дикторские классы являются строго заданными. Тестирование полученной системы предполагает использование доли правильных ответов (accuracy) в качестве целевой метрики оценки качества.\n",
    "\n",
    "**Данные:** в качестве данных для выполнения лабораторной работы предлагается использовать базу [VoxCeleb1](http://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html).\n",
    "\n",
    "**Содержание лабораторной работы**\n",
    "\n",
    "1. Подготовка данных для обучения и тестирования блока построения дикторских моделей.\t\t\t\t\t\t\t\n",
    "\n",
    "2. Обучение параметров блока построения дикторских моделей без учёта процедуры аугментации данных.\n",
    "\n",
    "3. Обучение параметров блока построения дикторских моделей с учётом процедуры аугментации данных.\n",
    "\n",
    "4. Тестированное блока построения дикторских моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dd2e7a-f784-4e0a-a9ec-229ae5a913c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f862cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPython extension to reload modules before executing user code\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import of modules\n",
    "import os\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "sys.path.append(os.path.realpath('..'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from common import download_dataset, concatenate, extract_dataset, part_extract, download_protocol, split_musan\n",
    "from exercises_blank import train_dataset_loader, test_dataset_loader, ResNet, MainModel, train_network, test_network\n",
    "from ResNetBlocks import BasicBlock\n",
    "from LossFunction import AAMSoftmaxLoss\n",
    "from Optimizer import SGDOptimizer\n",
    "from Scheduler import OneCycleLRScheduler\n",
    "from load_save_pth import saveParameters, loadParameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3baf71",
   "metadata": {},
   "source": [
    "**1. Подготовка данных для обучения и тестирования детектора речевой активности**\n",
    "\n",
    "В ходе выполнения лабораторной работы необходимы данные для выполнения процедуры обучения и процедуры тестирования нейросетевого блока генерации дикторских моделей. Возьмём в качестве этих данных звукозаписи, сохраненные в формат *wav*, из корпуса [VoxCeleb1 dev set](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/vox1.html). Данный корпус содержит 148,642 звукозаписи (частота дискретизации равна 16кГц) для 1,211 дикторов женского и мужского пола, разговаривающих преимущественно на английском языке.\n",
    "\n",
    "В рамках настоящего пункта требуется выполнить загрузку и распаковку звуковых wav-файлов из корпуса VoxCeleb1 dev set.\n",
    "\n",
    "![Рисунок 1](https://analyticsindiamag.com/wp-content/uploads/2020/12/image.png \"VoxCeleb. Крупномасштабная аудиовизуальная база данных человеческой речи.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd7ecdb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download VoxCeleb1 (test set)\n",
    "# with open('../data/lists/datasets.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# download_dataset(lines, user='voxceleb1902', password='nx0bl2v2', save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0fd3c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate archives for VoxCeleb1 dev set\n",
    "# with open('../data/lists/concat_arch.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# concatenate(lines, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed3a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract VoxCeleb1 dev set\n",
    "# extract_dataset(save_path='../data/voxceleb1_dev', fname='../data/vox1_dev_wav.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "875ec4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download VoxCeleb1 identification protocol\n",
    "# with open('../data/lists/protocols.txt', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "    \n",
    "# download_protocol(lines, save_path='../data/voxceleb1_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698d6aee",
   "metadata": {},
   "source": [
    "**2. Обучение параметров блока построения дикторских моделей без учёта процедуры аугментации данных**\n",
    "\n",
    "Построение современных дикторских моделей, как правило, выполняется с использованием нейросетевых архитектур, многие из которых позаимствованы из области обработки цифровых изображений. Одними из наиболее распространенных нейросетевых архитектур, используемыми для построения дикторских моделей, являются [ResNet-подобные архитектуры](https://arxiv.org/pdf/1512.03385.pdf). В рамках настоящего пункта предлагается выполнить адаптацию нейросетевой архитектуры ResNet34 для решения задачи генерации дикторских моделей (дикторских эмбеддингов). *Дикторский эмбеддинг* – это высокоуровневый вектор-признаков, состоящий, например, из 128, 256 и т.п. значений, содержащий особенности голоса конкретного человека. При решении задачи распознавания диктора можно выделить эталонные и тестовые дикторские эмбеддинги. *Эталонные эмбеддинги* формируются на этапе регистрации дикторской модели определённого человека и находятся в некотором хранилище данных. *Тестовые эмбеддинги* формируются на этапе непосредственного использования системы голосовой биометрии на практике, когда некоторый пользователь пытается получить доступ к соответствующим ресурсам. Система голосовой биометрии сравнивает по определённой метрике эталонные и тестовые эмбеддинги, формируя оценку сравнения, которая, после её обработки блоком принятия решения, позволяет сделать вывод о том, эмбеддинги одинаковых или разных дикторов сравниваются между собой.\n",
    "\n",
    "Адаптация различных нейросетевых архитектур из обработки изображений к решению задачи построения дикторских моделей является непростой задачей. Возьмём за основу готовое решение, предложенной в рамках [следующей публикации](https://arxiv.org/pdf/2002.06033.pdf) и адаптируем его применительно к выполнению настоящей лабораторной работы.\n",
    "\n",
    "Необходимо отметить, что построение дикторских моделей, как правило, требует наличия *акустических признаков*, вычисленных для звукозаписей тренировочной, валидационной и тестовой баз данных. В качестве примера подобных признаков в рамках настоящей лабораторной работы воспользуемся *логарифмами энергий на выходе мел-банка фильтров*. Важно отметить, что акустические признаки подвергаются некоторым процедурам предобработки перед их непосредственной передачей в блок построения дикторских моделей. В качестве этих процедур можно выделить: нормализация и масштабирование признаков, сохранение только речевых фреймов на основе разметки детектора речевой активности и т.п.\n",
    "\n",
    "После того, как акустические признаки подготовлены, они могут быть переданы на блок построения дикторских моделей. Как правило, структура современных дикторских моделей соответствует структуре [x-векторных архитектур](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf). Эти архитектуры состоят из четырёх ключевых элементов: \n",
    "\n",
    "1. **Фреймовый уровень.** Предназначен для формирования локальных представлений голоса конкретного человека. На этом уровне как раз и применяются нейросетевые архитектуры на базе свёрточных нейронных сетей, например, ResNet, позволяющих с использованием каскадной схемы из множества фильтров с локальной маской захватить некоторый локальный контекст шаблона голоса человека. Выходом фреймового уровня является набор высокоуровневых представлений (карт-признаков), содержащих локальные особенности голоса человека.\n",
    "\n",
    "2. **Уровень статистического пулинга** позволяет сформировать промежуточный вектор-признаков, фиксированной длины, которая является одинаковой для звукозаписи любой длительности. В ходе работы блока статистического пулинга происходит удаление временной размерности, присутствующей в картах-признаков. Это достигается путём выполнения процедуры усреднения карт-признаков вдоль оси времени. Выходом уровня статистического пулинга являются вектор среднего и вектор среднеквадратического отклонения, вычисленные на основе карт-признаков. Эти вектора конкатенируются и передаются для дальнейшей обработки на сегментом уровне.\n",
    "\n",
    "3. **Сегментный уровень.** Предназначен для трансформации промежуточного вектора, как правило, высокой размерности, в компактный вектор-признаков, представляющий собой дикторский эмбеддинг. Необходимо отметить, что на сегментном уровне расположены один или несколько полносвязных нейросетевых слоёв, а обработка данных выполняется по отношению ко всей звукозаписи, а не только к некоторому её локальному контексту, как на фреймовом уровне.\n",
    "\n",
    "4. **Уровень выходного слоя.** Представляет полносвязный слой с softmax-функциями активации. Количество активаций равно числу дикторов в тренирочной выборке. На вход выходноя слоя подаётся дикторский эмбеддинг, а на выходе – формируется набор апостериорных вероятностей, определяющих принадлежность эмбеддинга к одному из дикторских классов в тренировочной выборке. Необходимо отметить, что, как правило, в современных нейросетевых системах построения дикторских моделей выходной используется только на этапе обучения параметров и на этапе тестирования не используется (на этапе тестирования используются только три первых уровня архитектуры).\n",
    "\n",
    "Обучение модели генерации дикторских эмбеддингов выполняется путём решения задачи *классификации* или, выражаясь терминами из области биометрии, *идентификации на закрытом множестве* (количество дикторских меток является строго фиксированным). В качестве используемой стоимостной функции выступает *категориальная кросс-энтропия*. Обучение выполняется с помощью мини-батчей, содержащих короткие фрагменты карт акустических признаков (длительностью несколько секунд) различных дикторов из тренировочной базы данных. Обучение на коротких фрагментов позволяет избежать сильного переобучения нейросетевой модели. При выполнении процедуры обучения требуется подобрать набор гиперпараметров, выбрать обучения и метод численной оптимизации.\n",
    "\n",
    "Для успешного выполнения настоящего пункта необходимо сделать следующее:\n",
    "\n",
    "1. Сгенерировать списки тренировочных, валидационных и тестовых данных на основе идентификационного протокола базы VoxCeleb1, содержащегося в файле **../data/voxceleb1_test/iden_split.txt**. При генерации списков требуется исключить из них звукозаписи дикторов, которые входят в базу [VoxCeleb1 test set](https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip). Это позволит выполнить тестирования обученных блоков генерации дикторских моделей на протоколе [VoxCeleb1-O cleaned](https://www.robots.ox.ac.uk/~vgg/data/voxceleb/meta/veri_test2.txt), который составлен по отношению к данным из VoxCeleb1 test set, в лабораторной работе №4.\n",
    "\n",
    "2. Инициализировать обучаемую дикторскую модель, выбрав любой возможный вариант её архитектуры, предлагаемый в рамках лабораторной работы. При реализации блока статистического пулинга предлагается выбрать либо его классический вариант, предложенный в [следующей работе](https://www.danielpovey.com/files/2018_icassp_xvectors.pdf), либо его более продвинутую версию основанную на использовании [механизмов внимания](https://arxiv.org/pdf/1803.10963.pdf). Использование последней версии статистического пулинга позволяет реализовать детектор речевой активности прямо внутри блока построения дикторских моделей.\n",
    "\n",
    "3. Инициализировать загрузчики тренировочной и валидационной выборки.\n",
    "\n",
    "4. Инициализировать оптимизатор и планировщик для выполнения процедуры обучения.\n",
    "\n",
    "5. Описать процедуру валидации/тестирования блока построения дикторских моделей.\n",
    "\n",
    "6. Описать процедуру обучения и запустить её, контролируя значения стоимостной функции и доли правильных ответов на тренировочном множестве, а также долю правильных ответов на валидационном множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3380f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select hyperparameters\n",
    "\n",
    "# Acoustic features\n",
    "n_mels            = 40                                   # number of mel filters in bank filters\n",
    "log_input         = True                                 # logarithm of features by level\n",
    "\n",
    "# Neural network archtecture\n",
    "layers            = [3, 4, 6, 3]                         # number of ResNet blocks in different level of frame level\n",
    "activation        = nn.ReLU                              # activation function used in ResNet blocks\n",
    "num_filters       = [32, 64, 128, 256]                   # number of filters of ResNet blocks in different level of frame level\n",
    "encoder_type      = 'SP'                                 # type of statistic pooling layer ('SP'  – classical statistic pooling \n",
    "                                                         # layer and 'ASP' – attentive statistic pooling)\n",
    "nOut              = 512                                  # embedding size\n",
    "\n",
    "# Loss function for angular losses\n",
    "margin            = 0.35                                 # margin parameter\n",
    "scale             = 32.0                                 # scale parameter\n",
    "\n",
    "# Train dataloader\n",
    "max_frames_train  = 200                                  # number of frame to train\n",
    "train_path        = '../data/voxceleb1_dev/wav'          # path to train wav files\n",
    "batch_size_train  = 128                                  # batch size to train\n",
    "pin_memory        = False                                # pin memory\n",
    "num_workers_train = 5                                    # number of workers to train\n",
    "shuffle           = True                                 # shuffling of training examples\n",
    "\n",
    "# Validation dataloader\n",
    "max_frames_val    = 1000                                 # number of frame to validate\n",
    "val_path          = '../data/voxceleb1_dev/wav'          # path to val wav files\n",
    "batch_size_val    = 128                                  # batch size to validate\n",
    "num_workers_val   = 5                                    # number of workers to validate\n",
    "\n",
    "# Test dataloader\n",
    "max_frames_test   = 1000                                 # number of frame to test\n",
    "test_path         = '../data/voxceleb1_dev/wav'          # path to val wav files\n",
    "batch_size_test   = 128                                  # batch size to test\n",
    "num_workers_test  = 5                                    # number of workers to test\n",
    "\n",
    "# Optimizer\n",
    "lr                = 2.5                                  # learning rate value\n",
    "weight_decay      = 0                                    # weight decay value\n",
    "\n",
    "# Scheduler\n",
    "val_interval      = 5                                    # frequency of validation step\n",
    "max_epoch         = 40                                   # number of epoches\n",
    "\n",
    "# Augmentation\n",
    "musan_path        = '../data/musan_split'                # path to splitted SLR17 dataset\n",
    "rir_path          = '../data/RIRS_NOISES/simulated_rirs' # path to SLR28 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "773bf64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data lists\n",
    "train_list = []\n",
    "val_list   = []\n",
    "test_list  = []\n",
    "\n",
    "with open('../data/voxceleb1_test/iden_split.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "black_list = os.listdir('../data/voxceleb1_test/wav')   # exclude speaker IDs from VoxCeleb1 test set\n",
    "num_train_spk = []                                      # number of train speakers\n",
    "\n",
    "for line in lines:\n",
    "    line   = line.strip().split(' ')\n",
    "    spk_id = line[1].split('/')[0]\n",
    "    \n",
    "    if not (spk_id in black_list):\n",
    "        num_train_spk.append(spk_id)\n",
    "        \n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Train list\n",
    "    if (line[0] == '1'):\n",
    "        train_list.append(' '.join([spk_id, line[1]]))\n",
    "    \n",
    "    # Validation list\n",
    "    elif (line[0] == '2'):\n",
    "        val_list.append(' '.join([spk_id, line[1]]))\n",
    "    \n",
    "    # Test list\n",
    "    elif (line[0] == '3'):\n",
    "        test_list.append(' '.join([spk_id, line[1]]))\n",
    "        \n",
    "num_train_spk = len(set(num_train_spk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e497662d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 512, encoder SP.\n",
      "Initialised AAM softmax margin 0.350 scale 32.000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danya/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GeForce GTX 1050 Ti which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/home/danya/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/home/danya/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GTX 1050 Ti with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GTX 1050 Ti GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model      = ResNet(BasicBlock, layers=layers, activation=activation, num_filters=num_filters, nOut=nOut, encoder_type=encoder_type, n_mels=n_mels, log_input=log_input)\n",
    "trainfunc  = AAMSoftmaxLoss(nOut=nOut, nClasses=num_train_spk, margin=margin, scale=scale)\n",
    "main_model = MainModel(model, trainfunc).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec6d5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataloader (without augmentation)\n",
    "train_dataset = train_dataset_loader(train_list=train_list, max_frames=max_frames_train, train_path=train_path)\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size_train, pin_memory=pin_memory, num_workers=num_workers_train, shuffle=shuffle)\n",
    "\n",
    "# Initialize validation dataloader\n",
    "val_dataset = test_dataset_loader(test_list=val_list, max_frames=max_frames_val, test_path=val_path)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size_val, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5358d236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised SGD optimizer.\n",
      "Initialised OneCycle LR scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = SGDOptimizer(main_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = OneCycleLRScheduler(optimizer, \n",
    "                                pct_start=0.30, \n",
    "                                cycle_momentum=False, \n",
    "                                max_lr=lr, \n",
    "                                div_factor=20, \n",
    "                                final_div_factor=10000, \n",
    "                                total_steps=max_epoch*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a49e81d-12d8-4e20-977c-069064fe647a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.0+cu128\n",
      "CUDA available: True\n",
      "CUDA version: 12.8\n",
      "GPU device: NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"GPU device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'No GPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef333a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danya/develop/sr_labs_book/lab3/exercises_blank.py:204: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=False):\n"
     ]
    },
    {
     "ename": "AcceleratorError",
     "evalue": "CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAcceleratorError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m num_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, max_epoch):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_loss, train_top1 = \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{:1.0f}\u001b[39;00m\u001b[33m, Loss (train set) \u001b[39m\u001b[38;5;132;01m{:f}\u001b[39;00m\u001b[33m, Accuracy (train set) \u001b[39m\u001b[38;5;132;01m{:2.3f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m.format(num_epoch, train_loss, train_top1))\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (num_epoch + \u001b[32m1\u001b[39m)%val_interval == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/lab3/exercises_blank.py:293\u001b[39m, in \u001b[36mtrain_network\u001b[39m\u001b[34m(train_loader, main_model, optimizer, scheduler, num_epoch, verbose)\u001b[39m\n\u001b[32m    290\u001b[39m optimizer.zero_grad()\n\u001b[32m    292\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m nloss, prec1 = \u001b[43mmain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m    296\u001b[39m nloss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/lab3/exercises_blank.py:255\u001b[39m, in \u001b[36mMainModel.forward\u001b[39m\u001b[34m(self, data, label)\u001b[39m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, label=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    254\u001b[39m     data = data.reshape(-\u001b[32m1\u001b[39m, data.size()[-\u001b[32m1\u001b[39m]).cuda()  \n\u001b[32m--> \u001b[39m\u001b[32m255\u001b[39m     outp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__S__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m label == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m outp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/lab3/exercises_blank.py:205\u001b[39m, in \u001b[36mResNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.cuda.amp.autocast(enabled=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m         x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtorchfb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m + \u001b[32m1e-6\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_input: x = x.log()\n\u001b[32m    209\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.instancenorm(x).unsqueeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/lab3/preproc.py:25\u001b[39m, in \u001b[36mPreEmphasis.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28minput\u001b[39m.size()) == \u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mThe number of dimensions of input tensor must be 2!\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28minput\u001b[39m.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28minput\u001b[39m = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mreflect\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m F.conv1d(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m.flipped_filter).squeeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/nn/functional.py:5294\u001b[39m, in \u001b[36mpad\u001b[39m\u001b[34m(input, pad, mode, value)\u001b[39m\n\u001b[32m   5287\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mreplicate\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   5288\u001b[39m             \u001b[38;5;66;03m# Use slow decomp whose backward will be in terms of index_put.\u001b[39;00m\n\u001b[32m   5289\u001b[39m             \u001b[38;5;66;03m# importlib is required because the import cannot be top level\u001b[39;00m\n\u001b[32m   5290\u001b[39m             \u001b[38;5;66;03m# (cycle) and cannot be nested (TS doesn't support)\u001b[39;00m\n\u001b[32m   5291\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\n\u001b[32m   5292\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtorch._decomp.decompositions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5293\u001b[39m             )._replication_pad(\u001b[38;5;28minput\u001b[39m, pad)\n\u001b[32m-> \u001b[39m\u001b[32m5294\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mAcceleratorError\u001b[39m: CUDA error: no kernel image is available for execution on the device\nSearch for `cudaErrorNoKernelImageForDevice' in https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__TYPES.html for more information.\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 0\n",
    "checkpoint_flag = False\n",
    "\n",
    "if checkpoint_flag:\n",
    "    start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models/lab3_model_0004.pth')\n",
    "    start_epoch = start_epoch + 1\n",
    "\n",
    "# Train model\n",
    "for num_epoch in range(start_epoch, max_epoch):\n",
    "    train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "    \n",
    "    print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "    if (num_epoch + 1)%val_interval == 0:\n",
    "        _, val_top1 = test_network(val_loader, main_model)\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "        saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab3_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0844d8e-ffe4-4daf-889d-b1892d4f19c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size is 512, encoder SP.\n",
      "Initialised AAM softmax margin 0.350 scale 32.000.\n",
      "Epoch 0, Batch 1, LR 0.125000 Loss 19.106623, Accuracy 0.781%\n",
      "Epoch 0, Batch 2, LR 0.125000 Loss 19.054892, Accuracy 0.391%\n",
      "Epoch 0, Batch 3, LR 0.125000 Loss 19.025270, Accuracy 0.260%\n",
      "Epoch 0, Batch 4, LR 0.125000 Loss 19.010688, Accuracy 0.195%\n",
      "Epoch 0, Batch 5, LR 0.125001 Loss 19.048732, Accuracy 0.312%\n",
      "Epoch 0, Batch 6, LR 0.125001 Loss 19.070664, Accuracy 0.260%\n",
      "Epoch 0, Batch 7, LR 0.125001 Loss 19.036075, Accuracy 0.223%\n",
      "Epoch 0, Batch 8, LR 0.125002 Loss 19.020221, Accuracy 0.195%\n",
      "Epoch 0, Batch 9, LR 0.125002 Loss 19.013136, Accuracy 0.174%\n",
      "Epoch 0, Batch 10, LR 0.125003 Loss 19.021961, Accuracy 0.156%\n",
      "Epoch 0, Batch 11, LR 0.125004 Loss 19.028703, Accuracy 0.142%\n",
      "Epoch 0, Batch 12, LR 0.125004 Loss 19.030684, Accuracy 0.130%\n",
      "Epoch 0, Batch 13, LR 0.125005 Loss 19.030754, Accuracy 0.120%\n",
      "Epoch 0, Batch 14, LR 0.125006 Loss 19.033341, Accuracy 0.112%\n",
      "Epoch 0, Batch 15, LR 0.125007 Loss 19.029070, Accuracy 0.104%\n",
      "Epoch 0, Batch 16, LR 0.125008 Loss 19.034012, Accuracy 0.098%\n",
      "Epoch 0, Batch 17, LR 0.125010 Loss 19.028852, Accuracy 0.092%\n",
      "Epoch 0, Batch 18, LR 0.125011 Loss 19.037832, Accuracy 0.087%\n",
      "Epoch 0, Batch 19, LR 0.125012 Loss 19.040022, Accuracy 0.082%\n",
      "Epoch 0, Batch 20, LR 0.125013 Loss 19.037031, Accuracy 0.078%\n",
      "Epoch 0, Batch 21, LR 0.125015 Loss 19.032030, Accuracy 0.074%\n",
      "Epoch 0, Batch 22, LR 0.125016 Loss 19.030889, Accuracy 0.071%\n",
      "Epoch 0, Batch 23, LR 0.125018 Loss 19.030972, Accuracy 0.068%\n",
      "Epoch 0, Batch 24, LR 0.125020 Loss 19.031923, Accuracy 0.065%\n",
      "Epoch 0, Batch 25, LR 0.125021 Loss 19.021904, Accuracy 0.062%\n",
      "Epoch 0, Batch 26, LR 0.125023 Loss 19.026178, Accuracy 0.060%\n",
      "Epoch 0, Batch 27, LR 0.125025 Loss 19.027234, Accuracy 0.058%\n",
      "Epoch 0, Batch 28, LR 0.125027 Loss 19.026135, Accuracy 0.056%\n",
      "Epoch 0, Batch 29, LR 0.125029 Loss 19.024598, Accuracy 0.081%\n",
      "Epoch 0, Batch 30, LR 0.125031 Loss 19.021023, Accuracy 0.078%\n",
      "Epoch 0, Batch 31, LR 0.125033 Loss 19.020556, Accuracy 0.076%\n",
      "Epoch 0, Batch 32, LR 0.125036 Loss 19.021933, Accuracy 0.073%\n",
      "Epoch 0, Batch 33, LR 0.125038 Loss 19.018262, Accuracy 0.071%\n",
      "Epoch 0, Batch 34, LR 0.125040 Loss 19.020730, Accuracy 0.069%\n",
      "Epoch 0, Batch 35, LR 0.125043 Loss 19.024723, Accuracy 0.067%\n",
      "Epoch 0, Batch 36, LR 0.125045 Loss 19.025916, Accuracy 0.065%\n",
      "Epoch 0, Batch 37, LR 0.125048 Loss 19.026844, Accuracy 0.063%\n",
      "Epoch 0, Batch 38, LR 0.125051 Loss 19.024280, Accuracy 0.062%\n",
      "Epoch 0, Batch 39, LR 0.125054 Loss 19.023834, Accuracy 0.060%\n",
      "Epoch 0, Batch 40, LR 0.125056 Loss 19.019013, Accuracy 0.078%\n",
      "Epoch 0, Batch 41, LR 0.125059 Loss 19.019120, Accuracy 0.076%\n",
      "Epoch 0, Batch 42, LR 0.125062 Loss 19.022314, Accuracy 0.074%\n",
      "Epoch 0, Batch 43, LR 0.125065 Loss 19.024459, Accuracy 0.073%\n",
      "Epoch 0, Batch 44, LR 0.125069 Loss 19.024960, Accuracy 0.071%\n",
      "Epoch 0, Batch 45, LR 0.125072 Loss 19.021832, Accuracy 0.069%\n",
      "Epoch 0, Batch 46, LR 0.125075 Loss 19.020352, Accuracy 0.068%\n",
      "Epoch 0, Batch 47, LR 0.125079 Loss 19.021984, Accuracy 0.083%\n",
      "Epoch 0, Batch 48, LR 0.125082 Loss 19.023583, Accuracy 0.098%\n",
      "Epoch 0, Batch 49, LR 0.125086 Loss 19.029068, Accuracy 0.096%\n",
      "Epoch 0, Batch 50, LR 0.125089 Loss 19.027721, Accuracy 0.094%\n",
      "Epoch 0, Batch 51, LR 0.125093 Loss 19.027087, Accuracy 0.092%\n",
      "Epoch 0, Batch 52, LR 0.125097 Loss 19.027773, Accuracy 0.090%\n",
      "Epoch 0, Batch 53, LR 0.125100 Loss 19.028261, Accuracy 0.088%\n",
      "Epoch 0, Batch 54, LR 0.125104 Loss 19.031371, Accuracy 0.087%\n",
      "Epoch 0, Batch 55, LR 0.125108 Loss 19.030460, Accuracy 0.085%\n",
      "Epoch 0, Batch 56, LR 0.125112 Loss 19.031653, Accuracy 0.084%\n",
      "Epoch 0, Batch 57, LR 0.125116 Loss 19.031555, Accuracy 0.082%\n",
      "Epoch 0, Batch 58, LR 0.125121 Loss 19.029980, Accuracy 0.081%\n",
      "Epoch 0, Batch 59, LR 0.125125 Loss 19.032100, Accuracy 0.079%\n",
      "Epoch 0, Batch 60, LR 0.125129 Loss 19.032244, Accuracy 0.091%\n",
      "Epoch 0, Batch 61, LR 0.125134 Loss 19.036133, Accuracy 0.090%\n",
      "Epoch 0, Batch 62, LR 0.125138 Loss 19.036578, Accuracy 0.088%\n",
      "Epoch 0, Batch 63, LR 0.125143 Loss 19.037105, Accuracy 0.087%\n",
      "Epoch 0, Batch 64, LR 0.125147 Loss 19.038680, Accuracy 0.085%\n",
      "Epoch 0, Batch 65, LR 0.125152 Loss 19.041392, Accuracy 0.084%\n",
      "Epoch 0, Batch 66, LR 0.125157 Loss 19.042201, Accuracy 0.083%\n",
      "Epoch 0, Batch 67, LR 0.125162 Loss 19.042756, Accuracy 0.082%\n",
      "Epoch 0, Batch 68, LR 0.125167 Loss 19.043706, Accuracy 0.080%\n",
      "Epoch 0, Batch 69, LR 0.125172 Loss 19.044206, Accuracy 0.079%\n",
      "Epoch 0, Batch 70, LR 0.125177 Loss 19.047516, Accuracy 0.078%\n",
      "Epoch 0, Batch 71, LR 0.125182 Loss 19.045996, Accuracy 0.077%\n",
      "Epoch 0, Batch 72, LR 0.125187 Loss 19.043270, Accuracy 0.076%\n",
      "Epoch 0, Batch 73, LR 0.125192 Loss 19.041936, Accuracy 0.075%\n",
      "Epoch 0, Batch 74, LR 0.125198 Loss 19.041143, Accuracy 0.074%\n",
      "Epoch 0, Batch 75, LR 0.125203 Loss 19.040278, Accuracy 0.073%\n",
      "Epoch 0, Batch 76, LR 0.125209 Loss 19.041304, Accuracy 0.072%\n",
      "Epoch 0, Batch 77, LR 0.125214 Loss 19.040389, Accuracy 0.071%\n",
      "Epoch 0, Batch 78, LR 0.125220 Loss 19.039871, Accuracy 0.070%\n",
      "Epoch 0, Batch 79, LR 0.125226 Loss 19.041350, Accuracy 0.069%\n",
      "Epoch 0, Batch 80, LR 0.125232 Loss 19.040692, Accuracy 0.068%\n",
      "Epoch 0, Batch 81, LR 0.125238 Loss 19.041946, Accuracy 0.077%\n",
      "Epoch 0, Batch 82, LR 0.125244 Loss 19.043748, Accuracy 0.076%\n",
      "Epoch 0, Batch 83, LR 0.125250 Loss 19.046339, Accuracy 0.085%\n",
      "Epoch 0, Batch 84, LR 0.125256 Loss 19.046462, Accuracy 0.084%\n",
      "Epoch 0, Batch 85, LR 0.125262 Loss 19.048022, Accuracy 0.083%\n",
      "Epoch 0, Batch 86, LR 0.125268 Loss 19.046379, Accuracy 0.082%\n",
      "Epoch 0, Batch 87, LR 0.125275 Loss 19.045577, Accuracy 0.081%\n",
      "Epoch 0, Batch 88, LR 0.125281 Loss 19.044983, Accuracy 0.080%\n",
      "Epoch 0, Batch 89, LR 0.125288 Loss 19.044493, Accuracy 0.079%\n",
      "Epoch 0, Batch 90, LR 0.125294 Loss 19.044808, Accuracy 0.078%\n",
      "Epoch 0, Batch 91, LR 0.125301 Loss 19.046759, Accuracy 0.077%\n",
      "Epoch 0, Batch 92, LR 0.125307 Loss 19.048552, Accuracy 0.076%\n",
      "Epoch 0, Batch 93, LR 0.125314 Loss 19.050869, Accuracy 0.076%\n",
      "Epoch 0, Batch 94, LR 0.125321 Loss 19.049782, Accuracy 0.075%\n",
      "Epoch 0, Batch 95, LR 0.125328 Loss 19.051398, Accuracy 0.074%\n",
      "Epoch 0, Batch 96, LR 0.125335 Loss 19.052237, Accuracy 0.073%\n",
      "Epoch 0, Batch 97, LR 0.125342 Loss 19.052459, Accuracy 0.072%\n",
      "Epoch 0, Batch 98, LR 0.125349 Loss 19.054622, Accuracy 0.072%\n",
      "Epoch 0, Batch 99, LR 0.125357 Loss 19.057619, Accuracy 0.071%\n",
      "Epoch 0, Batch 100, LR 0.125364 Loss 19.055395, Accuracy 0.070%\n",
      "Epoch 0, Batch 101, LR 0.125371 Loss 19.056884, Accuracy 0.070%\n",
      "Epoch 0, Batch 102, LR 0.125379 Loss 19.056558, Accuracy 0.069%\n",
      "Epoch 0, Batch 103, LR 0.125386 Loss 19.058277, Accuracy 0.068%\n",
      "Epoch 0, Batch 104, LR 0.125394 Loss 19.057571, Accuracy 0.068%\n",
      "Epoch 0, Batch 105, LR 0.125402 Loss 19.057525, Accuracy 0.067%\n",
      "Epoch 0, Batch 106, LR 0.125409 Loss 19.057727, Accuracy 0.066%\n",
      "Epoch 0, Batch 107, LR 0.125417 Loss 19.057077, Accuracy 0.066%\n",
      "Epoch 0, Batch 108, LR 0.125425 Loss 19.056447, Accuracy 0.065%\n",
      "Epoch 0, Batch 109, LR 0.125433 Loss 19.055275, Accuracy 0.065%\n",
      "Epoch 0, Batch 110, LR 0.125441 Loss 19.055789, Accuracy 0.064%\n",
      "Epoch 0, Batch 111, LR 0.125449 Loss 19.056548, Accuracy 0.070%\n",
      "Epoch 0, Batch 112, LR 0.125457 Loss 19.055803, Accuracy 0.070%\n",
      "Epoch 0, Batch 113, LR 0.125466 Loss 19.054468, Accuracy 0.069%\n",
      "Epoch 0, Batch 114, LR 0.125474 Loss 19.053380, Accuracy 0.075%\n",
      "Epoch 0, Batch 115, LR 0.125482 Loss 19.052646, Accuracy 0.075%\n",
      "Epoch 0, Batch 116, LR 0.125491 Loss 19.051326, Accuracy 0.074%\n",
      "Epoch 0, Batch 117, LR 0.125500 Loss 19.053850, Accuracy 0.073%\n",
      "Epoch 0, Batch 118, LR 0.125508 Loss 19.052193, Accuracy 0.073%\n",
      "Epoch 0, Batch 119, LR 0.125517 Loss 19.051401, Accuracy 0.079%\n",
      "Epoch 0, Batch 120, LR 0.125526 Loss 19.052383, Accuracy 0.078%\n",
      "Epoch 0, Batch 121, LR 0.125535 Loss 19.051661, Accuracy 0.077%\n",
      "Epoch 0, Batch 122, LR 0.125544 Loss 19.049985, Accuracy 0.077%\n",
      "Epoch 0, Batch 123, LR 0.125553 Loss 19.049862, Accuracy 0.076%\n",
      "Epoch 0, Batch 124, LR 0.125562 Loss 19.048884, Accuracy 0.076%\n",
      "Epoch 0, Batch 125, LR 0.125571 Loss 19.049621, Accuracy 0.075%\n",
      "Epoch 0, Batch 126, LR 0.125580 Loss 19.050296, Accuracy 0.074%\n",
      "Epoch 0, Batch 127, LR 0.125589 Loss 19.049844, Accuracy 0.074%\n",
      "Epoch 0, Batch 128, LR 0.125599 Loss 19.050864, Accuracy 0.073%\n",
      "Epoch 0, Batch 129, LR 0.125608 Loss 19.051149, Accuracy 0.079%\n",
      "Epoch 0, Batch 130, LR 0.125618 Loss 19.050107, Accuracy 0.078%\n",
      "Epoch 0, Batch 131, LR 0.125627 Loss 19.050774, Accuracy 0.078%\n",
      "Epoch 0, Batch 132, LR 0.125637 Loss 19.051695, Accuracy 0.083%\n",
      "Epoch 0, Batch 133, LR 0.125647 Loss 19.050948, Accuracy 0.082%\n",
      "Epoch 0, Batch 134, LR 0.125657 Loss 19.051111, Accuracy 0.082%\n",
      "Epoch 0, Batch 135, LR 0.125667 Loss 19.052098, Accuracy 0.081%\n",
      "Epoch 0, Batch 136, LR 0.125677 Loss 19.052083, Accuracy 0.080%\n",
      "Epoch 0, Batch 137, LR 0.125687 Loss 19.051295, Accuracy 0.080%\n",
      "Epoch 0, Batch 138, LR 0.125697 Loss 19.051259, Accuracy 0.079%\n",
      "Epoch 0, Batch 139, LR 0.125707 Loss 19.050767, Accuracy 0.079%\n",
      "Epoch 0, Batch 140, LR 0.125717 Loss 19.052308, Accuracy 0.078%\n",
      "Epoch 0, Batch 141, LR 0.125728 Loss 19.052865, Accuracy 0.078%\n",
      "Epoch 0, Batch 142, LR 0.125738 Loss 19.054397, Accuracy 0.077%\n",
      "Epoch 0, Batch 143, LR 0.125749 Loss 19.053638, Accuracy 0.076%\n",
      "Epoch 0, Batch 144, LR 0.125759 Loss 19.052969, Accuracy 0.076%\n",
      "Epoch 0, Batch 145, LR 0.125770 Loss 19.052071, Accuracy 0.075%\n",
      "Epoch 0, Batch 146, LR 0.125781 Loss 19.052241, Accuracy 0.075%\n",
      "Epoch 0, Batch 147, LR 0.125791 Loss 19.051362, Accuracy 0.074%\n",
      "Epoch 0, Batch 148, LR 0.125802 Loss 19.052187, Accuracy 0.074%\n",
      "Epoch 0, Batch 149, LR 0.125813 Loss 19.051673, Accuracy 0.079%\n",
      "Epoch 0, Batch 150, LR 0.125824 Loss 19.051105, Accuracy 0.078%\n",
      "Epoch 0, Batch 151, LR 0.125835 Loss 19.050293, Accuracy 0.078%\n",
      "Epoch 0, Batch 152, LR 0.125846 Loss 19.049071, Accuracy 0.077%\n",
      "Epoch 0, Batch 153, LR 0.125858 Loss 19.048798, Accuracy 0.077%\n",
      "Epoch 0, Batch 154, LR 0.125869 Loss 19.048118, Accuracy 0.076%\n",
      "Epoch 0, Batch 155, LR 0.125880 Loss 19.048236, Accuracy 0.076%\n",
      "Epoch 0, Batch 156, LR 0.125892 Loss 19.047928, Accuracy 0.080%\n",
      "Epoch 0, Batch 157, LR 0.125903 Loss 19.048405, Accuracy 0.080%\n",
      "Epoch 0, Batch 158, LR 0.125915 Loss 19.048339, Accuracy 0.084%\n",
      "Epoch 0, Batch 159, LR 0.125927 Loss 19.048897, Accuracy 0.084%\n",
      "Epoch 0, Batch 160, LR 0.125939 Loss 19.050167, Accuracy 0.083%\n",
      "Epoch 0, Batch 161, LR 0.125950 Loss 19.050322, Accuracy 0.082%\n",
      "Epoch 0, Batch 162, LR 0.125962 Loss 19.050579, Accuracy 0.082%\n",
      "Epoch 0, Batch 163, LR 0.125974 Loss 19.050899, Accuracy 0.081%\n",
      "Epoch 0, Batch 164, LR 0.125986 Loss 19.050820, Accuracy 0.081%\n",
      "Epoch 0, Batch 165, LR 0.125998 Loss 19.049328, Accuracy 0.080%\n",
      "Epoch 0, Batch 166, LR 0.126011 Loss 19.049248, Accuracy 0.080%\n",
      "Epoch 0, Batch 167, LR 0.126023 Loss 19.049357, Accuracy 0.080%\n",
      "Epoch 0, Batch 168, LR 0.126035 Loss 19.050286, Accuracy 0.079%\n",
      "Epoch 0, Batch 169, LR 0.126048 Loss 19.050031, Accuracy 0.083%\n",
      "Epoch 0, Batch 170, LR 0.126060 Loss 19.049375, Accuracy 0.083%\n",
      "Epoch 0, Batch 171, LR 0.126073 Loss 19.049973, Accuracy 0.082%\n",
      "Epoch 0, Batch 172, LR 0.126086 Loss 19.049188, Accuracy 0.082%\n",
      "Epoch 0, Batch 173, LR 0.126098 Loss 19.049279, Accuracy 0.081%\n",
      "Epoch 0, Batch 174, LR 0.126111 Loss 19.050110, Accuracy 0.081%\n",
      "Epoch 0, Batch 175, LR 0.126124 Loss 19.050454, Accuracy 0.080%\n",
      "Epoch 0, Batch 176, LR 0.126137 Loss 19.050049, Accuracy 0.080%\n",
      "Epoch 0, Batch 177, LR 0.126150 Loss 19.049325, Accuracy 0.079%\n",
      "Epoch 0, Batch 178, LR 0.126163 Loss 19.049666, Accuracy 0.079%\n",
      "Epoch 0, Batch 179, LR 0.126176 Loss 19.049774, Accuracy 0.079%\n",
      "Epoch 0, Batch 180, LR 0.126189 Loss 19.051452, Accuracy 0.078%\n",
      "Epoch 0, Batch 181, LR 0.126203 Loss 19.053335, Accuracy 0.078%\n",
      "Epoch 0, Batch 182, LR 0.126216 Loss 19.053287, Accuracy 0.077%\n",
      "Epoch 0, Batch 183, LR 0.126230 Loss 19.053273, Accuracy 0.077%\n",
      "Epoch 0, Batch 184, LR 0.126243 Loss 19.052858, Accuracy 0.076%\n",
      "Epoch 0, Batch 185, LR 0.126257 Loss 19.053001, Accuracy 0.076%\n",
      "Epoch 0, Batch 186, LR 0.126271 Loss 19.054714, Accuracy 0.076%\n",
      "Epoch 0, Batch 187, LR 0.126284 Loss 19.054432, Accuracy 0.075%\n",
      "Epoch 0, Batch 188, LR 0.126298 Loss 19.055547, Accuracy 0.075%\n",
      "Epoch 0, Batch 189, LR 0.126312 Loss 19.056012, Accuracy 0.079%\n",
      "Epoch 0, Batch 190, LR 0.126326 Loss 19.056570, Accuracy 0.078%\n",
      "Epoch 0, Batch 191, LR 0.126340 Loss 19.057531, Accuracy 0.078%\n",
      "Epoch 0, Batch 192, LR 0.126354 Loss 19.056688, Accuracy 0.077%\n",
      "Epoch 0, Batch 193, LR 0.126368 Loss 19.058473, Accuracy 0.077%\n",
      "Epoch 0, Batch 194, LR 0.126383 Loss 19.059143, Accuracy 0.077%\n",
      "Epoch 0, Batch 195, LR 0.126397 Loss 19.060630, Accuracy 0.076%\n",
      "Epoch 0, Batch 196, LR 0.126412 Loss 19.060545, Accuracy 0.076%\n",
      "Epoch 0, Batch 197, LR 0.126426 Loss 19.060969, Accuracy 0.075%\n",
      "Epoch 0, Batch 198, LR 0.126441 Loss 19.062085, Accuracy 0.075%\n",
      "Epoch 0, Batch 199, LR 0.126455 Loss 19.062588, Accuracy 0.075%\n",
      "Epoch 0, Batch 200, LR 0.126470 Loss 19.063084, Accuracy 0.074%\n",
      "Epoch 0, Batch 201, LR 0.126485 Loss 19.062654, Accuracy 0.078%\n",
      "Epoch 0, Batch 202, LR 0.126500 Loss 19.063066, Accuracy 0.077%\n",
      "Epoch 0, Batch 203, LR 0.126515 Loss 19.063415, Accuracy 0.077%\n",
      "Epoch 0, Batch 204, LR 0.126530 Loss 19.063391, Accuracy 0.077%\n",
      "Epoch 0, Batch 205, LR 0.126545 Loss 19.063404, Accuracy 0.076%\n",
      "Epoch 0, Batch 206, LR 0.126560 Loss 19.062951, Accuracy 0.076%\n",
      "Epoch 0, Batch 207, LR 0.126575 Loss 19.062404, Accuracy 0.075%\n",
      "Epoch 0, Batch 208, LR 0.126591 Loss 19.062570, Accuracy 0.075%\n",
      "Epoch 0, Batch 209, LR 0.126606 Loss 19.062726, Accuracy 0.075%\n",
      "Epoch 0, Batch 210, LR 0.126621 Loss 19.063855, Accuracy 0.074%\n",
      "Epoch 0, Batch 211, LR 0.126637 Loss 19.063684, Accuracy 0.078%\n",
      "Epoch 0, Batch 212, LR 0.126653 Loss 19.063232, Accuracy 0.077%\n",
      "Epoch 0, Batch 213, LR 0.126668 Loss 19.062595, Accuracy 0.077%\n",
      "Epoch 0, Batch 214, LR 0.126684 Loss 19.061822, Accuracy 0.077%\n",
      "Epoch 0, Batch 215, LR 0.126700 Loss 19.063284, Accuracy 0.076%\n",
      "Epoch 0, Batch 216, LR 0.126716 Loss 19.062690, Accuracy 0.076%\n",
      "Epoch 0, Batch 217, LR 0.126732 Loss 19.063695, Accuracy 0.076%\n",
      "Epoch 0, Batch 218, LR 0.126748 Loss 19.063845, Accuracy 0.075%\n",
      "Epoch 0, Batch 219, LR 0.126764 Loss 19.064142, Accuracy 0.075%\n",
      "Epoch 0, Batch 220, LR 0.126780 Loss 19.064656, Accuracy 0.075%\n",
      "Epoch 0, Batch 221, LR 0.126797 Loss 19.064447, Accuracy 0.074%\n",
      "Epoch 0, Batch 222, LR 0.126813 Loss 19.064167, Accuracy 0.074%\n",
      "Epoch 0, Batch 223, LR 0.126829 Loss 19.064494, Accuracy 0.074%\n",
      "Epoch 0, Batch 224, LR 0.126846 Loss 19.064608, Accuracy 0.077%\n",
      "Epoch 0, Batch 225, LR 0.126863 Loss 19.065186, Accuracy 0.076%\n",
      "Epoch 0, Batch 226, LR 0.126879 Loss 19.064806, Accuracy 0.080%\n",
      "Epoch 0, Batch 227, LR 0.126896 Loss 19.064312, Accuracy 0.079%\n",
      "Epoch 0, Batch 228, LR 0.126913 Loss 19.065335, Accuracy 0.079%\n",
      "Epoch 0, Batch 229, LR 0.126930 Loss 19.064463, Accuracy 0.078%\n",
      "Epoch 0, Batch 230, LR 0.126947 Loss 19.064832, Accuracy 0.078%\n",
      "Epoch 0, Batch 231, LR 0.126964 Loss 19.064607, Accuracy 0.078%\n",
      "Epoch 0, Batch 232, LR 0.126981 Loss 19.064822, Accuracy 0.081%\n",
      "Epoch 0, Batch 233, LR 0.126998 Loss 19.064569, Accuracy 0.080%\n",
      "Epoch 0, Batch 234, LR 0.127015 Loss 19.065067, Accuracy 0.080%\n",
      "Epoch 0, Batch 235, LR 0.127032 Loss 19.064621, Accuracy 0.080%\n",
      "Epoch 0, Batch 236, LR 0.127050 Loss 19.064115, Accuracy 0.079%\n",
      "Epoch 0, Batch 237, LR 0.127067 Loss 19.064172, Accuracy 0.079%\n",
      "Epoch 0, Batch 238, LR 0.127085 Loss 19.064144, Accuracy 0.079%\n",
      "Epoch 0, Batch 239, LR 0.127103 Loss 19.064171, Accuracy 0.078%\n",
      "Epoch 0, Batch 240, LR 0.127120 Loss 19.063594, Accuracy 0.078%\n",
      "Epoch 0, Batch 241, LR 0.127138 Loss 19.063675, Accuracy 0.078%\n",
      "Epoch 0, Batch 242, LR 0.127156 Loss 19.063985, Accuracy 0.077%\n",
      "Epoch 0, Batch 243, LR 0.127174 Loss 19.063774, Accuracy 0.080%\n",
      "Epoch 0, Batch 244, LR 0.127192 Loss 19.063578, Accuracy 0.083%\n",
      "Epoch 0, Batch 245, LR 0.127210 Loss 19.063479, Accuracy 0.083%\n",
      "Epoch 0, Batch 246, LR 0.127228 Loss 19.063791, Accuracy 0.083%\n",
      "Epoch 0, Batch 247, LR 0.127246 Loss 19.063902, Accuracy 0.082%\n",
      "Epoch 0, Batch 248, LR 0.127265 Loss 19.063265, Accuracy 0.082%\n",
      "Epoch 0, Batch 249, LR 0.127283 Loss 19.063359, Accuracy 0.082%\n",
      "Epoch 0, Batch 250, LR 0.127301 Loss 19.063450, Accuracy 0.081%\n",
      "Epoch 0, Batch 251, LR 0.127320 Loss 19.063863, Accuracy 0.081%\n",
      "Epoch 0, Batch 252, LR 0.127338 Loss 19.063860, Accuracy 0.081%\n",
      "Epoch 0, Batch 253, LR 0.127357 Loss 19.064455, Accuracy 0.083%\n",
      "Epoch 0, Batch 254, LR 0.127376 Loss 19.064595, Accuracy 0.083%\n",
      "Epoch 0, Batch 255, LR 0.127395 Loss 19.064083, Accuracy 0.083%\n",
      "Epoch 0, Batch 256, LR 0.127414 Loss 19.063454, Accuracy 0.085%\n",
      "Epoch 0, Batch 257, LR 0.127432 Loss 19.064069, Accuracy 0.085%\n",
      "Epoch 0, Batch 258, LR 0.127452 Loss 19.064433, Accuracy 0.088%\n",
      "Epoch 0, Batch 259, LR 0.127471 Loss 19.064810, Accuracy 0.087%\n",
      "Epoch 0, Batch 260, LR 0.127490 Loss 19.065301, Accuracy 0.087%\n",
      "Epoch 0, Batch 261, LR 0.127509 Loss 19.065606, Accuracy 0.087%\n",
      "Epoch 0, Batch 262, LR 0.127528 Loss 19.064650, Accuracy 0.086%\n",
      "Epoch 0, Batch 263, LR 0.127548 Loss 19.064669, Accuracy 0.086%\n",
      "Epoch 0, Batch 264, LR 0.127567 Loss 19.064342, Accuracy 0.086%\n",
      "Epoch 0, Batch 265, LR 0.127587 Loss 19.063711, Accuracy 0.088%\n",
      "Epoch 0, Batch 266, LR 0.127606 Loss 19.063513, Accuracy 0.088%\n",
      "Epoch 0, Batch 267, LR 0.127626 Loss 19.063919, Accuracy 0.088%\n",
      "Epoch 0, Batch 268, LR 0.127646 Loss 19.064341, Accuracy 0.087%\n",
      "Epoch 0, Batch 269, LR 0.127666 Loss 19.065232, Accuracy 0.087%\n",
      "Epoch 0, Batch 270, LR 0.127686 Loss 19.064610, Accuracy 0.087%\n",
      "Epoch 0, Batch 271, LR 0.127706 Loss 19.064613, Accuracy 0.089%\n",
      "Epoch 0, Batch 272, LR 0.127726 Loss 19.064367, Accuracy 0.089%\n",
      "Epoch 0, Batch 273, LR 0.127746 Loss 19.064073, Accuracy 0.089%\n",
      "Epoch 0, Batch 274, LR 0.127766 Loss 19.064870, Accuracy 0.088%\n",
      "Epoch 0, Batch 275, LR 0.127786 Loss 19.064530, Accuracy 0.091%\n",
      "Epoch 0, Batch 276, LR 0.127807 Loss 19.064113, Accuracy 0.091%\n",
      "Epoch 0, Batch 277, LR 0.127827 Loss 19.063929, Accuracy 0.090%\n",
      "Epoch 0, Batch 278, LR 0.127848 Loss 19.064901, Accuracy 0.090%\n",
      "Epoch 0, Batch 279, LR 0.127868 Loss 19.065044, Accuracy 0.090%\n",
      "Epoch 0, Batch 280, LR 0.127889 Loss 19.064728, Accuracy 0.089%\n",
      "Epoch 0, Batch 281, LR 0.127910 Loss 19.064486, Accuracy 0.089%\n",
      "Epoch 0, Batch 282, LR 0.127931 Loss 19.064738, Accuracy 0.089%\n",
      "Epoch 0, Batch 283, LR 0.127951 Loss 19.064376, Accuracy 0.088%\n",
      "Epoch 0, Batch 284, LR 0.127972 Loss 19.064892, Accuracy 0.088%\n",
      "Epoch 0, Batch 285, LR 0.127993 Loss 19.065483, Accuracy 0.088%\n",
      "Epoch 0, Batch 286, LR 0.128015 Loss 19.065681, Accuracy 0.087%\n",
      "Epoch 0, Batch 287, LR 0.128036 Loss 19.064524, Accuracy 0.087%\n",
      "Epoch 0, Batch 288, LR 0.128057 Loss 19.064200, Accuracy 0.087%\n",
      "Epoch 0, Batch 289, LR 0.128078 Loss 19.064107, Accuracy 0.087%\n",
      "Epoch 0, Batch 290, LR 0.128100 Loss 19.063446, Accuracy 0.086%\n",
      "Epoch 0, Batch 291, LR 0.128121 Loss 19.063112, Accuracy 0.086%\n",
      "Epoch 0, Batch 292, LR 0.128143 Loss 19.062650, Accuracy 0.086%\n",
      "Epoch 0, Batch 293, LR 0.128164 Loss 19.062810, Accuracy 0.085%\n",
      "Epoch 0, Batch 294, LR 0.128186 Loss 19.063490, Accuracy 0.085%\n",
      "Epoch 0, Batch 295, LR 0.128208 Loss 19.063998, Accuracy 0.085%\n",
      "Epoch 0, Batch 296, LR 0.128230 Loss 19.064437, Accuracy 0.084%\n",
      "Epoch 0, Batch 297, LR 0.128252 Loss 19.064714, Accuracy 0.084%\n",
      "Epoch 0, Batch 298, LR 0.128274 Loss 19.065205, Accuracy 0.084%\n",
      "Epoch 0, Batch 299, LR 0.128296 Loss 19.065806, Accuracy 0.084%\n",
      "Epoch 0, Batch 300, LR 0.128318 Loss 19.065545, Accuracy 0.083%\n",
      "Epoch 0, Batch 301, LR 0.128340 Loss 19.065691, Accuracy 0.083%\n",
      "Epoch 0, Batch 302, LR 0.128362 Loss 19.065372, Accuracy 0.083%\n",
      "Epoch 0, Batch 303, LR 0.128385 Loss 19.065336, Accuracy 0.083%\n",
      "Epoch 0, Batch 304, LR 0.128407 Loss 19.065052, Accuracy 0.082%\n",
      "Epoch 0, Batch 305, LR 0.128430 Loss 19.065649, Accuracy 0.082%\n",
      "Epoch 0, Batch 306, LR 0.128452 Loss 19.065414, Accuracy 0.082%\n",
      "Epoch 0, Batch 307, LR 0.128475 Loss 19.065582, Accuracy 0.081%\n",
      "Epoch 0, Batch 308, LR 0.128498 Loss 19.066030, Accuracy 0.081%\n",
      "Epoch 0, Batch 309, LR 0.128520 Loss 19.066796, Accuracy 0.083%\n",
      "Epoch 0, Batch 310, LR 0.128543 Loss 19.066679, Accuracy 0.083%\n",
      "Epoch 0, Batch 311, LR 0.128566 Loss 19.066566, Accuracy 0.083%\n",
      "Epoch 0, Batch 312, LR 0.128589 Loss 19.066697, Accuracy 0.083%\n",
      "Epoch 0, Batch 313, LR 0.128612 Loss 19.066370, Accuracy 0.082%\n",
      "Epoch 0, Batch 314, LR 0.128636 Loss 19.066147, Accuracy 0.082%\n",
      "Epoch 0, Batch 315, LR 0.128659 Loss 19.066756, Accuracy 0.082%\n",
      "Epoch 0, Batch 316, LR 0.128682 Loss 19.066426, Accuracy 0.082%\n",
      "Epoch 0, Batch 317, LR 0.128706 Loss 19.066458, Accuracy 0.081%\n",
      "Epoch 0, Batch 318, LR 0.128729 Loss 19.066143, Accuracy 0.081%\n",
      "Epoch 0, Batch 319, LR 0.128753 Loss 19.066159, Accuracy 0.081%\n",
      "Epoch 0, Batch 320, LR 0.128776 Loss 19.065544, Accuracy 0.081%\n",
      "Epoch 0, Batch 321, LR 0.128800 Loss 19.064807, Accuracy 0.080%\n",
      "Epoch 0, Batch 322, LR 0.128824 Loss 19.064237, Accuracy 0.080%\n",
      "Epoch 0, Batch 323, LR 0.128848 Loss 19.064555, Accuracy 0.082%\n",
      "Epoch 0, Batch 324, LR 0.128872 Loss 19.063900, Accuracy 0.084%\n",
      "Epoch 0, Batch 325, LR 0.128896 Loss 19.063362, Accuracy 0.084%\n",
      "Epoch 0, Batch 326, LR 0.128920 Loss 19.063042, Accuracy 0.086%\n",
      "Epoch 0, Batch 327, LR 0.128944 Loss 19.062545, Accuracy 0.086%\n",
      "Epoch 0, Batch 328, LR 0.128968 Loss 19.062194, Accuracy 0.086%\n",
      "Epoch 0, Batch 329, LR 0.128992 Loss 19.062267, Accuracy 0.085%\n",
      "Epoch 0, Batch 330, LR 0.129017 Loss 19.062289, Accuracy 0.085%\n",
      "Epoch 0, Batch 331, LR 0.129041 Loss 19.061812, Accuracy 0.085%\n",
      "Epoch 0, Batch 332, LR 0.129066 Loss 19.061795, Accuracy 0.085%\n",
      "Epoch 0, Batch 333, LR 0.129090 Loss 19.062044, Accuracy 0.084%\n",
      "Epoch 0, Batch 334, LR 0.129115 Loss 19.061327, Accuracy 0.084%\n",
      "Epoch 0, Batch 335, LR 0.129140 Loss 19.061492, Accuracy 0.084%\n",
      "Epoch 0, Batch 336, LR 0.129164 Loss 19.061967, Accuracy 0.084%\n",
      "Epoch 0, Batch 337, LR 0.129189 Loss 19.062280, Accuracy 0.083%\n",
      "Epoch 0, Batch 338, LR 0.129214 Loss 19.062371, Accuracy 0.083%\n",
      "Epoch 0, Batch 339, LR 0.129239 Loss 19.062186, Accuracy 0.083%\n",
      "Epoch 0, Batch 340, LR 0.129264 Loss 19.062397, Accuracy 0.083%\n",
      "Epoch 0, Batch 341, LR 0.129290 Loss 19.062812, Accuracy 0.082%\n",
      "Epoch 0, Batch 342, LR 0.129315 Loss 19.062350, Accuracy 0.082%\n",
      "Epoch 0, Batch 343, LR 0.129340 Loss 19.062368, Accuracy 0.082%\n",
      "Epoch 0, Batch 344, LR 0.129366 Loss 19.061834, Accuracy 0.084%\n",
      "Epoch 0, Batch 345, LR 0.129391 Loss 19.061549, Accuracy 0.084%\n",
      "Epoch 0, Batch 346, LR 0.129417 Loss 19.061357, Accuracy 0.084%\n",
      "Epoch 0, Batch 347, LR 0.129442 Loss 19.060590, Accuracy 0.083%\n",
      "Epoch 0, Batch 348, LR 0.129468 Loss 19.061048, Accuracy 0.083%\n",
      "Epoch 0, Batch 349, LR 0.129494 Loss 19.061362, Accuracy 0.083%\n",
      "Epoch 0, Batch 350, LR 0.129520 Loss 19.061282, Accuracy 0.083%\n",
      "Epoch 0, Batch 351, LR 0.129545 Loss 19.061522, Accuracy 0.085%\n",
      "Epoch 0, Batch 352, LR 0.129571 Loss 19.062002, Accuracy 0.084%\n",
      "Epoch 0, Batch 353, LR 0.129597 Loss 19.062523, Accuracy 0.084%\n",
      "Epoch 0, Batch 354, LR 0.129624 Loss 19.062792, Accuracy 0.084%\n",
      "Epoch 0, Batch 355, LR 0.129650 Loss 19.062644, Accuracy 0.086%\n",
      "Epoch 0, Batch 356, LR 0.129676 Loss 19.062524, Accuracy 0.086%\n",
      "Epoch 0, Batch 357, LR 0.129703 Loss 19.062296, Accuracy 0.085%\n",
      "Epoch 0, Batch 358, LR 0.129729 Loss 19.062531, Accuracy 0.087%\n",
      "Epoch 0, Batch 359, LR 0.129755 Loss 19.062656, Accuracy 0.087%\n",
      "Epoch 0, Batch 360, LR 0.129782 Loss 19.062915, Accuracy 0.087%\n",
      "Epoch 0, Batch 361, LR 0.129809 Loss 19.063269, Accuracy 0.087%\n",
      "Epoch 0, Batch 362, LR 0.129835 Loss 19.063090, Accuracy 0.086%\n",
      "Epoch 0, Batch 363, LR 0.129862 Loss 19.062500, Accuracy 0.086%\n",
      "Epoch 0, Batch 364, LR 0.129889 Loss 19.062890, Accuracy 0.086%\n",
      "Epoch 0, Batch 365, LR 0.129916 Loss 19.063442, Accuracy 0.086%\n",
      "Epoch 0, Batch 366, LR 0.129943 Loss 19.063289, Accuracy 0.085%\n",
      "Epoch 0, Batch 367, LR 0.129970 Loss 19.062957, Accuracy 0.085%\n",
      "Epoch 0, Batch 368, LR 0.129997 Loss 19.062399, Accuracy 0.085%\n",
      "Epoch 0, Batch 369, LR 0.130025 Loss 19.062076, Accuracy 0.085%\n",
      "Epoch 0, Batch 370, LR 0.130052 Loss 19.061889, Accuracy 0.084%\n",
      "Epoch 0, Batch 371, LR 0.130079 Loss 19.061818, Accuracy 0.084%\n",
      "Epoch 0, Batch 372, LR 0.130107 Loss 19.061635, Accuracy 0.084%\n",
      "Epoch 0, Batch 373, LR 0.130134 Loss 19.061244, Accuracy 0.084%\n",
      "Epoch 0, Batch 374, LR 0.130162 Loss 19.061187, Accuracy 0.084%\n",
      "Epoch 0, Batch 375, LR 0.130190 Loss 19.060859, Accuracy 0.083%\n",
      "Epoch 0, Batch 376, LR 0.130217 Loss 19.060876, Accuracy 0.083%\n",
      "Epoch 0, Batch 377, LR 0.130245 Loss 19.061095, Accuracy 0.083%\n",
      "Epoch 0, Batch 378, LR 0.130273 Loss 19.061195, Accuracy 0.083%\n",
      "Epoch 0, Batch 379, LR 0.130301 Loss 19.061811, Accuracy 0.082%\n",
      "Epoch 0, Batch 380, LR 0.130329 Loss 19.062212, Accuracy 0.082%\n",
      "Epoch 0, Batch 381, LR 0.130357 Loss 19.061972, Accuracy 0.082%\n",
      "Epoch 0, Batch 382, LR 0.130386 Loss 19.062537, Accuracy 0.082%\n",
      "Epoch 0, Batch 383, LR 0.130414 Loss 19.062310, Accuracy 0.082%\n",
      "Epoch 0, Batch 384, LR 0.130442 Loss 19.062051, Accuracy 0.081%\n",
      "Epoch 0, Batch 385, LR 0.130471 Loss 19.061721, Accuracy 0.083%\n",
      "Epoch 0, Batch 386, LR 0.130499 Loss 19.061498, Accuracy 0.083%\n",
      "Epoch 0, Batch 387, LR 0.130528 Loss 19.061388, Accuracy 0.083%\n",
      "Epoch 0, Batch 388, LR 0.130556 Loss 19.061552, Accuracy 0.085%\n",
      "Epoch 0, Batch 389, LR 0.130585 Loss 19.061744, Accuracy 0.084%\n",
      "Epoch 0, Batch 390, LR 0.130614 Loss 19.062029, Accuracy 0.084%\n",
      "Epoch 0, Batch 391, LR 0.130643 Loss 19.061643, Accuracy 0.084%\n",
      "Epoch 0, Batch 392, LR 0.130672 Loss 19.061724, Accuracy 0.084%\n",
      "Epoch 0, Batch 393, LR 0.130701 Loss 19.061687, Accuracy 0.083%\n",
      "Epoch 0, Batch 394, LR 0.130730 Loss 19.061335, Accuracy 0.083%\n",
      "Epoch 0, Batch 395, LR 0.130759 Loss 19.061179, Accuracy 0.083%\n",
      "Epoch 0, Batch 396, LR 0.130788 Loss 19.061075, Accuracy 0.083%\n",
      "Epoch 0, Batch 397, LR 0.130818 Loss 19.061121, Accuracy 0.085%\n",
      "Epoch 0, Batch 398, LR 0.130847 Loss 19.061203, Accuracy 0.084%\n",
      "Epoch 0, Batch 399, LR 0.130877 Loss 19.061286, Accuracy 0.084%\n",
      "Epoch 0, Batch 400, LR 0.130906 Loss 19.061531, Accuracy 0.086%\n",
      "Epoch 0, Batch 401, LR 0.130936 Loss 19.061571, Accuracy 0.086%\n",
      "Epoch 0, Batch 402, LR 0.130965 Loss 19.061428, Accuracy 0.086%\n",
      "Epoch 0, Batch 403, LR 0.130995 Loss 19.061309, Accuracy 0.085%\n",
      "Epoch 0, Batch 404, LR 0.131025 Loss 19.061256, Accuracy 0.085%\n",
      "Epoch 0, Batch 405, LR 0.131055 Loss 19.061166, Accuracy 0.085%\n",
      "Epoch 0, Batch 406, LR 0.131085 Loss 19.060946, Accuracy 0.087%\n",
      "Epoch 0, Batch 407, LR 0.131115 Loss 19.061192, Accuracy 0.086%\n",
      "Epoch 0, Batch 408, LR 0.131145 Loss 19.061637, Accuracy 0.086%\n",
      "Epoch 0, Batch 409, LR 0.131175 Loss 19.061536, Accuracy 0.086%\n",
      "Epoch 0, Batch 410, LR 0.131206 Loss 19.061812, Accuracy 0.086%\n",
      "Epoch 0, Batch 411, LR 0.131236 Loss 19.062293, Accuracy 0.086%\n",
      "Epoch 0, Batch 412, LR 0.131266 Loss 19.062229, Accuracy 0.085%\n",
      "Epoch 0, Batch 413, LR 0.131297 Loss 19.061977, Accuracy 0.085%\n",
      "Epoch 0, Batch 414, LR 0.131327 Loss 19.062341, Accuracy 0.085%\n",
      "Epoch 0, Batch 415, LR 0.131358 Loss 19.061933, Accuracy 0.085%\n",
      "Epoch 0, Batch 416, LR 0.131389 Loss 19.061916, Accuracy 0.085%\n",
      "Epoch 0, Batch 417, LR 0.131420 Loss 19.062066, Accuracy 0.084%\n",
      "Epoch 0, Batch 418, LR 0.131451 Loss 19.061785, Accuracy 0.084%\n",
      "Epoch 0, Batch 419, LR 0.131481 Loss 19.061955, Accuracy 0.084%\n",
      "Epoch 0, Batch 420, LR 0.131512 Loss 19.062184, Accuracy 0.084%\n",
      "Epoch 0, Batch 421, LR 0.131544 Loss 19.061850, Accuracy 0.084%\n",
      "Epoch 0, Batch 422, LR 0.131575 Loss 19.062170, Accuracy 0.083%\n",
      "Epoch 0, Batch 423, LR 0.131606 Loss 19.062242, Accuracy 0.083%\n",
      "Epoch 0, Batch 424, LR 0.131637 Loss 19.061881, Accuracy 0.083%\n",
      "Epoch 0, Batch 425, LR 0.131669 Loss 19.062551, Accuracy 0.083%\n",
      "Epoch 0, Batch 426, LR 0.131700 Loss 19.062497, Accuracy 0.083%\n",
      "Epoch 0, Batch 427, LR 0.131732 Loss 19.062872, Accuracy 0.082%\n",
      "Epoch 0, Batch 428, LR 0.131763 Loss 19.063039, Accuracy 0.082%\n",
      "Epoch 0, Batch 429, LR 0.131795 Loss 19.063014, Accuracy 0.082%\n",
      "Epoch 0, Batch 430, LR 0.131827 Loss 19.063010, Accuracy 0.082%\n",
      "Epoch 0, Batch 431, LR 0.131859 Loss 19.063097, Accuracy 0.082%\n",
      "Epoch 0, Batch 432, LR 0.131891 Loss 19.063220, Accuracy 0.081%\n",
      "Epoch 0, Batch 433, LR 0.131922 Loss 19.063598, Accuracy 0.081%\n",
      "Epoch 0, Batch 434, LR 0.131955 Loss 19.063343, Accuracy 0.081%\n",
      "Epoch 0, Batch 435, LR 0.131987 Loss 19.063493, Accuracy 0.081%\n",
      "Epoch 0, Batch 436, LR 0.132019 Loss 19.063494, Accuracy 0.081%\n",
      "Epoch 0, Batch 437, LR 0.132051 Loss 19.063238, Accuracy 0.080%\n",
      "Epoch 0, Batch 438, LR 0.132083 Loss 19.063163, Accuracy 0.082%\n",
      "Epoch 0, Batch 439, LR 0.132116 Loss 19.062856, Accuracy 0.082%\n",
      "Epoch 0, Batch 440, LR 0.132148 Loss 19.062711, Accuracy 0.083%\n",
      "Epoch 0, Batch 441, LR 0.132181 Loss 19.062437, Accuracy 0.085%\n",
      "Epoch 0, Batch 442, LR 0.132214 Loss 19.062079, Accuracy 0.085%\n",
      "Epoch 0, Batch 443, LR 0.132246 Loss 19.062426, Accuracy 0.085%\n",
      "Epoch 0, Batch 444, LR 0.132279 Loss 19.062521, Accuracy 0.086%\n",
      "Epoch 0, Batch 445, LR 0.132312 Loss 19.062430, Accuracy 0.086%\n",
      "Epoch 0, Batch 446, LR 0.132345 Loss 19.062713, Accuracy 0.086%\n",
      "Epoch 0, Batch 447, LR 0.132378 Loss 19.062641, Accuracy 0.086%\n",
      "Epoch 0, Batch 448, LR 0.132411 Loss 19.062900, Accuracy 0.085%\n",
      "Epoch 0, Batch 449, LR 0.132444 Loss 19.062661, Accuracy 0.085%\n",
      "Epoch 0, Batch 450, LR 0.132477 Loss 19.063052, Accuracy 0.085%\n",
      "Epoch 0, Batch 451, LR 0.132511 Loss 19.062691, Accuracy 0.085%\n",
      "Epoch 0, Batch 452, LR 0.132544 Loss 19.063120, Accuracy 0.085%\n",
      "Epoch 0, Batch 453, LR 0.132578 Loss 19.063614, Accuracy 0.085%\n",
      "Epoch 0, Batch 454, LR 0.132611 Loss 19.063547, Accuracy 0.084%\n",
      "Epoch 0, Batch 455, LR 0.132645 Loss 19.064136, Accuracy 0.084%\n",
      "Epoch 0, Batch 456, LR 0.132678 Loss 19.063921, Accuracy 0.084%\n",
      "Epoch 0, Batch 457, LR 0.132712 Loss 19.063798, Accuracy 0.084%\n",
      "Epoch 0, Batch 458, LR 0.132746 Loss 19.063825, Accuracy 0.084%\n",
      "Epoch 0, Batch 459, LR 0.132780 Loss 19.064058, Accuracy 0.083%\n",
      "Epoch 0, Batch 460, LR 0.132814 Loss 19.063848, Accuracy 0.085%\n",
      "Epoch 0, Batch 461, LR 0.132848 Loss 19.064200, Accuracy 0.085%\n",
      "Epoch 0, Batch 462, LR 0.132882 Loss 19.064286, Accuracy 0.085%\n",
      "Epoch 0, Batch 463, LR 0.132916 Loss 19.064407, Accuracy 0.086%\n",
      "Epoch 0, Batch 464, LR 0.132950 Loss 19.064029, Accuracy 0.086%\n",
      "Epoch 0, Batch 465, LR 0.132985 Loss 19.064053, Accuracy 0.087%\n",
      "Epoch 0, Batch 466, LR 0.133019 Loss 19.064447, Accuracy 0.087%\n",
      "Epoch 0, Batch 467, LR 0.133054 Loss 19.064734, Accuracy 0.087%\n",
      "Epoch 0, Batch 468, LR 0.133088 Loss 19.065036, Accuracy 0.087%\n",
      "Epoch 0, Batch 469, LR 0.133123 Loss 19.064602, Accuracy 0.087%\n",
      "Epoch 0, Batch 470, LR 0.133158 Loss 19.064839, Accuracy 0.086%\n",
      "Epoch 0, Batch 471, LR 0.133192 Loss 19.064835, Accuracy 0.086%\n",
      "Epoch 0, Batch 472, LR 0.133227 Loss 19.065121, Accuracy 0.086%\n",
      "Epoch 0, Batch 473, LR 0.133262 Loss 19.064880, Accuracy 0.086%\n",
      "Epoch 0, Batch 474, LR 0.133297 Loss 19.064808, Accuracy 0.087%\n",
      "Epoch 0, Batch 475, LR 0.133332 Loss 19.065117, Accuracy 0.087%\n",
      "Epoch 0, Batch 476, LR 0.133367 Loss 19.064636, Accuracy 0.087%\n",
      "Epoch 0, Batch 477, LR 0.133403 Loss 19.064736, Accuracy 0.087%\n",
      "Epoch 0, Batch 478, LR 0.133438 Loss 19.064697, Accuracy 0.087%\n",
      "Epoch 0, Batch 479, LR 0.133473 Loss 19.064685, Accuracy 0.086%\n",
      "Epoch 0, Batch 480, LR 0.133509 Loss 19.064293, Accuracy 0.086%\n",
      "Epoch 0, Batch 481, LR 0.133544 Loss 19.064441, Accuracy 0.086%\n",
      "Epoch 0, Batch 482, LR 0.133580 Loss 19.064419, Accuracy 0.088%\n",
      "Epoch 0, Batch 483, LR 0.133616 Loss 19.064204, Accuracy 0.087%\n",
      "Epoch 0, Batch 484, LR 0.133651 Loss 19.064160, Accuracy 0.087%\n",
      "Epoch 0, Batch 485, LR 0.133687 Loss 19.064250, Accuracy 0.087%\n",
      "Epoch 0, Batch 486, LR 0.133723 Loss 19.064304, Accuracy 0.088%\n",
      "Epoch 0, Batch 487, LR 0.133759 Loss 19.064157, Accuracy 0.088%\n",
      "Epoch 0, Batch 488, LR 0.133795 Loss 19.064065, Accuracy 0.088%\n",
      "Epoch 0, Batch 489, LR 0.133831 Loss 19.064145, Accuracy 0.088%\n",
      "Epoch 0, Batch 490, LR 0.133867 Loss 19.064094, Accuracy 0.088%\n",
      "Epoch 0, Batch 491, LR 0.133904 Loss 19.064197, Accuracy 0.088%\n",
      "Epoch 0, Batch 492, LR 0.133940 Loss 19.064285, Accuracy 0.087%\n",
      "Epoch 0, Batch 493, LR 0.133976 Loss 19.064643, Accuracy 0.087%\n",
      "Epoch 0, Batch 494, LR 0.134013 Loss 19.064948, Accuracy 0.087%\n",
      "Epoch 0, Batch 495, LR 0.134049 Loss 19.064621, Accuracy 0.087%\n",
      "Epoch 0, Batch 496, LR 0.134086 Loss 19.064404, Accuracy 0.090%\n",
      "Epoch 0, Batch 497, LR 0.134123 Loss 19.064143, Accuracy 0.090%\n",
      "Epoch 0, Batch 498, LR 0.134159 Loss 19.064505, Accuracy 0.089%\n",
      "Epoch 0, Batch 499, LR 0.134196 Loss 19.064650, Accuracy 0.089%\n",
      "Epoch 0, Batch 500, LR 0.134233 Loss 19.064855, Accuracy 0.089%\n",
      "Epoch 0, Batch 501, LR 0.134270 Loss 19.064789, Accuracy 0.090%\n",
      "Epoch 0, Batch 502, LR 0.134307 Loss 19.064537, Accuracy 0.090%\n",
      "Epoch 0, Batch 503, LR 0.134344 Loss 19.064794, Accuracy 0.090%\n",
      "Epoch 0, Batch 504, LR 0.134382 Loss 19.065301, Accuracy 0.090%\n",
      "Epoch 0, Batch 505, LR 0.134419 Loss 19.065720, Accuracy 0.090%\n",
      "Epoch 0, Batch 506, LR 0.134456 Loss 19.066100, Accuracy 0.090%\n",
      "Epoch 0, Batch 507, LR 0.134494 Loss 19.066047, Accuracy 0.089%\n",
      "Epoch 0, Batch 508, LR 0.134531 Loss 19.066402, Accuracy 0.089%\n",
      "Epoch 0, Batch 509, LR 0.134569 Loss 19.066208, Accuracy 0.089%\n",
      "Epoch 0, Batch 510, LR 0.134607 Loss 19.066109, Accuracy 0.089%\n",
      "Epoch 0, Batch 511, LR 0.134644 Loss 19.066345, Accuracy 0.089%\n",
      "Epoch 0, Batch 512, LR 0.134682 Loss 19.066438, Accuracy 0.089%\n",
      "Epoch 0, Batch 513, LR 0.134720 Loss 19.066669, Accuracy 0.088%\n",
      "Epoch 0, Batch 514, LR 0.134758 Loss 19.066598, Accuracy 0.090%\n",
      "Epoch 0, Batch 515, LR 0.134796 Loss 19.066803, Accuracy 0.090%\n",
      "Epoch 0, Batch 516, LR 0.134834 Loss 19.066599, Accuracy 0.089%\n",
      "Epoch 0, Batch 517, LR 0.134872 Loss 19.066580, Accuracy 0.089%\n",
      "Epoch 0, Batch 518, LR 0.134910 Loss 19.066371, Accuracy 0.090%\n",
      "Epoch 0, Batch 519, LR 0.134949 Loss 19.066218, Accuracy 0.090%\n",
      "Epoch 0, Batch 520, LR 0.134987 Loss 19.066025, Accuracy 0.090%\n",
      "Epoch 0, Batch 521, LR 0.135026 Loss 19.066455, Accuracy 0.090%\n",
      "Epoch 0, Batch 522, LR 0.135064 Loss 19.066407, Accuracy 0.090%\n",
      "Epoch 0, Batch 523, LR 0.135103 Loss 19.066534, Accuracy 0.090%\n",
      "Epoch 0, Batch 524, LR 0.135141 Loss 19.066673, Accuracy 0.089%\n",
      "Epoch 0, Batch 525, LR 0.135180 Loss 19.066557, Accuracy 0.089%\n",
      "Epoch 0, Batch 526, LR 0.135219 Loss 19.066485, Accuracy 0.089%\n",
      "Epoch 0, Batch 527, LR 0.135258 Loss 19.066782, Accuracy 0.089%\n",
      "Epoch 0, Batch 528, LR 0.135297 Loss 19.066843, Accuracy 0.089%\n",
      "Epoch 0, Batch 529, LR 0.135336 Loss 19.066893, Accuracy 0.089%\n",
      "Epoch 0, Batch 530, LR 0.135375 Loss 19.066934, Accuracy 0.088%\n",
      "Epoch 0, Batch 531, LR 0.135414 Loss 19.066823, Accuracy 0.088%\n",
      "Epoch 0, Batch 532, LR 0.135454 Loss 19.067222, Accuracy 0.088%\n",
      "Epoch 0, Batch 533, LR 0.135493 Loss 19.066863, Accuracy 0.088%\n",
      "Epoch 0, Batch 534, LR 0.135532 Loss 19.067026, Accuracy 0.088%\n",
      "Epoch 0, Batch 535, LR 0.135572 Loss 19.066877, Accuracy 0.088%\n",
      "Epoch 0, Batch 536, LR 0.135611 Loss 19.067449, Accuracy 0.087%\n",
      "Epoch 0, Batch 537, LR 0.135651 Loss 19.067345, Accuracy 0.087%\n",
      "Epoch 0, Batch 538, LR 0.135691 Loss 19.067268, Accuracy 0.087%\n",
      "Epoch 0, Batch 539, LR 0.135731 Loss 19.066815, Accuracy 0.088%\n",
      "Epoch 0, Batch 540, LR 0.135771 Loss 19.067153, Accuracy 0.088%\n",
      "Epoch 0, Batch 541, LR 0.135810 Loss 19.067583, Accuracy 0.090%\n",
      "Epoch 0, Batch 542, LR 0.135850 Loss 19.067339, Accuracy 0.089%\n",
      "Epoch 0, Batch 543, LR 0.135891 Loss 19.067101, Accuracy 0.089%\n",
      "Epoch 0, Batch 544, LR 0.135931 Loss 19.067091, Accuracy 0.089%\n",
      "Epoch 0, Batch 545, LR 0.135971 Loss 19.067224, Accuracy 0.089%\n",
      "Epoch 0, Batch 546, LR 0.136011 Loss 19.067439, Accuracy 0.089%\n",
      "Epoch 0, Batch 547, LR 0.136052 Loss 19.067429, Accuracy 0.089%\n",
      "Epoch 0, Batch 548, LR 0.136092 Loss 19.067739, Accuracy 0.088%\n",
      "Epoch 0, Batch 549, LR 0.136133 Loss 19.067811, Accuracy 0.088%\n",
      "Epoch 0, Batch 550, LR 0.136173 Loss 19.068136, Accuracy 0.088%\n",
      "Epoch 0, Batch 551, LR 0.136214 Loss 19.068511, Accuracy 0.088%\n",
      "Epoch 0, Batch 552, LR 0.136255 Loss 19.068708, Accuracy 0.088%\n",
      "Epoch 0, Batch 553, LR 0.136295 Loss 19.068506, Accuracy 0.088%\n",
      "Epoch 0, Batch 554, LR 0.136336 Loss 19.068940, Accuracy 0.087%\n",
      "Epoch 0, Batch 555, LR 0.136377 Loss 19.068889, Accuracy 0.087%\n",
      "Epoch 0, Batch 556, LR 0.136418 Loss 19.069250, Accuracy 0.087%\n",
      "Epoch 0, Batch 557, LR 0.136460 Loss 19.069366, Accuracy 0.087%\n",
      "Epoch 0, Batch 558, LR 0.136501 Loss 19.069754, Accuracy 0.087%\n",
      "Epoch 0, Batch 559, LR 0.136542 Loss 19.069820, Accuracy 0.087%\n",
      "Epoch 0, Batch 560, LR 0.136583 Loss 19.069767, Accuracy 0.086%\n",
      "Epoch 0, Batch 561, LR 0.136625 Loss 19.069766, Accuracy 0.086%\n",
      "Epoch 0, Batch 562, LR 0.136666 Loss 19.069489, Accuracy 0.086%\n",
      "Epoch 0, Batch 563, LR 0.136708 Loss 19.069186, Accuracy 0.086%\n",
      "Epoch 0, Batch 564, LR 0.136749 Loss 19.069078, Accuracy 0.086%\n",
      "Epoch 0, Batch 565, LR 0.136791 Loss 19.069192, Accuracy 0.086%\n",
      "Epoch 0, Batch 566, LR 0.136833 Loss 19.068902, Accuracy 0.086%\n",
      "Epoch 0, Batch 567, LR 0.136875 Loss 19.069068, Accuracy 0.085%\n",
      "Epoch 0, Batch 568, LR 0.136917 Loss 19.069020, Accuracy 0.087%\n",
      "Epoch 0, Batch 569, LR 0.136959 Loss 19.069299, Accuracy 0.087%\n",
      "Epoch 0, Batch 570, LR 0.137001 Loss 19.069172, Accuracy 0.086%\n",
      "Epoch 0, Batch 571, LR 0.137043 Loss 19.069050, Accuracy 0.086%\n",
      "Epoch 0, Batch 572, LR 0.137085 Loss 19.069323, Accuracy 0.086%\n",
      "Epoch 0, Batch 573, LR 0.137127 Loss 19.069093, Accuracy 0.086%\n",
      "Epoch 0, Batch 574, LR 0.137170 Loss 19.069250, Accuracy 0.086%\n",
      "Epoch 0, Batch 575, LR 0.137212 Loss 19.069109, Accuracy 0.086%\n",
      "Epoch 0, Batch 576, LR 0.137255 Loss 19.069401, Accuracy 0.085%\n",
      "Epoch 0, Batch 577, LR 0.137297 Loss 19.069270, Accuracy 0.085%\n",
      "Epoch 0, Batch 578, LR 0.137340 Loss 19.069152, Accuracy 0.085%\n",
      "Epoch 0, Batch 579, LR 0.137383 Loss 19.069235, Accuracy 0.086%\n",
      "Epoch 0, Batch 580, LR 0.137426 Loss 19.069082, Accuracy 0.086%\n",
      "Epoch 0, Batch 581, LR 0.137468 Loss 19.069381, Accuracy 0.086%\n",
      "Epoch 0, Batch 582, LR 0.137511 Loss 19.069539, Accuracy 0.086%\n",
      "Epoch 0, Batch 583, LR 0.137554 Loss 19.069714, Accuracy 0.086%\n",
      "Epoch 0, Batch 584, LR 0.137598 Loss 19.069642, Accuracy 0.086%\n",
      "Epoch 0, Batch 585, LR 0.137641 Loss 19.069955, Accuracy 0.085%\n",
      "Epoch 0, Batch 586, LR 0.137684 Loss 19.069986, Accuracy 0.085%\n",
      "Epoch 0, Batch 587, LR 0.137727 Loss 19.069917, Accuracy 0.085%\n",
      "Epoch 0, Batch 588, LR 0.137771 Loss 19.069790, Accuracy 0.085%\n",
      "Epoch 0, Batch 589, LR 0.137814 Loss 19.069861, Accuracy 0.085%\n",
      "Epoch 0, Batch 590, LR 0.137858 Loss 19.069952, Accuracy 0.085%\n",
      "Epoch 0, Batch 591, LR 0.137901 Loss 19.069814, Accuracy 0.085%\n",
      "Epoch 0, Batch 592, LR 0.137945 Loss 19.070111, Accuracy 0.084%\n",
      "Epoch 0, Batch 593, LR 0.137989 Loss 19.070151, Accuracy 0.084%\n",
      "Epoch 0, Batch 594, LR 0.138033 Loss 19.070119, Accuracy 0.084%\n",
      "Epoch 0, Batch 595, LR 0.138076 Loss 19.070478, Accuracy 0.084%\n",
      "Epoch 0, Batch 596, LR 0.138120 Loss 19.070331, Accuracy 0.084%\n",
      "Epoch 0, Batch 597, LR 0.138165 Loss 19.070393, Accuracy 0.084%\n",
      "Epoch 0, Batch 598, LR 0.138209 Loss 19.070453, Accuracy 0.084%\n",
      "Epoch 0, Batch 599, LR 0.138253 Loss 19.070125, Accuracy 0.083%\n",
      "Epoch 0, Batch 600, LR 0.138297 Loss 19.070381, Accuracy 0.083%\n",
      "Epoch 0, Batch 601, LR 0.138341 Loss 19.070419, Accuracy 0.083%\n",
      "Epoch 0, Batch 602, LR 0.138386 Loss 19.070347, Accuracy 0.083%\n",
      "Epoch 0, Batch 603, LR 0.138430 Loss 19.070174, Accuracy 0.083%\n",
      "Epoch 0, Batch 604, LR 0.138475 Loss 19.070269, Accuracy 0.083%\n",
      "Epoch 0, Batch 605, LR 0.138520 Loss 19.070408, Accuracy 0.083%\n",
      "Epoch 0, Batch 606, LR 0.138564 Loss 19.070194, Accuracy 0.083%\n",
      "Epoch 0, Batch 607, LR 0.138609 Loss 19.070133, Accuracy 0.084%\n",
      "Epoch 0, Batch 608, LR 0.138654 Loss 19.070159, Accuracy 0.084%\n",
      "Epoch 0, Batch 609, LR 0.138699 Loss 19.069850, Accuracy 0.083%\n",
      "Epoch 0, Batch 610, LR 0.138744 Loss 19.069638, Accuracy 0.083%\n",
      "Epoch 0, Batch 611, LR 0.138789 Loss 19.069524, Accuracy 0.083%\n",
      "Epoch 0, Batch 612, LR 0.138834 Loss 19.069650, Accuracy 0.083%\n",
      "Epoch 0, Batch 613, LR 0.138879 Loss 19.069782, Accuracy 0.083%\n",
      "Epoch 0, Batch 614, LR 0.138925 Loss 19.069951, Accuracy 0.083%\n",
      "Epoch 0, Batch 615, LR 0.138970 Loss 19.070128, Accuracy 0.083%\n",
      "Epoch 0, Batch 616, LR 0.139016 Loss 19.070348, Accuracy 0.082%\n",
      "Epoch 0, Batch 617, LR 0.139061 Loss 19.070576, Accuracy 0.082%\n",
      "Epoch 0, Batch 618, LR 0.139107 Loss 19.070661, Accuracy 0.082%\n",
      "Epoch 0, Batch 619, LR 0.139152 Loss 19.070650, Accuracy 0.082%\n",
      "Epoch 0, Batch 620, LR 0.139198 Loss 19.070410, Accuracy 0.082%\n",
      "Epoch 0, Batch 621, LR 0.139244 Loss 19.070513, Accuracy 0.083%\n",
      "Epoch 0, Batch 622, LR 0.139290 Loss 19.070163, Accuracy 0.083%\n",
      "Epoch 0, Batch 623, LR 0.139336 Loss 19.070174, Accuracy 0.083%\n",
      "Epoch 0, Batch 624, LR 0.139382 Loss 19.070112, Accuracy 0.084%\n",
      "Epoch 0, Batch 625, LR 0.139428 Loss 19.070063, Accuracy 0.084%\n",
      "Epoch 0, Batch 626, LR 0.139474 Loss 19.070368, Accuracy 0.084%\n",
      "Epoch 0, Batch 627, LR 0.139520 Loss 19.069959, Accuracy 0.085%\n",
      "Epoch 0, Batch 628, LR 0.139567 Loss 19.070062, Accuracy 0.085%\n",
      "Epoch 0, Batch 629, LR 0.139613 Loss 19.070115, Accuracy 0.084%\n",
      "Epoch 0, Batch 630, LR 0.139660 Loss 19.070090, Accuracy 0.084%\n",
      "Epoch 0, Batch 631, LR 0.139706 Loss 19.070109, Accuracy 0.084%\n",
      "Epoch 0, Batch 632, LR 0.139753 Loss 19.070150, Accuracy 0.084%\n",
      "Epoch 0, Batch 633, LR 0.139799 Loss 19.070019, Accuracy 0.084%\n",
      "Epoch 0, Batch 634, LR 0.139846 Loss 19.069936, Accuracy 0.084%\n",
      "Epoch 0, Batch 635, LR 0.139893 Loss 19.070090, Accuracy 0.084%\n",
      "Epoch 0, Batch 636, LR 0.139940 Loss 19.070085, Accuracy 0.084%\n",
      "Epoch 0, Batch 637, LR 0.139987 Loss 19.069852, Accuracy 0.083%\n",
      "Epoch 0, Batch 638, LR 0.140034 Loss 19.070175, Accuracy 0.083%\n",
      "Epoch 0, Batch 639, LR 0.140081 Loss 19.070196, Accuracy 0.084%\n",
      "Epoch 0, Batch 640, LR 0.140128 Loss 19.070441, Accuracy 0.084%\n",
      "Epoch 0, Batch 641, LR 0.140176 Loss 19.070197, Accuracy 0.085%\n",
      "Epoch 0, Batch 642, LR 0.140223 Loss 19.070265, Accuracy 0.085%\n",
      "Epoch 0, Batch 643, LR 0.140271 Loss 19.070014, Accuracy 0.085%\n",
      "Epoch 0, Batch 644, LR 0.140318 Loss 19.069919, Accuracy 0.085%\n",
      "Epoch 0, Batch 645, LR 0.140366 Loss 19.069884, Accuracy 0.086%\n",
      "Epoch 0, Batch 646, LR 0.140413 Loss 19.069874, Accuracy 0.086%\n",
      "Epoch 0, Batch 647, LR 0.140461 Loss 19.070060, Accuracy 0.086%\n",
      "Epoch 0, Batch 648, LR 0.140509 Loss 19.069852, Accuracy 0.086%\n",
      "Epoch 0, Batch 649, LR 0.140557 Loss 19.069923, Accuracy 0.085%\n",
      "Epoch 0, Batch 650, LR 0.140605 Loss 19.069739, Accuracy 0.085%\n",
      "Epoch 0, Batch 651, LR 0.140653 Loss 19.070039, Accuracy 0.085%\n",
      "Epoch 0, Batch 652, LR 0.140701 Loss 19.069551, Accuracy 0.085%\n",
      "Epoch 0, Batch 653, LR 0.140749 Loss 19.069419, Accuracy 0.085%\n",
      "Epoch 0, Batch 654, LR 0.140797 Loss 19.069395, Accuracy 0.085%\n",
      "Epoch 0, Batch 655, LR 0.140845 Loss 19.069179, Accuracy 0.085%\n",
      "Epoch 0, Batch 656, LR 0.140894 Loss 19.069426, Accuracy 0.085%\n",
      "Epoch 0, Batch 657, LR 0.140942 Loss 19.069229, Accuracy 0.084%\n",
      "Epoch 0, Batch 658, LR 0.140991 Loss 19.069029, Accuracy 0.084%\n",
      "Epoch 0, Batch 659, LR 0.141039 Loss 19.069149, Accuracy 0.084%\n",
      "Epoch 0, Batch 660, LR 0.141088 Loss 19.068938, Accuracy 0.084%\n",
      "Epoch 0, Batch 661, LR 0.141137 Loss 19.069097, Accuracy 0.084%\n",
      "Epoch 0, Batch 662, LR 0.141186 Loss 19.068885, Accuracy 0.084%\n",
      "Epoch 0, Batch 663, LR 0.141235 Loss 19.068972, Accuracy 0.084%\n",
      "Epoch 0, Batch 664, LR 0.141284 Loss 19.068744, Accuracy 0.084%\n",
      "Epoch 0, Batch 665, LR 0.141333 Loss 19.068552, Accuracy 0.086%\n",
      "Epoch 0, Batch 666, LR 0.141382 Loss 19.068792, Accuracy 0.086%\n",
      "Epoch 0, Batch 667, LR 0.141431 Loss 19.068866, Accuracy 0.086%\n",
      "Epoch 0, Batch 668, LR 0.141480 Loss 19.069260, Accuracy 0.085%\n",
      "Epoch 0, Batch 669, LR 0.141529 Loss 19.069173, Accuracy 0.085%\n",
      "Epoch 0, Batch 670, LR 0.141579 Loss 19.068926, Accuracy 0.085%\n",
      "Epoch 0, Batch 671, LR 0.141628 Loss 19.069245, Accuracy 0.085%\n",
      "Epoch 0, Batch 672, LR 0.141678 Loss 19.069509, Accuracy 0.085%\n",
      "Epoch 0, Batch 673, LR 0.141728 Loss 19.069594, Accuracy 0.085%\n",
      "Epoch 0, Batch 674, LR 0.141777 Loss 19.069715, Accuracy 0.085%\n",
      "Epoch 0, Batch 675, LR 0.141827 Loss 19.069456, Accuracy 0.084%\n",
      "Epoch 0, Batch 676, LR 0.141877 Loss 19.069322, Accuracy 0.084%\n",
      "Epoch 0, Batch 677, LR 0.141927 Loss 19.069048, Accuracy 0.084%\n",
      "Epoch 0, Batch 678, LR 0.141977 Loss 19.068667, Accuracy 0.084%\n",
      "Epoch 0, Batch 679, LR 0.142027 Loss 19.068452, Accuracy 0.084%\n",
      "Epoch 0, Batch 680, LR 0.142077 Loss 19.068754, Accuracy 0.084%\n",
      "Epoch 0, Batch 681, LR 0.142127 Loss 19.068781, Accuracy 0.084%\n",
      "Epoch 0, Batch 682, LR 0.142178 Loss 19.068662, Accuracy 0.084%\n",
      "Epoch 0, Batch 683, LR 0.142228 Loss 19.068498, Accuracy 0.084%\n",
      "Epoch 0, Batch 684, LR 0.142278 Loss 19.068129, Accuracy 0.083%\n",
      "Epoch 0, Batch 685, LR 0.142329 Loss 19.068208, Accuracy 0.083%\n",
      "Epoch 0, Batch 686, LR 0.142379 Loss 19.068074, Accuracy 0.083%\n",
      "Epoch 0, Batch 687, LR 0.142430 Loss 19.067927, Accuracy 0.083%\n",
      "Epoch 0, Batch 688, LR 0.142481 Loss 19.068000, Accuracy 0.083%\n",
      "Epoch 0, Batch 689, LR 0.142532 Loss 19.068093, Accuracy 0.083%\n",
      "Epoch 0, Batch 690, LR 0.142582 Loss 19.068141, Accuracy 0.083%\n",
      "Epoch 0, Batch 691, LR 0.142633 Loss 19.068265, Accuracy 0.083%\n",
      "Epoch 0, Batch 692, LR 0.142684 Loss 19.068040, Accuracy 0.082%\n",
      "Epoch 0, Batch 693, LR 0.142736 Loss 19.068117, Accuracy 0.082%\n",
      "Epoch 0, Batch 694, LR 0.142787 Loss 19.067868, Accuracy 0.083%\n",
      "Epoch 0, Batch 695, LR 0.142838 Loss 19.067927, Accuracy 0.083%\n",
      "Epoch 0, Batch 696, LR 0.142889 Loss 19.067690, Accuracy 0.083%\n",
      "Epoch 0, Batch 697, LR 0.142941 Loss 19.067659, Accuracy 0.083%\n",
      "Epoch 0, Batch 698, LR 0.142992 Loss 19.067668, Accuracy 0.083%\n",
      "Epoch 0, Batch 699, LR 0.143044 Loss 19.067767, Accuracy 0.083%\n",
      "Epoch 0, Batch 700, LR 0.143095 Loss 19.067365, Accuracy 0.083%\n",
      "Epoch 0, Batch 701, LR 0.143147 Loss 19.067422, Accuracy 0.082%\n",
      "Epoch 0, Batch 702, LR 0.143199 Loss 19.067093, Accuracy 0.082%\n",
      "Epoch 0, Batch 703, LR 0.143251 Loss 19.067155, Accuracy 0.082%\n",
      "Epoch 0, Batch 704, LR 0.143302 Loss 19.067229, Accuracy 0.082%\n",
      "Epoch 0, Batch 705, LR 0.143354 Loss 19.067354, Accuracy 0.082%\n",
      "Epoch 0, Batch 706, LR 0.143406 Loss 19.067679, Accuracy 0.082%\n",
      "Epoch 0, Batch 707, LR 0.143459 Loss 19.067622, Accuracy 0.082%\n",
      "Epoch 0, Batch 708, LR 0.143511 Loss 19.067529, Accuracy 0.082%\n",
      "Epoch 0, Batch 709, LR 0.143563 Loss 19.067538, Accuracy 0.082%\n",
      "Epoch 0, Batch 710, LR 0.143615 Loss 19.067705, Accuracy 0.081%\n",
      "Epoch 0, Batch 711, LR 0.143668 Loss 19.067983, Accuracy 0.081%\n",
      "Epoch 0, Batch 712, LR 0.143720 Loss 19.068182, Accuracy 0.081%\n",
      "Epoch 0, Batch 713, LR 0.143773 Loss 19.067862, Accuracy 0.082%\n",
      "Epoch 0, Batch 714, LR 0.143825 Loss 19.067716, Accuracy 0.082%\n",
      "Epoch 0, Batch 715, LR 0.143878 Loss 19.067660, Accuracy 0.082%\n",
      "Epoch 0, Batch 716, LR 0.143931 Loss 19.067763, Accuracy 0.082%\n",
      "Epoch 0, Batch 717, LR 0.143984 Loss 19.067502, Accuracy 0.082%\n",
      "Epoch 0, Batch 718, LR 0.144037 Loss 19.067720, Accuracy 0.082%\n",
      "Epoch 0, Batch 719, LR 0.144090 Loss 19.067542, Accuracy 0.081%\n",
      "Epoch 0, Batch 720, LR 0.144143 Loss 19.067384, Accuracy 0.081%\n",
      "Epoch 0, Batch 721, LR 0.144196 Loss 19.067419, Accuracy 0.082%\n",
      "Epoch 0, Batch 722, LR 0.144249 Loss 19.067457, Accuracy 0.082%\n",
      "Epoch 0, Batch 723, LR 0.144302 Loss 19.067429, Accuracy 0.082%\n",
      "Epoch 0, Batch 724, LR 0.144356 Loss 19.067681, Accuracy 0.083%\n",
      "Epoch 0, Batch 725, LR 0.144409 Loss 19.067877, Accuracy 0.083%\n",
      "Epoch 0, Batch 726, LR 0.144463 Loss 19.067932, Accuracy 0.084%\n",
      "Epoch 0, Batch 727, LR 0.144516 Loss 19.068032, Accuracy 0.084%\n",
      "Epoch 0, Batch 728, LR 0.144570 Loss 19.067715, Accuracy 0.084%\n",
      "Epoch 0, Batch 729, LR 0.144624 Loss 19.067595, Accuracy 0.084%\n",
      "Epoch 0, Batch 730, LR 0.144677 Loss 19.067215, Accuracy 0.083%\n",
      "Epoch 0, Batch 731, LR 0.144731 Loss 19.066972, Accuracy 0.083%\n",
      "Epoch 0, Batch 732, LR 0.144785 Loss 19.066952, Accuracy 0.083%\n",
      "Epoch 0, Batch 733, LR 0.144839 Loss 19.066829, Accuracy 0.083%\n",
      "Epoch 0, Batch 734, LR 0.144893 Loss 19.066963, Accuracy 0.083%\n",
      "Epoch 0, Batch 735, LR 0.144948 Loss 19.067016, Accuracy 0.083%\n",
      "Epoch 0, Batch 736, LR 0.145002 Loss 19.067038, Accuracy 0.083%\n",
      "Epoch 0, Batch 737, LR 0.145056 Loss 19.067026, Accuracy 0.083%\n",
      "Epoch 0, Batch 738, LR 0.145110 Loss 19.067153, Accuracy 0.083%\n",
      "Epoch 0, Batch 739, LR 0.145165 Loss 19.067083, Accuracy 0.084%\n",
      "Epoch 0, Batch 740, LR 0.145219 Loss 19.067099, Accuracy 0.083%\n",
      "Epoch 0, Batch 741, LR 0.145274 Loss 19.067063, Accuracy 0.083%\n",
      "Epoch 0, Batch 742, LR 0.145329 Loss 19.067097, Accuracy 0.083%\n",
      "Epoch 0, Batch 743, LR 0.145383 Loss 19.067597, Accuracy 0.083%\n",
      "Epoch 0, Batch 744, LR 0.145438 Loss 19.067605, Accuracy 0.083%\n",
      "Epoch 0, Batch 745, LR 0.145493 Loss 19.067554, Accuracy 0.083%\n",
      "Epoch 0, Batch 746, LR 0.145548 Loss 19.067701, Accuracy 0.083%\n",
      "Epoch 0, Batch 747, LR 0.145603 Loss 19.067720, Accuracy 0.083%\n",
      "Epoch 0, Batch 748, LR 0.145658 Loss 19.067626, Accuracy 0.083%\n",
      "Epoch 0, Batch 749, LR 0.145713 Loss 19.067545, Accuracy 0.082%\n",
      "Epoch 0, Batch 750, LR 0.145769 Loss 19.067556, Accuracy 0.082%\n",
      "Epoch 0, Batch 751, LR 0.145824 Loss 19.067634, Accuracy 0.082%\n",
      "Epoch 0, Batch 752, LR 0.145879 Loss 19.067646, Accuracy 0.082%\n",
      "Epoch 0, Batch 753, LR 0.145935 Loss 19.067460, Accuracy 0.082%\n",
      "Epoch 0, Batch 754, LR 0.145991 Loss 19.067100, Accuracy 0.082%\n",
      "Epoch 0, Batch 755, LR 0.146046 Loss 19.067251, Accuracy 0.082%\n",
      "Epoch 0, Batch 756, LR 0.146102 Loss 19.067177, Accuracy 0.082%\n",
      "Epoch 0, Batch 757, LR 0.146158 Loss 19.067189, Accuracy 0.082%\n",
      "Epoch 0, Batch 758, LR 0.146213 Loss 19.067267, Accuracy 0.081%\n",
      "Epoch 0, Batch 759, LR 0.146269 Loss 19.067156, Accuracy 0.081%\n",
      "Epoch 0, Batch 760, LR 0.146325 Loss 19.067314, Accuracy 0.081%\n",
      "Epoch 0, Batch 761, LR 0.146381 Loss 19.067091, Accuracy 0.081%\n",
      "Epoch 0, Batch 762, LR 0.146438 Loss 19.067097, Accuracy 0.081%\n",
      "Epoch 0, Batch 763, LR 0.146494 Loss 19.067409, Accuracy 0.081%\n",
      "Epoch 0, Batch 764, LR 0.146550 Loss 19.067616, Accuracy 0.081%\n",
      "Epoch 0, Batch 765, LR 0.146606 Loss 19.067439, Accuracy 0.081%\n",
      "Epoch 0, Batch 766, LR 0.146663 Loss 19.067328, Accuracy 0.081%\n",
      "Epoch 0, Batch 767, LR 0.146719 Loss 19.067335, Accuracy 0.080%\n",
      "Epoch 0, Batch 768, LR 0.146776 Loss 19.067193, Accuracy 0.080%\n",
      "Epoch 0, Batch 769, LR 0.146833 Loss 19.067023, Accuracy 0.080%\n",
      "Epoch 0, Batch 770, LR 0.146889 Loss 19.067009, Accuracy 0.080%\n",
      "Epoch 0, Batch 771, LR 0.146946 Loss 19.066937, Accuracy 0.080%\n",
      "Epoch 0, Batch 772, LR 0.147003 Loss 19.066747, Accuracy 0.080%\n",
      "Epoch 0, Batch 773, LR 0.147060 Loss 19.066975, Accuracy 0.080%\n",
      "Epoch 0, Batch 774, LR 0.147117 Loss 19.066930, Accuracy 0.080%\n",
      "Epoch 0, Batch 775, LR 0.147174 Loss 19.067035, Accuracy 0.080%\n",
      "Epoch 0, Batch 776, LR 0.147231 Loss 19.066519, Accuracy 0.080%\n",
      "Epoch 0, Batch 777, LR 0.147288 Loss 19.066751, Accuracy 0.080%\n",
      "Epoch 0, Batch 778, LR 0.147346 Loss 19.066856, Accuracy 0.080%\n",
      "Epoch 0, Batch 779, LR 0.147403 Loss 19.066829, Accuracy 0.080%\n",
      "Epoch 0, Batch 780, LR 0.147460 Loss 19.067207, Accuracy 0.080%\n",
      "Epoch 0, Batch 781, LR 0.147518 Loss 19.067184, Accuracy 0.080%\n",
      "Epoch 0, Batch 782, LR 0.147576 Loss 19.067255, Accuracy 0.080%\n",
      "Epoch 0, Batch 783, LR 0.147633 Loss 19.067223, Accuracy 0.080%\n",
      "Epoch 0, Batch 784, LR 0.147691 Loss 19.067306, Accuracy 0.080%\n",
      "Epoch 0, Batch 785, LR 0.147749 Loss 19.067457, Accuracy 0.080%\n",
      "Epoch 0, Batch 786, LR 0.147807 Loss 19.067417, Accuracy 0.080%\n",
      "Epoch 0, Batch 787, LR 0.147865 Loss 19.067363, Accuracy 0.079%\n",
      "Epoch 0, Batch 788, LR 0.147923 Loss 19.067542, Accuracy 0.079%\n",
      "Epoch 0, Batch 789, LR 0.147981 Loss 19.067498, Accuracy 0.079%\n",
      "Epoch 0, Batch 790, LR 0.148039 Loss 19.067449, Accuracy 0.079%\n",
      "Epoch 0, Batch 791, LR 0.148097 Loss 19.067388, Accuracy 0.079%\n",
      "Epoch 0, Batch 792, LR 0.148155 Loss 19.067434, Accuracy 0.079%\n",
      "Epoch 0, Batch 793, LR 0.148214 Loss 19.067420, Accuracy 0.079%\n",
      "Epoch 0, Batch 794, LR 0.148272 Loss 19.067410, Accuracy 0.079%\n",
      "Epoch 0, Batch 795, LR 0.148331 Loss 19.067433, Accuracy 0.079%\n",
      "Epoch 0, Batch 796, LR 0.148389 Loss 19.067541, Accuracy 0.079%\n",
      "Epoch 0, Batch 797, LR 0.148448 Loss 19.067670, Accuracy 0.079%\n",
      "Epoch 0, Batch 798, LR 0.148507 Loss 19.067581, Accuracy 0.079%\n",
      "Epoch 0, Batch 799, LR 0.148566 Loss 19.067779, Accuracy 0.079%\n",
      "Epoch 0, Batch 800, LR 0.148625 Loss 19.067956, Accuracy 0.080%\n",
      "Epoch 0, Batch 801, LR 0.148684 Loss 19.068089, Accuracy 0.080%\n",
      "Epoch 0, Batch 802, LR 0.148743 Loss 19.068032, Accuracy 0.081%\n",
      "Epoch 0, Batch 803, LR 0.148802 Loss 19.067836, Accuracy 0.081%\n",
      "Epoch 0, Batch 804, LR 0.148861 Loss 19.067673, Accuracy 0.081%\n",
      "Epoch 0, Batch 805, LR 0.148920 Loss 19.067757, Accuracy 0.081%\n",
      "Epoch 0, Batch 806, LR 0.148980 Loss 19.067666, Accuracy 0.080%\n",
      "Epoch 0, Batch 807, LR 0.149039 Loss 19.067952, Accuracy 0.081%\n",
      "Epoch 0, Batch 808, LR 0.149098 Loss 19.067607, Accuracy 0.082%\n",
      "Epoch 0, Batch 809, LR 0.149158 Loss 19.067427, Accuracy 0.082%\n",
      "Epoch 0, Batch 810, LR 0.149218 Loss 19.067250, Accuracy 0.082%\n",
      "Epoch 0, Batch 811, LR 0.149277 Loss 19.067018, Accuracy 0.082%\n",
      "Epoch 0, Batch 812, LR 0.149337 Loss 19.067069, Accuracy 0.082%\n",
      "Epoch 0, Batch 813, LR 0.149397 Loss 19.067035, Accuracy 0.082%\n",
      "Epoch 0, Batch 814, LR 0.149457 Loss 19.066899, Accuracy 0.082%\n",
      "Epoch 0, Batch 815, LR 0.149517 Loss 19.066738, Accuracy 0.082%\n",
      "Epoch 0, Batch 816, LR 0.149577 Loss 19.066761, Accuracy 0.082%\n",
      "Epoch 0, Batch 817, LR 0.149637 Loss 19.066663, Accuracy 0.082%\n",
      "Epoch 0, Batch 818, LR 0.149697 Loss 19.066479, Accuracy 0.082%\n",
      "Epoch 0, Batch 819, LR 0.149758 Loss 19.066208, Accuracy 0.082%\n",
      "Epoch 0, Batch 820, LR 0.149818 Loss 19.066143, Accuracy 0.082%\n",
      "Epoch 0, Batch 821, LR 0.149878 Loss 19.065996, Accuracy 0.082%\n",
      "Epoch 0, Batch 822, LR 0.149939 Loss 19.065933, Accuracy 0.082%\n",
      "Epoch 0, Batch 823, LR 0.149999 Loss 19.065982, Accuracy 0.082%\n",
      "Epoch 0, Batch 824, LR 0.150060 Loss 19.065873, Accuracy 0.082%\n",
      "Epoch 0, Batch 825, LR 0.150121 Loss 19.066083, Accuracy 0.081%\n",
      "Epoch 0, Batch 826, LR 0.150182 Loss 19.066195, Accuracy 0.081%\n",
      "Epoch 0, Batch 827, LR 0.150242 Loss 19.066123, Accuracy 0.081%\n",
      "Epoch 0, Batch 828, LR 0.150303 Loss 19.066235, Accuracy 0.081%\n",
      "Epoch 0, Batch 829, LR 0.150364 Loss 19.066251, Accuracy 0.081%\n",
      "Epoch 0, Batch 830, LR 0.150426 Loss 19.066213, Accuracy 0.081%\n",
      "Epoch 0, Batch 831, LR 0.150487 Loss 19.066066, Accuracy 0.081%\n",
      "Epoch 0, Batch 832, LR 0.150548 Loss 19.066158, Accuracy 0.081%\n",
      "Epoch 0, Batch 833, LR 0.150609 Loss 19.066047, Accuracy 0.081%\n",
      "Epoch 0, Batch 834, LR 0.150671 Loss 19.066193, Accuracy 0.081%\n",
      "Epoch 0, Batch 835, LR 0.150732 Loss 19.066055, Accuracy 0.080%\n",
      "Epoch 0, Batch 836, LR 0.150794 Loss 19.066081, Accuracy 0.080%\n",
      "Epoch 0, Batch 837, LR 0.150855 Loss 19.066042, Accuracy 0.080%\n",
      "Epoch 0, Batch 838, LR 0.150917 Loss 19.066061, Accuracy 0.080%\n",
      "Epoch 0, Batch 839, LR 0.150979 Loss 19.065730, Accuracy 0.080%\n",
      "Epoch 0, Batch 840, LR 0.151040 Loss 19.065896, Accuracy 0.080%\n",
      "Epoch 0, Batch 841, LR 0.151102 Loss 19.065907, Accuracy 0.080%\n",
      "Epoch 0, Batch 842, LR 0.151164 Loss 19.065973, Accuracy 0.080%\n",
      "Epoch 0, Batch 843, LR 0.151226 Loss 19.065877, Accuracy 0.080%\n",
      "Epoch 0, Batch 844, LR 0.151288 Loss 19.066073, Accuracy 0.080%\n",
      "Epoch 0, Batch 845, LR 0.151350 Loss 19.066301, Accuracy 0.080%\n",
      "Epoch 0, Batch 846, LR 0.151413 Loss 19.066051, Accuracy 0.079%\n",
      "Epoch 0, Batch 847, LR 0.151475 Loss 19.066009, Accuracy 0.079%\n",
      "Epoch 0, Batch 848, LR 0.151537 Loss 19.065937, Accuracy 0.079%\n",
      "Epoch 0, Batch 849, LR 0.151600 Loss 19.065941, Accuracy 0.079%\n",
      "Epoch 0, Batch 850, LR 0.151662 Loss 19.065955, Accuracy 0.080%\n",
      "Epoch 0, Batch 851, LR 0.151725 Loss 19.066158, Accuracy 0.080%\n",
      "Epoch 0, Batch 852, LR 0.151788 Loss 19.066190, Accuracy 0.080%\n",
      "Epoch 0, Batch 853, LR 0.151850 Loss 19.066283, Accuracy 0.080%\n",
      "Epoch 0, Batch 854, LR 0.151913 Loss 19.066333, Accuracy 0.080%\n",
      "Epoch 0, Batch 855, LR 0.151976 Loss 19.066451, Accuracy 0.079%\n",
      "Epoch 0, Batch 856, LR 0.152039 Loss 19.066302, Accuracy 0.079%\n",
      "Epoch 0, Batch 857, LR 0.152102 Loss 19.066230, Accuracy 0.079%\n",
      "Epoch 0, Batch 858, LR 0.152165 Loss 19.066344, Accuracy 0.079%\n",
      "Epoch 0, Batch 859, LR 0.152229 Loss 19.066306, Accuracy 0.079%\n",
      "Epoch 0, Batch 860, LR 0.152292 Loss 19.066170, Accuracy 0.079%\n",
      "Epoch 0, Batch 861, LR 0.152355 Loss 19.066020, Accuracy 0.080%\n",
      "Epoch 0, Batch 862, LR 0.152419 Loss 19.065894, Accuracy 0.080%\n",
      "Epoch 0, Batch 863, LR 0.152482 Loss 19.065826, Accuracy 0.080%\n",
      "Epoch 0, Batch 864, LR 0.152546 Loss 19.065739, Accuracy 0.080%\n",
      "Epoch 0, Batch 865, LR 0.152609 Loss 19.065672, Accuracy 0.079%\n",
      "Epoch 0, Batch 866, LR 0.152673 Loss 19.065671, Accuracy 0.079%\n",
      "Epoch 0, Batch 867, LR 0.152737 Loss 19.065470, Accuracy 0.079%\n",
      "Epoch 0, Batch 868, LR 0.152801 Loss 19.065327, Accuracy 0.079%\n",
      "Epoch 0, Batch 869, LR 0.152864 Loss 19.065522, Accuracy 0.079%\n",
      "Epoch 0, Batch 870, LR 0.152928 Loss 19.065472, Accuracy 0.079%\n",
      "Epoch 0, Batch 871, LR 0.152992 Loss 19.065365, Accuracy 0.079%\n",
      "Epoch 0, Batch 872, LR 0.153057 Loss 19.065588, Accuracy 0.079%\n",
      "Epoch 0, Batch 873, LR 0.153121 Loss 19.065506, Accuracy 0.079%\n",
      "Epoch 0, Batch 874, LR 0.153185 Loss 19.065413, Accuracy 0.079%\n",
      "Epoch 0, Batch 875, LR 0.153249 Loss 19.065538, Accuracy 0.079%\n",
      "Epoch 0, Batch 876, LR 0.153314 Loss 19.065466, Accuracy 0.078%\n",
      "Epoch 0, Batch 877, LR 0.153378 Loss 19.065855, Accuracy 0.079%\n",
      "Epoch 0, Batch 878, LR 0.153443 Loss 19.065740, Accuracy 0.079%\n",
      "Epoch 0, Batch 879, LR 0.153508 Loss 19.065742, Accuracy 0.079%\n",
      "Epoch 0, Batch 880, LR 0.153572 Loss 19.065792, Accuracy 0.079%\n",
      "Epoch 0, Batch 881, LR 0.153637 Loss 19.065416, Accuracy 0.079%\n",
      "Epoch 0, Batch 882, LR 0.153702 Loss 19.065345, Accuracy 0.079%\n",
      "Epoch 0, Batch 883, LR 0.153767 Loss 19.065272, Accuracy 0.079%\n",
      "Epoch 0, Batch 884, LR 0.153832 Loss 19.065189, Accuracy 0.079%\n",
      "Epoch 0, Batch 885, LR 0.153897 Loss 19.065399, Accuracy 0.079%\n",
      "Epoch 0, Batch 886, LR 0.153962 Loss 19.065400, Accuracy 0.078%\n",
      "Epoch 0, Batch 887, LR 0.154027 Loss 19.065297, Accuracy 0.078%\n",
      "Epoch 0, Batch 888, LR 0.154093 Loss 19.065335, Accuracy 0.078%\n",
      "Epoch 0, Batch 889, LR 0.154158 Loss 19.065616, Accuracy 0.078%\n",
      "Epoch 0, Batch 890, LR 0.154223 Loss 19.065866, Accuracy 0.078%\n",
      "Epoch 0, Batch 891, LR 0.154289 Loss 19.065952, Accuracy 0.078%\n",
      "Epoch 0, Batch 892, LR 0.154354 Loss 19.066126, Accuracy 0.078%\n",
      "Epoch 0, Batch 893, LR 0.154420 Loss 19.066279, Accuracy 0.078%\n",
      "Epoch 0, Batch 894, LR 0.154486 Loss 19.066372, Accuracy 0.078%\n",
      "Epoch 0, Batch 895, LR 0.154552 Loss 19.066363, Accuracy 0.078%\n",
      "Epoch 0, Batch 896, LR 0.154618 Loss 19.066334, Accuracy 0.078%\n",
      "Epoch 0, Batch 897, LR 0.154683 Loss 19.066342, Accuracy 0.078%\n",
      "Epoch 0, Batch 898, LR 0.154750 Loss 19.066382, Accuracy 0.077%\n",
      "Epoch 0, Batch 899, LR 0.154816 Loss 19.066248, Accuracy 0.077%\n",
      "Epoch 0, Batch 900, LR 0.154882 Loss 19.066084, Accuracy 0.077%\n",
      "Epoch 0, Batch 901, LR 0.154948 Loss 19.066061, Accuracy 0.077%\n",
      "Epoch 0, Batch 902, LR 0.155014 Loss 19.066071, Accuracy 0.077%\n",
      "Epoch 0, Batch 903, LR 0.155081 Loss 19.065858, Accuracy 0.078%\n",
      "Epoch 0, Batch 904, LR 0.155147 Loss 19.065862, Accuracy 0.078%\n",
      "Epoch 0, Batch 905, LR 0.155214 Loss 19.065574, Accuracy 0.078%\n",
      "Epoch 0, Batch 906, LR 0.155280 Loss 19.065671, Accuracy 0.078%\n",
      "Epoch 0, Batch 907, LR 0.155347 Loss 19.065563, Accuracy 0.078%\n",
      "Epoch 0, Batch 908, LR 0.155414 Loss 19.065814, Accuracy 0.078%\n",
      "Epoch 0, Batch 909, LR 0.155480 Loss 19.065923, Accuracy 0.078%\n",
      "Epoch 0, Batch 910, LR 0.155547 Loss 19.065983, Accuracy 0.078%\n",
      "Epoch 0, Batch 911, LR 0.155614 Loss 19.066094, Accuracy 0.079%\n",
      "Epoch 0, Batch 912, LR 0.155681 Loss 19.066178, Accuracy 0.079%\n",
      "Epoch 0, Batch 913, LR 0.155748 Loss 19.066165, Accuracy 0.079%\n",
      "Epoch 0, Batch 914, LR 0.155816 Loss 19.066109, Accuracy 0.079%\n",
      "Epoch 0, Batch 915, LR 0.155883 Loss 19.065943, Accuracy 0.079%\n",
      "Epoch 0, Batch 916, LR 0.155950 Loss 19.066042, Accuracy 0.078%\n",
      "Epoch 0, Batch 917, LR 0.156018 Loss 19.065839, Accuracy 0.078%\n",
      "Epoch 0, Batch 918, LR 0.156085 Loss 19.065607, Accuracy 0.078%\n",
      "Epoch 0, Batch 919, LR 0.156153 Loss 19.065863, Accuracy 0.078%\n",
      "Epoch 0, Batch 920, LR 0.156220 Loss 19.065669, Accuracy 0.078%\n",
      "Epoch 0, Batch 921, LR 0.156288 Loss 19.065532, Accuracy 0.078%\n",
      "Epoch 0, Batch 922, LR 0.156356 Loss 19.065445, Accuracy 0.078%\n",
      "Epoch 0, Batch 923, LR 0.156423 Loss 19.065479, Accuracy 0.078%\n",
      "Epoch 0, Batch 924, LR 0.156491 Loss 19.065678, Accuracy 0.078%\n",
      "Epoch 0, Batch 925, LR 0.156559 Loss 19.065609, Accuracy 0.079%\n",
      "Epoch 0, Batch 926, LR 0.156627 Loss 19.065593, Accuracy 0.078%\n",
      "Epoch 0, Batch 927, LR 0.156695 Loss 19.065491, Accuracy 0.078%\n",
      "Epoch 0, Batch 928, LR 0.156764 Loss 19.065397, Accuracy 0.078%\n",
      "Epoch 0, Batch 929, LR 0.156832 Loss 19.065452, Accuracy 0.078%\n",
      "Epoch 0, Batch 930, LR 0.156900 Loss 19.065352, Accuracy 0.079%\n",
      "Epoch 0, Batch 931, LR 0.156969 Loss 19.065333, Accuracy 0.079%\n",
      "Epoch 0, Batch 932, LR 0.157037 Loss 19.065385, Accuracy 0.079%\n",
      "Epoch 0, Batch 933, LR 0.157106 Loss 19.065565, Accuracy 0.079%\n",
      "Epoch 0, Batch 934, LR 0.157174 Loss 19.065715, Accuracy 0.079%\n",
      "Epoch 0, Batch 935, LR 0.157243 Loss 19.065624, Accuracy 0.079%\n",
      "Epoch 0, Batch 936, LR 0.157312 Loss 19.065761, Accuracy 0.078%\n",
      "Epoch 0, Batch 937, LR 0.157381 Loss 19.065695, Accuracy 0.078%\n",
      "Epoch 0, Batch 938, LR 0.157449 Loss 19.065607, Accuracy 0.078%\n",
      "Epoch 0, Batch 939, LR 0.157518 Loss 19.065676, Accuracy 0.078%\n",
      "Epoch 0, Batch 940, LR 0.157588 Loss 19.065719, Accuracy 0.078%\n",
      "Epoch 0, Batch 941, LR 0.157657 Loss 19.065535, Accuracy 0.078%\n",
      "Epoch 0, Batch 942, LR 0.157726 Loss 19.065651, Accuracy 0.078%\n",
      "Epoch 0, Batch 943, LR 0.157795 Loss 19.065673, Accuracy 0.078%\n",
      "Epoch 0, Batch 944, LR 0.157864 Loss 19.065543, Accuracy 0.078%\n",
      "Epoch 0, Batch 945, LR 0.157934 Loss 19.065461, Accuracy 0.078%\n",
      "Epoch 0, Batch 946, LR 0.158003 Loss 19.065414, Accuracy 0.078%\n",
      "Epoch 0, Batch 947, LR 0.158073 Loss 19.065446, Accuracy 0.078%\n",
      "Epoch 0, Batch 948, LR 0.158143 Loss 19.065403, Accuracy 0.077%\n",
      "Epoch 0, Batch 949, LR 0.158212 Loss 19.065233, Accuracy 0.077%\n",
      "Epoch 0, Batch 950, LR 0.158282 Loss 19.065303, Accuracy 0.078%\n",
      "Epoch 0, Batch 951, LR 0.158352 Loss 19.065267, Accuracy 0.078%\n",
      "Epoch 0, Batch 952, LR 0.158422 Loss 19.065106, Accuracy 0.078%\n",
      "Epoch 0, Batch 953, LR 0.158492 Loss 19.064964, Accuracy 0.079%\n",
      "Epoch 0, Batch 954, LR 0.158562 Loss 19.065061, Accuracy 0.079%\n",
      "Epoch 0, Batch 955, LR 0.158632 Loss 19.065100, Accuracy 0.079%\n",
      "Epoch 0, Batch 956, LR 0.158702 Loss 19.065233, Accuracy 0.078%\n",
      "Epoch 0, Batch 957, LR 0.158772 Loss 19.065177, Accuracy 0.079%\n",
      "Epoch 0, Batch 958, LR 0.158843 Loss 19.065096, Accuracy 0.079%\n",
      "Epoch 0, Batch 959, LR 0.158913 Loss 19.065105, Accuracy 0.079%\n",
      "Epoch 0, Batch 960, LR 0.158984 Loss 19.065095, Accuracy 0.079%\n",
      "Epoch 0, Batch 961, LR 0.159054 Loss 19.065015, Accuracy 0.079%\n",
      "Epoch 0, Batch 962, LR 0.159125 Loss 19.064937, Accuracy 0.079%\n",
      "Epoch 0, Batch 963, LR 0.159196 Loss 19.064891, Accuracy 0.079%\n",
      "Epoch 0, Batch 964, LR 0.159266 Loss 19.065010, Accuracy 0.079%\n",
      "Epoch 0, Batch 965, LR 0.159337 Loss 19.064917, Accuracy 0.079%\n",
      "Epoch 0, Batch 966, LR 0.159408 Loss 19.064941, Accuracy 0.079%\n",
      "Epoch 0, Batch 967, LR 0.159479 Loss 19.064894, Accuracy 0.079%\n",
      "Epoch 0, Batch 968, LR 0.159550 Loss 19.065220, Accuracy 0.079%\n",
      "Epoch 0, Batch 969, LR 0.159621 Loss 19.065245, Accuracy 0.079%\n",
      "Epoch 0, Batch 970, LR 0.159693 Loss 19.065224, Accuracy 0.079%\n",
      "Epoch 0, Batch 971, LR 0.159764 Loss 19.065195, Accuracy 0.079%\n",
      "Epoch 0, Batch 972, LR 0.159835 Loss 19.065283, Accuracy 0.080%\n",
      "Epoch 0, Batch 973, LR 0.159907 Loss 19.065200, Accuracy 0.079%\n",
      "Epoch 0, Batch 974, LR 0.159978 Loss 19.065195, Accuracy 0.079%\n",
      "Epoch 0, Batch 975, LR 0.160050 Loss 19.065114, Accuracy 0.079%\n",
      "Epoch 0, Batch 976, LR 0.160122 Loss 19.065009, Accuracy 0.079%\n",
      "Epoch 0, Batch 977, LR 0.160193 Loss 19.064936, Accuracy 0.079%\n",
      "Epoch 0, Batch 978, LR 0.160265 Loss 19.064981, Accuracy 0.079%\n",
      "Epoch 0, Batch 979, LR 0.160337 Loss 19.064975, Accuracy 0.079%\n",
      "Epoch 0, Batch 980, LR 0.160409 Loss 19.065076, Accuracy 0.079%\n",
      "Epoch 0, Batch 981, LR 0.160481 Loss 19.064974, Accuracy 0.079%\n",
      "Epoch 0, Batch 982, LR 0.160553 Loss 19.065015, Accuracy 0.079%\n",
      "Epoch 0, Batch 983, LR 0.160625 Loss 19.065050, Accuracy 0.079%\n",
      "Epoch 0, Batch 984, LR 0.160697 Loss 19.065143, Accuracy 0.079%\n",
      "Epoch 0, Batch 985, LR 0.160770 Loss 19.065016, Accuracy 0.079%\n",
      "Epoch 0, Batch 986, LR 0.160842 Loss 19.064962, Accuracy 0.078%\n",
      "Epoch 0, Batch 987, LR 0.160914 Loss 19.064924, Accuracy 0.078%\n",
      "Epoch 0, Batch 988, LR 0.160987 Loss 19.064987, Accuracy 0.078%\n",
      "Epoch 0, Batch 989, LR 0.161060 Loss 19.065270, Accuracy 0.078%\n",
      "Epoch 0, Batch 990, LR 0.161132 Loss 19.065331, Accuracy 0.078%\n",
      "Epoch 0, Batch 991, LR 0.161205 Loss 19.065344, Accuracy 0.078%\n",
      "Epoch 0, Batch 992, LR 0.161278 Loss 19.065413, Accuracy 0.078%\n",
      "Epoch 0, Batch 993, LR 0.161351 Loss 19.065310, Accuracy 0.079%\n",
      "Epoch 0, Batch 994, LR 0.161424 Loss 19.065462, Accuracy 0.079%\n",
      "Epoch 0, Batch 995, LR 0.161497 Loss 19.065377, Accuracy 0.079%\n",
      "Epoch 0, Batch 996, LR 0.161570 Loss 19.065281, Accuracy 0.078%\n",
      "Epoch 0, Batch 997, LR 0.161643 Loss 19.065187, Accuracy 0.078%\n",
      "Epoch 0, Batch 998, LR 0.161716 Loss 19.064985, Accuracy 0.078%\n",
      "Epoch 0, Batch 999, LR 0.161789 Loss 19.064997, Accuracy 0.079%\n",
      "Epoch 0, Batch 1000, LR 0.161863 Loss 19.064790, Accuracy 0.079%\n",
      "Epoch 0, Batch 1001, LR 0.161936 Loss 19.064958, Accuracy 0.079%\n",
      "Epoch 0, Batch 1002, LR 0.162010 Loss 19.065022, Accuracy 0.079%\n",
      "Epoch 0, Batch 1003, LR 0.162083 Loss 19.064896, Accuracy 0.079%\n",
      "Epoch 0, Batch 1004, LR 0.162157 Loss 19.064858, Accuracy 0.079%\n",
      "Epoch 0, Batch 1005, LR 0.162231 Loss 19.064843, Accuracy 0.079%\n",
      "Epoch 0, Batch 1006, LR 0.162305 Loss 19.064823, Accuracy 0.079%\n",
      "Epoch 0, Batch 1007, LR 0.162378 Loss 19.064816, Accuracy 0.079%\n",
      "Epoch 0, Batch 1008, LR 0.162452 Loss 19.064935, Accuracy 0.079%\n",
      "Epoch 0, Batch 1009, LR 0.162526 Loss 19.064899, Accuracy 0.079%\n",
      "Epoch 0, Batch 1010, LR 0.162601 Loss 19.064977, Accuracy 0.079%\n",
      "Epoch 0, Batch 1011, LR 0.162675 Loss 19.064965, Accuracy 0.079%\n",
      "Epoch 0, Batch 1012, LR 0.162749 Loss 19.064980, Accuracy 0.080%\n",
      "Epoch 0, Batch 1013, LR 0.162823 Loss 19.065281, Accuracy 0.079%\n",
      "Epoch 0, Batch 1014, LR 0.162898 Loss 19.065429, Accuracy 0.079%\n",
      "Epoch 0, Batch 1015, LR 0.162972 Loss 19.065561, Accuracy 0.079%\n",
      "Epoch 0, Batch 1016, LR 0.163047 Loss 19.065559, Accuracy 0.079%\n",
      "Epoch 0, Batch 1017, LR 0.163121 Loss 19.065675, Accuracy 0.079%\n",
      "Epoch 0, Batch 1018, LR 0.163196 Loss 19.065582, Accuracy 0.079%\n",
      "Epoch 0, Batch 1019, LR 0.163271 Loss 19.065453, Accuracy 0.079%\n",
      "Epoch 0, Batch 1020, LR 0.163345 Loss 19.065616, Accuracy 0.079%\n",
      "Epoch 0, Batch 1021, LR 0.163420 Loss 19.065685, Accuracy 0.079%\n",
      "Epoch 0, Batch 1022, LR 0.163495 Loss 19.065630, Accuracy 0.079%\n",
      "Epoch 0, Batch 1023, LR 0.163570 Loss 19.065717, Accuracy 0.079%\n",
      "Epoch 0, Batch 1024, LR 0.163645 Loss 19.065678, Accuracy 0.079%\n",
      "Epoch 0, Batch 1025, LR 0.163721 Loss 19.065701, Accuracy 0.079%\n",
      "Epoch 0, Batch 1026, LR 0.163796 Loss 19.065714, Accuracy 0.078%\n",
      "Epoch 0, Batch 1027, LR 0.163871 Loss 19.065845, Accuracy 0.078%\n",
      "Epoch 0, Batch 1028, LR 0.163947 Loss 19.065799, Accuracy 0.078%\n",
      "Epoch 0, Batch 1029, LR 0.164022 Loss 19.065687, Accuracy 0.078%\n",
      "Epoch 0, Batch 1030, LR 0.164098 Loss 19.065679, Accuracy 0.078%\n",
      "Epoch 0, Batch 1031, LR 0.164173 Loss 19.065454, Accuracy 0.078%\n",
      "Epoch 0, Batch 1032, LR 0.164249 Loss 19.065399, Accuracy 0.078%\n",
      "Epoch 0, Batch 1033, LR 0.164325 Loss 19.065655, Accuracy 0.078%\n",
      "Epoch 0, Batch 1034, LR 0.164400 Loss 19.065849, Accuracy 0.078%\n",
      "Epoch 0, Batch 1035, LR 0.164476 Loss 19.065745, Accuracy 0.078%\n",
      "Epoch 0, Batch 1036, LR 0.164552 Loss 19.065826, Accuracy 0.078%\n",
      "Epoch 0, Batch 1037, LR 0.164628 Loss 19.065887, Accuracy 0.078%\n",
      "Epoch 0, Batch 1038, LR 0.164704 Loss 19.065760, Accuracy 0.079%\n",
      "Epoch 0, Batch 1039, LR 0.164781 Loss 19.065632, Accuracy 0.079%\n",
      "Epoch 0, Batch 1040, LR 0.164857 Loss 19.065673, Accuracy 0.079%\n",
      "Epoch 0, Batch 1041, LR 0.164933 Loss 19.065826, Accuracy 0.079%\n",
      "Epoch 0, Batch 1042, LR 0.165010 Loss 19.065728, Accuracy 0.079%\n",
      "Epoch 0, Batch 1043, LR 0.165086 Loss 19.065841, Accuracy 0.079%\n",
      "Epoch 0, Batch 1044, LR 0.165163 Loss 19.065939, Accuracy 0.079%\n",
      "Epoch 0, Batch 1045, LR 0.165239 Loss 19.066067, Accuracy 0.079%\n",
      "Epoch 0, Batch 1046, LR 0.165316 Loss 19.066242, Accuracy 0.079%\n",
      "Epoch 0, Batch 1047, LR 0.165393 Loss 19.066244, Accuracy 0.079%\n",
      "Epoch 0, Loss (train set) 19.066244, Accuracy (train set) 0.079%\n",
      "Epoch 1, Batch 1, LR 0.165469 Loss 19.114737, Accuracy 0.781%\n",
      "Epoch 1, Batch 2, LR 0.165546 Loss 19.126493, Accuracy 0.391%\n",
      "Epoch 1, Batch 3, LR 0.165623 Loss 19.091558, Accuracy 0.260%\n",
      "Epoch 1, Batch 4, LR 0.165700 Loss 19.111817, Accuracy 0.391%\n",
      "Epoch 1, Batch 5, LR 0.165778 Loss 19.080585, Accuracy 0.312%\n",
      "Epoch 1, Batch 6, LR 0.165855 Loss 19.053617, Accuracy 0.260%\n",
      "Epoch 1, Batch 7, LR 0.165932 Loss 19.044785, Accuracy 0.335%\n",
      "Epoch 1, Batch 8, LR 0.166009 Loss 19.049443, Accuracy 0.391%\n",
      "Epoch 1, Batch 9, LR 0.166087 Loss 19.041634, Accuracy 0.347%\n",
      "Epoch 1, Batch 10, LR 0.166164 Loss 19.054387, Accuracy 0.312%\n",
      "Epoch 1, Batch 11, LR 0.166242 Loss 19.058519, Accuracy 0.284%\n",
      "Epoch 1, Batch 12, LR 0.166319 Loss 19.057724, Accuracy 0.260%\n",
      "Epoch 1, Batch 13, LR 0.166397 Loss 19.054728, Accuracy 0.300%\n",
      "Epoch 1, Batch 14, LR 0.166475 Loss 19.070698, Accuracy 0.279%\n",
      "Epoch 1, Batch 15, LR 0.166553 Loss 19.062156, Accuracy 0.260%\n",
      "Epoch 1, Batch 16, LR 0.166631 Loss 19.060950, Accuracy 0.244%\n",
      "Epoch 1, Batch 17, LR 0.166708 Loss 19.051988, Accuracy 0.230%\n",
      "Epoch 1, Batch 18, LR 0.166787 Loss 19.049651, Accuracy 0.217%\n",
      "Epoch 1, Batch 19, LR 0.166865 Loss 19.057199, Accuracy 0.206%\n",
      "Epoch 1, Batch 20, LR 0.166943 Loss 19.050528, Accuracy 0.195%\n",
      "Epoch 1, Batch 21, LR 0.167021 Loss 19.046191, Accuracy 0.223%\n",
      "Epoch 1, Batch 22, LR 0.167099 Loss 19.044953, Accuracy 0.213%\n",
      "Epoch 1, Batch 23, LR 0.167178 Loss 19.036346, Accuracy 0.204%\n",
      "Epoch 1, Batch 24, LR 0.167256 Loss 19.036905, Accuracy 0.195%\n",
      "Epoch 1, Batch 25, LR 0.167335 Loss 19.037309, Accuracy 0.188%\n",
      "Epoch 1, Batch 26, LR 0.167413 Loss 19.030066, Accuracy 0.180%\n",
      "Epoch 1, Batch 27, LR 0.167492 Loss 19.031272, Accuracy 0.174%\n",
      "Epoch 1, Batch 28, LR 0.167571 Loss 19.032406, Accuracy 0.167%\n",
      "Epoch 1, Batch 29, LR 0.167650 Loss 19.037695, Accuracy 0.162%\n",
      "Epoch 1, Batch 30, LR 0.167729 Loss 19.042262, Accuracy 0.156%\n",
      "Epoch 1, Batch 31, LR 0.167808 Loss 19.046139, Accuracy 0.151%\n",
      "Epoch 1, Batch 32, LR 0.167887 Loss 19.047948, Accuracy 0.146%\n",
      "Epoch 1, Batch 33, LR 0.167966 Loss 19.055222, Accuracy 0.142%\n",
      "Epoch 1, Batch 34, LR 0.168045 Loss 19.051121, Accuracy 0.138%\n",
      "Epoch 1, Batch 35, LR 0.168124 Loss 19.054803, Accuracy 0.134%\n",
      "Epoch 1, Batch 36, LR 0.168204 Loss 19.056417, Accuracy 0.130%\n",
      "Epoch 1, Batch 37, LR 0.168283 Loss 19.058926, Accuracy 0.127%\n",
      "Epoch 1, Batch 38, LR 0.168362 Loss 19.057734, Accuracy 0.123%\n",
      "Epoch 1, Batch 39, LR 0.168442 Loss 19.063964, Accuracy 0.120%\n",
      "Epoch 1, Batch 40, LR 0.168522 Loss 19.067062, Accuracy 0.117%\n",
      "Epoch 1, Batch 41, LR 0.168601 Loss 19.068184, Accuracy 0.114%\n",
      "Epoch 1, Batch 42, LR 0.168681 Loss 19.067077, Accuracy 0.130%\n",
      "Epoch 1, Batch 43, LR 0.168761 Loss 19.072336, Accuracy 0.127%\n",
      "Epoch 1, Batch 44, LR 0.168841 Loss 19.071412, Accuracy 0.124%\n",
      "Epoch 1, Batch 45, LR 0.168921 Loss 19.067953, Accuracy 0.122%\n",
      "Epoch 1, Batch 46, LR 0.169001 Loss 19.066874, Accuracy 0.119%\n",
      "Epoch 1, Batch 47, LR 0.169081 Loss 19.071237, Accuracy 0.116%\n",
      "Epoch 1, Batch 48, LR 0.169161 Loss 19.069469, Accuracy 0.114%\n",
      "Epoch 1, Batch 49, LR 0.169241 Loss 19.067080, Accuracy 0.112%\n",
      "Epoch 1, Batch 50, LR 0.169322 Loss 19.068489, Accuracy 0.109%\n",
      "Epoch 1, Batch 51, LR 0.169402 Loss 19.067843, Accuracy 0.107%\n",
      "Epoch 1, Batch 52, LR 0.169483 Loss 19.063971, Accuracy 0.105%\n",
      "Epoch 1, Batch 53, LR 0.169563 Loss 19.062349, Accuracy 0.103%\n",
      "Epoch 1, Batch 54, LR 0.169644 Loss 19.061819, Accuracy 0.116%\n",
      "Epoch 1, Batch 55, LR 0.169725 Loss 19.061051, Accuracy 0.114%\n",
      "Epoch 1, Batch 56, LR 0.169805 Loss 19.062996, Accuracy 0.112%\n",
      "Epoch 1, Batch 57, LR 0.169886 Loss 19.059920, Accuracy 0.110%\n",
      "Epoch 1, Batch 58, LR 0.169967 Loss 19.057438, Accuracy 0.108%\n",
      "Epoch 1, Batch 59, LR 0.170048 Loss 19.058697, Accuracy 0.106%\n",
      "Epoch 1, Batch 60, LR 0.170129 Loss 19.059640, Accuracy 0.104%\n",
      "Epoch 1, Batch 61, LR 0.170210 Loss 19.058956, Accuracy 0.102%\n",
      "Epoch 1, Batch 62, LR 0.170291 Loss 19.058709, Accuracy 0.101%\n",
      "Epoch 1, Batch 63, LR 0.170373 Loss 19.063401, Accuracy 0.099%\n",
      "Epoch 1, Batch 64, LR 0.170454 Loss 19.066662, Accuracy 0.098%\n",
      "Epoch 1, Batch 65, LR 0.170535 Loss 19.068786, Accuracy 0.096%\n",
      "Epoch 1, Batch 66, LR 0.170617 Loss 19.067127, Accuracy 0.095%\n",
      "Epoch 1, Batch 67, LR 0.170698 Loss 19.063766, Accuracy 0.105%\n",
      "Epoch 1, Batch 68, LR 0.170780 Loss 19.061649, Accuracy 0.103%\n",
      "Epoch 1, Batch 69, LR 0.170862 Loss 19.061199, Accuracy 0.102%\n",
      "Epoch 1, Batch 70, LR 0.170944 Loss 19.061225, Accuracy 0.100%\n",
      "Epoch 1, Batch 71, LR 0.171025 Loss 19.061921, Accuracy 0.110%\n",
      "Epoch 1, Batch 72, LR 0.171107 Loss 19.064577, Accuracy 0.109%\n",
      "Epoch 1, Batch 73, LR 0.171189 Loss 19.063723, Accuracy 0.107%\n",
      "Epoch 1, Batch 74, LR 0.171271 Loss 19.060958, Accuracy 0.106%\n",
      "Epoch 1, Batch 75, LR 0.171353 Loss 19.060151, Accuracy 0.104%\n",
      "Epoch 1, Batch 76, LR 0.171436 Loss 19.054427, Accuracy 0.103%\n",
      "Epoch 1, Batch 77, LR 0.171518 Loss 19.053767, Accuracy 0.101%\n",
      "Epoch 1, Batch 78, LR 0.171600 Loss 19.053785, Accuracy 0.100%\n",
      "Epoch 1, Batch 79, LR 0.171683 Loss 19.053138, Accuracy 0.099%\n",
      "Epoch 1, Batch 80, LR 0.171765 Loss 19.053733, Accuracy 0.098%\n",
      "Epoch 1, Batch 81, LR 0.171848 Loss 19.051769, Accuracy 0.096%\n",
      "Epoch 1, Batch 82, LR 0.171930 Loss 19.051223, Accuracy 0.095%\n",
      "Epoch 1, Batch 83, LR 0.172013 Loss 19.051933, Accuracy 0.104%\n",
      "Epoch 1, Batch 84, LR 0.172096 Loss 19.053357, Accuracy 0.102%\n",
      "Epoch 1, Batch 85, LR 0.172179 Loss 19.054264, Accuracy 0.101%\n",
      "Epoch 1, Batch 86, LR 0.172261 Loss 19.055295, Accuracy 0.100%\n",
      "Epoch 1, Batch 87, LR 0.172344 Loss 19.054057, Accuracy 0.099%\n",
      "Epoch 1, Batch 88, LR 0.172428 Loss 19.057011, Accuracy 0.098%\n",
      "Epoch 1, Batch 89, LR 0.172511 Loss 19.055320, Accuracy 0.097%\n",
      "Epoch 1, Batch 90, LR 0.172594 Loss 19.055526, Accuracy 0.095%\n",
      "Epoch 1, Batch 91, LR 0.172677 Loss 19.057004, Accuracy 0.094%\n",
      "Epoch 1, Batch 92, LR 0.172760 Loss 19.056157, Accuracy 0.093%\n",
      "Epoch 1, Batch 93, LR 0.172844 Loss 19.055060, Accuracy 0.092%\n",
      "Epoch 1, Batch 94, LR 0.172927 Loss 19.055225, Accuracy 0.091%\n",
      "Epoch 1, Batch 95, LR 0.173011 Loss 19.053157, Accuracy 0.090%\n",
      "Epoch 1, Batch 96, LR 0.173094 Loss 19.050153, Accuracy 0.090%\n",
      "Epoch 1, Batch 97, LR 0.173178 Loss 19.051164, Accuracy 0.089%\n",
      "Epoch 1, Batch 98, LR 0.173262 Loss 19.052565, Accuracy 0.088%\n",
      "Epoch 1, Batch 99, LR 0.173346 Loss 19.053916, Accuracy 0.087%\n",
      "Epoch 1, Batch 100, LR 0.173430 Loss 19.053036, Accuracy 0.094%\n",
      "Epoch 1, Batch 101, LR 0.173514 Loss 19.055350, Accuracy 0.093%\n",
      "Epoch 1, Batch 102, LR 0.173598 Loss 19.055503, Accuracy 0.092%\n",
      "Epoch 1, Batch 103, LR 0.173682 Loss 19.057016, Accuracy 0.091%\n",
      "Epoch 1, Batch 104, LR 0.173766 Loss 19.056882, Accuracy 0.090%\n",
      "Epoch 1, Batch 105, LR 0.173850 Loss 19.057844, Accuracy 0.089%\n",
      "Epoch 1, Batch 106, LR 0.173935 Loss 19.058052, Accuracy 0.088%\n",
      "Epoch 1, Batch 107, LR 0.174019 Loss 19.058364, Accuracy 0.088%\n",
      "Epoch 1, Batch 108, LR 0.174103 Loss 19.058829, Accuracy 0.087%\n",
      "Epoch 1, Batch 109, LR 0.174188 Loss 19.058605, Accuracy 0.086%\n",
      "Epoch 1, Batch 110, LR 0.174273 Loss 19.059905, Accuracy 0.085%\n",
      "Epoch 1, Batch 111, LR 0.174357 Loss 19.058497, Accuracy 0.091%\n",
      "Epoch 1, Batch 112, LR 0.174442 Loss 19.057766, Accuracy 0.091%\n",
      "Epoch 1, Batch 113, LR 0.174527 Loss 19.056875, Accuracy 0.090%\n",
      "Epoch 1, Batch 114, LR 0.174612 Loss 19.056966, Accuracy 0.089%\n",
      "Epoch 1, Batch 115, LR 0.174697 Loss 19.058035, Accuracy 0.088%\n",
      "Epoch 1, Batch 116, LR 0.174782 Loss 19.057262, Accuracy 0.088%\n",
      "Epoch 1, Batch 117, LR 0.174867 Loss 19.056604, Accuracy 0.087%\n",
      "Epoch 1, Batch 118, LR 0.174952 Loss 19.056533, Accuracy 0.086%\n",
      "Epoch 1, Batch 119, LR 0.175037 Loss 19.057678, Accuracy 0.085%\n",
      "Epoch 1, Batch 120, LR 0.175123 Loss 19.059001, Accuracy 0.085%\n",
      "Epoch 1, Batch 121, LR 0.175208 Loss 19.057988, Accuracy 0.084%\n",
      "Epoch 1, Batch 122, LR 0.175294 Loss 19.057399, Accuracy 0.083%\n",
      "Epoch 1, Batch 123, LR 0.175379 Loss 19.056076, Accuracy 0.083%\n",
      "Epoch 1, Batch 124, LR 0.175465 Loss 19.055626, Accuracy 0.082%\n",
      "Epoch 1, Batch 125, LR 0.175550 Loss 19.057407, Accuracy 0.081%\n",
      "Epoch 1, Batch 126, LR 0.175636 Loss 19.056377, Accuracy 0.081%\n",
      "Epoch 1, Batch 127, LR 0.175722 Loss 19.056893, Accuracy 0.080%\n",
      "Epoch 1, Batch 128, LR 0.175808 Loss 19.058272, Accuracy 0.079%\n",
      "Epoch 1, Batch 129, LR 0.175894 Loss 19.057615, Accuracy 0.079%\n",
      "Epoch 1, Batch 130, LR 0.175980 Loss 19.058963, Accuracy 0.078%\n",
      "Epoch 1, Batch 131, LR 0.176066 Loss 19.060985, Accuracy 0.078%\n",
      "Epoch 1, Batch 132, LR 0.176152 Loss 19.060306, Accuracy 0.077%\n",
      "Epoch 1, Batch 133, LR 0.176238 Loss 19.060164, Accuracy 0.076%\n",
      "Epoch 1, Batch 134, LR 0.176325 Loss 19.059438, Accuracy 0.076%\n",
      "Epoch 1, Batch 135, LR 0.176411 Loss 19.059505, Accuracy 0.075%\n",
      "Epoch 1, Batch 136, LR 0.176498 Loss 19.059406, Accuracy 0.075%\n",
      "Epoch 1, Batch 137, LR 0.176584 Loss 19.058941, Accuracy 0.074%\n",
      "Epoch 1, Batch 138, LR 0.176671 Loss 19.059264, Accuracy 0.074%\n",
      "Epoch 1, Batch 139, LR 0.176757 Loss 19.058508, Accuracy 0.073%\n",
      "Epoch 1, Batch 140, LR 0.176844 Loss 19.058337, Accuracy 0.073%\n",
      "Epoch 1, Batch 141, LR 0.176931 Loss 19.057509, Accuracy 0.072%\n",
      "Epoch 1, Batch 142, LR 0.177018 Loss 19.058211, Accuracy 0.072%\n",
      "Epoch 1, Batch 143, LR 0.177105 Loss 19.057990, Accuracy 0.071%\n",
      "Epoch 1, Batch 144, LR 0.177192 Loss 19.057874, Accuracy 0.071%\n",
      "Epoch 1, Batch 145, LR 0.177279 Loss 19.057951, Accuracy 0.070%\n",
      "Epoch 1, Batch 146, LR 0.177366 Loss 19.058747, Accuracy 0.070%\n",
      "Epoch 1, Batch 147, LR 0.177453 Loss 19.058607, Accuracy 0.069%\n",
      "Epoch 1, Batch 148, LR 0.177541 Loss 19.057223, Accuracy 0.074%\n",
      "Epoch 1, Batch 149, LR 0.177628 Loss 19.055961, Accuracy 0.073%\n",
      "Epoch 1, Batch 150, LR 0.177716 Loss 19.056696, Accuracy 0.073%\n",
      "Epoch 1, Batch 151, LR 0.177803 Loss 19.057428, Accuracy 0.072%\n",
      "Epoch 1, Batch 152, LR 0.177891 Loss 19.058044, Accuracy 0.072%\n",
      "Epoch 1, Batch 153, LR 0.177978 Loss 19.057984, Accuracy 0.071%\n",
      "Epoch 1, Batch 154, LR 0.178066 Loss 19.058240, Accuracy 0.071%\n",
      "Epoch 1, Batch 155, LR 0.178154 Loss 19.057742, Accuracy 0.071%\n",
      "Epoch 1, Batch 156, LR 0.178242 Loss 19.056697, Accuracy 0.070%\n",
      "Epoch 1, Batch 157, LR 0.178330 Loss 19.056517, Accuracy 0.070%\n",
      "Epoch 1, Batch 158, LR 0.178418 Loss 19.055330, Accuracy 0.069%\n",
      "Epoch 1, Batch 159, LR 0.178506 Loss 19.054503, Accuracy 0.069%\n",
      "Epoch 1, Batch 160, LR 0.178594 Loss 19.054846, Accuracy 0.073%\n",
      "Epoch 1, Batch 161, LR 0.178682 Loss 19.055041, Accuracy 0.073%\n",
      "Epoch 1, Batch 162, LR 0.178771 Loss 19.056487, Accuracy 0.072%\n",
      "Epoch 1, Batch 163, LR 0.178859 Loss 19.056691, Accuracy 0.072%\n",
      "Epoch 1, Batch 164, LR 0.178948 Loss 19.057607, Accuracy 0.071%\n",
      "Epoch 1, Batch 165, LR 0.179036 Loss 19.057573, Accuracy 0.071%\n",
      "Epoch 1, Batch 166, LR 0.179125 Loss 19.056484, Accuracy 0.071%\n",
      "Epoch 1, Batch 167, LR 0.179213 Loss 19.057270, Accuracy 0.070%\n",
      "Epoch 1, Batch 168, LR 0.179302 Loss 19.058008, Accuracy 0.070%\n",
      "Epoch 1, Batch 169, LR 0.179391 Loss 19.057739, Accuracy 0.069%\n",
      "Epoch 1, Batch 170, LR 0.179480 Loss 19.058351, Accuracy 0.069%\n",
      "Epoch 1, Batch 171, LR 0.179569 Loss 19.057699, Accuracy 0.069%\n",
      "Epoch 1, Batch 172, LR 0.179658 Loss 19.056840, Accuracy 0.068%\n",
      "Epoch 1, Batch 173, LR 0.179747 Loss 19.056071, Accuracy 0.068%\n",
      "Epoch 1, Batch 174, LR 0.179836 Loss 19.056027, Accuracy 0.067%\n",
      "Epoch 1, Batch 175, LR 0.179925 Loss 19.055099, Accuracy 0.067%\n",
      "Epoch 1, Batch 176, LR 0.180014 Loss 19.055223, Accuracy 0.067%\n",
      "Epoch 1, Batch 177, LR 0.180104 Loss 19.055113, Accuracy 0.066%\n",
      "Epoch 1, Batch 178, LR 0.180193 Loss 19.053949, Accuracy 0.066%\n",
      "Epoch 1, Batch 179, LR 0.180283 Loss 19.053946, Accuracy 0.065%\n",
      "Epoch 1, Batch 180, LR 0.180372 Loss 19.053760, Accuracy 0.065%\n",
      "Epoch 1, Batch 181, LR 0.180462 Loss 19.053304, Accuracy 0.065%\n",
      "Epoch 1, Batch 182, LR 0.180552 Loss 19.053362, Accuracy 0.064%\n",
      "Epoch 1, Batch 183, LR 0.180642 Loss 19.052973, Accuracy 0.064%\n",
      "Epoch 1, Batch 184, LR 0.180731 Loss 19.053688, Accuracy 0.064%\n",
      "Epoch 1, Batch 185, LR 0.180821 Loss 19.054048, Accuracy 0.063%\n",
      "Epoch 1, Batch 186, LR 0.180911 Loss 19.054030, Accuracy 0.063%\n",
      "Epoch 1, Batch 187, LR 0.181001 Loss 19.053263, Accuracy 0.063%\n",
      "Epoch 1, Batch 188, LR 0.181092 Loss 19.053405, Accuracy 0.062%\n",
      "Epoch 1, Batch 189, LR 0.181182 Loss 19.052903, Accuracy 0.062%\n",
      "Epoch 1, Batch 190, LR 0.181272 Loss 19.053209, Accuracy 0.062%\n",
      "Epoch 1, Batch 191, LR 0.181363 Loss 19.053616, Accuracy 0.061%\n",
      "Epoch 1, Batch 192, LR 0.181453 Loss 19.053893, Accuracy 0.061%\n",
      "Epoch 1, Batch 193, LR 0.181543 Loss 19.053218, Accuracy 0.061%\n",
      "Epoch 1, Batch 194, LR 0.181634 Loss 19.052759, Accuracy 0.060%\n",
      "Epoch 1, Batch 195, LR 0.181725 Loss 19.052701, Accuracy 0.060%\n",
      "Epoch 1, Batch 196, LR 0.181815 Loss 19.052191, Accuracy 0.060%\n",
      "Epoch 1, Batch 197, LR 0.181906 Loss 19.051249, Accuracy 0.059%\n",
      "Epoch 1, Batch 198, LR 0.181997 Loss 19.050886, Accuracy 0.059%\n",
      "Epoch 1, Batch 199, LR 0.182088 Loss 19.052373, Accuracy 0.059%\n",
      "Epoch 1, Batch 200, LR 0.182179 Loss 19.051751, Accuracy 0.059%\n",
      "Epoch 1, Batch 201, LR 0.182270 Loss 19.052005, Accuracy 0.062%\n",
      "Epoch 1, Batch 202, LR 0.182361 Loss 19.051559, Accuracy 0.062%\n",
      "Epoch 1, Batch 203, LR 0.182452 Loss 19.051062, Accuracy 0.062%\n",
      "Epoch 1, Batch 204, LR 0.182544 Loss 19.050807, Accuracy 0.061%\n",
      "Epoch 1, Batch 205, LR 0.182635 Loss 19.051023, Accuracy 0.061%\n",
      "Epoch 1, Batch 206, LR 0.182726 Loss 19.050514, Accuracy 0.061%\n",
      "Epoch 1, Batch 207, LR 0.182818 Loss 19.050330, Accuracy 0.060%\n",
      "Epoch 1, Batch 208, LR 0.182910 Loss 19.050489, Accuracy 0.060%\n",
      "Epoch 1, Batch 209, LR 0.183001 Loss 19.051485, Accuracy 0.060%\n",
      "Epoch 1, Batch 210, LR 0.183093 Loss 19.051621, Accuracy 0.063%\n",
      "Epoch 1, Batch 211, LR 0.183185 Loss 19.053358, Accuracy 0.063%\n",
      "Epoch 1, Batch 212, LR 0.183276 Loss 19.052693, Accuracy 0.063%\n",
      "Epoch 1, Batch 213, LR 0.183368 Loss 19.052431, Accuracy 0.062%\n",
      "Epoch 1, Batch 214, LR 0.183460 Loss 19.052726, Accuracy 0.062%\n",
      "Epoch 1, Batch 215, LR 0.183552 Loss 19.051996, Accuracy 0.069%\n",
      "Epoch 1, Batch 216, LR 0.183645 Loss 19.052261, Accuracy 0.069%\n",
      "Epoch 1, Batch 217, LR 0.183737 Loss 19.051958, Accuracy 0.068%\n",
      "Epoch 1, Batch 218, LR 0.183829 Loss 19.052030, Accuracy 0.068%\n",
      "Epoch 1, Batch 219, LR 0.183921 Loss 19.052079, Accuracy 0.068%\n",
      "Epoch 1, Batch 220, LR 0.184014 Loss 19.052269, Accuracy 0.067%\n",
      "Epoch 1, Batch 221, LR 0.184106 Loss 19.052064, Accuracy 0.067%\n",
      "Epoch 1, Batch 222, LR 0.184199 Loss 19.052094, Accuracy 0.067%\n",
      "Epoch 1, Batch 223, LR 0.184291 Loss 19.052225, Accuracy 0.067%\n",
      "Epoch 1, Batch 224, LR 0.184384 Loss 19.053416, Accuracy 0.066%\n",
      "Epoch 1, Batch 225, LR 0.184477 Loss 19.053375, Accuracy 0.066%\n",
      "Epoch 1, Batch 226, LR 0.184570 Loss 19.053455, Accuracy 0.066%\n",
      "Epoch 1, Batch 227, LR 0.184663 Loss 19.054133, Accuracy 0.065%\n",
      "Epoch 1, Batch 228, LR 0.184756 Loss 19.054280, Accuracy 0.065%\n",
      "Epoch 1, Batch 229, LR 0.184849 Loss 19.054430, Accuracy 0.065%\n",
      "Epoch 1, Batch 230, LR 0.184942 Loss 19.054663, Accuracy 0.065%\n",
      "Epoch 1, Batch 231, LR 0.185035 Loss 19.054699, Accuracy 0.064%\n",
      "Epoch 1, Batch 232, LR 0.185128 Loss 19.054095, Accuracy 0.067%\n",
      "Epoch 1, Batch 233, LR 0.185222 Loss 19.054227, Accuracy 0.067%\n",
      "Epoch 1, Batch 234, LR 0.185315 Loss 19.053718, Accuracy 0.067%\n",
      "Epoch 1, Batch 235, LR 0.185408 Loss 19.053316, Accuracy 0.066%\n",
      "Epoch 1, Batch 236, LR 0.185502 Loss 19.053326, Accuracy 0.066%\n",
      "Epoch 1, Batch 237, LR 0.185596 Loss 19.052939, Accuracy 0.066%\n",
      "Epoch 1, Batch 238, LR 0.185689 Loss 19.053256, Accuracy 0.066%\n",
      "Epoch 1, Batch 239, LR 0.185783 Loss 19.053272, Accuracy 0.065%\n",
      "Epoch 1, Batch 240, LR 0.185877 Loss 19.054216, Accuracy 0.065%\n",
      "Epoch 1, Batch 241, LR 0.185971 Loss 19.054617, Accuracy 0.065%\n",
      "Epoch 1, Batch 242, LR 0.186065 Loss 19.054287, Accuracy 0.065%\n",
      "Epoch 1, Batch 243, LR 0.186159 Loss 19.053845, Accuracy 0.064%\n",
      "Epoch 1, Batch 244, LR 0.186253 Loss 19.053577, Accuracy 0.064%\n",
      "Epoch 1, Batch 245, LR 0.186347 Loss 19.054166, Accuracy 0.064%\n",
      "Epoch 1, Batch 246, LR 0.186441 Loss 19.054268, Accuracy 0.067%\n",
      "Epoch 1, Batch 247, LR 0.186536 Loss 19.054390, Accuracy 0.066%\n",
      "Epoch 1, Batch 248, LR 0.186630 Loss 19.054589, Accuracy 0.066%\n",
      "Epoch 1, Batch 249, LR 0.186724 Loss 19.055564, Accuracy 0.066%\n",
      "Epoch 1, Batch 250, LR 0.186819 Loss 19.055178, Accuracy 0.066%\n",
      "Epoch 1, Batch 251, LR 0.186914 Loss 19.055379, Accuracy 0.065%\n",
      "Epoch 1, Batch 252, LR 0.187008 Loss 19.055471, Accuracy 0.065%\n",
      "Epoch 1, Batch 253, LR 0.187103 Loss 19.055513, Accuracy 0.065%\n",
      "Epoch 1, Batch 254, LR 0.187198 Loss 19.054735, Accuracy 0.065%\n",
      "Epoch 1, Batch 255, LR 0.187293 Loss 19.054273, Accuracy 0.067%\n",
      "Epoch 1, Batch 256, LR 0.187388 Loss 19.053907, Accuracy 0.067%\n",
      "Epoch 1, Batch 257, LR 0.187483 Loss 19.053595, Accuracy 0.067%\n",
      "Epoch 1, Batch 258, LR 0.187578 Loss 19.053638, Accuracy 0.067%\n",
      "Epoch 1, Batch 259, LR 0.187673 Loss 19.053947, Accuracy 0.066%\n",
      "Epoch 1, Batch 260, LR 0.187768 Loss 19.053932, Accuracy 0.066%\n",
      "Epoch 1, Batch 261, LR 0.187863 Loss 19.054146, Accuracy 0.066%\n",
      "Epoch 1, Batch 262, LR 0.187959 Loss 19.054089, Accuracy 0.066%\n",
      "Epoch 1, Batch 263, LR 0.188054 Loss 19.054040, Accuracy 0.065%\n",
      "Epoch 1, Batch 264, LR 0.188150 Loss 19.054016, Accuracy 0.065%\n",
      "Epoch 1, Batch 265, LR 0.188245 Loss 19.054278, Accuracy 0.068%\n",
      "Epoch 1, Batch 266, LR 0.188341 Loss 19.054190, Accuracy 0.068%\n",
      "Epoch 1, Batch 267, LR 0.188437 Loss 19.054387, Accuracy 0.067%\n",
      "Epoch 1, Batch 268, LR 0.188533 Loss 19.053626, Accuracy 0.067%\n",
      "Epoch 1, Batch 269, LR 0.188628 Loss 19.053688, Accuracy 0.067%\n",
      "Epoch 1, Batch 270, LR 0.188724 Loss 19.053002, Accuracy 0.067%\n",
      "Epoch 1, Batch 271, LR 0.188820 Loss 19.053480, Accuracy 0.066%\n",
      "Epoch 1, Batch 272, LR 0.188916 Loss 19.053521, Accuracy 0.066%\n",
      "Epoch 1, Batch 273, LR 0.189013 Loss 19.053244, Accuracy 0.069%\n",
      "Epoch 1, Batch 274, LR 0.189109 Loss 19.053455, Accuracy 0.068%\n",
      "Epoch 1, Batch 275, LR 0.189205 Loss 19.052498, Accuracy 0.068%\n",
      "Epoch 1, Batch 276, LR 0.189301 Loss 19.052330, Accuracy 0.068%\n",
      "Epoch 1, Batch 277, LR 0.189398 Loss 19.052594, Accuracy 0.068%\n",
      "Epoch 1, Batch 278, LR 0.189494 Loss 19.052844, Accuracy 0.067%\n",
      "Epoch 1, Batch 279, LR 0.189591 Loss 19.052704, Accuracy 0.067%\n",
      "Epoch 1, Batch 280, LR 0.189688 Loss 19.052768, Accuracy 0.070%\n",
      "Epoch 1, Batch 281, LR 0.189784 Loss 19.052067, Accuracy 0.070%\n",
      "Epoch 1, Batch 282, LR 0.189881 Loss 19.052019, Accuracy 0.072%\n",
      "Epoch 1, Batch 283, LR 0.189978 Loss 19.052777, Accuracy 0.072%\n",
      "Epoch 1, Batch 284, LR 0.190075 Loss 19.052671, Accuracy 0.072%\n",
      "Epoch 1, Batch 285, LR 0.190172 Loss 19.052956, Accuracy 0.074%\n",
      "Epoch 1, Batch 286, LR 0.190269 Loss 19.053235, Accuracy 0.074%\n",
      "Epoch 1, Batch 287, LR 0.190366 Loss 19.052981, Accuracy 0.076%\n",
      "Epoch 1, Batch 288, LR 0.190463 Loss 19.052722, Accuracy 0.076%\n",
      "Epoch 1, Batch 289, LR 0.190560 Loss 19.053405, Accuracy 0.076%\n",
      "Epoch 1, Batch 290, LR 0.190658 Loss 19.053241, Accuracy 0.075%\n",
      "Epoch 1, Batch 291, LR 0.190755 Loss 19.053421, Accuracy 0.075%\n",
      "Epoch 1, Batch 292, LR 0.190853 Loss 19.053255, Accuracy 0.075%\n",
      "Epoch 1, Batch 293, LR 0.190950 Loss 19.053671, Accuracy 0.075%\n",
      "Epoch 1, Batch 294, LR 0.191048 Loss 19.053386, Accuracy 0.074%\n",
      "Epoch 1, Batch 295, LR 0.191146 Loss 19.053202, Accuracy 0.074%\n",
      "Epoch 1, Batch 296, LR 0.191243 Loss 19.053283, Accuracy 0.074%\n",
      "Epoch 1, Batch 297, LR 0.191341 Loss 19.052868, Accuracy 0.074%\n",
      "Epoch 1, Batch 298, LR 0.191439 Loss 19.052523, Accuracy 0.073%\n",
      "Epoch 1, Batch 299, LR 0.191537 Loss 19.052316, Accuracy 0.073%\n",
      "Epoch 1, Batch 300, LR 0.191635 Loss 19.051759, Accuracy 0.073%\n",
      "Epoch 1, Batch 301, LR 0.191733 Loss 19.051534, Accuracy 0.073%\n",
      "Epoch 1, Batch 302, LR 0.191831 Loss 19.051290, Accuracy 0.072%\n",
      "Epoch 1, Batch 303, LR 0.191930 Loss 19.051800, Accuracy 0.072%\n",
      "Epoch 1, Batch 304, LR 0.192028 Loss 19.052171, Accuracy 0.072%\n",
      "Epoch 1, Batch 305, LR 0.192126 Loss 19.052052, Accuracy 0.074%\n",
      "Epoch 1, Batch 306, LR 0.192225 Loss 19.051965, Accuracy 0.074%\n",
      "Epoch 1, Batch 307, LR 0.192323 Loss 19.051913, Accuracy 0.074%\n",
      "Epoch 1, Batch 308, LR 0.192422 Loss 19.052438, Accuracy 0.076%\n",
      "Epoch 1, Batch 309, LR 0.192521 Loss 19.052266, Accuracy 0.076%\n",
      "Epoch 1, Batch 310, LR 0.192619 Loss 19.051305, Accuracy 0.076%\n",
      "Epoch 1, Batch 311, LR 0.192718 Loss 19.051270, Accuracy 0.075%\n",
      "Epoch 1, Batch 312, LR 0.192817 Loss 19.051208, Accuracy 0.075%\n",
      "Epoch 1, Batch 313, LR 0.192916 Loss 19.051601, Accuracy 0.075%\n",
      "Epoch 1, Batch 314, LR 0.193015 Loss 19.051153, Accuracy 0.077%\n",
      "Epoch 1, Batch 315, LR 0.193114 Loss 19.050868, Accuracy 0.077%\n",
      "Epoch 1, Batch 316, LR 0.193213 Loss 19.051177, Accuracy 0.077%\n",
      "Epoch 1, Batch 317, LR 0.193312 Loss 19.051059, Accuracy 0.076%\n",
      "Epoch 1, Batch 318, LR 0.193412 Loss 19.051370, Accuracy 0.076%\n",
      "Epoch 1, Batch 319, LR 0.193511 Loss 19.051871, Accuracy 0.076%\n",
      "Epoch 1, Batch 320, LR 0.193611 Loss 19.051735, Accuracy 0.076%\n",
      "Epoch 1, Batch 321, LR 0.193710 Loss 19.051260, Accuracy 0.075%\n",
      "Epoch 1, Batch 322, LR 0.193810 Loss 19.050890, Accuracy 0.075%\n",
      "Epoch 1, Batch 323, LR 0.193909 Loss 19.051410, Accuracy 0.075%\n",
      "Epoch 1, Batch 324, LR 0.194009 Loss 19.051652, Accuracy 0.075%\n",
      "Epoch 1, Batch 325, LR 0.194109 Loss 19.051510, Accuracy 0.075%\n",
      "Epoch 1, Batch 326, LR 0.194209 Loss 19.050965, Accuracy 0.074%\n",
      "Epoch 1, Batch 327, LR 0.194309 Loss 19.050886, Accuracy 0.074%\n",
      "Epoch 1, Batch 328, LR 0.194409 Loss 19.051109, Accuracy 0.074%\n",
      "Epoch 1, Batch 329, LR 0.194509 Loss 19.050919, Accuracy 0.074%\n",
      "Epoch 1, Batch 330, LR 0.194609 Loss 19.051245, Accuracy 0.073%\n",
      "Epoch 1, Batch 331, LR 0.194709 Loss 19.051623, Accuracy 0.073%\n",
      "Epoch 1, Batch 332, LR 0.194809 Loss 19.051543, Accuracy 0.073%\n",
      "Epoch 1, Batch 333, LR 0.194910 Loss 19.051713, Accuracy 0.073%\n",
      "Epoch 1, Batch 334, LR 0.195010 Loss 19.052109, Accuracy 0.073%\n",
      "Epoch 1, Batch 335, LR 0.195111 Loss 19.051777, Accuracy 0.072%\n",
      "Epoch 1, Batch 336, LR 0.195211 Loss 19.052319, Accuracy 0.072%\n",
      "Epoch 1, Batch 337, LR 0.195312 Loss 19.052358, Accuracy 0.072%\n",
      "Epoch 1, Batch 338, LR 0.195412 Loss 19.052727, Accuracy 0.074%\n",
      "Epoch 1, Batch 339, LR 0.195513 Loss 19.052722, Accuracy 0.074%\n",
      "Epoch 1, Batch 340, LR 0.195614 Loss 19.052262, Accuracy 0.074%\n",
      "Epoch 1, Batch 341, LR 0.195715 Loss 19.051998, Accuracy 0.073%\n",
      "Epoch 1, Batch 342, LR 0.195816 Loss 19.052185, Accuracy 0.073%\n",
      "Epoch 1, Batch 343, LR 0.195917 Loss 19.051686, Accuracy 0.073%\n",
      "Epoch 1, Batch 344, LR 0.196018 Loss 19.051695, Accuracy 0.075%\n",
      "Epoch 1, Batch 345, LR 0.196119 Loss 19.051714, Accuracy 0.075%\n",
      "Epoch 1, Batch 346, LR 0.196221 Loss 19.051634, Accuracy 0.075%\n",
      "Epoch 1, Batch 347, LR 0.196322 Loss 19.052147, Accuracy 0.074%\n",
      "Epoch 1, Batch 348, LR 0.196423 Loss 19.052043, Accuracy 0.074%\n",
      "Epoch 1, Batch 349, LR 0.196525 Loss 19.052379, Accuracy 0.074%\n",
      "Epoch 1, Batch 350, LR 0.196626 Loss 19.052186, Accuracy 0.074%\n",
      "Epoch 1, Batch 351, LR 0.196728 Loss 19.052652, Accuracy 0.073%\n",
      "Epoch 1, Batch 352, LR 0.196830 Loss 19.052891, Accuracy 0.073%\n",
      "Epoch 1, Batch 353, LR 0.196931 Loss 19.052374, Accuracy 0.073%\n",
      "Epoch 1, Batch 354, LR 0.197033 Loss 19.052458, Accuracy 0.073%\n",
      "Epoch 1, Batch 355, LR 0.197135 Loss 19.052555, Accuracy 0.075%\n",
      "Epoch 1, Batch 356, LR 0.197237 Loss 19.052880, Accuracy 0.075%\n",
      "Epoch 1, Batch 357, LR 0.197339 Loss 19.052604, Accuracy 0.074%\n",
      "Epoch 1, Batch 358, LR 0.197441 Loss 19.052808, Accuracy 0.074%\n",
      "Epoch 1, Batch 359, LR 0.197543 Loss 19.052982, Accuracy 0.074%\n",
      "Epoch 1, Batch 360, LR 0.197646 Loss 19.053350, Accuracy 0.074%\n",
      "Epoch 1, Batch 361, LR 0.197748 Loss 19.052780, Accuracy 0.074%\n",
      "Epoch 1, Batch 362, LR 0.197850 Loss 19.052959, Accuracy 0.076%\n",
      "Epoch 1, Batch 363, LR 0.197953 Loss 19.052741, Accuracy 0.075%\n",
      "Epoch 1, Batch 364, LR 0.198055 Loss 19.052946, Accuracy 0.075%\n",
      "Epoch 1, Batch 365, LR 0.198158 Loss 19.052553, Accuracy 0.075%\n",
      "Epoch 1, Batch 366, LR 0.198260 Loss 19.052881, Accuracy 0.075%\n",
      "Epoch 1, Batch 367, LR 0.198363 Loss 19.052921, Accuracy 0.075%\n",
      "Epoch 1, Batch 368, LR 0.198466 Loss 19.052817, Accuracy 0.074%\n",
      "Epoch 1, Batch 369, LR 0.198569 Loss 19.052852, Accuracy 0.074%\n",
      "Epoch 1, Batch 370, LR 0.198672 Loss 19.052790, Accuracy 0.074%\n",
      "Epoch 1, Batch 371, LR 0.198775 Loss 19.053562, Accuracy 0.074%\n",
      "Epoch 1, Batch 372, LR 0.198878 Loss 19.053895, Accuracy 0.074%\n",
      "Epoch 1, Batch 373, LR 0.198981 Loss 19.054098, Accuracy 0.073%\n",
      "Epoch 1, Batch 374, LR 0.199084 Loss 19.054243, Accuracy 0.073%\n",
      "Epoch 1, Batch 375, LR 0.199187 Loss 19.054739, Accuracy 0.073%\n",
      "Epoch 1, Batch 376, LR 0.199291 Loss 19.054750, Accuracy 0.073%\n",
      "Epoch 1, Batch 377, LR 0.199394 Loss 19.054823, Accuracy 0.073%\n",
      "Epoch 1, Batch 378, LR 0.199498 Loss 19.054795, Accuracy 0.072%\n",
      "Epoch 1, Batch 379, LR 0.199601 Loss 19.054399, Accuracy 0.072%\n",
      "Epoch 1, Batch 380, LR 0.199705 Loss 19.053612, Accuracy 0.074%\n",
      "Epoch 1, Batch 381, LR 0.199809 Loss 19.053976, Accuracy 0.074%\n",
      "Epoch 1, Batch 382, LR 0.199912 Loss 19.054051, Accuracy 0.074%\n",
      "Epoch 1, Batch 383, LR 0.200016 Loss 19.054384, Accuracy 0.075%\n",
      "Epoch 1, Batch 384, LR 0.200120 Loss 19.054174, Accuracy 0.075%\n",
      "Epoch 1, Batch 385, LR 0.200224 Loss 19.053921, Accuracy 0.075%\n",
      "Epoch 1, Batch 386, LR 0.200328 Loss 19.053657, Accuracy 0.075%\n",
      "Epoch 1, Batch 387, LR 0.200432 Loss 19.054006, Accuracy 0.075%\n",
      "Epoch 1, Batch 388, LR 0.200536 Loss 19.054263, Accuracy 0.075%\n",
      "Epoch 1, Batch 389, LR 0.200641 Loss 19.054252, Accuracy 0.076%\n",
      "Epoch 1, Batch 390, LR 0.200745 Loss 19.053895, Accuracy 0.076%\n",
      "Epoch 1, Batch 391, LR 0.200849 Loss 19.053712, Accuracy 0.076%\n",
      "Epoch 1, Batch 392, LR 0.200954 Loss 19.053419, Accuracy 0.078%\n",
      "Epoch 1, Batch 393, LR 0.201058 Loss 19.053732, Accuracy 0.078%\n",
      "Epoch 1, Batch 394, LR 0.201163 Loss 19.053705, Accuracy 0.077%\n",
      "Epoch 1, Batch 395, LR 0.201268 Loss 19.053749, Accuracy 0.077%\n",
      "Epoch 1, Batch 396, LR 0.201372 Loss 19.053695, Accuracy 0.077%\n",
      "Epoch 1, Batch 397, LR 0.201477 Loss 19.053516, Accuracy 0.077%\n",
      "Epoch 1, Batch 398, LR 0.201582 Loss 19.053144, Accuracy 0.077%\n",
      "Epoch 1, Batch 399, LR 0.201687 Loss 19.053440, Accuracy 0.076%\n",
      "Epoch 1, Batch 400, LR 0.201792 Loss 19.052987, Accuracy 0.076%\n",
      "Epoch 1, Batch 401, LR 0.201897 Loss 19.053446, Accuracy 0.076%\n",
      "Epoch 1, Batch 402, LR 0.202002 Loss 19.053660, Accuracy 0.078%\n",
      "Epoch 1, Batch 403, LR 0.202107 Loss 19.053811, Accuracy 0.078%\n",
      "Epoch 1, Batch 404, LR 0.202213 Loss 19.053446, Accuracy 0.077%\n",
      "Epoch 1, Batch 405, LR 0.202318 Loss 19.053202, Accuracy 0.077%\n",
      "Epoch 1, Batch 406, LR 0.202424 Loss 19.053182, Accuracy 0.079%\n",
      "Epoch 1, Batch 407, LR 0.202529 Loss 19.053463, Accuracy 0.079%\n",
      "Epoch 1, Batch 408, LR 0.202635 Loss 19.053450, Accuracy 0.079%\n",
      "Epoch 1, Batch 409, LR 0.202740 Loss 19.053068, Accuracy 0.078%\n",
      "Epoch 1, Batch 410, LR 0.202846 Loss 19.052803, Accuracy 0.078%\n",
      "Epoch 1, Batch 411, LR 0.202952 Loss 19.052983, Accuracy 0.080%\n",
      "Epoch 1, Batch 412, LR 0.203058 Loss 19.052966, Accuracy 0.080%\n",
      "Epoch 1, Batch 413, LR 0.203164 Loss 19.052532, Accuracy 0.079%\n",
      "Epoch 1, Batch 414, LR 0.203270 Loss 19.052687, Accuracy 0.079%\n",
      "Epoch 1, Batch 415, LR 0.203376 Loss 19.052521, Accuracy 0.079%\n",
      "Epoch 1, Batch 416, LR 0.203482 Loss 19.053034, Accuracy 0.079%\n",
      "Epoch 1, Batch 417, LR 0.203588 Loss 19.052623, Accuracy 0.079%\n",
      "Epoch 1, Batch 418, LR 0.203694 Loss 19.052702, Accuracy 0.078%\n",
      "Epoch 1, Batch 419, LR 0.203801 Loss 19.053000, Accuracy 0.080%\n",
      "Epoch 1, Batch 420, LR 0.203907 Loss 19.052875, Accuracy 0.080%\n",
      "Epoch 1, Batch 421, LR 0.204013 Loss 19.052478, Accuracy 0.080%\n",
      "Epoch 1, Batch 422, LR 0.204120 Loss 19.052814, Accuracy 0.080%\n",
      "Epoch 1, Batch 423, LR 0.204227 Loss 19.052442, Accuracy 0.079%\n",
      "Epoch 1, Batch 424, LR 0.204333 Loss 19.052684, Accuracy 0.079%\n",
      "Epoch 1, Batch 425, LR 0.204440 Loss 19.052690, Accuracy 0.079%\n",
      "Epoch 1, Batch 426, LR 0.204547 Loss 19.052544, Accuracy 0.081%\n",
      "Epoch 1, Batch 427, LR 0.204654 Loss 19.052390, Accuracy 0.081%\n",
      "Epoch 1, Batch 428, LR 0.204761 Loss 19.052509, Accuracy 0.080%\n",
      "Epoch 1, Batch 429, LR 0.204868 Loss 19.052414, Accuracy 0.080%\n",
      "Epoch 1, Batch 430, LR 0.204975 Loss 19.052338, Accuracy 0.080%\n",
      "Epoch 1, Batch 431, LR 0.205082 Loss 19.052808, Accuracy 0.080%\n",
      "Epoch 1, Batch 432, LR 0.205189 Loss 19.052882, Accuracy 0.080%\n",
      "Epoch 1, Batch 433, LR 0.205297 Loss 19.053033, Accuracy 0.079%\n",
      "Epoch 1, Batch 434, LR 0.205404 Loss 19.053265, Accuracy 0.079%\n",
      "Epoch 1, Batch 435, LR 0.205511 Loss 19.053385, Accuracy 0.081%\n",
      "Epoch 1, Batch 436, LR 0.205619 Loss 19.052967, Accuracy 0.081%\n",
      "Epoch 1, Batch 437, LR 0.205726 Loss 19.053236, Accuracy 0.080%\n",
      "Epoch 1, Batch 438, LR 0.205834 Loss 19.053723, Accuracy 0.082%\n",
      "Epoch 1, Batch 439, LR 0.205942 Loss 19.053503, Accuracy 0.082%\n",
      "Epoch 1, Batch 440, LR 0.206050 Loss 19.053117, Accuracy 0.082%\n",
      "Epoch 1, Batch 441, LR 0.206157 Loss 19.053342, Accuracy 0.081%\n",
      "Epoch 1, Batch 442, LR 0.206265 Loss 19.053248, Accuracy 0.081%\n",
      "Epoch 1, Batch 443, LR 0.206373 Loss 19.053000, Accuracy 0.083%\n",
      "Epoch 1, Batch 444, LR 0.206481 Loss 19.052874, Accuracy 0.083%\n",
      "Epoch 1, Batch 445, LR 0.206590 Loss 19.052868, Accuracy 0.083%\n",
      "Epoch 1, Batch 446, LR 0.206698 Loss 19.052902, Accuracy 0.082%\n",
      "Epoch 1, Batch 447, LR 0.206806 Loss 19.052951, Accuracy 0.082%\n",
      "Epoch 1, Batch 448, LR 0.206914 Loss 19.052810, Accuracy 0.082%\n",
      "Epoch 1, Batch 449, LR 0.207023 Loss 19.052782, Accuracy 0.082%\n",
      "Epoch 1, Batch 450, LR 0.207131 Loss 19.052900, Accuracy 0.082%\n",
      "Epoch 1, Batch 451, LR 0.207240 Loss 19.052619, Accuracy 0.081%\n",
      "Epoch 1, Batch 452, LR 0.207349 Loss 19.052209, Accuracy 0.081%\n",
      "Epoch 1, Batch 453, LR 0.207457 Loss 19.052246, Accuracy 0.081%\n",
      "Epoch 1, Batch 454, LR 0.207566 Loss 19.052828, Accuracy 0.081%\n",
      "Epoch 1, Batch 455, LR 0.207675 Loss 19.052973, Accuracy 0.081%\n",
      "Epoch 1, Batch 456, LR 0.207784 Loss 19.052725, Accuracy 0.081%\n",
      "Epoch 1, Batch 457, LR 0.207893 Loss 19.052806, Accuracy 0.080%\n",
      "Epoch 1, Batch 458, LR 0.208002 Loss 19.052692, Accuracy 0.080%\n",
      "Epoch 1, Batch 459, LR 0.208111 Loss 19.052776, Accuracy 0.080%\n",
      "Epoch 1, Batch 460, LR 0.208220 Loss 19.052875, Accuracy 0.080%\n",
      "Epoch 1, Batch 461, LR 0.208329 Loss 19.053004, Accuracy 0.080%\n",
      "Epoch 1, Batch 462, LR 0.208439 Loss 19.052912, Accuracy 0.079%\n",
      "Epoch 1, Batch 463, LR 0.208548 Loss 19.052704, Accuracy 0.079%\n",
      "Epoch 1, Batch 464, LR 0.208657 Loss 19.053151, Accuracy 0.079%\n",
      "Epoch 1, Batch 465, LR 0.208767 Loss 19.053117, Accuracy 0.079%\n",
      "Epoch 1, Batch 466, LR 0.208876 Loss 19.052879, Accuracy 0.079%\n",
      "Epoch 1, Batch 467, LR 0.208986 Loss 19.053010, Accuracy 0.079%\n",
      "Epoch 1, Batch 468, LR 0.209096 Loss 19.052969, Accuracy 0.078%\n",
      "Epoch 1, Batch 469, LR 0.209206 Loss 19.053121, Accuracy 0.078%\n",
      "Epoch 1, Batch 470, LR 0.209316 Loss 19.053210, Accuracy 0.078%\n",
      "Epoch 1, Batch 471, LR 0.209425 Loss 19.052968, Accuracy 0.078%\n",
      "Epoch 1, Batch 472, LR 0.209535 Loss 19.053003, Accuracy 0.078%\n",
      "Epoch 1, Batch 473, LR 0.209646 Loss 19.052797, Accuracy 0.078%\n",
      "Epoch 1, Batch 474, LR 0.209756 Loss 19.052824, Accuracy 0.077%\n",
      "Epoch 1, Batch 475, LR 0.209866 Loss 19.052988, Accuracy 0.077%\n",
      "Epoch 1, Batch 476, LR 0.209976 Loss 19.053153, Accuracy 0.077%\n",
      "Epoch 1, Batch 477, LR 0.210086 Loss 19.052937, Accuracy 0.079%\n",
      "Epoch 1, Batch 478, LR 0.210197 Loss 19.053215, Accuracy 0.078%\n",
      "Epoch 1, Batch 479, LR 0.210307 Loss 19.053314, Accuracy 0.078%\n",
      "Epoch 1, Batch 480, LR 0.210418 Loss 19.053540, Accuracy 0.080%\n",
      "Epoch 1, Batch 481, LR 0.210529 Loss 19.053315, Accuracy 0.080%\n",
      "Epoch 1, Batch 482, LR 0.210639 Loss 19.053402, Accuracy 0.079%\n",
      "Epoch 1, Batch 483, LR 0.210750 Loss 19.053396, Accuracy 0.079%\n",
      "Epoch 1, Batch 484, LR 0.210861 Loss 19.053522, Accuracy 0.079%\n",
      "Epoch 1, Batch 485, LR 0.210972 Loss 19.053594, Accuracy 0.079%\n",
      "Epoch 1, Batch 486, LR 0.211083 Loss 19.053652, Accuracy 0.079%\n",
      "Epoch 1, Batch 487, LR 0.211194 Loss 19.054529, Accuracy 0.079%\n",
      "Epoch 1, Batch 488, LR 0.211305 Loss 19.054203, Accuracy 0.078%\n",
      "Epoch 1, Batch 489, LR 0.211416 Loss 19.053983, Accuracy 0.080%\n",
      "Epoch 1, Batch 490, LR 0.211527 Loss 19.054196, Accuracy 0.080%\n",
      "Epoch 1, Batch 491, LR 0.211639 Loss 19.054142, Accuracy 0.080%\n",
      "Epoch 1, Batch 492, LR 0.211750 Loss 19.054213, Accuracy 0.079%\n",
      "Epoch 1, Batch 493, LR 0.211861 Loss 19.054045, Accuracy 0.079%\n",
      "Epoch 1, Batch 494, LR 0.211973 Loss 19.054027, Accuracy 0.079%\n",
      "Epoch 1, Batch 495, LR 0.212085 Loss 19.053900, Accuracy 0.079%\n",
      "Epoch 1, Batch 496, LR 0.212196 Loss 19.054200, Accuracy 0.079%\n",
      "Epoch 1, Batch 497, LR 0.212308 Loss 19.053738, Accuracy 0.079%\n",
      "Epoch 1, Batch 498, LR 0.212420 Loss 19.053798, Accuracy 0.078%\n",
      "Epoch 1, Batch 499, LR 0.212532 Loss 19.054135, Accuracy 0.078%\n",
      "Epoch 1, Batch 500, LR 0.212643 Loss 19.054558, Accuracy 0.078%\n",
      "Epoch 1, Batch 501, LR 0.212755 Loss 19.054612, Accuracy 0.078%\n",
      "Epoch 1, Batch 502, LR 0.212868 Loss 19.054916, Accuracy 0.078%\n",
      "Epoch 1, Batch 503, LR 0.212980 Loss 19.054950, Accuracy 0.078%\n",
      "Epoch 1, Batch 504, LR 0.213092 Loss 19.055028, Accuracy 0.078%\n",
      "Epoch 1, Batch 505, LR 0.213204 Loss 19.055359, Accuracy 0.077%\n",
      "Epoch 1, Batch 506, LR 0.213317 Loss 19.055452, Accuracy 0.077%\n",
      "Epoch 1, Batch 507, LR 0.213429 Loss 19.055298, Accuracy 0.077%\n",
      "Epoch 1, Batch 508, LR 0.213541 Loss 19.055342, Accuracy 0.077%\n",
      "Epoch 1, Batch 509, LR 0.213654 Loss 19.055191, Accuracy 0.077%\n",
      "Epoch 1, Batch 510, LR 0.213767 Loss 19.055364, Accuracy 0.077%\n",
      "Epoch 1, Batch 511, LR 0.213879 Loss 19.055215, Accuracy 0.076%\n",
      "Epoch 1, Batch 512, LR 0.213992 Loss 19.055523, Accuracy 0.076%\n",
      "Epoch 1, Batch 513, LR 0.214105 Loss 19.055657, Accuracy 0.076%\n",
      "Epoch 1, Batch 514, LR 0.214218 Loss 19.055845, Accuracy 0.076%\n",
      "Epoch 1, Batch 515, LR 0.214331 Loss 19.055747, Accuracy 0.076%\n",
      "Epoch 1, Batch 516, LR 0.214444 Loss 19.055348, Accuracy 0.077%\n",
      "Epoch 1, Batch 517, LR 0.214557 Loss 19.055455, Accuracy 0.077%\n",
      "Epoch 1, Batch 518, LR 0.214670 Loss 19.055632, Accuracy 0.077%\n",
      "Epoch 1, Batch 519, LR 0.214783 Loss 19.055879, Accuracy 0.077%\n",
      "Epoch 1, Batch 520, LR 0.214897 Loss 19.055933, Accuracy 0.077%\n",
      "Epoch 1, Batch 521, LR 0.215010 Loss 19.056323, Accuracy 0.076%\n",
      "Epoch 1, Batch 522, LR 0.215123 Loss 19.056656, Accuracy 0.076%\n",
      "Epoch 1, Batch 523, LR 0.215237 Loss 19.056688, Accuracy 0.076%\n",
      "Epoch 1, Batch 524, LR 0.215350 Loss 19.056776, Accuracy 0.076%\n",
      "Epoch 1, Batch 525, LR 0.215464 Loss 19.056886, Accuracy 0.076%\n",
      "Epoch 1, Batch 526, LR 0.215578 Loss 19.056785, Accuracy 0.076%\n",
      "Epoch 1, Batch 527, LR 0.215692 Loss 19.056709, Accuracy 0.076%\n",
      "Epoch 1, Batch 528, LR 0.215805 Loss 19.056801, Accuracy 0.075%\n",
      "Epoch 1, Batch 529, LR 0.215919 Loss 19.056645, Accuracy 0.075%\n",
      "Epoch 1, Batch 530, LR 0.216033 Loss 19.056740, Accuracy 0.077%\n",
      "Epoch 1, Batch 531, LR 0.216147 Loss 19.056627, Accuracy 0.077%\n",
      "Epoch 1, Batch 532, LR 0.216262 Loss 19.056709, Accuracy 0.076%\n",
      "Epoch 1, Batch 533, LR 0.216376 Loss 19.056505, Accuracy 0.076%\n",
      "Epoch 1, Batch 534, LR 0.216490 Loss 19.056739, Accuracy 0.076%\n",
      "Epoch 1, Batch 535, LR 0.216604 Loss 19.056835, Accuracy 0.076%\n",
      "Epoch 1, Batch 536, LR 0.216719 Loss 19.056945, Accuracy 0.076%\n",
      "Epoch 1, Batch 537, LR 0.216833 Loss 19.056985, Accuracy 0.076%\n",
      "Epoch 1, Batch 538, LR 0.216948 Loss 19.056861, Accuracy 0.076%\n",
      "Epoch 1, Batch 539, LR 0.217062 Loss 19.056605, Accuracy 0.075%\n",
      "Epoch 1, Batch 540, LR 0.217177 Loss 19.055996, Accuracy 0.077%\n",
      "Epoch 1, Batch 541, LR 0.217292 Loss 19.056307, Accuracy 0.077%\n",
      "Epoch 1, Batch 542, LR 0.217407 Loss 19.056315, Accuracy 0.076%\n",
      "Epoch 1, Batch 543, LR 0.217521 Loss 19.056328, Accuracy 0.076%\n",
      "Epoch 1, Batch 544, LR 0.217636 Loss 19.056161, Accuracy 0.076%\n",
      "Epoch 1, Batch 545, LR 0.217751 Loss 19.056173, Accuracy 0.076%\n",
      "Epoch 1, Batch 546, LR 0.217867 Loss 19.056177, Accuracy 0.076%\n",
      "Epoch 1, Batch 547, LR 0.217982 Loss 19.055750, Accuracy 0.076%\n",
      "Epoch 1, Batch 548, LR 0.218097 Loss 19.055389, Accuracy 0.076%\n",
      "Epoch 1, Batch 549, LR 0.218212 Loss 19.055282, Accuracy 0.075%\n",
      "Epoch 1, Batch 550, LR 0.218328 Loss 19.055395, Accuracy 0.075%\n",
      "Epoch 1, Batch 551, LR 0.218443 Loss 19.055222, Accuracy 0.075%\n",
      "Epoch 1, Batch 552, LR 0.218559 Loss 19.055338, Accuracy 0.075%\n",
      "Epoch 1, Batch 553, LR 0.218674 Loss 19.055624, Accuracy 0.075%\n",
      "Epoch 1, Batch 554, LR 0.218790 Loss 19.055365, Accuracy 0.075%\n",
      "Epoch 1, Batch 555, LR 0.218905 Loss 19.055244, Accuracy 0.075%\n",
      "Epoch 1, Batch 556, LR 0.219021 Loss 19.055101, Accuracy 0.074%\n",
      "Epoch 1, Batch 557, LR 0.219137 Loss 19.055287, Accuracy 0.074%\n",
      "Epoch 1, Batch 558, LR 0.219253 Loss 19.055226, Accuracy 0.074%\n",
      "Epoch 1, Batch 559, LR 0.219369 Loss 19.055278, Accuracy 0.074%\n",
      "Epoch 1, Batch 560, LR 0.219485 Loss 19.055248, Accuracy 0.074%\n",
      "Epoch 1, Batch 561, LR 0.219601 Loss 19.054825, Accuracy 0.074%\n",
      "Epoch 1, Batch 562, LR 0.219717 Loss 19.054941, Accuracy 0.074%\n",
      "Epoch 1, Batch 563, LR 0.219833 Loss 19.054598, Accuracy 0.074%\n",
      "Epoch 1, Batch 564, LR 0.219950 Loss 19.054848, Accuracy 0.073%\n",
      "Epoch 1, Batch 565, LR 0.220066 Loss 19.055227, Accuracy 0.073%\n",
      "Epoch 1, Batch 566, LR 0.220183 Loss 19.055426, Accuracy 0.073%\n",
      "Epoch 1, Batch 567, LR 0.220299 Loss 19.055184, Accuracy 0.073%\n",
      "Epoch 1, Batch 568, LR 0.220416 Loss 19.054871, Accuracy 0.073%\n",
      "Epoch 1, Batch 569, LR 0.220532 Loss 19.054702, Accuracy 0.073%\n",
      "Epoch 1, Batch 570, LR 0.220649 Loss 19.054644, Accuracy 0.073%\n",
      "Epoch 1, Batch 571, LR 0.220766 Loss 19.054595, Accuracy 0.073%\n",
      "Epoch 1, Batch 572, LR 0.220883 Loss 19.054929, Accuracy 0.072%\n",
      "Epoch 1, Batch 573, LR 0.221000 Loss 19.054981, Accuracy 0.072%\n",
      "Epoch 1, Batch 574, LR 0.221117 Loss 19.054839, Accuracy 0.073%\n",
      "Epoch 1, Batch 575, LR 0.221234 Loss 19.054875, Accuracy 0.073%\n",
      "Epoch 1, Batch 576, LR 0.221351 Loss 19.054848, Accuracy 0.073%\n",
      "Epoch 1, Batch 577, LR 0.221468 Loss 19.055027, Accuracy 0.073%\n",
      "Epoch 1, Batch 578, LR 0.221585 Loss 19.054829, Accuracy 0.073%\n",
      "Epoch 1, Batch 579, LR 0.221703 Loss 19.054839, Accuracy 0.073%\n",
      "Epoch 1, Batch 580, LR 0.221820 Loss 19.055112, Accuracy 0.073%\n",
      "Epoch 1, Batch 581, LR 0.221938 Loss 19.054976, Accuracy 0.073%\n",
      "Epoch 1, Batch 582, LR 0.222055 Loss 19.054869, Accuracy 0.072%\n",
      "Epoch 1, Batch 583, LR 0.222173 Loss 19.054704, Accuracy 0.074%\n",
      "Epoch 1, Batch 584, LR 0.222291 Loss 19.054882, Accuracy 0.074%\n",
      "Epoch 1, Batch 585, LR 0.222408 Loss 19.054914, Accuracy 0.073%\n",
      "Epoch 1, Batch 586, LR 0.222526 Loss 19.054746, Accuracy 0.075%\n",
      "Epoch 1, Batch 587, LR 0.222644 Loss 19.054825, Accuracy 0.075%\n",
      "Epoch 1, Batch 588, LR 0.222762 Loss 19.054895, Accuracy 0.074%\n",
      "Epoch 1, Batch 589, LR 0.222880 Loss 19.054969, Accuracy 0.074%\n",
      "Epoch 1, Batch 590, LR 0.222998 Loss 19.055068, Accuracy 0.075%\n",
      "Epoch 1, Batch 591, LR 0.223116 Loss 19.055267, Accuracy 0.075%\n",
      "Epoch 1, Batch 592, LR 0.223234 Loss 19.055471, Accuracy 0.075%\n",
      "Epoch 1, Batch 593, LR 0.223353 Loss 19.055280, Accuracy 0.075%\n",
      "Epoch 1, Batch 594, LR 0.223471 Loss 19.055055, Accuracy 0.075%\n",
      "Epoch 1, Batch 595, LR 0.223590 Loss 19.055069, Accuracy 0.075%\n",
      "Epoch 1, Batch 596, LR 0.223708 Loss 19.055325, Accuracy 0.075%\n",
      "Epoch 1, Batch 597, LR 0.223827 Loss 19.055282, Accuracy 0.076%\n",
      "Epoch 1, Batch 598, LR 0.223945 Loss 19.055297, Accuracy 0.076%\n",
      "Epoch 1, Batch 599, LR 0.224064 Loss 19.055360, Accuracy 0.076%\n",
      "Epoch 1, Batch 600, LR 0.224183 Loss 19.055272, Accuracy 0.076%\n",
      "Epoch 1, Batch 601, LR 0.224302 Loss 19.055333, Accuracy 0.075%\n",
      "Epoch 1, Batch 602, LR 0.224420 Loss 19.055032, Accuracy 0.077%\n",
      "Epoch 1, Batch 603, LR 0.224539 Loss 19.055275, Accuracy 0.076%\n",
      "Epoch 1, Batch 604, LR 0.224658 Loss 19.055346, Accuracy 0.076%\n",
      "Epoch 1, Batch 605, LR 0.224778 Loss 19.055487, Accuracy 0.076%\n",
      "Epoch 1, Batch 606, LR 0.224897 Loss 19.055543, Accuracy 0.076%\n",
      "Epoch 1, Batch 607, LR 0.225016 Loss 19.055579, Accuracy 0.076%\n",
      "Epoch 1, Batch 608, LR 0.225135 Loss 19.055551, Accuracy 0.076%\n",
      "Epoch 1, Batch 609, LR 0.225255 Loss 19.055539, Accuracy 0.076%\n",
      "Epoch 1, Batch 610, LR 0.225374 Loss 19.055959, Accuracy 0.076%\n",
      "Epoch 1, Batch 611, LR 0.225494 Loss 19.055859, Accuracy 0.075%\n",
      "Epoch 1, Batch 612, LR 0.225613 Loss 19.056017, Accuracy 0.075%\n",
      "Epoch 1, Batch 613, LR 0.225733 Loss 19.055933, Accuracy 0.075%\n",
      "Epoch 1, Batch 614, LR 0.225853 Loss 19.055742, Accuracy 0.075%\n",
      "Epoch 1, Batch 615, LR 0.225972 Loss 19.055706, Accuracy 0.075%\n",
      "Epoch 1, Batch 616, LR 0.226092 Loss 19.055619, Accuracy 0.075%\n",
      "Epoch 1, Batch 617, LR 0.226212 Loss 19.055799, Accuracy 0.075%\n",
      "Epoch 1, Batch 618, LR 0.226332 Loss 19.055664, Accuracy 0.075%\n",
      "Epoch 1, Batch 619, LR 0.226452 Loss 19.056032, Accuracy 0.074%\n",
      "Epoch 1, Batch 620, LR 0.226572 Loss 19.056295, Accuracy 0.074%\n",
      "Epoch 1, Batch 621, LR 0.226693 Loss 19.056204, Accuracy 0.074%\n",
      "Epoch 1, Batch 622, LR 0.226813 Loss 19.056314, Accuracy 0.074%\n",
      "Epoch 1, Batch 623, LR 0.226933 Loss 19.056536, Accuracy 0.074%\n",
      "Epoch 1, Batch 624, LR 0.227054 Loss 19.056726, Accuracy 0.074%\n",
      "Epoch 1, Batch 625, LR 0.227174 Loss 19.056547, Accuracy 0.075%\n",
      "Epoch 1, Batch 626, LR 0.227295 Loss 19.056513, Accuracy 0.075%\n",
      "Epoch 1, Batch 627, LR 0.227415 Loss 19.056545, Accuracy 0.075%\n",
      "Epoch 1, Batch 628, LR 0.227536 Loss 19.056556, Accuracy 0.075%\n",
      "Epoch 1, Batch 629, LR 0.227657 Loss 19.056567, Accuracy 0.075%\n",
      "Epoch 1, Batch 630, LR 0.227778 Loss 19.056639, Accuracy 0.074%\n",
      "Epoch 1, Batch 631, LR 0.227898 Loss 19.056848, Accuracy 0.074%\n",
      "Epoch 1, Batch 632, LR 0.228019 Loss 19.056560, Accuracy 0.074%\n",
      "Epoch 1, Batch 633, LR 0.228140 Loss 19.056472, Accuracy 0.074%\n",
      "Epoch 1, Batch 634, LR 0.228261 Loss 19.056596, Accuracy 0.074%\n",
      "Epoch 1, Batch 635, LR 0.228383 Loss 19.056617, Accuracy 0.074%\n",
      "Epoch 1, Batch 636, LR 0.228504 Loss 19.056706, Accuracy 0.074%\n",
      "Epoch 1, Batch 637, LR 0.228625 Loss 19.056635, Accuracy 0.074%\n",
      "Epoch 1, Batch 638, LR 0.228746 Loss 19.056491, Accuracy 0.073%\n",
      "Epoch 1, Batch 639, LR 0.228868 Loss 19.056647, Accuracy 0.073%\n",
      "Epoch 1, Batch 640, LR 0.228989 Loss 19.056774, Accuracy 0.073%\n",
      "Epoch 1, Batch 641, LR 0.229111 Loss 19.056729, Accuracy 0.073%\n",
      "Epoch 1, Batch 642, LR 0.229233 Loss 19.056785, Accuracy 0.073%\n",
      "Epoch 1, Batch 643, LR 0.229354 Loss 19.056480, Accuracy 0.073%\n",
      "Epoch 1, Batch 644, LR 0.229476 Loss 19.056486, Accuracy 0.073%\n",
      "Epoch 1, Batch 645, LR 0.229598 Loss 19.056753, Accuracy 0.073%\n",
      "Epoch 1, Batch 646, LR 0.229720 Loss 19.057018, Accuracy 0.073%\n",
      "Epoch 1, Batch 647, LR 0.229842 Loss 19.057347, Accuracy 0.072%\n",
      "Epoch 1, Batch 648, LR 0.229964 Loss 19.057069, Accuracy 0.072%\n",
      "Epoch 1, Batch 649, LR 0.230086 Loss 19.057001, Accuracy 0.072%\n",
      "Epoch 1, Batch 650, LR 0.230208 Loss 19.057183, Accuracy 0.072%\n",
      "Epoch 1, Batch 651, LR 0.230330 Loss 19.057390, Accuracy 0.072%\n",
      "Epoch 1, Batch 652, LR 0.230453 Loss 19.057361, Accuracy 0.072%\n",
      "Epoch 1, Batch 653, LR 0.230575 Loss 19.057341, Accuracy 0.072%\n",
      "Epoch 1, Batch 654, LR 0.230697 Loss 19.057391, Accuracy 0.072%\n",
      "Epoch 1, Batch 655, LR 0.230820 Loss 19.057247, Accuracy 0.072%\n",
      "Epoch 1, Batch 656, LR 0.230942 Loss 19.057257, Accuracy 0.071%\n",
      "Epoch 1, Batch 657, LR 0.231065 Loss 19.057205, Accuracy 0.071%\n",
      "Epoch 1, Batch 658, LR 0.231188 Loss 19.057507, Accuracy 0.072%\n",
      "Epoch 1, Batch 659, LR 0.231311 Loss 19.057849, Accuracy 0.072%\n",
      "Epoch 1, Batch 660, LR 0.231433 Loss 19.057982, Accuracy 0.072%\n",
      "Epoch 1, Batch 661, LR 0.231556 Loss 19.058214, Accuracy 0.072%\n",
      "Epoch 1, Batch 662, LR 0.231679 Loss 19.058305, Accuracy 0.072%\n",
      "Epoch 1, Batch 663, LR 0.231802 Loss 19.058347, Accuracy 0.072%\n",
      "Epoch 1, Batch 664, LR 0.231925 Loss 19.058432, Accuracy 0.072%\n",
      "Epoch 1, Batch 665, LR 0.232049 Loss 19.058610, Accuracy 0.072%\n",
      "Epoch 1, Batch 666, LR 0.232172 Loss 19.058673, Accuracy 0.072%\n",
      "Epoch 1, Batch 667, LR 0.232295 Loss 19.058898, Accuracy 0.071%\n",
      "Epoch 1, Batch 668, LR 0.232419 Loss 19.059100, Accuracy 0.071%\n",
      "Epoch 1, Batch 669, LR 0.232542 Loss 19.059033, Accuracy 0.071%\n",
      "Epoch 1, Batch 670, LR 0.232666 Loss 19.059140, Accuracy 0.071%\n",
      "Epoch 1, Batch 671, LR 0.232789 Loss 19.058802, Accuracy 0.072%\n",
      "Epoch 1, Batch 672, LR 0.232913 Loss 19.059012, Accuracy 0.072%\n",
      "Epoch 1, Batch 673, LR 0.233037 Loss 19.059324, Accuracy 0.072%\n",
      "Epoch 1, Batch 674, LR 0.233160 Loss 19.059454, Accuracy 0.073%\n",
      "Epoch 1, Batch 675, LR 0.233284 Loss 19.059520, Accuracy 0.073%\n",
      "Epoch 1, Batch 676, LR 0.233408 Loss 19.059317, Accuracy 0.073%\n",
      "Epoch 1, Batch 677, LR 0.233532 Loss 19.059412, Accuracy 0.073%\n",
      "Epoch 1, Batch 678, LR 0.233656 Loss 19.059444, Accuracy 0.073%\n",
      "Epoch 1, Batch 679, LR 0.233780 Loss 19.059458, Accuracy 0.072%\n",
      "Epoch 1, Batch 680, LR 0.233904 Loss 19.059772, Accuracy 0.072%\n",
      "Epoch 1, Batch 681, LR 0.234029 Loss 19.059805, Accuracy 0.072%\n",
      "Epoch 1, Batch 682, LR 0.234153 Loss 19.060119, Accuracy 0.072%\n",
      "Epoch 1, Batch 683, LR 0.234277 Loss 19.060307, Accuracy 0.072%\n",
      "Epoch 1, Batch 684, LR 0.234402 Loss 19.060348, Accuracy 0.072%\n",
      "Epoch 1, Batch 685, LR 0.234526 Loss 19.060421, Accuracy 0.072%\n",
      "Epoch 1, Batch 686, LR 0.234651 Loss 19.060401, Accuracy 0.072%\n",
      "Epoch 1, Batch 687, LR 0.234776 Loss 19.060281, Accuracy 0.072%\n",
      "Epoch 1, Batch 688, LR 0.234900 Loss 19.059978, Accuracy 0.073%\n",
      "Epoch 1, Batch 689, LR 0.235025 Loss 19.060039, Accuracy 0.073%\n",
      "Epoch 1, Batch 690, LR 0.235150 Loss 19.060051, Accuracy 0.072%\n",
      "Epoch 1, Batch 691, LR 0.235275 Loss 19.059960, Accuracy 0.072%\n",
      "Epoch 1, Batch 692, LR 0.235400 Loss 19.060012, Accuracy 0.072%\n",
      "Epoch 1, Batch 693, LR 0.235525 Loss 19.060136, Accuracy 0.072%\n",
      "Epoch 1, Batch 694, LR 0.235650 Loss 19.059757, Accuracy 0.072%\n",
      "Epoch 1, Batch 695, LR 0.235775 Loss 19.059924, Accuracy 0.072%\n",
      "Epoch 1, Batch 696, LR 0.235901 Loss 19.059810, Accuracy 0.072%\n",
      "Epoch 1, Batch 697, LR 0.236026 Loss 19.059943, Accuracy 0.072%\n",
      "Epoch 1, Batch 698, LR 0.236151 Loss 19.059988, Accuracy 0.073%\n",
      "Epoch 1, Batch 699, LR 0.236277 Loss 19.059862, Accuracy 0.073%\n",
      "Epoch 1, Batch 700, LR 0.236402 Loss 19.060009, Accuracy 0.073%\n",
      "Epoch 1, Batch 701, LR 0.236528 Loss 19.059719, Accuracy 0.074%\n",
      "Epoch 1, Batch 702, LR 0.236654 Loss 19.059783, Accuracy 0.073%\n",
      "Epoch 1, Batch 703, LR 0.236780 Loss 19.059848, Accuracy 0.073%\n",
      "Epoch 1, Batch 704, LR 0.236905 Loss 19.059797, Accuracy 0.074%\n",
      "Epoch 1, Batch 705, LR 0.237031 Loss 19.059615, Accuracy 0.074%\n",
      "Epoch 1, Batch 706, LR 0.237157 Loss 19.059640, Accuracy 0.074%\n",
      "Epoch 1, Batch 707, LR 0.237283 Loss 19.059842, Accuracy 0.074%\n",
      "Epoch 1, Batch 708, LR 0.237409 Loss 19.060139, Accuracy 0.074%\n",
      "Epoch 1, Batch 709, LR 0.237535 Loss 19.060114, Accuracy 0.074%\n",
      "Epoch 1, Batch 710, LR 0.237662 Loss 19.060129, Accuracy 0.074%\n",
      "Epoch 1, Batch 711, LR 0.237788 Loss 19.060173, Accuracy 0.074%\n",
      "Epoch 1, Batch 712, LR 0.237914 Loss 19.059640, Accuracy 0.075%\n",
      "Epoch 1, Batch 713, LR 0.238041 Loss 19.059705, Accuracy 0.075%\n",
      "Epoch 1, Batch 714, LR 0.238167 Loss 19.059619, Accuracy 0.075%\n",
      "Epoch 1, Batch 715, LR 0.238294 Loss 19.059229, Accuracy 0.075%\n",
      "Epoch 1, Batch 716, LR 0.238420 Loss 19.059179, Accuracy 0.075%\n",
      "Epoch 1, Batch 717, LR 0.238547 Loss 19.059203, Accuracy 0.075%\n",
      "Epoch 1, Batch 718, LR 0.238674 Loss 19.059021, Accuracy 0.076%\n",
      "Epoch 1, Batch 719, LR 0.238801 Loss 19.059004, Accuracy 0.076%\n",
      "Epoch 1, Batch 720, LR 0.238927 Loss 19.058663, Accuracy 0.076%\n",
      "Epoch 1, Batch 721, LR 0.239054 Loss 19.058680, Accuracy 0.076%\n",
      "Epoch 1, Batch 722, LR 0.239181 Loss 19.058771, Accuracy 0.076%\n",
      "Epoch 1, Batch 723, LR 0.239308 Loss 19.058799, Accuracy 0.076%\n",
      "Epoch 1, Batch 724, LR 0.239436 Loss 19.058614, Accuracy 0.076%\n",
      "Epoch 1, Batch 725, LR 0.239563 Loss 19.058682, Accuracy 0.075%\n",
      "Epoch 1, Batch 726, LR 0.239690 Loss 19.058692, Accuracy 0.075%\n",
      "Epoch 1, Batch 727, LR 0.239818 Loss 19.058456, Accuracy 0.075%\n",
      "Epoch 1, Batch 728, LR 0.239945 Loss 19.058508, Accuracy 0.075%\n",
      "Epoch 1, Batch 729, LR 0.240072 Loss 19.058475, Accuracy 0.075%\n",
      "Epoch 1, Batch 730, LR 0.240200 Loss 19.058310, Accuracy 0.075%\n",
      "Epoch 1, Batch 731, LR 0.240328 Loss 19.058192, Accuracy 0.075%\n",
      "Epoch 1, Batch 732, LR 0.240455 Loss 19.058526, Accuracy 0.075%\n",
      "Epoch 1, Batch 733, LR 0.240583 Loss 19.058596, Accuracy 0.075%\n",
      "Epoch 1, Batch 734, LR 0.240711 Loss 19.058940, Accuracy 0.075%\n",
      "Epoch 1, Batch 735, LR 0.240839 Loss 19.058806, Accuracy 0.074%\n",
      "Epoch 1, Batch 736, LR 0.240967 Loss 19.058755, Accuracy 0.074%\n",
      "Epoch 1, Batch 737, LR 0.241095 Loss 19.058666, Accuracy 0.074%\n",
      "Epoch 1, Batch 738, LR 0.241223 Loss 19.058937, Accuracy 0.074%\n",
      "Epoch 1, Batch 739, LR 0.241351 Loss 19.058937, Accuracy 0.074%\n",
      "Epoch 1, Batch 740, LR 0.241479 Loss 19.058957, Accuracy 0.074%\n",
      "Epoch 1, Batch 741, LR 0.241608 Loss 19.059276, Accuracy 0.075%\n",
      "Epoch 1, Batch 742, LR 0.241736 Loss 19.059586, Accuracy 0.075%\n",
      "Epoch 1, Batch 743, LR 0.241864 Loss 19.059539, Accuracy 0.075%\n",
      "Epoch 1, Batch 744, LR 0.241993 Loss 19.059585, Accuracy 0.075%\n",
      "Epoch 1, Batch 745, LR 0.242121 Loss 19.059774, Accuracy 0.074%\n",
      "Epoch 1, Batch 746, LR 0.242250 Loss 19.059644, Accuracy 0.074%\n",
      "Epoch 1, Batch 747, LR 0.242379 Loss 19.059517, Accuracy 0.074%\n",
      "Epoch 1, Batch 748, LR 0.242507 Loss 19.059154, Accuracy 0.075%\n",
      "Epoch 1, Batch 749, LR 0.242636 Loss 19.059108, Accuracy 0.075%\n",
      "Epoch 1, Batch 750, LR 0.242765 Loss 19.058895, Accuracy 0.075%\n",
      "Epoch 1, Batch 751, LR 0.242894 Loss 19.058808, Accuracy 0.075%\n",
      "Epoch 1, Batch 752, LR 0.243023 Loss 19.058790, Accuracy 0.076%\n",
      "Epoch 1, Batch 753, LR 0.243152 Loss 19.058792, Accuracy 0.076%\n",
      "Epoch 1, Batch 754, LR 0.243281 Loss 19.058707, Accuracy 0.076%\n",
      "Epoch 1, Batch 755, LR 0.243411 Loss 19.058524, Accuracy 0.076%\n",
      "Epoch 1, Batch 756, LR 0.243540 Loss 19.058430, Accuracy 0.075%\n",
      "Epoch 1, Batch 757, LR 0.243669 Loss 19.058621, Accuracy 0.075%\n",
      "Epoch 1, Batch 758, LR 0.243799 Loss 19.058593, Accuracy 0.075%\n",
      "Epoch 1, Batch 759, LR 0.243928 Loss 19.058759, Accuracy 0.075%\n",
      "Epoch 1, Batch 760, LR 0.244058 Loss 19.058907, Accuracy 0.075%\n",
      "Epoch 1, Batch 761, LR 0.244187 Loss 19.058877, Accuracy 0.075%\n",
      "Epoch 1, Batch 762, LR 0.244317 Loss 19.059029, Accuracy 0.075%\n",
      "Epoch 1, Batch 763, LR 0.244447 Loss 19.058856, Accuracy 0.075%\n",
      "Epoch 1, Batch 764, LR 0.244577 Loss 19.058939, Accuracy 0.075%\n",
      "Epoch 1, Batch 765, LR 0.244707 Loss 19.058905, Accuracy 0.075%\n",
      "Epoch 1, Batch 766, LR 0.244837 Loss 19.058816, Accuracy 0.074%\n",
      "Epoch 1, Batch 767, LR 0.244967 Loss 19.058569, Accuracy 0.074%\n",
      "Epoch 1, Batch 768, LR 0.245097 Loss 19.058512, Accuracy 0.074%\n",
      "Epoch 1, Batch 769, LR 0.245227 Loss 19.058305, Accuracy 0.074%\n",
      "Epoch 1, Batch 770, LR 0.245357 Loss 19.058683, Accuracy 0.074%\n",
      "Epoch 1, Batch 771, LR 0.245487 Loss 19.058780, Accuracy 0.074%\n",
      "Epoch 1, Batch 772, LR 0.245618 Loss 19.058740, Accuracy 0.074%\n",
      "Epoch 1, Batch 773, LR 0.245748 Loss 19.058984, Accuracy 0.074%\n",
      "Epoch 1, Batch 774, LR 0.245879 Loss 19.058924, Accuracy 0.074%\n",
      "Epoch 1, Batch 775, LR 0.246009 Loss 19.058922, Accuracy 0.074%\n",
      "Epoch 1, Batch 776, LR 0.246140 Loss 19.058735, Accuracy 0.075%\n",
      "Epoch 1, Batch 777, LR 0.246271 Loss 19.058634, Accuracy 0.075%\n",
      "Epoch 1, Batch 778, LR 0.246401 Loss 19.058698, Accuracy 0.075%\n",
      "Epoch 1, Batch 779, LR 0.246532 Loss 19.058665, Accuracy 0.075%\n",
      "Epoch 1, Batch 780, LR 0.246663 Loss 19.058388, Accuracy 0.075%\n",
      "Epoch 1, Batch 781, LR 0.246794 Loss 19.058597, Accuracy 0.075%\n",
      "Epoch 1, Batch 782, LR 0.246925 Loss 19.058643, Accuracy 0.075%\n",
      "Epoch 1, Batch 783, LR 0.247056 Loss 19.058510, Accuracy 0.075%\n",
      "Epoch 1, Batch 784, LR 0.247187 Loss 19.058380, Accuracy 0.076%\n",
      "Epoch 1, Batch 785, LR 0.247319 Loss 19.058401, Accuracy 0.076%\n",
      "Epoch 1, Batch 786, LR 0.247450 Loss 19.058267, Accuracy 0.076%\n",
      "Epoch 1, Batch 787, LR 0.247581 Loss 19.058539, Accuracy 0.075%\n",
      "Epoch 1, Batch 788, LR 0.247713 Loss 19.058782, Accuracy 0.075%\n",
      "Epoch 1, Batch 789, LR 0.247844 Loss 19.058574, Accuracy 0.075%\n",
      "Epoch 1, Batch 790, LR 0.247976 Loss 19.058535, Accuracy 0.075%\n",
      "Epoch 1, Batch 791, LR 0.248107 Loss 19.058502, Accuracy 0.075%\n",
      "Epoch 1, Batch 792, LR 0.248239 Loss 19.058667, Accuracy 0.075%\n",
      "Epoch 1, Batch 793, LR 0.248371 Loss 19.058680, Accuracy 0.075%\n",
      "Epoch 1, Batch 794, LR 0.248503 Loss 19.058603, Accuracy 0.075%\n",
      "Epoch 1, Batch 795, LR 0.248635 Loss 19.058709, Accuracy 0.075%\n",
      "Epoch 1, Batch 796, LR 0.248767 Loss 19.059038, Accuracy 0.075%\n",
      "Epoch 1, Batch 797, LR 0.248899 Loss 19.059326, Accuracy 0.074%\n",
      "Epoch 1, Batch 798, LR 0.249031 Loss 19.059423, Accuracy 0.074%\n",
      "Epoch 1, Batch 799, LR 0.249163 Loss 19.059247, Accuracy 0.075%\n",
      "Epoch 1, Batch 800, LR 0.249295 Loss 19.059182, Accuracy 0.075%\n",
      "Epoch 1, Batch 801, LR 0.249427 Loss 19.059196, Accuracy 0.075%\n",
      "Epoch 1, Batch 802, LR 0.249560 Loss 19.059180, Accuracy 0.075%\n",
      "Epoch 1, Batch 803, LR 0.249692 Loss 19.059362, Accuracy 0.075%\n",
      "Epoch 1, Batch 804, LR 0.249825 Loss 19.059306, Accuracy 0.075%\n",
      "Epoch 1, Batch 805, LR 0.249957 Loss 19.059221, Accuracy 0.075%\n",
      "Epoch 1, Batch 806, LR 0.250090 Loss 19.059244, Accuracy 0.075%\n",
      "Epoch 1, Batch 807, LR 0.250223 Loss 19.059036, Accuracy 0.075%\n",
      "Epoch 1, Batch 808, LR 0.250355 Loss 19.059104, Accuracy 0.075%\n",
      "Epoch 1, Batch 809, LR 0.250488 Loss 19.059228, Accuracy 0.075%\n",
      "Epoch 1, Batch 810, LR 0.250621 Loss 19.059266, Accuracy 0.075%\n",
      "Epoch 1, Batch 811, LR 0.250754 Loss 19.059477, Accuracy 0.075%\n",
      "Epoch 1, Batch 812, LR 0.250887 Loss 19.059512, Accuracy 0.075%\n",
      "Epoch 1, Batch 813, LR 0.251020 Loss 19.059555, Accuracy 0.075%\n",
      "Epoch 1, Batch 814, LR 0.251153 Loss 19.059481, Accuracy 0.075%\n",
      "Epoch 1, Batch 815, LR 0.251287 Loss 19.059659, Accuracy 0.075%\n",
      "Epoch 1, Batch 816, LR 0.251420 Loss 19.059570, Accuracy 0.075%\n",
      "Epoch 1, Batch 817, LR 0.251553 Loss 19.059609, Accuracy 0.075%\n",
      "Epoch 1, Batch 818, LR 0.251687 Loss 19.059487, Accuracy 0.074%\n",
      "Epoch 1, Batch 819, LR 0.251820 Loss 19.059716, Accuracy 0.074%\n",
      "Epoch 1, Batch 820, LR 0.251954 Loss 19.059804, Accuracy 0.074%\n",
      "Epoch 1, Batch 821, LR 0.252087 Loss 19.059521, Accuracy 0.074%\n",
      "Epoch 1, Batch 822, LR 0.252221 Loss 19.059589, Accuracy 0.074%\n",
      "Epoch 1, Batch 823, LR 0.252355 Loss 19.059799, Accuracy 0.075%\n",
      "Epoch 1, Batch 824, LR 0.252489 Loss 19.059618, Accuracy 0.075%\n",
      "Epoch 1, Batch 825, LR 0.252622 Loss 19.059395, Accuracy 0.076%\n",
      "Epoch 1, Batch 826, LR 0.252756 Loss 19.059455, Accuracy 0.076%\n",
      "Epoch 1, Batch 827, LR 0.252890 Loss 19.059508, Accuracy 0.076%\n",
      "Epoch 1, Batch 828, LR 0.253025 Loss 19.059444, Accuracy 0.075%\n",
      "Epoch 1, Batch 829, LR 0.253159 Loss 19.059393, Accuracy 0.075%\n",
      "Epoch 1, Batch 830, LR 0.253293 Loss 19.059165, Accuracy 0.075%\n",
      "Epoch 1, Batch 831, LR 0.253427 Loss 19.058988, Accuracy 0.075%\n",
      "Epoch 1, Batch 832, LR 0.253562 Loss 19.059262, Accuracy 0.075%\n",
      "Epoch 1, Batch 833, LR 0.253696 Loss 19.059050, Accuracy 0.076%\n",
      "Epoch 1, Batch 834, LR 0.253830 Loss 19.058937, Accuracy 0.077%\n",
      "Epoch 1, Batch 835, LR 0.253965 Loss 19.058832, Accuracy 0.077%\n",
      "Epoch 1, Batch 836, LR 0.254100 Loss 19.058854, Accuracy 0.078%\n",
      "Epoch 1, Batch 837, LR 0.254234 Loss 19.059048, Accuracy 0.077%\n",
      "Epoch 1, Batch 838, LR 0.254369 Loss 19.059235, Accuracy 0.077%\n",
      "Epoch 1, Batch 839, LR 0.254504 Loss 19.059136, Accuracy 0.077%\n",
      "Epoch 1, Batch 840, LR 0.254639 Loss 19.059436, Accuracy 0.077%\n",
      "Epoch 1, Batch 841, LR 0.254774 Loss 19.059530, Accuracy 0.077%\n",
      "Epoch 1, Batch 842, LR 0.254909 Loss 19.059711, Accuracy 0.077%\n",
      "Epoch 1, Batch 843, LR 0.255044 Loss 19.059677, Accuracy 0.077%\n",
      "Epoch 1, Batch 844, LR 0.255179 Loss 19.059632, Accuracy 0.077%\n",
      "Epoch 1, Batch 845, LR 0.255314 Loss 19.059681, Accuracy 0.077%\n",
      "Epoch 1, Batch 846, LR 0.255449 Loss 19.059532, Accuracy 0.077%\n",
      "Epoch 1, Batch 847, LR 0.255585 Loss 19.059425, Accuracy 0.077%\n",
      "Epoch 1, Batch 848, LR 0.255720 Loss 19.059513, Accuracy 0.076%\n",
      "Epoch 1, Batch 849, LR 0.255856 Loss 19.059468, Accuracy 0.076%\n",
      "Epoch 1, Batch 850, LR 0.255991 Loss 19.059287, Accuracy 0.076%\n",
      "Epoch 1, Batch 851, LR 0.256127 Loss 19.059263, Accuracy 0.076%\n",
      "Epoch 1, Batch 852, LR 0.256263 Loss 19.059179, Accuracy 0.076%\n",
      "Epoch 1, Batch 853, LR 0.256398 Loss 19.059040, Accuracy 0.077%\n",
      "Epoch 1, Batch 854, LR 0.256534 Loss 19.059070, Accuracy 0.077%\n",
      "Epoch 1, Batch 855, LR 0.256670 Loss 19.059036, Accuracy 0.077%\n",
      "Epoch 1, Batch 856, LR 0.256806 Loss 19.059190, Accuracy 0.077%\n",
      "Epoch 1, Batch 857, LR 0.256942 Loss 19.059035, Accuracy 0.077%\n",
      "Epoch 1, Batch 858, LR 0.257078 Loss 19.059152, Accuracy 0.076%\n",
      "Epoch 1, Batch 859, LR 0.257214 Loss 19.059322, Accuracy 0.076%\n",
      "Epoch 1, Batch 860, LR 0.257350 Loss 19.059370, Accuracy 0.076%\n",
      "Epoch 1, Batch 861, LR 0.257487 Loss 19.059100, Accuracy 0.076%\n",
      "Epoch 1, Batch 862, LR 0.257623 Loss 19.059324, Accuracy 0.076%\n",
      "Epoch 1, Batch 863, LR 0.257759 Loss 19.059312, Accuracy 0.076%\n",
      "Epoch 1, Batch 864, LR 0.257896 Loss 19.059464, Accuracy 0.076%\n",
      "Epoch 1, Batch 865, LR 0.258032 Loss 19.059416, Accuracy 0.076%\n",
      "Epoch 1, Batch 866, LR 0.258169 Loss 19.059450, Accuracy 0.076%\n",
      "Epoch 1, Batch 867, LR 0.258306 Loss 19.059349, Accuracy 0.077%\n",
      "Epoch 1, Batch 868, LR 0.258442 Loss 19.059496, Accuracy 0.077%\n",
      "Epoch 1, Batch 869, LR 0.258579 Loss 19.059553, Accuracy 0.077%\n",
      "Epoch 1, Batch 870, LR 0.258716 Loss 19.059606, Accuracy 0.077%\n",
      "Epoch 1, Batch 871, LR 0.258853 Loss 19.059479, Accuracy 0.077%\n",
      "Epoch 1, Batch 872, LR 0.258990 Loss 19.059466, Accuracy 0.078%\n",
      "Epoch 1, Batch 873, LR 0.259127 Loss 19.059324, Accuracy 0.078%\n",
      "Epoch 1, Batch 874, LR 0.259264 Loss 19.059286, Accuracy 0.078%\n",
      "Epoch 1, Batch 875, LR 0.259401 Loss 19.059438, Accuracy 0.078%\n",
      "Epoch 1, Batch 876, LR 0.259539 Loss 19.059579, Accuracy 0.078%\n",
      "Epoch 1, Batch 877, LR 0.259676 Loss 19.059784, Accuracy 0.078%\n",
      "Epoch 1, Batch 878, LR 0.259813 Loss 19.059614, Accuracy 0.078%\n",
      "Epoch 1, Batch 879, LR 0.259951 Loss 19.059727, Accuracy 0.078%\n",
      "Epoch 1, Batch 880, LR 0.260088 Loss 19.059625, Accuracy 0.078%\n",
      "Epoch 1, Batch 881, LR 0.260226 Loss 19.059668, Accuracy 0.078%\n",
      "Epoch 1, Batch 882, LR 0.260364 Loss 19.059870, Accuracy 0.078%\n",
      "Epoch 1, Batch 883, LR 0.260501 Loss 19.060022, Accuracy 0.078%\n",
      "Epoch 1, Batch 884, LR 0.260639 Loss 19.060075, Accuracy 0.078%\n",
      "Epoch 1, Batch 885, LR 0.260777 Loss 19.060101, Accuracy 0.078%\n",
      "Epoch 1, Batch 886, LR 0.260915 Loss 19.059936, Accuracy 0.078%\n",
      "Epoch 1, Batch 887, LR 0.261053 Loss 19.059810, Accuracy 0.078%\n",
      "Epoch 1, Batch 888, LR 0.261191 Loss 19.059708, Accuracy 0.078%\n",
      "Epoch 1, Batch 889, LR 0.261329 Loss 19.059703, Accuracy 0.078%\n",
      "Epoch 1, Batch 890, LR 0.261467 Loss 19.059619, Accuracy 0.078%\n",
      "Epoch 1, Batch 891, LR 0.261605 Loss 19.059660, Accuracy 0.078%\n",
      "Epoch 1, Batch 892, LR 0.261744 Loss 19.059561, Accuracy 0.078%\n",
      "Epoch 1, Batch 893, LR 0.261882 Loss 19.059454, Accuracy 0.078%\n",
      "Epoch 1, Batch 894, LR 0.262021 Loss 19.059472, Accuracy 0.078%\n",
      "Epoch 1, Batch 895, LR 0.262159 Loss 19.059511, Accuracy 0.078%\n",
      "Epoch 1, Batch 896, LR 0.262298 Loss 19.059362, Accuracy 0.078%\n",
      "Epoch 1, Batch 897, LR 0.262436 Loss 19.059126, Accuracy 0.078%\n",
      "Epoch 1, Batch 898, LR 0.262575 Loss 19.059291, Accuracy 0.077%\n",
      "Epoch 1, Batch 899, LR 0.262714 Loss 19.059253, Accuracy 0.077%\n",
      "Epoch 1, Batch 900, LR 0.262853 Loss 19.059086, Accuracy 0.077%\n",
      "Epoch 1, Batch 901, LR 0.262991 Loss 19.059252, Accuracy 0.077%\n",
      "Epoch 1, Batch 902, LR 0.263130 Loss 19.059448, Accuracy 0.077%\n",
      "Epoch 1, Batch 903, LR 0.263269 Loss 19.059572, Accuracy 0.077%\n",
      "Epoch 1, Batch 904, LR 0.263409 Loss 19.059562, Accuracy 0.078%\n",
      "Epoch 1, Batch 905, LR 0.263548 Loss 19.059481, Accuracy 0.078%\n",
      "Epoch 1, Batch 906, LR 0.263687 Loss 19.059580, Accuracy 0.078%\n",
      "Epoch 1, Batch 907, LR 0.263826 Loss 19.059697, Accuracy 0.078%\n",
      "Epoch 1, Batch 908, LR 0.263966 Loss 19.059593, Accuracy 0.077%\n",
      "Epoch 1, Batch 909, LR 0.264105 Loss 19.059825, Accuracy 0.077%\n",
      "Epoch 1, Batch 910, LR 0.264245 Loss 19.059996, Accuracy 0.077%\n",
      "Epoch 1, Batch 911, LR 0.264384 Loss 19.059945, Accuracy 0.078%\n",
      "Epoch 1, Batch 912, LR 0.264524 Loss 19.060068, Accuracy 0.078%\n",
      "Epoch 1, Batch 913, LR 0.264663 Loss 19.060154, Accuracy 0.078%\n",
      "Epoch 1, Batch 914, LR 0.264803 Loss 19.060176, Accuracy 0.078%\n",
      "Epoch 1, Batch 915, LR 0.264943 Loss 19.060313, Accuracy 0.078%\n",
      "Epoch 1, Batch 916, LR 0.265083 Loss 19.060323, Accuracy 0.078%\n",
      "Epoch 1, Batch 917, LR 0.265223 Loss 19.060146, Accuracy 0.078%\n",
      "Epoch 1, Batch 918, LR 0.265363 Loss 19.060101, Accuracy 0.077%\n",
      "Epoch 1, Batch 919, LR 0.265503 Loss 19.060126, Accuracy 0.077%\n",
      "Epoch 1, Batch 920, LR 0.265643 Loss 19.060396, Accuracy 0.077%\n",
      "Epoch 1, Batch 921, LR 0.265783 Loss 19.060197, Accuracy 0.077%\n",
      "Epoch 1, Batch 922, LR 0.265924 Loss 19.060168, Accuracy 0.077%\n",
      "Epoch 1, Batch 923, LR 0.266064 Loss 19.060193, Accuracy 0.077%\n",
      "Epoch 1, Batch 924, LR 0.266204 Loss 19.060294, Accuracy 0.077%\n",
      "Epoch 1, Batch 925, LR 0.266345 Loss 19.060533, Accuracy 0.077%\n",
      "Epoch 1, Batch 926, LR 0.266485 Loss 19.060619, Accuracy 0.077%\n",
      "Epoch 1, Batch 927, LR 0.266626 Loss 19.060493, Accuracy 0.077%\n",
      "Epoch 1, Batch 928, LR 0.266767 Loss 19.060286, Accuracy 0.077%\n",
      "Epoch 1, Batch 929, LR 0.266907 Loss 19.060177, Accuracy 0.077%\n",
      "Epoch 1, Batch 930, LR 0.267048 Loss 19.060145, Accuracy 0.076%\n",
      "Epoch 1, Batch 931, LR 0.267189 Loss 19.059925, Accuracy 0.076%\n",
      "Epoch 1, Batch 932, LR 0.267330 Loss 19.059841, Accuracy 0.077%\n",
      "Epoch 1, Batch 933, LR 0.267471 Loss 19.059902, Accuracy 0.077%\n",
      "Epoch 1, Batch 934, LR 0.267612 Loss 19.059769, Accuracy 0.077%\n",
      "Epoch 1, Batch 935, LR 0.267753 Loss 19.059800, Accuracy 0.077%\n",
      "Epoch 1, Batch 936, LR 0.267894 Loss 19.059758, Accuracy 0.077%\n",
      "Epoch 1, Batch 937, LR 0.268036 Loss 19.059478, Accuracy 0.077%\n",
      "Epoch 1, Batch 938, LR 0.268177 Loss 19.059218, Accuracy 0.077%\n",
      "Epoch 1, Batch 939, LR 0.268318 Loss 19.059406, Accuracy 0.077%\n",
      "Epoch 1, Batch 940, LR 0.268460 Loss 19.059443, Accuracy 0.077%\n",
      "Epoch 1, Batch 941, LR 0.268601 Loss 19.059580, Accuracy 0.077%\n",
      "Epoch 1, Batch 942, LR 0.268743 Loss 19.059566, Accuracy 0.077%\n",
      "Epoch 1, Batch 943, LR 0.268885 Loss 19.059465, Accuracy 0.077%\n",
      "Epoch 1, Batch 944, LR 0.269026 Loss 19.059276, Accuracy 0.077%\n",
      "Epoch 1, Batch 945, LR 0.269168 Loss 19.059301, Accuracy 0.077%\n",
      "Epoch 1, Batch 946, LR 0.269310 Loss 19.059352, Accuracy 0.077%\n",
      "Epoch 1, Batch 947, LR 0.269452 Loss 19.059317, Accuracy 0.077%\n",
      "Epoch 1, Batch 948, LR 0.269594 Loss 19.059369, Accuracy 0.077%\n",
      "Epoch 1, Batch 949, LR 0.269736 Loss 19.059348, Accuracy 0.077%\n",
      "Epoch 1, Batch 950, LR 0.269878 Loss 19.059519, Accuracy 0.076%\n",
      "Epoch 1, Batch 951, LR 0.270020 Loss 19.059233, Accuracy 0.076%\n",
      "Epoch 1, Batch 952, LR 0.270162 Loss 19.059159, Accuracy 0.076%\n",
      "Epoch 1, Batch 953, LR 0.270305 Loss 19.058973, Accuracy 0.076%\n",
      "Epoch 1, Batch 954, LR 0.270447 Loss 19.058943, Accuracy 0.076%\n",
      "Epoch 1, Batch 955, LR 0.270590 Loss 19.059023, Accuracy 0.076%\n",
      "Epoch 1, Batch 956, LR 0.270732 Loss 19.058961, Accuracy 0.076%\n",
      "Epoch 1, Batch 957, LR 0.270875 Loss 19.059055, Accuracy 0.076%\n",
      "Epoch 1, Batch 958, LR 0.271017 Loss 19.059034, Accuracy 0.076%\n",
      "Epoch 1, Batch 959, LR 0.271160 Loss 19.059092, Accuracy 0.076%\n",
      "Epoch 1, Batch 960, LR 0.271303 Loss 19.059002, Accuracy 0.076%\n",
      "Epoch 1, Batch 961, LR 0.271446 Loss 19.059138, Accuracy 0.076%\n",
      "Epoch 1, Batch 962, LR 0.271588 Loss 19.059112, Accuracy 0.076%\n",
      "Epoch 1, Batch 963, LR 0.271731 Loss 19.059191, Accuracy 0.076%\n",
      "Epoch 1, Batch 964, LR 0.271874 Loss 19.059346, Accuracy 0.076%\n",
      "Epoch 1, Batch 965, LR 0.272017 Loss 19.059509, Accuracy 0.076%\n",
      "Epoch 1, Batch 966, LR 0.272161 Loss 19.059732, Accuracy 0.076%\n",
      "Epoch 1, Batch 967, LR 0.272304 Loss 19.059497, Accuracy 0.076%\n",
      "Epoch 1, Batch 968, LR 0.272447 Loss 19.059629, Accuracy 0.076%\n",
      "Epoch 1, Batch 969, LR 0.272590 Loss 19.059505, Accuracy 0.077%\n",
      "Epoch 1, Batch 970, LR 0.272734 Loss 19.059460, Accuracy 0.077%\n",
      "Epoch 1, Batch 971, LR 0.272877 Loss 19.059201, Accuracy 0.077%\n",
      "Epoch 1, Batch 972, LR 0.273021 Loss 19.059096, Accuracy 0.077%\n",
      "Epoch 1, Batch 973, LR 0.273164 Loss 19.059128, Accuracy 0.077%\n",
      "Epoch 1, Batch 974, LR 0.273308 Loss 19.059155, Accuracy 0.077%\n",
      "Epoch 1, Batch 975, LR 0.273452 Loss 19.059284, Accuracy 0.077%\n",
      "Epoch 1, Batch 976, LR 0.273596 Loss 19.059479, Accuracy 0.077%\n",
      "Epoch 1, Batch 977, LR 0.273740 Loss 19.059388, Accuracy 0.077%\n",
      "Epoch 1, Batch 978, LR 0.273883 Loss 19.059391, Accuracy 0.077%\n",
      "Epoch 1, Batch 979, LR 0.274027 Loss 19.059358, Accuracy 0.077%\n",
      "Epoch 1, Batch 980, LR 0.274172 Loss 19.059508, Accuracy 0.077%\n",
      "Epoch 1, Batch 981, LR 0.274316 Loss 19.059661, Accuracy 0.077%\n",
      "Epoch 1, Batch 982, LR 0.274460 Loss 19.059698, Accuracy 0.077%\n",
      "Epoch 1, Batch 983, LR 0.274604 Loss 19.059842, Accuracy 0.077%\n",
      "Epoch 1, Batch 984, LR 0.274748 Loss 19.059842, Accuracy 0.077%\n",
      "Epoch 1, Batch 985, LR 0.274893 Loss 19.059831, Accuracy 0.078%\n",
      "Epoch 1, Batch 986, LR 0.275037 Loss 19.059614, Accuracy 0.079%\n",
      "Epoch 1, Batch 987, LR 0.275182 Loss 19.059618, Accuracy 0.079%\n",
      "Epoch 1, Batch 988, LR 0.275326 Loss 19.059619, Accuracy 0.079%\n",
      "Epoch 1, Batch 989, LR 0.275471 Loss 19.059641, Accuracy 0.079%\n",
      "Epoch 1, Batch 990, LR 0.275616 Loss 19.059597, Accuracy 0.079%\n",
      "Epoch 1, Batch 991, LR 0.275761 Loss 19.059674, Accuracy 0.079%\n",
      "Epoch 1, Batch 992, LR 0.275905 Loss 19.059613, Accuracy 0.079%\n",
      "Epoch 1, Batch 993, LR 0.276050 Loss 19.059530, Accuracy 0.079%\n",
      "Epoch 1, Batch 994, LR 0.276195 Loss 19.059430, Accuracy 0.079%\n",
      "Epoch 1, Batch 995, LR 0.276340 Loss 19.059523, Accuracy 0.079%\n",
      "Epoch 1, Batch 996, LR 0.276485 Loss 19.059709, Accuracy 0.078%\n",
      "Epoch 1, Batch 997, LR 0.276631 Loss 19.059710, Accuracy 0.078%\n",
      "Epoch 1, Batch 998, LR 0.276776 Loss 19.059968, Accuracy 0.078%\n",
      "Epoch 1, Batch 999, LR 0.276921 Loss 19.059866, Accuracy 0.078%\n",
      "Epoch 1, Batch 1000, LR 0.277066 Loss 19.059952, Accuracy 0.078%\n",
      "Epoch 1, Batch 1001, LR 0.277212 Loss 19.059935, Accuracy 0.078%\n",
      "Epoch 1, Batch 1002, LR 0.277357 Loss 19.059660, Accuracy 0.078%\n",
      "Epoch 1, Batch 1003, LR 0.277503 Loss 19.059802, Accuracy 0.078%\n",
      "Epoch 1, Batch 1004, LR 0.277648 Loss 19.059819, Accuracy 0.078%\n",
      "Epoch 1, Batch 1005, LR 0.277794 Loss 19.059838, Accuracy 0.078%\n",
      "Epoch 1, Batch 1006, LR 0.277940 Loss 19.060032, Accuracy 0.078%\n",
      "Epoch 1, Batch 1007, LR 0.278086 Loss 19.059874, Accuracy 0.078%\n",
      "Epoch 1, Batch 1008, LR 0.278232 Loss 19.059748, Accuracy 0.078%\n",
      "Epoch 1, Batch 1009, LR 0.278378 Loss 19.059806, Accuracy 0.077%\n",
      "Epoch 1, Batch 1010, LR 0.278524 Loss 19.059857, Accuracy 0.077%\n",
      "Epoch 1, Batch 1011, LR 0.278670 Loss 19.059811, Accuracy 0.078%\n",
      "Epoch 1, Batch 1012, LR 0.278816 Loss 19.059944, Accuracy 0.078%\n",
      "Epoch 1, Batch 1013, LR 0.278962 Loss 19.059986, Accuracy 0.078%\n",
      "Epoch 1, Batch 1014, LR 0.279108 Loss 19.060033, Accuracy 0.078%\n",
      "Epoch 1, Batch 1015, LR 0.279255 Loss 19.060113, Accuracy 0.078%\n",
      "Epoch 1, Batch 1016, LR 0.279401 Loss 19.060276, Accuracy 0.078%\n",
      "Epoch 1, Batch 1017, LR 0.279547 Loss 19.060443, Accuracy 0.078%\n",
      "Epoch 1, Batch 1018, LR 0.279694 Loss 19.060429, Accuracy 0.078%\n",
      "Epoch 1, Batch 1019, LR 0.279841 Loss 19.060282, Accuracy 0.077%\n",
      "Epoch 1, Batch 1020, LR 0.279987 Loss 19.060226, Accuracy 0.077%\n",
      "Epoch 1, Batch 1021, LR 0.280134 Loss 19.060238, Accuracy 0.077%\n",
      "Epoch 1, Batch 1022, LR 0.280281 Loss 19.060170, Accuracy 0.077%\n",
      "Epoch 1, Batch 1023, LR 0.280427 Loss 19.060180, Accuracy 0.077%\n",
      "Epoch 1, Batch 1024, LR 0.280574 Loss 19.060247, Accuracy 0.078%\n",
      "Epoch 1, Batch 1025, LR 0.280721 Loss 19.060227, Accuracy 0.078%\n",
      "Epoch 1, Batch 1026, LR 0.280868 Loss 19.060253, Accuracy 0.078%\n",
      "Epoch 1, Batch 1027, LR 0.281016 Loss 19.060130, Accuracy 0.078%\n",
      "Epoch 1, Batch 1028, LR 0.281163 Loss 19.060200, Accuracy 0.078%\n",
      "Epoch 1, Batch 1029, LR 0.281310 Loss 19.060092, Accuracy 0.078%\n",
      "Epoch 1, Batch 1030, LR 0.281457 Loss 19.060089, Accuracy 0.079%\n",
      "Epoch 1, Batch 1031, LR 0.281605 Loss 19.060027, Accuracy 0.079%\n",
      "Epoch 1, Batch 1032, LR 0.281752 Loss 19.060154, Accuracy 0.079%\n",
      "Epoch 1, Batch 1033, LR 0.281899 Loss 19.060046, Accuracy 0.079%\n",
      "Epoch 1, Batch 1034, LR 0.282047 Loss 19.060035, Accuracy 0.079%\n",
      "Epoch 1, Batch 1035, LR 0.282195 Loss 19.059936, Accuracy 0.079%\n",
      "Epoch 1, Batch 1036, LR 0.282342 Loss 19.059870, Accuracy 0.078%\n",
      "Epoch 1, Batch 1037, LR 0.282490 Loss 19.059835, Accuracy 0.078%\n",
      "Epoch 1, Batch 1038, LR 0.282638 Loss 19.059868, Accuracy 0.078%\n",
      "Epoch 1, Batch 1039, LR 0.282786 Loss 19.059885, Accuracy 0.078%\n",
      "Epoch 1, Batch 1040, LR 0.282934 Loss 19.059973, Accuracy 0.078%\n",
      "Epoch 1, Batch 1041, LR 0.283082 Loss 19.059947, Accuracy 0.078%\n",
      "Epoch 1, Batch 1042, LR 0.283230 Loss 19.060053, Accuracy 0.078%\n",
      "Epoch 1, Batch 1043, LR 0.283378 Loss 19.060110, Accuracy 0.078%\n",
      "Epoch 1, Batch 1044, LR 0.283526 Loss 19.060040, Accuracy 0.078%\n",
      "Epoch 1, Batch 1045, LR 0.283674 Loss 19.060035, Accuracy 0.078%\n",
      "Epoch 1, Batch 1046, LR 0.283823 Loss 19.059998, Accuracy 0.078%\n",
      "Epoch 1, Batch 1047, LR 0.283971 Loss 19.060007, Accuracy 0.078%\n",
      "Epoch 1, Loss (train set) 19.060007, Accuracy (train set) 0.078%\n",
      "Epoch 2, Batch 1, LR 0.284120 Loss 19.010607, Accuracy 0.000%\n",
      "Epoch 2, Batch 2, LR 0.284268 Loss 19.115802, Accuracy 0.000%\n",
      "Epoch 2, Batch 3, LR 0.284417 Loss 19.081040, Accuracy 0.000%\n",
      "Epoch 2, Batch 4, LR 0.284565 Loss 19.127560, Accuracy 0.000%\n",
      "Epoch 2, Batch 5, LR 0.284714 Loss 19.082555, Accuracy 0.000%\n",
      "Epoch 2, Batch 6, LR 0.284863 Loss 19.051205, Accuracy 0.000%\n",
      "Epoch 2, Batch 7, LR 0.285012 Loss 19.077721, Accuracy 0.000%\n",
      "Epoch 2, Batch 8, LR 0.285161 Loss 19.073055, Accuracy 0.000%\n",
      "Epoch 2, Batch 9, LR 0.285310 Loss 19.063350, Accuracy 0.087%\n",
      "Epoch 2, Batch 10, LR 0.285459 Loss 19.073429, Accuracy 0.156%\n",
      "Epoch 2, Batch 11, LR 0.285608 Loss 19.082394, Accuracy 0.142%\n",
      "Epoch 2, Batch 12, LR 0.285757 Loss 19.104833, Accuracy 0.130%\n",
      "Epoch 2, Batch 13, LR 0.285906 Loss 19.106488, Accuracy 0.120%\n",
      "Epoch 2, Batch 14, LR 0.286055 Loss 19.098552, Accuracy 0.112%\n",
      "Epoch 2, Batch 15, LR 0.286205 Loss 19.092274, Accuracy 0.156%\n",
      "Epoch 2, Batch 16, LR 0.286354 Loss 19.091282, Accuracy 0.146%\n",
      "Epoch 2, Batch 17, LR 0.286504 Loss 19.096970, Accuracy 0.138%\n",
      "Epoch 2, Batch 18, LR 0.286653 Loss 19.091219, Accuracy 0.174%\n",
      "Epoch 2, Batch 19, LR 0.286803 Loss 19.093872, Accuracy 0.206%\n",
      "Epoch 2, Batch 20, LR 0.286952 Loss 19.089272, Accuracy 0.195%\n",
      "Epoch 2, Batch 21, LR 0.287102 Loss 19.082297, Accuracy 0.186%\n",
      "Epoch 2, Batch 22, LR 0.287252 Loss 19.083308, Accuracy 0.178%\n",
      "Epoch 2, Batch 23, LR 0.287402 Loss 19.088942, Accuracy 0.170%\n",
      "Epoch 2, Batch 24, LR 0.287552 Loss 19.089235, Accuracy 0.163%\n",
      "Epoch 2, Batch 25, LR 0.287702 Loss 19.094632, Accuracy 0.156%\n",
      "Epoch 2, Batch 26, LR 0.287852 Loss 19.082708, Accuracy 0.150%\n",
      "Epoch 2, Batch 27, LR 0.288002 Loss 19.085730, Accuracy 0.145%\n",
      "Epoch 2, Batch 28, LR 0.288152 Loss 19.086367, Accuracy 0.140%\n",
      "Epoch 2, Batch 29, LR 0.288302 Loss 19.081533, Accuracy 0.135%\n",
      "Epoch 2, Batch 30, LR 0.288453 Loss 19.080875, Accuracy 0.130%\n",
      "Epoch 2, Batch 31, LR 0.288603 Loss 19.075209, Accuracy 0.126%\n",
      "Epoch 2, Batch 32, LR 0.288754 Loss 19.070106, Accuracy 0.122%\n",
      "Epoch 2, Batch 33, LR 0.288904 Loss 19.066367, Accuracy 0.118%\n",
      "Epoch 2, Batch 34, LR 0.289055 Loss 19.066051, Accuracy 0.138%\n",
      "Epoch 2, Batch 35, LR 0.289205 Loss 19.062655, Accuracy 0.134%\n",
      "Epoch 2, Batch 36, LR 0.289356 Loss 19.062004, Accuracy 0.152%\n",
      "Epoch 2, Batch 37, LR 0.289507 Loss 19.060315, Accuracy 0.148%\n",
      "Epoch 2, Batch 38, LR 0.289658 Loss 19.064387, Accuracy 0.144%\n",
      "Epoch 2, Batch 39, LR 0.289808 Loss 19.060162, Accuracy 0.140%\n",
      "Epoch 2, Batch 40, LR 0.289959 Loss 19.058845, Accuracy 0.137%\n",
      "Epoch 2, Batch 41, LR 0.290110 Loss 19.059874, Accuracy 0.133%\n",
      "Epoch 2, Batch 42, LR 0.290262 Loss 19.062100, Accuracy 0.130%\n",
      "Epoch 2, Batch 43, LR 0.290413 Loss 19.064877, Accuracy 0.127%\n",
      "Epoch 2, Batch 44, LR 0.290564 Loss 19.064372, Accuracy 0.124%\n",
      "Epoch 2, Batch 45, LR 0.290715 Loss 19.062336, Accuracy 0.122%\n",
      "Epoch 2, Batch 46, LR 0.290867 Loss 19.059807, Accuracy 0.119%\n",
      "Epoch 2, Batch 47, LR 0.291018 Loss 19.059137, Accuracy 0.133%\n",
      "Epoch 2, Batch 48, LR 0.291169 Loss 19.059960, Accuracy 0.130%\n",
      "Epoch 2, Batch 49, LR 0.291321 Loss 19.059292, Accuracy 0.128%\n",
      "Epoch 2, Batch 50, LR 0.291473 Loss 19.059704, Accuracy 0.125%\n",
      "Epoch 2, Batch 51, LR 0.291624 Loss 19.058442, Accuracy 0.123%\n",
      "Epoch 2, Batch 52, LR 0.291776 Loss 19.057699, Accuracy 0.120%\n",
      "Epoch 2, Batch 53, LR 0.291928 Loss 19.061642, Accuracy 0.133%\n",
      "Epoch 2, Batch 54, LR 0.292080 Loss 19.062850, Accuracy 0.130%\n",
      "Epoch 2, Batch 55, LR 0.292231 Loss 19.062075, Accuracy 0.128%\n",
      "Epoch 2, Batch 56, LR 0.292383 Loss 19.061353, Accuracy 0.126%\n",
      "Epoch 2, Batch 57, LR 0.292535 Loss 19.061295, Accuracy 0.123%\n",
      "Epoch 2, Batch 58, LR 0.292688 Loss 19.061014, Accuracy 0.121%\n",
      "Epoch 2, Batch 59, LR 0.292840 Loss 19.060276, Accuracy 0.132%\n",
      "Epoch 2, Batch 60, LR 0.292992 Loss 19.063423, Accuracy 0.130%\n",
      "Epoch 2, Batch 61, LR 0.293144 Loss 19.060824, Accuracy 0.128%\n",
      "Epoch 2, Batch 62, LR 0.293297 Loss 19.060269, Accuracy 0.126%\n",
      "Epoch 2, Batch 63, LR 0.293449 Loss 19.062300, Accuracy 0.124%\n",
      "Epoch 2, Batch 64, LR 0.293602 Loss 19.061445, Accuracy 0.122%\n",
      "Epoch 2, Batch 65, LR 0.293754 Loss 19.061062, Accuracy 0.132%\n",
      "Epoch 2, Batch 66, LR 0.293907 Loss 19.060850, Accuracy 0.130%\n",
      "Epoch 2, Batch 67, LR 0.294059 Loss 19.061837, Accuracy 0.128%\n",
      "Epoch 2, Batch 68, LR 0.294212 Loss 19.064219, Accuracy 0.126%\n",
      "Epoch 2, Batch 69, LR 0.294365 Loss 19.059136, Accuracy 0.136%\n",
      "Epoch 2, Batch 70, LR 0.294518 Loss 19.060815, Accuracy 0.134%\n",
      "Epoch 2, Batch 71, LR 0.294671 Loss 19.060274, Accuracy 0.132%\n",
      "Epoch 2, Batch 72, LR 0.294824 Loss 19.059404, Accuracy 0.130%\n",
      "Epoch 2, Batch 73, LR 0.294977 Loss 19.063055, Accuracy 0.128%\n",
      "Epoch 2, Batch 74, LR 0.295130 Loss 19.061594, Accuracy 0.127%\n",
      "Epoch 2, Batch 75, LR 0.295283 Loss 19.064580, Accuracy 0.125%\n",
      "Epoch 2, Batch 76, LR 0.295436 Loss 19.065788, Accuracy 0.123%\n",
      "Epoch 2, Batch 77, LR 0.295590 Loss 19.064930, Accuracy 0.122%\n",
      "Epoch 2, Batch 78, LR 0.295743 Loss 19.066105, Accuracy 0.120%\n",
      "Epoch 2, Batch 79, LR 0.295897 Loss 19.067447, Accuracy 0.129%\n",
      "Epoch 2, Batch 80, LR 0.296050 Loss 19.068438, Accuracy 0.127%\n",
      "Epoch 2, Batch 81, LR 0.296204 Loss 19.066311, Accuracy 0.125%\n",
      "Epoch 2, Batch 82, LR 0.296357 Loss 19.065236, Accuracy 0.124%\n",
      "Epoch 2, Batch 83, LR 0.296511 Loss 19.069868, Accuracy 0.122%\n",
      "Epoch 2, Batch 84, LR 0.296665 Loss 19.065944, Accuracy 0.121%\n",
      "Epoch 2, Batch 85, LR 0.296819 Loss 19.067350, Accuracy 0.119%\n",
      "Epoch 2, Batch 86, LR 0.296972 Loss 19.067049, Accuracy 0.136%\n",
      "Epoch 2, Batch 87, LR 0.297126 Loss 19.066792, Accuracy 0.135%\n",
      "Epoch 2, Batch 88, LR 0.297280 Loss 19.068833, Accuracy 0.133%\n",
      "Epoch 2, Batch 89, LR 0.297434 Loss 19.069583, Accuracy 0.132%\n",
      "Epoch 2, Batch 90, LR 0.297589 Loss 19.068660, Accuracy 0.130%\n",
      "Epoch 2, Batch 91, LR 0.297743 Loss 19.069113, Accuracy 0.129%\n",
      "Epoch 2, Batch 92, LR 0.297897 Loss 19.069103, Accuracy 0.127%\n",
      "Epoch 2, Batch 93, LR 0.298051 Loss 19.070546, Accuracy 0.134%\n",
      "Epoch 2, Batch 94, LR 0.298206 Loss 19.071052, Accuracy 0.133%\n",
      "Epoch 2, Batch 95, LR 0.298360 Loss 19.071830, Accuracy 0.132%\n",
      "Epoch 2, Batch 96, LR 0.298515 Loss 19.070840, Accuracy 0.138%\n",
      "Epoch 2, Batch 97, LR 0.298669 Loss 19.072102, Accuracy 0.137%\n",
      "Epoch 2, Batch 98, LR 0.298824 Loss 19.070600, Accuracy 0.136%\n",
      "Epoch 2, Batch 99, LR 0.298979 Loss 19.069060, Accuracy 0.134%\n",
      "Epoch 2, Batch 100, LR 0.299134 Loss 19.071005, Accuracy 0.133%\n",
      "Epoch 2, Batch 101, LR 0.299288 Loss 19.070151, Accuracy 0.131%\n",
      "Epoch 2, Batch 102, LR 0.299443 Loss 19.072263, Accuracy 0.130%\n",
      "Epoch 2, Batch 103, LR 0.299598 Loss 19.072623, Accuracy 0.129%\n",
      "Epoch 2, Batch 104, LR 0.299753 Loss 19.072741, Accuracy 0.128%\n",
      "Epoch 2, Batch 105, LR 0.299908 Loss 19.074292, Accuracy 0.126%\n",
      "Epoch 2, Batch 106, LR 0.300064 Loss 19.076043, Accuracy 0.125%\n",
      "Epoch 2, Batch 107, LR 0.300219 Loss 19.076840, Accuracy 0.124%\n",
      "Epoch 2, Batch 108, LR 0.300374 Loss 19.077273, Accuracy 0.123%\n",
      "Epoch 2, Batch 109, LR 0.300529 Loss 19.077441, Accuracy 0.122%\n",
      "Epoch 2, Batch 110, LR 0.300685 Loss 19.080027, Accuracy 0.121%\n",
      "Epoch 2, Batch 111, LR 0.300840 Loss 19.078865, Accuracy 0.120%\n",
      "Epoch 2, Batch 112, LR 0.300996 Loss 19.079430, Accuracy 0.119%\n",
      "Epoch 2, Batch 113, LR 0.301151 Loss 19.077813, Accuracy 0.118%\n",
      "Epoch 2, Batch 114, LR 0.301307 Loss 19.077063, Accuracy 0.117%\n",
      "Epoch 2, Batch 115, LR 0.301463 Loss 19.077329, Accuracy 0.115%\n",
      "Epoch 2, Batch 116, LR 0.301619 Loss 19.076868, Accuracy 0.114%\n",
      "Epoch 2, Batch 117, LR 0.301774 Loss 19.075501, Accuracy 0.114%\n",
      "Epoch 2, Batch 118, LR 0.301930 Loss 19.075913, Accuracy 0.113%\n",
      "Epoch 2, Batch 119, LR 0.302086 Loss 19.074871, Accuracy 0.112%\n",
      "Epoch 2, Batch 120, LR 0.302242 Loss 19.074775, Accuracy 0.111%\n",
      "Epoch 2, Batch 121, LR 0.302398 Loss 19.074895, Accuracy 0.110%\n",
      "Epoch 2, Batch 122, LR 0.302555 Loss 19.075370, Accuracy 0.109%\n",
      "Epoch 2, Batch 123, LR 0.302711 Loss 19.075077, Accuracy 0.108%\n",
      "Epoch 2, Batch 124, LR 0.302867 Loss 19.072877, Accuracy 0.107%\n",
      "Epoch 2, Batch 125, LR 0.303024 Loss 19.074095, Accuracy 0.106%\n",
      "Epoch 2, Batch 126, LR 0.303180 Loss 19.073563, Accuracy 0.105%\n",
      "Epoch 2, Batch 127, LR 0.303336 Loss 19.073784, Accuracy 0.111%\n",
      "Epoch 2, Batch 128, LR 0.303493 Loss 19.074911, Accuracy 0.110%\n",
      "Epoch 2, Batch 129, LR 0.303650 Loss 19.074374, Accuracy 0.109%\n",
      "Epoch 2, Batch 130, LR 0.303806 Loss 19.074906, Accuracy 0.108%\n",
      "Epoch 2, Batch 131, LR 0.303963 Loss 19.074792, Accuracy 0.107%\n",
      "Epoch 2, Batch 132, LR 0.304120 Loss 19.075020, Accuracy 0.107%\n",
      "Epoch 2, Batch 133, LR 0.304277 Loss 19.074139, Accuracy 0.106%\n",
      "Epoch 2, Batch 134, LR 0.304434 Loss 19.073964, Accuracy 0.105%\n",
      "Epoch 2, Batch 135, LR 0.304591 Loss 19.074391, Accuracy 0.104%\n",
      "Epoch 2, Batch 136, LR 0.304748 Loss 19.074415, Accuracy 0.103%\n",
      "Epoch 2, Batch 137, LR 0.304905 Loss 19.074759, Accuracy 0.103%\n",
      "Epoch 2, Batch 138, LR 0.305062 Loss 19.074309, Accuracy 0.102%\n",
      "Epoch 2, Batch 139, LR 0.305219 Loss 19.074664, Accuracy 0.101%\n",
      "Epoch 2, Batch 140, LR 0.305376 Loss 19.074073, Accuracy 0.100%\n",
      "Epoch 2, Batch 141, LR 0.305534 Loss 19.074579, Accuracy 0.100%\n",
      "Epoch 2, Batch 142, LR 0.305691 Loss 19.075839, Accuracy 0.099%\n",
      "Epoch 2, Batch 143, LR 0.305849 Loss 19.076262, Accuracy 0.098%\n",
      "Epoch 2, Batch 144, LR 0.306006 Loss 19.075790, Accuracy 0.098%\n",
      "Epoch 2, Batch 145, LR 0.306164 Loss 19.076138, Accuracy 0.108%\n",
      "Epoch 2, Batch 146, LR 0.306322 Loss 19.076866, Accuracy 0.112%\n",
      "Epoch 2, Batch 147, LR 0.306479 Loss 19.077370, Accuracy 0.112%\n",
      "Epoch 2, Batch 148, LR 0.306637 Loss 19.078955, Accuracy 0.111%\n",
      "Epoch 2, Batch 149, LR 0.306795 Loss 19.079729, Accuracy 0.110%\n",
      "Epoch 2, Batch 150, LR 0.306953 Loss 19.079248, Accuracy 0.109%\n",
      "Epoch 2, Batch 151, LR 0.307111 Loss 19.079879, Accuracy 0.114%\n",
      "Epoch 2, Batch 152, LR 0.307269 Loss 19.078383, Accuracy 0.113%\n",
      "Epoch 2, Batch 153, LR 0.307427 Loss 19.077382, Accuracy 0.112%\n",
      "Epoch 2, Batch 154, LR 0.307585 Loss 19.077705, Accuracy 0.112%\n",
      "Epoch 2, Batch 155, LR 0.307744 Loss 19.078185, Accuracy 0.111%\n",
      "Epoch 2, Batch 156, LR 0.307902 Loss 19.079340, Accuracy 0.110%\n",
      "Epoch 2, Batch 157, LR 0.308060 Loss 19.078655, Accuracy 0.109%\n",
      "Epoch 2, Batch 158, LR 0.308219 Loss 19.078671, Accuracy 0.109%\n",
      "Epoch 2, Batch 159, LR 0.308377 Loss 19.079256, Accuracy 0.108%\n",
      "Epoch 2, Batch 160, LR 0.308536 Loss 19.078481, Accuracy 0.107%\n",
      "Epoch 2, Batch 161, LR 0.308694 Loss 19.078425, Accuracy 0.107%\n",
      "Epoch 2, Batch 162, LR 0.308853 Loss 19.078431, Accuracy 0.106%\n",
      "Epoch 2, Batch 163, LR 0.309012 Loss 19.077438, Accuracy 0.110%\n",
      "Epoch 2, Batch 164, LR 0.309171 Loss 19.077044, Accuracy 0.110%\n",
      "Epoch 2, Batch 165, LR 0.309329 Loss 19.077810, Accuracy 0.109%\n",
      "Epoch 2, Batch 166, LR 0.309488 Loss 19.078761, Accuracy 0.108%\n",
      "Epoch 2, Batch 167, LR 0.309647 Loss 19.077887, Accuracy 0.108%\n",
      "Epoch 2, Batch 168, LR 0.309806 Loss 19.079326, Accuracy 0.107%\n",
      "Epoch 2, Batch 169, LR 0.309966 Loss 19.079964, Accuracy 0.106%\n",
      "Epoch 2, Batch 170, LR 0.310125 Loss 19.080005, Accuracy 0.110%\n",
      "Epoch 2, Batch 171, LR 0.310284 Loss 19.078389, Accuracy 0.110%\n",
      "Epoch 2, Batch 172, LR 0.310443 Loss 19.077852, Accuracy 0.109%\n",
      "Epoch 2, Batch 173, LR 0.310603 Loss 19.077942, Accuracy 0.108%\n",
      "Epoch 2, Batch 174, LR 0.310762 Loss 19.077662, Accuracy 0.108%\n",
      "Epoch 2, Batch 175, LR 0.310922 Loss 19.076807, Accuracy 0.112%\n",
      "Epoch 2, Batch 176, LR 0.311081 Loss 19.077558, Accuracy 0.111%\n",
      "Epoch 2, Batch 177, LR 0.311241 Loss 19.077964, Accuracy 0.110%\n",
      "Epoch 2, Batch 178, LR 0.311401 Loss 19.078666, Accuracy 0.110%\n",
      "Epoch 2, Batch 179, LR 0.311560 Loss 19.077968, Accuracy 0.109%\n",
      "Epoch 2, Batch 180, LR 0.311720 Loss 19.078807, Accuracy 0.109%\n",
      "Epoch 2, Batch 181, LR 0.311880 Loss 19.077916, Accuracy 0.108%\n",
      "Epoch 2, Batch 182, LR 0.312040 Loss 19.077455, Accuracy 0.107%\n",
      "Epoch 2, Batch 183, LR 0.312200 Loss 19.077420, Accuracy 0.107%\n",
      "Epoch 2, Batch 184, LR 0.312360 Loss 19.076999, Accuracy 0.106%\n",
      "Epoch 2, Batch 185, LR 0.312520 Loss 19.076344, Accuracy 0.106%\n",
      "Epoch 2, Batch 186, LR 0.312680 Loss 19.076018, Accuracy 0.113%\n",
      "Epoch 2, Batch 187, LR 0.312841 Loss 19.076639, Accuracy 0.113%\n",
      "Epoch 2, Batch 188, LR 0.313001 Loss 19.076226, Accuracy 0.112%\n",
      "Epoch 2, Batch 189, LR 0.313161 Loss 19.075776, Accuracy 0.112%\n",
      "Epoch 2, Batch 190, LR 0.313322 Loss 19.075996, Accuracy 0.111%\n",
      "Epoch 2, Batch 191, LR 0.313482 Loss 19.075909, Accuracy 0.110%\n",
      "Epoch 2, Batch 192, LR 0.313643 Loss 19.076966, Accuracy 0.110%\n",
      "Epoch 2, Batch 193, LR 0.313803 Loss 19.077250, Accuracy 0.109%\n",
      "Epoch 2, Batch 194, LR 0.313964 Loss 19.075848, Accuracy 0.109%\n",
      "Epoch 2, Batch 195, LR 0.314125 Loss 19.076676, Accuracy 0.108%\n",
      "Epoch 2, Batch 196, LR 0.314286 Loss 19.076653, Accuracy 0.108%\n",
      "Epoch 2, Batch 197, LR 0.314447 Loss 19.076273, Accuracy 0.107%\n",
      "Epoch 2, Batch 198, LR 0.314608 Loss 19.076227, Accuracy 0.107%\n",
      "Epoch 2, Batch 199, LR 0.314769 Loss 19.075524, Accuracy 0.114%\n",
      "Epoch 2, Batch 200, LR 0.314930 Loss 19.075720, Accuracy 0.113%\n",
      "Epoch 2, Batch 201, LR 0.315091 Loss 19.075258, Accuracy 0.113%\n",
      "Epoch 2, Batch 202, LR 0.315252 Loss 19.076342, Accuracy 0.112%\n",
      "Epoch 2, Batch 203, LR 0.315413 Loss 19.075291, Accuracy 0.112%\n",
      "Epoch 2, Batch 204, LR 0.315574 Loss 19.075587, Accuracy 0.111%\n",
      "Epoch 2, Batch 205, LR 0.315736 Loss 19.076047, Accuracy 0.111%\n",
      "Epoch 2, Batch 206, LR 0.315897 Loss 19.076203, Accuracy 0.110%\n",
      "Epoch 2, Batch 207, LR 0.316059 Loss 19.076708, Accuracy 0.109%\n",
      "Epoch 2, Batch 208, LR 0.316220 Loss 19.075434, Accuracy 0.109%\n",
      "Epoch 2, Batch 209, LR 0.316382 Loss 19.075132, Accuracy 0.108%\n",
      "Epoch 2, Batch 210, LR 0.316544 Loss 19.074479, Accuracy 0.108%\n",
      "Epoch 2, Batch 211, LR 0.316705 Loss 19.074742, Accuracy 0.107%\n",
      "Epoch 2, Batch 212, LR 0.316867 Loss 19.074291, Accuracy 0.107%\n",
      "Epoch 2, Batch 213, LR 0.317029 Loss 19.074411, Accuracy 0.106%\n",
      "Epoch 2, Batch 214, LR 0.317191 Loss 19.074626, Accuracy 0.106%\n",
      "Epoch 2, Batch 215, LR 0.317353 Loss 19.074554, Accuracy 0.105%\n",
      "Epoch 2, Batch 216, LR 0.317515 Loss 19.074449, Accuracy 0.105%\n",
      "Epoch 2, Batch 217, LR 0.317677 Loss 19.073202, Accuracy 0.104%\n",
      "Epoch 2, Batch 218, LR 0.317839 Loss 19.072787, Accuracy 0.104%\n",
      "Epoch 2, Batch 219, LR 0.318002 Loss 19.072121, Accuracy 0.103%\n",
      "Epoch 2, Batch 220, LR 0.318164 Loss 19.072594, Accuracy 0.103%\n",
      "Epoch 2, Batch 221, LR 0.318326 Loss 19.072517, Accuracy 0.103%\n",
      "Epoch 2, Batch 222, LR 0.318489 Loss 19.072739, Accuracy 0.102%\n",
      "Epoch 2, Batch 223, LR 0.318651 Loss 19.072540, Accuracy 0.102%\n",
      "Epoch 2, Batch 224, LR 0.318814 Loss 19.072787, Accuracy 0.101%\n",
      "Epoch 2, Batch 225, LR 0.318976 Loss 19.072506, Accuracy 0.101%\n",
      "Epoch 2, Batch 226, LR 0.319139 Loss 19.072733, Accuracy 0.100%\n",
      "Epoch 2, Batch 227, LR 0.319302 Loss 19.072360, Accuracy 0.100%\n",
      "Epoch 2, Batch 228, LR 0.319465 Loss 19.072946, Accuracy 0.099%\n",
      "Epoch 2, Batch 229, LR 0.319628 Loss 19.071923, Accuracy 0.099%\n",
      "Epoch 2, Batch 230, LR 0.319790 Loss 19.071561, Accuracy 0.099%\n",
      "Epoch 2, Batch 231, LR 0.319953 Loss 19.071074, Accuracy 0.098%\n",
      "Epoch 2, Batch 232, LR 0.320117 Loss 19.069827, Accuracy 0.101%\n",
      "Epoch 2, Batch 233, LR 0.320280 Loss 19.069120, Accuracy 0.101%\n",
      "Epoch 2, Batch 234, LR 0.320443 Loss 19.068865, Accuracy 0.100%\n",
      "Epoch 2, Batch 235, LR 0.320606 Loss 19.068938, Accuracy 0.100%\n",
      "Epoch 2, Batch 236, LR 0.320769 Loss 19.068164, Accuracy 0.099%\n",
      "Epoch 2, Batch 237, LR 0.320933 Loss 19.067485, Accuracy 0.099%\n",
      "Epoch 2, Batch 238, LR 0.321096 Loss 19.067221, Accuracy 0.098%\n",
      "Epoch 2, Batch 239, LR 0.321260 Loss 19.067200, Accuracy 0.098%\n",
      "Epoch 2, Batch 240, LR 0.321423 Loss 19.067324, Accuracy 0.098%\n",
      "Epoch 2, Batch 241, LR 0.321587 Loss 19.068085, Accuracy 0.097%\n",
      "Epoch 2, Batch 242, LR 0.321751 Loss 19.067655, Accuracy 0.097%\n",
      "Epoch 2, Batch 243, LR 0.321914 Loss 19.067843, Accuracy 0.096%\n",
      "Epoch 2, Batch 244, LR 0.322078 Loss 19.068255, Accuracy 0.096%\n",
      "Epoch 2, Batch 245, LR 0.322242 Loss 19.068917, Accuracy 0.096%\n",
      "Epoch 2, Batch 246, LR 0.322406 Loss 19.069483, Accuracy 0.095%\n",
      "Epoch 2, Batch 247, LR 0.322570 Loss 19.068956, Accuracy 0.095%\n",
      "Epoch 2, Batch 248, LR 0.322734 Loss 19.069603, Accuracy 0.095%\n",
      "Epoch 2, Batch 249, LR 0.322898 Loss 19.070035, Accuracy 0.094%\n",
      "Epoch 2, Batch 250, LR 0.323062 Loss 19.069071, Accuracy 0.094%\n",
      "Epoch 2, Batch 251, LR 0.323226 Loss 19.068476, Accuracy 0.093%\n",
      "Epoch 2, Batch 252, LR 0.323391 Loss 19.068854, Accuracy 0.093%\n",
      "Epoch 2, Batch 253, LR 0.323555 Loss 19.068701, Accuracy 0.093%\n",
      "Epoch 2, Batch 254, LR 0.323719 Loss 19.067813, Accuracy 0.092%\n",
      "Epoch 2, Batch 255, LR 0.323884 Loss 19.067929, Accuracy 0.092%\n",
      "Epoch 2, Batch 256, LR 0.324048 Loss 19.067778, Accuracy 0.092%\n",
      "Epoch 2, Batch 257, LR 0.324213 Loss 19.067529, Accuracy 0.094%\n",
      "Epoch 2, Batch 258, LR 0.324378 Loss 19.067935, Accuracy 0.094%\n",
      "Epoch 2, Batch 259, LR 0.324542 Loss 19.067891, Accuracy 0.094%\n",
      "Epoch 2, Batch 260, LR 0.324707 Loss 19.068526, Accuracy 0.093%\n",
      "Epoch 2, Batch 261, LR 0.324872 Loss 19.068481, Accuracy 0.093%\n",
      "Epoch 2, Batch 262, LR 0.325037 Loss 19.068432, Accuracy 0.095%\n",
      "Epoch 2, Batch 263, LR 0.325202 Loss 19.068151, Accuracy 0.095%\n",
      "Epoch 2, Batch 264, LR 0.325367 Loss 19.068394, Accuracy 0.095%\n",
      "Epoch 2, Batch 265, LR 0.325532 Loss 19.068318, Accuracy 0.097%\n",
      "Epoch 2, Batch 266, LR 0.325697 Loss 19.068385, Accuracy 0.097%\n",
      "Epoch 2, Batch 267, LR 0.325863 Loss 19.069091, Accuracy 0.097%\n",
      "Epoch 2, Batch 268, LR 0.326028 Loss 19.069489, Accuracy 0.096%\n",
      "Epoch 2, Batch 269, LR 0.326193 Loss 19.069147, Accuracy 0.096%\n",
      "Epoch 2, Batch 270, LR 0.326359 Loss 19.069083, Accuracy 0.095%\n",
      "Epoch 2, Batch 271, LR 0.326524 Loss 19.069709, Accuracy 0.095%\n",
      "Epoch 2, Batch 272, LR 0.326690 Loss 19.068792, Accuracy 0.095%\n",
      "Epoch 2, Batch 273, LR 0.326855 Loss 19.068667, Accuracy 0.094%\n",
      "Epoch 2, Batch 274, LR 0.327021 Loss 19.068351, Accuracy 0.094%\n",
      "Epoch 2, Batch 275, LR 0.327187 Loss 19.068424, Accuracy 0.094%\n",
      "Epoch 2, Batch 276, LR 0.327352 Loss 19.068202, Accuracy 0.093%\n",
      "Epoch 2, Batch 277, LR 0.327518 Loss 19.068267, Accuracy 0.093%\n",
      "Epoch 2, Batch 278, LR 0.327684 Loss 19.068035, Accuracy 0.093%\n",
      "Epoch 2, Batch 279, LR 0.327850 Loss 19.067572, Accuracy 0.092%\n",
      "Epoch 2, Batch 280, LR 0.328016 Loss 19.067299, Accuracy 0.092%\n",
      "Epoch 2, Batch 281, LR 0.328182 Loss 19.067773, Accuracy 0.092%\n",
      "Epoch 2, Batch 282, LR 0.328348 Loss 19.068239, Accuracy 0.091%\n",
      "Epoch 2, Batch 283, LR 0.328514 Loss 19.067571, Accuracy 0.091%\n",
      "Epoch 2, Batch 284, LR 0.328681 Loss 19.067132, Accuracy 0.091%\n",
      "Epoch 2, Batch 285, LR 0.328847 Loss 19.067058, Accuracy 0.090%\n",
      "Epoch 2, Batch 286, LR 0.329013 Loss 19.066786, Accuracy 0.090%\n",
      "Epoch 2, Batch 287, LR 0.329180 Loss 19.066735, Accuracy 0.090%\n",
      "Epoch 2, Batch 288, LR 0.329346 Loss 19.065952, Accuracy 0.090%\n",
      "Epoch 2, Batch 289, LR 0.329513 Loss 19.065854, Accuracy 0.089%\n",
      "Epoch 2, Batch 290, LR 0.329680 Loss 19.065497, Accuracy 0.089%\n",
      "Epoch 2, Batch 291, LR 0.329846 Loss 19.065654, Accuracy 0.089%\n",
      "Epoch 2, Batch 292, LR 0.330013 Loss 19.066167, Accuracy 0.088%\n",
      "Epoch 2, Batch 293, LR 0.330180 Loss 19.065837, Accuracy 0.088%\n",
      "Epoch 2, Batch 294, LR 0.330347 Loss 19.065645, Accuracy 0.088%\n",
      "Epoch 2, Batch 295, LR 0.330514 Loss 19.065586, Accuracy 0.087%\n",
      "Epoch 2, Batch 296, LR 0.330681 Loss 19.065486, Accuracy 0.087%\n",
      "Epoch 2, Batch 297, LR 0.330848 Loss 19.064784, Accuracy 0.087%\n",
      "Epoch 2, Batch 298, LR 0.331015 Loss 19.064325, Accuracy 0.087%\n",
      "Epoch 2, Batch 299, LR 0.331182 Loss 19.064489, Accuracy 0.086%\n",
      "Epoch 2, Batch 300, LR 0.331349 Loss 19.064238, Accuracy 0.086%\n",
      "Epoch 2, Batch 301, LR 0.331517 Loss 19.064186, Accuracy 0.086%\n",
      "Epoch 2, Batch 302, LR 0.331684 Loss 19.063524, Accuracy 0.088%\n",
      "Epoch 2, Batch 303, LR 0.331852 Loss 19.062911, Accuracy 0.088%\n",
      "Epoch 2, Batch 304, LR 0.332019 Loss 19.063016, Accuracy 0.087%\n",
      "Epoch 2, Batch 305, LR 0.332187 Loss 19.062358, Accuracy 0.087%\n",
      "Epoch 2, Batch 306, LR 0.332354 Loss 19.062414, Accuracy 0.087%\n",
      "Epoch 2, Batch 307, LR 0.332522 Loss 19.062261, Accuracy 0.087%\n",
      "Epoch 2, Batch 308, LR 0.332690 Loss 19.062641, Accuracy 0.086%\n",
      "Epoch 2, Batch 309, LR 0.332857 Loss 19.063244, Accuracy 0.086%\n",
      "Epoch 2, Batch 310, LR 0.333025 Loss 19.063289, Accuracy 0.086%\n",
      "Epoch 2, Batch 311, LR 0.333193 Loss 19.063234, Accuracy 0.088%\n",
      "Epoch 2, Batch 312, LR 0.333361 Loss 19.062518, Accuracy 0.088%\n",
      "Epoch 2, Batch 313, LR 0.333529 Loss 19.062195, Accuracy 0.090%\n",
      "Epoch 2, Batch 314, LR 0.333697 Loss 19.061604, Accuracy 0.090%\n",
      "Epoch 2, Batch 315, LR 0.333866 Loss 19.061942, Accuracy 0.089%\n",
      "Epoch 2, Batch 316, LR 0.334034 Loss 19.062301, Accuracy 0.089%\n",
      "Epoch 2, Batch 317, LR 0.334202 Loss 19.062170, Accuracy 0.089%\n",
      "Epoch 2, Batch 318, LR 0.334370 Loss 19.061539, Accuracy 0.088%\n",
      "Epoch 2, Batch 319, LR 0.334539 Loss 19.061510, Accuracy 0.088%\n",
      "Epoch 2, Batch 320, LR 0.334707 Loss 19.061733, Accuracy 0.088%\n",
      "Epoch 2, Batch 321, LR 0.334876 Loss 19.061157, Accuracy 0.088%\n",
      "Epoch 2, Batch 322, LR 0.335044 Loss 19.061673, Accuracy 0.087%\n",
      "Epoch 2, Batch 323, LR 0.335213 Loss 19.061764, Accuracy 0.087%\n",
      "Epoch 2, Batch 324, LR 0.335382 Loss 19.062234, Accuracy 0.087%\n",
      "Epoch 2, Batch 325, LR 0.335551 Loss 19.061991, Accuracy 0.087%\n",
      "Epoch 2, Batch 326, LR 0.335719 Loss 19.062058, Accuracy 0.086%\n",
      "Epoch 2, Batch 327, LR 0.335888 Loss 19.062244, Accuracy 0.086%\n",
      "Epoch 2, Batch 328, LR 0.336057 Loss 19.062284, Accuracy 0.086%\n",
      "Epoch 2, Batch 329, LR 0.336226 Loss 19.062313, Accuracy 0.085%\n",
      "Epoch 2, Batch 330, LR 0.336395 Loss 19.062292, Accuracy 0.085%\n",
      "Epoch 2, Batch 331, LR 0.336565 Loss 19.062271, Accuracy 0.087%\n",
      "Epoch 2, Batch 332, LR 0.336734 Loss 19.062572, Accuracy 0.087%\n",
      "Epoch 2, Batch 333, LR 0.336903 Loss 19.062599, Accuracy 0.087%\n",
      "Epoch 2, Batch 334, LR 0.337072 Loss 19.062574, Accuracy 0.089%\n",
      "Epoch 2, Batch 335, LR 0.337242 Loss 19.063121, Accuracy 0.089%\n",
      "Epoch 2, Batch 336, LR 0.337411 Loss 19.062845, Accuracy 0.088%\n",
      "Epoch 2, Batch 337, LR 0.337581 Loss 19.062510, Accuracy 0.088%\n",
      "Epoch 2, Batch 338, LR 0.337750 Loss 19.062244, Accuracy 0.088%\n",
      "Epoch 2, Batch 339, LR 0.337920 Loss 19.061645, Accuracy 0.090%\n",
      "Epoch 2, Batch 340, LR 0.338090 Loss 19.061902, Accuracy 0.090%\n",
      "Epoch 2, Batch 341, LR 0.338259 Loss 19.061284, Accuracy 0.092%\n",
      "Epoch 2, Batch 342, LR 0.338429 Loss 19.061167, Accuracy 0.091%\n",
      "Epoch 2, Batch 343, LR 0.338599 Loss 19.060990, Accuracy 0.091%\n",
      "Epoch 2, Batch 344, LR 0.338769 Loss 19.061140, Accuracy 0.091%\n",
      "Epoch 2, Batch 345, LR 0.338939 Loss 19.060799, Accuracy 0.091%\n",
      "Epoch 2, Batch 346, LR 0.339109 Loss 19.060742, Accuracy 0.090%\n",
      "Epoch 2, Batch 347, LR 0.339279 Loss 19.060515, Accuracy 0.090%\n",
      "Epoch 2, Batch 348, LR 0.339450 Loss 19.060619, Accuracy 0.090%\n",
      "Epoch 2, Batch 349, LR 0.339620 Loss 19.060816, Accuracy 0.090%\n",
      "Epoch 2, Batch 350, LR 0.339790 Loss 19.061059, Accuracy 0.092%\n",
      "Epoch 2, Batch 351, LR 0.339960 Loss 19.061223, Accuracy 0.091%\n",
      "Epoch 2, Batch 352, LR 0.340131 Loss 19.061448, Accuracy 0.091%\n",
      "Epoch 2, Batch 353, LR 0.340301 Loss 19.061450, Accuracy 0.091%\n",
      "Epoch 2, Batch 354, LR 0.340472 Loss 19.062134, Accuracy 0.093%\n",
      "Epoch 2, Batch 355, LR 0.340643 Loss 19.062110, Accuracy 0.092%\n",
      "Epoch 2, Batch 356, LR 0.340813 Loss 19.062294, Accuracy 0.092%\n",
      "Epoch 2, Batch 357, LR 0.340984 Loss 19.062882, Accuracy 0.094%\n",
      "Epoch 2, Batch 358, LR 0.341155 Loss 19.062777, Accuracy 0.094%\n",
      "Epoch 2, Batch 359, LR 0.341326 Loss 19.062819, Accuracy 0.094%\n",
      "Epoch 2, Batch 360, LR 0.341496 Loss 19.062827, Accuracy 0.093%\n",
      "Epoch 2, Batch 361, LR 0.341667 Loss 19.063130, Accuracy 0.093%\n",
      "Epoch 2, Batch 362, LR 0.341839 Loss 19.063261, Accuracy 0.093%\n",
      "Epoch 2, Batch 363, LR 0.342010 Loss 19.063812, Accuracy 0.093%\n",
      "Epoch 2, Batch 364, LR 0.342181 Loss 19.063743, Accuracy 0.092%\n",
      "Epoch 2, Batch 365, LR 0.342352 Loss 19.063365, Accuracy 0.092%\n",
      "Epoch 2, Batch 366, LR 0.342523 Loss 19.062885, Accuracy 0.092%\n",
      "Epoch 2, Batch 367, LR 0.342695 Loss 19.062590, Accuracy 0.092%\n",
      "Epoch 2, Batch 368, LR 0.342866 Loss 19.062520, Accuracy 0.091%\n",
      "Epoch 2, Batch 369, LR 0.343037 Loss 19.062889, Accuracy 0.091%\n",
      "Epoch 2, Batch 370, LR 0.343209 Loss 19.063131, Accuracy 0.091%\n",
      "Epoch 2, Batch 371, LR 0.343381 Loss 19.063613, Accuracy 0.091%\n",
      "Epoch 2, Batch 372, LR 0.343552 Loss 19.063785, Accuracy 0.090%\n",
      "Epoch 2, Batch 373, LR 0.343724 Loss 19.063408, Accuracy 0.090%\n",
      "Epoch 2, Batch 374, LR 0.343896 Loss 19.063086, Accuracy 0.090%\n",
      "Epoch 2, Batch 375, LR 0.344067 Loss 19.063483, Accuracy 0.090%\n",
      "Epoch 2, Batch 376, LR 0.344239 Loss 19.062699, Accuracy 0.089%\n",
      "Epoch 2, Batch 377, LR 0.344411 Loss 19.062044, Accuracy 0.089%\n",
      "Epoch 2, Batch 378, LR 0.344583 Loss 19.062028, Accuracy 0.089%\n",
      "Epoch 2, Batch 379, LR 0.344755 Loss 19.062082, Accuracy 0.089%\n",
      "Epoch 2, Batch 380, LR 0.344928 Loss 19.061835, Accuracy 0.088%\n",
      "Epoch 2, Batch 381, LR 0.345100 Loss 19.061769, Accuracy 0.088%\n",
      "Epoch 2, Batch 382, LR 0.345272 Loss 19.061421, Accuracy 0.088%\n",
      "Epoch 2, Batch 383, LR 0.345444 Loss 19.061223, Accuracy 0.088%\n",
      "Epoch 2, Batch 384, LR 0.345617 Loss 19.061601, Accuracy 0.087%\n",
      "Epoch 2, Batch 385, LR 0.345789 Loss 19.061403, Accuracy 0.087%\n",
      "Epoch 2, Batch 386, LR 0.345962 Loss 19.061418, Accuracy 0.087%\n",
      "Epoch 2, Batch 387, LR 0.346134 Loss 19.061941, Accuracy 0.087%\n",
      "Epoch 2, Batch 388, LR 0.346307 Loss 19.061904, Accuracy 0.087%\n",
      "Epoch 2, Batch 389, LR 0.346479 Loss 19.062008, Accuracy 0.086%\n",
      "Epoch 2, Batch 390, LR 0.346652 Loss 19.061920, Accuracy 0.086%\n",
      "Epoch 2, Batch 391, LR 0.346825 Loss 19.061794, Accuracy 0.086%\n",
      "Epoch 2, Batch 392, LR 0.346998 Loss 19.062012, Accuracy 0.086%\n",
      "Epoch 2, Batch 393, LR 0.347171 Loss 19.062164, Accuracy 0.085%\n",
      "Epoch 2, Batch 394, LR 0.347344 Loss 19.062132, Accuracy 0.085%\n",
      "Epoch 2, Batch 395, LR 0.347517 Loss 19.061943, Accuracy 0.085%\n",
      "Epoch 2, Batch 396, LR 0.347690 Loss 19.061974, Accuracy 0.085%\n",
      "Epoch 2, Batch 397, LR 0.347863 Loss 19.061898, Accuracy 0.087%\n",
      "Epoch 2, Batch 398, LR 0.348036 Loss 19.062087, Accuracy 0.086%\n",
      "Epoch 2, Batch 399, LR 0.348209 Loss 19.062587, Accuracy 0.086%\n",
      "Epoch 2, Batch 400, LR 0.348383 Loss 19.062272, Accuracy 0.086%\n",
      "Epoch 2, Batch 401, LR 0.348556 Loss 19.062802, Accuracy 0.088%\n",
      "Epoch 2, Batch 402, LR 0.348730 Loss 19.063086, Accuracy 0.087%\n",
      "Epoch 2, Batch 403, LR 0.348903 Loss 19.062727, Accuracy 0.087%\n",
      "Epoch 2, Batch 404, LR 0.349077 Loss 19.062739, Accuracy 0.087%\n",
      "Epoch 2, Batch 405, LR 0.349250 Loss 19.062781, Accuracy 0.087%\n",
      "Epoch 2, Batch 406, LR 0.349424 Loss 19.062964, Accuracy 0.087%\n",
      "Epoch 2, Batch 407, LR 0.349598 Loss 19.062971, Accuracy 0.088%\n",
      "Epoch 2, Batch 408, LR 0.349772 Loss 19.062843, Accuracy 0.088%\n",
      "Epoch 2, Batch 409, LR 0.349946 Loss 19.062726, Accuracy 0.088%\n",
      "Epoch 2, Batch 410, LR 0.350119 Loss 19.063089, Accuracy 0.088%\n",
      "Epoch 2, Batch 411, LR 0.350293 Loss 19.063420, Accuracy 0.087%\n",
      "Epoch 2, Batch 412, LR 0.350468 Loss 19.063285, Accuracy 0.087%\n",
      "Epoch 2, Batch 413, LR 0.350642 Loss 19.063598, Accuracy 0.087%\n",
      "Epoch 2, Batch 414, LR 0.350816 Loss 19.064072, Accuracy 0.087%\n",
      "Epoch 2, Batch 415, LR 0.350990 Loss 19.063912, Accuracy 0.088%\n",
      "Epoch 2, Batch 416, LR 0.351164 Loss 19.063709, Accuracy 0.088%\n",
      "Epoch 2, Batch 417, LR 0.351339 Loss 19.063771, Accuracy 0.088%\n",
      "Epoch 2, Batch 418, LR 0.351513 Loss 19.063826, Accuracy 0.088%\n",
      "Epoch 2, Batch 419, LR 0.351688 Loss 19.063804, Accuracy 0.088%\n",
      "Epoch 2, Batch 420, LR 0.351862 Loss 19.063603, Accuracy 0.087%\n",
      "Epoch 2, Batch 421, LR 0.352037 Loss 19.063305, Accuracy 0.087%\n",
      "Epoch 2, Batch 422, LR 0.352211 Loss 19.063936, Accuracy 0.087%\n",
      "Epoch 2, Batch 423, LR 0.352386 Loss 19.063748, Accuracy 0.087%\n",
      "Epoch 2, Batch 424, LR 0.352561 Loss 19.063664, Accuracy 0.087%\n",
      "Epoch 2, Batch 425, LR 0.352736 Loss 19.063696, Accuracy 0.086%\n",
      "Epoch 2, Batch 426, LR 0.352911 Loss 19.063706, Accuracy 0.086%\n",
      "Epoch 2, Batch 427, LR 0.353086 Loss 19.064038, Accuracy 0.086%\n",
      "Epoch 2, Batch 428, LR 0.353261 Loss 19.063925, Accuracy 0.086%\n",
      "Epoch 2, Batch 429, LR 0.353436 Loss 19.064246, Accuracy 0.086%\n",
      "Epoch 2, Batch 430, LR 0.353611 Loss 19.064137, Accuracy 0.087%\n",
      "Epoch 2, Batch 431, LR 0.353786 Loss 19.064001, Accuracy 0.087%\n",
      "Epoch 2, Batch 432, LR 0.353961 Loss 19.063967, Accuracy 0.089%\n",
      "Epoch 2, Batch 433, LR 0.354137 Loss 19.063949, Accuracy 0.088%\n",
      "Epoch 2, Batch 434, LR 0.354312 Loss 19.064128, Accuracy 0.088%\n",
      "Epoch 2, Batch 435, LR 0.354487 Loss 19.064351, Accuracy 0.088%\n",
      "Epoch 2, Batch 436, LR 0.354663 Loss 19.064218, Accuracy 0.088%\n",
      "Epoch 2, Batch 437, LR 0.354838 Loss 19.064175, Accuracy 0.088%\n",
      "Epoch 2, Batch 438, LR 0.355014 Loss 19.064179, Accuracy 0.087%\n",
      "Epoch 2, Batch 439, LR 0.355190 Loss 19.064202, Accuracy 0.087%\n",
      "Epoch 2, Batch 440, LR 0.355366 Loss 19.064053, Accuracy 0.087%\n",
      "Epoch 2, Batch 441, LR 0.355541 Loss 19.064244, Accuracy 0.087%\n",
      "Epoch 2, Batch 442, LR 0.355717 Loss 19.064581, Accuracy 0.087%\n",
      "Epoch 2, Batch 443, LR 0.355893 Loss 19.064805, Accuracy 0.088%\n",
      "Epoch 2, Batch 444, LR 0.356069 Loss 19.065004, Accuracy 0.088%\n",
      "Epoch 2, Batch 445, LR 0.356245 Loss 19.064643, Accuracy 0.090%\n",
      "Epoch 2, Batch 446, LR 0.356421 Loss 19.064753, Accuracy 0.089%\n",
      "Epoch 2, Batch 447, LR 0.356597 Loss 19.064738, Accuracy 0.089%\n",
      "Epoch 2, Batch 448, LR 0.356774 Loss 19.064550, Accuracy 0.089%\n",
      "Epoch 2, Batch 449, LR 0.356950 Loss 19.064299, Accuracy 0.089%\n",
      "Epoch 2, Batch 450, LR 0.357126 Loss 19.063965, Accuracy 0.089%\n",
      "Epoch 2, Batch 451, LR 0.357303 Loss 19.063501, Accuracy 0.088%\n",
      "Epoch 2, Batch 452, LR 0.357479 Loss 19.063093, Accuracy 0.088%\n",
      "Epoch 2, Batch 453, LR 0.357656 Loss 19.063260, Accuracy 0.088%\n",
      "Epoch 2, Batch 454, LR 0.357832 Loss 19.063474, Accuracy 0.088%\n",
      "Epoch 2, Batch 455, LR 0.358009 Loss 19.063561, Accuracy 0.089%\n",
      "Epoch 2, Batch 456, LR 0.358185 Loss 19.063513, Accuracy 0.089%\n",
      "Epoch 2, Batch 457, LR 0.358362 Loss 19.063454, Accuracy 0.089%\n",
      "Epoch 2, Batch 458, LR 0.358539 Loss 19.063653, Accuracy 0.089%\n",
      "Epoch 2, Batch 459, LR 0.358716 Loss 19.063172, Accuracy 0.090%\n",
      "Epoch 2, Batch 460, LR 0.358893 Loss 19.063299, Accuracy 0.090%\n",
      "Epoch 2, Batch 461, LR 0.359070 Loss 19.063477, Accuracy 0.092%\n",
      "Epoch 2, Batch 462, LR 0.359247 Loss 19.063378, Accuracy 0.091%\n",
      "Epoch 2, Batch 463, LR 0.359424 Loss 19.063389, Accuracy 0.091%\n",
      "Epoch 2, Batch 464, LR 0.359601 Loss 19.063383, Accuracy 0.091%\n",
      "Epoch 2, Batch 465, LR 0.359778 Loss 19.063426, Accuracy 0.092%\n",
      "Epoch 2, Batch 466, LR 0.359956 Loss 19.063459, Accuracy 0.092%\n",
      "Epoch 2, Batch 467, LR 0.360133 Loss 19.062986, Accuracy 0.092%\n",
      "Epoch 2, Batch 468, LR 0.360310 Loss 19.063149, Accuracy 0.092%\n",
      "Epoch 2, Batch 469, LR 0.360488 Loss 19.062899, Accuracy 0.092%\n",
      "Epoch 2, Batch 470, LR 0.360665 Loss 19.062886, Accuracy 0.091%\n",
      "Epoch 2, Batch 471, LR 0.360843 Loss 19.062363, Accuracy 0.091%\n",
      "Epoch 2, Batch 472, LR 0.361021 Loss 19.062792, Accuracy 0.091%\n",
      "Epoch 2, Batch 473, LR 0.361198 Loss 19.062703, Accuracy 0.091%\n",
      "Epoch 2, Batch 474, LR 0.361376 Loss 19.062820, Accuracy 0.091%\n",
      "Epoch 2, Batch 475, LR 0.361554 Loss 19.062627, Accuracy 0.090%\n",
      "Epoch 2, Batch 476, LR 0.361732 Loss 19.062431, Accuracy 0.090%\n",
      "Epoch 2, Batch 477, LR 0.361910 Loss 19.062243, Accuracy 0.090%\n",
      "Epoch 2, Batch 478, LR 0.362088 Loss 19.062013, Accuracy 0.090%\n",
      "Epoch 2, Batch 479, LR 0.362266 Loss 19.062145, Accuracy 0.090%\n",
      "Epoch 2, Batch 480, LR 0.362444 Loss 19.062435, Accuracy 0.090%\n",
      "Epoch 2, Batch 481, LR 0.362622 Loss 19.062540, Accuracy 0.091%\n",
      "Epoch 2, Batch 482, LR 0.362800 Loss 19.062677, Accuracy 0.091%\n",
      "Epoch 2, Batch 483, LR 0.362979 Loss 19.062681, Accuracy 0.091%\n",
      "Epoch 2, Batch 484, LR 0.363157 Loss 19.062576, Accuracy 0.092%\n",
      "Epoch 2, Batch 485, LR 0.363336 Loss 19.062421, Accuracy 0.092%\n",
      "Epoch 2, Batch 486, LR 0.363514 Loss 19.062688, Accuracy 0.093%\n",
      "Epoch 2, Batch 487, LR 0.363693 Loss 19.062522, Accuracy 0.093%\n",
      "Epoch 2, Batch 488, LR 0.363871 Loss 19.062412, Accuracy 0.093%\n",
      "Epoch 2, Batch 489, LR 0.364050 Loss 19.062956, Accuracy 0.093%\n",
      "Epoch 2, Batch 490, LR 0.364229 Loss 19.062869, Accuracy 0.092%\n",
      "Epoch 2, Batch 491, LR 0.364407 Loss 19.063106, Accuracy 0.092%\n",
      "Epoch 2, Batch 492, LR 0.364586 Loss 19.063031, Accuracy 0.092%\n",
      "Epoch 2, Batch 493, LR 0.364765 Loss 19.063006, Accuracy 0.093%\n",
      "Epoch 2, Batch 494, LR 0.364944 Loss 19.062884, Accuracy 0.093%\n",
      "Epoch 2, Batch 495, LR 0.365123 Loss 19.062866, Accuracy 0.093%\n",
      "Epoch 2, Batch 496, LR 0.365302 Loss 19.062592, Accuracy 0.093%\n",
      "Epoch 2, Batch 497, LR 0.365481 Loss 19.062831, Accuracy 0.093%\n",
      "Epoch 2, Batch 498, LR 0.365660 Loss 19.062377, Accuracy 0.093%\n",
      "Epoch 2, Batch 499, LR 0.365840 Loss 19.062452, Accuracy 0.094%\n",
      "Epoch 2, Batch 500, LR 0.366019 Loss 19.062306, Accuracy 0.094%\n",
      "Epoch 2, Batch 501, LR 0.366198 Loss 19.062236, Accuracy 0.094%\n",
      "Epoch 2, Batch 502, LR 0.366378 Loss 19.062271, Accuracy 0.093%\n",
      "Epoch 2, Batch 503, LR 0.366557 Loss 19.061694, Accuracy 0.093%\n",
      "Epoch 2, Batch 504, LR 0.366737 Loss 19.061708, Accuracy 0.093%\n",
      "Epoch 2, Batch 505, LR 0.366916 Loss 19.061839, Accuracy 0.093%\n",
      "Epoch 2, Batch 506, LR 0.367096 Loss 19.062227, Accuracy 0.093%\n",
      "Epoch 2, Batch 507, LR 0.367276 Loss 19.061935, Accuracy 0.092%\n",
      "Epoch 2, Batch 508, LR 0.367456 Loss 19.061980, Accuracy 0.092%\n",
      "Epoch 2, Batch 509, LR 0.367635 Loss 19.061229, Accuracy 0.092%\n",
      "Epoch 2, Batch 510, LR 0.367815 Loss 19.061446, Accuracy 0.092%\n",
      "Epoch 2, Batch 511, LR 0.367995 Loss 19.061744, Accuracy 0.092%\n",
      "Epoch 2, Batch 512, LR 0.368175 Loss 19.061669, Accuracy 0.092%\n",
      "Epoch 2, Batch 513, LR 0.368355 Loss 19.061876, Accuracy 0.093%\n",
      "Epoch 2, Batch 514, LR 0.368536 Loss 19.061666, Accuracy 0.093%\n",
      "Epoch 2, Batch 515, LR 0.368716 Loss 19.061558, Accuracy 0.093%\n",
      "Epoch 2, Batch 516, LR 0.368896 Loss 19.061419, Accuracy 0.094%\n",
      "Epoch 2, Batch 517, LR 0.369076 Loss 19.061050, Accuracy 0.094%\n",
      "Epoch 2, Batch 518, LR 0.369257 Loss 19.061188, Accuracy 0.094%\n",
      "Epoch 2, Batch 519, LR 0.369437 Loss 19.060868, Accuracy 0.093%\n",
      "Epoch 2, Batch 520, LR 0.369618 Loss 19.060814, Accuracy 0.093%\n",
      "Epoch 2, Batch 521, LR 0.369798 Loss 19.060721, Accuracy 0.093%\n",
      "Epoch 2, Batch 522, LR 0.369979 Loss 19.060977, Accuracy 0.093%\n",
      "Epoch 2, Batch 523, LR 0.370159 Loss 19.061145, Accuracy 0.093%\n",
      "Epoch 2, Batch 524, LR 0.370340 Loss 19.061118, Accuracy 0.092%\n",
      "Epoch 2, Batch 525, LR 0.370521 Loss 19.061375, Accuracy 0.092%\n",
      "Epoch 2, Batch 526, LR 0.370702 Loss 19.060880, Accuracy 0.092%\n",
      "Epoch 2, Batch 527, LR 0.370883 Loss 19.060862, Accuracy 0.092%\n",
      "Epoch 2, Batch 528, LR 0.371064 Loss 19.060374, Accuracy 0.092%\n",
      "Epoch 2, Batch 529, LR 0.371245 Loss 19.060413, Accuracy 0.092%\n",
      "Epoch 2, Batch 530, LR 0.371426 Loss 19.060285, Accuracy 0.091%\n",
      "Epoch 2, Batch 531, LR 0.371607 Loss 19.060233, Accuracy 0.091%\n",
      "Epoch 2, Batch 532, LR 0.371788 Loss 19.060294, Accuracy 0.093%\n",
      "Epoch 2, Batch 533, LR 0.371969 Loss 19.060413, Accuracy 0.092%\n",
      "Epoch 2, Batch 534, LR 0.372151 Loss 19.060319, Accuracy 0.092%\n",
      "Epoch 2, Batch 535, LR 0.372332 Loss 19.060433, Accuracy 0.092%\n",
      "Epoch 2, Batch 536, LR 0.372514 Loss 19.060694, Accuracy 0.092%\n",
      "Epoch 2, Batch 537, LR 0.372695 Loss 19.060329, Accuracy 0.092%\n",
      "Epoch 2, Batch 538, LR 0.372877 Loss 19.060169, Accuracy 0.091%\n",
      "Epoch 2, Batch 539, LR 0.373058 Loss 19.060437, Accuracy 0.091%\n",
      "Epoch 2, Batch 540, LR 0.373240 Loss 19.060877, Accuracy 0.091%\n",
      "Epoch 2, Batch 541, LR 0.373422 Loss 19.060906, Accuracy 0.091%\n",
      "Epoch 2, Batch 542, LR 0.373603 Loss 19.060944, Accuracy 0.091%\n",
      "Epoch 2, Batch 543, LR 0.373785 Loss 19.061019, Accuracy 0.091%\n",
      "Epoch 2, Batch 544, LR 0.373967 Loss 19.060574, Accuracy 0.090%\n",
      "Epoch 2, Batch 545, LR 0.374149 Loss 19.060497, Accuracy 0.090%\n",
      "Epoch 2, Batch 546, LR 0.374331 Loss 19.060752, Accuracy 0.090%\n",
      "Epoch 2, Batch 547, LR 0.374513 Loss 19.060790, Accuracy 0.090%\n",
      "Epoch 2, Batch 548, LR 0.374695 Loss 19.060627, Accuracy 0.090%\n",
      "Epoch 2, Batch 549, LR 0.374878 Loss 19.060634, Accuracy 0.090%\n",
      "Epoch 2, Batch 550, LR 0.375060 Loss 19.060443, Accuracy 0.089%\n",
      "Epoch 2, Batch 551, LR 0.375242 Loss 19.060827, Accuracy 0.089%\n",
      "Epoch 2, Batch 552, LR 0.375424 Loss 19.060489, Accuracy 0.089%\n",
      "Epoch 2, Batch 553, LR 0.375607 Loss 19.060891, Accuracy 0.089%\n",
      "Epoch 2, Batch 554, LR 0.375789 Loss 19.061284, Accuracy 0.089%\n",
      "Epoch 2, Batch 555, LR 0.375972 Loss 19.061154, Accuracy 0.089%\n",
      "Epoch 2, Batch 556, LR 0.376155 Loss 19.061283, Accuracy 0.089%\n",
      "Epoch 2, Batch 557, LR 0.376337 Loss 19.061275, Accuracy 0.088%\n",
      "Epoch 2, Batch 558, LR 0.376520 Loss 19.061369, Accuracy 0.088%\n",
      "Epoch 2, Batch 559, LR 0.376703 Loss 19.061248, Accuracy 0.089%\n",
      "Epoch 2, Batch 560, LR 0.376886 Loss 19.060996, Accuracy 0.089%\n",
      "Epoch 2, Batch 561, LR 0.377068 Loss 19.061103, Accuracy 0.089%\n",
      "Epoch 2, Batch 562, LR 0.377251 Loss 19.061078, Accuracy 0.089%\n",
      "Epoch 2, Batch 563, LR 0.377434 Loss 19.061227, Accuracy 0.089%\n",
      "Epoch 2, Batch 564, LR 0.377617 Loss 19.061310, Accuracy 0.089%\n",
      "Epoch 2, Batch 565, LR 0.377801 Loss 19.061179, Accuracy 0.090%\n",
      "Epoch 2, Batch 566, LR 0.377984 Loss 19.061397, Accuracy 0.090%\n",
      "Epoch 2, Batch 567, LR 0.378167 Loss 19.061417, Accuracy 0.090%\n",
      "Epoch 2, Batch 568, LR 0.378350 Loss 19.061243, Accuracy 0.089%\n",
      "Epoch 2, Batch 569, LR 0.378534 Loss 19.061724, Accuracy 0.089%\n",
      "Epoch 2, Batch 570, LR 0.378717 Loss 19.061923, Accuracy 0.089%\n",
      "Epoch 2, Batch 571, LR 0.378901 Loss 19.061709, Accuracy 0.089%\n",
      "Epoch 2, Batch 572, LR 0.379084 Loss 19.061209, Accuracy 0.089%\n",
      "Epoch 2, Batch 573, LR 0.379268 Loss 19.061364, Accuracy 0.089%\n",
      "Epoch 2, Batch 574, LR 0.379451 Loss 19.061708, Accuracy 0.088%\n",
      "Epoch 2, Batch 575, LR 0.379635 Loss 19.061777, Accuracy 0.088%\n",
      "Epoch 2, Batch 576, LR 0.379819 Loss 19.061724, Accuracy 0.088%\n",
      "Epoch 2, Batch 577, LR 0.380003 Loss 19.061499, Accuracy 0.089%\n",
      "Epoch 2, Batch 578, LR 0.380187 Loss 19.061576, Accuracy 0.089%\n",
      "Epoch 2, Batch 579, LR 0.380371 Loss 19.061576, Accuracy 0.090%\n",
      "Epoch 2, Batch 580, LR 0.380555 Loss 19.061580, Accuracy 0.090%\n",
      "Epoch 2, Batch 581, LR 0.380739 Loss 19.061653, Accuracy 0.090%\n",
      "Epoch 2, Batch 582, LR 0.380923 Loss 19.061483, Accuracy 0.090%\n",
      "Epoch 2, Batch 583, LR 0.381107 Loss 19.061692, Accuracy 0.090%\n",
      "Epoch 2, Batch 584, LR 0.381291 Loss 19.061323, Accuracy 0.091%\n",
      "Epoch 2, Batch 585, LR 0.381476 Loss 19.061427, Accuracy 0.091%\n",
      "Epoch 2, Batch 586, LR 0.381660 Loss 19.061439, Accuracy 0.092%\n",
      "Epoch 2, Batch 587, LR 0.381844 Loss 19.061484, Accuracy 0.093%\n",
      "Epoch 2, Batch 588, LR 0.382029 Loss 19.061139, Accuracy 0.093%\n",
      "Epoch 2, Batch 589, LR 0.382213 Loss 19.061172, Accuracy 0.093%\n",
      "Epoch 2, Batch 590, LR 0.382398 Loss 19.061193, Accuracy 0.093%\n",
      "Epoch 2, Batch 591, LR 0.382583 Loss 19.061158, Accuracy 0.093%\n",
      "Epoch 2, Batch 592, LR 0.382767 Loss 19.061309, Accuracy 0.092%\n",
      "Epoch 2, Batch 593, LR 0.382952 Loss 19.060828, Accuracy 0.092%\n",
      "Epoch 2, Batch 594, LR 0.383137 Loss 19.060849, Accuracy 0.092%\n",
      "Epoch 2, Batch 595, LR 0.383322 Loss 19.060918, Accuracy 0.092%\n",
      "Epoch 2, Batch 596, LR 0.383507 Loss 19.061080, Accuracy 0.093%\n",
      "Epoch 2, Batch 597, LR 0.383692 Loss 19.061003, Accuracy 0.094%\n",
      "Epoch 2, Batch 598, LR 0.383877 Loss 19.060921, Accuracy 0.094%\n",
      "Epoch 2, Batch 599, LR 0.384062 Loss 19.061085, Accuracy 0.094%\n",
      "Epoch 2, Batch 600, LR 0.384247 Loss 19.061555, Accuracy 0.094%\n",
      "Epoch 2, Batch 601, LR 0.384432 Loss 19.061297, Accuracy 0.095%\n",
      "Epoch 2, Batch 602, LR 0.384618 Loss 19.061338, Accuracy 0.095%\n",
      "Epoch 2, Batch 603, LR 0.384803 Loss 19.061426, Accuracy 0.095%\n",
      "Epoch 2, Batch 604, LR 0.384988 Loss 19.061621, Accuracy 0.094%\n",
      "Epoch 2, Batch 605, LR 0.385174 Loss 19.061727, Accuracy 0.094%\n",
      "Epoch 2, Batch 606, LR 0.385359 Loss 19.062073, Accuracy 0.094%\n",
      "Epoch 2, Batch 607, LR 0.385545 Loss 19.062000, Accuracy 0.094%\n",
      "Epoch 2, Batch 608, LR 0.385731 Loss 19.062153, Accuracy 0.094%\n",
      "Epoch 2, Batch 609, LR 0.385916 Loss 19.062144, Accuracy 0.094%\n",
      "Epoch 2, Batch 610, LR 0.386102 Loss 19.062061, Accuracy 0.093%\n",
      "Epoch 2, Batch 611, LR 0.386288 Loss 19.061858, Accuracy 0.093%\n",
      "Epoch 2, Batch 612, LR 0.386474 Loss 19.062090, Accuracy 0.093%\n",
      "Epoch 2, Batch 613, LR 0.386660 Loss 19.062153, Accuracy 0.093%\n",
      "Epoch 2, Batch 614, LR 0.386846 Loss 19.062056, Accuracy 0.093%\n",
      "Epoch 2, Batch 615, LR 0.387032 Loss 19.062067, Accuracy 0.093%\n",
      "Epoch 2, Batch 616, LR 0.387218 Loss 19.062089, Accuracy 0.093%\n",
      "Epoch 2, Batch 617, LR 0.387404 Loss 19.062277, Accuracy 0.092%\n",
      "Epoch 2, Batch 618, LR 0.387590 Loss 19.062562, Accuracy 0.094%\n",
      "Epoch 2, Batch 619, LR 0.387776 Loss 19.062330, Accuracy 0.093%\n",
      "Epoch 2, Batch 620, LR 0.387963 Loss 19.062505, Accuracy 0.093%\n",
      "Epoch 2, Batch 621, LR 0.388149 Loss 19.062289, Accuracy 0.093%\n",
      "Epoch 2, Batch 622, LR 0.388336 Loss 19.062162, Accuracy 0.093%\n",
      "Epoch 2, Batch 623, LR 0.388522 Loss 19.062018, Accuracy 0.093%\n",
      "Epoch 2, Batch 624, LR 0.388709 Loss 19.061928, Accuracy 0.093%\n",
      "Epoch 2, Batch 625, LR 0.388895 Loss 19.061911, Accuracy 0.092%\n",
      "Epoch 2, Batch 626, LR 0.389082 Loss 19.062252, Accuracy 0.092%\n",
      "Epoch 2, Batch 627, LR 0.389269 Loss 19.062052, Accuracy 0.092%\n",
      "Epoch 2, Batch 628, LR 0.389455 Loss 19.061981, Accuracy 0.093%\n",
      "Epoch 2, Batch 629, LR 0.389642 Loss 19.061901, Accuracy 0.093%\n",
      "Epoch 2, Batch 630, LR 0.389829 Loss 19.061745, Accuracy 0.093%\n",
      "Epoch 2, Batch 631, LR 0.390016 Loss 19.061789, Accuracy 0.093%\n",
      "Epoch 2, Batch 632, LR 0.390203 Loss 19.061922, Accuracy 0.093%\n",
      "Epoch 2, Batch 633, LR 0.390390 Loss 19.061600, Accuracy 0.093%\n",
      "Epoch 2, Batch 634, LR 0.390577 Loss 19.061561, Accuracy 0.092%\n",
      "Epoch 2, Batch 635, LR 0.390765 Loss 19.061614, Accuracy 0.092%\n",
      "Epoch 2, Batch 636, LR 0.390952 Loss 19.061355, Accuracy 0.092%\n",
      "Epoch 2, Batch 637, LR 0.391139 Loss 19.061165, Accuracy 0.092%\n",
      "Epoch 2, Batch 638, LR 0.391327 Loss 19.061537, Accuracy 0.092%\n",
      "Epoch 2, Batch 639, LR 0.391514 Loss 19.061707, Accuracy 0.092%\n",
      "Epoch 2, Batch 640, LR 0.391701 Loss 19.061817, Accuracy 0.092%\n",
      "Epoch 2, Batch 641, LR 0.391889 Loss 19.061800, Accuracy 0.093%\n",
      "Epoch 2, Batch 642, LR 0.392077 Loss 19.061899, Accuracy 0.092%\n",
      "Epoch 2, Batch 643, LR 0.392264 Loss 19.061501, Accuracy 0.092%\n",
      "Epoch 2, Batch 644, LR 0.392452 Loss 19.061847, Accuracy 0.092%\n",
      "Epoch 2, Batch 645, LR 0.392640 Loss 19.061674, Accuracy 0.092%\n",
      "Epoch 2, Batch 646, LR 0.392828 Loss 19.061745, Accuracy 0.092%\n",
      "Epoch 2, Batch 647, LR 0.393015 Loss 19.061660, Accuracy 0.092%\n",
      "Epoch 2, Batch 648, LR 0.393203 Loss 19.061972, Accuracy 0.092%\n",
      "Epoch 2, Batch 649, LR 0.393391 Loss 19.061802, Accuracy 0.091%\n",
      "Epoch 2, Batch 650, LR 0.393579 Loss 19.061942, Accuracy 0.093%\n",
      "Epoch 2, Batch 651, LR 0.393768 Loss 19.061884, Accuracy 0.092%\n",
      "Epoch 2, Batch 652, LR 0.393956 Loss 19.061977, Accuracy 0.092%\n",
      "Epoch 2, Batch 653, LR 0.394144 Loss 19.062233, Accuracy 0.092%\n",
      "Epoch 2, Batch 654, LR 0.394332 Loss 19.062498, Accuracy 0.092%\n",
      "Epoch 2, Batch 655, LR 0.394521 Loss 19.062280, Accuracy 0.092%\n",
      "Epoch 2, Batch 656, LR 0.394709 Loss 19.062545, Accuracy 0.092%\n",
      "Epoch 2, Batch 657, LR 0.394898 Loss 19.062389, Accuracy 0.092%\n",
      "Epoch 2, Batch 658, LR 0.395086 Loss 19.062543, Accuracy 0.091%\n",
      "Epoch 2, Batch 659, LR 0.395275 Loss 19.062781, Accuracy 0.091%\n",
      "Epoch 2, Batch 660, LR 0.395463 Loss 19.062888, Accuracy 0.091%\n",
      "Epoch 2, Batch 661, LR 0.395652 Loss 19.062794, Accuracy 0.091%\n",
      "Epoch 2, Batch 662, LR 0.395841 Loss 19.062611, Accuracy 0.091%\n",
      "Epoch 2, Batch 663, LR 0.396030 Loss 19.062733, Accuracy 0.091%\n",
      "Epoch 2, Batch 664, LR 0.396218 Loss 19.062730, Accuracy 0.091%\n",
      "Epoch 2, Batch 665, LR 0.396407 Loss 19.062829, Accuracy 0.090%\n",
      "Epoch 2, Batch 666, LR 0.396596 Loss 19.063116, Accuracy 0.090%\n",
      "Epoch 2, Batch 667, LR 0.396785 Loss 19.062856, Accuracy 0.090%\n",
      "Epoch 2, Batch 668, LR 0.396974 Loss 19.062781, Accuracy 0.090%\n",
      "Epoch 2, Batch 669, LR 0.397164 Loss 19.062682, Accuracy 0.090%\n",
      "Epoch 2, Batch 670, LR 0.397353 Loss 19.062590, Accuracy 0.090%\n",
      "Epoch 2, Batch 671, LR 0.397542 Loss 19.062620, Accuracy 0.090%\n",
      "Epoch 2, Batch 672, LR 0.397731 Loss 19.062605, Accuracy 0.090%\n",
      "Epoch 2, Batch 673, LR 0.397921 Loss 19.062597, Accuracy 0.091%\n",
      "Epoch 2, Batch 674, LR 0.398110 Loss 19.062440, Accuracy 0.092%\n",
      "Epoch 2, Batch 675, LR 0.398300 Loss 19.062597, Accuracy 0.091%\n",
      "Epoch 2, Batch 676, LR 0.398489 Loss 19.062714, Accuracy 0.091%\n",
      "Epoch 2, Batch 677, LR 0.398679 Loss 19.062822, Accuracy 0.091%\n",
      "Epoch 2, Batch 678, LR 0.398868 Loss 19.062811, Accuracy 0.091%\n",
      "Epoch 2, Batch 679, LR 0.399058 Loss 19.062963, Accuracy 0.091%\n",
      "Epoch 2, Batch 680, LR 0.399248 Loss 19.062813, Accuracy 0.092%\n",
      "Epoch 2, Batch 681, LR 0.399438 Loss 19.062639, Accuracy 0.093%\n",
      "Epoch 2, Batch 682, LR 0.399628 Loss 19.062385, Accuracy 0.093%\n",
      "Epoch 2, Batch 683, LR 0.399818 Loss 19.062696, Accuracy 0.093%\n",
      "Epoch 2, Batch 684, LR 0.400008 Loss 19.062743, Accuracy 0.093%\n",
      "Epoch 2, Batch 685, LR 0.400198 Loss 19.062823, Accuracy 0.092%\n",
      "Epoch 2, Batch 686, LR 0.400388 Loss 19.062958, Accuracy 0.092%\n",
      "Epoch 2, Batch 687, LR 0.400578 Loss 19.062922, Accuracy 0.092%\n",
      "Epoch 2, Batch 688, LR 0.400768 Loss 19.062878, Accuracy 0.093%\n",
      "Epoch 2, Batch 689, LR 0.400959 Loss 19.063040, Accuracy 0.093%\n",
      "Epoch 2, Batch 690, LR 0.401149 Loss 19.062855, Accuracy 0.093%\n",
      "Epoch 2, Batch 691, LR 0.401339 Loss 19.063117, Accuracy 0.093%\n",
      "Epoch 2, Batch 692, LR 0.401530 Loss 19.063296, Accuracy 0.094%\n",
      "Epoch 2, Batch 693, LR 0.401720 Loss 19.063268, Accuracy 0.094%\n",
      "Epoch 2, Batch 694, LR 0.401911 Loss 19.063041, Accuracy 0.093%\n",
      "Epoch 2, Batch 695, LR 0.402102 Loss 19.063342, Accuracy 0.093%\n",
      "Epoch 2, Batch 696, LR 0.402292 Loss 19.063251, Accuracy 0.093%\n",
      "Epoch 2, Batch 697, LR 0.402483 Loss 19.063142, Accuracy 0.093%\n",
      "Epoch 2, Batch 698, LR 0.402674 Loss 19.063125, Accuracy 0.093%\n",
      "Epoch 2, Batch 699, LR 0.402865 Loss 19.063154, Accuracy 0.093%\n",
      "Epoch 2, Batch 700, LR 0.403056 Loss 19.062982, Accuracy 0.093%\n",
      "Epoch 2, Batch 701, LR 0.403247 Loss 19.062805, Accuracy 0.093%\n",
      "Epoch 2, Batch 702, LR 0.403438 Loss 19.062705, Accuracy 0.092%\n",
      "Epoch 2, Batch 703, LR 0.403629 Loss 19.062708, Accuracy 0.092%\n",
      "Epoch 2, Batch 704, LR 0.403820 Loss 19.062645, Accuracy 0.092%\n",
      "Epoch 2, Batch 705, LR 0.404011 Loss 19.062942, Accuracy 0.092%\n",
      "Epoch 2, Batch 706, LR 0.404202 Loss 19.062975, Accuracy 0.092%\n",
      "Epoch 2, Batch 707, LR 0.404394 Loss 19.062833, Accuracy 0.092%\n",
      "Epoch 2, Batch 708, LR 0.404585 Loss 19.062894, Accuracy 0.092%\n",
      "Epoch 2, Batch 709, LR 0.404776 Loss 19.062892, Accuracy 0.091%\n",
      "Epoch 2, Batch 710, LR 0.404968 Loss 19.062890, Accuracy 0.091%\n",
      "Epoch 2, Batch 711, LR 0.405159 Loss 19.062654, Accuracy 0.091%\n",
      "Epoch 2, Batch 712, LR 0.405351 Loss 19.062748, Accuracy 0.091%\n",
      "Epoch 2, Batch 713, LR 0.405543 Loss 19.062696, Accuracy 0.092%\n",
      "Epoch 2, Batch 714, LR 0.405734 Loss 19.062380, Accuracy 0.092%\n",
      "Epoch 2, Batch 715, LR 0.405926 Loss 19.062591, Accuracy 0.092%\n",
      "Epoch 2, Batch 716, LR 0.406118 Loss 19.062633, Accuracy 0.092%\n",
      "Epoch 2, Batch 717, LR 0.406310 Loss 19.062695, Accuracy 0.092%\n",
      "Epoch 2, Batch 718, LR 0.406502 Loss 19.062521, Accuracy 0.091%\n",
      "Epoch 2, Batch 719, LR 0.406694 Loss 19.062260, Accuracy 0.091%\n",
      "Epoch 2, Batch 720, LR 0.406886 Loss 19.062288, Accuracy 0.091%\n",
      "Epoch 2, Batch 721, LR 0.407078 Loss 19.062203, Accuracy 0.091%\n",
      "Epoch 2, Batch 722, LR 0.407270 Loss 19.062390, Accuracy 0.091%\n",
      "Epoch 2, Batch 723, LR 0.407462 Loss 19.062457, Accuracy 0.091%\n",
      "Epoch 2, Batch 724, LR 0.407655 Loss 19.062171, Accuracy 0.091%\n",
      "Epoch 2, Batch 725, LR 0.407847 Loss 19.062410, Accuracy 0.091%\n",
      "Epoch 2, Batch 726, LR 0.408039 Loss 19.062472, Accuracy 0.090%\n",
      "Epoch 2, Batch 727, LR 0.408232 Loss 19.062419, Accuracy 0.090%\n",
      "Epoch 2, Batch 728, LR 0.408424 Loss 19.062322, Accuracy 0.090%\n",
      "Epoch 2, Batch 729, LR 0.408617 Loss 19.062259, Accuracy 0.090%\n",
      "Epoch 2, Batch 730, LR 0.408810 Loss 19.062366, Accuracy 0.090%\n",
      "Epoch 2, Batch 731, LR 0.409002 Loss 19.062347, Accuracy 0.090%\n",
      "Epoch 2, Batch 732, LR 0.409195 Loss 19.062070, Accuracy 0.090%\n",
      "Epoch 2, Batch 733, LR 0.409388 Loss 19.062245, Accuracy 0.090%\n",
      "Epoch 2, Batch 734, LR 0.409581 Loss 19.062379, Accuracy 0.089%\n",
      "Epoch 2, Batch 735, LR 0.409774 Loss 19.062302, Accuracy 0.089%\n",
      "Epoch 2, Batch 736, LR 0.409966 Loss 19.062458, Accuracy 0.089%\n",
      "Epoch 2, Batch 737, LR 0.410159 Loss 19.062404, Accuracy 0.089%\n",
      "Epoch 2, Batch 738, LR 0.410353 Loss 19.062512, Accuracy 0.089%\n",
      "Epoch 2, Batch 739, LR 0.410546 Loss 19.062222, Accuracy 0.089%\n",
      "Epoch 2, Batch 740, LR 0.410739 Loss 19.062024, Accuracy 0.089%\n",
      "Epoch 2, Batch 741, LR 0.410932 Loss 19.062216, Accuracy 0.089%\n",
      "Epoch 2, Batch 742, LR 0.411125 Loss 19.062194, Accuracy 0.089%\n",
      "Epoch 2, Batch 743, LR 0.411319 Loss 19.062026, Accuracy 0.089%\n",
      "Epoch 2, Batch 744, LR 0.411512 Loss 19.062125, Accuracy 0.090%\n",
      "Epoch 2, Batch 745, LR 0.411706 Loss 19.061630, Accuracy 0.090%\n",
      "Epoch 2, Batch 746, LR 0.411899 Loss 19.061971, Accuracy 0.090%\n",
      "Epoch 2, Batch 747, LR 0.412093 Loss 19.062156, Accuracy 0.090%\n",
      "Epoch 2, Batch 748, LR 0.412286 Loss 19.062085, Accuracy 0.090%\n",
      "Epoch 2, Batch 749, LR 0.412480 Loss 19.062257, Accuracy 0.090%\n",
      "Epoch 2, Batch 750, LR 0.412674 Loss 19.062022, Accuracy 0.090%\n",
      "Epoch 2, Batch 751, LR 0.412868 Loss 19.061817, Accuracy 0.089%\n",
      "Epoch 2, Batch 752, LR 0.413061 Loss 19.061837, Accuracy 0.090%\n",
      "Epoch 2, Batch 753, LR 0.413255 Loss 19.061887, Accuracy 0.091%\n",
      "Epoch 2, Batch 754, LR 0.413449 Loss 19.061814, Accuracy 0.091%\n",
      "Epoch 2, Batch 755, LR 0.413643 Loss 19.061842, Accuracy 0.092%\n",
      "Epoch 2, Batch 756, LR 0.413837 Loss 19.061886, Accuracy 0.092%\n",
      "Epoch 2, Batch 757, LR 0.414032 Loss 19.061857, Accuracy 0.092%\n",
      "Epoch 2, Batch 758, LR 0.414226 Loss 19.062163, Accuracy 0.092%\n",
      "Epoch 2, Batch 759, LR 0.414420 Loss 19.062102, Accuracy 0.092%\n",
      "Epoch 2, Batch 760, LR 0.414614 Loss 19.062075, Accuracy 0.093%\n",
      "Epoch 2, Batch 761, LR 0.414809 Loss 19.062047, Accuracy 0.092%\n",
      "Epoch 2, Batch 762, LR 0.415003 Loss 19.062067, Accuracy 0.092%\n",
      "Epoch 2, Batch 763, LR 0.415198 Loss 19.062143, Accuracy 0.092%\n",
      "Epoch 2, Batch 764, LR 0.415392 Loss 19.062125, Accuracy 0.092%\n",
      "Epoch 2, Batch 765, LR 0.415587 Loss 19.062088, Accuracy 0.092%\n",
      "Epoch 2, Batch 766, LR 0.415781 Loss 19.062133, Accuracy 0.093%\n",
      "Epoch 2, Batch 767, LR 0.415976 Loss 19.062341, Accuracy 0.093%\n",
      "Epoch 2, Batch 768, LR 0.416171 Loss 19.062470, Accuracy 0.093%\n",
      "Epoch 2, Batch 769, LR 0.416366 Loss 19.062340, Accuracy 0.092%\n",
      "Epoch 2, Batch 770, LR 0.416561 Loss 19.062266, Accuracy 0.092%\n",
      "Epoch 2, Batch 771, LR 0.416756 Loss 19.062043, Accuracy 0.092%\n",
      "Epoch 2, Batch 772, LR 0.416951 Loss 19.062190, Accuracy 0.092%\n",
      "Epoch 2, Batch 773, LR 0.417146 Loss 19.062350, Accuracy 0.092%\n",
      "Epoch 2, Batch 774, LR 0.417341 Loss 19.062058, Accuracy 0.092%\n",
      "Epoch 2, Batch 775, LR 0.417536 Loss 19.061999, Accuracy 0.093%\n",
      "Epoch 2, Batch 776, LR 0.417731 Loss 19.062029, Accuracy 0.093%\n",
      "Epoch 2, Batch 777, LR 0.417926 Loss 19.061739, Accuracy 0.093%\n",
      "Epoch 2, Batch 778, LR 0.418122 Loss 19.061638, Accuracy 0.092%\n",
      "Epoch 2, Batch 779, LR 0.418317 Loss 19.061629, Accuracy 0.092%\n",
      "Epoch 2, Batch 780, LR 0.418512 Loss 19.061398, Accuracy 0.092%\n",
      "Epoch 2, Batch 781, LR 0.418708 Loss 19.061690, Accuracy 0.092%\n",
      "Epoch 2, Batch 782, LR 0.418903 Loss 19.061798, Accuracy 0.092%\n",
      "Epoch 2, Batch 783, LR 0.419099 Loss 19.061788, Accuracy 0.092%\n",
      "Epoch 2, Batch 784, LR 0.419295 Loss 19.061710, Accuracy 0.092%\n",
      "Epoch 2, Batch 785, LR 0.419490 Loss 19.061783, Accuracy 0.092%\n",
      "Epoch 2, Batch 786, LR 0.419686 Loss 19.061824, Accuracy 0.092%\n",
      "Epoch 2, Batch 787, LR 0.419882 Loss 19.061853, Accuracy 0.092%\n",
      "Epoch 2, Batch 788, LR 0.420078 Loss 19.061976, Accuracy 0.092%\n",
      "Epoch 2, Batch 789, LR 0.420274 Loss 19.062195, Accuracy 0.092%\n",
      "Epoch 2, Batch 790, LR 0.420470 Loss 19.062127, Accuracy 0.092%\n",
      "Epoch 2, Batch 791, LR 0.420666 Loss 19.062244, Accuracy 0.092%\n",
      "Epoch 2, Batch 792, LR 0.420862 Loss 19.062452, Accuracy 0.092%\n",
      "Epoch 2, Batch 793, LR 0.421058 Loss 19.062457, Accuracy 0.092%\n",
      "Epoch 2, Batch 794, LR 0.421254 Loss 19.062449, Accuracy 0.092%\n",
      "Epoch 2, Batch 795, LR 0.421451 Loss 19.062624, Accuracy 0.091%\n",
      "Epoch 2, Batch 796, LR 0.421647 Loss 19.062656, Accuracy 0.091%\n",
      "Epoch 2, Batch 797, LR 0.421843 Loss 19.062376, Accuracy 0.091%\n",
      "Epoch 2, Batch 798, LR 0.422040 Loss 19.062078, Accuracy 0.092%\n",
      "Epoch 2, Batch 799, LR 0.422236 Loss 19.061974, Accuracy 0.092%\n",
      "Epoch 2, Batch 800, LR 0.422433 Loss 19.061730, Accuracy 0.093%\n",
      "Epoch 2, Batch 801, LR 0.422629 Loss 19.061760, Accuracy 0.093%\n",
      "Epoch 2, Batch 802, LR 0.422826 Loss 19.061878, Accuracy 0.093%\n",
      "Epoch 2, Batch 803, LR 0.423023 Loss 19.061744, Accuracy 0.092%\n",
      "Epoch 2, Batch 804, LR 0.423219 Loss 19.061803, Accuracy 0.092%\n",
      "Epoch 2, Batch 805, LR 0.423416 Loss 19.061738, Accuracy 0.092%\n",
      "Epoch 2, Batch 806, LR 0.423613 Loss 19.061833, Accuracy 0.092%\n",
      "Epoch 2, Batch 807, LR 0.423810 Loss 19.061845, Accuracy 0.093%\n",
      "Epoch 2, Batch 808, LR 0.424007 Loss 19.061959, Accuracy 0.093%\n",
      "Epoch 2, Batch 809, LR 0.424204 Loss 19.061981, Accuracy 0.093%\n",
      "Epoch 2, Batch 810, LR 0.424401 Loss 19.062060, Accuracy 0.093%\n",
      "Epoch 2, Batch 811, LR 0.424598 Loss 19.061877, Accuracy 0.092%\n",
      "Epoch 2, Batch 812, LR 0.424796 Loss 19.061746, Accuracy 0.092%\n",
      "Epoch 2, Batch 813, LR 0.424993 Loss 19.061848, Accuracy 0.092%\n",
      "Epoch 2, Batch 814, LR 0.425190 Loss 19.061762, Accuracy 0.092%\n",
      "Epoch 2, Batch 815, LR 0.425388 Loss 19.061913, Accuracy 0.092%\n",
      "Epoch 2, Batch 816, LR 0.425585 Loss 19.062057, Accuracy 0.092%\n",
      "Epoch 2, Batch 817, LR 0.425783 Loss 19.061836, Accuracy 0.093%\n",
      "Epoch 2, Batch 818, LR 0.425980 Loss 19.062047, Accuracy 0.093%\n",
      "Epoch 2, Batch 819, LR 0.426178 Loss 19.061719, Accuracy 0.093%\n",
      "Epoch 2, Batch 820, LR 0.426375 Loss 19.061792, Accuracy 0.092%\n",
      "Epoch 2, Batch 821, LR 0.426573 Loss 19.061773, Accuracy 0.092%\n",
      "Epoch 2, Batch 822, LR 0.426771 Loss 19.061803, Accuracy 0.092%\n",
      "Epoch 2, Batch 823, LR 0.426969 Loss 19.061624, Accuracy 0.092%\n",
      "Epoch 2, Batch 824, LR 0.427167 Loss 19.061699, Accuracy 0.092%\n",
      "Epoch 2, Batch 825, LR 0.427364 Loss 19.061714, Accuracy 0.092%\n",
      "Epoch 2, Batch 826, LR 0.427562 Loss 19.061674, Accuracy 0.092%\n",
      "Epoch 2, Batch 827, LR 0.427761 Loss 19.061448, Accuracy 0.092%\n",
      "Epoch 2, Batch 828, LR 0.427959 Loss 19.061220, Accuracy 0.092%\n",
      "Epoch 2, Batch 829, LR 0.428157 Loss 19.061241, Accuracy 0.091%\n",
      "Epoch 2, Batch 830, LR 0.428355 Loss 19.061304, Accuracy 0.091%\n",
      "Epoch 2, Batch 831, LR 0.428553 Loss 19.061225, Accuracy 0.092%\n",
      "Epoch 2, Batch 832, LR 0.428752 Loss 19.061154, Accuracy 0.092%\n",
      "Epoch 2, Batch 833, LR 0.428950 Loss 19.061211, Accuracy 0.092%\n",
      "Epoch 2, Batch 834, LR 0.429148 Loss 19.061139, Accuracy 0.092%\n",
      "Epoch 2, Batch 835, LR 0.429347 Loss 19.061162, Accuracy 0.092%\n",
      "Epoch 2, Batch 836, LR 0.429545 Loss 19.061314, Accuracy 0.092%\n",
      "Epoch 2, Batch 837, LR 0.429744 Loss 19.061478, Accuracy 0.091%\n",
      "Epoch 2, Batch 838, LR 0.429943 Loss 19.061458, Accuracy 0.091%\n",
      "Epoch 2, Batch 839, LR 0.430141 Loss 19.061295, Accuracy 0.091%\n",
      "Epoch 2, Batch 840, LR 0.430340 Loss 19.061494, Accuracy 0.091%\n",
      "Epoch 2, Batch 841, LR 0.430539 Loss 19.061544, Accuracy 0.091%\n",
      "Epoch 2, Batch 842, LR 0.430738 Loss 19.061574, Accuracy 0.091%\n",
      "Epoch 2, Batch 843, LR 0.430937 Loss 19.061398, Accuracy 0.091%\n",
      "Epoch 2, Batch 844, LR 0.431136 Loss 19.061617, Accuracy 0.091%\n",
      "Epoch 2, Batch 845, LR 0.431335 Loss 19.061726, Accuracy 0.091%\n",
      "Epoch 2, Batch 846, LR 0.431534 Loss 19.061665, Accuracy 0.090%\n",
      "Epoch 2, Batch 847, LR 0.431733 Loss 19.061718, Accuracy 0.090%\n",
      "Epoch 2, Batch 848, LR 0.431932 Loss 19.061722, Accuracy 0.090%\n",
      "Epoch 2, Batch 849, LR 0.432131 Loss 19.061784, Accuracy 0.090%\n",
      "Epoch 2, Batch 850, LR 0.432331 Loss 19.061572, Accuracy 0.090%\n",
      "Epoch 2, Batch 851, LR 0.432530 Loss 19.061476, Accuracy 0.090%\n",
      "Epoch 2, Batch 852, LR 0.432730 Loss 19.061230, Accuracy 0.090%\n",
      "Epoch 2, Batch 853, LR 0.432929 Loss 19.061334, Accuracy 0.090%\n",
      "Epoch 2, Batch 854, LR 0.433129 Loss 19.061410, Accuracy 0.090%\n",
      "Epoch 2, Batch 855, LR 0.433328 Loss 19.061414, Accuracy 0.090%\n",
      "Epoch 2, Batch 856, LR 0.433528 Loss 19.061472, Accuracy 0.089%\n",
      "Epoch 2, Batch 857, LR 0.433728 Loss 19.061334, Accuracy 0.090%\n",
      "Epoch 2, Batch 858, LR 0.433927 Loss 19.061390, Accuracy 0.090%\n",
      "Epoch 2, Batch 859, LR 0.434127 Loss 19.061512, Accuracy 0.090%\n",
      "Epoch 2, Batch 860, LR 0.434327 Loss 19.061419, Accuracy 0.090%\n",
      "Epoch 2, Batch 861, LR 0.434527 Loss 19.061356, Accuracy 0.090%\n",
      "Epoch 2, Batch 862, LR 0.434727 Loss 19.061378, Accuracy 0.090%\n",
      "Epoch 2, Batch 863, LR 0.434927 Loss 19.061183, Accuracy 0.091%\n",
      "Epoch 2, Batch 864, LR 0.435127 Loss 19.061506, Accuracy 0.090%\n",
      "Epoch 2, Batch 865, LR 0.435327 Loss 19.061651, Accuracy 0.090%\n",
      "Epoch 2, Batch 866, LR 0.435527 Loss 19.061618, Accuracy 0.090%\n",
      "Epoch 2, Batch 867, LR 0.435728 Loss 19.061589, Accuracy 0.090%\n",
      "Epoch 2, Batch 868, LR 0.435928 Loss 19.061737, Accuracy 0.090%\n",
      "Epoch 2, Batch 869, LR 0.436128 Loss 19.061541, Accuracy 0.090%\n",
      "Epoch 2, Batch 870, LR 0.436329 Loss 19.061561, Accuracy 0.090%\n",
      "Epoch 2, Batch 871, LR 0.436529 Loss 19.061588, Accuracy 0.090%\n",
      "Epoch 2, Batch 872, LR 0.436730 Loss 19.061570, Accuracy 0.090%\n",
      "Epoch 2, Batch 873, LR 0.436930 Loss 19.061515, Accuracy 0.089%\n",
      "Epoch 2, Batch 874, LR 0.437131 Loss 19.061561, Accuracy 0.089%\n",
      "Epoch 2, Batch 875, LR 0.437332 Loss 19.061764, Accuracy 0.089%\n",
      "Epoch 2, Batch 876, LR 0.437532 Loss 19.061370, Accuracy 0.089%\n",
      "Epoch 2, Batch 877, LR 0.437733 Loss 19.061274, Accuracy 0.089%\n",
      "Epoch 2, Batch 878, LR 0.437934 Loss 19.061374, Accuracy 0.089%\n",
      "Epoch 2, Batch 879, LR 0.438135 Loss 19.061291, Accuracy 0.089%\n",
      "Epoch 2, Batch 880, LR 0.438336 Loss 19.061327, Accuracy 0.089%\n",
      "Epoch 2, Batch 881, LR 0.438537 Loss 19.061141, Accuracy 0.089%\n",
      "Epoch 2, Batch 882, LR 0.438738 Loss 19.061045, Accuracy 0.089%\n",
      "Epoch 2, Batch 883, LR 0.438939 Loss 19.061038, Accuracy 0.089%\n",
      "Epoch 2, Batch 884, LR 0.439140 Loss 19.061100, Accuracy 0.089%\n",
      "Epoch 2, Batch 885, LR 0.439341 Loss 19.060805, Accuracy 0.089%\n",
      "Epoch 2, Batch 886, LR 0.439543 Loss 19.060847, Accuracy 0.089%\n",
      "Epoch 2, Batch 887, LR 0.439744 Loss 19.060842, Accuracy 0.089%\n",
      "Epoch 2, Batch 888, LR 0.439945 Loss 19.060666, Accuracy 0.089%\n",
      "Epoch 2, Batch 889, LR 0.440147 Loss 19.060670, Accuracy 0.089%\n",
      "Epoch 2, Batch 890, LR 0.440348 Loss 19.060744, Accuracy 0.089%\n",
      "Epoch 2, Batch 891, LR 0.440550 Loss 19.060607, Accuracy 0.089%\n",
      "Epoch 2, Batch 892, LR 0.440752 Loss 19.060610, Accuracy 0.088%\n",
      "Epoch 2, Batch 893, LR 0.440953 Loss 19.060523, Accuracy 0.088%\n",
      "Epoch 2, Batch 894, LR 0.441155 Loss 19.060257, Accuracy 0.089%\n",
      "Epoch 2, Batch 895, LR 0.441357 Loss 19.060261, Accuracy 0.089%\n",
      "Epoch 2, Batch 896, LR 0.441559 Loss 19.060419, Accuracy 0.089%\n",
      "Epoch 2, Batch 897, LR 0.441760 Loss 19.060527, Accuracy 0.089%\n",
      "Epoch 2, Batch 898, LR 0.441962 Loss 19.060450, Accuracy 0.089%\n",
      "Epoch 2, Batch 899, LR 0.442164 Loss 19.060643, Accuracy 0.089%\n",
      "Epoch 2, Batch 900, LR 0.442366 Loss 19.060655, Accuracy 0.089%\n",
      "Epoch 2, Batch 901, LR 0.442569 Loss 19.060489, Accuracy 0.088%\n",
      "Epoch 2, Batch 902, LR 0.442771 Loss 19.060845, Accuracy 0.089%\n",
      "Epoch 2, Batch 903, LR 0.442973 Loss 19.060953, Accuracy 0.089%\n",
      "Epoch 2, Batch 904, LR 0.443175 Loss 19.060933, Accuracy 0.089%\n",
      "Epoch 2, Batch 905, LR 0.443378 Loss 19.060696, Accuracy 0.090%\n",
      "Epoch 2, Batch 906, LR 0.443580 Loss 19.060734, Accuracy 0.090%\n",
      "Epoch 2, Batch 907, LR 0.443782 Loss 19.060758, Accuracy 0.090%\n",
      "Epoch 2, Batch 908, LR 0.443985 Loss 19.060849, Accuracy 0.090%\n",
      "Epoch 2, Batch 909, LR 0.444187 Loss 19.060919, Accuracy 0.090%\n",
      "Epoch 2, Batch 910, LR 0.444390 Loss 19.061072, Accuracy 0.090%\n",
      "Epoch 2, Batch 911, LR 0.444593 Loss 19.061122, Accuracy 0.090%\n",
      "Epoch 2, Batch 912, LR 0.444795 Loss 19.060943, Accuracy 0.090%\n",
      "Epoch 2, Batch 913, LR 0.444998 Loss 19.060583, Accuracy 0.090%\n",
      "Epoch 2, Batch 914, LR 0.445201 Loss 19.060433, Accuracy 0.090%\n",
      "Epoch 2, Batch 915, LR 0.445404 Loss 19.060267, Accuracy 0.090%\n",
      "Epoch 2, Batch 916, LR 0.445607 Loss 19.060276, Accuracy 0.090%\n",
      "Epoch 2, Batch 917, LR 0.445810 Loss 19.060247, Accuracy 0.089%\n",
      "Epoch 2, Batch 918, LR 0.446013 Loss 19.060244, Accuracy 0.089%\n",
      "Epoch 2, Batch 919, LR 0.446216 Loss 19.060051, Accuracy 0.089%\n",
      "Epoch 2, Batch 920, LR 0.446419 Loss 19.060221, Accuracy 0.089%\n",
      "Epoch 2, Batch 921, LR 0.446622 Loss 19.060348, Accuracy 0.089%\n",
      "Epoch 2, Batch 922, LR 0.446825 Loss 19.060380, Accuracy 0.089%\n",
      "Epoch 2, Batch 923, LR 0.447029 Loss 19.060171, Accuracy 0.089%\n",
      "Epoch 2, Batch 924, LR 0.447232 Loss 19.060005, Accuracy 0.089%\n",
      "Epoch 2, Batch 925, LR 0.447435 Loss 19.059992, Accuracy 0.089%\n",
      "Epoch 2, Batch 926, LR 0.447639 Loss 19.060085, Accuracy 0.089%\n",
      "Epoch 2, Batch 927, LR 0.447842 Loss 19.059998, Accuracy 0.089%\n",
      "Epoch 2, Batch 928, LR 0.448046 Loss 19.059835, Accuracy 0.089%\n",
      "Epoch 2, Batch 929, LR 0.448250 Loss 19.059865, Accuracy 0.090%\n",
      "Epoch 2, Batch 930, LR 0.448453 Loss 19.059760, Accuracy 0.090%\n",
      "Epoch 2, Batch 931, LR 0.448657 Loss 19.059590, Accuracy 0.090%\n",
      "Epoch 2, Batch 932, LR 0.448861 Loss 19.059559, Accuracy 0.090%\n",
      "Epoch 2, Batch 933, LR 0.449065 Loss 19.059567, Accuracy 0.090%\n",
      "Epoch 2, Batch 934, LR 0.449269 Loss 19.059368, Accuracy 0.090%\n",
      "Epoch 2, Batch 935, LR 0.449472 Loss 19.059213, Accuracy 0.089%\n",
      "Epoch 2, Batch 936, LR 0.449676 Loss 19.059361, Accuracy 0.089%\n",
      "Epoch 2, Batch 937, LR 0.449881 Loss 19.059266, Accuracy 0.089%\n",
      "Epoch 2, Batch 938, LR 0.450085 Loss 19.059626, Accuracy 0.089%\n",
      "Epoch 2, Batch 939, LR 0.450289 Loss 19.059830, Accuracy 0.089%\n",
      "Epoch 2, Batch 940, LR 0.450493 Loss 19.059767, Accuracy 0.089%\n",
      "Epoch 2, Batch 941, LR 0.450697 Loss 19.059815, Accuracy 0.090%\n",
      "Epoch 2, Batch 942, LR 0.450902 Loss 19.059783, Accuracy 0.090%\n",
      "Epoch 2, Batch 943, LR 0.451106 Loss 19.059939, Accuracy 0.090%\n",
      "Epoch 2, Batch 944, LR 0.451310 Loss 19.060034, Accuracy 0.090%\n",
      "Epoch 2, Batch 945, LR 0.451515 Loss 19.059956, Accuracy 0.091%\n",
      "Epoch 2, Batch 946, LR 0.451719 Loss 19.060006, Accuracy 0.091%\n",
      "Epoch 2, Batch 947, LR 0.451924 Loss 19.060136, Accuracy 0.091%\n",
      "Epoch 2, Batch 948, LR 0.452129 Loss 19.060216, Accuracy 0.091%\n",
      "Epoch 2, Batch 949, LR 0.452333 Loss 19.060092, Accuracy 0.091%\n",
      "Epoch 2, Batch 950, LR 0.452538 Loss 19.060064, Accuracy 0.091%\n",
      "Epoch 2, Batch 951, LR 0.452743 Loss 19.059989, Accuracy 0.091%\n",
      "Epoch 2, Batch 952, LR 0.452948 Loss 19.059981, Accuracy 0.091%\n",
      "Epoch 2, Batch 953, LR 0.453153 Loss 19.059959, Accuracy 0.091%\n",
      "Epoch 2, Batch 954, LR 0.453358 Loss 19.059993, Accuracy 0.091%\n",
      "Epoch 2, Batch 955, LR 0.453563 Loss 19.060040, Accuracy 0.091%\n",
      "Epoch 2, Batch 956, LR 0.453768 Loss 19.060084, Accuracy 0.091%\n",
      "Epoch 2, Batch 957, LR 0.453973 Loss 19.060001, Accuracy 0.091%\n",
      "Epoch 2, Batch 958, LR 0.454178 Loss 19.059954, Accuracy 0.091%\n",
      "Epoch 2, Batch 959, LR 0.454383 Loss 19.059781, Accuracy 0.090%\n",
      "Epoch 2, Batch 960, LR 0.454589 Loss 19.059928, Accuracy 0.090%\n",
      "Epoch 2, Batch 961, LR 0.454794 Loss 19.059867, Accuracy 0.090%\n",
      "Epoch 2, Batch 962, LR 0.454999 Loss 19.060038, Accuracy 0.090%\n",
      "Epoch 2, Batch 963, LR 0.455205 Loss 19.059813, Accuracy 0.090%\n",
      "Epoch 2, Batch 964, LR 0.455410 Loss 19.059686, Accuracy 0.090%\n",
      "Epoch 2, Batch 965, LR 0.455616 Loss 19.059785, Accuracy 0.090%\n",
      "Epoch 2, Batch 966, LR 0.455822 Loss 19.059684, Accuracy 0.090%\n",
      "Epoch 2, Batch 967, LR 0.456027 Loss 19.059830, Accuracy 0.090%\n",
      "Epoch 2, Batch 968, LR 0.456233 Loss 19.059794, Accuracy 0.090%\n",
      "Epoch 2, Batch 969, LR 0.456439 Loss 19.059673, Accuracy 0.089%\n",
      "Epoch 2, Batch 970, LR 0.456645 Loss 19.059777, Accuracy 0.089%\n",
      "Epoch 2, Batch 971, LR 0.456850 Loss 19.059877, Accuracy 0.089%\n",
      "Epoch 2, Batch 972, LR 0.457056 Loss 19.059706, Accuracy 0.089%\n",
      "Epoch 2, Batch 973, LR 0.457262 Loss 19.059824, Accuracy 0.089%\n",
      "Epoch 2, Batch 974, LR 0.457468 Loss 19.059826, Accuracy 0.089%\n",
      "Epoch 2, Batch 975, LR 0.457675 Loss 19.059959, Accuracy 0.089%\n",
      "Epoch 2, Batch 976, LR 0.457881 Loss 19.059987, Accuracy 0.089%\n",
      "Epoch 2, Batch 977, LR 0.458087 Loss 19.059866, Accuracy 0.089%\n",
      "Epoch 2, Batch 978, LR 0.458293 Loss 19.059953, Accuracy 0.089%\n",
      "Epoch 2, Batch 979, LR 0.458499 Loss 19.059710, Accuracy 0.089%\n",
      "Epoch 2, Batch 980, LR 0.458706 Loss 19.059830, Accuracy 0.088%\n",
      "Epoch 2, Batch 981, LR 0.458912 Loss 19.059709, Accuracy 0.088%\n",
      "Epoch 2, Batch 982, LR 0.459119 Loss 19.059645, Accuracy 0.088%\n",
      "Epoch 2, Batch 983, LR 0.459325 Loss 19.059890, Accuracy 0.088%\n",
      "Epoch 2, Batch 984, LR 0.459532 Loss 19.060061, Accuracy 0.089%\n",
      "Epoch 2, Batch 985, LR 0.459738 Loss 19.059898, Accuracy 0.089%\n",
      "Epoch 2, Batch 986, LR 0.459945 Loss 19.060072, Accuracy 0.089%\n",
      "Epoch 2, Batch 987, LR 0.460152 Loss 19.059975, Accuracy 0.089%\n",
      "Epoch 2, Batch 988, LR 0.460359 Loss 19.059933, Accuracy 0.089%\n",
      "Epoch 2, Batch 989, LR 0.460566 Loss 19.059565, Accuracy 0.090%\n",
      "Epoch 2, Batch 990, LR 0.460772 Loss 19.059398, Accuracy 0.090%\n",
      "Epoch 2, Batch 991, LR 0.460979 Loss 19.059580, Accuracy 0.090%\n",
      "Epoch 2, Batch 992, LR 0.461186 Loss 19.059554, Accuracy 0.091%\n",
      "Epoch 2, Batch 993, LR 0.461393 Loss 19.059562, Accuracy 0.090%\n",
      "Epoch 2, Batch 994, LR 0.461601 Loss 19.059457, Accuracy 0.090%\n",
      "Epoch 2, Batch 995, LR 0.461808 Loss 19.059414, Accuracy 0.090%\n",
      "Epoch 2, Batch 996, LR 0.462015 Loss 19.059424, Accuracy 0.090%\n",
      "Epoch 2, Batch 997, LR 0.462222 Loss 19.059308, Accuracy 0.090%\n",
      "Epoch 2, Batch 998, LR 0.462430 Loss 19.059267, Accuracy 0.090%\n",
      "Epoch 2, Batch 999, LR 0.462637 Loss 19.059170, Accuracy 0.090%\n",
      "Epoch 2, Batch 1000, LR 0.462844 Loss 19.059312, Accuracy 0.090%\n",
      "Epoch 2, Batch 1001, LR 0.463052 Loss 19.059402, Accuracy 0.090%\n",
      "Epoch 2, Batch 1002, LR 0.463259 Loss 19.059573, Accuracy 0.090%\n",
      "Epoch 2, Batch 1003, LR 0.463467 Loss 19.059433, Accuracy 0.090%\n",
      "Epoch 2, Batch 1004, LR 0.463675 Loss 19.059346, Accuracy 0.090%\n",
      "Epoch 2, Batch 1005, LR 0.463882 Loss 19.059484, Accuracy 0.090%\n",
      "Epoch 2, Batch 1006, LR 0.464090 Loss 19.059585, Accuracy 0.090%\n",
      "Epoch 2, Batch 1007, LR 0.464298 Loss 19.059757, Accuracy 0.090%\n",
      "Epoch 2, Batch 1008, LR 0.464506 Loss 19.059852, Accuracy 0.090%\n",
      "Epoch 2, Batch 1009, LR 0.464714 Loss 19.059830, Accuracy 0.090%\n",
      "Epoch 2, Batch 1010, LR 0.464922 Loss 19.060019, Accuracy 0.090%\n",
      "Epoch 2, Batch 1011, LR 0.465130 Loss 19.059859, Accuracy 0.090%\n",
      "Epoch 2, Batch 1012, LR 0.465338 Loss 19.059731, Accuracy 0.090%\n",
      "Epoch 2, Batch 1013, LR 0.465546 Loss 19.059887, Accuracy 0.089%\n",
      "Epoch 2, Batch 1014, LR 0.465754 Loss 19.059909, Accuracy 0.090%\n",
      "Epoch 2, Batch 1015, LR 0.465962 Loss 19.059827, Accuracy 0.090%\n",
      "Epoch 2, Batch 1016, LR 0.466170 Loss 19.059807, Accuracy 0.090%\n",
      "Epoch 2, Batch 1017, LR 0.466379 Loss 19.059705, Accuracy 0.091%\n",
      "Epoch 2, Batch 1018, LR 0.466587 Loss 19.059955, Accuracy 0.091%\n",
      "Epoch 2, Batch 1019, LR 0.466796 Loss 19.059920, Accuracy 0.090%\n",
      "Epoch 2, Batch 1020, LR 0.467004 Loss 19.059883, Accuracy 0.090%\n",
      "Epoch 2, Batch 1021, LR 0.467213 Loss 19.059822, Accuracy 0.090%\n",
      "Epoch 2, Batch 1022, LR 0.467421 Loss 19.059857, Accuracy 0.090%\n",
      "Epoch 2, Batch 1023, LR 0.467630 Loss 19.060055, Accuracy 0.090%\n",
      "Epoch 2, Batch 1024, LR 0.467839 Loss 19.060162, Accuracy 0.090%\n",
      "Epoch 2, Batch 1025, LR 0.468047 Loss 19.059971, Accuracy 0.090%\n",
      "Epoch 2, Batch 1026, LR 0.468256 Loss 19.059889, Accuracy 0.090%\n",
      "Epoch 2, Batch 1027, LR 0.468465 Loss 19.059948, Accuracy 0.090%\n",
      "Epoch 2, Batch 1028, LR 0.468674 Loss 19.060011, Accuracy 0.090%\n",
      "Epoch 2, Batch 1029, LR 0.468883 Loss 19.060050, Accuracy 0.090%\n",
      "Epoch 2, Batch 1030, LR 0.469092 Loss 19.060120, Accuracy 0.090%\n",
      "Epoch 2, Batch 1031, LR 0.469301 Loss 19.060128, Accuracy 0.089%\n",
      "Epoch 2, Batch 1032, LR 0.469510 Loss 19.060132, Accuracy 0.089%\n",
      "Epoch 2, Batch 1033, LR 0.469719 Loss 19.060152, Accuracy 0.089%\n",
      "Epoch 2, Batch 1034, LR 0.469928 Loss 19.060166, Accuracy 0.089%\n",
      "Epoch 2, Batch 1035, LR 0.470138 Loss 19.060040, Accuracy 0.089%\n",
      "Epoch 2, Batch 1036, LR 0.470347 Loss 19.060037, Accuracy 0.089%\n",
      "Epoch 2, Batch 1037, LR 0.470556 Loss 19.059974, Accuracy 0.089%\n",
      "Epoch 2, Batch 1038, LR 0.470766 Loss 19.059945, Accuracy 0.090%\n",
      "Epoch 2, Batch 1039, LR 0.470975 Loss 19.059961, Accuracy 0.089%\n",
      "Epoch 2, Batch 1040, LR 0.471185 Loss 19.059750, Accuracy 0.090%\n",
      "Epoch 2, Batch 1041, LR 0.471395 Loss 19.059846, Accuracy 0.090%\n",
      "Epoch 2, Batch 1042, LR 0.471604 Loss 19.059951, Accuracy 0.090%\n",
      "Epoch 2, Batch 1043, LR 0.471814 Loss 19.059895, Accuracy 0.090%\n",
      "Epoch 2, Batch 1044, LR 0.472024 Loss 19.059864, Accuracy 0.090%\n",
      "Epoch 2, Batch 1045, LR 0.472233 Loss 19.059779, Accuracy 0.090%\n",
      "Epoch 2, Batch 1046, LR 0.472443 Loss 19.059929, Accuracy 0.090%\n",
      "Epoch 2, Batch 1047, LR 0.472653 Loss 19.059846, Accuracy 0.090%\n",
      "Epoch 2, Loss (train set) 19.059846, Accuracy (train set) 0.090%\n",
      "Epoch 3, Batch 1, LR 0.472863 Loss 18.945391, Accuracy 0.000%\n",
      "Epoch 3, Batch 2, LR 0.473073 Loss 19.028076, Accuracy 0.000%\n",
      "Epoch 3, Batch 3, LR 0.473283 Loss 19.053583, Accuracy 0.260%\n",
      "Epoch 3, Batch 4, LR 0.473493 Loss 19.069382, Accuracy 0.195%\n",
      "Epoch 3, Batch 5, LR 0.473704 Loss 19.016881, Accuracy 0.156%\n",
      "Epoch 3, Batch 6, LR 0.473914 Loss 19.032059, Accuracy 0.130%\n",
      "Epoch 3, Batch 7, LR 0.474124 Loss 19.031072, Accuracy 0.112%\n",
      "Epoch 3, Batch 8, LR 0.474334 Loss 19.013171, Accuracy 0.098%\n",
      "Epoch 3, Batch 9, LR 0.474545 Loss 19.024252, Accuracy 0.174%\n",
      "Epoch 3, Batch 10, LR 0.474755 Loss 19.024650, Accuracy 0.156%\n",
      "Epoch 3, Batch 11, LR 0.474966 Loss 19.018448, Accuracy 0.213%\n",
      "Epoch 3, Batch 12, LR 0.475176 Loss 19.019398, Accuracy 0.195%\n",
      "Epoch 3, Batch 13, LR 0.475387 Loss 19.043320, Accuracy 0.180%\n",
      "Epoch 3, Batch 14, LR 0.475598 Loss 19.057021, Accuracy 0.167%\n",
      "Epoch 3, Batch 15, LR 0.475808 Loss 19.049809, Accuracy 0.156%\n",
      "Epoch 3, Batch 16, LR 0.476019 Loss 19.057928, Accuracy 0.146%\n",
      "Epoch 3, Batch 17, LR 0.476230 Loss 19.070610, Accuracy 0.138%\n",
      "Epoch 3, Batch 18, LR 0.476441 Loss 19.070441, Accuracy 0.130%\n",
      "Epoch 3, Batch 19, LR 0.476652 Loss 19.078031, Accuracy 0.123%\n",
      "Epoch 3, Batch 20, LR 0.476862 Loss 19.070055, Accuracy 0.156%\n",
      "Epoch 3, Batch 21, LR 0.477074 Loss 19.072836, Accuracy 0.149%\n",
      "Epoch 3, Batch 22, LR 0.477285 Loss 19.068162, Accuracy 0.142%\n",
      "Epoch 3, Batch 23, LR 0.477496 Loss 19.061494, Accuracy 0.136%\n",
      "Epoch 3, Batch 24, LR 0.477707 Loss 19.064820, Accuracy 0.130%\n",
      "Epoch 3, Batch 25, LR 0.477918 Loss 19.066002, Accuracy 0.125%\n",
      "Epoch 3, Batch 26, LR 0.478129 Loss 19.060532, Accuracy 0.120%\n",
      "Epoch 3, Batch 27, LR 0.478341 Loss 19.058041, Accuracy 0.116%\n",
      "Epoch 3, Batch 28, LR 0.478552 Loss 19.055853, Accuracy 0.112%\n",
      "Epoch 3, Batch 29, LR 0.478763 Loss 19.061345, Accuracy 0.108%\n",
      "Epoch 3, Batch 30, LR 0.478975 Loss 19.064740, Accuracy 0.104%\n",
      "Epoch 3, Batch 31, LR 0.479187 Loss 19.062195, Accuracy 0.101%\n",
      "Epoch 3, Batch 32, LR 0.479398 Loss 19.064409, Accuracy 0.098%\n",
      "Epoch 3, Batch 33, LR 0.479610 Loss 19.064887, Accuracy 0.095%\n",
      "Epoch 3, Batch 34, LR 0.479821 Loss 19.063529, Accuracy 0.092%\n",
      "Epoch 3, Batch 35, LR 0.480033 Loss 19.061014, Accuracy 0.089%\n",
      "Epoch 3, Batch 36, LR 0.480245 Loss 19.059376, Accuracy 0.109%\n",
      "Epoch 3, Batch 37, LR 0.480457 Loss 19.059420, Accuracy 0.106%\n",
      "Epoch 3, Batch 38, LR 0.480669 Loss 19.058877, Accuracy 0.103%\n",
      "Epoch 3, Batch 39, LR 0.480881 Loss 19.054586, Accuracy 0.120%\n",
      "Epoch 3, Batch 40, LR 0.481093 Loss 19.053059, Accuracy 0.117%\n",
      "Epoch 3, Batch 41, LR 0.481305 Loss 19.053091, Accuracy 0.114%\n",
      "Epoch 3, Batch 42, LR 0.481517 Loss 19.052784, Accuracy 0.112%\n",
      "Epoch 3, Batch 43, LR 0.481729 Loss 19.055364, Accuracy 0.109%\n",
      "Epoch 3, Batch 44, LR 0.481941 Loss 19.059655, Accuracy 0.107%\n",
      "Epoch 3, Batch 45, LR 0.482153 Loss 19.058329, Accuracy 0.104%\n",
      "Epoch 3, Batch 46, LR 0.482366 Loss 19.061602, Accuracy 0.102%\n",
      "Epoch 3, Batch 47, LR 0.482578 Loss 19.060007, Accuracy 0.100%\n",
      "Epoch 3, Batch 48, LR 0.482791 Loss 19.060859, Accuracy 0.098%\n",
      "Epoch 3, Batch 49, LR 0.483003 Loss 19.061251, Accuracy 0.096%\n",
      "Epoch 3, Batch 50, LR 0.483216 Loss 19.056213, Accuracy 0.094%\n",
      "Epoch 3, Batch 51, LR 0.483428 Loss 19.057555, Accuracy 0.092%\n",
      "Epoch 3, Batch 52, LR 0.483641 Loss 19.057369, Accuracy 0.090%\n",
      "Epoch 3, Batch 53, LR 0.483853 Loss 19.056500, Accuracy 0.088%\n",
      "Epoch 3, Batch 54, LR 0.484066 Loss 19.055044, Accuracy 0.087%\n",
      "Epoch 3, Batch 55, LR 0.484279 Loss 19.057976, Accuracy 0.085%\n",
      "Epoch 3, Batch 56, LR 0.484492 Loss 19.061148, Accuracy 0.084%\n",
      "Epoch 3, Batch 57, LR 0.484705 Loss 19.060268, Accuracy 0.096%\n",
      "Epoch 3, Batch 58, LR 0.484918 Loss 19.056266, Accuracy 0.094%\n",
      "Epoch 3, Batch 59, LR 0.485131 Loss 19.057110, Accuracy 0.093%\n",
      "Epoch 3, Batch 60, LR 0.485344 Loss 19.058840, Accuracy 0.091%\n",
      "Epoch 3, Batch 61, LR 0.485557 Loss 19.056714, Accuracy 0.102%\n",
      "Epoch 3, Batch 62, LR 0.485770 Loss 19.057901, Accuracy 0.113%\n",
      "Epoch 3, Batch 63, LR 0.485983 Loss 19.057440, Accuracy 0.112%\n",
      "Epoch 3, Batch 64, LR 0.486196 Loss 19.057921, Accuracy 0.110%\n",
      "Epoch 3, Batch 65, LR 0.486410 Loss 19.054891, Accuracy 0.108%\n",
      "Epoch 3, Batch 66, LR 0.486623 Loss 19.050917, Accuracy 0.107%\n",
      "Epoch 3, Batch 67, LR 0.486836 Loss 19.051031, Accuracy 0.105%\n",
      "Epoch 3, Batch 68, LR 0.487050 Loss 19.051189, Accuracy 0.103%\n",
      "Epoch 3, Batch 69, LR 0.487263 Loss 19.051822, Accuracy 0.102%\n",
      "Epoch 3, Batch 70, LR 0.487477 Loss 19.052961, Accuracy 0.112%\n",
      "Epoch 3, Batch 71, LR 0.487691 Loss 19.053398, Accuracy 0.110%\n",
      "Epoch 3, Batch 72, LR 0.487904 Loss 19.053088, Accuracy 0.109%\n",
      "Epoch 3, Batch 73, LR 0.488118 Loss 19.056143, Accuracy 0.107%\n",
      "Epoch 3, Batch 74, LR 0.488332 Loss 19.056220, Accuracy 0.106%\n",
      "Epoch 3, Batch 75, LR 0.488545 Loss 19.057547, Accuracy 0.115%\n",
      "Epoch 3, Batch 76, LR 0.488759 Loss 19.057488, Accuracy 0.113%\n",
      "Epoch 3, Batch 77, LR 0.488973 Loss 19.057962, Accuracy 0.112%\n",
      "Epoch 3, Batch 78, LR 0.489187 Loss 19.058990, Accuracy 0.110%\n",
      "Epoch 3, Batch 79, LR 0.489401 Loss 19.062044, Accuracy 0.109%\n",
      "Epoch 3, Batch 80, LR 0.489615 Loss 19.059950, Accuracy 0.107%\n",
      "Epoch 3, Batch 81, LR 0.489829 Loss 19.059059, Accuracy 0.106%\n",
      "Epoch 3, Batch 82, LR 0.490044 Loss 19.057031, Accuracy 0.105%\n",
      "Epoch 3, Batch 83, LR 0.490258 Loss 19.056910, Accuracy 0.104%\n",
      "Epoch 3, Batch 84, LR 0.490472 Loss 19.057620, Accuracy 0.102%\n",
      "Epoch 3, Batch 85, LR 0.490686 Loss 19.056861, Accuracy 0.101%\n",
      "Epoch 3, Batch 86, LR 0.490901 Loss 19.055847, Accuracy 0.109%\n",
      "Epoch 3, Batch 87, LR 0.491115 Loss 19.054084, Accuracy 0.108%\n",
      "Epoch 3, Batch 88, LR 0.491330 Loss 19.053631, Accuracy 0.107%\n",
      "Epoch 3, Batch 89, LR 0.491544 Loss 19.052602, Accuracy 0.105%\n",
      "Epoch 3, Batch 90, LR 0.491759 Loss 19.051668, Accuracy 0.104%\n",
      "Epoch 3, Batch 91, LR 0.491973 Loss 19.051249, Accuracy 0.112%\n",
      "Epoch 3, Batch 92, LR 0.492188 Loss 19.052306, Accuracy 0.110%\n",
      "Epoch 3, Batch 93, LR 0.492403 Loss 19.053697, Accuracy 0.109%\n",
      "Epoch 3, Batch 94, LR 0.492618 Loss 19.053331, Accuracy 0.108%\n",
      "Epoch 3, Batch 95, LR 0.492833 Loss 19.055782, Accuracy 0.107%\n",
      "Epoch 3, Batch 96, LR 0.493047 Loss 19.057628, Accuracy 0.106%\n",
      "Epoch 3, Batch 97, LR 0.493262 Loss 19.057424, Accuracy 0.105%\n",
      "Epoch 3, Batch 98, LR 0.493477 Loss 19.055875, Accuracy 0.104%\n",
      "Epoch 3, Batch 99, LR 0.493692 Loss 19.057342, Accuracy 0.103%\n",
      "Epoch 3, Batch 100, LR 0.493908 Loss 19.057272, Accuracy 0.102%\n",
      "Epoch 3, Batch 101, LR 0.494123 Loss 19.056935, Accuracy 0.101%\n",
      "Epoch 3, Batch 102, LR 0.494338 Loss 19.058964, Accuracy 0.100%\n",
      "Epoch 3, Batch 103, LR 0.494553 Loss 19.057739, Accuracy 0.099%\n",
      "Epoch 3, Batch 104, LR 0.494768 Loss 19.059041, Accuracy 0.098%\n",
      "Epoch 3, Batch 105, LR 0.494984 Loss 19.059008, Accuracy 0.097%\n",
      "Epoch 3, Batch 106, LR 0.495199 Loss 19.059909, Accuracy 0.096%\n",
      "Epoch 3, Batch 107, LR 0.495415 Loss 19.058698, Accuracy 0.095%\n",
      "Epoch 3, Batch 108, LR 0.495630 Loss 19.058952, Accuracy 0.101%\n",
      "Epoch 3, Batch 109, LR 0.495846 Loss 19.059507, Accuracy 0.100%\n",
      "Epoch 3, Batch 110, LR 0.496061 Loss 19.060945, Accuracy 0.099%\n",
      "Epoch 3, Batch 111, LR 0.496277 Loss 19.057901, Accuracy 0.099%\n",
      "Epoch 3, Batch 112, LR 0.496493 Loss 19.057552, Accuracy 0.098%\n",
      "Epoch 3, Batch 113, LR 0.496708 Loss 19.060095, Accuracy 0.097%\n",
      "Epoch 3, Batch 114, LR 0.496924 Loss 19.059715, Accuracy 0.096%\n",
      "Epoch 3, Batch 115, LR 0.497140 Loss 19.059621, Accuracy 0.095%\n",
      "Epoch 3, Batch 116, LR 0.497356 Loss 19.059824, Accuracy 0.094%\n",
      "Epoch 3, Batch 117, LR 0.497572 Loss 19.059534, Accuracy 0.093%\n",
      "Epoch 3, Batch 118, LR 0.497788 Loss 19.057108, Accuracy 0.093%\n",
      "Epoch 3, Batch 119, LR 0.498004 Loss 19.057332, Accuracy 0.092%\n",
      "Epoch 3, Batch 120, LR 0.498220 Loss 19.056575, Accuracy 0.098%\n",
      "Epoch 3, Batch 121, LR 0.498436 Loss 19.056771, Accuracy 0.097%\n",
      "Epoch 3, Batch 122, LR 0.498653 Loss 19.056537, Accuracy 0.096%\n",
      "Epoch 3, Batch 123, LR 0.498869 Loss 19.056897, Accuracy 0.095%\n",
      "Epoch 3, Batch 124, LR 0.499085 Loss 19.056416, Accuracy 0.095%\n",
      "Epoch 3, Batch 125, LR 0.499302 Loss 19.057365, Accuracy 0.094%\n",
      "Epoch 3, Batch 126, LR 0.499518 Loss 19.058124, Accuracy 0.093%\n",
      "Epoch 3, Batch 127, LR 0.499735 Loss 19.059187, Accuracy 0.092%\n",
      "Epoch 3, Batch 128, LR 0.499951 Loss 19.060312, Accuracy 0.092%\n",
      "Epoch 3, Batch 129, LR 0.500168 Loss 19.061071, Accuracy 0.091%\n",
      "Epoch 3, Batch 130, LR 0.500384 Loss 19.059727, Accuracy 0.090%\n",
      "Epoch 3, Batch 131, LR 0.500601 Loss 19.058843, Accuracy 0.089%\n",
      "Epoch 3, Batch 132, LR 0.500818 Loss 19.059891, Accuracy 0.089%\n",
      "Epoch 3, Batch 133, LR 0.501034 Loss 19.059992, Accuracy 0.088%\n",
      "Epoch 3, Batch 134, LR 0.501251 Loss 19.060158, Accuracy 0.087%\n",
      "Epoch 3, Batch 135, LR 0.501468 Loss 19.062105, Accuracy 0.087%\n",
      "Epoch 3, Batch 136, LR 0.501685 Loss 19.061685, Accuracy 0.086%\n",
      "Epoch 3, Batch 137, LR 0.501902 Loss 19.060690, Accuracy 0.086%\n",
      "Epoch 3, Batch 138, LR 0.502119 Loss 19.060680, Accuracy 0.085%\n",
      "Epoch 3, Batch 139, LR 0.502336 Loss 19.059802, Accuracy 0.090%\n",
      "Epoch 3, Batch 140, LR 0.502553 Loss 19.059547, Accuracy 0.095%\n",
      "Epoch 3, Batch 141, LR 0.502771 Loss 19.058890, Accuracy 0.100%\n",
      "Epoch 3, Batch 142, LR 0.502988 Loss 19.059343, Accuracy 0.099%\n",
      "Epoch 3, Batch 143, LR 0.503205 Loss 19.059795, Accuracy 0.098%\n",
      "Epoch 3, Batch 144, LR 0.503422 Loss 19.058795, Accuracy 0.098%\n",
      "Epoch 3, Batch 145, LR 0.503640 Loss 19.056680, Accuracy 0.102%\n",
      "Epoch 3, Batch 146, LR 0.503857 Loss 19.057892, Accuracy 0.102%\n",
      "Epoch 3, Batch 147, LR 0.504075 Loss 19.056464, Accuracy 0.101%\n",
      "Epoch 3, Batch 148, LR 0.504292 Loss 19.055425, Accuracy 0.100%\n",
      "Epoch 3, Batch 149, LR 0.504510 Loss 19.055689, Accuracy 0.100%\n",
      "Epoch 3, Batch 150, LR 0.504727 Loss 19.056189, Accuracy 0.099%\n",
      "Epoch 3, Batch 151, LR 0.504945 Loss 19.057061, Accuracy 0.098%\n",
      "Epoch 3, Batch 152, LR 0.505163 Loss 19.057460, Accuracy 0.098%\n",
      "Epoch 3, Batch 153, LR 0.505381 Loss 19.056170, Accuracy 0.097%\n",
      "Epoch 3, Batch 154, LR 0.505599 Loss 19.056683, Accuracy 0.096%\n",
      "Epoch 3, Batch 155, LR 0.505816 Loss 19.056132, Accuracy 0.096%\n",
      "Epoch 3, Batch 156, LR 0.506034 Loss 19.056668, Accuracy 0.095%\n",
      "Epoch 3, Batch 157, LR 0.506252 Loss 19.055268, Accuracy 0.095%\n",
      "Epoch 3, Batch 158, LR 0.506470 Loss 19.054344, Accuracy 0.099%\n",
      "Epoch 3, Batch 159, LR 0.506689 Loss 19.054131, Accuracy 0.098%\n",
      "Epoch 3, Batch 160, LR 0.506907 Loss 19.055403, Accuracy 0.098%\n",
      "Epoch 3, Batch 161, LR 0.507125 Loss 19.056298, Accuracy 0.097%\n",
      "Epoch 3, Batch 162, LR 0.507343 Loss 19.056861, Accuracy 0.101%\n",
      "Epoch 3, Batch 163, LR 0.507561 Loss 19.058156, Accuracy 0.101%\n",
      "Epoch 3, Batch 164, LR 0.507780 Loss 19.057628, Accuracy 0.100%\n",
      "Epoch 3, Batch 165, LR 0.507998 Loss 19.056632, Accuracy 0.099%\n",
      "Epoch 3, Batch 166, LR 0.508217 Loss 19.057022, Accuracy 0.099%\n",
      "Epoch 3, Batch 167, LR 0.508435 Loss 19.056950, Accuracy 0.098%\n",
      "Epoch 3, Batch 168, LR 0.508654 Loss 19.056245, Accuracy 0.098%\n",
      "Epoch 3, Batch 169, LR 0.508872 Loss 19.056778, Accuracy 0.097%\n",
      "Epoch 3, Batch 170, LR 0.509091 Loss 19.055019, Accuracy 0.097%\n",
      "Epoch 3, Batch 171, LR 0.509310 Loss 19.054907, Accuracy 0.096%\n",
      "Epoch 3, Batch 172, LR 0.509528 Loss 19.054982, Accuracy 0.095%\n",
      "Epoch 3, Batch 173, LR 0.509747 Loss 19.054787, Accuracy 0.095%\n",
      "Epoch 3, Batch 174, LR 0.509966 Loss 19.054362, Accuracy 0.094%\n",
      "Epoch 3, Batch 175, LR 0.510185 Loss 19.053484, Accuracy 0.094%\n",
      "Epoch 3, Batch 176, LR 0.510404 Loss 19.054138, Accuracy 0.093%\n",
      "Epoch 3, Batch 177, LR 0.510623 Loss 19.053840, Accuracy 0.093%\n",
      "Epoch 3, Batch 178, LR 0.510842 Loss 19.054718, Accuracy 0.097%\n",
      "Epoch 3, Batch 179, LR 0.511061 Loss 19.054800, Accuracy 0.096%\n",
      "Epoch 3, Batch 180, LR 0.511280 Loss 19.055927, Accuracy 0.095%\n",
      "Epoch 3, Batch 181, LR 0.511499 Loss 19.056478, Accuracy 0.095%\n",
      "Epoch 3, Batch 182, LR 0.511719 Loss 19.056051, Accuracy 0.094%\n",
      "Epoch 3, Batch 183, LR 0.511938 Loss 19.056365, Accuracy 0.098%\n",
      "Epoch 3, Batch 184, LR 0.512157 Loss 19.055963, Accuracy 0.098%\n",
      "Epoch 3, Batch 185, LR 0.512377 Loss 19.056296, Accuracy 0.097%\n",
      "Epoch 3, Batch 186, LR 0.512596 Loss 19.056674, Accuracy 0.097%\n",
      "Epoch 3, Batch 187, LR 0.512816 Loss 19.056686, Accuracy 0.096%\n",
      "Epoch 3, Batch 188, LR 0.513035 Loss 19.056655, Accuracy 0.096%\n",
      "Epoch 3, Batch 189, LR 0.513255 Loss 19.056455, Accuracy 0.095%\n",
      "Epoch 3, Batch 190, LR 0.513474 Loss 19.057427, Accuracy 0.095%\n",
      "Epoch 3, Batch 191, LR 0.513694 Loss 19.057085, Accuracy 0.094%\n",
      "Epoch 3, Batch 192, LR 0.513914 Loss 19.057116, Accuracy 0.094%\n",
      "Epoch 3, Batch 193, LR 0.514134 Loss 19.057577, Accuracy 0.093%\n",
      "Epoch 3, Batch 194, LR 0.514354 Loss 19.058398, Accuracy 0.093%\n",
      "Epoch 3, Batch 195, LR 0.514573 Loss 19.058062, Accuracy 0.092%\n",
      "Epoch 3, Batch 196, LR 0.514793 Loss 19.058760, Accuracy 0.092%\n",
      "Epoch 3, Batch 197, LR 0.515013 Loss 19.059399, Accuracy 0.091%\n",
      "Epoch 3, Batch 198, LR 0.515233 Loss 19.060327, Accuracy 0.091%\n",
      "Epoch 3, Batch 199, LR 0.515454 Loss 19.060311, Accuracy 0.090%\n",
      "Epoch 3, Batch 200, LR 0.515674 Loss 19.059646, Accuracy 0.090%\n",
      "Epoch 3, Batch 201, LR 0.515894 Loss 19.059302, Accuracy 0.089%\n",
      "Epoch 3, Batch 202, LR 0.516114 Loss 19.060154, Accuracy 0.089%\n",
      "Epoch 3, Batch 203, LR 0.516334 Loss 19.060240, Accuracy 0.089%\n",
      "Epoch 3, Batch 204, LR 0.516555 Loss 19.060548, Accuracy 0.088%\n",
      "Epoch 3, Batch 205, LR 0.516775 Loss 19.060682, Accuracy 0.088%\n",
      "Epoch 3, Batch 206, LR 0.516996 Loss 19.061298, Accuracy 0.087%\n",
      "Epoch 3, Batch 207, LR 0.517216 Loss 19.061415, Accuracy 0.087%\n",
      "Epoch 3, Batch 208, LR 0.517437 Loss 19.060804, Accuracy 0.086%\n",
      "Epoch 3, Batch 209, LR 0.517657 Loss 19.060146, Accuracy 0.086%\n",
      "Epoch 3, Batch 210, LR 0.517878 Loss 19.059828, Accuracy 0.086%\n",
      "Epoch 3, Batch 211, LR 0.518099 Loss 19.059415, Accuracy 0.085%\n",
      "Epoch 3, Batch 212, LR 0.518319 Loss 19.060711, Accuracy 0.085%\n",
      "Epoch 3, Batch 213, LR 0.518540 Loss 19.061546, Accuracy 0.084%\n",
      "Epoch 3, Batch 214, LR 0.518761 Loss 19.061983, Accuracy 0.084%\n",
      "Epoch 3, Batch 215, LR 0.518982 Loss 19.061875, Accuracy 0.084%\n",
      "Epoch 3, Batch 216, LR 0.519203 Loss 19.061986, Accuracy 0.083%\n",
      "Epoch 3, Batch 217, LR 0.519424 Loss 19.062164, Accuracy 0.083%\n",
      "Epoch 3, Batch 218, LR 0.519645 Loss 19.063119, Accuracy 0.082%\n",
      "Epoch 3, Batch 219, LR 0.519866 Loss 19.063945, Accuracy 0.086%\n",
      "Epoch 3, Batch 220, LR 0.520087 Loss 19.063813, Accuracy 0.085%\n",
      "Epoch 3, Batch 221, LR 0.520308 Loss 19.064390, Accuracy 0.088%\n",
      "Epoch 3, Batch 222, LR 0.520530 Loss 19.064600, Accuracy 0.088%\n",
      "Epoch 3, Batch 223, LR 0.520751 Loss 19.063803, Accuracy 0.088%\n",
      "Epoch 3, Batch 224, LR 0.520972 Loss 19.064622, Accuracy 0.087%\n",
      "Epoch 3, Batch 225, LR 0.521194 Loss 19.063910, Accuracy 0.087%\n",
      "Epoch 3, Batch 226, LR 0.521415 Loss 19.062544, Accuracy 0.086%\n",
      "Epoch 3, Batch 227, LR 0.521637 Loss 19.062505, Accuracy 0.086%\n",
      "Epoch 3, Batch 228, LR 0.521858 Loss 19.062965, Accuracy 0.086%\n",
      "Epoch 3, Batch 229, LR 0.522080 Loss 19.062307, Accuracy 0.085%\n",
      "Epoch 3, Batch 230, LR 0.522301 Loss 19.061826, Accuracy 0.085%\n",
      "Epoch 3, Batch 231, LR 0.522523 Loss 19.061829, Accuracy 0.085%\n",
      "Epoch 3, Batch 232, LR 0.522745 Loss 19.061555, Accuracy 0.084%\n",
      "Epoch 3, Batch 233, LR 0.522967 Loss 19.061791, Accuracy 0.084%\n",
      "Epoch 3, Batch 234, LR 0.523188 Loss 19.061979, Accuracy 0.083%\n",
      "Epoch 3, Batch 235, LR 0.523410 Loss 19.062914, Accuracy 0.083%\n",
      "Epoch 3, Batch 236, LR 0.523632 Loss 19.062891, Accuracy 0.083%\n",
      "Epoch 3, Batch 237, LR 0.523854 Loss 19.062026, Accuracy 0.082%\n",
      "Epoch 3, Batch 238, LR 0.524076 Loss 19.061850, Accuracy 0.082%\n",
      "Epoch 3, Batch 239, LR 0.524298 Loss 19.061599, Accuracy 0.082%\n",
      "Epoch 3, Batch 240, LR 0.524520 Loss 19.061543, Accuracy 0.081%\n",
      "Epoch 3, Batch 241, LR 0.524743 Loss 19.061814, Accuracy 0.084%\n",
      "Epoch 3, Batch 242, LR 0.524965 Loss 19.061996, Accuracy 0.087%\n",
      "Epoch 3, Batch 243, LR 0.525187 Loss 19.062531, Accuracy 0.087%\n",
      "Epoch 3, Batch 244, LR 0.525410 Loss 19.062346, Accuracy 0.086%\n",
      "Epoch 3, Batch 245, LR 0.525632 Loss 19.062080, Accuracy 0.086%\n",
      "Epoch 3, Batch 246, LR 0.525854 Loss 19.062278, Accuracy 0.086%\n",
      "Epoch 3, Batch 247, LR 0.526077 Loss 19.062891, Accuracy 0.085%\n",
      "Epoch 3, Batch 248, LR 0.526299 Loss 19.062498, Accuracy 0.085%\n",
      "Epoch 3, Batch 249, LR 0.526522 Loss 19.063422, Accuracy 0.085%\n",
      "Epoch 3, Batch 250, LR 0.526745 Loss 19.063841, Accuracy 0.084%\n",
      "Epoch 3, Batch 251, LR 0.526967 Loss 19.063044, Accuracy 0.084%\n",
      "Epoch 3, Batch 252, LR 0.527190 Loss 19.063007, Accuracy 0.084%\n",
      "Epoch 3, Batch 253, LR 0.527413 Loss 19.064065, Accuracy 0.083%\n",
      "Epoch 3, Batch 254, LR 0.527636 Loss 19.063763, Accuracy 0.083%\n",
      "Epoch 3, Batch 255, LR 0.527858 Loss 19.063491, Accuracy 0.083%\n",
      "Epoch 3, Batch 256, LR 0.528081 Loss 19.063921, Accuracy 0.082%\n",
      "Epoch 3, Batch 257, LR 0.528304 Loss 19.064791, Accuracy 0.082%\n",
      "Epoch 3, Batch 258, LR 0.528527 Loss 19.065100, Accuracy 0.082%\n",
      "Epoch 3, Batch 259, LR 0.528750 Loss 19.065625, Accuracy 0.081%\n",
      "Epoch 3, Batch 260, LR 0.528973 Loss 19.065484, Accuracy 0.081%\n",
      "Epoch 3, Batch 261, LR 0.529197 Loss 19.065475, Accuracy 0.081%\n",
      "Epoch 3, Batch 262, LR 0.529420 Loss 19.065741, Accuracy 0.081%\n",
      "Epoch 3, Batch 263, LR 0.529643 Loss 19.064719, Accuracy 0.080%\n",
      "Epoch 3, Batch 264, LR 0.529866 Loss 19.064776, Accuracy 0.080%\n",
      "Epoch 3, Batch 265, LR 0.530090 Loss 19.064737, Accuracy 0.080%\n",
      "Epoch 3, Batch 266, LR 0.530313 Loss 19.064126, Accuracy 0.079%\n",
      "Epoch 3, Batch 267, LR 0.530537 Loss 19.063623, Accuracy 0.079%\n",
      "Epoch 3, Batch 268, LR 0.530760 Loss 19.063354, Accuracy 0.082%\n",
      "Epoch 3, Batch 269, LR 0.530984 Loss 19.063216, Accuracy 0.081%\n",
      "Epoch 3, Batch 270, LR 0.531207 Loss 19.063374, Accuracy 0.081%\n",
      "Epoch 3, Batch 271, LR 0.531431 Loss 19.063638, Accuracy 0.081%\n",
      "Epoch 3, Batch 272, LR 0.531655 Loss 19.063414, Accuracy 0.080%\n",
      "Epoch 3, Batch 273, LR 0.531878 Loss 19.063836, Accuracy 0.080%\n",
      "Epoch 3, Batch 274, LR 0.532102 Loss 19.063348, Accuracy 0.083%\n",
      "Epoch 3, Batch 275, LR 0.532326 Loss 19.063074, Accuracy 0.082%\n",
      "Epoch 3, Batch 276, LR 0.532550 Loss 19.063866, Accuracy 0.082%\n",
      "Epoch 3, Batch 277, LR 0.532774 Loss 19.063890, Accuracy 0.082%\n",
      "Epoch 3, Batch 278, LR 0.532998 Loss 19.064850, Accuracy 0.081%\n",
      "Epoch 3, Batch 279, LR 0.533222 Loss 19.065722, Accuracy 0.081%\n",
      "Epoch 3, Batch 280, LR 0.533446 Loss 19.066136, Accuracy 0.081%\n",
      "Epoch 3, Batch 281, LR 0.533670 Loss 19.065626, Accuracy 0.081%\n",
      "Epoch 3, Batch 282, LR 0.533894 Loss 19.065469, Accuracy 0.080%\n",
      "Epoch 3, Batch 283, LR 0.534119 Loss 19.065761, Accuracy 0.080%\n",
      "Epoch 3, Batch 284, LR 0.534343 Loss 19.065724, Accuracy 0.080%\n",
      "Epoch 3, Batch 285, LR 0.534567 Loss 19.066025, Accuracy 0.079%\n",
      "Epoch 3, Batch 286, LR 0.534792 Loss 19.065416, Accuracy 0.082%\n",
      "Epoch 3, Batch 287, LR 0.535016 Loss 19.064805, Accuracy 0.082%\n",
      "Epoch 3, Batch 288, LR 0.535241 Loss 19.064782, Accuracy 0.081%\n",
      "Epoch 3, Batch 289, LR 0.535465 Loss 19.064578, Accuracy 0.081%\n",
      "Epoch 3, Batch 290, LR 0.535690 Loss 19.064461, Accuracy 0.081%\n",
      "Epoch 3, Batch 291, LR 0.535914 Loss 19.064695, Accuracy 0.081%\n",
      "Epoch 3, Batch 292, LR 0.536139 Loss 19.064729, Accuracy 0.080%\n",
      "Epoch 3, Batch 293, LR 0.536364 Loss 19.065156, Accuracy 0.080%\n",
      "Epoch 3, Batch 294, LR 0.536589 Loss 19.064954, Accuracy 0.080%\n",
      "Epoch 3, Batch 295, LR 0.536813 Loss 19.064735, Accuracy 0.079%\n",
      "Epoch 3, Batch 296, LR 0.537038 Loss 19.064708, Accuracy 0.082%\n",
      "Epoch 3, Batch 297, LR 0.537263 Loss 19.065168, Accuracy 0.082%\n",
      "Epoch 3, Batch 298, LR 0.537488 Loss 19.065803, Accuracy 0.081%\n",
      "Epoch 3, Batch 299, LR 0.537713 Loss 19.065790, Accuracy 0.081%\n",
      "Epoch 3, Batch 300, LR 0.537938 Loss 19.065425, Accuracy 0.081%\n",
      "Epoch 3, Batch 301, LR 0.538163 Loss 19.065338, Accuracy 0.080%\n",
      "Epoch 3, Batch 302, LR 0.538388 Loss 19.065221, Accuracy 0.080%\n",
      "Epoch 3, Batch 303, LR 0.538614 Loss 19.065510, Accuracy 0.080%\n",
      "Epoch 3, Batch 304, LR 0.538839 Loss 19.065408, Accuracy 0.082%\n",
      "Epoch 3, Batch 305, LR 0.539064 Loss 19.065805, Accuracy 0.082%\n",
      "Epoch 3, Batch 306, LR 0.539290 Loss 19.065335, Accuracy 0.082%\n",
      "Epoch 3, Batch 307, LR 0.539515 Loss 19.065568, Accuracy 0.081%\n",
      "Epoch 3, Batch 308, LR 0.539740 Loss 19.065904, Accuracy 0.081%\n",
      "Epoch 3, Batch 309, LR 0.539966 Loss 19.066435, Accuracy 0.081%\n",
      "Epoch 3, Batch 310, LR 0.540191 Loss 19.066064, Accuracy 0.081%\n",
      "Epoch 3, Batch 311, LR 0.540417 Loss 19.066160, Accuracy 0.080%\n",
      "Epoch 3, Batch 312, LR 0.540643 Loss 19.066073, Accuracy 0.080%\n",
      "Epoch 3, Batch 313, LR 0.540868 Loss 19.066311, Accuracy 0.082%\n",
      "Epoch 3, Batch 314, LR 0.541094 Loss 19.066882, Accuracy 0.082%\n",
      "Epoch 3, Batch 315, LR 0.541320 Loss 19.066520, Accuracy 0.084%\n",
      "Epoch 3, Batch 316, LR 0.541546 Loss 19.066083, Accuracy 0.089%\n",
      "Epoch 3, Batch 317, LR 0.541772 Loss 19.066057, Accuracy 0.089%\n",
      "Epoch 3, Batch 318, LR 0.541998 Loss 19.065664, Accuracy 0.088%\n",
      "Epoch 3, Batch 319, LR 0.542224 Loss 19.065105, Accuracy 0.088%\n",
      "Epoch 3, Batch 320, LR 0.542450 Loss 19.065152, Accuracy 0.088%\n",
      "Epoch 3, Batch 321, LR 0.542676 Loss 19.065049, Accuracy 0.088%\n",
      "Epoch 3, Batch 322, LR 0.542902 Loss 19.064803, Accuracy 0.087%\n",
      "Epoch 3, Batch 323, LR 0.543128 Loss 19.064443, Accuracy 0.087%\n",
      "Epoch 3, Batch 324, LR 0.543354 Loss 19.064065, Accuracy 0.087%\n",
      "Epoch 3, Batch 325, LR 0.543581 Loss 19.064569, Accuracy 0.087%\n",
      "Epoch 3, Batch 326, LR 0.543807 Loss 19.064033, Accuracy 0.086%\n",
      "Epoch 3, Batch 327, LR 0.544033 Loss 19.063715, Accuracy 0.086%\n",
      "Epoch 3, Batch 328, LR 0.544260 Loss 19.064186, Accuracy 0.086%\n",
      "Epoch 3, Batch 329, LR 0.544486 Loss 19.064374, Accuracy 0.085%\n",
      "Epoch 3, Batch 330, LR 0.544713 Loss 19.064319, Accuracy 0.085%\n",
      "Epoch 3, Batch 331, LR 0.544939 Loss 19.064776, Accuracy 0.085%\n",
      "Epoch 3, Batch 332, LR 0.545166 Loss 19.064527, Accuracy 0.085%\n",
      "Epoch 3, Batch 333, LR 0.545392 Loss 19.064322, Accuracy 0.084%\n",
      "Epoch 3, Batch 334, LR 0.545619 Loss 19.064638, Accuracy 0.084%\n",
      "Epoch 3, Batch 335, LR 0.545846 Loss 19.064454, Accuracy 0.084%\n",
      "Epoch 3, Batch 336, LR 0.546073 Loss 19.064682, Accuracy 0.084%\n",
      "Epoch 3, Batch 337, LR 0.546300 Loss 19.065097, Accuracy 0.083%\n",
      "Epoch 3, Batch 338, LR 0.546526 Loss 19.064505, Accuracy 0.083%\n",
      "Epoch 3, Batch 339, LR 0.546753 Loss 19.065044, Accuracy 0.083%\n",
      "Epoch 3, Batch 340, LR 0.546980 Loss 19.065187, Accuracy 0.083%\n",
      "Epoch 3, Batch 341, LR 0.547207 Loss 19.064829, Accuracy 0.082%\n",
      "Epoch 3, Batch 342, LR 0.547434 Loss 19.064936, Accuracy 0.082%\n",
      "Epoch 3, Batch 343, LR 0.547662 Loss 19.065015, Accuracy 0.082%\n",
      "Epoch 3, Batch 344, LR 0.547889 Loss 19.064531, Accuracy 0.082%\n",
      "Epoch 3, Batch 345, LR 0.548116 Loss 19.064789, Accuracy 0.082%\n",
      "Epoch 3, Batch 346, LR 0.548343 Loss 19.064724, Accuracy 0.081%\n",
      "Epoch 3, Batch 347, LR 0.548571 Loss 19.065422, Accuracy 0.081%\n",
      "Epoch 3, Batch 348, LR 0.548798 Loss 19.065623, Accuracy 0.081%\n",
      "Epoch 3, Batch 349, LR 0.549025 Loss 19.065564, Accuracy 0.081%\n",
      "Epoch 3, Batch 350, LR 0.549253 Loss 19.065054, Accuracy 0.080%\n",
      "Epoch 3, Batch 351, LR 0.549480 Loss 19.064967, Accuracy 0.080%\n",
      "Epoch 3, Batch 352, LR 0.549708 Loss 19.064454, Accuracy 0.080%\n",
      "Epoch 3, Batch 353, LR 0.549936 Loss 19.064823, Accuracy 0.080%\n",
      "Epoch 3, Batch 354, LR 0.550163 Loss 19.065009, Accuracy 0.079%\n",
      "Epoch 3, Batch 355, LR 0.550391 Loss 19.065095, Accuracy 0.079%\n",
      "Epoch 3, Batch 356, LR 0.550619 Loss 19.064837, Accuracy 0.079%\n",
      "Epoch 3, Batch 357, LR 0.550847 Loss 19.064982, Accuracy 0.079%\n",
      "Epoch 3, Batch 358, LR 0.551074 Loss 19.064715, Accuracy 0.079%\n",
      "Epoch 3, Batch 359, LR 0.551302 Loss 19.064420, Accuracy 0.083%\n",
      "Epoch 3, Batch 360, LR 0.551530 Loss 19.064905, Accuracy 0.082%\n",
      "Epoch 3, Batch 361, LR 0.551758 Loss 19.064821, Accuracy 0.084%\n",
      "Epoch 3, Batch 362, LR 0.551986 Loss 19.065004, Accuracy 0.086%\n",
      "Epoch 3, Batch 363, LR 0.552214 Loss 19.065382, Accuracy 0.086%\n",
      "Epoch 3, Batch 364, LR 0.552442 Loss 19.065331, Accuracy 0.086%\n",
      "Epoch 3, Batch 365, LR 0.552671 Loss 19.065186, Accuracy 0.088%\n",
      "Epoch 3, Batch 366, LR 0.552899 Loss 19.065242, Accuracy 0.088%\n",
      "Epoch 3, Batch 367, LR 0.553127 Loss 19.064583, Accuracy 0.089%\n",
      "Epoch 3, Batch 368, LR 0.553356 Loss 19.064305, Accuracy 0.089%\n",
      "Epoch 3, Batch 369, LR 0.553584 Loss 19.064290, Accuracy 0.089%\n",
      "Epoch 3, Batch 370, LR 0.553812 Loss 19.064370, Accuracy 0.089%\n",
      "Epoch 3, Batch 371, LR 0.554041 Loss 19.064212, Accuracy 0.088%\n",
      "Epoch 3, Batch 372, LR 0.554269 Loss 19.064476, Accuracy 0.088%\n",
      "Epoch 3, Batch 373, LR 0.554498 Loss 19.064458, Accuracy 0.088%\n",
      "Epoch 3, Batch 374, LR 0.554726 Loss 19.064662, Accuracy 0.088%\n",
      "Epoch 3, Batch 375, LR 0.554955 Loss 19.064161, Accuracy 0.087%\n",
      "Epoch 3, Batch 376, LR 0.555184 Loss 19.064126, Accuracy 0.089%\n",
      "Epoch 3, Batch 377, LR 0.555413 Loss 19.063841, Accuracy 0.089%\n",
      "Epoch 3, Batch 378, LR 0.555641 Loss 19.063314, Accuracy 0.091%\n",
      "Epoch 3, Batch 379, LR 0.555870 Loss 19.063596, Accuracy 0.091%\n",
      "Epoch 3, Batch 380, LR 0.556099 Loss 19.063824, Accuracy 0.090%\n",
      "Epoch 3, Batch 381, LR 0.556328 Loss 19.063765, Accuracy 0.090%\n",
      "Epoch 3, Batch 382, LR 0.556557 Loss 19.064014, Accuracy 0.090%\n",
      "Epoch 3, Batch 383, LR 0.556786 Loss 19.064084, Accuracy 0.090%\n",
      "Epoch 3, Batch 384, LR 0.557015 Loss 19.063839, Accuracy 0.090%\n",
      "Epoch 3, Batch 385, LR 0.557244 Loss 19.063971, Accuracy 0.089%\n",
      "Epoch 3, Batch 386, LR 0.557473 Loss 19.063651, Accuracy 0.089%\n",
      "Epoch 3, Batch 387, LR 0.557703 Loss 19.063809, Accuracy 0.089%\n",
      "Epoch 3, Batch 388, LR 0.557932 Loss 19.063842, Accuracy 0.089%\n",
      "Epoch 3, Batch 389, LR 0.558161 Loss 19.063712, Accuracy 0.088%\n",
      "Epoch 3, Batch 390, LR 0.558391 Loss 19.063550, Accuracy 0.088%\n",
      "Epoch 3, Batch 391, LR 0.558620 Loss 19.063827, Accuracy 0.088%\n",
      "Epoch 3, Batch 392, LR 0.558850 Loss 19.063534, Accuracy 0.088%\n",
      "Epoch 3, Batch 393, LR 0.559079 Loss 19.063603, Accuracy 0.087%\n",
      "Epoch 3, Batch 394, LR 0.559309 Loss 19.063005, Accuracy 0.087%\n",
      "Epoch 3, Batch 395, LR 0.559538 Loss 19.062924, Accuracy 0.087%\n",
      "Epoch 3, Batch 396, LR 0.559768 Loss 19.062816, Accuracy 0.087%\n",
      "Epoch 3, Batch 397, LR 0.559998 Loss 19.062575, Accuracy 0.087%\n",
      "Epoch 3, Batch 398, LR 0.560227 Loss 19.062283, Accuracy 0.086%\n",
      "Epoch 3, Batch 399, LR 0.560457 Loss 19.062513, Accuracy 0.086%\n",
      "Epoch 3, Batch 400, LR 0.560687 Loss 19.062151, Accuracy 0.086%\n",
      "Epoch 3, Batch 401, LR 0.560917 Loss 19.061808, Accuracy 0.086%\n",
      "Epoch 3, Batch 402, LR 0.561147 Loss 19.061756, Accuracy 0.086%\n",
      "Epoch 3, Batch 403, LR 0.561377 Loss 19.061866, Accuracy 0.085%\n",
      "Epoch 3, Batch 404, LR 0.561607 Loss 19.062032, Accuracy 0.085%\n",
      "Epoch 3, Batch 405, LR 0.561837 Loss 19.061369, Accuracy 0.085%\n",
      "Epoch 3, Batch 406, LR 0.562067 Loss 19.061111, Accuracy 0.085%\n",
      "Epoch 3, Batch 407, LR 0.562297 Loss 19.060872, Accuracy 0.084%\n",
      "Epoch 3, Batch 408, LR 0.562527 Loss 19.060782, Accuracy 0.084%\n",
      "Epoch 3, Batch 409, LR 0.562758 Loss 19.060196, Accuracy 0.086%\n",
      "Epoch 3, Batch 410, LR 0.562988 Loss 19.059993, Accuracy 0.088%\n",
      "Epoch 3, Batch 411, LR 0.563218 Loss 19.060080, Accuracy 0.087%\n",
      "Epoch 3, Batch 412, LR 0.563449 Loss 19.060274, Accuracy 0.087%\n",
      "Epoch 3, Batch 413, LR 0.563679 Loss 19.060872, Accuracy 0.087%\n",
      "Epoch 3, Batch 414, LR 0.563910 Loss 19.060618, Accuracy 0.089%\n",
      "Epoch 3, Batch 415, LR 0.564140 Loss 19.060681, Accuracy 0.088%\n",
      "Epoch 3, Batch 416, LR 0.564371 Loss 19.060230, Accuracy 0.088%\n",
      "Epoch 3, Batch 417, LR 0.564601 Loss 19.060023, Accuracy 0.088%\n",
      "Epoch 3, Batch 418, LR 0.564832 Loss 19.059572, Accuracy 0.090%\n",
      "Epoch 3, Batch 419, LR 0.565063 Loss 19.059472, Accuracy 0.089%\n",
      "Epoch 3, Batch 420, LR 0.565294 Loss 19.059544, Accuracy 0.089%\n",
      "Epoch 3, Batch 421, LR 0.565524 Loss 19.059510, Accuracy 0.089%\n",
      "Epoch 3, Batch 422, LR 0.565755 Loss 19.059562, Accuracy 0.089%\n",
      "Epoch 3, Batch 423, LR 0.565986 Loss 19.059676, Accuracy 0.090%\n",
      "Epoch 3, Batch 424, LR 0.566217 Loss 19.059674, Accuracy 0.090%\n",
      "Epoch 3, Batch 425, LR 0.566448 Loss 19.059788, Accuracy 0.090%\n",
      "Epoch 3, Batch 426, LR 0.566679 Loss 19.059907, Accuracy 0.090%\n",
      "Epoch 3, Batch 427, LR 0.566910 Loss 19.060381, Accuracy 0.090%\n",
      "Epoch 3, Batch 428, LR 0.567141 Loss 19.060474, Accuracy 0.089%\n",
      "Epoch 3, Batch 429, LR 0.567373 Loss 19.060771, Accuracy 0.089%\n",
      "Epoch 3, Batch 430, LR 0.567604 Loss 19.060561, Accuracy 0.089%\n",
      "Epoch 3, Batch 431, LR 0.567835 Loss 19.060873, Accuracy 0.089%\n",
      "Epoch 3, Batch 432, LR 0.568067 Loss 19.060736, Accuracy 0.089%\n",
      "Epoch 3, Batch 433, LR 0.568298 Loss 19.060483, Accuracy 0.088%\n",
      "Epoch 3, Batch 434, LR 0.568529 Loss 19.060820, Accuracy 0.088%\n",
      "Epoch 3, Batch 435, LR 0.568761 Loss 19.060849, Accuracy 0.088%\n",
      "Epoch 3, Batch 436, LR 0.568992 Loss 19.061121, Accuracy 0.088%\n",
      "Epoch 3, Batch 437, LR 0.569224 Loss 19.061075, Accuracy 0.088%\n",
      "Epoch 3, Batch 438, LR 0.569456 Loss 19.061391, Accuracy 0.087%\n",
      "Epoch 3, Batch 439, LR 0.569687 Loss 19.060846, Accuracy 0.089%\n",
      "Epoch 3, Batch 440, LR 0.569919 Loss 19.061028, Accuracy 0.089%\n",
      "Epoch 3, Batch 441, LR 0.570151 Loss 19.060804, Accuracy 0.089%\n",
      "Epoch 3, Batch 442, LR 0.570382 Loss 19.060689, Accuracy 0.088%\n",
      "Epoch 3, Batch 443, LR 0.570614 Loss 19.060553, Accuracy 0.090%\n",
      "Epoch 3, Batch 444, LR 0.570846 Loss 19.060148, Accuracy 0.090%\n",
      "Epoch 3, Batch 445, LR 0.571078 Loss 19.060009, Accuracy 0.090%\n",
      "Epoch 3, Batch 446, LR 0.571310 Loss 19.059974, Accuracy 0.089%\n",
      "Epoch 3, Batch 447, LR 0.571542 Loss 19.059729, Accuracy 0.089%\n",
      "Epoch 3, Batch 448, LR 0.571774 Loss 19.059815, Accuracy 0.089%\n",
      "Epoch 3, Batch 449, LR 0.572006 Loss 19.060378, Accuracy 0.089%\n",
      "Epoch 3, Batch 450, LR 0.572239 Loss 19.060153, Accuracy 0.090%\n",
      "Epoch 3, Batch 451, LR 0.572471 Loss 19.060151, Accuracy 0.090%\n",
      "Epoch 3, Batch 452, LR 0.572703 Loss 19.060339, Accuracy 0.090%\n",
      "Epoch 3, Batch 453, LR 0.572935 Loss 19.060485, Accuracy 0.090%\n",
      "Epoch 3, Batch 454, LR 0.573168 Loss 19.060161, Accuracy 0.089%\n",
      "Epoch 3, Batch 455, LR 0.573400 Loss 19.059983, Accuracy 0.089%\n",
      "Epoch 3, Batch 456, LR 0.573633 Loss 19.060073, Accuracy 0.089%\n",
      "Epoch 3, Batch 457, LR 0.573865 Loss 19.059828, Accuracy 0.089%\n",
      "Epoch 3, Batch 458, LR 0.574098 Loss 19.059611, Accuracy 0.089%\n",
      "Epoch 3, Batch 459, LR 0.574330 Loss 19.059166, Accuracy 0.089%\n",
      "Epoch 3, Batch 460, LR 0.574563 Loss 19.059025, Accuracy 0.088%\n",
      "Epoch 3, Batch 461, LR 0.574795 Loss 19.059451, Accuracy 0.088%\n",
      "Epoch 3, Batch 462, LR 0.575028 Loss 19.059770, Accuracy 0.088%\n",
      "Epoch 3, Batch 463, LR 0.575261 Loss 19.059842, Accuracy 0.088%\n",
      "Epoch 3, Batch 464, LR 0.575494 Loss 19.059952, Accuracy 0.089%\n",
      "Epoch 3, Batch 465, LR 0.575727 Loss 19.059825, Accuracy 0.089%\n",
      "Epoch 3, Batch 466, LR 0.575960 Loss 19.059370, Accuracy 0.089%\n",
      "Epoch 3, Batch 467, LR 0.576193 Loss 19.059956, Accuracy 0.089%\n",
      "Epoch 3, Batch 468, LR 0.576426 Loss 19.059942, Accuracy 0.090%\n",
      "Epoch 3, Batch 469, LR 0.576659 Loss 19.060120, Accuracy 0.090%\n",
      "Epoch 3, Batch 470, LR 0.576892 Loss 19.060322, Accuracy 0.090%\n",
      "Epoch 3, Batch 471, LR 0.577125 Loss 19.060385, Accuracy 0.090%\n",
      "Epoch 3, Batch 472, LR 0.577358 Loss 19.060504, Accuracy 0.089%\n",
      "Epoch 3, Batch 473, LR 0.577591 Loss 19.060577, Accuracy 0.089%\n",
      "Epoch 3, Batch 474, LR 0.577825 Loss 19.060709, Accuracy 0.089%\n",
      "Epoch 3, Batch 475, LR 0.578058 Loss 19.060856, Accuracy 0.089%\n",
      "Epoch 3, Batch 476, LR 0.578291 Loss 19.061146, Accuracy 0.089%\n",
      "Epoch 3, Batch 477, LR 0.578525 Loss 19.060635, Accuracy 0.090%\n",
      "Epoch 3, Batch 478, LR 0.578758 Loss 19.060203, Accuracy 0.092%\n",
      "Epoch 3, Batch 479, LR 0.578992 Loss 19.060195, Accuracy 0.091%\n",
      "Epoch 3, Batch 480, LR 0.579225 Loss 19.060678, Accuracy 0.091%\n",
      "Epoch 3, Batch 481, LR 0.579459 Loss 19.060040, Accuracy 0.091%\n",
      "Epoch 3, Batch 482, LR 0.579692 Loss 19.059740, Accuracy 0.091%\n",
      "Epoch 3, Batch 483, LR 0.579926 Loss 19.059999, Accuracy 0.091%\n",
      "Epoch 3, Batch 484, LR 0.580160 Loss 19.060314, Accuracy 0.090%\n",
      "Epoch 3, Batch 485, LR 0.580394 Loss 19.060478, Accuracy 0.092%\n",
      "Epoch 3, Batch 486, LR 0.580627 Loss 19.060579, Accuracy 0.092%\n",
      "Epoch 3, Batch 487, LR 0.580861 Loss 19.060446, Accuracy 0.091%\n",
      "Epoch 3, Batch 488, LR 0.581095 Loss 19.060849, Accuracy 0.091%\n",
      "Epoch 3, Batch 489, LR 0.581329 Loss 19.060719, Accuracy 0.091%\n",
      "Epoch 3, Batch 490, LR 0.581563 Loss 19.060906, Accuracy 0.091%\n",
      "Epoch 3, Batch 491, LR 0.581797 Loss 19.060664, Accuracy 0.091%\n",
      "Epoch 3, Batch 492, LR 0.582031 Loss 19.060901, Accuracy 0.091%\n",
      "Epoch 3, Batch 493, LR 0.582266 Loss 19.060821, Accuracy 0.090%\n",
      "Epoch 3, Batch 494, LR 0.582500 Loss 19.060814, Accuracy 0.090%\n",
      "Epoch 3, Batch 495, LR 0.582734 Loss 19.060824, Accuracy 0.090%\n",
      "Epoch 3, Batch 496, LR 0.582968 Loss 19.060520, Accuracy 0.090%\n",
      "Epoch 3, Batch 497, LR 0.583203 Loss 19.060513, Accuracy 0.090%\n",
      "Epoch 3, Batch 498, LR 0.583437 Loss 19.060416, Accuracy 0.089%\n",
      "Epoch 3, Batch 499, LR 0.583671 Loss 19.060281, Accuracy 0.089%\n",
      "Epoch 3, Batch 500, LR 0.583906 Loss 19.060556, Accuracy 0.089%\n",
      "Epoch 3, Batch 501, LR 0.584140 Loss 19.060886, Accuracy 0.089%\n",
      "Epoch 3, Batch 502, LR 0.584375 Loss 19.061139, Accuracy 0.089%\n",
      "Epoch 3, Batch 503, LR 0.584610 Loss 19.060973, Accuracy 0.089%\n",
      "Epoch 3, Batch 504, LR 0.584844 Loss 19.060996, Accuracy 0.088%\n",
      "Epoch 3, Batch 505, LR 0.585079 Loss 19.060794, Accuracy 0.088%\n",
      "Epoch 3, Batch 506, LR 0.585314 Loss 19.061099, Accuracy 0.088%\n",
      "Epoch 3, Batch 507, LR 0.585548 Loss 19.061421, Accuracy 0.088%\n",
      "Epoch 3, Batch 508, LR 0.585783 Loss 19.061478, Accuracy 0.088%\n",
      "Epoch 3, Batch 509, LR 0.586018 Loss 19.061525, Accuracy 0.087%\n",
      "Epoch 3, Batch 510, LR 0.586253 Loss 19.061484, Accuracy 0.087%\n",
      "Epoch 3, Batch 511, LR 0.586488 Loss 19.061523, Accuracy 0.087%\n",
      "Epoch 3, Batch 512, LR 0.586723 Loss 19.061436, Accuracy 0.087%\n",
      "Epoch 3, Batch 513, LR 0.586958 Loss 19.062002, Accuracy 0.087%\n",
      "Epoch 3, Batch 514, LR 0.587193 Loss 19.061801, Accuracy 0.087%\n",
      "Epoch 3, Batch 515, LR 0.587428 Loss 19.061923, Accuracy 0.086%\n",
      "Epoch 3, Batch 516, LR 0.587664 Loss 19.061876, Accuracy 0.086%\n",
      "Epoch 3, Batch 517, LR 0.587899 Loss 19.061402, Accuracy 0.086%\n",
      "Epoch 3, Batch 518, LR 0.588134 Loss 19.061239, Accuracy 0.086%\n",
      "Epoch 3, Batch 519, LR 0.588369 Loss 19.061451, Accuracy 0.086%\n",
      "Epoch 3, Batch 520, LR 0.588605 Loss 19.061772, Accuracy 0.086%\n",
      "Epoch 3, Batch 521, LR 0.588840 Loss 19.061969, Accuracy 0.085%\n",
      "Epoch 3, Batch 522, LR 0.589076 Loss 19.061719, Accuracy 0.085%\n",
      "Epoch 3, Batch 523, LR 0.589311 Loss 19.061806, Accuracy 0.085%\n",
      "Epoch 3, Batch 524, LR 0.589547 Loss 19.062010, Accuracy 0.085%\n",
      "Epoch 3, Batch 525, LR 0.589782 Loss 19.061846, Accuracy 0.086%\n",
      "Epoch 3, Batch 526, LR 0.590018 Loss 19.061670, Accuracy 0.086%\n",
      "Epoch 3, Batch 527, LR 0.590254 Loss 19.061869, Accuracy 0.086%\n",
      "Epoch 3, Batch 528, LR 0.590489 Loss 19.061767, Accuracy 0.086%\n",
      "Epoch 3, Batch 529, LR 0.590725 Loss 19.061678, Accuracy 0.086%\n",
      "Epoch 3, Batch 530, LR 0.590961 Loss 19.061743, Accuracy 0.085%\n",
      "Epoch 3, Batch 531, LR 0.591197 Loss 19.062079, Accuracy 0.085%\n",
      "Epoch 3, Batch 532, LR 0.591433 Loss 19.061848, Accuracy 0.085%\n",
      "Epoch 3, Batch 533, LR 0.591669 Loss 19.061701, Accuracy 0.085%\n",
      "Epoch 3, Batch 534, LR 0.591905 Loss 19.061668, Accuracy 0.085%\n",
      "Epoch 3, Batch 535, LR 0.592141 Loss 19.061390, Accuracy 0.085%\n",
      "Epoch 3, Batch 536, LR 0.592377 Loss 19.061553, Accuracy 0.085%\n",
      "Epoch 3, Batch 537, LR 0.592613 Loss 19.061768, Accuracy 0.084%\n",
      "Epoch 3, Batch 538, LR 0.592849 Loss 19.061821, Accuracy 0.084%\n",
      "Epoch 3, Batch 539, LR 0.593086 Loss 19.062055, Accuracy 0.084%\n",
      "Epoch 3, Batch 540, LR 0.593322 Loss 19.061955, Accuracy 0.084%\n",
      "Epoch 3, Batch 541, LR 0.593558 Loss 19.061992, Accuracy 0.084%\n",
      "Epoch 3, Batch 542, LR 0.593794 Loss 19.062079, Accuracy 0.084%\n",
      "Epoch 3, Batch 543, LR 0.594031 Loss 19.061952, Accuracy 0.083%\n",
      "Epoch 3, Batch 544, LR 0.594267 Loss 19.061445, Accuracy 0.083%\n",
      "Epoch 3, Batch 545, LR 0.594504 Loss 19.061308, Accuracy 0.083%\n",
      "Epoch 3, Batch 546, LR 0.594740 Loss 19.060931, Accuracy 0.083%\n",
      "Epoch 3, Batch 547, LR 0.594977 Loss 19.060576, Accuracy 0.083%\n",
      "Epoch 3, Batch 548, LR 0.595214 Loss 19.060907, Accuracy 0.083%\n",
      "Epoch 3, Batch 549, LR 0.595450 Loss 19.060823, Accuracy 0.083%\n",
      "Epoch 3, Batch 550, LR 0.595687 Loss 19.060676, Accuracy 0.082%\n",
      "Epoch 3, Batch 551, LR 0.595924 Loss 19.061015, Accuracy 0.082%\n",
      "Epoch 3, Batch 552, LR 0.596161 Loss 19.061067, Accuracy 0.082%\n",
      "Epoch 3, Batch 553, LR 0.596398 Loss 19.061036, Accuracy 0.082%\n",
      "Epoch 3, Batch 554, LR 0.596634 Loss 19.061232, Accuracy 0.082%\n",
      "Epoch 3, Batch 555, LR 0.596871 Loss 19.061324, Accuracy 0.082%\n",
      "Epoch 3, Batch 556, LR 0.597108 Loss 19.061440, Accuracy 0.081%\n",
      "Epoch 3, Batch 557, LR 0.597345 Loss 19.061249, Accuracy 0.081%\n",
      "Epoch 3, Batch 558, LR 0.597582 Loss 19.061204, Accuracy 0.083%\n",
      "Epoch 3, Batch 559, LR 0.597820 Loss 19.061331, Accuracy 0.082%\n",
      "Epoch 3, Batch 560, LR 0.598057 Loss 19.061293, Accuracy 0.082%\n",
      "Epoch 3, Batch 561, LR 0.598294 Loss 19.060905, Accuracy 0.082%\n",
      "Epoch 3, Batch 562, LR 0.598531 Loss 19.060828, Accuracy 0.082%\n",
      "Epoch 3, Batch 563, LR 0.598769 Loss 19.060993, Accuracy 0.082%\n",
      "Epoch 3, Batch 564, LR 0.599006 Loss 19.060747, Accuracy 0.082%\n",
      "Epoch 3, Batch 565, LR 0.599243 Loss 19.060598, Accuracy 0.082%\n",
      "Epoch 3, Batch 566, LR 0.599481 Loss 19.060512, Accuracy 0.081%\n",
      "Epoch 3, Batch 567, LR 0.599718 Loss 19.060503, Accuracy 0.081%\n",
      "Epoch 3, Batch 568, LR 0.599956 Loss 19.060743, Accuracy 0.081%\n",
      "Epoch 3, Batch 569, LR 0.600193 Loss 19.060946, Accuracy 0.081%\n",
      "Epoch 3, Batch 570, LR 0.600431 Loss 19.060831, Accuracy 0.082%\n",
      "Epoch 3, Batch 571, LR 0.600669 Loss 19.060785, Accuracy 0.082%\n",
      "Epoch 3, Batch 572, LR 0.600906 Loss 19.060726, Accuracy 0.082%\n",
      "Epoch 3, Batch 573, LR 0.601144 Loss 19.060480, Accuracy 0.082%\n",
      "Epoch 3, Batch 574, LR 0.601382 Loss 19.060554, Accuracy 0.082%\n",
      "Epoch 3, Batch 575, LR 0.601620 Loss 19.060770, Accuracy 0.082%\n",
      "Epoch 3, Batch 576, LR 0.601858 Loss 19.060657, Accuracy 0.081%\n",
      "Epoch 3, Batch 577, LR 0.602096 Loss 19.060791, Accuracy 0.081%\n",
      "Epoch 3, Batch 578, LR 0.602334 Loss 19.060886, Accuracy 0.081%\n",
      "Epoch 3, Batch 579, LR 0.602572 Loss 19.061226, Accuracy 0.081%\n",
      "Epoch 3, Batch 580, LR 0.602810 Loss 19.061437, Accuracy 0.081%\n",
      "Epoch 3, Batch 581, LR 0.603048 Loss 19.061584, Accuracy 0.082%\n",
      "Epoch 3, Batch 582, LR 0.603286 Loss 19.061648, Accuracy 0.082%\n",
      "Epoch 3, Batch 583, LR 0.603524 Loss 19.061516, Accuracy 0.082%\n",
      "Epoch 3, Batch 584, LR 0.603762 Loss 19.061268, Accuracy 0.082%\n",
      "Epoch 3, Batch 585, LR 0.604001 Loss 19.061448, Accuracy 0.081%\n",
      "Epoch 3, Batch 586, LR 0.604239 Loss 19.061674, Accuracy 0.081%\n",
      "Epoch 3, Batch 587, LR 0.604477 Loss 19.061745, Accuracy 0.083%\n",
      "Epoch 3, Batch 588, LR 0.604716 Loss 19.061621, Accuracy 0.082%\n",
      "Epoch 3, Batch 589, LR 0.604954 Loss 19.061461, Accuracy 0.082%\n",
      "Epoch 3, Batch 590, LR 0.605193 Loss 19.061902, Accuracy 0.082%\n",
      "Epoch 3, Batch 591, LR 0.605431 Loss 19.061841, Accuracy 0.082%\n",
      "Epoch 3, Batch 592, LR 0.605670 Loss 19.061870, Accuracy 0.082%\n",
      "Epoch 3, Batch 593, LR 0.605909 Loss 19.061888, Accuracy 0.082%\n",
      "Epoch 3, Batch 594, LR 0.606147 Loss 19.062197, Accuracy 0.082%\n",
      "Epoch 3, Batch 595, LR 0.606386 Loss 19.062010, Accuracy 0.081%\n",
      "Epoch 3, Batch 596, LR 0.606625 Loss 19.061750, Accuracy 0.081%\n",
      "Epoch 3, Batch 597, LR 0.606864 Loss 19.061739, Accuracy 0.081%\n",
      "Epoch 3, Batch 598, LR 0.607102 Loss 19.061818, Accuracy 0.081%\n",
      "Epoch 3, Batch 599, LR 0.607341 Loss 19.062125, Accuracy 0.081%\n",
      "Epoch 3, Batch 600, LR 0.607580 Loss 19.062301, Accuracy 0.081%\n",
      "Epoch 3, Batch 601, LR 0.607819 Loss 19.062017, Accuracy 0.081%\n",
      "Epoch 3, Batch 602, LR 0.608058 Loss 19.062015, Accuracy 0.080%\n",
      "Epoch 3, Batch 603, LR 0.608297 Loss 19.062140, Accuracy 0.080%\n",
      "Epoch 3, Batch 604, LR 0.608537 Loss 19.062456, Accuracy 0.080%\n",
      "Epoch 3, Batch 605, LR 0.608776 Loss 19.062518, Accuracy 0.080%\n",
      "Epoch 3, Batch 606, LR 0.609015 Loss 19.062467, Accuracy 0.080%\n",
      "Epoch 3, Batch 607, LR 0.609254 Loss 19.062571, Accuracy 0.080%\n",
      "Epoch 3, Batch 608, LR 0.609494 Loss 19.062428, Accuracy 0.080%\n",
      "Epoch 3, Batch 609, LR 0.609733 Loss 19.062232, Accuracy 0.080%\n",
      "Epoch 3, Batch 610, LR 0.609972 Loss 19.062516, Accuracy 0.079%\n",
      "Epoch 3, Batch 611, LR 0.610212 Loss 19.062702, Accuracy 0.079%\n",
      "Epoch 3, Batch 612, LR 0.610451 Loss 19.062762, Accuracy 0.079%\n",
      "Epoch 3, Batch 613, LR 0.610691 Loss 19.062672, Accuracy 0.079%\n",
      "Epoch 3, Batch 614, LR 0.610930 Loss 19.062493, Accuracy 0.079%\n",
      "Epoch 3, Batch 615, LR 0.611170 Loss 19.062494, Accuracy 0.079%\n",
      "Epoch 3, Batch 616, LR 0.611410 Loss 19.062521, Accuracy 0.079%\n",
      "Epoch 3, Batch 617, LR 0.611649 Loss 19.062513, Accuracy 0.079%\n",
      "Epoch 3, Batch 618, LR 0.611889 Loss 19.062779, Accuracy 0.078%\n",
      "Epoch 3, Batch 619, LR 0.612129 Loss 19.062712, Accuracy 0.078%\n",
      "Epoch 3, Batch 620, LR 0.612369 Loss 19.062883, Accuracy 0.078%\n",
      "Epoch 3, Batch 621, LR 0.612608 Loss 19.062781, Accuracy 0.078%\n",
      "Epoch 3, Batch 622, LR 0.612848 Loss 19.062797, Accuracy 0.078%\n",
      "Epoch 3, Batch 623, LR 0.613088 Loss 19.063115, Accuracy 0.078%\n",
      "Epoch 3, Batch 624, LR 0.613328 Loss 19.062903, Accuracy 0.078%\n",
      "Epoch 3, Batch 625, LR 0.613568 Loss 19.062790, Accuracy 0.077%\n",
      "Epoch 3, Batch 626, LR 0.613808 Loss 19.062801, Accuracy 0.077%\n",
      "Epoch 3, Batch 627, LR 0.614049 Loss 19.062552, Accuracy 0.077%\n",
      "Epoch 3, Batch 628, LR 0.614289 Loss 19.062539, Accuracy 0.078%\n",
      "Epoch 3, Batch 629, LR 0.614529 Loss 19.062660, Accuracy 0.078%\n",
      "Epoch 3, Batch 630, LR 0.614769 Loss 19.062971, Accuracy 0.078%\n",
      "Epoch 3, Batch 631, LR 0.615010 Loss 19.062732, Accuracy 0.078%\n",
      "Epoch 3, Batch 632, LR 0.615250 Loss 19.062855, Accuracy 0.078%\n",
      "Epoch 3, Batch 633, LR 0.615490 Loss 19.062864, Accuracy 0.078%\n",
      "Epoch 3, Batch 634, LR 0.615731 Loss 19.063209, Accuracy 0.079%\n",
      "Epoch 3, Batch 635, LR 0.615971 Loss 19.063157, Accuracy 0.080%\n",
      "Epoch 3, Batch 636, LR 0.616212 Loss 19.063445, Accuracy 0.080%\n",
      "Epoch 3, Batch 637, LR 0.616452 Loss 19.063645, Accuracy 0.080%\n",
      "Epoch 3, Batch 638, LR 0.616693 Loss 19.063589, Accuracy 0.080%\n",
      "Epoch 3, Batch 639, LR 0.616934 Loss 19.063454, Accuracy 0.079%\n",
      "Epoch 3, Batch 640, LR 0.617174 Loss 19.063430, Accuracy 0.079%\n",
      "Epoch 3, Batch 641, LR 0.617415 Loss 19.063174, Accuracy 0.079%\n",
      "Epoch 3, Batch 642, LR 0.617656 Loss 19.063163, Accuracy 0.079%\n",
      "Epoch 3, Batch 643, LR 0.617897 Loss 19.063080, Accuracy 0.079%\n",
      "Epoch 3, Batch 644, LR 0.618138 Loss 19.063125, Accuracy 0.079%\n",
      "Epoch 3, Batch 645, LR 0.618379 Loss 19.063165, Accuracy 0.079%\n",
      "Epoch 3, Batch 646, LR 0.618620 Loss 19.063419, Accuracy 0.079%\n",
      "Epoch 3, Batch 647, LR 0.618861 Loss 19.063299, Accuracy 0.078%\n",
      "Epoch 3, Batch 648, LR 0.619102 Loss 19.063073, Accuracy 0.080%\n",
      "Epoch 3, Batch 649, LR 0.619343 Loss 19.063061, Accuracy 0.079%\n",
      "Epoch 3, Batch 650, LR 0.619584 Loss 19.062785, Accuracy 0.079%\n",
      "Epoch 3, Batch 651, LR 0.619825 Loss 19.062615, Accuracy 0.079%\n",
      "Epoch 3, Batch 652, LR 0.620066 Loss 19.062681, Accuracy 0.079%\n",
      "Epoch 3, Batch 653, LR 0.620307 Loss 19.062598, Accuracy 0.079%\n",
      "Epoch 3, Batch 654, LR 0.620549 Loss 19.062574, Accuracy 0.079%\n",
      "Epoch 3, Batch 655, LR 0.620790 Loss 19.062585, Accuracy 0.079%\n",
      "Epoch 3, Batch 656, LR 0.621032 Loss 19.062505, Accuracy 0.080%\n",
      "Epoch 3, Batch 657, LR 0.621273 Loss 19.062465, Accuracy 0.080%\n",
      "Epoch 3, Batch 658, LR 0.621514 Loss 19.062506, Accuracy 0.080%\n",
      "Epoch 3, Batch 659, LR 0.621756 Loss 19.062586, Accuracy 0.079%\n",
      "Epoch 3, Batch 660, LR 0.621998 Loss 19.062322, Accuracy 0.080%\n",
      "Epoch 3, Batch 661, LR 0.622239 Loss 19.062314, Accuracy 0.080%\n",
      "Epoch 3, Batch 662, LR 0.622481 Loss 19.062222, Accuracy 0.080%\n",
      "Epoch 3, Batch 663, LR 0.622723 Loss 19.062357, Accuracy 0.080%\n",
      "Epoch 3, Batch 664, LR 0.622964 Loss 19.062471, Accuracy 0.080%\n",
      "Epoch 3, Batch 665, LR 0.623206 Loss 19.062604, Accuracy 0.080%\n",
      "Epoch 3, Batch 666, LR 0.623448 Loss 19.062542, Accuracy 0.080%\n",
      "Epoch 3, Batch 667, LR 0.623690 Loss 19.062542, Accuracy 0.080%\n",
      "Epoch 3, Batch 668, LR 0.623932 Loss 19.062676, Accuracy 0.080%\n",
      "Epoch 3, Batch 669, LR 0.624174 Loss 19.062674, Accuracy 0.079%\n",
      "Epoch 3, Batch 670, LR 0.624416 Loss 19.062769, Accuracy 0.079%\n",
      "Epoch 3, Batch 671, LR 0.624658 Loss 19.062839, Accuracy 0.079%\n",
      "Epoch 3, Batch 672, LR 0.624900 Loss 19.062484, Accuracy 0.080%\n",
      "Epoch 3, Batch 673, LR 0.625142 Loss 19.062539, Accuracy 0.080%\n",
      "Epoch 3, Batch 674, LR 0.625384 Loss 19.062760, Accuracy 0.080%\n",
      "Epoch 3, Batch 675, LR 0.625626 Loss 19.062885, Accuracy 0.081%\n",
      "Epoch 3, Batch 676, LR 0.625869 Loss 19.062981, Accuracy 0.081%\n",
      "Epoch 3, Batch 677, LR 0.626111 Loss 19.062805, Accuracy 0.081%\n",
      "Epoch 3, Batch 678, LR 0.626353 Loss 19.062733, Accuracy 0.081%\n",
      "Epoch 3, Batch 679, LR 0.626596 Loss 19.062684, Accuracy 0.081%\n",
      "Epoch 3, Batch 680, LR 0.626838 Loss 19.062558, Accuracy 0.080%\n",
      "Epoch 3, Batch 681, LR 0.627080 Loss 19.062623, Accuracy 0.080%\n",
      "Epoch 3, Batch 682, LR 0.627323 Loss 19.062880, Accuracy 0.080%\n",
      "Epoch 3, Batch 683, LR 0.627566 Loss 19.062759, Accuracy 0.080%\n",
      "Epoch 3, Batch 684, LR 0.627808 Loss 19.062747, Accuracy 0.080%\n",
      "Epoch 3, Batch 685, LR 0.628051 Loss 19.062928, Accuracy 0.080%\n",
      "Epoch 3, Batch 686, LR 0.628293 Loss 19.062913, Accuracy 0.081%\n",
      "Epoch 3, Batch 687, LR 0.628536 Loss 19.063258, Accuracy 0.081%\n",
      "Epoch 3, Batch 688, LR 0.628779 Loss 19.063106, Accuracy 0.081%\n",
      "Epoch 3, Batch 689, LR 0.629022 Loss 19.063126, Accuracy 0.081%\n",
      "Epoch 3, Batch 690, LR 0.629265 Loss 19.063465, Accuracy 0.080%\n",
      "Epoch 3, Batch 691, LR 0.629508 Loss 19.063276, Accuracy 0.080%\n",
      "Epoch 3, Batch 692, LR 0.629751 Loss 19.063127, Accuracy 0.080%\n",
      "Epoch 3, Batch 693, LR 0.629993 Loss 19.063168, Accuracy 0.081%\n",
      "Epoch 3, Batch 694, LR 0.630237 Loss 19.062971, Accuracy 0.081%\n",
      "Epoch 3, Batch 695, LR 0.630480 Loss 19.062839, Accuracy 0.081%\n",
      "Epoch 3, Batch 696, LR 0.630723 Loss 19.062691, Accuracy 0.082%\n",
      "Epoch 3, Batch 697, LR 0.630966 Loss 19.062782, Accuracy 0.082%\n",
      "Epoch 3, Batch 698, LR 0.631209 Loss 19.062754, Accuracy 0.082%\n",
      "Epoch 3, Batch 699, LR 0.631452 Loss 19.062925, Accuracy 0.082%\n",
      "Epoch 3, Batch 700, LR 0.631696 Loss 19.062904, Accuracy 0.081%\n",
      "Epoch 3, Batch 701, LR 0.631939 Loss 19.062884, Accuracy 0.081%\n",
      "Epoch 3, Batch 702, LR 0.632182 Loss 19.063068, Accuracy 0.081%\n",
      "Epoch 3, Batch 703, LR 0.632426 Loss 19.063164, Accuracy 0.081%\n",
      "Epoch 3, Batch 704, LR 0.632669 Loss 19.062951, Accuracy 0.081%\n",
      "Epoch 3, Batch 705, LR 0.632913 Loss 19.063204, Accuracy 0.081%\n",
      "Epoch 3, Batch 706, LR 0.633156 Loss 19.063292, Accuracy 0.081%\n",
      "Epoch 3, Batch 707, LR 0.633400 Loss 19.063002, Accuracy 0.081%\n",
      "Epoch 3, Batch 708, LR 0.633643 Loss 19.063255, Accuracy 0.081%\n",
      "Epoch 3, Batch 709, LR 0.633887 Loss 19.063181, Accuracy 0.082%\n",
      "Epoch 3, Batch 710, LR 0.634131 Loss 19.063166, Accuracy 0.081%\n",
      "Epoch 3, Batch 711, LR 0.634375 Loss 19.063252, Accuracy 0.081%\n",
      "Epoch 3, Batch 712, LR 0.634618 Loss 19.063581, Accuracy 0.081%\n",
      "Epoch 3, Batch 713, LR 0.634862 Loss 19.063603, Accuracy 0.081%\n",
      "Epoch 3, Batch 714, LR 0.635106 Loss 19.063676, Accuracy 0.081%\n",
      "Epoch 3, Batch 715, LR 0.635350 Loss 19.063371, Accuracy 0.081%\n",
      "Epoch 3, Batch 716, LR 0.635594 Loss 19.063278, Accuracy 0.081%\n",
      "Epoch 3, Batch 717, LR 0.635838 Loss 19.063391, Accuracy 0.081%\n",
      "Epoch 3, Batch 718, LR 0.636082 Loss 19.063317, Accuracy 0.081%\n",
      "Epoch 3, Batch 719, LR 0.636326 Loss 19.063512, Accuracy 0.080%\n",
      "Epoch 3, Batch 720, LR 0.636570 Loss 19.063286, Accuracy 0.080%\n",
      "Epoch 3, Batch 721, LR 0.636814 Loss 19.062870, Accuracy 0.080%\n",
      "Epoch 3, Batch 722, LR 0.637059 Loss 19.062836, Accuracy 0.080%\n",
      "Epoch 3, Batch 723, LR 0.637303 Loss 19.062745, Accuracy 0.080%\n",
      "Epoch 3, Batch 724, LR 0.637547 Loss 19.062714, Accuracy 0.080%\n",
      "Epoch 3, Batch 725, LR 0.637792 Loss 19.062384, Accuracy 0.080%\n",
      "Epoch 3, Batch 726, LR 0.638036 Loss 19.062351, Accuracy 0.080%\n",
      "Epoch 3, Batch 727, LR 0.638280 Loss 19.062320, Accuracy 0.080%\n",
      "Epoch 3, Batch 728, LR 0.638525 Loss 19.062259, Accuracy 0.079%\n",
      "Epoch 3, Batch 729, LR 0.638769 Loss 19.062382, Accuracy 0.079%\n",
      "Epoch 3, Batch 730, LR 0.639014 Loss 19.062695, Accuracy 0.079%\n",
      "Epoch 3, Batch 731, LR 0.639258 Loss 19.062704, Accuracy 0.079%\n",
      "Epoch 3, Batch 732, LR 0.639503 Loss 19.062383, Accuracy 0.079%\n",
      "Epoch 3, Batch 733, LR 0.639748 Loss 19.062496, Accuracy 0.079%\n",
      "Epoch 3, Batch 734, LR 0.639993 Loss 19.062492, Accuracy 0.079%\n",
      "Epoch 3, Batch 735, LR 0.640237 Loss 19.062282, Accuracy 0.079%\n",
      "Epoch 3, Batch 736, LR 0.640482 Loss 19.062247, Accuracy 0.079%\n",
      "Epoch 3, Batch 737, LR 0.640727 Loss 19.062320, Accuracy 0.078%\n",
      "Epoch 3, Batch 738, LR 0.640972 Loss 19.062142, Accuracy 0.079%\n",
      "Epoch 3, Batch 739, LR 0.641217 Loss 19.062240, Accuracy 0.079%\n",
      "Epoch 3, Batch 740, LR 0.641462 Loss 19.062001, Accuracy 0.080%\n",
      "Epoch 3, Batch 741, LR 0.641707 Loss 19.061888, Accuracy 0.080%\n",
      "Epoch 3, Batch 742, LR 0.641952 Loss 19.061744, Accuracy 0.081%\n",
      "Epoch 3, Batch 743, LR 0.642197 Loss 19.061674, Accuracy 0.082%\n",
      "Epoch 3, Batch 744, LR 0.642442 Loss 19.061501, Accuracy 0.082%\n",
      "Epoch 3, Batch 745, LR 0.642687 Loss 19.061465, Accuracy 0.082%\n",
      "Epoch 3, Batch 746, LR 0.642932 Loss 19.061397, Accuracy 0.082%\n",
      "Epoch 3, Batch 747, LR 0.643178 Loss 19.061467, Accuracy 0.082%\n",
      "Epoch 3, Batch 748, LR 0.643423 Loss 19.061712, Accuracy 0.081%\n",
      "Epoch 3, Batch 749, LR 0.643668 Loss 19.061721, Accuracy 0.081%\n",
      "Epoch 3, Batch 750, LR 0.643914 Loss 19.061752, Accuracy 0.081%\n",
      "Epoch 3, Batch 751, LR 0.644159 Loss 19.061862, Accuracy 0.081%\n",
      "Epoch 3, Batch 752, LR 0.644405 Loss 19.062143, Accuracy 0.081%\n",
      "Epoch 3, Batch 753, LR 0.644650 Loss 19.062252, Accuracy 0.081%\n",
      "Epoch 3, Batch 754, LR 0.644896 Loss 19.062163, Accuracy 0.081%\n",
      "Epoch 3, Batch 755, LR 0.645141 Loss 19.062268, Accuracy 0.081%\n",
      "Epoch 3, Batch 756, LR 0.645387 Loss 19.062479, Accuracy 0.081%\n",
      "Epoch 3, Batch 757, LR 0.645633 Loss 19.062486, Accuracy 0.080%\n",
      "Epoch 3, Batch 758, LR 0.645878 Loss 19.062610, Accuracy 0.080%\n",
      "Epoch 3, Batch 759, LR 0.646124 Loss 19.062628, Accuracy 0.080%\n",
      "Epoch 3, Batch 760, LR 0.646370 Loss 19.062696, Accuracy 0.080%\n",
      "Epoch 3, Batch 761, LR 0.646616 Loss 19.062562, Accuracy 0.080%\n",
      "Epoch 3, Batch 762, LR 0.646862 Loss 19.062846, Accuracy 0.080%\n",
      "Epoch 3, Batch 763, LR 0.647108 Loss 19.062700, Accuracy 0.081%\n",
      "Epoch 3, Batch 764, LR 0.647354 Loss 19.062662, Accuracy 0.081%\n",
      "Epoch 3, Batch 765, LR 0.647600 Loss 19.062834, Accuracy 0.081%\n",
      "Epoch 3, Batch 766, LR 0.647846 Loss 19.062786, Accuracy 0.082%\n",
      "Epoch 3, Batch 767, LR 0.648092 Loss 19.063165, Accuracy 0.081%\n",
      "Epoch 3, Batch 768, LR 0.648338 Loss 19.062977, Accuracy 0.081%\n",
      "Epoch 3, Batch 769, LR 0.648584 Loss 19.062938, Accuracy 0.081%\n",
      "Epoch 3, Batch 770, LR 0.648830 Loss 19.063053, Accuracy 0.081%\n",
      "Epoch 3, Batch 771, LR 0.649077 Loss 19.063005, Accuracy 0.081%\n",
      "Epoch 3, Batch 772, LR 0.649323 Loss 19.062958, Accuracy 0.082%\n",
      "Epoch 3, Batch 773, LR 0.649569 Loss 19.063050, Accuracy 0.082%\n",
      "Epoch 3, Batch 774, LR 0.649816 Loss 19.062942, Accuracy 0.082%\n",
      "Epoch 3, Batch 775, LR 0.650062 Loss 19.063192, Accuracy 0.083%\n",
      "Epoch 3, Batch 776, LR 0.650309 Loss 19.063435, Accuracy 0.083%\n",
      "Epoch 3, Batch 777, LR 0.650555 Loss 19.063457, Accuracy 0.082%\n",
      "Epoch 3, Batch 778, LR 0.650802 Loss 19.063770, Accuracy 0.082%\n",
      "Epoch 3, Batch 779, LR 0.651048 Loss 19.063706, Accuracy 0.082%\n",
      "Epoch 3, Batch 780, LR 0.651295 Loss 19.063712, Accuracy 0.082%\n",
      "Epoch 3, Batch 781, LR 0.651542 Loss 19.063549, Accuracy 0.082%\n",
      "Epoch 3, Batch 782, LR 0.651788 Loss 19.063452, Accuracy 0.083%\n",
      "Epoch 3, Batch 783, LR 0.652035 Loss 19.063588, Accuracy 0.083%\n",
      "Epoch 3, Batch 784, LR 0.652282 Loss 19.063590, Accuracy 0.083%\n",
      "Epoch 3, Batch 785, LR 0.652529 Loss 19.063933, Accuracy 0.083%\n",
      "Epoch 3, Batch 786, LR 0.652776 Loss 19.064210, Accuracy 0.082%\n",
      "Epoch 3, Batch 787, LR 0.653023 Loss 19.064451, Accuracy 0.082%\n",
      "Epoch 3, Batch 788, LR 0.653270 Loss 19.064245, Accuracy 0.082%\n",
      "Epoch 3, Batch 789, LR 0.653517 Loss 19.064260, Accuracy 0.082%\n",
      "Epoch 3, Batch 790, LR 0.653764 Loss 19.064246, Accuracy 0.082%\n",
      "Epoch 3, Batch 791, LR 0.654011 Loss 19.064201, Accuracy 0.082%\n",
      "Epoch 3, Batch 792, LR 0.654258 Loss 19.064378, Accuracy 0.082%\n",
      "Epoch 3, Batch 793, LR 0.654505 Loss 19.064125, Accuracy 0.082%\n",
      "Epoch 3, Batch 794, LR 0.654752 Loss 19.064153, Accuracy 0.082%\n",
      "Epoch 3, Batch 795, LR 0.655000 Loss 19.064179, Accuracy 0.082%\n",
      "Epoch 3, Batch 796, LR 0.655247 Loss 19.064077, Accuracy 0.081%\n",
      "Epoch 3, Batch 797, LR 0.655494 Loss 19.063808, Accuracy 0.081%\n",
      "Epoch 3, Batch 798, LR 0.655742 Loss 19.063705, Accuracy 0.082%\n",
      "Epoch 3, Batch 799, LR 0.655989 Loss 19.063495, Accuracy 0.082%\n",
      "Epoch 3, Batch 800, LR 0.656237 Loss 19.063464, Accuracy 0.082%\n",
      "Epoch 3, Batch 801, LR 0.656484 Loss 19.063569, Accuracy 0.082%\n",
      "Epoch 3, Batch 802, LR 0.656732 Loss 19.063573, Accuracy 0.082%\n",
      "Epoch 3, Batch 803, LR 0.656979 Loss 19.063748, Accuracy 0.082%\n",
      "Epoch 3, Batch 804, LR 0.657227 Loss 19.063502, Accuracy 0.082%\n",
      "Epoch 3, Batch 805, LR 0.657475 Loss 19.063561, Accuracy 0.082%\n",
      "Epoch 3, Batch 806, LR 0.657722 Loss 19.063349, Accuracy 0.081%\n",
      "Epoch 3, Batch 807, LR 0.657970 Loss 19.063072, Accuracy 0.081%\n",
      "Epoch 3, Batch 808, LR 0.658218 Loss 19.063175, Accuracy 0.081%\n",
      "Epoch 3, Batch 809, LR 0.658466 Loss 19.063013, Accuracy 0.081%\n",
      "Epoch 3, Batch 810, LR 0.658713 Loss 19.063078, Accuracy 0.082%\n",
      "Epoch 3, Batch 811, LR 0.658961 Loss 19.062978, Accuracy 0.082%\n",
      "Epoch 3, Batch 812, LR 0.659209 Loss 19.063017, Accuracy 0.082%\n",
      "Epoch 3, Batch 813, LR 0.659457 Loss 19.062903, Accuracy 0.082%\n",
      "Epoch 3, Batch 814, LR 0.659705 Loss 19.062912, Accuracy 0.082%\n",
      "Epoch 3, Batch 815, LR 0.659953 Loss 19.062863, Accuracy 0.081%\n",
      "Epoch 3, Batch 816, LR 0.660202 Loss 19.062816, Accuracy 0.081%\n",
      "Epoch 3, Batch 817, LR 0.660450 Loss 19.062589, Accuracy 0.081%\n",
      "Epoch 3, Batch 818, LR 0.660698 Loss 19.062466, Accuracy 0.081%\n",
      "Epoch 3, Batch 819, LR 0.660946 Loss 19.062274, Accuracy 0.082%\n",
      "Epoch 3, Batch 820, LR 0.661195 Loss 19.062314, Accuracy 0.082%\n",
      "Epoch 3, Batch 821, LR 0.661443 Loss 19.062280, Accuracy 0.082%\n",
      "Epoch 3, Batch 822, LR 0.661691 Loss 19.062466, Accuracy 0.082%\n",
      "Epoch 3, Batch 823, LR 0.661940 Loss 19.062407, Accuracy 0.082%\n",
      "Epoch 3, Batch 824, LR 0.662188 Loss 19.062440, Accuracy 0.082%\n",
      "Epoch 3, Batch 825, LR 0.662437 Loss 19.062251, Accuracy 0.082%\n",
      "Epoch 3, Batch 826, LR 0.662685 Loss 19.062412, Accuracy 0.082%\n",
      "Epoch 3, Batch 827, LR 0.662934 Loss 19.062372, Accuracy 0.082%\n",
      "Epoch 3, Batch 828, LR 0.663182 Loss 19.062307, Accuracy 0.082%\n",
      "Epoch 3, Batch 829, LR 0.663431 Loss 19.062441, Accuracy 0.082%\n",
      "Epoch 3, Batch 830, LR 0.663680 Loss 19.062597, Accuracy 0.082%\n",
      "Epoch 3, Batch 831, LR 0.663928 Loss 19.062758, Accuracy 0.082%\n",
      "Epoch 3, Batch 832, LR 0.664177 Loss 19.062994, Accuracy 0.082%\n",
      "Epoch 3, Batch 833, LR 0.664426 Loss 19.063031, Accuracy 0.082%\n",
      "Epoch 3, Batch 834, LR 0.664675 Loss 19.063256, Accuracy 0.081%\n",
      "Epoch 3, Batch 835, LR 0.664924 Loss 19.063132, Accuracy 0.081%\n",
      "Epoch 3, Batch 836, LR 0.665173 Loss 19.063219, Accuracy 0.081%\n",
      "Epoch 3, Batch 837, LR 0.665422 Loss 19.063182, Accuracy 0.081%\n",
      "Epoch 3, Batch 838, LR 0.665671 Loss 19.063045, Accuracy 0.081%\n",
      "Epoch 3, Batch 839, LR 0.665920 Loss 19.062914, Accuracy 0.082%\n",
      "Epoch 3, Batch 840, LR 0.666169 Loss 19.062969, Accuracy 0.082%\n",
      "Epoch 3, Batch 841, LR 0.666418 Loss 19.062955, Accuracy 0.083%\n",
      "Epoch 3, Batch 842, LR 0.666667 Loss 19.063048, Accuracy 0.083%\n",
      "Epoch 3, Batch 843, LR 0.666916 Loss 19.062981, Accuracy 0.082%\n",
      "Epoch 3, Batch 844, LR 0.667166 Loss 19.063034, Accuracy 0.082%\n",
      "Epoch 3, Batch 845, LR 0.667415 Loss 19.063228, Accuracy 0.082%\n",
      "Epoch 3, Batch 846, LR 0.667664 Loss 19.063470, Accuracy 0.082%\n",
      "Epoch 3, Batch 847, LR 0.667914 Loss 19.063607, Accuracy 0.082%\n",
      "Epoch 3, Batch 848, LR 0.668163 Loss 19.063730, Accuracy 0.082%\n",
      "Epoch 3, Batch 849, LR 0.668412 Loss 19.063851, Accuracy 0.082%\n",
      "Epoch 3, Batch 850, LR 0.668662 Loss 19.063789, Accuracy 0.082%\n",
      "Epoch 3, Batch 851, LR 0.668911 Loss 19.063951, Accuracy 0.082%\n",
      "Epoch 3, Batch 852, LR 0.669161 Loss 19.064009, Accuracy 0.082%\n",
      "Epoch 3, Batch 853, LR 0.669411 Loss 19.063924, Accuracy 0.082%\n",
      "Epoch 3, Batch 854, LR 0.669660 Loss 19.064070, Accuracy 0.082%\n",
      "Epoch 3, Batch 855, LR 0.669910 Loss 19.064043, Accuracy 0.082%\n",
      "Epoch 3, Batch 856, LR 0.670160 Loss 19.064073, Accuracy 0.082%\n",
      "Epoch 3, Batch 857, LR 0.670410 Loss 19.064062, Accuracy 0.082%\n",
      "Epoch 3, Batch 858, LR 0.670659 Loss 19.064121, Accuracy 0.082%\n",
      "Epoch 3, Batch 859, LR 0.670909 Loss 19.063932, Accuracy 0.082%\n",
      "Epoch 3, Batch 860, LR 0.671159 Loss 19.063666, Accuracy 0.083%\n",
      "Epoch 3, Batch 861, LR 0.671409 Loss 19.063619, Accuracy 0.083%\n",
      "Epoch 3, Batch 862, LR 0.671659 Loss 19.063544, Accuracy 0.082%\n",
      "Epoch 3, Batch 863, LR 0.671909 Loss 19.063529, Accuracy 0.082%\n",
      "Epoch 3, Batch 864, LR 0.672159 Loss 19.063334, Accuracy 0.082%\n",
      "Epoch 3, Batch 865, LR 0.672409 Loss 19.063424, Accuracy 0.082%\n",
      "Epoch 3, Batch 866, LR 0.672659 Loss 19.063287, Accuracy 0.082%\n",
      "Epoch 3, Batch 867, LR 0.672910 Loss 19.063308, Accuracy 0.083%\n",
      "Epoch 3, Batch 868, LR 0.673160 Loss 19.063570, Accuracy 0.083%\n",
      "Epoch 3, Batch 869, LR 0.673410 Loss 19.063554, Accuracy 0.083%\n",
      "Epoch 3, Batch 870, LR 0.673660 Loss 19.063429, Accuracy 0.083%\n",
      "Epoch 3, Batch 871, LR 0.673911 Loss 19.063448, Accuracy 0.083%\n",
      "Epoch 3, Batch 872, LR 0.674161 Loss 19.063240, Accuracy 0.082%\n",
      "Epoch 3, Batch 873, LR 0.674411 Loss 19.063121, Accuracy 0.083%\n",
      "Epoch 3, Batch 874, LR 0.674662 Loss 19.063184, Accuracy 0.083%\n",
      "Epoch 3, Batch 875, LR 0.674912 Loss 19.063156, Accuracy 0.083%\n",
      "Epoch 3, Batch 876, LR 0.675163 Loss 19.062991, Accuracy 0.083%\n",
      "Epoch 3, Batch 877, LR 0.675414 Loss 19.063073, Accuracy 0.083%\n",
      "Epoch 3, Batch 878, LR 0.675664 Loss 19.063146, Accuracy 0.083%\n",
      "Epoch 3, Batch 879, LR 0.675915 Loss 19.063189, Accuracy 0.083%\n",
      "Epoch 3, Batch 880, LR 0.676166 Loss 19.062989, Accuracy 0.083%\n",
      "Epoch 3, Batch 881, LR 0.676416 Loss 19.062967, Accuracy 0.082%\n",
      "Epoch 3, Batch 882, LR 0.676667 Loss 19.063064, Accuracy 0.082%\n",
      "Epoch 3, Batch 883, LR 0.676918 Loss 19.062866, Accuracy 0.082%\n",
      "Epoch 3, Batch 884, LR 0.677169 Loss 19.062819, Accuracy 0.083%\n",
      "Epoch 3, Batch 885, LR 0.677420 Loss 19.062834, Accuracy 0.083%\n",
      "Epoch 3, Batch 886, LR 0.677671 Loss 19.062928, Accuracy 0.083%\n",
      "Epoch 3, Batch 887, LR 0.677922 Loss 19.062784, Accuracy 0.083%\n",
      "Epoch 3, Batch 888, LR 0.678173 Loss 19.062813, Accuracy 0.083%\n",
      "Epoch 3, Batch 889, LR 0.678424 Loss 19.062839, Accuracy 0.083%\n",
      "Epoch 3, Batch 890, LR 0.678675 Loss 19.062865, Accuracy 0.083%\n",
      "Epoch 3, Batch 891, LR 0.678926 Loss 19.062818, Accuracy 0.083%\n",
      "Epoch 3, Batch 892, LR 0.679177 Loss 19.062704, Accuracy 0.083%\n",
      "Epoch 3, Batch 893, LR 0.679428 Loss 19.062793, Accuracy 0.083%\n",
      "Epoch 3, Batch 894, LR 0.679680 Loss 19.062873, Accuracy 0.083%\n",
      "Epoch 3, Batch 895, LR 0.679931 Loss 19.062853, Accuracy 0.083%\n",
      "Epoch 3, Batch 896, LR 0.680182 Loss 19.063042, Accuracy 0.083%\n",
      "Epoch 3, Batch 897, LR 0.680434 Loss 19.063052, Accuracy 0.083%\n",
      "Epoch 3, Batch 898, LR 0.680685 Loss 19.063034, Accuracy 0.083%\n",
      "Epoch 3, Batch 899, LR 0.680936 Loss 19.063214, Accuracy 0.083%\n",
      "Epoch 3, Batch 900, LR 0.681188 Loss 19.063289, Accuracy 0.082%\n",
      "Epoch 3, Batch 901, LR 0.681439 Loss 19.063499, Accuracy 0.082%\n",
      "Epoch 3, Batch 902, LR 0.681691 Loss 19.063660, Accuracy 0.082%\n",
      "Epoch 3, Batch 903, LR 0.681943 Loss 19.063746, Accuracy 0.082%\n",
      "Epoch 3, Batch 904, LR 0.682194 Loss 19.063661, Accuracy 0.083%\n",
      "Epoch 3, Batch 905, LR 0.682446 Loss 19.063590, Accuracy 0.083%\n",
      "Epoch 3, Batch 906, LR 0.682698 Loss 19.063656, Accuracy 0.083%\n",
      "Epoch 3, Batch 907, LR 0.682949 Loss 19.063732, Accuracy 0.083%\n",
      "Epoch 3, Batch 908, LR 0.683201 Loss 19.063671, Accuracy 0.083%\n",
      "Epoch 3, Batch 909, LR 0.683453 Loss 19.063538, Accuracy 0.083%\n",
      "Epoch 3, Batch 910, LR 0.683705 Loss 19.063610, Accuracy 0.082%\n",
      "Epoch 3, Batch 911, LR 0.683957 Loss 19.063414, Accuracy 0.082%\n",
      "Epoch 3, Batch 912, LR 0.684209 Loss 19.063604, Accuracy 0.082%\n",
      "Epoch 3, Batch 913, LR 0.684461 Loss 19.063518, Accuracy 0.082%\n",
      "Epoch 3, Batch 914, LR 0.684713 Loss 19.063452, Accuracy 0.082%\n",
      "Epoch 3, Batch 915, LR 0.684965 Loss 19.063721, Accuracy 0.082%\n",
      "Epoch 3, Batch 916, LR 0.685217 Loss 19.063800, Accuracy 0.082%\n",
      "Epoch 3, Batch 917, LR 0.685469 Loss 19.063863, Accuracy 0.082%\n",
      "Epoch 3, Batch 918, LR 0.685722 Loss 19.063741, Accuracy 0.082%\n",
      "Epoch 3, Batch 919, LR 0.685974 Loss 19.063735, Accuracy 0.082%\n",
      "Epoch 3, Batch 920, LR 0.686226 Loss 19.063757, Accuracy 0.082%\n",
      "Epoch 3, Batch 921, LR 0.686478 Loss 19.063569, Accuracy 0.081%\n",
      "Epoch 3, Batch 922, LR 0.686731 Loss 19.063525, Accuracy 0.081%\n",
      "Epoch 3, Batch 923, LR 0.686983 Loss 19.063582, Accuracy 0.081%\n",
      "Epoch 3, Batch 924, LR 0.687236 Loss 19.063705, Accuracy 0.081%\n",
      "Epoch 3, Batch 925, LR 0.687488 Loss 19.063661, Accuracy 0.081%\n",
      "Epoch 3, Batch 926, LR 0.687741 Loss 19.063688, Accuracy 0.081%\n",
      "Epoch 3, Batch 927, LR 0.687993 Loss 19.063544, Accuracy 0.081%\n",
      "Epoch 3, Batch 928, LR 0.688246 Loss 19.063641, Accuracy 0.081%\n",
      "Epoch 3, Batch 929, LR 0.688498 Loss 19.063369, Accuracy 0.082%\n",
      "Epoch 3, Batch 930, LR 0.688751 Loss 19.063397, Accuracy 0.081%\n",
      "Epoch 3, Batch 931, LR 0.689004 Loss 19.063348, Accuracy 0.081%\n",
      "Epoch 3, Batch 932, LR 0.689256 Loss 19.063557, Accuracy 0.081%\n",
      "Epoch 3, Batch 933, LR 0.689509 Loss 19.063550, Accuracy 0.081%\n",
      "Epoch 3, Batch 934, LR 0.689762 Loss 19.063567, Accuracy 0.081%\n",
      "Epoch 3, Batch 935, LR 0.690015 Loss 19.063623, Accuracy 0.082%\n",
      "Epoch 3, Batch 936, LR 0.690268 Loss 19.063666, Accuracy 0.082%\n",
      "Epoch 3, Batch 937, LR 0.690521 Loss 19.063685, Accuracy 0.082%\n",
      "Epoch 3, Batch 938, LR 0.690774 Loss 19.063779, Accuracy 0.082%\n",
      "Epoch 3, Batch 939, LR 0.691027 Loss 19.063782, Accuracy 0.082%\n",
      "Epoch 3, Batch 940, LR 0.691280 Loss 19.063721, Accuracy 0.083%\n",
      "Epoch 3, Batch 941, LR 0.691533 Loss 19.063714, Accuracy 0.083%\n",
      "Epoch 3, Batch 942, LR 0.691786 Loss 19.063790, Accuracy 0.083%\n",
      "Epoch 3, Batch 943, LR 0.692039 Loss 19.063920, Accuracy 0.083%\n",
      "Epoch 3, Batch 944, LR 0.692293 Loss 19.064008, Accuracy 0.083%\n",
      "Epoch 3, Batch 945, LR 0.692546 Loss 19.064066, Accuracy 0.083%\n",
      "Epoch 3, Batch 946, LR 0.692799 Loss 19.064219, Accuracy 0.083%\n",
      "Epoch 3, Batch 947, LR 0.693052 Loss 19.064161, Accuracy 0.082%\n",
      "Epoch 3, Batch 948, LR 0.693306 Loss 19.064291, Accuracy 0.082%\n",
      "Epoch 3, Batch 949, LR 0.693559 Loss 19.064140, Accuracy 0.082%\n",
      "Epoch 3, Batch 950, LR 0.693813 Loss 19.063971, Accuracy 0.083%\n",
      "Epoch 3, Batch 951, LR 0.694066 Loss 19.063935, Accuracy 0.083%\n",
      "Epoch 3, Batch 952, LR 0.694320 Loss 19.063794, Accuracy 0.083%\n",
      "Epoch 3, Batch 953, LR 0.694573 Loss 19.063817, Accuracy 0.083%\n",
      "Epoch 3, Batch 954, LR 0.694827 Loss 19.063819, Accuracy 0.083%\n",
      "Epoch 3, Batch 955, LR 0.695080 Loss 19.063711, Accuracy 0.083%\n",
      "Epoch 3, Batch 956, LR 0.695334 Loss 19.063782, Accuracy 0.083%\n",
      "Epoch 3, Batch 957, LR 0.695588 Loss 19.063847, Accuracy 0.082%\n",
      "Epoch 3, Batch 958, LR 0.695842 Loss 19.063686, Accuracy 0.082%\n",
      "Epoch 3, Batch 959, LR 0.696095 Loss 19.063799, Accuracy 0.083%\n",
      "Epoch 3, Batch 960, LR 0.696349 Loss 19.063879, Accuracy 0.083%\n",
      "Epoch 3, Batch 961, LR 0.696603 Loss 19.063753, Accuracy 0.083%\n",
      "Epoch 3, Batch 962, LR 0.696857 Loss 19.063823, Accuracy 0.083%\n",
      "Epoch 3, Batch 963, LR 0.697111 Loss 19.063874, Accuracy 0.083%\n",
      "Epoch 3, Batch 964, LR 0.697365 Loss 19.063862, Accuracy 0.083%\n",
      "Epoch 3, Batch 965, LR 0.697619 Loss 19.063886, Accuracy 0.083%\n",
      "Epoch 3, Batch 966, LR 0.697873 Loss 19.063738, Accuracy 0.082%\n",
      "Epoch 3, Batch 967, LR 0.698127 Loss 19.063710, Accuracy 0.082%\n",
      "Epoch 3, Batch 968, LR 0.698381 Loss 19.063802, Accuracy 0.082%\n",
      "Epoch 3, Batch 969, LR 0.698635 Loss 19.063801, Accuracy 0.083%\n",
      "Epoch 3, Batch 970, LR 0.698890 Loss 19.063629, Accuracy 0.083%\n",
      "Epoch 3, Batch 971, LR 0.699144 Loss 19.063514, Accuracy 0.083%\n",
      "Epoch 3, Batch 972, LR 0.699398 Loss 19.063508, Accuracy 0.083%\n",
      "Epoch 3, Batch 973, LR 0.699653 Loss 19.063409, Accuracy 0.083%\n",
      "Epoch 3, Batch 974, LR 0.699907 Loss 19.063523, Accuracy 0.083%\n",
      "Epoch 3, Batch 975, LR 0.700161 Loss 19.063285, Accuracy 0.083%\n",
      "Epoch 3, Batch 976, LR 0.700416 Loss 19.063496, Accuracy 0.083%\n",
      "Epoch 3, Batch 977, LR 0.700670 Loss 19.063501, Accuracy 0.083%\n",
      "Epoch 3, Batch 978, LR 0.700925 Loss 19.063579, Accuracy 0.083%\n",
      "Epoch 3, Batch 979, LR 0.701179 Loss 19.063501, Accuracy 0.083%\n",
      "Epoch 3, Batch 980, LR 0.701434 Loss 19.063406, Accuracy 0.083%\n",
      "Epoch 3, Batch 981, LR 0.701689 Loss 19.063498, Accuracy 0.083%\n",
      "Epoch 3, Batch 982, LR 0.701943 Loss 19.063465, Accuracy 0.083%\n",
      "Epoch 3, Batch 983, LR 0.702198 Loss 19.063386, Accuracy 0.083%\n",
      "Epoch 3, Batch 984, LR 0.702453 Loss 19.063566, Accuracy 0.083%\n",
      "Epoch 3, Batch 985, LR 0.702708 Loss 19.063667, Accuracy 0.082%\n",
      "Epoch 3, Batch 986, LR 0.702962 Loss 19.063650, Accuracy 0.083%\n",
      "Epoch 3, Batch 987, LR 0.703217 Loss 19.063760, Accuracy 0.083%\n",
      "Epoch 3, Batch 988, LR 0.703472 Loss 19.063722, Accuracy 0.083%\n",
      "Epoch 3, Batch 989, LR 0.703727 Loss 19.063665, Accuracy 0.084%\n",
      "Epoch 3, Batch 990, LR 0.703982 Loss 19.063758, Accuracy 0.084%\n",
      "Epoch 3, Batch 991, LR 0.704237 Loss 19.063758, Accuracy 0.084%\n",
      "Epoch 3, Batch 992, LR 0.704492 Loss 19.063634, Accuracy 0.083%\n",
      "Epoch 3, Batch 993, LR 0.704747 Loss 19.063769, Accuracy 0.084%\n",
      "Epoch 3, Batch 994, LR 0.705002 Loss 19.063827, Accuracy 0.084%\n",
      "Epoch 3, Batch 995, LR 0.705258 Loss 19.063862, Accuracy 0.084%\n",
      "Epoch 3, Batch 996, LR 0.705513 Loss 19.063632, Accuracy 0.084%\n",
      "Epoch 3, Batch 997, LR 0.705768 Loss 19.063525, Accuracy 0.084%\n",
      "Epoch 3, Batch 998, LR 0.706023 Loss 19.063619, Accuracy 0.084%\n",
      "Epoch 3, Batch 999, LR 0.706279 Loss 19.063752, Accuracy 0.084%\n",
      "Epoch 3, Batch 1000, LR 0.706534 Loss 19.063769, Accuracy 0.084%\n",
      "Epoch 3, Batch 1001, LR 0.706789 Loss 19.064045, Accuracy 0.084%\n",
      "Epoch 3, Batch 1002, LR 0.707045 Loss 19.063990, Accuracy 0.083%\n",
      "Epoch 3, Batch 1003, LR 0.707300 Loss 19.064090, Accuracy 0.083%\n",
      "Epoch 3, Batch 1004, LR 0.707556 Loss 19.064076, Accuracy 0.083%\n",
      "Epoch 3, Batch 1005, LR 0.707811 Loss 19.064104, Accuracy 0.083%\n",
      "Epoch 3, Batch 1006, LR 0.708067 Loss 19.064285, Accuracy 0.083%\n",
      "Epoch 3, Batch 1007, LR 0.708323 Loss 19.064202, Accuracy 0.083%\n",
      "Epoch 3, Batch 1008, LR 0.708578 Loss 19.064070, Accuracy 0.083%\n",
      "Epoch 3, Batch 1009, LR 0.708834 Loss 19.063944, Accuracy 0.083%\n",
      "Epoch 3, Batch 1010, LR 0.709090 Loss 19.063914, Accuracy 0.083%\n",
      "Epoch 3, Batch 1011, LR 0.709346 Loss 19.063935, Accuracy 0.083%\n",
      "Epoch 3, Batch 1012, LR 0.709601 Loss 19.063964, Accuracy 0.083%\n",
      "Epoch 3, Batch 1013, LR 0.709857 Loss 19.063969, Accuracy 0.083%\n",
      "Epoch 3, Batch 1014, LR 0.710113 Loss 19.064139, Accuracy 0.082%\n",
      "Epoch 3, Batch 1015, LR 0.710369 Loss 19.063937, Accuracy 0.082%\n",
      "Epoch 3, Batch 1016, LR 0.710625 Loss 19.064174, Accuracy 0.082%\n",
      "Epoch 3, Batch 1017, LR 0.710881 Loss 19.064186, Accuracy 0.082%\n",
      "Epoch 3, Batch 1018, LR 0.711137 Loss 19.064402, Accuracy 0.082%\n",
      "Epoch 3, Batch 1019, LR 0.711393 Loss 19.064495, Accuracy 0.082%\n",
      "Epoch 3, Batch 1020, LR 0.711649 Loss 19.064373, Accuracy 0.082%\n",
      "Epoch 3, Batch 1021, LR 0.711905 Loss 19.064615, Accuracy 0.082%\n",
      "Epoch 3, Batch 1022, LR 0.712162 Loss 19.064476, Accuracy 0.082%\n",
      "Epoch 3, Batch 1023, LR 0.712418 Loss 19.064581, Accuracy 0.082%\n",
      "Epoch 3, Batch 1024, LR 0.712674 Loss 19.064482, Accuracy 0.082%\n",
      "Epoch 3, Batch 1025, LR 0.712930 Loss 19.064620, Accuracy 0.082%\n",
      "Epoch 3, Batch 1026, LR 0.713187 Loss 19.064539, Accuracy 0.082%\n",
      "Epoch 3, Batch 1027, LR 0.713443 Loss 19.064811, Accuracy 0.082%\n",
      "Epoch 3, Batch 1028, LR 0.713700 Loss 19.064907, Accuracy 0.082%\n",
      "Epoch 3, Batch 1029, LR 0.713956 Loss 19.064996, Accuracy 0.082%\n",
      "Epoch 3, Batch 1030, LR 0.714212 Loss 19.064899, Accuracy 0.082%\n",
      "Epoch 3, Batch 1031, LR 0.714469 Loss 19.064803, Accuracy 0.082%\n",
      "Epoch 3, Batch 1032, LR 0.714726 Loss 19.064962, Accuracy 0.082%\n",
      "Epoch 3, Batch 1033, LR 0.714982 Loss 19.065014, Accuracy 0.082%\n",
      "Epoch 3, Batch 1034, LR 0.715239 Loss 19.065063, Accuracy 0.082%\n",
      "Epoch 3, Batch 1035, LR 0.715495 Loss 19.065176, Accuracy 0.082%\n",
      "Epoch 3, Batch 1036, LR 0.715752 Loss 19.065191, Accuracy 0.081%\n",
      "Epoch 3, Batch 1037, LR 0.716009 Loss 19.065031, Accuracy 0.082%\n",
      "Epoch 3, Batch 1038, LR 0.716266 Loss 19.065059, Accuracy 0.082%\n",
      "Epoch 3, Batch 1039, LR 0.716523 Loss 19.065037, Accuracy 0.082%\n",
      "Epoch 3, Batch 1040, LR 0.716779 Loss 19.064851, Accuracy 0.083%\n",
      "Epoch 3, Batch 1041, LR 0.717036 Loss 19.064677, Accuracy 0.083%\n",
      "Epoch 3, Batch 1042, LR 0.717293 Loss 19.064602, Accuracy 0.083%\n",
      "Epoch 3, Batch 1043, LR 0.717550 Loss 19.064465, Accuracy 0.083%\n",
      "Epoch 3, Batch 1044, LR 0.717807 Loss 19.064215, Accuracy 0.083%\n",
      "Epoch 3, Batch 1045, LR 0.718064 Loss 19.064322, Accuracy 0.083%\n",
      "Epoch 3, Batch 1046, LR 0.718321 Loss 19.064075, Accuracy 0.083%\n",
      "Epoch 3, Batch 1047, LR 0.718579 Loss 19.064028, Accuracy 0.084%\n",
      "Epoch 3, Loss (train set) 19.064028, Accuracy (train set) 0.084%\n",
      "Epoch 4, Batch 1, LR 0.718836 Loss 19.036133, Accuracy 0.000%\n",
      "Epoch 4, Batch 2, LR 0.719093 Loss 19.059203, Accuracy 0.391%\n",
      "Epoch 4, Batch 3, LR 0.719350 Loss 19.119517, Accuracy 0.260%\n",
      "Epoch 4, Batch 4, LR 0.719607 Loss 19.140079, Accuracy 0.195%\n",
      "Epoch 4, Batch 5, LR 0.719865 Loss 19.126767, Accuracy 0.156%\n",
      "Epoch 4, Batch 6, LR 0.720122 Loss 19.129163, Accuracy 0.130%\n",
      "Epoch 4, Batch 7, LR 0.720379 Loss 19.149484, Accuracy 0.112%\n",
      "Epoch 4, Batch 8, LR 0.720637 Loss 19.137022, Accuracy 0.098%\n",
      "Epoch 4, Batch 9, LR 0.720894 Loss 19.134502, Accuracy 0.087%\n",
      "Epoch 4, Batch 10, LR 0.721152 Loss 19.135515, Accuracy 0.234%\n",
      "Epoch 4, Batch 11, LR 0.721409 Loss 19.111156, Accuracy 0.284%\n",
      "Epoch 4, Batch 12, LR 0.721667 Loss 19.109862, Accuracy 0.260%\n",
      "Epoch 4, Batch 13, LR 0.721925 Loss 19.110452, Accuracy 0.240%\n",
      "Epoch 4, Batch 14, LR 0.722182 Loss 19.114747, Accuracy 0.279%\n",
      "Epoch 4, Batch 15, LR 0.722440 Loss 19.112345, Accuracy 0.260%\n",
      "Epoch 4, Batch 16, LR 0.722698 Loss 19.103071, Accuracy 0.244%\n",
      "Epoch 4, Batch 17, LR 0.722955 Loss 19.103607, Accuracy 0.276%\n",
      "Epoch 4, Batch 18, LR 0.723213 Loss 19.103026, Accuracy 0.260%\n",
      "Epoch 4, Batch 19, LR 0.723471 Loss 19.107592, Accuracy 0.247%\n",
      "Epoch 4, Batch 20, LR 0.723729 Loss 19.105938, Accuracy 0.234%\n",
      "Epoch 4, Batch 21, LR 0.723987 Loss 19.111102, Accuracy 0.223%\n",
      "Epoch 4, Batch 22, LR 0.724245 Loss 19.100876, Accuracy 0.249%\n",
      "Epoch 4, Batch 23, LR 0.724503 Loss 19.093961, Accuracy 0.272%\n",
      "Epoch 4, Batch 24, LR 0.724761 Loss 19.098222, Accuracy 0.260%\n",
      "Epoch 4, Batch 25, LR 0.725019 Loss 19.097592, Accuracy 0.250%\n",
      "Epoch 4, Batch 26, LR 0.725277 Loss 19.094433, Accuracy 0.240%\n",
      "Epoch 4, Batch 27, LR 0.725535 Loss 19.091328, Accuracy 0.231%\n",
      "Epoch 4, Batch 28, LR 0.725793 Loss 19.091481, Accuracy 0.223%\n",
      "Epoch 4, Batch 29, LR 0.726051 Loss 19.090332, Accuracy 0.216%\n",
      "Epoch 4, Batch 30, LR 0.726310 Loss 19.085008, Accuracy 0.208%\n",
      "Epoch 4, Batch 31, LR 0.726568 Loss 19.085258, Accuracy 0.202%\n",
      "Epoch 4, Batch 32, LR 0.726826 Loss 19.089149, Accuracy 0.195%\n",
      "Epoch 4, Batch 33, LR 0.727084 Loss 19.092958, Accuracy 0.189%\n",
      "Epoch 4, Batch 34, LR 0.727343 Loss 19.090902, Accuracy 0.207%\n",
      "Epoch 4, Batch 35, LR 0.727601 Loss 19.087369, Accuracy 0.201%\n",
      "Epoch 4, Batch 36, LR 0.727860 Loss 19.084848, Accuracy 0.195%\n",
      "Epoch 4, Batch 37, LR 0.728118 Loss 19.084588, Accuracy 0.190%\n",
      "Epoch 4, Batch 38, LR 0.728377 Loss 19.085494, Accuracy 0.185%\n",
      "Epoch 4, Batch 39, LR 0.728635 Loss 19.083728, Accuracy 0.180%\n",
      "Epoch 4, Batch 40, LR 0.728894 Loss 19.086480, Accuracy 0.176%\n",
      "Epoch 4, Batch 41, LR 0.729153 Loss 19.087420, Accuracy 0.171%\n",
      "Epoch 4, Batch 42, LR 0.729411 Loss 19.087173, Accuracy 0.186%\n",
      "Epoch 4, Batch 43, LR 0.729670 Loss 19.090302, Accuracy 0.182%\n",
      "Epoch 4, Batch 44, LR 0.729929 Loss 19.090192, Accuracy 0.178%\n",
      "Epoch 4, Batch 45, LR 0.730187 Loss 19.083669, Accuracy 0.174%\n",
      "Epoch 4, Batch 46, LR 0.730446 Loss 19.076297, Accuracy 0.170%\n",
      "Epoch 4, Batch 47, LR 0.730705 Loss 19.079379, Accuracy 0.166%\n",
      "Epoch 4, Batch 48, LR 0.730964 Loss 19.078489, Accuracy 0.163%\n",
      "Epoch 4, Batch 49, LR 0.731223 Loss 19.076067, Accuracy 0.159%\n",
      "Epoch 4, Batch 50, LR 0.731482 Loss 19.077591, Accuracy 0.156%\n",
      "Epoch 4, Batch 51, LR 0.731741 Loss 19.080658, Accuracy 0.153%\n",
      "Epoch 4, Batch 52, LR 0.732000 Loss 19.080662, Accuracy 0.150%\n",
      "Epoch 4, Batch 53, LR 0.732259 Loss 19.076455, Accuracy 0.147%\n",
      "Epoch 4, Batch 54, LR 0.732518 Loss 19.073469, Accuracy 0.145%\n",
      "Epoch 4, Batch 55, LR 0.732777 Loss 19.072920, Accuracy 0.142%\n",
      "Epoch 4, Batch 56, LR 0.733036 Loss 19.068884, Accuracy 0.140%\n",
      "Epoch 4, Batch 57, LR 0.733296 Loss 19.070725, Accuracy 0.137%\n",
      "Epoch 4, Batch 58, LR 0.733555 Loss 19.072403, Accuracy 0.135%\n",
      "Epoch 4, Batch 59, LR 0.733814 Loss 19.072978, Accuracy 0.132%\n",
      "Epoch 4, Batch 60, LR 0.734074 Loss 19.072167, Accuracy 0.130%\n",
      "Epoch 4, Batch 61, LR 0.734333 Loss 19.074133, Accuracy 0.128%\n",
      "Epoch 4, Batch 62, LR 0.734592 Loss 19.070341, Accuracy 0.126%\n",
      "Epoch 4, Batch 63, LR 0.734852 Loss 19.068296, Accuracy 0.136%\n",
      "Epoch 4, Batch 64, LR 0.735111 Loss 19.065776, Accuracy 0.134%\n",
      "Epoch 4, Batch 65, LR 0.735371 Loss 19.066611, Accuracy 0.132%\n",
      "Epoch 4, Batch 66, LR 0.735630 Loss 19.067730, Accuracy 0.130%\n",
      "Epoch 4, Batch 67, LR 0.735890 Loss 19.067590, Accuracy 0.128%\n",
      "Epoch 4, Batch 68, LR 0.736149 Loss 19.067832, Accuracy 0.126%\n",
      "Epoch 4, Batch 69, LR 0.736409 Loss 19.066894, Accuracy 0.125%\n",
      "Epoch 4, Batch 70, LR 0.736669 Loss 19.067933, Accuracy 0.123%\n",
      "Epoch 4, Batch 71, LR 0.736929 Loss 19.068478, Accuracy 0.121%\n",
      "Epoch 4, Batch 72, LR 0.737188 Loss 19.068116, Accuracy 0.119%\n",
      "Epoch 4, Batch 73, LR 0.737448 Loss 19.068509, Accuracy 0.118%\n",
      "Epoch 4, Batch 74, LR 0.737708 Loss 19.070646, Accuracy 0.127%\n",
      "Epoch 4, Batch 75, LR 0.737968 Loss 19.071129, Accuracy 0.125%\n",
      "Epoch 4, Batch 76, LR 0.738228 Loss 19.070085, Accuracy 0.123%\n",
      "Epoch 4, Batch 77, LR 0.738488 Loss 19.070943, Accuracy 0.122%\n",
      "Epoch 4, Batch 78, LR 0.738748 Loss 19.070321, Accuracy 0.120%\n",
      "Epoch 4, Batch 79, LR 0.739008 Loss 19.069641, Accuracy 0.119%\n",
      "Epoch 4, Batch 80, LR 0.739268 Loss 19.069686, Accuracy 0.117%\n",
      "Epoch 4, Batch 81, LR 0.739528 Loss 19.071043, Accuracy 0.116%\n",
      "Epoch 4, Batch 82, LR 0.739788 Loss 19.070242, Accuracy 0.114%\n",
      "Epoch 4, Batch 83, LR 0.740048 Loss 19.068947, Accuracy 0.113%\n",
      "Epoch 4, Batch 84, LR 0.740308 Loss 19.066580, Accuracy 0.112%\n",
      "Epoch 4, Batch 85, LR 0.740568 Loss 19.066634, Accuracy 0.110%\n",
      "Epoch 4, Batch 86, LR 0.740829 Loss 19.065245, Accuracy 0.109%\n",
      "Epoch 4, Batch 87, LR 0.741089 Loss 19.065074, Accuracy 0.108%\n",
      "Epoch 4, Batch 88, LR 0.741349 Loss 19.066565, Accuracy 0.107%\n",
      "Epoch 4, Batch 89, LR 0.741610 Loss 19.065196, Accuracy 0.114%\n",
      "Epoch 4, Batch 90, LR 0.741870 Loss 19.064378, Accuracy 0.113%\n",
      "Epoch 4, Batch 91, LR 0.742131 Loss 19.062778, Accuracy 0.112%\n",
      "Epoch 4, Batch 92, LR 0.742391 Loss 19.066349, Accuracy 0.110%\n",
      "Epoch 4, Batch 93, LR 0.742652 Loss 19.066092, Accuracy 0.109%\n",
      "Epoch 4, Batch 94, LR 0.742912 Loss 19.065440, Accuracy 0.108%\n",
      "Epoch 4, Batch 95, LR 0.743173 Loss 19.067240, Accuracy 0.115%\n",
      "Epoch 4, Batch 96, LR 0.743433 Loss 19.065875, Accuracy 0.114%\n",
      "Epoch 4, Batch 97, LR 0.743694 Loss 19.067738, Accuracy 0.113%\n",
      "Epoch 4, Batch 98, LR 0.743955 Loss 19.068551, Accuracy 0.112%\n",
      "Epoch 4, Batch 99, LR 0.744215 Loss 19.067321, Accuracy 0.110%\n",
      "Epoch 4, Batch 100, LR 0.744476 Loss 19.065785, Accuracy 0.109%\n",
      "Epoch 4, Batch 101, LR 0.744737 Loss 19.064594, Accuracy 0.116%\n",
      "Epoch 4, Batch 102, LR 0.744998 Loss 19.062676, Accuracy 0.115%\n",
      "Epoch 4, Batch 103, LR 0.745259 Loss 19.062354, Accuracy 0.114%\n",
      "Epoch 4, Batch 104, LR 0.745520 Loss 19.064078, Accuracy 0.113%\n",
      "Epoch 4, Batch 105, LR 0.745780 Loss 19.064342, Accuracy 0.112%\n",
      "Epoch 4, Batch 106, LR 0.746041 Loss 19.064296, Accuracy 0.111%\n",
      "Epoch 4, Batch 107, LR 0.746302 Loss 19.066593, Accuracy 0.110%\n",
      "Epoch 4, Batch 108, LR 0.746563 Loss 19.065084, Accuracy 0.109%\n",
      "Epoch 4, Batch 109, LR 0.746825 Loss 19.066182, Accuracy 0.108%\n",
      "Epoch 4, Batch 110, LR 0.747086 Loss 19.065122, Accuracy 0.107%\n",
      "Epoch 4, Batch 111, LR 0.747347 Loss 19.065937, Accuracy 0.113%\n",
      "Epoch 4, Batch 112, LR 0.747608 Loss 19.066658, Accuracy 0.119%\n",
      "Epoch 4, Batch 113, LR 0.747869 Loss 19.068697, Accuracy 0.118%\n",
      "Epoch 4, Batch 114, LR 0.748130 Loss 19.069851, Accuracy 0.117%\n",
      "Epoch 4, Batch 115, LR 0.748392 Loss 19.067034, Accuracy 0.122%\n",
      "Epoch 4, Batch 116, LR 0.748653 Loss 19.067310, Accuracy 0.121%\n",
      "Epoch 4, Batch 117, LR 0.748914 Loss 19.067781, Accuracy 0.120%\n",
      "Epoch 4, Batch 118, LR 0.749176 Loss 19.069056, Accuracy 0.119%\n",
      "Epoch 4, Batch 119, LR 0.749437 Loss 19.067898, Accuracy 0.118%\n",
      "Epoch 4, Batch 120, LR 0.749699 Loss 19.068466, Accuracy 0.124%\n",
      "Epoch 4, Batch 121, LR 0.749960 Loss 19.067426, Accuracy 0.129%\n",
      "Epoch 4, Batch 122, LR 0.750222 Loss 19.067279, Accuracy 0.128%\n",
      "Epoch 4, Batch 123, LR 0.750483 Loss 19.066431, Accuracy 0.127%\n",
      "Epoch 4, Batch 124, LR 0.750745 Loss 19.067476, Accuracy 0.126%\n",
      "Epoch 4, Batch 125, LR 0.751007 Loss 19.067088, Accuracy 0.125%\n",
      "Epoch 4, Batch 126, LR 0.751268 Loss 19.067982, Accuracy 0.124%\n",
      "Epoch 4, Batch 127, LR 0.751530 Loss 19.067802, Accuracy 0.123%\n",
      "Epoch 4, Batch 128, LR 0.751792 Loss 19.067981, Accuracy 0.122%\n",
      "Epoch 4, Batch 129, LR 0.752054 Loss 19.069195, Accuracy 0.121%\n",
      "Epoch 4, Batch 130, LR 0.752315 Loss 19.068273, Accuracy 0.126%\n",
      "Epoch 4, Batch 131, LR 0.752577 Loss 19.069152, Accuracy 0.125%\n",
      "Epoch 4, Batch 132, LR 0.752839 Loss 19.068456, Accuracy 0.130%\n",
      "Epoch 4, Batch 133, LR 0.753101 Loss 19.068882, Accuracy 0.129%\n",
      "Epoch 4, Batch 134, LR 0.753363 Loss 19.067273, Accuracy 0.128%\n",
      "Epoch 4, Batch 135, LR 0.753625 Loss 19.066780, Accuracy 0.127%\n",
      "Epoch 4, Batch 136, LR 0.753887 Loss 19.066915, Accuracy 0.126%\n",
      "Epoch 4, Batch 137, LR 0.754149 Loss 19.067311, Accuracy 0.125%\n",
      "Epoch 4, Batch 138, LR 0.754411 Loss 19.068234, Accuracy 0.125%\n",
      "Epoch 4, Batch 139, LR 0.754673 Loss 19.069830, Accuracy 0.124%\n",
      "Epoch 4, Batch 140, LR 0.754936 Loss 19.068707, Accuracy 0.123%\n",
      "Epoch 4, Batch 141, LR 0.755198 Loss 19.069732, Accuracy 0.122%\n",
      "Epoch 4, Batch 142, LR 0.755460 Loss 19.069337, Accuracy 0.121%\n",
      "Epoch 4, Batch 143, LR 0.755722 Loss 19.068530, Accuracy 0.120%\n",
      "Epoch 4, Batch 144, LR 0.755985 Loss 19.067294, Accuracy 0.119%\n",
      "Epoch 4, Batch 145, LR 0.756247 Loss 19.066123, Accuracy 0.119%\n",
      "Epoch 4, Batch 146, LR 0.756509 Loss 19.064810, Accuracy 0.118%\n",
      "Epoch 4, Batch 147, LR 0.756772 Loss 19.064408, Accuracy 0.117%\n",
      "Epoch 4, Batch 148, LR 0.757034 Loss 19.063780, Accuracy 0.116%\n",
      "Epoch 4, Batch 149, LR 0.757297 Loss 19.064684, Accuracy 0.115%\n",
      "Epoch 4, Batch 150, LR 0.757559 Loss 19.063675, Accuracy 0.120%\n",
      "Epoch 4, Batch 151, LR 0.757822 Loss 19.063296, Accuracy 0.119%\n",
      "Epoch 4, Batch 152, LR 0.758084 Loss 19.063064, Accuracy 0.118%\n",
      "Epoch 4, Batch 153, LR 0.758347 Loss 19.063016, Accuracy 0.117%\n",
      "Epoch 4, Batch 154, LR 0.758610 Loss 19.061659, Accuracy 0.117%\n",
      "Epoch 4, Batch 155, LR 0.758872 Loss 19.061835, Accuracy 0.116%\n",
      "Epoch 4, Batch 156, LR 0.759135 Loss 19.063378, Accuracy 0.115%\n",
      "Epoch 4, Batch 157, LR 0.759398 Loss 19.063114, Accuracy 0.114%\n",
      "Epoch 4, Batch 158, LR 0.759661 Loss 19.063729, Accuracy 0.114%\n",
      "Epoch 4, Batch 159, LR 0.759923 Loss 19.062470, Accuracy 0.113%\n",
      "Epoch 4, Batch 160, LR 0.760186 Loss 19.062014, Accuracy 0.112%\n",
      "Epoch 4, Batch 161, LR 0.760449 Loss 19.061416, Accuracy 0.112%\n",
      "Epoch 4, Batch 162, LR 0.760712 Loss 19.060726, Accuracy 0.116%\n",
      "Epoch 4, Batch 163, LR 0.760975 Loss 19.059505, Accuracy 0.115%\n",
      "Epoch 4, Batch 164, LR 0.761238 Loss 19.059558, Accuracy 0.114%\n",
      "Epoch 4, Batch 165, LR 0.761501 Loss 19.060209, Accuracy 0.118%\n",
      "Epoch 4, Batch 166, LR 0.761764 Loss 19.059221, Accuracy 0.118%\n",
      "Epoch 4, Batch 167, LR 0.762027 Loss 19.058916, Accuracy 0.117%\n",
      "Epoch 4, Batch 168, LR 0.762290 Loss 19.059087, Accuracy 0.116%\n",
      "Epoch 4, Batch 169, LR 0.762554 Loss 19.058795, Accuracy 0.116%\n",
      "Epoch 4, Batch 170, LR 0.762817 Loss 19.059142, Accuracy 0.115%\n",
      "Epoch 4, Batch 171, LR 0.763080 Loss 19.058278, Accuracy 0.114%\n",
      "Epoch 4, Batch 172, LR 0.763343 Loss 19.058262, Accuracy 0.118%\n",
      "Epoch 4, Batch 173, LR 0.763607 Loss 19.058606, Accuracy 0.117%\n",
      "Epoch 4, Batch 174, LR 0.763870 Loss 19.058908, Accuracy 0.117%\n",
      "Epoch 4, Batch 175, LR 0.764133 Loss 19.058194, Accuracy 0.116%\n",
      "Epoch 4, Batch 176, LR 0.764397 Loss 19.057507, Accuracy 0.115%\n",
      "Epoch 4, Batch 177, LR 0.764660 Loss 19.057894, Accuracy 0.115%\n",
      "Epoch 4, Batch 178, LR 0.764924 Loss 19.057480, Accuracy 0.114%\n",
      "Epoch 4, Batch 179, LR 0.765187 Loss 19.058185, Accuracy 0.113%\n",
      "Epoch 4, Batch 180, LR 0.765451 Loss 19.057933, Accuracy 0.117%\n",
      "Epoch 4, Batch 181, LR 0.765714 Loss 19.057666, Accuracy 0.117%\n",
      "Epoch 4, Batch 182, LR 0.765978 Loss 19.057260, Accuracy 0.120%\n",
      "Epoch 4, Batch 183, LR 0.766242 Loss 19.058700, Accuracy 0.120%\n",
      "Epoch 4, Batch 184, LR 0.766505 Loss 19.059652, Accuracy 0.119%\n",
      "Epoch 4, Batch 185, LR 0.766769 Loss 19.060351, Accuracy 0.118%\n",
      "Epoch 4, Batch 186, LR 0.767033 Loss 19.059400, Accuracy 0.122%\n",
      "Epoch 4, Batch 187, LR 0.767297 Loss 19.059415, Accuracy 0.121%\n",
      "Epoch 4, Batch 188, LR 0.767560 Loss 19.059658, Accuracy 0.121%\n",
      "Epoch 4, Batch 189, LR 0.767824 Loss 19.058971, Accuracy 0.120%\n",
      "Epoch 4, Batch 190, LR 0.768088 Loss 19.059929, Accuracy 0.119%\n",
      "Epoch 4, Batch 191, LR 0.768352 Loss 19.060014, Accuracy 0.119%\n",
      "Epoch 4, Batch 192, LR 0.768616 Loss 19.060243, Accuracy 0.118%\n",
      "Epoch 4, Batch 193, LR 0.768880 Loss 19.059545, Accuracy 0.121%\n",
      "Epoch 4, Batch 194, LR 0.769144 Loss 19.059912, Accuracy 0.125%\n",
      "Epoch 4, Batch 195, LR 0.769408 Loss 19.060718, Accuracy 0.124%\n",
      "Epoch 4, Batch 196, LR 0.769672 Loss 19.061045, Accuracy 0.124%\n",
      "Epoch 4, Batch 197, LR 0.769936 Loss 19.060969, Accuracy 0.123%\n",
      "Epoch 4, Batch 198, LR 0.770200 Loss 19.060549, Accuracy 0.122%\n",
      "Epoch 4, Batch 199, LR 0.770465 Loss 19.060608, Accuracy 0.122%\n",
      "Epoch 4, Batch 200, LR 0.770729 Loss 19.059939, Accuracy 0.121%\n",
      "Epoch 4, Batch 201, LR 0.770993 Loss 19.060454, Accuracy 0.120%\n",
      "Epoch 4, Batch 202, LR 0.771257 Loss 19.059973, Accuracy 0.120%\n",
      "Epoch 4, Batch 203, LR 0.771522 Loss 19.059912, Accuracy 0.119%\n",
      "Epoch 4, Batch 204, LR 0.771786 Loss 19.059613, Accuracy 0.119%\n",
      "Epoch 4, Batch 205, LR 0.772051 Loss 19.059325, Accuracy 0.118%\n",
      "Epoch 4, Batch 206, LR 0.772315 Loss 19.058120, Accuracy 0.118%\n",
      "Epoch 4, Batch 207, LR 0.772579 Loss 19.058572, Accuracy 0.117%\n",
      "Epoch 4, Batch 208, LR 0.772844 Loss 19.057891, Accuracy 0.116%\n",
      "Epoch 4, Batch 209, LR 0.773108 Loss 19.058884, Accuracy 0.116%\n",
      "Epoch 4, Batch 210, LR 0.773373 Loss 19.059472, Accuracy 0.115%\n",
      "Epoch 4, Batch 211, LR 0.773638 Loss 19.060105, Accuracy 0.115%\n",
      "Epoch 4, Batch 212, LR 0.773902 Loss 19.059741, Accuracy 0.114%\n",
      "Epoch 4, Batch 213, LR 0.774167 Loss 19.060065, Accuracy 0.114%\n",
      "Epoch 4, Batch 214, LR 0.774432 Loss 19.058790, Accuracy 0.113%\n",
      "Epoch 4, Batch 215, LR 0.774696 Loss 19.058360, Accuracy 0.116%\n",
      "Epoch 4, Batch 216, LR 0.774961 Loss 19.058616, Accuracy 0.116%\n",
      "Epoch 4, Batch 217, LR 0.775226 Loss 19.058775, Accuracy 0.119%\n",
      "Epoch 4, Batch 218, LR 0.775491 Loss 19.058009, Accuracy 0.118%\n",
      "Epoch 4, Batch 219, LR 0.775756 Loss 19.057951, Accuracy 0.121%\n",
      "Epoch 4, Batch 220, LR 0.776021 Loss 19.058670, Accuracy 0.121%\n",
      "Epoch 4, Batch 221, LR 0.776286 Loss 19.058599, Accuracy 0.120%\n",
      "Epoch 4, Batch 222, LR 0.776551 Loss 19.058724, Accuracy 0.120%\n",
      "Epoch 4, Batch 223, LR 0.776816 Loss 19.058875, Accuracy 0.119%\n",
      "Epoch 4, Batch 224, LR 0.777081 Loss 19.058813, Accuracy 0.119%\n",
      "Epoch 4, Batch 225, LR 0.777346 Loss 19.058460, Accuracy 0.118%\n",
      "Epoch 4, Batch 226, LR 0.777611 Loss 19.058091, Accuracy 0.118%\n",
      "Epoch 4, Batch 227, LR 0.777876 Loss 19.057779, Accuracy 0.117%\n",
      "Epoch 4, Batch 228, LR 0.778141 Loss 19.058437, Accuracy 0.117%\n",
      "Epoch 4, Batch 229, LR 0.778406 Loss 19.058411, Accuracy 0.116%\n",
      "Epoch 4, Batch 230, LR 0.778672 Loss 19.058840, Accuracy 0.119%\n",
      "Epoch 4, Batch 231, LR 0.778937 Loss 19.058802, Accuracy 0.118%\n",
      "Epoch 4, Batch 232, LR 0.779202 Loss 19.058283, Accuracy 0.118%\n",
      "Epoch 4, Batch 233, LR 0.779467 Loss 19.058432, Accuracy 0.117%\n",
      "Epoch 4, Batch 234, LR 0.779733 Loss 19.058328, Accuracy 0.120%\n",
      "Epoch 4, Batch 235, LR 0.779998 Loss 19.058017, Accuracy 0.120%\n",
      "Epoch 4, Batch 236, LR 0.780264 Loss 19.058260, Accuracy 0.119%\n",
      "Epoch 4, Batch 237, LR 0.780529 Loss 19.058224, Accuracy 0.119%\n",
      "Epoch 4, Batch 238, LR 0.780795 Loss 19.057121, Accuracy 0.118%\n",
      "Epoch 4, Batch 239, LR 0.781060 Loss 19.057666, Accuracy 0.118%\n",
      "Epoch 4, Batch 240, LR 0.781326 Loss 19.057902, Accuracy 0.117%\n",
      "Epoch 4, Batch 241, LR 0.781591 Loss 19.058352, Accuracy 0.117%\n",
      "Epoch 4, Batch 242, LR 0.781857 Loss 19.059108, Accuracy 0.116%\n",
      "Epoch 4, Batch 243, LR 0.782123 Loss 19.059106, Accuracy 0.116%\n",
      "Epoch 4, Batch 244, LR 0.782388 Loss 19.058745, Accuracy 0.118%\n",
      "Epoch 4, Batch 245, LR 0.782654 Loss 19.058183, Accuracy 0.118%\n",
      "Epoch 4, Batch 246, LR 0.782920 Loss 19.058477, Accuracy 0.121%\n",
      "Epoch 4, Batch 247, LR 0.783186 Loss 19.058543, Accuracy 0.120%\n",
      "Epoch 4, Batch 248, LR 0.783452 Loss 19.058062, Accuracy 0.120%\n",
      "Epoch 4, Batch 249, LR 0.783717 Loss 19.058251, Accuracy 0.119%\n",
      "Epoch 4, Batch 250, LR 0.783983 Loss 19.057462, Accuracy 0.122%\n",
      "Epoch 4, Batch 251, LR 0.784249 Loss 19.056867, Accuracy 0.121%\n",
      "Epoch 4, Batch 252, LR 0.784515 Loss 19.056349, Accuracy 0.121%\n",
      "Epoch 4, Batch 253, LR 0.784781 Loss 19.056108, Accuracy 0.120%\n",
      "Epoch 4, Batch 254, LR 0.785047 Loss 19.056753, Accuracy 0.120%\n",
      "Epoch 4, Batch 255, LR 0.785313 Loss 19.056006, Accuracy 0.119%\n",
      "Epoch 4, Batch 256, LR 0.785579 Loss 19.055528, Accuracy 0.119%\n",
      "Epoch 4, Batch 257, LR 0.785846 Loss 19.055340, Accuracy 0.119%\n",
      "Epoch 4, Batch 258, LR 0.786112 Loss 19.054886, Accuracy 0.118%\n",
      "Epoch 4, Batch 259, LR 0.786378 Loss 19.055078, Accuracy 0.118%\n",
      "Epoch 4, Batch 260, LR 0.786644 Loss 19.054981, Accuracy 0.117%\n",
      "Epoch 4, Batch 261, LR 0.786910 Loss 19.055064, Accuracy 0.117%\n",
      "Epoch 4, Batch 262, LR 0.787177 Loss 19.055029, Accuracy 0.116%\n",
      "Epoch 4, Batch 263, LR 0.787443 Loss 19.055298, Accuracy 0.116%\n",
      "Epoch 4, Batch 264, LR 0.787709 Loss 19.055533, Accuracy 0.115%\n",
      "Epoch 4, Batch 265, LR 0.787976 Loss 19.055869, Accuracy 0.115%\n",
      "Epoch 4, Batch 266, LR 0.788242 Loss 19.055642, Accuracy 0.115%\n",
      "Epoch 4, Batch 267, LR 0.788509 Loss 19.056098, Accuracy 0.114%\n",
      "Epoch 4, Batch 268, LR 0.788775 Loss 19.056302, Accuracy 0.114%\n",
      "Epoch 4, Batch 269, LR 0.789042 Loss 19.055968, Accuracy 0.113%\n",
      "Epoch 4, Batch 270, LR 0.789308 Loss 19.056287, Accuracy 0.113%\n",
      "Epoch 4, Batch 271, LR 0.789575 Loss 19.057085, Accuracy 0.112%\n",
      "Epoch 4, Batch 272, LR 0.789842 Loss 19.057103, Accuracy 0.112%\n",
      "Epoch 4, Batch 273, LR 0.790108 Loss 19.057049, Accuracy 0.112%\n",
      "Epoch 4, Batch 274, LR 0.790375 Loss 19.057449, Accuracy 0.111%\n",
      "Epoch 4, Batch 275, LR 0.790642 Loss 19.057280, Accuracy 0.111%\n",
      "Epoch 4, Batch 276, LR 0.790908 Loss 19.056668, Accuracy 0.110%\n",
      "Epoch 4, Batch 277, LR 0.791175 Loss 19.056451, Accuracy 0.110%\n",
      "Epoch 4, Batch 278, LR 0.791442 Loss 19.055522, Accuracy 0.110%\n",
      "Epoch 4, Batch 279, LR 0.791709 Loss 19.056066, Accuracy 0.109%\n",
      "Epoch 4, Batch 280, LR 0.791976 Loss 19.055992, Accuracy 0.109%\n",
      "Epoch 4, Batch 281, LR 0.792243 Loss 19.055085, Accuracy 0.108%\n",
      "Epoch 4, Batch 282, LR 0.792510 Loss 19.055282, Accuracy 0.114%\n",
      "Epoch 4, Batch 283, LR 0.792777 Loss 19.055643, Accuracy 0.116%\n",
      "Epoch 4, Batch 284, LR 0.793044 Loss 19.055044, Accuracy 0.116%\n",
      "Epoch 4, Batch 285, LR 0.793311 Loss 19.055099, Accuracy 0.118%\n",
      "Epoch 4, Batch 286, LR 0.793578 Loss 19.054677, Accuracy 0.117%\n",
      "Epoch 4, Batch 287, LR 0.793845 Loss 19.054997, Accuracy 0.117%\n",
      "Epoch 4, Batch 288, LR 0.794112 Loss 19.055810, Accuracy 0.117%\n",
      "Epoch 4, Batch 289, LR 0.794379 Loss 19.056088, Accuracy 0.116%\n",
      "Epoch 4, Batch 290, LR 0.794646 Loss 19.056217, Accuracy 0.116%\n",
      "Epoch 4, Batch 291, LR 0.794914 Loss 19.056314, Accuracy 0.118%\n",
      "Epoch 4, Batch 292, LR 0.795181 Loss 19.056971, Accuracy 0.118%\n",
      "Epoch 4, Batch 293, LR 0.795448 Loss 19.056558, Accuracy 0.117%\n",
      "Epoch 4, Batch 294, LR 0.795716 Loss 19.057003, Accuracy 0.120%\n",
      "Epoch 4, Batch 295, LR 0.795983 Loss 19.056650, Accuracy 0.119%\n",
      "Epoch 4, Batch 296, LR 0.796250 Loss 19.056680, Accuracy 0.119%\n",
      "Epoch 4, Batch 297, LR 0.796518 Loss 19.056537, Accuracy 0.118%\n",
      "Epoch 4, Batch 298, LR 0.796785 Loss 19.056187, Accuracy 0.118%\n",
      "Epoch 4, Batch 299, LR 0.797053 Loss 19.056220, Accuracy 0.118%\n",
      "Epoch 4, Batch 300, LR 0.797320 Loss 19.055919, Accuracy 0.117%\n",
      "Epoch 4, Batch 301, LR 0.797588 Loss 19.056180, Accuracy 0.117%\n",
      "Epoch 4, Batch 302, LR 0.797856 Loss 19.056616, Accuracy 0.119%\n",
      "Epoch 4, Batch 303, LR 0.798123 Loss 19.056791, Accuracy 0.121%\n",
      "Epoch 4, Batch 304, LR 0.798391 Loss 19.056459, Accuracy 0.121%\n",
      "Epoch 4, Batch 305, LR 0.798659 Loss 19.056759, Accuracy 0.120%\n",
      "Epoch 4, Batch 306, LR 0.798926 Loss 19.056419, Accuracy 0.120%\n",
      "Epoch 4, Batch 307, LR 0.799194 Loss 19.056600, Accuracy 0.120%\n",
      "Epoch 4, Batch 308, LR 0.799462 Loss 19.055669, Accuracy 0.119%\n",
      "Epoch 4, Batch 309, LR 0.799730 Loss 19.055893, Accuracy 0.119%\n",
      "Epoch 4, Batch 310, LR 0.799998 Loss 19.056021, Accuracy 0.118%\n",
      "Epoch 4, Batch 311, LR 0.800265 Loss 19.055539, Accuracy 0.118%\n",
      "Epoch 4, Batch 312, LR 0.800533 Loss 19.055472, Accuracy 0.118%\n",
      "Epoch 4, Batch 313, LR 0.800801 Loss 19.056043, Accuracy 0.117%\n",
      "Epoch 4, Batch 314, LR 0.801069 Loss 19.055583, Accuracy 0.117%\n",
      "Epoch 4, Batch 315, LR 0.801337 Loss 19.055255, Accuracy 0.117%\n",
      "Epoch 4, Batch 316, LR 0.801605 Loss 19.054741, Accuracy 0.119%\n",
      "Epoch 4, Batch 317, LR 0.801873 Loss 19.055025, Accuracy 0.121%\n",
      "Epoch 4, Batch 318, LR 0.802142 Loss 19.054636, Accuracy 0.120%\n",
      "Epoch 4, Batch 319, LR 0.802410 Loss 19.054652, Accuracy 0.120%\n",
      "Epoch 4, Batch 320, LR 0.802678 Loss 19.053887, Accuracy 0.120%\n",
      "Epoch 4, Batch 321, LR 0.802946 Loss 19.054343, Accuracy 0.119%\n",
      "Epoch 4, Batch 322, LR 0.803214 Loss 19.054397, Accuracy 0.119%\n",
      "Epoch 4, Batch 323, LR 0.803483 Loss 19.054008, Accuracy 0.119%\n",
      "Epoch 4, Batch 324, LR 0.803751 Loss 19.054033, Accuracy 0.118%\n",
      "Epoch 4, Batch 325, LR 0.804019 Loss 19.054493, Accuracy 0.118%\n",
      "Epoch 4, Batch 326, LR 0.804288 Loss 19.055083, Accuracy 0.117%\n",
      "Epoch 4, Batch 327, LR 0.804556 Loss 19.055011, Accuracy 0.117%\n",
      "Epoch 4, Batch 328, LR 0.804824 Loss 19.054926, Accuracy 0.117%\n",
      "Epoch 4, Batch 329, LR 0.805093 Loss 19.055326, Accuracy 0.116%\n",
      "Epoch 4, Batch 330, LR 0.805361 Loss 19.055026, Accuracy 0.116%\n",
      "Epoch 4, Batch 331, LR 0.805630 Loss 19.055235, Accuracy 0.116%\n",
      "Epoch 4, Batch 332, LR 0.805899 Loss 19.056029, Accuracy 0.115%\n",
      "Epoch 4, Batch 333, LR 0.806167 Loss 19.056481, Accuracy 0.115%\n",
      "Epoch 4, Batch 334, LR 0.806436 Loss 19.056805, Accuracy 0.115%\n",
      "Epoch 4, Batch 335, LR 0.806704 Loss 19.057055, Accuracy 0.114%\n",
      "Epoch 4, Batch 336, LR 0.806973 Loss 19.056957, Accuracy 0.114%\n",
      "Epoch 4, Batch 337, LR 0.807242 Loss 19.056799, Accuracy 0.114%\n",
      "Epoch 4, Batch 338, LR 0.807511 Loss 19.055955, Accuracy 0.113%\n",
      "Epoch 4, Batch 339, LR 0.807779 Loss 19.055706, Accuracy 0.113%\n",
      "Epoch 4, Batch 340, LR 0.808048 Loss 19.056647, Accuracy 0.113%\n",
      "Epoch 4, Batch 341, LR 0.808317 Loss 19.056974, Accuracy 0.112%\n",
      "Epoch 4, Batch 342, LR 0.808586 Loss 19.057048, Accuracy 0.112%\n",
      "Epoch 4, Batch 343, LR 0.808855 Loss 19.057016, Accuracy 0.112%\n",
      "Epoch 4, Batch 344, LR 0.809124 Loss 19.056633, Accuracy 0.111%\n",
      "Epoch 4, Batch 345, LR 0.809393 Loss 19.056414, Accuracy 0.111%\n",
      "Epoch 4, Batch 346, LR 0.809662 Loss 19.056322, Accuracy 0.111%\n",
      "Epoch 4, Batch 347, LR 0.809931 Loss 19.056460, Accuracy 0.110%\n",
      "Epoch 4, Batch 348, LR 0.810200 Loss 19.056534, Accuracy 0.110%\n",
      "Epoch 4, Batch 349, LR 0.810469 Loss 19.056671, Accuracy 0.110%\n",
      "Epoch 4, Batch 350, LR 0.810738 Loss 19.056506, Accuracy 0.109%\n",
      "Epoch 4, Batch 351, LR 0.811007 Loss 19.056640, Accuracy 0.109%\n",
      "Epoch 4, Batch 352, LR 0.811276 Loss 19.056820, Accuracy 0.109%\n",
      "Epoch 4, Batch 353, LR 0.811546 Loss 19.057622, Accuracy 0.108%\n",
      "Epoch 4, Batch 354, LR 0.811815 Loss 19.057569, Accuracy 0.108%\n",
      "Epoch 4, Batch 355, LR 0.812084 Loss 19.058640, Accuracy 0.108%\n",
      "Epoch 4, Batch 356, LR 0.812353 Loss 19.058294, Accuracy 0.108%\n",
      "Epoch 4, Batch 357, LR 0.812623 Loss 19.058231, Accuracy 0.107%\n",
      "Epoch 4, Batch 358, LR 0.812892 Loss 19.057889, Accuracy 0.107%\n",
      "Epoch 4, Batch 359, LR 0.813162 Loss 19.057850, Accuracy 0.107%\n",
      "Epoch 4, Batch 360, LR 0.813431 Loss 19.057641, Accuracy 0.106%\n",
      "Epoch 4, Batch 361, LR 0.813700 Loss 19.057872, Accuracy 0.106%\n",
      "Epoch 4, Batch 362, LR 0.813970 Loss 19.057323, Accuracy 0.108%\n",
      "Epoch 4, Batch 363, LR 0.814240 Loss 19.057590, Accuracy 0.108%\n",
      "Epoch 4, Batch 364, LR 0.814509 Loss 19.057457, Accuracy 0.107%\n",
      "Epoch 4, Batch 365, LR 0.814779 Loss 19.057485, Accuracy 0.107%\n",
      "Epoch 4, Batch 366, LR 0.815048 Loss 19.058045, Accuracy 0.107%\n",
      "Epoch 4, Batch 367, LR 0.815318 Loss 19.058115, Accuracy 0.106%\n",
      "Epoch 4, Batch 368, LR 0.815588 Loss 19.058499, Accuracy 0.106%\n",
      "Epoch 4, Batch 369, LR 0.815857 Loss 19.058584, Accuracy 0.106%\n",
      "Epoch 4, Batch 370, LR 0.816127 Loss 19.058825, Accuracy 0.106%\n",
      "Epoch 4, Batch 371, LR 0.816397 Loss 19.058617, Accuracy 0.105%\n",
      "Epoch 4, Batch 372, LR 0.816667 Loss 19.058465, Accuracy 0.105%\n",
      "Epoch 4, Batch 373, LR 0.816937 Loss 19.058526, Accuracy 0.105%\n",
      "Epoch 4, Batch 374, LR 0.817206 Loss 19.058632, Accuracy 0.104%\n",
      "Epoch 4, Batch 375, LR 0.817476 Loss 19.058737, Accuracy 0.106%\n",
      "Epoch 4, Batch 376, LR 0.817746 Loss 19.058888, Accuracy 0.106%\n",
      "Epoch 4, Batch 377, LR 0.818016 Loss 19.058924, Accuracy 0.106%\n",
      "Epoch 4, Batch 378, LR 0.818286 Loss 19.058990, Accuracy 0.107%\n",
      "Epoch 4, Batch 379, LR 0.818556 Loss 19.058774, Accuracy 0.107%\n",
      "Epoch 4, Batch 380, LR 0.818826 Loss 19.059365, Accuracy 0.107%\n",
      "Epoch 4, Batch 381, LR 0.819096 Loss 19.059684, Accuracy 0.107%\n",
      "Epoch 4, Batch 382, LR 0.819367 Loss 19.059570, Accuracy 0.106%\n",
      "Epoch 4, Batch 383, LR 0.819637 Loss 19.059791, Accuracy 0.106%\n",
      "Epoch 4, Batch 384, LR 0.819907 Loss 19.060081, Accuracy 0.106%\n",
      "Epoch 4, Batch 385, LR 0.820177 Loss 19.060033, Accuracy 0.108%\n",
      "Epoch 4, Batch 386, LR 0.820447 Loss 19.060168, Accuracy 0.107%\n",
      "Epoch 4, Batch 387, LR 0.820718 Loss 19.059820, Accuracy 0.107%\n",
      "Epoch 4, Batch 388, LR 0.820988 Loss 19.060059, Accuracy 0.107%\n",
      "Epoch 4, Batch 389, LR 0.821258 Loss 19.060402, Accuracy 0.106%\n",
      "Epoch 4, Batch 390, LR 0.821529 Loss 19.060228, Accuracy 0.106%\n",
      "Epoch 4, Batch 391, LR 0.821799 Loss 19.059752, Accuracy 0.106%\n",
      "Epoch 4, Batch 392, LR 0.822070 Loss 19.059955, Accuracy 0.106%\n",
      "Epoch 4, Batch 393, LR 0.822340 Loss 19.059907, Accuracy 0.105%\n",
      "Epoch 4, Batch 394, LR 0.822610 Loss 19.059684, Accuracy 0.105%\n",
      "Epoch 4, Batch 395, LR 0.822881 Loss 19.059854, Accuracy 0.105%\n",
      "Epoch 4, Batch 396, LR 0.823152 Loss 19.060362, Accuracy 0.107%\n",
      "Epoch 4, Batch 397, LR 0.823422 Loss 19.060048, Accuracy 0.106%\n",
      "Epoch 4, Batch 398, LR 0.823693 Loss 19.059932, Accuracy 0.108%\n",
      "Epoch 4, Batch 399, LR 0.823963 Loss 19.060215, Accuracy 0.108%\n",
      "Epoch 4, Batch 400, LR 0.824234 Loss 19.060049, Accuracy 0.109%\n",
      "Epoch 4, Batch 401, LR 0.824505 Loss 19.060615, Accuracy 0.111%\n",
      "Epoch 4, Batch 402, LR 0.824775 Loss 19.060609, Accuracy 0.111%\n",
      "Epoch 4, Batch 403, LR 0.825046 Loss 19.060258, Accuracy 0.112%\n",
      "Epoch 4, Batch 404, LR 0.825317 Loss 19.060341, Accuracy 0.112%\n",
      "Epoch 4, Batch 405, LR 0.825588 Loss 19.060483, Accuracy 0.112%\n",
      "Epoch 4, Batch 406, LR 0.825859 Loss 19.060824, Accuracy 0.112%\n",
      "Epoch 4, Batch 407, LR 0.826130 Loss 19.060755, Accuracy 0.111%\n",
      "Epoch 4, Batch 408, LR 0.826401 Loss 19.060712, Accuracy 0.111%\n",
      "Epoch 4, Batch 409, LR 0.826672 Loss 19.060216, Accuracy 0.113%\n",
      "Epoch 4, Batch 410, LR 0.826942 Loss 19.060612, Accuracy 0.112%\n",
      "Epoch 4, Batch 411, LR 0.827214 Loss 19.061001, Accuracy 0.112%\n",
      "Epoch 4, Batch 412, LR 0.827485 Loss 19.060503, Accuracy 0.114%\n",
      "Epoch 4, Batch 413, LR 0.827756 Loss 19.060744, Accuracy 0.113%\n",
      "Epoch 4, Batch 414, LR 0.828027 Loss 19.060834, Accuracy 0.113%\n",
      "Epoch 4, Batch 415, LR 0.828298 Loss 19.060829, Accuracy 0.113%\n",
      "Epoch 4, Batch 416, LR 0.828569 Loss 19.061286, Accuracy 0.113%\n",
      "Epoch 4, Batch 417, LR 0.828840 Loss 19.061364, Accuracy 0.112%\n",
      "Epoch 4, Batch 418, LR 0.829111 Loss 19.061325, Accuracy 0.112%\n",
      "Epoch 4, Batch 419, LR 0.829383 Loss 19.061680, Accuracy 0.112%\n",
      "Epoch 4, Batch 420, LR 0.829654 Loss 19.061741, Accuracy 0.112%\n",
      "Epoch 4, Batch 421, LR 0.829925 Loss 19.061454, Accuracy 0.111%\n",
      "Epoch 4, Batch 422, LR 0.830197 Loss 19.061110, Accuracy 0.111%\n",
      "Epoch 4, Batch 423, LR 0.830468 Loss 19.061366, Accuracy 0.111%\n",
      "Epoch 4, Batch 424, LR 0.830739 Loss 19.061712, Accuracy 0.111%\n",
      "Epoch 4, Batch 425, LR 0.831011 Loss 19.061903, Accuracy 0.110%\n",
      "Epoch 4, Batch 426, LR 0.831282 Loss 19.062191, Accuracy 0.110%\n",
      "Epoch 4, Batch 427, LR 0.831554 Loss 19.062329, Accuracy 0.110%\n",
      "Epoch 4, Batch 428, LR 0.831825 Loss 19.062204, Accuracy 0.110%\n",
      "Epoch 4, Batch 429, LR 0.832097 Loss 19.062122, Accuracy 0.109%\n",
      "Epoch 4, Batch 430, LR 0.832368 Loss 19.061791, Accuracy 0.111%\n",
      "Epoch 4, Batch 431, LR 0.832640 Loss 19.061811, Accuracy 0.111%\n",
      "Epoch 4, Batch 432, LR 0.832912 Loss 19.062037, Accuracy 0.110%\n",
      "Epoch 4, Batch 433, LR 0.833183 Loss 19.062157, Accuracy 0.110%\n",
      "Epoch 4, Batch 434, LR 0.833455 Loss 19.062446, Accuracy 0.110%\n",
      "Epoch 4, Batch 435, LR 0.833727 Loss 19.061942, Accuracy 0.110%\n",
      "Epoch 4, Batch 436, LR 0.833999 Loss 19.062106, Accuracy 0.109%\n",
      "Epoch 4, Batch 437, LR 0.834270 Loss 19.062183, Accuracy 0.109%\n",
      "Epoch 4, Batch 438, LR 0.834542 Loss 19.062311, Accuracy 0.109%\n",
      "Epoch 4, Batch 439, LR 0.834814 Loss 19.062004, Accuracy 0.109%\n",
      "Epoch 4, Batch 440, LR 0.835086 Loss 19.061716, Accuracy 0.108%\n",
      "Epoch 4, Batch 441, LR 0.835358 Loss 19.061523, Accuracy 0.108%\n",
      "Epoch 4, Batch 442, LR 0.835630 Loss 19.061643, Accuracy 0.108%\n",
      "Epoch 4, Batch 443, LR 0.835902 Loss 19.061859, Accuracy 0.109%\n",
      "Epoch 4, Batch 444, LR 0.836174 Loss 19.061626, Accuracy 0.111%\n",
      "Epoch 4, Batch 445, LR 0.836446 Loss 19.061573, Accuracy 0.111%\n",
      "Epoch 4, Batch 446, LR 0.836718 Loss 19.061784, Accuracy 0.110%\n",
      "Epoch 4, Batch 447, LR 0.836990 Loss 19.061544, Accuracy 0.110%\n",
      "Epoch 4, Batch 448, LR 0.837262 Loss 19.061857, Accuracy 0.110%\n",
      "Epoch 4, Batch 449, LR 0.837534 Loss 19.061727, Accuracy 0.110%\n",
      "Epoch 4, Batch 450, LR 0.837806 Loss 19.061621, Accuracy 0.109%\n",
      "Epoch 4, Batch 451, LR 0.838079 Loss 19.061499, Accuracy 0.109%\n",
      "Epoch 4, Batch 452, LR 0.838351 Loss 19.061446, Accuracy 0.111%\n",
      "Epoch 4, Batch 453, LR 0.838623 Loss 19.061560, Accuracy 0.110%\n",
      "Epoch 4, Batch 454, LR 0.838895 Loss 19.061583, Accuracy 0.110%\n",
      "Epoch 4, Batch 455, LR 0.839168 Loss 19.061499, Accuracy 0.110%\n",
      "Epoch 4, Batch 456, LR 0.839440 Loss 19.061599, Accuracy 0.110%\n",
      "Epoch 4, Batch 457, LR 0.839713 Loss 19.061319, Accuracy 0.109%\n",
      "Epoch 4, Batch 458, LR 0.839985 Loss 19.060951, Accuracy 0.109%\n",
      "Epoch 4, Batch 459, LR 0.840257 Loss 19.061148, Accuracy 0.109%\n",
      "Epoch 4, Batch 460, LR 0.840530 Loss 19.061589, Accuracy 0.109%\n",
      "Epoch 4, Batch 461, LR 0.840802 Loss 19.061793, Accuracy 0.108%\n",
      "Epoch 4, Batch 462, LR 0.841075 Loss 19.061436, Accuracy 0.110%\n",
      "Epoch 4, Batch 463, LR 0.841348 Loss 19.061469, Accuracy 0.110%\n",
      "Epoch 4, Batch 464, LR 0.841620 Loss 19.061424, Accuracy 0.109%\n",
      "Epoch 4, Batch 465, LR 0.841893 Loss 19.061730, Accuracy 0.109%\n",
      "Epoch 4, Batch 466, LR 0.842165 Loss 19.061882, Accuracy 0.111%\n",
      "Epoch 4, Batch 467, LR 0.842438 Loss 19.061855, Accuracy 0.110%\n",
      "Epoch 4, Batch 468, LR 0.842711 Loss 19.061482, Accuracy 0.110%\n",
      "Epoch 4, Batch 469, LR 0.842984 Loss 19.061452, Accuracy 0.110%\n",
      "Epoch 4, Batch 470, LR 0.843256 Loss 19.061393, Accuracy 0.110%\n",
      "Epoch 4, Batch 471, LR 0.843529 Loss 19.061366, Accuracy 0.109%\n",
      "Epoch 4, Batch 472, LR 0.843802 Loss 19.061134, Accuracy 0.109%\n",
      "Epoch 4, Batch 473, LR 0.844075 Loss 19.061074, Accuracy 0.109%\n",
      "Epoch 4, Batch 474, LR 0.844348 Loss 19.061351, Accuracy 0.109%\n",
      "Epoch 4, Batch 475, LR 0.844621 Loss 19.061739, Accuracy 0.109%\n",
      "Epoch 4, Batch 476, LR 0.844894 Loss 19.061983, Accuracy 0.110%\n",
      "Epoch 4, Batch 477, LR 0.845167 Loss 19.062049, Accuracy 0.110%\n",
      "Epoch 4, Batch 478, LR 0.845440 Loss 19.061977, Accuracy 0.110%\n",
      "Epoch 4, Batch 479, LR 0.845713 Loss 19.062034, Accuracy 0.111%\n",
      "Epoch 4, Batch 480, LR 0.845986 Loss 19.062150, Accuracy 0.111%\n",
      "Epoch 4, Batch 481, LR 0.846259 Loss 19.062303, Accuracy 0.110%\n",
      "Epoch 4, Batch 482, LR 0.846532 Loss 19.062267, Accuracy 0.110%\n",
      "Epoch 4, Batch 483, LR 0.846805 Loss 19.062587, Accuracy 0.110%\n",
      "Epoch 4, Batch 484, LR 0.847078 Loss 19.062577, Accuracy 0.110%\n",
      "Epoch 4, Batch 485, LR 0.847351 Loss 19.062443, Accuracy 0.110%\n",
      "Epoch 4, Batch 486, LR 0.847625 Loss 19.062571, Accuracy 0.109%\n",
      "Epoch 4, Batch 487, LR 0.847898 Loss 19.062504, Accuracy 0.109%\n",
      "Epoch 4, Batch 488, LR 0.848171 Loss 19.062914, Accuracy 0.109%\n",
      "Epoch 4, Batch 489, LR 0.848445 Loss 19.062884, Accuracy 0.110%\n",
      "Epoch 4, Batch 490, LR 0.848718 Loss 19.062952, Accuracy 0.110%\n",
      "Epoch 4, Batch 491, LR 0.848991 Loss 19.063054, Accuracy 0.110%\n",
      "Epoch 4, Batch 492, LR 0.849265 Loss 19.063700, Accuracy 0.110%\n",
      "Epoch 4, Batch 493, LR 0.849538 Loss 19.063757, Accuracy 0.109%\n",
      "Epoch 4, Batch 494, LR 0.849812 Loss 19.064073, Accuracy 0.109%\n",
      "Epoch 4, Batch 495, LR 0.850085 Loss 19.064181, Accuracy 0.109%\n",
      "Epoch 4, Batch 496, LR 0.850359 Loss 19.063844, Accuracy 0.110%\n",
      "Epoch 4, Batch 497, LR 0.850632 Loss 19.063956, Accuracy 0.110%\n",
      "Epoch 4, Batch 498, LR 0.850906 Loss 19.064145, Accuracy 0.110%\n",
      "Epoch 4, Batch 499, LR 0.851179 Loss 19.064188, Accuracy 0.110%\n",
      "Epoch 4, Batch 500, LR 0.851453 Loss 19.064013, Accuracy 0.109%\n",
      "Epoch 4, Batch 501, LR 0.851727 Loss 19.063898, Accuracy 0.109%\n",
      "Epoch 4, Batch 502, LR 0.852000 Loss 19.063700, Accuracy 0.109%\n",
      "Epoch 4, Batch 503, LR 0.852274 Loss 19.063974, Accuracy 0.109%\n",
      "Epoch 4, Batch 504, LR 0.852548 Loss 19.064181, Accuracy 0.109%\n",
      "Epoch 4, Batch 505, LR 0.852822 Loss 19.064597, Accuracy 0.108%\n",
      "Epoch 4, Batch 506, LR 0.853096 Loss 19.064932, Accuracy 0.108%\n",
      "Epoch 4, Batch 507, LR 0.853369 Loss 19.064798, Accuracy 0.108%\n",
      "Epoch 4, Batch 508, LR 0.853643 Loss 19.064923, Accuracy 0.108%\n",
      "Epoch 4, Batch 509, LR 0.853917 Loss 19.064873, Accuracy 0.107%\n",
      "Epoch 4, Batch 510, LR 0.854191 Loss 19.064678, Accuracy 0.107%\n",
      "Epoch 4, Batch 511, LR 0.854465 Loss 19.064533, Accuracy 0.107%\n",
      "Epoch 4, Batch 512, LR 0.854739 Loss 19.064863, Accuracy 0.107%\n",
      "Epoch 4, Batch 513, LR 0.855013 Loss 19.065059, Accuracy 0.107%\n",
      "Epoch 4, Batch 514, LR 0.855287 Loss 19.064964, Accuracy 0.106%\n",
      "Epoch 4, Batch 515, LR 0.855561 Loss 19.065010, Accuracy 0.109%\n",
      "Epoch 4, Batch 516, LR 0.855835 Loss 19.064994, Accuracy 0.109%\n",
      "Epoch 4, Batch 517, LR 0.856109 Loss 19.064692, Accuracy 0.109%\n",
      "Epoch 4, Batch 518, LR 0.856384 Loss 19.064508, Accuracy 0.109%\n",
      "Epoch 4, Batch 519, LR 0.856658 Loss 19.064492, Accuracy 0.110%\n",
      "Epoch 4, Batch 520, LR 0.856932 Loss 19.064617, Accuracy 0.111%\n",
      "Epoch 4, Batch 521, LR 0.857206 Loss 19.065293, Accuracy 0.111%\n",
      "Epoch 4, Batch 522, LR 0.857480 Loss 19.065523, Accuracy 0.112%\n",
      "Epoch 4, Batch 523, LR 0.857755 Loss 19.065574, Accuracy 0.112%\n",
      "Epoch 4, Batch 524, LR 0.858029 Loss 19.065270, Accuracy 0.112%\n",
      "Epoch 4, Batch 525, LR 0.858303 Loss 19.065070, Accuracy 0.113%\n",
      "Epoch 4, Batch 526, LR 0.858578 Loss 19.064860, Accuracy 0.113%\n",
      "Epoch 4, Batch 527, LR 0.858852 Loss 19.065083, Accuracy 0.113%\n",
      "Epoch 4, Batch 528, LR 0.859127 Loss 19.064878, Accuracy 0.114%\n",
      "Epoch 4, Batch 529, LR 0.859401 Loss 19.064943, Accuracy 0.114%\n",
      "Epoch 4, Batch 530, LR 0.859676 Loss 19.064832, Accuracy 0.114%\n",
      "Epoch 4, Batch 531, LR 0.859950 Loss 19.064702, Accuracy 0.113%\n",
      "Epoch 4, Batch 532, LR 0.860225 Loss 19.064956, Accuracy 0.113%\n",
      "Epoch 4, Batch 533, LR 0.860499 Loss 19.064746, Accuracy 0.114%\n",
      "Epoch 4, Batch 534, LR 0.860774 Loss 19.064995, Accuracy 0.116%\n",
      "Epoch 4, Batch 535, LR 0.861049 Loss 19.064730, Accuracy 0.115%\n",
      "Epoch 4, Batch 536, LR 0.861323 Loss 19.064862, Accuracy 0.115%\n",
      "Epoch 4, Batch 537, LR 0.861598 Loss 19.065302, Accuracy 0.115%\n",
      "Epoch 4, Batch 538, LR 0.861873 Loss 19.065489, Accuracy 0.115%\n",
      "Epoch 4, Batch 539, LR 0.862148 Loss 19.065247, Accuracy 0.115%\n",
      "Epoch 4, Batch 540, LR 0.862422 Loss 19.064800, Accuracy 0.114%\n",
      "Epoch 4, Batch 541, LR 0.862697 Loss 19.064827, Accuracy 0.114%\n",
      "Epoch 4, Batch 542, LR 0.862972 Loss 19.064723, Accuracy 0.114%\n",
      "Epoch 4, Batch 543, LR 0.863247 Loss 19.064931, Accuracy 0.114%\n",
      "Epoch 4, Batch 544, LR 0.863522 Loss 19.064656, Accuracy 0.113%\n",
      "Epoch 4, Batch 545, LR 0.863797 Loss 19.064431, Accuracy 0.113%\n",
      "Epoch 4, Batch 546, LR 0.864072 Loss 19.064269, Accuracy 0.113%\n",
      "Epoch 4, Batch 547, LR 0.864347 Loss 19.064254, Accuracy 0.113%\n",
      "Epoch 4, Batch 548, LR 0.864622 Loss 19.064373, Accuracy 0.113%\n",
      "Epoch 4, Batch 549, LR 0.864897 Loss 19.064462, Accuracy 0.112%\n",
      "Epoch 4, Batch 550, LR 0.865172 Loss 19.064545, Accuracy 0.112%\n",
      "Epoch 4, Batch 551, LR 0.865447 Loss 19.064475, Accuracy 0.112%\n",
      "Epoch 4, Batch 552, LR 0.865722 Loss 19.064116, Accuracy 0.112%\n",
      "Epoch 4, Batch 553, LR 0.865997 Loss 19.064407, Accuracy 0.112%\n",
      "Epoch 4, Batch 554, LR 0.866272 Loss 19.064502, Accuracy 0.111%\n",
      "Epoch 4, Batch 555, LR 0.866547 Loss 19.064427, Accuracy 0.111%\n",
      "Epoch 4, Batch 556, LR 0.866823 Loss 19.064403, Accuracy 0.111%\n",
      "Epoch 4, Batch 557, LR 0.867098 Loss 19.064678, Accuracy 0.111%\n",
      "Epoch 4, Batch 558, LR 0.867373 Loss 19.064586, Accuracy 0.111%\n",
      "Epoch 4, Batch 559, LR 0.867649 Loss 19.064211, Accuracy 0.110%\n",
      "Epoch 4, Batch 560, LR 0.867924 Loss 19.064211, Accuracy 0.110%\n",
      "Epoch 4, Batch 561, LR 0.868199 Loss 19.063998, Accuracy 0.110%\n",
      "Epoch 4, Batch 562, LR 0.868475 Loss 19.063825, Accuracy 0.110%\n",
      "Epoch 4, Batch 563, LR 0.868750 Loss 19.063746, Accuracy 0.110%\n",
      "Epoch 4, Batch 564, LR 0.869026 Loss 19.063923, Accuracy 0.109%\n",
      "Epoch 4, Batch 565, LR 0.869301 Loss 19.064151, Accuracy 0.109%\n",
      "Epoch 4, Batch 566, LR 0.869577 Loss 19.064349, Accuracy 0.109%\n",
      "Epoch 4, Batch 567, LR 0.869852 Loss 19.064791, Accuracy 0.109%\n",
      "Epoch 4, Batch 568, LR 0.870128 Loss 19.064804, Accuracy 0.109%\n",
      "Epoch 4, Batch 569, LR 0.870403 Loss 19.064851, Accuracy 0.108%\n",
      "Epoch 4, Batch 570, LR 0.870679 Loss 19.064534, Accuracy 0.108%\n",
      "Epoch 4, Batch 571, LR 0.870955 Loss 19.064140, Accuracy 0.108%\n",
      "Epoch 4, Batch 572, LR 0.871230 Loss 19.063945, Accuracy 0.108%\n",
      "Epoch 4, Batch 573, LR 0.871506 Loss 19.063957, Accuracy 0.108%\n",
      "Epoch 4, Batch 574, LR 0.871782 Loss 19.063928, Accuracy 0.108%\n",
      "Epoch 4, Batch 575, LR 0.872057 Loss 19.063986, Accuracy 0.107%\n",
      "Epoch 4, Batch 576, LR 0.872333 Loss 19.063733, Accuracy 0.107%\n",
      "Epoch 4, Batch 577, LR 0.872609 Loss 19.063907, Accuracy 0.107%\n",
      "Epoch 4, Batch 578, LR 0.872885 Loss 19.063976, Accuracy 0.107%\n",
      "Epoch 4, Batch 579, LR 0.873161 Loss 19.064350, Accuracy 0.107%\n",
      "Epoch 4, Batch 580, LR 0.873437 Loss 19.064780, Accuracy 0.106%\n",
      "Epoch 4, Batch 581, LR 0.873713 Loss 19.064571, Accuracy 0.106%\n",
      "Epoch 4, Batch 582, LR 0.873988 Loss 19.064783, Accuracy 0.106%\n",
      "Epoch 4, Batch 583, LR 0.874264 Loss 19.064961, Accuracy 0.106%\n",
      "Epoch 4, Batch 584, LR 0.874540 Loss 19.065187, Accuracy 0.106%\n",
      "Epoch 4, Batch 585, LR 0.874817 Loss 19.065415, Accuracy 0.106%\n",
      "Epoch 4, Batch 586, LR 0.875093 Loss 19.065600, Accuracy 0.105%\n",
      "Epoch 4, Batch 587, LR 0.875369 Loss 19.065439, Accuracy 0.105%\n",
      "Epoch 4, Batch 588, LR 0.875645 Loss 19.065284, Accuracy 0.106%\n",
      "Epoch 4, Batch 589, LR 0.875921 Loss 19.065139, Accuracy 0.106%\n",
      "Epoch 4, Batch 590, LR 0.876197 Loss 19.065181, Accuracy 0.106%\n",
      "Epoch 4, Batch 591, LR 0.876473 Loss 19.065357, Accuracy 0.108%\n",
      "Epoch 4, Batch 592, LR 0.876750 Loss 19.065189, Accuracy 0.108%\n",
      "Epoch 4, Batch 593, LR 0.877026 Loss 19.065343, Accuracy 0.108%\n",
      "Epoch 4, Batch 594, LR 0.877302 Loss 19.065477, Accuracy 0.108%\n",
      "Epoch 4, Batch 595, LR 0.877578 Loss 19.065480, Accuracy 0.108%\n",
      "Epoch 4, Batch 596, LR 0.877855 Loss 19.065250, Accuracy 0.107%\n",
      "Epoch 4, Batch 597, LR 0.878131 Loss 19.065542, Accuracy 0.107%\n",
      "Epoch 4, Batch 598, LR 0.878407 Loss 19.065583, Accuracy 0.107%\n",
      "Epoch 4, Batch 599, LR 0.878684 Loss 19.065919, Accuracy 0.107%\n",
      "Epoch 4, Batch 600, LR 0.878960 Loss 19.065889, Accuracy 0.108%\n",
      "Epoch 4, Batch 601, LR 0.879237 Loss 19.066089, Accuracy 0.108%\n",
      "Epoch 4, Batch 602, LR 0.879513 Loss 19.065918, Accuracy 0.109%\n",
      "Epoch 4, Batch 603, LR 0.879790 Loss 19.065820, Accuracy 0.109%\n",
      "Epoch 4, Batch 604, LR 0.880066 Loss 19.066278, Accuracy 0.109%\n",
      "Epoch 4, Batch 605, LR 0.880343 Loss 19.066234, Accuracy 0.108%\n",
      "Epoch 4, Batch 606, LR 0.880620 Loss 19.065973, Accuracy 0.108%\n",
      "Epoch 4, Batch 607, LR 0.880896 Loss 19.066039, Accuracy 0.108%\n",
      "Epoch 4, Batch 608, LR 0.881173 Loss 19.065969, Accuracy 0.108%\n",
      "Epoch 4, Batch 609, LR 0.881450 Loss 19.066392, Accuracy 0.108%\n",
      "Epoch 4, Batch 610, LR 0.881726 Loss 19.066342, Accuracy 0.108%\n",
      "Epoch 4, Batch 611, LR 0.882003 Loss 19.066417, Accuracy 0.107%\n",
      "Epoch 4, Batch 612, LR 0.882280 Loss 19.066317, Accuracy 0.107%\n",
      "Epoch 4, Batch 613, LR 0.882557 Loss 19.066508, Accuracy 0.107%\n",
      "Epoch 4, Batch 614, LR 0.882833 Loss 19.066399, Accuracy 0.108%\n",
      "Epoch 4, Batch 615, LR 0.883110 Loss 19.066369, Accuracy 0.108%\n",
      "Epoch 4, Batch 616, LR 0.883387 Loss 19.066431, Accuracy 0.108%\n",
      "Epoch 4, Batch 617, LR 0.883664 Loss 19.066146, Accuracy 0.109%\n",
      "Epoch 4, Batch 618, LR 0.883941 Loss 19.066288, Accuracy 0.109%\n",
      "Epoch 4, Batch 619, LR 0.884218 Loss 19.066247, Accuracy 0.109%\n",
      "Epoch 4, Batch 620, LR 0.884495 Loss 19.066167, Accuracy 0.108%\n",
      "Epoch 4, Batch 621, LR 0.884772 Loss 19.066295, Accuracy 0.109%\n",
      "Epoch 4, Batch 622, LR 0.885049 Loss 19.066044, Accuracy 0.109%\n",
      "Epoch 4, Batch 623, LR 0.885326 Loss 19.066049, Accuracy 0.109%\n",
      "Epoch 4, Batch 624, LR 0.885603 Loss 19.066463, Accuracy 0.111%\n",
      "Epoch 4, Batch 625, LR 0.885880 Loss 19.066719, Accuracy 0.111%\n",
      "Epoch 4, Batch 626, LR 0.886157 Loss 19.066687, Accuracy 0.111%\n",
      "Epoch 4, Batch 627, LR 0.886434 Loss 19.066646, Accuracy 0.111%\n",
      "Epoch 4, Batch 628, LR 0.886712 Loss 19.066615, Accuracy 0.111%\n",
      "Epoch 4, Batch 629, LR 0.886989 Loss 19.066924, Accuracy 0.111%\n",
      "Epoch 4, Batch 630, LR 0.887266 Loss 19.067119, Accuracy 0.110%\n",
      "Epoch 4, Batch 631, LR 0.887543 Loss 19.067045, Accuracy 0.110%\n",
      "Epoch 4, Batch 632, LR 0.887821 Loss 19.067091, Accuracy 0.110%\n",
      "Epoch 4, Batch 633, LR 0.888098 Loss 19.066859, Accuracy 0.110%\n",
      "Epoch 4, Batch 634, LR 0.888375 Loss 19.066845, Accuracy 0.110%\n",
      "Epoch 4, Batch 635, LR 0.888653 Loss 19.066731, Accuracy 0.109%\n",
      "Epoch 4, Batch 636, LR 0.888930 Loss 19.067254, Accuracy 0.109%\n",
      "Epoch 4, Batch 637, LR 0.889208 Loss 19.067540, Accuracy 0.109%\n",
      "Epoch 4, Batch 638, LR 0.889485 Loss 19.067385, Accuracy 0.109%\n",
      "Epoch 4, Batch 639, LR 0.889763 Loss 19.067257, Accuracy 0.109%\n",
      "Epoch 4, Batch 640, LR 0.890040 Loss 19.067307, Accuracy 0.110%\n",
      "Epoch 4, Batch 641, LR 0.890318 Loss 19.067321, Accuracy 0.110%\n",
      "Epoch 4, Batch 642, LR 0.890595 Loss 19.067319, Accuracy 0.110%\n",
      "Epoch 4, Batch 643, LR 0.890873 Loss 19.067707, Accuracy 0.109%\n",
      "Epoch 4, Batch 644, LR 0.891150 Loss 19.067292, Accuracy 0.109%\n",
      "Epoch 4, Batch 645, LR 0.891428 Loss 19.067557, Accuracy 0.109%\n",
      "Epoch 4, Batch 646, LR 0.891706 Loss 19.067616, Accuracy 0.109%\n",
      "Epoch 4, Batch 647, LR 0.891983 Loss 19.067541, Accuracy 0.110%\n",
      "Epoch 4, Batch 648, LR 0.892261 Loss 19.067680, Accuracy 0.110%\n",
      "Epoch 4, Batch 649, LR 0.892539 Loss 19.067325, Accuracy 0.110%\n",
      "Epoch 4, Batch 650, LR 0.892817 Loss 19.067290, Accuracy 0.109%\n",
      "Epoch 4, Batch 651, LR 0.893094 Loss 19.067659, Accuracy 0.109%\n",
      "Epoch 4, Batch 652, LR 0.893372 Loss 19.067614, Accuracy 0.109%\n",
      "Epoch 4, Batch 653, LR 0.893650 Loss 19.067372, Accuracy 0.109%\n",
      "Epoch 4, Batch 654, LR 0.893928 Loss 19.067219, Accuracy 0.109%\n",
      "Epoch 4, Batch 655, LR 0.894206 Loss 19.067557, Accuracy 0.109%\n",
      "Epoch 4, Batch 656, LR 0.894484 Loss 19.067249, Accuracy 0.108%\n",
      "Epoch 4, Batch 657, LR 0.894762 Loss 19.067138, Accuracy 0.108%\n",
      "Epoch 4, Batch 658, LR 0.895040 Loss 19.067200, Accuracy 0.108%\n",
      "Epoch 4, Batch 659, LR 0.895318 Loss 19.067184, Accuracy 0.109%\n",
      "Epoch 4, Batch 660, LR 0.895596 Loss 19.067068, Accuracy 0.109%\n",
      "Epoch 4, Batch 661, LR 0.895874 Loss 19.067294, Accuracy 0.109%\n",
      "Epoch 4, Batch 662, LR 0.896152 Loss 19.067407, Accuracy 0.109%\n",
      "Epoch 4, Batch 663, LR 0.896430 Loss 19.067887, Accuracy 0.108%\n",
      "Epoch 4, Batch 664, LR 0.896708 Loss 19.067946, Accuracy 0.108%\n",
      "Epoch 4, Batch 665, LR 0.896986 Loss 19.068080, Accuracy 0.108%\n",
      "Epoch 4, Batch 666, LR 0.897265 Loss 19.067854, Accuracy 0.108%\n",
      "Epoch 4, Batch 667, LR 0.897543 Loss 19.067982, Accuracy 0.108%\n",
      "Epoch 4, Batch 668, LR 0.897821 Loss 19.067874, Accuracy 0.108%\n",
      "Epoch 4, Batch 669, LR 0.898099 Loss 19.068039, Accuracy 0.107%\n",
      "Epoch 4, Batch 670, LR 0.898378 Loss 19.068172, Accuracy 0.107%\n",
      "Epoch 4, Batch 671, LR 0.898656 Loss 19.067878, Accuracy 0.108%\n",
      "Epoch 4, Batch 672, LR 0.898934 Loss 19.068111, Accuracy 0.108%\n",
      "Epoch 4, Batch 673, LR 0.899213 Loss 19.067935, Accuracy 0.108%\n",
      "Epoch 4, Batch 674, LR 0.899491 Loss 19.067670, Accuracy 0.108%\n",
      "Epoch 4, Batch 675, LR 0.899770 Loss 19.067593, Accuracy 0.108%\n",
      "Epoch 4, Batch 676, LR 0.900048 Loss 19.067556, Accuracy 0.107%\n",
      "Epoch 4, Batch 677, LR 0.900327 Loss 19.067680, Accuracy 0.107%\n",
      "Epoch 4, Batch 678, LR 0.900605 Loss 19.067594, Accuracy 0.107%\n",
      "Epoch 4, Batch 679, LR 0.900884 Loss 19.067798, Accuracy 0.107%\n",
      "Epoch 4, Batch 680, LR 0.901162 Loss 19.067768, Accuracy 0.107%\n",
      "Epoch 4, Batch 681, LR 0.901441 Loss 19.067924, Accuracy 0.107%\n",
      "Epoch 4, Batch 682, LR 0.901719 Loss 19.068148, Accuracy 0.107%\n",
      "Epoch 4, Batch 683, LR 0.901998 Loss 19.068179, Accuracy 0.106%\n",
      "Epoch 4, Batch 684, LR 0.902277 Loss 19.067885, Accuracy 0.106%\n",
      "Epoch 4, Batch 685, LR 0.902555 Loss 19.067941, Accuracy 0.106%\n",
      "Epoch 4, Batch 686, LR 0.902834 Loss 19.067800, Accuracy 0.106%\n",
      "Epoch 4, Batch 687, LR 0.903113 Loss 19.068080, Accuracy 0.106%\n",
      "Epoch 4, Batch 688, LR 0.903392 Loss 19.068211, Accuracy 0.106%\n",
      "Epoch 4, Batch 689, LR 0.903670 Loss 19.068293, Accuracy 0.105%\n",
      "Epoch 4, Batch 690, LR 0.903949 Loss 19.068534, Accuracy 0.105%\n",
      "Epoch 4, Batch 691, LR 0.904228 Loss 19.068410, Accuracy 0.105%\n",
      "Epoch 4, Batch 692, LR 0.904507 Loss 19.068248, Accuracy 0.105%\n",
      "Epoch 4, Batch 693, LR 0.904786 Loss 19.068288, Accuracy 0.105%\n",
      "Epoch 4, Batch 694, LR 0.905065 Loss 19.068702, Accuracy 0.105%\n",
      "Epoch 4, Batch 695, LR 0.905344 Loss 19.068535, Accuracy 0.105%\n",
      "Epoch 4, Batch 696, LR 0.905623 Loss 19.068631, Accuracy 0.104%\n",
      "Epoch 4, Batch 697, LR 0.905902 Loss 19.068734, Accuracy 0.104%\n",
      "Epoch 4, Batch 698, LR 0.906181 Loss 19.068769, Accuracy 0.104%\n",
      "Epoch 4, Batch 699, LR 0.906460 Loss 19.068574, Accuracy 0.104%\n",
      "Epoch 4, Batch 700, LR 0.906739 Loss 19.068551, Accuracy 0.104%\n",
      "Epoch 4, Batch 701, LR 0.907018 Loss 19.068427, Accuracy 0.104%\n",
      "Epoch 4, Batch 702, LR 0.907297 Loss 19.068370, Accuracy 0.103%\n",
      "Epoch 4, Batch 703, LR 0.907576 Loss 19.068308, Accuracy 0.103%\n",
      "Epoch 4, Batch 704, LR 0.907855 Loss 19.068399, Accuracy 0.103%\n",
      "Epoch 4, Batch 705, LR 0.908134 Loss 19.068448, Accuracy 0.103%\n",
      "Epoch 4, Batch 706, LR 0.908414 Loss 19.068605, Accuracy 0.103%\n",
      "Epoch 4, Batch 707, LR 0.908693 Loss 19.068519, Accuracy 0.103%\n",
      "Epoch 4, Batch 708, LR 0.908972 Loss 19.068481, Accuracy 0.103%\n",
      "Epoch 4, Batch 709, LR 0.909251 Loss 19.068269, Accuracy 0.102%\n",
      "Epoch 4, Batch 710, LR 0.909531 Loss 19.067910, Accuracy 0.102%\n",
      "Epoch 4, Batch 711, LR 0.909810 Loss 19.068002, Accuracy 0.102%\n",
      "Epoch 4, Batch 712, LR 0.910089 Loss 19.067603, Accuracy 0.102%\n",
      "Epoch 4, Batch 713, LR 0.910369 Loss 19.067435, Accuracy 0.102%\n",
      "Epoch 4, Batch 714, LR 0.910648 Loss 19.067694, Accuracy 0.102%\n",
      "Epoch 4, Batch 715, LR 0.910928 Loss 19.067466, Accuracy 0.102%\n",
      "Epoch 4, Batch 716, LR 0.911207 Loss 19.067261, Accuracy 0.101%\n",
      "Epoch 4, Batch 717, LR 0.911487 Loss 19.067320, Accuracy 0.101%\n",
      "Epoch 4, Batch 718, LR 0.911766 Loss 19.067416, Accuracy 0.101%\n",
      "Epoch 4, Batch 719, LR 0.912046 Loss 19.067142, Accuracy 0.101%\n",
      "Epoch 4, Batch 720, LR 0.912325 Loss 19.067036, Accuracy 0.101%\n",
      "Epoch 4, Batch 721, LR 0.912605 Loss 19.066997, Accuracy 0.102%\n",
      "Epoch 4, Batch 722, LR 0.912885 Loss 19.066978, Accuracy 0.102%\n",
      "Epoch 4, Batch 723, LR 0.913164 Loss 19.066994, Accuracy 0.102%\n",
      "Epoch 4, Batch 724, LR 0.913444 Loss 19.066972, Accuracy 0.101%\n",
      "Epoch 4, Batch 725, LR 0.913724 Loss 19.067023, Accuracy 0.102%\n",
      "Epoch 4, Batch 726, LR 0.914003 Loss 19.066980, Accuracy 0.102%\n",
      "Epoch 4, Batch 727, LR 0.914283 Loss 19.066776, Accuracy 0.102%\n",
      "Epoch 4, Batch 728, LR 0.914563 Loss 19.066870, Accuracy 0.102%\n",
      "Epoch 4, Batch 729, LR 0.914843 Loss 19.066749, Accuracy 0.102%\n",
      "Epoch 4, Batch 730, LR 0.915122 Loss 19.066743, Accuracy 0.102%\n",
      "Epoch 4, Batch 731, LR 0.915402 Loss 19.066738, Accuracy 0.102%\n",
      "Epoch 4, Batch 732, LR 0.915682 Loss 19.066696, Accuracy 0.101%\n",
      "Epoch 4, Batch 733, LR 0.915962 Loss 19.066731, Accuracy 0.101%\n",
      "Epoch 4, Batch 734, LR 0.916242 Loss 19.066908, Accuracy 0.101%\n",
      "Epoch 4, Batch 735, LR 0.916522 Loss 19.066907, Accuracy 0.101%\n",
      "Epoch 4, Batch 736, LR 0.916802 Loss 19.066786, Accuracy 0.101%\n",
      "Epoch 4, Batch 737, LR 0.917082 Loss 19.066798, Accuracy 0.102%\n",
      "Epoch 4, Batch 738, LR 0.917362 Loss 19.066750, Accuracy 0.102%\n",
      "Epoch 4, Batch 739, LR 0.917642 Loss 19.066477, Accuracy 0.101%\n",
      "Epoch 4, Batch 740, LR 0.917922 Loss 19.066504, Accuracy 0.101%\n",
      "Epoch 4, Batch 741, LR 0.918202 Loss 19.066618, Accuracy 0.101%\n",
      "Epoch 4, Batch 742, LR 0.918482 Loss 19.066788, Accuracy 0.101%\n",
      "Epoch 4, Batch 743, LR 0.918762 Loss 19.066554, Accuracy 0.101%\n",
      "Epoch 4, Batch 744, LR 0.919043 Loss 19.066508, Accuracy 0.101%\n",
      "Epoch 4, Batch 745, LR 0.919323 Loss 19.066384, Accuracy 0.101%\n",
      "Epoch 4, Batch 746, LR 0.919603 Loss 19.066472, Accuracy 0.101%\n",
      "Epoch 4, Batch 747, LR 0.919883 Loss 19.066544, Accuracy 0.100%\n",
      "Epoch 4, Batch 748, LR 0.920163 Loss 19.066697, Accuracy 0.100%\n",
      "Epoch 4, Batch 749, LR 0.920444 Loss 19.066775, Accuracy 0.100%\n",
      "Epoch 4, Batch 750, LR 0.920724 Loss 19.066710, Accuracy 0.100%\n",
      "Epoch 4, Batch 751, LR 0.921004 Loss 19.066843, Accuracy 0.100%\n",
      "Epoch 4, Batch 752, LR 0.921285 Loss 19.066760, Accuracy 0.100%\n",
      "Epoch 4, Batch 753, LR 0.921565 Loss 19.066804, Accuracy 0.100%\n",
      "Epoch 4, Batch 754, LR 0.921846 Loss 19.066664, Accuracy 0.099%\n",
      "Epoch 4, Batch 755, LR 0.922126 Loss 19.066763, Accuracy 0.099%\n",
      "Epoch 4, Batch 756, LR 0.922406 Loss 19.066892, Accuracy 0.099%\n",
      "Epoch 4, Batch 757, LR 0.922687 Loss 19.066956, Accuracy 0.099%\n",
      "Epoch 4, Batch 758, LR 0.922967 Loss 19.067131, Accuracy 0.099%\n",
      "Epoch 4, Batch 759, LR 0.923248 Loss 19.067161, Accuracy 0.099%\n",
      "Epoch 4, Batch 760, LR 0.923529 Loss 19.067185, Accuracy 0.099%\n",
      "Epoch 4, Batch 761, LR 0.923809 Loss 19.067131, Accuracy 0.099%\n",
      "Epoch 4, Batch 762, LR 0.924090 Loss 19.067291, Accuracy 0.098%\n",
      "Epoch 4, Batch 763, LR 0.924370 Loss 19.067353, Accuracy 0.098%\n",
      "Epoch 4, Batch 764, LR 0.924651 Loss 19.067472, Accuracy 0.098%\n",
      "Epoch 4, Batch 765, LR 0.924932 Loss 19.067480, Accuracy 0.099%\n",
      "Epoch 4, Batch 766, LR 0.925212 Loss 19.067536, Accuracy 0.099%\n",
      "Epoch 4, Batch 767, LR 0.925493 Loss 19.067470, Accuracy 0.099%\n",
      "Epoch 4, Batch 768, LR 0.925774 Loss 19.067334, Accuracy 0.099%\n",
      "Epoch 4, Batch 769, LR 0.926055 Loss 19.067240, Accuracy 0.099%\n",
      "Epoch 4, Batch 770, LR 0.926336 Loss 19.067219, Accuracy 0.098%\n",
      "Epoch 4, Batch 771, LR 0.926616 Loss 19.067276, Accuracy 0.098%\n",
      "Epoch 4, Batch 772, LR 0.926897 Loss 19.067269, Accuracy 0.098%\n",
      "Epoch 4, Batch 773, LR 0.927178 Loss 19.067194, Accuracy 0.098%\n",
      "Epoch 4, Batch 774, LR 0.927459 Loss 19.067061, Accuracy 0.099%\n",
      "Epoch 4, Batch 775, LR 0.927740 Loss 19.067401, Accuracy 0.099%\n",
      "Epoch 4, Batch 776, LR 0.928021 Loss 19.067539, Accuracy 0.099%\n",
      "Epoch 4, Batch 777, LR 0.928302 Loss 19.067515, Accuracy 0.099%\n",
      "Epoch 4, Batch 778, LR 0.928583 Loss 19.067814, Accuracy 0.099%\n",
      "Epoch 4, Batch 779, LR 0.928864 Loss 19.067791, Accuracy 0.099%\n",
      "Epoch 4, Batch 780, LR 0.929145 Loss 19.067628, Accuracy 0.099%\n",
      "Epoch 4, Batch 781, LR 0.929426 Loss 19.067603, Accuracy 0.099%\n",
      "Epoch 4, Batch 782, LR 0.929707 Loss 19.067754, Accuracy 0.099%\n",
      "Epoch 4, Batch 783, LR 0.929988 Loss 19.067653, Accuracy 0.099%\n",
      "Epoch 4, Batch 784, LR 0.930269 Loss 19.067717, Accuracy 0.099%\n",
      "Epoch 4, Batch 785, LR 0.930550 Loss 19.067636, Accuracy 0.099%\n",
      "Epoch 4, Batch 786, LR 0.930832 Loss 19.067407, Accuracy 0.098%\n",
      "Epoch 4, Batch 787, LR 0.931113 Loss 19.067395, Accuracy 0.098%\n",
      "Epoch 4, Batch 788, LR 0.931394 Loss 19.067327, Accuracy 0.098%\n",
      "Epoch 4, Batch 789, LR 0.931675 Loss 19.067343, Accuracy 0.098%\n",
      "Epoch 4, Batch 790, LR 0.931957 Loss 19.067262, Accuracy 0.098%\n",
      "Epoch 4, Batch 791, LR 0.932238 Loss 19.067243, Accuracy 0.098%\n",
      "Epoch 4, Batch 792, LR 0.932519 Loss 19.067412, Accuracy 0.098%\n",
      "Epoch 4, Batch 793, LR 0.932801 Loss 19.067352, Accuracy 0.098%\n",
      "Epoch 4, Batch 794, LR 0.933082 Loss 19.067229, Accuracy 0.097%\n",
      "Epoch 4, Batch 795, LR 0.933363 Loss 19.067322, Accuracy 0.097%\n",
      "Epoch 4, Batch 796, LR 0.933645 Loss 19.067157, Accuracy 0.097%\n",
      "Epoch 4, Batch 797, LR 0.933926 Loss 19.066932, Accuracy 0.097%\n",
      "Epoch 4, Batch 798, LR 0.934208 Loss 19.066719, Accuracy 0.097%\n",
      "Epoch 4, Batch 799, LR 0.934489 Loss 19.066657, Accuracy 0.097%\n",
      "Epoch 4, Batch 800, LR 0.934771 Loss 19.066607, Accuracy 0.097%\n",
      "Epoch 4, Batch 801, LR 0.935052 Loss 19.066880, Accuracy 0.099%\n",
      "Epoch 4, Batch 802, LR 0.935334 Loss 19.067070, Accuracy 0.098%\n",
      "Epoch 4, Batch 803, LR 0.935615 Loss 19.067271, Accuracy 0.098%\n",
      "Epoch 4, Batch 804, LR 0.935897 Loss 19.067352, Accuracy 0.098%\n",
      "Epoch 4, Batch 805, LR 0.936179 Loss 19.067108, Accuracy 0.099%\n",
      "Epoch 4, Batch 806, LR 0.936460 Loss 19.067341, Accuracy 0.099%\n",
      "Epoch 4, Batch 807, LR 0.936742 Loss 19.067096, Accuracy 0.099%\n",
      "Epoch 4, Batch 808, LR 0.937024 Loss 19.067150, Accuracy 0.099%\n",
      "Epoch 4, Batch 809, LR 0.937305 Loss 19.067163, Accuracy 0.099%\n",
      "Epoch 4, Batch 810, LR 0.937587 Loss 19.067022, Accuracy 0.098%\n",
      "Epoch 4, Batch 811, LR 0.937869 Loss 19.066909, Accuracy 0.098%\n",
      "Epoch 4, Batch 812, LR 0.938151 Loss 19.066954, Accuracy 0.098%\n",
      "Epoch 4, Batch 813, LR 0.938433 Loss 19.066886, Accuracy 0.098%\n",
      "Epoch 4, Batch 814, LR 0.938715 Loss 19.067142, Accuracy 0.098%\n",
      "Epoch 4, Batch 815, LR 0.938996 Loss 19.067436, Accuracy 0.098%\n",
      "Epoch 4, Batch 816, LR 0.939278 Loss 19.067331, Accuracy 0.098%\n",
      "Epoch 4, Batch 817, LR 0.939560 Loss 19.067358, Accuracy 0.098%\n",
      "Epoch 4, Batch 818, LR 0.939842 Loss 19.067412, Accuracy 0.097%\n",
      "Epoch 4, Batch 819, LR 0.940124 Loss 19.067566, Accuracy 0.097%\n",
      "Epoch 4, Batch 820, LR 0.940406 Loss 19.067687, Accuracy 0.097%\n",
      "Epoch 4, Batch 821, LR 0.940688 Loss 19.067834, Accuracy 0.097%\n",
      "Epoch 4, Batch 822, LR 0.940970 Loss 19.067592, Accuracy 0.097%\n",
      "Epoch 4, Batch 823, LR 0.941252 Loss 19.067477, Accuracy 0.097%\n",
      "Epoch 4, Batch 824, LR 0.941534 Loss 19.067245, Accuracy 0.097%\n",
      "Epoch 4, Batch 825, LR 0.941816 Loss 19.067220, Accuracy 0.097%\n",
      "Epoch 4, Batch 826, LR 0.942099 Loss 19.067227, Accuracy 0.096%\n",
      "Epoch 4, Batch 827, LR 0.942381 Loss 19.066977, Accuracy 0.096%\n",
      "Epoch 4, Batch 828, LR 0.942663 Loss 19.066925, Accuracy 0.096%\n",
      "Epoch 4, Batch 829, LR 0.942945 Loss 19.066852, Accuracy 0.096%\n",
      "Epoch 4, Batch 830, LR 0.943227 Loss 19.066964, Accuracy 0.096%\n",
      "Epoch 4, Batch 831, LR 0.943509 Loss 19.067121, Accuracy 0.096%\n",
      "Epoch 4, Batch 832, LR 0.943792 Loss 19.066858, Accuracy 0.096%\n",
      "Epoch 4, Batch 833, LR 0.944074 Loss 19.066935, Accuracy 0.096%\n",
      "Epoch 4, Batch 834, LR 0.944356 Loss 19.067064, Accuracy 0.096%\n",
      "Epoch 4, Batch 835, LR 0.944639 Loss 19.066880, Accuracy 0.095%\n",
      "Epoch 4, Batch 836, LR 0.944921 Loss 19.066975, Accuracy 0.096%\n",
      "Epoch 4, Batch 837, LR 0.945203 Loss 19.066881, Accuracy 0.096%\n",
      "Epoch 4, Batch 838, LR 0.945486 Loss 19.066839, Accuracy 0.096%\n",
      "Epoch 4, Batch 839, LR 0.945768 Loss 19.066676, Accuracy 0.096%\n",
      "Epoch 4, Batch 840, LR 0.946051 Loss 19.066775, Accuracy 0.096%\n",
      "Epoch 4, Batch 841, LR 0.946333 Loss 19.066756, Accuracy 0.096%\n",
      "Epoch 4, Batch 842, LR 0.946616 Loss 19.066718, Accuracy 0.096%\n",
      "Epoch 4, Batch 843, LR 0.946898 Loss 19.066828, Accuracy 0.095%\n",
      "Epoch 4, Batch 844, LR 0.947181 Loss 19.066745, Accuracy 0.095%\n",
      "Epoch 4, Batch 845, LR 0.947463 Loss 19.066814, Accuracy 0.095%\n",
      "Epoch 4, Batch 846, LR 0.947746 Loss 19.066721, Accuracy 0.095%\n",
      "Epoch 4, Batch 847, LR 0.948029 Loss 19.066578, Accuracy 0.095%\n",
      "Epoch 4, Batch 848, LR 0.948311 Loss 19.066494, Accuracy 0.095%\n",
      "Epoch 4, Batch 849, LR 0.948594 Loss 19.066642, Accuracy 0.095%\n",
      "Epoch 4, Batch 850, LR 0.948876 Loss 19.066353, Accuracy 0.095%\n",
      "Epoch 4, Batch 851, LR 0.949159 Loss 19.066358, Accuracy 0.095%\n",
      "Epoch 4, Batch 852, LR 0.949442 Loss 19.066393, Accuracy 0.094%\n",
      "Epoch 4, Batch 853, LR 0.949725 Loss 19.066477, Accuracy 0.094%\n",
      "Epoch 4, Batch 854, LR 0.950007 Loss 19.066337, Accuracy 0.095%\n",
      "Epoch 4, Batch 855, LR 0.950290 Loss 19.066206, Accuracy 0.095%\n",
      "Epoch 4, Batch 856, LR 0.950573 Loss 19.066311, Accuracy 0.095%\n",
      "Epoch 4, Batch 857, LR 0.950856 Loss 19.066579, Accuracy 0.095%\n",
      "Epoch 4, Batch 858, LR 0.951139 Loss 19.066665, Accuracy 0.095%\n",
      "Epoch 4, Batch 859, LR 0.951422 Loss 19.066655, Accuracy 0.095%\n",
      "Epoch 4, Batch 860, LR 0.951705 Loss 19.066654, Accuracy 0.094%\n",
      "Epoch 4, Batch 861, LR 0.951987 Loss 19.066707, Accuracy 0.095%\n",
      "Epoch 4, Batch 862, LR 0.952270 Loss 19.066648, Accuracy 0.095%\n",
      "Epoch 4, Batch 863, LR 0.952553 Loss 19.066577, Accuracy 0.095%\n",
      "Epoch 4, Batch 864, LR 0.952836 Loss 19.066376, Accuracy 0.095%\n",
      "Epoch 4, Batch 865, LR 0.953119 Loss 19.066559, Accuracy 0.095%\n",
      "Epoch 4, Batch 866, LR 0.953402 Loss 19.066782, Accuracy 0.095%\n",
      "Epoch 4, Batch 867, LR 0.953685 Loss 19.066530, Accuracy 0.095%\n",
      "Epoch 4, Batch 868, LR 0.953969 Loss 19.066807, Accuracy 0.095%\n",
      "Epoch 4, Batch 869, LR 0.954252 Loss 19.066647, Accuracy 0.094%\n",
      "Epoch 4, Batch 870, LR 0.954535 Loss 19.066680, Accuracy 0.094%\n",
      "Epoch 4, Batch 871, LR 0.954818 Loss 19.066869, Accuracy 0.094%\n",
      "Epoch 4, Batch 872, LR 0.955101 Loss 19.066972, Accuracy 0.094%\n",
      "Epoch 4, Batch 873, LR 0.955384 Loss 19.066924, Accuracy 0.094%\n",
      "Epoch 4, Batch 874, LR 0.955668 Loss 19.066813, Accuracy 0.094%\n",
      "Epoch 4, Batch 875, LR 0.955951 Loss 19.066841, Accuracy 0.094%\n",
      "Epoch 4, Batch 876, LR 0.956234 Loss 19.066632, Accuracy 0.095%\n",
      "Epoch 4, Batch 877, LR 0.956517 Loss 19.066640, Accuracy 0.094%\n",
      "Epoch 4, Batch 878, LR 0.956801 Loss 19.066761, Accuracy 0.094%\n",
      "Epoch 4, Batch 879, LR 0.957084 Loss 19.066637, Accuracy 0.094%\n",
      "Epoch 4, Batch 880, LR 0.957367 Loss 19.066433, Accuracy 0.094%\n",
      "Epoch 4, Batch 881, LR 0.957651 Loss 19.066458, Accuracy 0.094%\n",
      "Epoch 4, Batch 882, LR 0.957934 Loss 19.066601, Accuracy 0.094%\n",
      "Epoch 4, Batch 883, LR 0.958218 Loss 19.066803, Accuracy 0.095%\n",
      "Epoch 4, Batch 884, LR 0.958501 Loss 19.066907, Accuracy 0.095%\n",
      "Epoch 4, Batch 885, LR 0.958784 Loss 19.067042, Accuracy 0.094%\n",
      "Epoch 4, Batch 886, LR 0.959068 Loss 19.067156, Accuracy 0.094%\n",
      "Epoch 4, Batch 887, LR 0.959351 Loss 19.067266, Accuracy 0.095%\n",
      "Epoch 4, Batch 888, LR 0.959635 Loss 19.067286, Accuracy 0.095%\n",
      "Epoch 4, Batch 889, LR 0.959919 Loss 19.067193, Accuracy 0.097%\n",
      "Epoch 4, Batch 890, LR 0.960202 Loss 19.067151, Accuracy 0.097%\n",
      "Epoch 4, Batch 891, LR 0.960486 Loss 19.067113, Accuracy 0.096%\n",
      "Epoch 4, Batch 892, LR 0.960769 Loss 19.067044, Accuracy 0.096%\n",
      "Epoch 4, Batch 893, LR 0.961053 Loss 19.067119, Accuracy 0.096%\n",
      "Epoch 4, Batch 894, LR 0.961337 Loss 19.067060, Accuracy 0.096%\n",
      "Epoch 4, Batch 895, LR 0.961620 Loss 19.066910, Accuracy 0.096%\n",
      "Epoch 4, Batch 896, LR 0.961904 Loss 19.066928, Accuracy 0.096%\n",
      "Epoch 4, Batch 897, LR 0.962188 Loss 19.067177, Accuracy 0.096%\n",
      "Epoch 4, Batch 898, LR 0.962471 Loss 19.067096, Accuracy 0.096%\n",
      "Epoch 4, Batch 899, LR 0.962755 Loss 19.067218, Accuracy 0.096%\n",
      "Epoch 4, Batch 900, LR 0.963039 Loss 19.067132, Accuracy 0.095%\n",
      "Epoch 4, Batch 901, LR 0.963323 Loss 19.067071, Accuracy 0.095%\n",
      "Epoch 4, Batch 902, LR 0.963607 Loss 19.066998, Accuracy 0.095%\n",
      "Epoch 4, Batch 903, LR 0.963891 Loss 19.067105, Accuracy 0.095%\n",
      "Epoch 4, Batch 904, LR 0.964174 Loss 19.067035, Accuracy 0.095%\n",
      "Epoch 4, Batch 905, LR 0.964458 Loss 19.067022, Accuracy 0.095%\n",
      "Epoch 4, Batch 906, LR 0.964742 Loss 19.066974, Accuracy 0.095%\n",
      "Epoch 4, Batch 907, LR 0.965026 Loss 19.066848, Accuracy 0.095%\n",
      "Epoch 4, Batch 908, LR 0.965310 Loss 19.066793, Accuracy 0.095%\n",
      "Epoch 4, Batch 909, LR 0.965594 Loss 19.066634, Accuracy 0.095%\n",
      "Epoch 4, Batch 910, LR 0.965878 Loss 19.066931, Accuracy 0.095%\n",
      "Epoch 4, Batch 911, LR 0.966162 Loss 19.066942, Accuracy 0.095%\n",
      "Epoch 4, Batch 912, LR 0.966446 Loss 19.066898, Accuracy 0.095%\n",
      "Epoch 4, Batch 913, LR 0.966730 Loss 19.066879, Accuracy 0.095%\n",
      "Epoch 4, Batch 914, LR 0.967014 Loss 19.066768, Accuracy 0.095%\n",
      "Epoch 4, Batch 915, LR 0.967299 Loss 19.066871, Accuracy 0.095%\n",
      "Epoch 4, Batch 916, LR 0.967583 Loss 19.066715, Accuracy 0.095%\n",
      "Epoch 4, Batch 917, LR 0.967867 Loss 19.066745, Accuracy 0.095%\n",
      "Epoch 4, Batch 918, LR 0.968151 Loss 19.066974, Accuracy 0.094%\n",
      "Epoch 4, Batch 919, LR 0.968435 Loss 19.067156, Accuracy 0.094%\n",
      "Epoch 4, Batch 920, LR 0.968719 Loss 19.067070, Accuracy 0.094%\n",
      "Epoch 4, Batch 921, LR 0.969004 Loss 19.067241, Accuracy 0.094%\n",
      "Epoch 4, Batch 922, LR 0.969288 Loss 19.067103, Accuracy 0.095%\n",
      "Epoch 4, Batch 923, LR 0.969572 Loss 19.066865, Accuracy 0.095%\n",
      "Epoch 4, Batch 924, LR 0.969857 Loss 19.066648, Accuracy 0.095%\n",
      "Epoch 4, Batch 925, LR 0.970141 Loss 19.066701, Accuracy 0.095%\n",
      "Epoch 4, Batch 926, LR 0.970425 Loss 19.066509, Accuracy 0.094%\n",
      "Epoch 4, Batch 927, LR 0.970710 Loss 19.066619, Accuracy 0.094%\n",
      "Epoch 4, Batch 928, LR 0.970994 Loss 19.066559, Accuracy 0.094%\n",
      "Epoch 4, Batch 929, LR 0.971279 Loss 19.066824, Accuracy 0.094%\n",
      "Epoch 4, Batch 930, LR 0.971563 Loss 19.066871, Accuracy 0.094%\n",
      "Epoch 4, Batch 931, LR 0.971847 Loss 19.066928, Accuracy 0.094%\n",
      "Epoch 4, Batch 932, LR 0.972132 Loss 19.066993, Accuracy 0.094%\n",
      "Epoch 4, Batch 933, LR 0.972416 Loss 19.067198, Accuracy 0.094%\n",
      "Epoch 4, Batch 934, LR 0.972701 Loss 19.066955, Accuracy 0.094%\n",
      "Epoch 4, Batch 935, LR 0.972985 Loss 19.066937, Accuracy 0.094%\n",
      "Epoch 4, Batch 936, LR 0.973270 Loss 19.066746, Accuracy 0.093%\n",
      "Epoch 4, Batch 937, LR 0.973555 Loss 19.066586, Accuracy 0.093%\n",
      "Epoch 4, Batch 938, LR 0.973839 Loss 19.066652, Accuracy 0.093%\n",
      "Epoch 4, Batch 939, LR 0.974124 Loss 19.066453, Accuracy 0.093%\n",
      "Epoch 4, Batch 940, LR 0.974409 Loss 19.066412, Accuracy 0.093%\n",
      "Epoch 4, Batch 941, LR 0.974693 Loss 19.066368, Accuracy 0.093%\n",
      "Epoch 4, Batch 942, LR 0.974978 Loss 19.066344, Accuracy 0.093%\n",
      "Epoch 4, Batch 943, LR 0.975263 Loss 19.066331, Accuracy 0.093%\n",
      "Epoch 4, Batch 944, LR 0.975547 Loss 19.066307, Accuracy 0.093%\n",
      "Epoch 4, Batch 945, LR 0.975832 Loss 19.066440, Accuracy 0.093%\n",
      "Epoch 4, Batch 946, LR 0.976117 Loss 19.066480, Accuracy 0.093%\n",
      "Epoch 4, Batch 947, LR 0.976402 Loss 19.066610, Accuracy 0.093%\n",
      "Epoch 4, Batch 948, LR 0.976687 Loss 19.066443, Accuracy 0.093%\n",
      "Epoch 4, Batch 949, LR 0.976971 Loss 19.066267, Accuracy 0.094%\n",
      "Epoch 4, Batch 950, LR 0.977256 Loss 19.066096, Accuracy 0.094%\n",
      "Epoch 4, Batch 951, LR 0.977541 Loss 19.066023, Accuracy 0.094%\n",
      "Epoch 4, Batch 952, LR 0.977826 Loss 19.065907, Accuracy 0.094%\n",
      "Epoch 4, Batch 953, LR 0.978111 Loss 19.065913, Accuracy 0.093%\n",
      "Epoch 4, Batch 954, LR 0.978396 Loss 19.065785, Accuracy 0.094%\n",
      "Epoch 4, Batch 955, LR 0.978681 Loss 19.065907, Accuracy 0.094%\n",
      "Epoch 4, Batch 956, LR 0.978966 Loss 19.065988, Accuracy 0.094%\n",
      "Epoch 4, Batch 957, LR 0.979251 Loss 19.066052, Accuracy 0.094%\n",
      "Epoch 4, Batch 958, LR 0.979536 Loss 19.065947, Accuracy 0.094%\n",
      "Epoch 4, Batch 959, LR 0.979821 Loss 19.065972, Accuracy 0.094%\n",
      "Epoch 4, Batch 960, LR 0.980106 Loss 19.065955, Accuracy 0.094%\n",
      "Epoch 4, Batch 961, LR 0.980391 Loss 19.065955, Accuracy 0.093%\n",
      "Epoch 4, Batch 962, LR 0.980676 Loss 19.066048, Accuracy 0.093%\n",
      "Epoch 4, Batch 963, LR 0.980961 Loss 19.066001, Accuracy 0.093%\n",
      "Epoch 4, Batch 964, LR 0.981247 Loss 19.065883, Accuracy 0.093%\n",
      "Epoch 4, Batch 965, LR 0.981532 Loss 19.065955, Accuracy 0.093%\n",
      "Epoch 4, Batch 966, LR 0.981817 Loss 19.065936, Accuracy 0.093%\n",
      "Epoch 4, Batch 967, LR 0.982102 Loss 19.065899, Accuracy 0.093%\n",
      "Epoch 4, Batch 968, LR 0.982387 Loss 19.065775, Accuracy 0.093%\n",
      "Epoch 4, Batch 969, LR 0.982673 Loss 19.065827, Accuracy 0.093%\n",
      "Epoch 4, Batch 970, LR 0.982958 Loss 19.065867, Accuracy 0.093%\n",
      "Epoch 4, Batch 971, LR 0.983243 Loss 19.065923, Accuracy 0.093%\n",
      "Epoch 4, Batch 972, LR 0.983529 Loss 19.066106, Accuracy 0.092%\n",
      "Epoch 4, Batch 973, LR 0.983814 Loss 19.066047, Accuracy 0.092%\n",
      "Epoch 4, Batch 974, LR 0.984099 Loss 19.065971, Accuracy 0.092%\n",
      "Epoch 4, Batch 975, LR 0.984385 Loss 19.065929, Accuracy 0.092%\n",
      "Epoch 4, Batch 976, LR 0.984670 Loss 19.066035, Accuracy 0.092%\n",
      "Epoch 4, Batch 977, LR 0.984955 Loss 19.066065, Accuracy 0.092%\n",
      "Epoch 4, Batch 978, LR 0.985241 Loss 19.066242, Accuracy 0.092%\n",
      "Epoch 4, Batch 979, LR 0.985526 Loss 19.066297, Accuracy 0.092%\n",
      "Epoch 4, Batch 980, LR 0.985812 Loss 19.066183, Accuracy 0.092%\n",
      "Epoch 4, Batch 981, LR 0.986097 Loss 19.066245, Accuracy 0.092%\n",
      "Epoch 4, Batch 982, LR 0.986383 Loss 19.066110, Accuracy 0.091%\n",
      "Epoch 4, Batch 983, LR 0.986668 Loss 19.065878, Accuracy 0.093%\n",
      "Epoch 4, Batch 984, LR 0.986954 Loss 19.065812, Accuracy 0.093%\n",
      "Epoch 4, Batch 985, LR 0.987240 Loss 19.065846, Accuracy 0.093%\n",
      "Epoch 4, Batch 986, LR 0.987525 Loss 19.065823, Accuracy 0.093%\n",
      "Epoch 4, Batch 987, LR 0.987811 Loss 19.065670, Accuracy 0.093%\n",
      "Epoch 4, Batch 988, LR 0.988096 Loss 19.065614, Accuracy 0.093%\n",
      "Epoch 4, Batch 989, LR 0.988382 Loss 19.065643, Accuracy 0.092%\n",
      "Epoch 4, Batch 990, LR 0.988668 Loss 19.065462, Accuracy 0.092%\n",
      "Epoch 4, Batch 991, LR 0.988954 Loss 19.065436, Accuracy 0.092%\n",
      "Epoch 4, Batch 992, LR 0.989239 Loss 19.065250, Accuracy 0.092%\n",
      "Epoch 4, Batch 993, LR 0.989525 Loss 19.065154, Accuracy 0.092%\n",
      "Epoch 4, Batch 994, LR 0.989811 Loss 19.064993, Accuracy 0.092%\n",
      "Epoch 4, Batch 995, LR 0.990097 Loss 19.064769, Accuracy 0.092%\n",
      "Epoch 4, Batch 996, LR 0.990382 Loss 19.064811, Accuracy 0.092%\n",
      "Epoch 4, Batch 997, LR 0.990668 Loss 19.064977, Accuracy 0.092%\n",
      "Epoch 4, Batch 998, LR 0.990954 Loss 19.065061, Accuracy 0.092%\n",
      "Epoch 4, Batch 999, LR 0.991240 Loss 19.064998, Accuracy 0.091%\n",
      "Epoch 4, Batch 1000, LR 0.991526 Loss 19.064936, Accuracy 0.091%\n",
      "Epoch 4, Batch 1001, LR 0.991812 Loss 19.065006, Accuracy 0.092%\n",
      "Epoch 4, Batch 1002, LR 0.992098 Loss 19.064970, Accuracy 0.092%\n",
      "Epoch 4, Batch 1003, LR 0.992384 Loss 19.064966, Accuracy 0.093%\n",
      "Epoch 4, Batch 1004, LR 0.992670 Loss 19.064933, Accuracy 0.093%\n",
      "Epoch 4, Batch 1005, LR 0.992956 Loss 19.064905, Accuracy 0.093%\n",
      "Epoch 4, Batch 1006, LR 0.993242 Loss 19.064943, Accuracy 0.093%\n",
      "Epoch 4, Batch 1007, LR 0.993528 Loss 19.064755, Accuracy 0.094%\n",
      "Epoch 4, Batch 1008, LR 0.993814 Loss 19.064903, Accuracy 0.094%\n",
      "Epoch 4, Batch 1009, LR 0.994100 Loss 19.064866, Accuracy 0.094%\n",
      "Epoch 4, Batch 1010, LR 0.994386 Loss 19.065012, Accuracy 0.094%\n",
      "Epoch 4, Batch 1011, LR 0.994672 Loss 19.065105, Accuracy 0.094%\n",
      "Epoch 4, Batch 1012, LR 0.994958 Loss 19.065101, Accuracy 0.093%\n",
      "Epoch 4, Batch 1013, LR 0.995244 Loss 19.065185, Accuracy 0.093%\n",
      "Epoch 4, Batch 1014, LR 0.995530 Loss 19.065118, Accuracy 0.093%\n",
      "Epoch 4, Batch 1015, LR 0.995817 Loss 19.065317, Accuracy 0.093%\n",
      "Epoch 4, Batch 1016, LR 0.996103 Loss 19.065056, Accuracy 0.093%\n",
      "Epoch 4, Batch 1017, LR 0.996389 Loss 19.065135, Accuracy 0.093%\n",
      "Epoch 4, Batch 1018, LR 0.996675 Loss 19.065171, Accuracy 0.094%\n",
      "Epoch 4, Batch 1019, LR 0.996962 Loss 19.065035, Accuracy 0.094%\n",
      "Epoch 4, Batch 1020, LR 0.997248 Loss 19.065131, Accuracy 0.093%\n",
      "Epoch 4, Batch 1021, LR 0.997534 Loss 19.065199, Accuracy 0.093%\n",
      "Epoch 4, Batch 1022, LR 0.997820 Loss 19.065165, Accuracy 0.093%\n",
      "Epoch 4, Batch 1023, LR 0.998107 Loss 19.065012, Accuracy 0.093%\n",
      "Epoch 4, Batch 1024, LR 0.998393 Loss 19.065223, Accuracy 0.093%\n",
      "Epoch 4, Batch 1025, LR 0.998680 Loss 19.065117, Accuracy 0.093%\n",
      "Epoch 4, Batch 1026, LR 0.998966 Loss 19.065139, Accuracy 0.093%\n",
      "Epoch 4, Batch 1027, LR 0.999252 Loss 19.065272, Accuracy 0.093%\n",
      "Epoch 4, Batch 1028, LR 0.999539 Loss 19.065297, Accuracy 0.093%\n",
      "Epoch 4, Batch 1029, LR 0.999825 Loss 19.064967, Accuracy 0.093%\n",
      "Epoch 4, Batch 1030, LR 1.000112 Loss 19.064920, Accuracy 0.093%\n",
      "Epoch 4, Batch 1031, LR 1.000398 Loss 19.064972, Accuracy 0.092%\n",
      "Epoch 4, Batch 1032, LR 1.000685 Loss 19.064922, Accuracy 0.093%\n",
      "Epoch 4, Batch 1033, LR 1.000971 Loss 19.065096, Accuracy 0.093%\n",
      "Epoch 4, Batch 1034, LR 1.001258 Loss 19.065180, Accuracy 0.093%\n",
      "Epoch 4, Batch 1035, LR 1.001545 Loss 19.064977, Accuracy 0.094%\n",
      "Epoch 4, Batch 1036, LR 1.001831 Loss 19.065120, Accuracy 0.094%\n",
      "Epoch 4, Batch 1037, LR 1.002118 Loss 19.065121, Accuracy 0.094%\n",
      "Epoch 4, Batch 1038, LR 1.002404 Loss 19.065151, Accuracy 0.094%\n",
      "Epoch 4, Batch 1039, LR 1.002691 Loss 19.065290, Accuracy 0.094%\n",
      "Epoch 4, Batch 1040, LR 1.002978 Loss 19.065282, Accuracy 0.094%\n",
      "Epoch 4, Batch 1041, LR 1.003264 Loss 19.065177, Accuracy 0.094%\n",
      "Epoch 4, Batch 1042, LR 1.003551 Loss 19.065123, Accuracy 0.094%\n",
      "Epoch 4, Batch 1043, LR 1.003838 Loss 19.065148, Accuracy 0.094%\n",
      "Epoch 4, Batch 1044, LR 1.004125 Loss 19.064924, Accuracy 0.094%\n",
      "Epoch 4, Batch 1045, LR 1.004411 Loss 19.064820, Accuracy 0.093%\n",
      "Epoch 4, Batch 1046, LR 1.004698 Loss 19.064867, Accuracy 0.093%\n",
      "Epoch 4, Batch 1047, LR 1.004985 Loss 19.064684, Accuracy 0.093%\n",
      "Epoch 4, Loss (train set) 19.064684, Accuracy (train set) 0.093%\n",
      "Epoch 4, Accuracy (validation set) 0.103%\n",
      "Epoch 5, Batch 1, LR 1.005272 Loss 18.921770, Accuracy 0.000%\n",
      "Epoch 5, Batch 2, LR 1.005559 Loss 19.025977, Accuracy 0.391%\n",
      "Epoch 5, Batch 3, LR 1.005846 Loss 19.081444, Accuracy 0.260%\n",
      "Epoch 5, Batch 4, LR 1.006133 Loss 19.109064, Accuracy 0.195%\n",
      "Epoch 5, Batch 5, LR 1.006419 Loss 19.060238, Accuracy 0.156%\n",
      "Epoch 5, Batch 6, LR 1.006706 Loss 19.059851, Accuracy 0.130%\n",
      "Epoch 5, Batch 7, LR 1.006993 Loss 19.031155, Accuracy 0.112%\n",
      "Epoch 5, Batch 8, LR 1.007280 Loss 19.060536, Accuracy 0.098%\n",
      "Epoch 5, Batch 9, LR 1.007567 Loss 19.054042, Accuracy 0.087%\n",
      "Epoch 5, Batch 10, LR 1.007854 Loss 19.076816, Accuracy 0.078%\n",
      "Epoch 5, Batch 11, LR 1.008141 Loss 19.073153, Accuracy 0.142%\n",
      "Epoch 5, Batch 12, LR 1.008428 Loss 19.080129, Accuracy 0.130%\n",
      "Epoch 5, Batch 13, LR 1.008715 Loss 19.071157, Accuracy 0.120%\n",
      "Epoch 5, Batch 14, LR 1.009002 Loss 19.071141, Accuracy 0.112%\n",
      "Epoch 5, Batch 15, LR 1.009290 Loss 19.077101, Accuracy 0.104%\n",
      "Epoch 5, Batch 16, LR 1.009577 Loss 19.073220, Accuracy 0.098%\n",
      "Epoch 5, Batch 17, LR 1.009864 Loss 19.075337, Accuracy 0.092%\n",
      "Epoch 5, Batch 18, LR 1.010151 Loss 19.067152, Accuracy 0.087%\n",
      "Epoch 5, Batch 19, LR 1.010438 Loss 19.066649, Accuracy 0.082%\n",
      "Epoch 5, Batch 20, LR 1.010725 Loss 19.065999, Accuracy 0.078%\n",
      "Epoch 5, Batch 21, LR 1.011013 Loss 19.062394, Accuracy 0.074%\n",
      "Epoch 5, Batch 22, LR 1.011300 Loss 19.059009, Accuracy 0.142%\n",
      "Epoch 5, Batch 23, LR 1.011587 Loss 19.057593, Accuracy 0.136%\n",
      "Epoch 5, Batch 24, LR 1.011874 Loss 19.063816, Accuracy 0.130%\n",
      "Epoch 5, Batch 25, LR 1.012162 Loss 19.064335, Accuracy 0.125%\n",
      "Epoch 5, Batch 26, LR 1.012449 Loss 19.062940, Accuracy 0.120%\n",
      "Epoch 5, Batch 27, LR 1.012736 Loss 19.068554, Accuracy 0.116%\n",
      "Epoch 5, Batch 28, LR 1.013024 Loss 19.071939, Accuracy 0.112%\n",
      "Epoch 5, Batch 29, LR 1.013311 Loss 19.069541, Accuracy 0.108%\n",
      "Epoch 5, Batch 30, LR 1.013598 Loss 19.068201, Accuracy 0.104%\n",
      "Epoch 5, Batch 31, LR 1.013886 Loss 19.063812, Accuracy 0.101%\n",
      "Epoch 5, Batch 32, LR 1.014173 Loss 19.065282, Accuracy 0.098%\n",
      "Epoch 5, Batch 33, LR 1.014461 Loss 19.057593, Accuracy 0.095%\n",
      "Epoch 5, Batch 34, LR 1.014748 Loss 19.054791, Accuracy 0.092%\n",
      "Epoch 5, Batch 35, LR 1.015036 Loss 19.053634, Accuracy 0.112%\n",
      "Epoch 5, Batch 36, LR 1.015323 Loss 19.057374, Accuracy 0.109%\n",
      "Epoch 5, Batch 37, LR 1.015611 Loss 19.055019, Accuracy 0.106%\n",
      "Epoch 5, Batch 38, LR 1.015898 Loss 19.054117, Accuracy 0.103%\n",
      "Epoch 5, Batch 39, LR 1.016186 Loss 19.056198, Accuracy 0.100%\n",
      "Epoch 5, Batch 40, LR 1.016473 Loss 19.060150, Accuracy 0.098%\n",
      "Epoch 5, Batch 41, LR 1.016761 Loss 19.064196, Accuracy 0.114%\n",
      "Epoch 5, Batch 42, LR 1.017048 Loss 19.059775, Accuracy 0.112%\n",
      "Epoch 5, Batch 43, LR 1.017336 Loss 19.054771, Accuracy 0.127%\n",
      "Epoch 5, Batch 44, LR 1.017624 Loss 19.056291, Accuracy 0.124%\n",
      "Epoch 5, Batch 45, LR 1.017911 Loss 19.057098, Accuracy 0.122%\n",
      "Epoch 5, Batch 46, LR 1.018199 Loss 19.053768, Accuracy 0.119%\n",
      "Epoch 5, Batch 47, LR 1.018487 Loss 19.057015, Accuracy 0.116%\n",
      "Epoch 5, Batch 48, LR 1.018774 Loss 19.058275, Accuracy 0.130%\n",
      "Epoch 5, Batch 49, LR 1.019062 Loss 19.059512, Accuracy 0.128%\n",
      "Epoch 5, Batch 50, LR 1.019350 Loss 19.056308, Accuracy 0.125%\n",
      "Epoch 5, Batch 51, LR 1.019638 Loss 19.054140, Accuracy 0.123%\n",
      "Epoch 5, Batch 52, LR 1.019926 Loss 19.051443, Accuracy 0.120%\n",
      "Epoch 5, Batch 53, LR 1.020213 Loss 19.052564, Accuracy 0.118%\n",
      "Epoch 5, Batch 54, LR 1.020501 Loss 19.048986, Accuracy 0.116%\n",
      "Epoch 5, Batch 55, LR 1.020789 Loss 19.047469, Accuracy 0.114%\n",
      "Epoch 5, Batch 56, LR 1.021077 Loss 19.050513, Accuracy 0.112%\n",
      "Epoch 5, Batch 57, LR 1.021365 Loss 19.049192, Accuracy 0.110%\n",
      "Epoch 5, Batch 58, LR 1.021653 Loss 19.051741, Accuracy 0.108%\n",
      "Epoch 5, Batch 59, LR 1.021941 Loss 19.053834, Accuracy 0.106%\n",
      "Epoch 5, Batch 60, LR 1.022229 Loss 19.052370, Accuracy 0.104%\n",
      "Epoch 5, Batch 61, LR 1.022516 Loss 19.052803, Accuracy 0.102%\n",
      "Epoch 5, Batch 62, LR 1.022804 Loss 19.050712, Accuracy 0.101%\n",
      "Epoch 5, Batch 63, LR 1.023092 Loss 19.050496, Accuracy 0.099%\n",
      "Epoch 5, Batch 64, LR 1.023380 Loss 19.049233, Accuracy 0.098%\n",
      "Epoch 5, Batch 65, LR 1.023668 Loss 19.050200, Accuracy 0.096%\n",
      "Epoch 5, Batch 66, LR 1.023957 Loss 19.048308, Accuracy 0.095%\n",
      "Epoch 5, Batch 67, LR 1.024245 Loss 19.050217, Accuracy 0.093%\n",
      "Epoch 5, Batch 68, LR 1.024533 Loss 19.049896, Accuracy 0.092%\n",
      "Epoch 5, Batch 69, LR 1.024821 Loss 19.052874, Accuracy 0.091%\n",
      "Epoch 5, Batch 70, LR 1.025109 Loss 19.052678, Accuracy 0.089%\n",
      "Epoch 5, Batch 71, LR 1.025397 Loss 19.053309, Accuracy 0.088%\n",
      "Epoch 5, Batch 72, LR 1.025685 Loss 19.054797, Accuracy 0.087%\n",
      "Epoch 5, Batch 73, LR 1.025973 Loss 19.053747, Accuracy 0.086%\n",
      "Epoch 5, Batch 74, LR 1.026262 Loss 19.056356, Accuracy 0.084%\n",
      "Epoch 5, Batch 75, LR 1.026550 Loss 19.056392, Accuracy 0.083%\n",
      "Epoch 5, Batch 76, LR 1.026838 Loss 19.058397, Accuracy 0.082%\n",
      "Epoch 5, Batch 77, LR 1.027126 Loss 19.061051, Accuracy 0.081%\n",
      "Epoch 5, Batch 78, LR 1.027414 Loss 19.060024, Accuracy 0.080%\n",
      "Epoch 5, Batch 79, LR 1.027703 Loss 19.062704, Accuracy 0.099%\n",
      "Epoch 5, Batch 80, LR 1.027991 Loss 19.062517, Accuracy 0.098%\n",
      "Epoch 5, Batch 81, LR 1.028279 Loss 19.061844, Accuracy 0.106%\n",
      "Epoch 5, Batch 82, LR 1.028568 Loss 19.058924, Accuracy 0.105%\n",
      "Epoch 5, Batch 83, LR 1.028856 Loss 19.056477, Accuracy 0.104%\n",
      "Epoch 5, Batch 84, LR 1.029144 Loss 19.057861, Accuracy 0.102%\n",
      "Epoch 5, Batch 85, LR 1.029433 Loss 19.057021, Accuracy 0.101%\n",
      "Epoch 5, Batch 86, LR 1.029721 Loss 19.058051, Accuracy 0.100%\n",
      "Epoch 5, Batch 87, LR 1.030010 Loss 19.057567, Accuracy 0.099%\n",
      "Epoch 5, Batch 88, LR 1.030298 Loss 19.057518, Accuracy 0.098%\n",
      "Epoch 5, Batch 89, LR 1.030587 Loss 19.054259, Accuracy 0.097%\n",
      "Epoch 5, Batch 90, LR 1.030875 Loss 19.052183, Accuracy 0.095%\n",
      "Epoch 5, Batch 91, LR 1.031164 Loss 19.052489, Accuracy 0.094%\n",
      "Epoch 5, Batch 92, LR 1.031452 Loss 19.052217, Accuracy 0.093%\n",
      "Epoch 5, Batch 93, LR 1.031741 Loss 19.052740, Accuracy 0.092%\n",
      "Epoch 5, Batch 94, LR 1.032029 Loss 19.053739, Accuracy 0.091%\n",
      "Epoch 5, Batch 95, LR 1.032318 Loss 19.053239, Accuracy 0.090%\n",
      "Epoch 5, Batch 96, LR 1.032606 Loss 19.053258, Accuracy 0.090%\n",
      "Epoch 5, Batch 97, LR 1.032895 Loss 19.052245, Accuracy 0.089%\n",
      "Epoch 5, Batch 98, LR 1.033183 Loss 19.051865, Accuracy 0.096%\n",
      "Epoch 5, Batch 99, LR 1.033472 Loss 19.051430, Accuracy 0.095%\n",
      "Epoch 5, Batch 100, LR 1.033761 Loss 19.052054, Accuracy 0.094%\n",
      "Epoch 5, Batch 101, LR 1.034049 Loss 19.052800, Accuracy 0.093%\n",
      "Epoch 5, Batch 102, LR 1.034338 Loss 19.051301, Accuracy 0.092%\n",
      "Epoch 5, Batch 103, LR 1.034627 Loss 19.051122, Accuracy 0.091%\n",
      "Epoch 5, Batch 104, LR 1.034915 Loss 19.051947, Accuracy 0.090%\n",
      "Epoch 5, Batch 105, LR 1.035204 Loss 19.052199, Accuracy 0.089%\n",
      "Epoch 5, Batch 106, LR 1.035493 Loss 19.052503, Accuracy 0.088%\n",
      "Epoch 5, Batch 107, LR 1.035782 Loss 19.052433, Accuracy 0.088%\n",
      "Epoch 5, Batch 108, LR 1.036071 Loss 19.052538, Accuracy 0.087%\n",
      "Epoch 5, Batch 109, LR 1.036359 Loss 19.050905, Accuracy 0.093%\n",
      "Epoch 5, Batch 110, LR 1.036648 Loss 19.049647, Accuracy 0.092%\n",
      "Epoch 5, Batch 111, LR 1.036937 Loss 19.048548, Accuracy 0.091%\n",
      "Epoch 5, Batch 112, LR 1.037226 Loss 19.047088, Accuracy 0.091%\n",
      "Epoch 5, Batch 113, LR 1.037515 Loss 19.047073, Accuracy 0.090%\n",
      "Epoch 5, Batch 114, LR 1.037804 Loss 19.047421, Accuracy 0.089%\n",
      "Epoch 5, Batch 115, LR 1.038093 Loss 19.048894, Accuracy 0.088%\n",
      "Epoch 5, Batch 116, LR 1.038381 Loss 19.049701, Accuracy 0.088%\n",
      "Epoch 5, Batch 117, LR 1.038670 Loss 19.048567, Accuracy 0.087%\n",
      "Epoch 5, Batch 118, LR 1.038959 Loss 19.047426, Accuracy 0.086%\n",
      "Epoch 5, Batch 119, LR 1.039248 Loss 19.047326, Accuracy 0.085%\n",
      "Epoch 5, Batch 120, LR 1.039537 Loss 19.047480, Accuracy 0.085%\n",
      "Epoch 5, Batch 121, LR 1.039826 Loss 19.047539, Accuracy 0.084%\n",
      "Epoch 5, Batch 122, LR 1.040115 Loss 19.046451, Accuracy 0.090%\n",
      "Epoch 5, Batch 123, LR 1.040404 Loss 19.046246, Accuracy 0.095%\n",
      "Epoch 5, Batch 124, LR 1.040693 Loss 19.044655, Accuracy 0.095%\n",
      "Epoch 5, Batch 125, LR 1.040983 Loss 19.044711, Accuracy 0.094%\n",
      "Epoch 5, Batch 126, LR 1.041272 Loss 19.045299, Accuracy 0.093%\n",
      "Epoch 5, Batch 127, LR 1.041561 Loss 19.044485, Accuracy 0.092%\n",
      "Epoch 5, Batch 128, LR 1.041850 Loss 19.043800, Accuracy 0.098%\n",
      "Epoch 5, Batch 129, LR 1.042139 Loss 19.044354, Accuracy 0.097%\n",
      "Epoch 5, Batch 130, LR 1.042428 Loss 19.045208, Accuracy 0.102%\n",
      "Epoch 5, Batch 131, LR 1.042717 Loss 19.045793, Accuracy 0.101%\n",
      "Epoch 5, Batch 132, LR 1.043007 Loss 19.046259, Accuracy 0.101%\n",
      "Epoch 5, Batch 133, LR 1.043296 Loss 19.046034, Accuracy 0.100%\n",
      "Epoch 5, Batch 134, LR 1.043585 Loss 19.044352, Accuracy 0.099%\n",
      "Epoch 5, Batch 135, LR 1.043874 Loss 19.043784, Accuracy 0.098%\n",
      "Epoch 5, Batch 136, LR 1.044164 Loss 19.043567, Accuracy 0.098%\n",
      "Epoch 5, Batch 137, LR 1.044453 Loss 19.043602, Accuracy 0.097%\n",
      "Epoch 5, Batch 138, LR 1.044742 Loss 19.044265, Accuracy 0.096%\n",
      "Epoch 5, Batch 139, LR 1.045031 Loss 19.043635, Accuracy 0.096%\n",
      "Epoch 5, Batch 140, LR 1.045321 Loss 19.043597, Accuracy 0.095%\n",
      "Epoch 5, Batch 141, LR 1.045610 Loss 19.044898, Accuracy 0.094%\n",
      "Epoch 5, Batch 142, LR 1.045900 Loss 19.045711, Accuracy 0.094%\n",
      "Epoch 5, Batch 143, LR 1.046189 Loss 19.045365, Accuracy 0.093%\n",
      "Epoch 5, Batch 144, LR 1.046478 Loss 19.046960, Accuracy 0.092%\n",
      "Epoch 5, Batch 145, LR 1.046768 Loss 19.047330, Accuracy 0.092%\n",
      "Epoch 5, Batch 146, LR 1.047057 Loss 19.047192, Accuracy 0.091%\n",
      "Epoch 5, Batch 147, LR 1.047347 Loss 19.048389, Accuracy 0.090%\n",
      "Epoch 5, Batch 148, LR 1.047636 Loss 19.048324, Accuracy 0.090%\n",
      "Epoch 5, Batch 149, LR 1.047926 Loss 19.048546, Accuracy 0.089%\n",
      "Epoch 5, Batch 150, LR 1.048215 Loss 19.048111, Accuracy 0.089%\n",
      "Epoch 5, Batch 151, LR 1.048505 Loss 19.048005, Accuracy 0.093%\n",
      "Epoch 5, Batch 152, LR 1.048794 Loss 19.048137, Accuracy 0.093%\n",
      "Epoch 5, Batch 153, LR 1.049084 Loss 19.048226, Accuracy 0.092%\n",
      "Epoch 5, Batch 154, LR 1.049373 Loss 19.048089, Accuracy 0.091%\n",
      "Epoch 5, Batch 155, LR 1.049663 Loss 19.047701, Accuracy 0.091%\n",
      "Epoch 5, Batch 156, LR 1.049952 Loss 19.047880, Accuracy 0.090%\n",
      "Epoch 5, Batch 157, LR 1.050242 Loss 19.048655, Accuracy 0.090%\n",
      "Epoch 5, Batch 158, LR 1.050532 Loss 19.048801, Accuracy 0.094%\n",
      "Epoch 5, Batch 159, LR 1.050821 Loss 19.048203, Accuracy 0.093%\n",
      "Epoch 5, Batch 160, LR 1.051111 Loss 19.047996, Accuracy 0.093%\n",
      "Epoch 5, Batch 161, LR 1.051401 Loss 19.048242, Accuracy 0.092%\n",
      "Epoch 5, Batch 162, LR 1.051690 Loss 19.049014, Accuracy 0.092%\n",
      "Epoch 5, Batch 163, LR 1.051980 Loss 19.048182, Accuracy 0.096%\n",
      "Epoch 5, Batch 164, LR 1.052270 Loss 19.048463, Accuracy 0.095%\n",
      "Epoch 5, Batch 165, LR 1.052559 Loss 19.047053, Accuracy 0.095%\n",
      "Epoch 5, Batch 166, LR 1.052849 Loss 19.047235, Accuracy 0.094%\n",
      "Epoch 5, Batch 167, LR 1.053139 Loss 19.047658, Accuracy 0.098%\n",
      "Epoch 5, Batch 168, LR 1.053429 Loss 19.047785, Accuracy 0.098%\n",
      "Epoch 5, Batch 169, LR 1.053719 Loss 19.048015, Accuracy 0.097%\n",
      "Epoch 5, Batch 170, LR 1.054008 Loss 19.047227, Accuracy 0.097%\n",
      "Epoch 5, Batch 171, LR 1.054298 Loss 19.047306, Accuracy 0.096%\n",
      "Epoch 5, Batch 172, LR 1.054588 Loss 19.047872, Accuracy 0.095%\n",
      "Epoch 5, Batch 173, LR 1.054878 Loss 19.047984, Accuracy 0.095%\n",
      "Epoch 5, Batch 174, LR 1.055168 Loss 19.047981, Accuracy 0.094%\n",
      "Epoch 5, Batch 175, LR 1.055458 Loss 19.048422, Accuracy 0.094%\n",
      "Epoch 5, Batch 176, LR 1.055748 Loss 19.048493, Accuracy 0.093%\n",
      "Epoch 5, Batch 177, LR 1.056038 Loss 19.048033, Accuracy 0.093%\n",
      "Epoch 5, Batch 178, LR 1.056328 Loss 19.048631, Accuracy 0.092%\n",
      "Epoch 5, Batch 179, LR 1.056618 Loss 19.049281, Accuracy 0.092%\n",
      "Epoch 5, Batch 180, LR 1.056908 Loss 19.049216, Accuracy 0.091%\n",
      "Epoch 5, Batch 181, LR 1.057198 Loss 19.049166, Accuracy 0.091%\n",
      "Epoch 5, Batch 182, LR 1.057488 Loss 19.049836, Accuracy 0.090%\n",
      "Epoch 5, Batch 183, LR 1.057778 Loss 19.050003, Accuracy 0.090%\n",
      "Epoch 5, Batch 184, LR 1.058068 Loss 19.051296, Accuracy 0.089%\n",
      "Epoch 5, Batch 185, LR 1.058358 Loss 19.052223, Accuracy 0.093%\n",
      "Epoch 5, Batch 186, LR 1.058648 Loss 19.052575, Accuracy 0.092%\n",
      "Epoch 5, Batch 187, LR 1.058938 Loss 19.052136, Accuracy 0.092%\n",
      "Epoch 5, Batch 188, LR 1.059228 Loss 19.052385, Accuracy 0.091%\n",
      "Epoch 5, Batch 189, LR 1.059518 Loss 19.052267, Accuracy 0.091%\n",
      "Epoch 5, Batch 190, LR 1.059808 Loss 19.051453, Accuracy 0.090%\n",
      "Epoch 5, Batch 191, LR 1.060099 Loss 19.050580, Accuracy 0.090%\n",
      "Epoch 5, Batch 192, LR 1.060389 Loss 19.050926, Accuracy 0.090%\n",
      "Epoch 5, Batch 193, LR 1.060679 Loss 19.049946, Accuracy 0.093%\n",
      "Epoch 5, Batch 194, LR 1.060969 Loss 19.050563, Accuracy 0.093%\n",
      "Epoch 5, Batch 195, LR 1.061259 Loss 19.051082, Accuracy 0.092%\n",
      "Epoch 5, Batch 196, LR 1.061550 Loss 19.051715, Accuracy 0.092%\n",
      "Epoch 5, Batch 197, LR 1.061840 Loss 19.051397, Accuracy 0.091%\n",
      "Epoch 5, Batch 198, LR 1.062130 Loss 19.051455, Accuracy 0.091%\n",
      "Epoch 5, Batch 199, LR 1.062420 Loss 19.052061, Accuracy 0.090%\n",
      "Epoch 5, Batch 200, LR 1.062711 Loss 19.051241, Accuracy 0.090%\n",
      "Epoch 5, Batch 201, LR 1.063001 Loss 19.052273, Accuracy 0.089%\n",
      "Epoch 5, Batch 202, LR 1.063291 Loss 19.051555, Accuracy 0.089%\n",
      "Epoch 5, Batch 203, LR 1.063582 Loss 19.052495, Accuracy 0.089%\n",
      "Epoch 5, Batch 204, LR 1.063872 Loss 19.052387, Accuracy 0.088%\n",
      "Epoch 5, Batch 205, LR 1.064162 Loss 19.052625, Accuracy 0.088%\n",
      "Epoch 5, Batch 206, LR 1.064453 Loss 19.052425, Accuracy 0.087%\n",
      "Epoch 5, Batch 207, LR 1.064743 Loss 19.052632, Accuracy 0.087%\n",
      "Epoch 5, Batch 208, LR 1.065034 Loss 19.051529, Accuracy 0.086%\n",
      "Epoch 5, Batch 209, LR 1.065324 Loss 19.050989, Accuracy 0.086%\n",
      "Epoch 5, Batch 210, LR 1.065615 Loss 19.051335, Accuracy 0.086%\n",
      "Epoch 5, Batch 211, LR 1.065905 Loss 19.051129, Accuracy 0.085%\n",
      "Epoch 5, Batch 212, LR 1.066196 Loss 19.051803, Accuracy 0.085%\n",
      "Epoch 5, Batch 213, LR 1.066486 Loss 19.051683, Accuracy 0.084%\n",
      "Epoch 5, Batch 214, LR 1.066777 Loss 19.051903, Accuracy 0.084%\n",
      "Epoch 5, Batch 215, LR 1.067067 Loss 19.051737, Accuracy 0.084%\n",
      "Epoch 5, Batch 216, LR 1.067358 Loss 19.051668, Accuracy 0.083%\n",
      "Epoch 5, Batch 217, LR 1.067648 Loss 19.052760, Accuracy 0.083%\n",
      "Epoch 5, Batch 218, LR 1.067939 Loss 19.052831, Accuracy 0.082%\n",
      "Epoch 5, Batch 219, LR 1.068229 Loss 19.052512, Accuracy 0.082%\n",
      "Epoch 5, Batch 220, LR 1.068520 Loss 19.052702, Accuracy 0.082%\n",
      "Epoch 5, Batch 221, LR 1.068811 Loss 19.053527, Accuracy 0.081%\n",
      "Epoch 5, Batch 222, LR 1.069101 Loss 19.052755, Accuracy 0.081%\n",
      "Epoch 5, Batch 223, LR 1.069392 Loss 19.052926, Accuracy 0.081%\n",
      "Epoch 5, Batch 224, LR 1.069683 Loss 19.053215, Accuracy 0.084%\n",
      "Epoch 5, Batch 225, LR 1.069973 Loss 19.053688, Accuracy 0.083%\n",
      "Epoch 5, Batch 226, LR 1.070264 Loss 19.053227, Accuracy 0.086%\n",
      "Epoch 5, Batch 227, LR 1.070555 Loss 19.054313, Accuracy 0.086%\n",
      "Epoch 5, Batch 228, LR 1.070845 Loss 19.054282, Accuracy 0.086%\n",
      "Epoch 5, Batch 229, LR 1.071136 Loss 19.054864, Accuracy 0.085%\n",
      "Epoch 5, Batch 230, LR 1.071427 Loss 19.054422, Accuracy 0.085%\n",
      "Epoch 5, Batch 231, LR 1.071718 Loss 19.055109, Accuracy 0.085%\n",
      "Epoch 5, Batch 232, LR 1.072009 Loss 19.055376, Accuracy 0.084%\n",
      "Epoch 5, Batch 233, LR 1.072299 Loss 19.055717, Accuracy 0.091%\n",
      "Epoch 5, Batch 234, LR 1.072590 Loss 19.055464, Accuracy 0.090%\n",
      "Epoch 5, Batch 235, LR 1.072881 Loss 19.056460, Accuracy 0.090%\n",
      "Epoch 5, Batch 236, LR 1.073172 Loss 19.055733, Accuracy 0.093%\n",
      "Epoch 5, Batch 237, LR 1.073463 Loss 19.055869, Accuracy 0.092%\n",
      "Epoch 5, Batch 238, LR 1.073754 Loss 19.055556, Accuracy 0.092%\n",
      "Epoch 5, Batch 239, LR 1.074044 Loss 19.055472, Accuracy 0.092%\n",
      "Epoch 5, Batch 240, LR 1.074335 Loss 19.055316, Accuracy 0.091%\n",
      "Epoch 5, Batch 241, LR 1.074626 Loss 19.055147, Accuracy 0.091%\n",
      "Epoch 5, Batch 242, LR 1.074917 Loss 19.054886, Accuracy 0.090%\n",
      "Epoch 5, Batch 243, LR 1.075208 Loss 19.055548, Accuracy 0.090%\n",
      "Epoch 5, Batch 244, LR 1.075499 Loss 19.055123, Accuracy 0.090%\n",
      "Epoch 5, Batch 245, LR 1.075790 Loss 19.055366, Accuracy 0.089%\n",
      "Epoch 5, Batch 246, LR 1.076081 Loss 19.055507, Accuracy 0.089%\n",
      "Epoch 5, Batch 247, LR 1.076372 Loss 19.054791, Accuracy 0.089%\n",
      "Epoch 5, Batch 248, LR 1.076663 Loss 19.055268, Accuracy 0.088%\n",
      "Epoch 5, Batch 249, LR 1.076954 Loss 19.055056, Accuracy 0.088%\n",
      "Epoch 5, Batch 250, LR 1.077245 Loss 19.055503, Accuracy 0.087%\n",
      "Epoch 5, Batch 251, LR 1.077536 Loss 19.055655, Accuracy 0.090%\n",
      "Epoch 5, Batch 252, LR 1.077828 Loss 19.055487, Accuracy 0.090%\n",
      "Epoch 5, Batch 253, LR 1.078119 Loss 19.055480, Accuracy 0.090%\n",
      "Epoch 5, Batch 254, LR 1.078410 Loss 19.055975, Accuracy 0.089%\n",
      "Epoch 5, Batch 255, LR 1.078701 Loss 19.056047, Accuracy 0.089%\n",
      "Epoch 5, Batch 256, LR 1.078992 Loss 19.055688, Accuracy 0.089%\n",
      "Epoch 5, Batch 257, LR 1.079283 Loss 19.055405, Accuracy 0.088%\n",
      "Epoch 5, Batch 258, LR 1.079574 Loss 19.055899, Accuracy 0.088%\n",
      "Epoch 5, Batch 259, LR 1.079866 Loss 19.055317, Accuracy 0.087%\n",
      "Epoch 5, Batch 260, LR 1.080157 Loss 19.054883, Accuracy 0.087%\n",
      "Epoch 5, Batch 261, LR 1.080448 Loss 19.054699, Accuracy 0.087%\n",
      "Epoch 5, Batch 262, LR 1.080739 Loss 19.054627, Accuracy 0.086%\n",
      "Epoch 5, Batch 263, LR 1.081030 Loss 19.054136, Accuracy 0.086%\n",
      "Epoch 5, Batch 264, LR 1.081322 Loss 19.054531, Accuracy 0.086%\n",
      "Epoch 5, Batch 265, LR 1.081613 Loss 19.054558, Accuracy 0.085%\n",
      "Epoch 5, Batch 266, LR 1.081904 Loss 19.054480, Accuracy 0.085%\n",
      "Epoch 5, Batch 267, LR 1.082196 Loss 19.054304, Accuracy 0.085%\n",
      "Epoch 5, Batch 268, LR 1.082487 Loss 19.054273, Accuracy 0.085%\n",
      "Epoch 5, Batch 269, LR 1.082778 Loss 19.053975, Accuracy 0.084%\n",
      "Epoch 5, Batch 270, LR 1.083070 Loss 19.054159, Accuracy 0.084%\n",
      "Epoch 5, Batch 271, LR 1.083361 Loss 19.054017, Accuracy 0.084%\n",
      "Epoch 5, Batch 272, LR 1.083652 Loss 19.053560, Accuracy 0.083%\n",
      "Epoch 5, Batch 273, LR 1.083944 Loss 19.053455, Accuracy 0.083%\n",
      "Epoch 5, Batch 274, LR 1.084235 Loss 19.053252, Accuracy 0.083%\n",
      "Epoch 5, Batch 275, LR 1.084527 Loss 19.053147, Accuracy 0.082%\n",
      "Epoch 5, Batch 276, LR 1.084818 Loss 19.052849, Accuracy 0.085%\n",
      "Epoch 5, Batch 277, LR 1.085110 Loss 19.053197, Accuracy 0.085%\n",
      "Epoch 5, Batch 278, LR 1.085401 Loss 19.053660, Accuracy 0.084%\n",
      "Epoch 5, Batch 279, LR 1.085692 Loss 19.053616, Accuracy 0.084%\n",
      "Epoch 5, Batch 280, LR 1.085984 Loss 19.053570, Accuracy 0.084%\n",
      "Epoch 5, Batch 281, LR 1.086275 Loss 19.053608, Accuracy 0.083%\n",
      "Epoch 5, Batch 282, LR 1.086567 Loss 19.053636, Accuracy 0.083%\n",
      "Epoch 5, Batch 283, LR 1.086859 Loss 19.053360, Accuracy 0.086%\n",
      "Epoch 5, Batch 284, LR 1.087150 Loss 19.053670, Accuracy 0.085%\n",
      "Epoch 5, Batch 285, LR 1.087442 Loss 19.054056, Accuracy 0.085%\n",
      "Epoch 5, Batch 286, LR 1.087733 Loss 19.053249, Accuracy 0.087%\n",
      "Epoch 5, Batch 287, LR 1.088025 Loss 19.053110, Accuracy 0.087%\n",
      "Epoch 5, Batch 288, LR 1.088316 Loss 19.052633, Accuracy 0.087%\n",
      "Epoch 5, Batch 289, LR 1.088608 Loss 19.051968, Accuracy 0.087%\n",
      "Epoch 5, Batch 290, LR 1.088900 Loss 19.052287, Accuracy 0.086%\n",
      "Epoch 5, Batch 291, LR 1.089191 Loss 19.052949, Accuracy 0.086%\n",
      "Epoch 5, Batch 292, LR 1.089483 Loss 19.052640, Accuracy 0.086%\n",
      "Epoch 5, Batch 293, LR 1.089775 Loss 19.052507, Accuracy 0.085%\n",
      "Epoch 5, Batch 294, LR 1.090066 Loss 19.052797, Accuracy 0.085%\n",
      "Epoch 5, Batch 295, LR 1.090358 Loss 19.052683, Accuracy 0.085%\n",
      "Epoch 5, Batch 296, LR 1.090650 Loss 19.052299, Accuracy 0.084%\n",
      "Epoch 5, Batch 297, LR 1.090942 Loss 19.051806, Accuracy 0.084%\n",
      "Epoch 5, Batch 298, LR 1.091233 Loss 19.052023, Accuracy 0.084%\n",
      "Epoch 5, Batch 299, LR 1.091525 Loss 19.051667, Accuracy 0.084%\n",
      "Epoch 5, Batch 300, LR 1.091817 Loss 19.052047, Accuracy 0.083%\n",
      "Epoch 5, Batch 301, LR 1.092109 Loss 19.052085, Accuracy 0.083%\n",
      "Epoch 5, Batch 302, LR 1.092400 Loss 19.052529, Accuracy 0.083%\n",
      "Epoch 5, Batch 303, LR 1.092692 Loss 19.052245, Accuracy 0.083%\n",
      "Epoch 5, Batch 304, LR 1.092984 Loss 19.052065, Accuracy 0.082%\n",
      "Epoch 5, Batch 305, LR 1.093276 Loss 19.051664, Accuracy 0.085%\n",
      "Epoch 5, Batch 306, LR 1.093568 Loss 19.051809, Accuracy 0.084%\n",
      "Epoch 5, Batch 307, LR 1.093860 Loss 19.052434, Accuracy 0.084%\n",
      "Epoch 5, Batch 308, LR 1.094152 Loss 19.052520, Accuracy 0.084%\n",
      "Epoch 5, Batch 309, LR 1.094443 Loss 19.052850, Accuracy 0.083%\n",
      "Epoch 5, Batch 310, LR 1.094735 Loss 19.052507, Accuracy 0.083%\n",
      "Epoch 5, Batch 311, LR 1.095027 Loss 19.052635, Accuracy 0.083%\n",
      "Epoch 5, Batch 312, LR 1.095319 Loss 19.052559, Accuracy 0.083%\n",
      "Epoch 5, Batch 313, LR 1.095611 Loss 19.052258, Accuracy 0.082%\n",
      "Epoch 5, Batch 314, LR 1.095903 Loss 19.052299, Accuracy 0.085%\n",
      "Epoch 5, Batch 315, LR 1.096195 Loss 19.052570, Accuracy 0.084%\n",
      "Epoch 5, Batch 316, LR 1.096487 Loss 19.053584, Accuracy 0.084%\n",
      "Epoch 5, Batch 317, LR 1.096779 Loss 19.053556, Accuracy 0.084%\n",
      "Epoch 5, Batch 318, LR 1.097071 Loss 19.053091, Accuracy 0.084%\n",
      "Epoch 5, Batch 319, LR 1.097363 Loss 19.053112, Accuracy 0.083%\n",
      "Epoch 5, Batch 320, LR 1.097655 Loss 19.052883, Accuracy 0.083%\n",
      "Epoch 5, Batch 321, LR 1.097947 Loss 19.053212, Accuracy 0.083%\n",
      "Epoch 5, Batch 322, LR 1.098239 Loss 19.053349, Accuracy 0.082%\n",
      "Epoch 5, Batch 323, LR 1.098531 Loss 19.053107, Accuracy 0.082%\n",
      "Epoch 5, Batch 324, LR 1.098824 Loss 19.052969, Accuracy 0.084%\n",
      "Epoch 5, Batch 325, LR 1.099116 Loss 19.052993, Accuracy 0.084%\n",
      "Epoch 5, Batch 326, LR 1.099408 Loss 19.052911, Accuracy 0.084%\n",
      "Epoch 5, Batch 327, LR 1.099700 Loss 19.053216, Accuracy 0.084%\n",
      "Epoch 5, Batch 328, LR 1.099992 Loss 19.052380, Accuracy 0.086%\n",
      "Epoch 5, Batch 329, LR 1.100284 Loss 19.052856, Accuracy 0.085%\n",
      "Epoch 5, Batch 330, LR 1.100576 Loss 19.052650, Accuracy 0.085%\n",
      "Epoch 5, Batch 331, LR 1.100869 Loss 19.052864, Accuracy 0.085%\n",
      "Epoch 5, Batch 332, LR 1.101161 Loss 19.053083, Accuracy 0.085%\n",
      "Epoch 5, Batch 333, LR 1.101453 Loss 19.052742, Accuracy 0.084%\n",
      "Epoch 5, Batch 334, LR 1.101745 Loss 19.052462, Accuracy 0.084%\n",
      "Epoch 5, Batch 335, LR 1.102037 Loss 19.053430, Accuracy 0.086%\n",
      "Epoch 5, Batch 336, LR 1.102330 Loss 19.053507, Accuracy 0.086%\n",
      "Epoch 5, Batch 337, LR 1.102622 Loss 19.053841, Accuracy 0.086%\n",
      "Epoch 5, Batch 338, LR 1.102914 Loss 19.053035, Accuracy 0.086%\n",
      "Epoch 5, Batch 339, LR 1.103207 Loss 19.052590, Accuracy 0.085%\n",
      "Epoch 5, Batch 340, LR 1.103499 Loss 19.052197, Accuracy 0.087%\n",
      "Epoch 5, Batch 341, LR 1.103791 Loss 19.052933, Accuracy 0.087%\n",
      "Epoch 5, Batch 342, LR 1.104084 Loss 19.053156, Accuracy 0.089%\n",
      "Epoch 5, Batch 343, LR 1.104376 Loss 19.052477, Accuracy 0.091%\n",
      "Epoch 5, Batch 344, LR 1.104668 Loss 19.052898, Accuracy 0.091%\n",
      "Epoch 5, Batch 345, LR 1.104961 Loss 19.052918, Accuracy 0.091%\n",
      "Epoch 5, Batch 346, LR 1.105253 Loss 19.052970, Accuracy 0.090%\n",
      "Epoch 5, Batch 347, LR 1.105545 Loss 19.052862, Accuracy 0.090%\n",
      "Epoch 5, Batch 348, LR 1.105838 Loss 19.053447, Accuracy 0.090%\n",
      "Epoch 5, Batch 349, LR 1.106130 Loss 19.053386, Accuracy 0.090%\n",
      "Epoch 5, Batch 350, LR 1.106423 Loss 19.052938, Accuracy 0.089%\n",
      "Epoch 5, Batch 351, LR 1.106715 Loss 19.053244, Accuracy 0.091%\n",
      "Epoch 5, Batch 352, LR 1.107008 Loss 19.053613, Accuracy 0.093%\n",
      "Epoch 5, Batch 353, LR 1.107300 Loss 19.053894, Accuracy 0.093%\n",
      "Epoch 5, Batch 354, LR 1.107593 Loss 19.053380, Accuracy 0.093%\n",
      "Epoch 5, Batch 355, LR 1.107885 Loss 19.054181, Accuracy 0.092%\n",
      "Epoch 5, Batch 356, LR 1.108178 Loss 19.054007, Accuracy 0.094%\n",
      "Epoch 5, Batch 357, LR 1.108470 Loss 19.054500, Accuracy 0.094%\n",
      "Epoch 5, Batch 358, LR 1.108763 Loss 19.054295, Accuracy 0.094%\n",
      "Epoch 5, Batch 359, LR 1.109055 Loss 19.055025, Accuracy 0.094%\n",
      "Epoch 5, Batch 360, LR 1.109348 Loss 19.054930, Accuracy 0.093%\n",
      "Epoch 5, Batch 361, LR 1.109640 Loss 19.054975, Accuracy 0.093%\n",
      "Epoch 5, Batch 362, LR 1.109933 Loss 19.055111, Accuracy 0.093%\n",
      "Epoch 5, Batch 363, LR 1.110226 Loss 19.055569, Accuracy 0.093%\n",
      "Epoch 5, Batch 364, LR 1.110518 Loss 19.056250, Accuracy 0.092%\n",
      "Epoch 5, Batch 365, LR 1.110811 Loss 19.056328, Accuracy 0.092%\n",
      "Epoch 5, Batch 366, LR 1.111104 Loss 19.056610, Accuracy 0.094%\n",
      "Epoch 5, Batch 367, LR 1.111396 Loss 19.057028, Accuracy 0.094%\n",
      "Epoch 5, Batch 368, LR 1.111689 Loss 19.056887, Accuracy 0.093%\n",
      "Epoch 5, Batch 369, LR 1.111982 Loss 19.056249, Accuracy 0.093%\n",
      "Epoch 5, Batch 370, LR 1.112274 Loss 19.056186, Accuracy 0.093%\n",
      "Epoch 5, Batch 371, LR 1.112567 Loss 19.056108, Accuracy 0.093%\n",
      "Epoch 5, Batch 372, LR 1.112860 Loss 19.056365, Accuracy 0.092%\n",
      "Epoch 5, Batch 373, LR 1.113152 Loss 19.056582, Accuracy 0.092%\n",
      "Epoch 5, Batch 374, LR 1.113445 Loss 19.056555, Accuracy 0.094%\n",
      "Epoch 5, Batch 375, LR 1.113738 Loss 19.057176, Accuracy 0.094%\n",
      "Epoch 5, Batch 376, LR 1.114031 Loss 19.057560, Accuracy 0.094%\n",
      "Epoch 5, Batch 377, LR 1.114324 Loss 19.057490, Accuracy 0.093%\n",
      "Epoch 5, Batch 378, LR 1.114616 Loss 19.057471, Accuracy 0.093%\n",
      "Epoch 5, Batch 379, LR 1.114909 Loss 19.058112, Accuracy 0.093%\n",
      "Epoch 5, Batch 380, LR 1.115202 Loss 19.058228, Accuracy 0.093%\n",
      "Epoch 5, Batch 381, LR 1.115495 Loss 19.058384, Accuracy 0.092%\n",
      "Epoch 5, Batch 382, LR 1.115788 Loss 19.059018, Accuracy 0.092%\n",
      "Epoch 5, Batch 383, LR 1.116080 Loss 19.058931, Accuracy 0.092%\n",
      "Epoch 5, Batch 384, LR 1.116373 Loss 19.058855, Accuracy 0.092%\n",
      "Epoch 5, Batch 385, LR 1.116666 Loss 19.058812, Accuracy 0.091%\n",
      "Epoch 5, Batch 386, LR 1.116959 Loss 19.059373, Accuracy 0.091%\n",
      "Epoch 5, Batch 387, LR 1.117252 Loss 19.059472, Accuracy 0.091%\n",
      "Epoch 5, Batch 388, LR 1.117545 Loss 19.059508, Accuracy 0.091%\n",
      "Epoch 5, Batch 389, LR 1.117838 Loss 19.059705, Accuracy 0.090%\n",
      "Epoch 5, Batch 390, LR 1.118131 Loss 19.059896, Accuracy 0.090%\n",
      "Epoch 5, Batch 391, LR 1.118424 Loss 19.059961, Accuracy 0.090%\n",
      "Epoch 5, Batch 392, LR 1.118717 Loss 19.059894, Accuracy 0.090%\n",
      "Epoch 5, Batch 393, LR 1.119010 Loss 19.059987, Accuracy 0.089%\n",
      "Epoch 5, Batch 394, LR 1.119303 Loss 19.059847, Accuracy 0.089%\n",
      "Epoch 5, Batch 395, LR 1.119596 Loss 19.059623, Accuracy 0.089%\n",
      "Epoch 5, Batch 396, LR 1.119889 Loss 19.059943, Accuracy 0.089%\n",
      "Epoch 5, Batch 397, LR 1.120182 Loss 19.060039, Accuracy 0.089%\n",
      "Epoch 5, Batch 398, LR 1.120475 Loss 19.059616, Accuracy 0.088%\n",
      "Epoch 5, Batch 399, LR 1.120768 Loss 19.059723, Accuracy 0.088%\n",
      "Epoch 5, Batch 400, LR 1.121061 Loss 19.059750, Accuracy 0.088%\n",
      "Epoch 5, Batch 401, LR 1.121354 Loss 19.059762, Accuracy 0.088%\n",
      "Epoch 5, Batch 402, LR 1.121647 Loss 19.059688, Accuracy 0.087%\n",
      "Epoch 5, Batch 403, LR 1.121940 Loss 19.060032, Accuracy 0.087%\n",
      "Epoch 5, Batch 404, LR 1.122233 Loss 19.059316, Accuracy 0.087%\n",
      "Epoch 5, Batch 405, LR 1.122526 Loss 19.059731, Accuracy 0.087%\n",
      "Epoch 5, Batch 406, LR 1.122820 Loss 19.059755, Accuracy 0.087%\n",
      "Epoch 5, Batch 407, LR 1.123113 Loss 19.059074, Accuracy 0.086%\n",
      "Epoch 5, Batch 408, LR 1.123406 Loss 19.059123, Accuracy 0.086%\n",
      "Epoch 5, Batch 409, LR 1.123699 Loss 19.058942, Accuracy 0.086%\n",
      "Epoch 5, Batch 410, LR 1.123992 Loss 19.058427, Accuracy 0.086%\n",
      "Epoch 5, Batch 411, LR 1.124285 Loss 19.058889, Accuracy 0.086%\n",
      "Epoch 5, Batch 412, LR 1.124579 Loss 19.058571, Accuracy 0.085%\n",
      "Epoch 5, Batch 413, LR 1.124872 Loss 19.058976, Accuracy 0.085%\n",
      "Epoch 5, Batch 414, LR 1.125165 Loss 19.059234, Accuracy 0.085%\n",
      "Epoch 5, Batch 415, LR 1.125458 Loss 19.059332, Accuracy 0.085%\n",
      "Epoch 5, Batch 416, LR 1.125752 Loss 19.059253, Accuracy 0.086%\n",
      "Epoch 5, Batch 417, LR 1.126045 Loss 19.059402, Accuracy 0.086%\n",
      "Epoch 5, Batch 418, LR 1.126338 Loss 19.059658, Accuracy 0.088%\n",
      "Epoch 5, Batch 419, LR 1.126631 Loss 19.059955, Accuracy 0.088%\n",
      "Epoch 5, Batch 420, LR 1.126925 Loss 19.060029, Accuracy 0.087%\n",
      "Epoch 5, Batch 421, LR 1.127218 Loss 19.060289, Accuracy 0.087%\n",
      "Epoch 5, Batch 422, LR 1.127511 Loss 19.060430, Accuracy 0.087%\n",
      "Epoch 5, Batch 423, LR 1.127805 Loss 19.060082, Accuracy 0.089%\n",
      "Epoch 5, Batch 424, LR 1.128098 Loss 19.060115, Accuracy 0.088%\n",
      "Epoch 5, Batch 425, LR 1.128391 Loss 19.059734, Accuracy 0.090%\n",
      "Epoch 5, Batch 426, LR 1.128685 Loss 19.060106, Accuracy 0.090%\n",
      "Epoch 5, Batch 427, LR 1.128978 Loss 19.059788, Accuracy 0.090%\n",
      "Epoch 5, Batch 428, LR 1.129272 Loss 19.059996, Accuracy 0.093%\n",
      "Epoch 5, Batch 429, LR 1.129565 Loss 19.060157, Accuracy 0.093%\n",
      "Epoch 5, Batch 430, LR 1.129858 Loss 19.060820, Accuracy 0.093%\n",
      "Epoch 5, Batch 431, LR 1.130152 Loss 19.060218, Accuracy 0.092%\n",
      "Epoch 5, Batch 432, LR 1.130445 Loss 19.060025, Accuracy 0.094%\n",
      "Epoch 5, Batch 433, LR 1.130739 Loss 19.060357, Accuracy 0.094%\n",
      "Epoch 5, Batch 434, LR 1.131032 Loss 19.060287, Accuracy 0.094%\n",
      "Epoch 5, Batch 435, LR 1.131326 Loss 19.060021, Accuracy 0.093%\n",
      "Epoch 5, Batch 436, LR 1.131619 Loss 19.059750, Accuracy 0.093%\n",
      "Epoch 5, Batch 437, LR 1.131913 Loss 19.059845, Accuracy 0.093%\n",
      "Epoch 5, Batch 438, LR 1.132206 Loss 19.060024, Accuracy 0.093%\n",
      "Epoch 5, Batch 439, LR 1.132500 Loss 19.060016, Accuracy 0.093%\n",
      "Epoch 5, Batch 440, LR 1.132793 Loss 19.060307, Accuracy 0.092%\n",
      "Epoch 5, Batch 441, LR 1.133087 Loss 19.059980, Accuracy 0.092%\n",
      "Epoch 5, Batch 442, LR 1.133380 Loss 19.059760, Accuracy 0.092%\n",
      "Epoch 5, Batch 443, LR 1.133674 Loss 19.060059, Accuracy 0.092%\n",
      "Epoch 5, Batch 444, LR 1.133967 Loss 19.059602, Accuracy 0.093%\n",
      "Epoch 5, Batch 445, LR 1.134261 Loss 19.059886, Accuracy 0.093%\n",
      "Epoch 5, Batch 446, LR 1.134555 Loss 19.060114, Accuracy 0.093%\n",
      "Epoch 5, Batch 447, LR 1.134848 Loss 19.060252, Accuracy 0.093%\n",
      "Epoch 5, Batch 448, LR 1.135142 Loss 19.060300, Accuracy 0.092%\n",
      "Epoch 5, Batch 449, LR 1.135435 Loss 19.060346, Accuracy 0.092%\n",
      "Epoch 5, Batch 450, LR 1.135729 Loss 19.060587, Accuracy 0.092%\n",
      "Epoch 5, Batch 451, LR 1.136023 Loss 19.060065, Accuracy 0.092%\n",
      "Epoch 5, Batch 452, LR 1.136316 Loss 19.060250, Accuracy 0.092%\n",
      "Epoch 5, Batch 453, LR 1.136610 Loss 19.060501, Accuracy 0.091%\n",
      "Epoch 5, Batch 454, LR 1.136904 Loss 19.060909, Accuracy 0.091%\n",
      "Epoch 5, Batch 455, LR 1.137197 Loss 19.060877, Accuracy 0.091%\n",
      "Epoch 5, Batch 456, LR 1.137491 Loss 19.061159, Accuracy 0.091%\n",
      "Epoch 5, Batch 457, LR 1.137785 Loss 19.061288, Accuracy 0.091%\n",
      "Epoch 5, Batch 458, LR 1.138079 Loss 19.061343, Accuracy 0.090%\n",
      "Epoch 5, Batch 459, LR 1.138372 Loss 19.061194, Accuracy 0.090%\n",
      "Epoch 5, Batch 460, LR 1.138666 Loss 19.061587, Accuracy 0.090%\n",
      "Epoch 5, Batch 461, LR 1.138960 Loss 19.061625, Accuracy 0.090%\n",
      "Epoch 5, Batch 462, LR 1.139254 Loss 19.061766, Accuracy 0.090%\n",
      "Epoch 5, Batch 463, LR 1.139547 Loss 19.061457, Accuracy 0.089%\n",
      "Epoch 5, Batch 464, LR 1.139841 Loss 19.061691, Accuracy 0.089%\n",
      "Epoch 5, Batch 465, LR 1.140135 Loss 19.061723, Accuracy 0.091%\n",
      "Epoch 5, Batch 466, LR 1.140429 Loss 19.061904, Accuracy 0.091%\n",
      "Epoch 5, Batch 467, LR 1.140723 Loss 19.061813, Accuracy 0.090%\n",
      "Epoch 5, Batch 468, LR 1.141016 Loss 19.061995, Accuracy 0.090%\n",
      "Epoch 5, Batch 469, LR 1.141310 Loss 19.062120, Accuracy 0.090%\n",
      "Epoch 5, Batch 470, LR 1.141604 Loss 19.061824, Accuracy 0.090%\n",
      "Epoch 5, Batch 471, LR 1.141898 Loss 19.061647, Accuracy 0.090%\n",
      "Epoch 5, Batch 472, LR 1.142192 Loss 19.061337, Accuracy 0.089%\n",
      "Epoch 5, Batch 473, LR 1.142486 Loss 19.060936, Accuracy 0.089%\n",
      "Epoch 5, Batch 474, LR 1.142780 Loss 19.060235, Accuracy 0.089%\n",
      "Epoch 5, Batch 475, LR 1.143074 Loss 19.060222, Accuracy 0.089%\n",
      "Epoch 5, Batch 476, LR 1.143368 Loss 19.059967, Accuracy 0.090%\n",
      "Epoch 5, Batch 477, LR 1.143661 Loss 19.060061, Accuracy 0.090%\n",
      "Epoch 5, Batch 478, LR 1.143955 Loss 19.060068, Accuracy 0.090%\n",
      "Epoch 5, Batch 479, LR 1.144249 Loss 19.060024, Accuracy 0.090%\n",
      "Epoch 5, Batch 480, LR 1.144543 Loss 19.059751, Accuracy 0.090%\n",
      "Epoch 5, Batch 481, LR 1.144837 Loss 19.060436, Accuracy 0.089%\n",
      "Epoch 5, Batch 482, LR 1.145131 Loss 19.059860, Accuracy 0.089%\n",
      "Epoch 5, Batch 483, LR 1.145425 Loss 19.060307, Accuracy 0.089%\n",
      "Epoch 5, Batch 484, LR 1.145719 Loss 19.059845, Accuracy 0.089%\n",
      "Epoch 5, Batch 485, LR 1.146013 Loss 19.059621, Accuracy 0.090%\n",
      "Epoch 5, Batch 486, LR 1.146307 Loss 19.059319, Accuracy 0.090%\n",
      "Epoch 5, Batch 487, LR 1.146601 Loss 19.059025, Accuracy 0.090%\n",
      "Epoch 5, Batch 488, LR 1.146895 Loss 19.059105, Accuracy 0.090%\n",
      "Epoch 5, Batch 489, LR 1.147189 Loss 19.059148, Accuracy 0.089%\n",
      "Epoch 5, Batch 490, LR 1.147484 Loss 19.059076, Accuracy 0.089%\n",
      "Epoch 5, Batch 491, LR 1.147778 Loss 19.059345, Accuracy 0.089%\n",
      "Epoch 5, Batch 492, LR 1.148072 Loss 19.059376, Accuracy 0.089%\n",
      "Epoch 5, Batch 493, LR 1.148366 Loss 19.059853, Accuracy 0.089%\n",
      "Epoch 5, Batch 494, LR 1.148660 Loss 19.060487, Accuracy 0.089%\n",
      "Epoch 5, Batch 495, LR 1.148954 Loss 19.060213, Accuracy 0.090%\n",
      "Epoch 5, Batch 496, LR 1.149248 Loss 19.060049, Accuracy 0.090%\n",
      "Epoch 5, Batch 497, LR 1.149542 Loss 19.059920, Accuracy 0.090%\n",
      "Epoch 5, Batch 498, LR 1.149836 Loss 19.059682, Accuracy 0.094%\n",
      "Epoch 5, Batch 499, LR 1.150131 Loss 19.059428, Accuracy 0.094%\n",
      "Epoch 5, Batch 500, LR 1.150425 Loss 19.059650, Accuracy 0.094%\n",
      "Epoch 5, Batch 501, LR 1.150719 Loss 19.059681, Accuracy 0.094%\n",
      "Epoch 5, Batch 502, LR 1.151013 Loss 19.059403, Accuracy 0.093%\n",
      "Epoch 5, Batch 503, LR 1.151307 Loss 19.059422, Accuracy 0.093%\n",
      "Epoch 5, Batch 504, LR 1.151602 Loss 19.059296, Accuracy 0.093%\n",
      "Epoch 5, Batch 505, LR 1.151896 Loss 19.059374, Accuracy 0.094%\n",
      "Epoch 5, Batch 506, LR 1.152190 Loss 19.058801, Accuracy 0.094%\n",
      "Epoch 5, Batch 507, LR 1.152484 Loss 19.058947, Accuracy 0.094%\n",
      "Epoch 5, Batch 508, LR 1.152779 Loss 19.058920, Accuracy 0.094%\n",
      "Epoch 5, Batch 509, LR 1.153073 Loss 19.059010, Accuracy 0.094%\n",
      "Epoch 5, Batch 510, LR 1.153367 Loss 19.059081, Accuracy 0.093%\n",
      "Epoch 5, Batch 511, LR 1.153661 Loss 19.059092, Accuracy 0.093%\n",
      "Epoch 5, Batch 512, LR 1.153956 Loss 19.059177, Accuracy 0.093%\n",
      "Epoch 5, Batch 513, LR 1.154250 Loss 19.059378, Accuracy 0.093%\n",
      "Epoch 5, Batch 514, LR 1.154544 Loss 19.059510, Accuracy 0.093%\n",
      "Epoch 5, Batch 515, LR 1.154839 Loss 19.059643, Accuracy 0.093%\n",
      "Epoch 5, Batch 516, LR 1.155133 Loss 19.059634, Accuracy 0.092%\n",
      "Epoch 5, Batch 517, LR 1.155427 Loss 19.059405, Accuracy 0.092%\n",
      "Epoch 5, Batch 518, LR 1.155722 Loss 19.059626, Accuracy 0.092%\n",
      "Epoch 5, Batch 519, LR 1.156016 Loss 19.059452, Accuracy 0.092%\n",
      "Epoch 5, Batch 520, LR 1.156310 Loss 19.059400, Accuracy 0.092%\n",
      "Epoch 5, Batch 521, LR 1.156605 Loss 19.059887, Accuracy 0.091%\n",
      "Epoch 5, Batch 522, LR 1.156899 Loss 19.059665, Accuracy 0.091%\n",
      "Epoch 5, Batch 523, LR 1.157193 Loss 19.059733, Accuracy 0.091%\n",
      "Epoch 5, Batch 524, LR 1.157488 Loss 19.059749, Accuracy 0.091%\n",
      "Epoch 5, Batch 525, LR 1.157782 Loss 19.059596, Accuracy 0.092%\n",
      "Epoch 5, Batch 526, LR 1.158077 Loss 19.059924, Accuracy 0.092%\n",
      "Epoch 5, Batch 527, LR 1.158371 Loss 19.060252, Accuracy 0.092%\n",
      "Epoch 5, Batch 528, LR 1.158666 Loss 19.059970, Accuracy 0.092%\n",
      "Epoch 5, Batch 529, LR 1.158960 Loss 19.059968, Accuracy 0.092%\n",
      "Epoch 5, Batch 530, LR 1.159255 Loss 19.059788, Accuracy 0.091%\n",
      "Epoch 5, Batch 531, LR 1.159549 Loss 19.059858, Accuracy 0.091%\n",
      "Epoch 5, Batch 532, LR 1.159843 Loss 19.060021, Accuracy 0.091%\n",
      "Epoch 5, Batch 533, LR 1.160138 Loss 19.059865, Accuracy 0.091%\n",
      "Epoch 5, Batch 534, LR 1.160432 Loss 19.059764, Accuracy 0.091%\n",
      "Epoch 5, Batch 535, LR 1.160727 Loss 19.059713, Accuracy 0.091%\n",
      "Epoch 5, Batch 536, LR 1.161022 Loss 19.059978, Accuracy 0.090%\n",
      "Epoch 5, Batch 537, LR 1.161316 Loss 19.059888, Accuracy 0.090%\n",
      "Epoch 5, Batch 538, LR 1.161611 Loss 19.060010, Accuracy 0.090%\n",
      "Epoch 5, Batch 539, LR 1.161905 Loss 19.060025, Accuracy 0.090%\n",
      "Epoch 5, Batch 540, LR 1.162200 Loss 19.059882, Accuracy 0.090%\n",
      "Epoch 5, Batch 541, LR 1.162494 Loss 19.059824, Accuracy 0.091%\n",
      "Epoch 5, Batch 542, LR 1.162789 Loss 19.060304, Accuracy 0.091%\n",
      "Epoch 5, Batch 543, LR 1.163083 Loss 19.060338, Accuracy 0.092%\n",
      "Epoch 5, Batch 544, LR 1.163378 Loss 19.060759, Accuracy 0.092%\n",
      "Epoch 5, Batch 545, LR 1.163673 Loss 19.061216, Accuracy 0.093%\n",
      "Epoch 5, Batch 546, LR 1.163967 Loss 19.061234, Accuracy 0.093%\n",
      "Epoch 5, Batch 547, LR 1.164262 Loss 19.061580, Accuracy 0.093%\n",
      "Epoch 5, Batch 548, LR 1.164557 Loss 19.061811, Accuracy 0.093%\n",
      "Epoch 5, Batch 549, LR 1.164851 Loss 19.061617, Accuracy 0.092%\n",
      "Epoch 5, Batch 550, LR 1.165146 Loss 19.061816, Accuracy 0.092%\n",
      "Epoch 5, Batch 551, LR 1.165441 Loss 19.061736, Accuracy 0.092%\n",
      "Epoch 5, Batch 552, LR 1.165735 Loss 19.061684, Accuracy 0.092%\n",
      "Epoch 5, Batch 553, LR 1.166030 Loss 19.062126, Accuracy 0.092%\n",
      "Epoch 5, Batch 554, LR 1.166325 Loss 19.062137, Accuracy 0.092%\n",
      "Epoch 5, Batch 555, LR 1.166619 Loss 19.062077, Accuracy 0.091%\n",
      "Epoch 5, Batch 556, LR 1.166914 Loss 19.062208, Accuracy 0.091%\n",
      "Epoch 5, Batch 557, LR 1.167209 Loss 19.062034, Accuracy 0.091%\n",
      "Epoch 5, Batch 558, LR 1.167503 Loss 19.061855, Accuracy 0.091%\n",
      "Epoch 5, Batch 559, LR 1.167798 Loss 19.062003, Accuracy 0.092%\n",
      "Epoch 5, Batch 560, LR 1.168093 Loss 19.062091, Accuracy 0.092%\n",
      "Epoch 5, Batch 561, LR 1.168388 Loss 19.062220, Accuracy 0.092%\n",
      "Epoch 5, Batch 562, LR 1.168682 Loss 19.061942, Accuracy 0.092%\n",
      "Epoch 5, Batch 563, LR 1.168977 Loss 19.061763, Accuracy 0.092%\n",
      "Epoch 5, Batch 564, LR 1.169272 Loss 19.061673, Accuracy 0.091%\n",
      "Epoch 5, Batch 565, LR 1.169567 Loss 19.061580, Accuracy 0.091%\n",
      "Epoch 5, Batch 566, LR 1.169862 Loss 19.061427, Accuracy 0.092%\n",
      "Epoch 5, Batch 567, LR 1.170156 Loss 19.061396, Accuracy 0.092%\n",
      "Epoch 5, Batch 568, LR 1.170451 Loss 19.061533, Accuracy 0.092%\n",
      "Epoch 5, Batch 569, LR 1.170746 Loss 19.061287, Accuracy 0.092%\n",
      "Epoch 5, Batch 570, LR 1.171041 Loss 19.061245, Accuracy 0.092%\n",
      "Epoch 5, Batch 571, LR 1.171336 Loss 19.061506, Accuracy 0.092%\n",
      "Epoch 5, Batch 572, LR 1.171631 Loss 19.061099, Accuracy 0.092%\n",
      "Epoch 5, Batch 573, LR 1.171925 Loss 19.061439, Accuracy 0.091%\n",
      "Epoch 5, Batch 574, LR 1.172220 Loss 19.061358, Accuracy 0.091%\n",
      "Epoch 5, Batch 575, LR 1.172515 Loss 19.061620, Accuracy 0.091%\n",
      "Epoch 5, Batch 576, LR 1.172810 Loss 19.061377, Accuracy 0.091%\n",
      "Epoch 5, Batch 577, LR 1.173105 Loss 19.061563, Accuracy 0.092%\n",
      "Epoch 5, Batch 578, LR 1.173400 Loss 19.061727, Accuracy 0.092%\n",
      "Epoch 5, Batch 579, LR 1.173695 Loss 19.061548, Accuracy 0.092%\n",
      "Epoch 5, Batch 580, LR 1.173990 Loss 19.061859, Accuracy 0.092%\n",
      "Epoch 5, Batch 581, LR 1.174285 Loss 19.062067, Accuracy 0.091%\n",
      "Epoch 5, Batch 582, LR 1.174580 Loss 19.062205, Accuracy 0.091%\n",
      "Epoch 5, Batch 583, LR 1.174875 Loss 19.062050, Accuracy 0.091%\n",
      "Epoch 5, Batch 584, LR 1.175169 Loss 19.062166, Accuracy 0.091%\n",
      "Epoch 5, Batch 585, LR 1.175464 Loss 19.062098, Accuracy 0.091%\n",
      "Epoch 5, Batch 586, LR 1.175759 Loss 19.062081, Accuracy 0.091%\n",
      "Epoch 5, Batch 587, LR 1.176054 Loss 19.062004, Accuracy 0.091%\n",
      "Epoch 5, Batch 588, LR 1.176349 Loss 19.062141, Accuracy 0.090%\n",
      "Epoch 5, Batch 589, LR 1.176644 Loss 19.062421, Accuracy 0.090%\n",
      "Epoch 5, Batch 590, LR 1.176939 Loss 19.062243, Accuracy 0.090%\n",
      "Epoch 5, Batch 591, LR 1.177234 Loss 19.062136, Accuracy 0.090%\n",
      "Epoch 5, Batch 592, LR 1.177529 Loss 19.062261, Accuracy 0.090%\n",
      "Epoch 5, Batch 593, LR 1.177824 Loss 19.062563, Accuracy 0.090%\n",
      "Epoch 5, Batch 594, LR 1.178120 Loss 19.062818, Accuracy 0.089%\n",
      "Epoch 5, Batch 595, LR 1.178415 Loss 19.062871, Accuracy 0.089%\n",
      "Epoch 5, Batch 596, LR 1.178710 Loss 19.063016, Accuracy 0.089%\n",
      "Epoch 5, Batch 597, LR 1.179005 Loss 19.062683, Accuracy 0.089%\n",
      "Epoch 5, Batch 598, LR 1.179300 Loss 19.062654, Accuracy 0.089%\n",
      "Epoch 5, Batch 599, LR 1.179595 Loss 19.062361, Accuracy 0.089%\n",
      "Epoch 5, Batch 600, LR 1.179890 Loss 19.062398, Accuracy 0.089%\n",
      "Epoch 5, Batch 601, LR 1.180185 Loss 19.062337, Accuracy 0.088%\n",
      "Epoch 5, Batch 602, LR 1.180480 Loss 19.062172, Accuracy 0.088%\n",
      "Epoch 5, Batch 603, LR 1.180775 Loss 19.062348, Accuracy 0.088%\n",
      "Epoch 5, Batch 604, LR 1.181070 Loss 19.062481, Accuracy 0.088%\n",
      "Epoch 5, Batch 605, LR 1.181366 Loss 19.062565, Accuracy 0.088%\n",
      "Epoch 5, Batch 606, LR 1.181661 Loss 19.062529, Accuracy 0.088%\n",
      "Epoch 5, Batch 607, LR 1.181956 Loss 19.062158, Accuracy 0.088%\n",
      "Epoch 5, Batch 608, LR 1.182251 Loss 19.062253, Accuracy 0.087%\n",
      "Epoch 5, Batch 609, LR 1.182546 Loss 19.062227, Accuracy 0.089%\n",
      "Epoch 5, Batch 610, LR 1.182841 Loss 19.062051, Accuracy 0.088%\n",
      "Epoch 5, Batch 611, LR 1.183137 Loss 19.061995, Accuracy 0.088%\n",
      "Epoch 5, Batch 612, LR 1.183432 Loss 19.061923, Accuracy 0.088%\n",
      "Epoch 5, Batch 613, LR 1.183727 Loss 19.061740, Accuracy 0.088%\n",
      "Epoch 5, Batch 614, LR 1.184022 Loss 19.061741, Accuracy 0.088%\n",
      "Epoch 5, Batch 615, LR 1.184317 Loss 19.061898, Accuracy 0.088%\n",
      "Epoch 5, Batch 616, LR 1.184613 Loss 19.061971, Accuracy 0.088%\n",
      "Epoch 5, Batch 617, LR 1.184908 Loss 19.061714, Accuracy 0.089%\n",
      "Epoch 5, Batch 618, LR 1.185203 Loss 19.061905, Accuracy 0.088%\n",
      "Epoch 5, Batch 619, LR 1.185498 Loss 19.061874, Accuracy 0.088%\n",
      "Epoch 5, Batch 620, LR 1.185794 Loss 19.061997, Accuracy 0.088%\n",
      "Epoch 5, Batch 621, LR 1.186089 Loss 19.061848, Accuracy 0.088%\n",
      "Epoch 5, Batch 622, LR 1.186384 Loss 19.061872, Accuracy 0.088%\n",
      "Epoch 5, Batch 623, LR 1.186679 Loss 19.061842, Accuracy 0.088%\n",
      "Epoch 5, Batch 624, LR 1.186975 Loss 19.061827, Accuracy 0.088%\n",
      "Epoch 5, Batch 625, LR 1.187270 Loss 19.061889, Accuracy 0.087%\n",
      "Epoch 5, Batch 626, LR 1.187565 Loss 19.061738, Accuracy 0.087%\n",
      "Epoch 5, Batch 627, LR 1.187861 Loss 19.061735, Accuracy 0.087%\n",
      "Epoch 5, Batch 628, LR 1.188156 Loss 19.061739, Accuracy 0.087%\n",
      "Epoch 5, Batch 629, LR 1.188451 Loss 19.061307, Accuracy 0.087%\n",
      "Epoch 5, Batch 630, LR 1.188747 Loss 19.061499, Accuracy 0.087%\n",
      "Epoch 5, Batch 631, LR 1.189042 Loss 19.061731, Accuracy 0.087%\n",
      "Epoch 5, Batch 632, LR 1.189337 Loss 19.061374, Accuracy 0.087%\n",
      "Epoch 5, Batch 633, LR 1.189633 Loss 19.061313, Accuracy 0.086%\n",
      "Epoch 5, Batch 634, LR 1.189928 Loss 19.061638, Accuracy 0.086%\n",
      "Epoch 5, Batch 635, LR 1.190223 Loss 19.061458, Accuracy 0.086%\n",
      "Epoch 5, Batch 636, LR 1.190519 Loss 19.061365, Accuracy 0.086%\n",
      "Epoch 5, Batch 637, LR 1.190814 Loss 19.061169, Accuracy 0.086%\n",
      "Epoch 5, Batch 638, LR 1.191109 Loss 19.061113, Accuracy 0.086%\n",
      "Epoch 5, Batch 639, LR 1.191405 Loss 19.060866, Accuracy 0.086%\n",
      "Epoch 5, Batch 640, LR 1.191700 Loss 19.060652, Accuracy 0.085%\n",
      "Epoch 5, Batch 641, LR 1.191996 Loss 19.060660, Accuracy 0.085%\n",
      "Epoch 5, Batch 642, LR 1.192291 Loss 19.061064, Accuracy 0.085%\n",
      "Epoch 5, Batch 643, LR 1.192587 Loss 19.061132, Accuracy 0.085%\n",
      "Epoch 5, Batch 644, LR 1.192882 Loss 19.061089, Accuracy 0.085%\n",
      "Epoch 5, Batch 645, LR 1.193177 Loss 19.061170, Accuracy 0.085%\n",
      "Epoch 5, Batch 646, LR 1.193473 Loss 19.061233, Accuracy 0.085%\n",
      "Epoch 5, Batch 647, LR 1.193768 Loss 19.061052, Accuracy 0.085%\n",
      "Epoch 5, Batch 648, LR 1.194064 Loss 19.060957, Accuracy 0.086%\n",
      "Epoch 5, Batch 649, LR 1.194359 Loss 19.061314, Accuracy 0.085%\n",
      "Epoch 5, Batch 650, LR 1.194655 Loss 19.061156, Accuracy 0.085%\n",
      "Epoch 5, Batch 651, LR 1.194950 Loss 19.061154, Accuracy 0.085%\n",
      "Epoch 5, Batch 652, LR 1.195246 Loss 19.061412, Accuracy 0.085%\n",
      "Epoch 5, Batch 653, LR 1.195541 Loss 19.061648, Accuracy 0.085%\n",
      "Epoch 5, Batch 654, LR 1.195837 Loss 19.061693, Accuracy 0.085%\n",
      "Epoch 5, Batch 655, LR 1.196132 Loss 19.061668, Accuracy 0.085%\n",
      "Epoch 5, Batch 656, LR 1.196428 Loss 19.061355, Accuracy 0.086%\n",
      "Epoch 5, Batch 657, LR 1.196723 Loss 19.061510, Accuracy 0.086%\n",
      "Epoch 5, Batch 658, LR 1.197019 Loss 19.061679, Accuracy 0.085%\n",
      "Epoch 5, Batch 659, LR 1.197315 Loss 19.062034, Accuracy 0.085%\n",
      "Epoch 5, Batch 660, LR 1.197610 Loss 19.062219, Accuracy 0.085%\n",
      "Epoch 5, Batch 661, LR 1.197906 Loss 19.062308, Accuracy 0.085%\n",
      "Epoch 5, Batch 662, LR 1.198201 Loss 19.062798, Accuracy 0.085%\n",
      "Epoch 5, Batch 663, LR 1.198497 Loss 19.062638, Accuracy 0.085%\n",
      "Epoch 5, Batch 664, LR 1.198792 Loss 19.062693, Accuracy 0.085%\n",
      "Epoch 5, Batch 665, LR 1.199088 Loss 19.062603, Accuracy 0.085%\n",
      "Epoch 5, Batch 666, LR 1.199384 Loss 19.062443, Accuracy 0.084%\n",
      "Epoch 5, Batch 667, LR 1.199679 Loss 19.062441, Accuracy 0.086%\n",
      "Epoch 5, Batch 668, LR 1.199975 Loss 19.062387, Accuracy 0.085%\n",
      "Epoch 5, Batch 669, LR 1.200270 Loss 19.062133, Accuracy 0.086%\n",
      "Epoch 5, Batch 670, LR 1.200566 Loss 19.062391, Accuracy 0.086%\n",
      "Epoch 5, Batch 671, LR 1.200862 Loss 19.062623, Accuracy 0.086%\n",
      "Epoch 5, Batch 672, LR 1.201157 Loss 19.062726, Accuracy 0.086%\n",
      "Epoch 5, Batch 673, LR 1.201453 Loss 19.062763, Accuracy 0.086%\n",
      "Epoch 5, Batch 674, LR 1.201749 Loss 19.062910, Accuracy 0.086%\n",
      "Epoch 5, Batch 675, LR 1.202044 Loss 19.062891, Accuracy 0.086%\n",
      "Epoch 5, Batch 676, LR 1.202340 Loss 19.062861, Accuracy 0.086%\n",
      "Epoch 5, Batch 677, LR 1.202636 Loss 19.062758, Accuracy 0.085%\n",
      "Epoch 5, Batch 678, LR 1.202931 Loss 19.062730, Accuracy 0.085%\n",
      "Epoch 5, Batch 679, LR 1.203227 Loss 19.062632, Accuracy 0.085%\n",
      "Epoch 5, Batch 680, LR 1.203523 Loss 19.062736, Accuracy 0.085%\n",
      "Epoch 5, Batch 681, LR 1.203818 Loss 19.062989, Accuracy 0.085%\n",
      "Epoch 5, Batch 682, LR 1.204114 Loss 19.062995, Accuracy 0.085%\n",
      "Epoch 5, Batch 683, LR 1.204410 Loss 19.063158, Accuracy 0.085%\n",
      "Epoch 5, Batch 684, LR 1.204706 Loss 19.063197, Accuracy 0.085%\n",
      "Epoch 5, Batch 685, LR 1.205001 Loss 19.063314, Accuracy 0.084%\n",
      "Epoch 5, Batch 686, LR 1.205297 Loss 19.063380, Accuracy 0.084%\n",
      "Epoch 5, Batch 687, LR 1.205593 Loss 19.063208, Accuracy 0.084%\n",
      "Epoch 5, Batch 688, LR 1.205889 Loss 19.063152, Accuracy 0.084%\n",
      "Epoch 5, Batch 689, LR 1.206184 Loss 19.063075, Accuracy 0.084%\n",
      "Epoch 5, Batch 690, LR 1.206480 Loss 19.062798, Accuracy 0.085%\n",
      "Epoch 5, Batch 691, LR 1.206776 Loss 19.062858, Accuracy 0.085%\n",
      "Epoch 5, Batch 692, LR 1.207072 Loss 19.062786, Accuracy 0.086%\n",
      "Epoch 5, Batch 693, LR 1.207367 Loss 19.062856, Accuracy 0.086%\n",
      "Epoch 5, Batch 694, LR 1.207663 Loss 19.062882, Accuracy 0.086%\n",
      "Epoch 5, Batch 695, LR 1.207959 Loss 19.062768, Accuracy 0.089%\n",
      "Epoch 5, Batch 696, LR 1.208255 Loss 19.062918, Accuracy 0.089%\n",
      "Epoch 5, Batch 697, LR 1.208551 Loss 19.062938, Accuracy 0.089%\n",
      "Epoch 5, Batch 698, LR 1.208846 Loss 19.062935, Accuracy 0.088%\n",
      "Epoch 5, Batch 699, LR 1.209142 Loss 19.062697, Accuracy 0.088%\n",
      "Epoch 5, Batch 700, LR 1.209438 Loss 19.062477, Accuracy 0.089%\n",
      "Epoch 5, Batch 701, LR 1.209734 Loss 19.062761, Accuracy 0.089%\n",
      "Epoch 5, Batch 702, LR 1.210030 Loss 19.063074, Accuracy 0.089%\n",
      "Epoch 5, Batch 703, LR 1.210326 Loss 19.063033, Accuracy 0.089%\n",
      "Epoch 5, Batch 704, LR 1.210621 Loss 19.063310, Accuracy 0.089%\n",
      "Epoch 5, Batch 705, LR 1.210917 Loss 19.063474, Accuracy 0.089%\n",
      "Epoch 5, Batch 706, LR 1.211213 Loss 19.063507, Accuracy 0.089%\n",
      "Epoch 5, Batch 707, LR 1.211509 Loss 19.063673, Accuracy 0.088%\n",
      "Epoch 5, Batch 708, LR 1.211805 Loss 19.063551, Accuracy 0.088%\n",
      "Epoch 5, Batch 709, LR 1.212101 Loss 19.063474, Accuracy 0.088%\n",
      "Epoch 5, Batch 710, LR 1.212397 Loss 19.063396, Accuracy 0.088%\n",
      "Epoch 5, Batch 711, LR 1.212693 Loss 19.063468, Accuracy 0.088%\n",
      "Epoch 5, Batch 712, LR 1.212989 Loss 19.063689, Accuracy 0.088%\n",
      "Epoch 5, Batch 713, LR 1.213284 Loss 19.064014, Accuracy 0.089%\n",
      "Epoch 5, Batch 714, LR 1.213580 Loss 19.063972, Accuracy 0.089%\n",
      "Epoch 5, Batch 715, LR 1.213876 Loss 19.064197, Accuracy 0.089%\n",
      "Epoch 5, Batch 716, LR 1.214172 Loss 19.064417, Accuracy 0.088%\n",
      "Epoch 5, Batch 717, LR 1.214468 Loss 19.064492, Accuracy 0.088%\n",
      "Epoch 5, Batch 718, LR 1.214764 Loss 19.064696, Accuracy 0.088%\n",
      "Epoch 5, Batch 719, LR 1.215060 Loss 19.064992, Accuracy 0.088%\n",
      "Epoch 5, Batch 720, LR 1.215356 Loss 19.065015, Accuracy 0.088%\n",
      "Epoch 5, Batch 721, LR 1.215652 Loss 19.065316, Accuracy 0.088%\n",
      "Epoch 5, Batch 722, LR 1.215948 Loss 19.065264, Accuracy 0.088%\n",
      "Epoch 5, Batch 723, LR 1.216244 Loss 19.065348, Accuracy 0.088%\n",
      "Epoch 5, Batch 724, LR 1.216540 Loss 19.065493, Accuracy 0.087%\n",
      "Epoch 5, Batch 725, LR 1.216836 Loss 19.065316, Accuracy 0.087%\n",
      "Epoch 5, Batch 726, LR 1.217132 Loss 19.065353, Accuracy 0.088%\n",
      "Epoch 5, Batch 727, LR 1.217428 Loss 19.065430, Accuracy 0.089%\n",
      "Epoch 5, Batch 728, LR 1.217724 Loss 19.065347, Accuracy 0.089%\n",
      "Epoch 5, Batch 729, LR 1.218020 Loss 19.065678, Accuracy 0.089%\n",
      "Epoch 5, Batch 730, LR 1.218316 Loss 19.065592, Accuracy 0.089%\n",
      "Epoch 5, Batch 731, LR 1.218612 Loss 19.065709, Accuracy 0.089%\n",
      "Epoch 5, Batch 732, LR 1.218908 Loss 19.065627, Accuracy 0.089%\n",
      "Epoch 5, Batch 733, LR 1.219204 Loss 19.065570, Accuracy 0.088%\n",
      "Epoch 5, Batch 734, LR 1.219500 Loss 19.065625, Accuracy 0.088%\n",
      "Epoch 5, Batch 735, LR 1.219796 Loss 19.065720, Accuracy 0.088%\n",
      "Epoch 5, Batch 736, LR 1.220092 Loss 19.065655, Accuracy 0.088%\n",
      "Epoch 5, Batch 737, LR 1.220388 Loss 19.065576, Accuracy 0.088%\n",
      "Epoch 5, Batch 738, LR 1.220684 Loss 19.065668, Accuracy 0.088%\n",
      "Epoch 5, Batch 739, LR 1.220980 Loss 19.065678, Accuracy 0.088%\n",
      "Epoch 5, Batch 740, LR 1.221276 Loss 19.065891, Accuracy 0.088%\n",
      "Epoch 5, Batch 741, LR 1.221572 Loss 19.066027, Accuracy 0.089%\n",
      "Epoch 5, Batch 742, LR 1.221869 Loss 19.065796, Accuracy 0.088%\n",
      "Epoch 5, Batch 743, LR 1.222165 Loss 19.065856, Accuracy 0.088%\n",
      "Epoch 5, Batch 744, LR 1.222461 Loss 19.065981, Accuracy 0.089%\n",
      "Epoch 5, Batch 745, LR 1.222757 Loss 19.065704, Accuracy 0.089%\n",
      "Epoch 5, Batch 746, LR 1.223053 Loss 19.065761, Accuracy 0.089%\n",
      "Epoch 5, Batch 747, LR 1.223349 Loss 19.065921, Accuracy 0.089%\n",
      "Epoch 5, Batch 748, LR 1.223645 Loss 19.065877, Accuracy 0.089%\n",
      "Epoch 5, Batch 749, LR 1.223941 Loss 19.065752, Accuracy 0.089%\n",
      "Epoch 5, Batch 750, LR 1.224237 Loss 19.065986, Accuracy 0.089%\n",
      "Epoch 5, Batch 751, LR 1.224534 Loss 19.065811, Accuracy 0.089%\n",
      "Epoch 5, Batch 752, LR 1.224830 Loss 19.065569, Accuracy 0.089%\n",
      "Epoch 5, Batch 753, LR 1.225126 Loss 19.065381, Accuracy 0.089%\n",
      "Epoch 5, Batch 754, LR 1.225422 Loss 19.065109, Accuracy 0.089%\n",
      "Epoch 5, Batch 755, LR 1.225718 Loss 19.065248, Accuracy 0.089%\n",
      "Epoch 5, Batch 756, LR 1.226014 Loss 19.065138, Accuracy 0.089%\n",
      "Epoch 5, Batch 757, LR 1.226311 Loss 19.065134, Accuracy 0.089%\n",
      "Epoch 5, Batch 758, LR 1.226607 Loss 19.064996, Accuracy 0.089%\n",
      "Epoch 5, Batch 759, LR 1.226903 Loss 19.065326, Accuracy 0.089%\n",
      "Epoch 5, Batch 760, LR 1.227199 Loss 19.065091, Accuracy 0.088%\n",
      "Epoch 5, Batch 761, LR 1.227495 Loss 19.065042, Accuracy 0.088%\n",
      "Epoch 5, Batch 762, LR 1.227791 Loss 19.065079, Accuracy 0.088%\n",
      "Epoch 5, Batch 763, LR 1.228088 Loss 19.065038, Accuracy 0.088%\n",
      "Epoch 5, Batch 764, LR 1.228384 Loss 19.064977, Accuracy 0.088%\n",
      "Epoch 5, Batch 765, LR 1.228680 Loss 19.064866, Accuracy 0.088%\n",
      "Epoch 5, Batch 766, LR 1.228976 Loss 19.065285, Accuracy 0.088%\n",
      "Epoch 5, Batch 767, LR 1.229273 Loss 19.065344, Accuracy 0.088%\n",
      "Epoch 5, Batch 768, LR 1.229569 Loss 19.065394, Accuracy 0.087%\n",
      "Epoch 5, Batch 769, LR 1.229865 Loss 19.065474, Accuracy 0.087%\n",
      "Epoch 5, Batch 770, LR 1.230161 Loss 19.065859, Accuracy 0.087%\n",
      "Epoch 5, Batch 771, LR 1.230457 Loss 19.065880, Accuracy 0.087%\n",
      "Epoch 5, Batch 772, LR 1.230754 Loss 19.065700, Accuracy 0.087%\n",
      "Epoch 5, Batch 773, LR 1.231050 Loss 19.065761, Accuracy 0.087%\n",
      "Epoch 5, Batch 774, LR 1.231346 Loss 19.065549, Accuracy 0.088%\n",
      "Epoch 5, Batch 775, LR 1.231642 Loss 19.065211, Accuracy 0.089%\n",
      "Epoch 5, Batch 776, LR 1.231939 Loss 19.065481, Accuracy 0.089%\n",
      "Epoch 5, Batch 777, LR 1.232235 Loss 19.065677, Accuracy 0.088%\n",
      "Epoch 5, Batch 778, LR 1.232531 Loss 19.065583, Accuracy 0.088%\n",
      "Epoch 5, Batch 779, LR 1.232828 Loss 19.065664, Accuracy 0.088%\n",
      "Epoch 5, Batch 780, LR 1.233124 Loss 19.065605, Accuracy 0.089%\n",
      "Epoch 5, Batch 781, LR 1.233420 Loss 19.065560, Accuracy 0.089%\n",
      "Epoch 5, Batch 782, LR 1.233716 Loss 19.065694, Accuracy 0.089%\n",
      "Epoch 5, Batch 783, LR 1.234013 Loss 19.065668, Accuracy 0.089%\n",
      "Epoch 5, Batch 784, LR 1.234309 Loss 19.065663, Accuracy 0.090%\n",
      "Epoch 5, Batch 785, LR 1.234605 Loss 19.065930, Accuracy 0.090%\n",
      "Epoch 5, Batch 786, LR 1.234902 Loss 19.065825, Accuracy 0.089%\n",
      "Epoch 5, Batch 787, LR 1.235198 Loss 19.065792, Accuracy 0.089%\n",
      "Epoch 5, Batch 788, LR 1.235494 Loss 19.065736, Accuracy 0.089%\n",
      "Epoch 5, Batch 789, LR 1.235791 Loss 19.065654, Accuracy 0.089%\n",
      "Epoch 5, Batch 790, LR 1.236087 Loss 19.065409, Accuracy 0.089%\n",
      "Epoch 5, Batch 791, LR 1.236383 Loss 19.065334, Accuracy 0.089%\n",
      "Epoch 5, Batch 792, LR 1.236680 Loss 19.065369, Accuracy 0.089%\n",
      "Epoch 5, Batch 793, LR 1.236976 Loss 19.065279, Accuracy 0.089%\n",
      "Epoch 5, Batch 794, LR 1.237272 Loss 19.065287, Accuracy 0.089%\n",
      "Epoch 5, Batch 795, LR 1.237569 Loss 19.065328, Accuracy 0.088%\n",
      "Epoch 5, Batch 796, LR 1.237865 Loss 19.065658, Accuracy 0.089%\n",
      "Epoch 5, Batch 797, LR 1.238161 Loss 19.065584, Accuracy 0.089%\n",
      "Epoch 5, Batch 798, LR 1.238458 Loss 19.065665, Accuracy 0.089%\n",
      "Epoch 5, Batch 799, LR 1.238754 Loss 19.065610, Accuracy 0.090%\n",
      "Epoch 5, Batch 800, LR 1.239051 Loss 19.065510, Accuracy 0.090%\n",
      "Epoch 5, Batch 801, LR 1.239347 Loss 19.065577, Accuracy 0.090%\n",
      "Epoch 5, Batch 802, LR 1.239643 Loss 19.065644, Accuracy 0.090%\n",
      "Epoch 5, Batch 803, LR 1.239940 Loss 19.065765, Accuracy 0.090%\n",
      "Epoch 5, Batch 804, LR 1.240236 Loss 19.065949, Accuracy 0.089%\n",
      "Epoch 5, Batch 805, LR 1.240533 Loss 19.066050, Accuracy 0.089%\n",
      "Epoch 5, Batch 806, LR 1.240829 Loss 19.066205, Accuracy 0.089%\n",
      "Epoch 5, Batch 807, LR 1.241125 Loss 19.066038, Accuracy 0.089%\n",
      "Epoch 5, Batch 808, LR 1.241422 Loss 19.065818, Accuracy 0.089%\n",
      "Epoch 5, Batch 809, LR 1.241718 Loss 19.065648, Accuracy 0.090%\n",
      "Epoch 5, Batch 810, LR 1.242015 Loss 19.065702, Accuracy 0.090%\n",
      "Epoch 5, Batch 811, LR 1.242311 Loss 19.065729, Accuracy 0.090%\n",
      "Epoch 5, Batch 812, LR 1.242608 Loss 19.065745, Accuracy 0.089%\n",
      "Epoch 5, Batch 813, LR 1.242904 Loss 19.065871, Accuracy 0.089%\n",
      "Epoch 5, Batch 814, LR 1.243200 Loss 19.065952, Accuracy 0.090%\n",
      "Epoch 5, Batch 815, LR 1.243497 Loss 19.065911, Accuracy 0.091%\n",
      "Epoch 5, Batch 816, LR 1.243793 Loss 19.065701, Accuracy 0.091%\n",
      "Epoch 5, Batch 817, LR 1.244090 Loss 19.065685, Accuracy 0.091%\n",
      "Epoch 5, Batch 818, LR 1.244386 Loss 19.065578, Accuracy 0.091%\n",
      "Epoch 5, Batch 819, LR 1.244683 Loss 19.065486, Accuracy 0.092%\n",
      "Epoch 5, Batch 820, LR 1.244979 Loss 19.065271, Accuracy 0.091%\n",
      "Epoch 5, Batch 821, LR 1.245276 Loss 19.065357, Accuracy 0.091%\n",
      "Epoch 5, Batch 822, LR 1.245572 Loss 19.065412, Accuracy 0.091%\n",
      "Epoch 5, Batch 823, LR 1.245869 Loss 19.065386, Accuracy 0.092%\n",
      "Epoch 5, Batch 824, LR 1.246165 Loss 19.065600, Accuracy 0.092%\n",
      "Epoch 5, Batch 825, LR 1.246462 Loss 19.065676, Accuracy 0.092%\n",
      "Epoch 5, Batch 826, LR 1.246758 Loss 19.065800, Accuracy 0.092%\n",
      "Epoch 5, Batch 827, LR 1.247055 Loss 19.065792, Accuracy 0.092%\n",
      "Epoch 5, Batch 828, LR 1.247351 Loss 19.065833, Accuracy 0.092%\n",
      "Epoch 5, Batch 829, LR 1.247648 Loss 19.065967, Accuracy 0.091%\n",
      "Epoch 5, Batch 830, LR 1.247944 Loss 19.066118, Accuracy 0.092%\n",
      "Epoch 5, Batch 831, LR 1.248241 Loss 19.066001, Accuracy 0.092%\n",
      "Epoch 5, Batch 832, LR 1.248537 Loss 19.066191, Accuracy 0.092%\n",
      "Epoch 5, Batch 833, LR 1.248834 Loss 19.066211, Accuracy 0.092%\n",
      "Epoch 5, Batch 834, LR 1.249130 Loss 19.066537, Accuracy 0.092%\n",
      "Epoch 5, Batch 835, LR 1.249427 Loss 19.066587, Accuracy 0.092%\n",
      "Epoch 5, Batch 836, LR 1.249723 Loss 19.066544, Accuracy 0.092%\n",
      "Epoch 5, Batch 837, LR 1.250020 Loss 19.066473, Accuracy 0.091%\n",
      "Epoch 5, Batch 838, LR 1.250316 Loss 19.066598, Accuracy 0.091%\n",
      "Epoch 5, Batch 839, LR 1.250613 Loss 19.066502, Accuracy 0.091%\n",
      "Epoch 5, Batch 840, LR 1.250910 Loss 19.066389, Accuracy 0.091%\n",
      "Epoch 5, Batch 841, LR 1.251206 Loss 19.066595, Accuracy 0.091%\n",
      "Epoch 5, Batch 842, LR 1.251503 Loss 19.066708, Accuracy 0.093%\n",
      "Epoch 5, Batch 843, LR 1.251799 Loss 19.066553, Accuracy 0.093%\n",
      "Epoch 5, Batch 844, LR 1.252096 Loss 19.066664, Accuracy 0.093%\n",
      "Epoch 5, Batch 845, LR 1.252392 Loss 19.066677, Accuracy 0.092%\n",
      "Epoch 5, Batch 846, LR 1.252689 Loss 19.066633, Accuracy 0.093%\n",
      "Epoch 5, Batch 847, LR 1.252986 Loss 19.066711, Accuracy 0.093%\n",
      "Epoch 5, Batch 848, LR 1.253282 Loss 19.066752, Accuracy 0.093%\n",
      "Epoch 5, Batch 849, LR 1.253579 Loss 19.066516, Accuracy 0.093%\n",
      "Epoch 5, Batch 850, LR 1.253875 Loss 19.066601, Accuracy 0.093%\n",
      "Epoch 5, Batch 851, LR 1.254172 Loss 19.066364, Accuracy 0.093%\n",
      "Epoch 5, Batch 852, LR 1.254468 Loss 19.066374, Accuracy 0.093%\n",
      "Epoch 5, Batch 853, LR 1.254765 Loss 19.066278, Accuracy 0.093%\n",
      "Epoch 5, Batch 854, LR 1.255062 Loss 19.066279, Accuracy 0.092%\n",
      "Epoch 5, Batch 855, LR 1.255358 Loss 19.066362, Accuracy 0.092%\n",
      "Epoch 5, Batch 856, LR 1.255655 Loss 19.066424, Accuracy 0.092%\n",
      "Epoch 5, Batch 857, LR 1.255952 Loss 19.066534, Accuracy 0.092%\n",
      "Epoch 5, Batch 858, LR 1.256248 Loss 19.066462, Accuracy 0.092%\n",
      "Epoch 5, Batch 859, LR 1.256545 Loss 19.066348, Accuracy 0.092%\n",
      "Epoch 5, Batch 860, LR 1.256841 Loss 19.066339, Accuracy 0.092%\n",
      "Epoch 5, Batch 861, LR 1.257138 Loss 19.066212, Accuracy 0.092%\n",
      "Epoch 5, Batch 862, LR 1.257435 Loss 19.066005, Accuracy 0.092%\n",
      "Epoch 5, Batch 863, LR 1.257731 Loss 19.066278, Accuracy 0.091%\n",
      "Epoch 5, Batch 864, LR 1.258028 Loss 19.066213, Accuracy 0.091%\n",
      "Epoch 5, Batch 865, LR 1.258325 Loss 19.066314, Accuracy 0.091%\n",
      "Epoch 5, Batch 866, LR 1.258621 Loss 19.066227, Accuracy 0.091%\n",
      "Epoch 5, Batch 867, LR 1.258918 Loss 19.066382, Accuracy 0.091%\n",
      "Epoch 5, Batch 868, LR 1.259215 Loss 19.066094, Accuracy 0.091%\n",
      "Epoch 5, Batch 869, LR 1.259511 Loss 19.066190, Accuracy 0.091%\n",
      "Epoch 5, Batch 870, LR 1.259808 Loss 19.066265, Accuracy 0.091%\n",
      "Epoch 5, Batch 871, LR 1.260105 Loss 19.066447, Accuracy 0.091%\n",
      "Epoch 5, Batch 872, LR 1.260401 Loss 19.066518, Accuracy 0.090%\n",
      "Epoch 5, Batch 873, LR 1.260698 Loss 19.066364, Accuracy 0.090%\n",
      "Epoch 5, Batch 874, LR 1.260995 Loss 19.066498, Accuracy 0.090%\n",
      "Epoch 5, Batch 875, LR 1.261291 Loss 19.066632, Accuracy 0.090%\n",
      "Epoch 5, Batch 876, LR 1.261588 Loss 19.066741, Accuracy 0.090%\n",
      "Epoch 5, Batch 877, LR 1.261885 Loss 19.066748, Accuracy 0.090%\n",
      "Epoch 5, Batch 878, LR 1.262181 Loss 19.066620, Accuracy 0.090%\n",
      "Epoch 5, Batch 879, LR 1.262478 Loss 19.066554, Accuracy 0.090%\n",
      "Epoch 5, Batch 880, LR 1.262775 Loss 19.066714, Accuracy 0.091%\n",
      "Epoch 5, Batch 881, LR 1.263071 Loss 19.066829, Accuracy 0.090%\n",
      "Epoch 5, Batch 882, LR 1.263368 Loss 19.066877, Accuracy 0.090%\n",
      "Epoch 5, Batch 883, LR 1.263665 Loss 19.066984, Accuracy 0.090%\n",
      "Epoch 5, Batch 884, LR 1.263961 Loss 19.066853, Accuracy 0.090%\n",
      "Epoch 5, Batch 885, LR 1.264258 Loss 19.067125, Accuracy 0.090%\n",
      "Epoch 5, Batch 886, LR 1.264555 Loss 19.067215, Accuracy 0.090%\n",
      "Epoch 5, Batch 887, LR 1.264852 Loss 19.067338, Accuracy 0.090%\n",
      "Epoch 5, Batch 888, LR 1.265148 Loss 19.067379, Accuracy 0.090%\n",
      "Epoch 5, Batch 889, LR 1.265445 Loss 19.067473, Accuracy 0.090%\n",
      "Epoch 5, Batch 890, LR 1.265742 Loss 19.067534, Accuracy 0.090%\n",
      "Epoch 5, Batch 891, LR 1.266038 Loss 19.067422, Accuracy 0.089%\n",
      "Epoch 5, Batch 892, LR 1.266335 Loss 19.067348, Accuracy 0.090%\n",
      "Epoch 5, Batch 893, LR 1.266632 Loss 19.067484, Accuracy 0.090%\n",
      "Epoch 5, Batch 894, LR 1.266929 Loss 19.067207, Accuracy 0.090%\n",
      "Epoch 5, Batch 895, LR 1.267225 Loss 19.067032, Accuracy 0.091%\n",
      "Epoch 5, Batch 896, LR 1.267522 Loss 19.066913, Accuracy 0.091%\n",
      "Epoch 5, Batch 897, LR 1.267819 Loss 19.066812, Accuracy 0.091%\n",
      "Epoch 5, Batch 898, LR 1.268116 Loss 19.066728, Accuracy 0.090%\n",
      "Epoch 5, Batch 899, LR 1.268412 Loss 19.066612, Accuracy 0.090%\n",
      "Epoch 5, Batch 900, LR 1.268709 Loss 19.066557, Accuracy 0.090%\n",
      "Epoch 5, Batch 901, LR 1.269006 Loss 19.066303, Accuracy 0.090%\n",
      "Epoch 5, Batch 902, LR 1.269303 Loss 19.065919, Accuracy 0.090%\n",
      "Epoch 5, Batch 903, LR 1.269599 Loss 19.065843, Accuracy 0.090%\n",
      "Epoch 5, Batch 904, LR 1.269896 Loss 19.065980, Accuracy 0.090%\n",
      "Epoch 5, Batch 905, LR 1.270193 Loss 19.066119, Accuracy 0.090%\n",
      "Epoch 5, Batch 906, LR 1.270490 Loss 19.066225, Accuracy 0.090%\n",
      "Epoch 5, Batch 907, LR 1.270786 Loss 19.066052, Accuracy 0.090%\n",
      "Epoch 5, Batch 908, LR 1.271083 Loss 19.066226, Accuracy 0.089%\n",
      "Epoch 5, Batch 909, LR 1.271380 Loss 19.066237, Accuracy 0.089%\n",
      "Epoch 5, Batch 910, LR 1.271677 Loss 19.066256, Accuracy 0.089%\n",
      "Epoch 5, Batch 911, LR 1.271974 Loss 19.066415, Accuracy 0.089%\n",
      "Epoch 5, Batch 912, LR 1.272270 Loss 19.066512, Accuracy 0.089%\n",
      "Epoch 5, Batch 913, LR 1.272567 Loss 19.066568, Accuracy 0.089%\n",
      "Epoch 5, Batch 914, LR 1.272864 Loss 19.066512, Accuracy 0.090%\n",
      "Epoch 5, Batch 915, LR 1.273161 Loss 19.066451, Accuracy 0.090%\n",
      "Epoch 5, Batch 916, LR 1.273458 Loss 19.066499, Accuracy 0.090%\n",
      "Epoch 5, Batch 917, LR 1.273754 Loss 19.066449, Accuracy 0.090%\n",
      "Epoch 5, Batch 918, LR 1.274051 Loss 19.066411, Accuracy 0.090%\n",
      "Epoch 5, Batch 919, LR 1.274348 Loss 19.066391, Accuracy 0.090%\n",
      "Epoch 5, Batch 920, LR 1.274645 Loss 19.066555, Accuracy 0.090%\n",
      "Epoch 5, Batch 921, LR 1.274942 Loss 19.066364, Accuracy 0.090%\n",
      "Epoch 5, Batch 922, LR 1.275238 Loss 19.066348, Accuracy 0.090%\n",
      "Epoch 5, Batch 923, LR 1.275535 Loss 19.066342, Accuracy 0.090%\n",
      "Epoch 5, Batch 924, LR 1.275832 Loss 19.066326, Accuracy 0.090%\n",
      "Epoch 5, Batch 925, LR 1.276129 Loss 19.066250, Accuracy 0.090%\n",
      "Epoch 5, Batch 926, LR 1.276426 Loss 19.066491, Accuracy 0.089%\n",
      "Epoch 5, Batch 927, LR 1.276722 Loss 19.066419, Accuracy 0.089%\n",
      "Epoch 5, Batch 928, LR 1.277019 Loss 19.066424, Accuracy 0.089%\n",
      "Epoch 5, Batch 929, LR 1.277316 Loss 19.066370, Accuracy 0.089%\n",
      "Epoch 5, Batch 930, LR 1.277613 Loss 19.066260, Accuracy 0.089%\n",
      "Epoch 5, Batch 931, LR 1.277910 Loss 19.066227, Accuracy 0.090%\n",
      "Epoch 5, Batch 932, LR 1.278207 Loss 19.066367, Accuracy 0.090%\n",
      "Epoch 5, Batch 933, LR 1.278503 Loss 19.066593, Accuracy 0.090%\n",
      "Epoch 5, Batch 934, LR 1.278800 Loss 19.066601, Accuracy 0.090%\n",
      "Epoch 5, Batch 935, LR 1.279097 Loss 19.066749, Accuracy 0.089%\n",
      "Epoch 5, Batch 936, LR 1.279394 Loss 19.066842, Accuracy 0.089%\n",
      "Epoch 5, Batch 937, LR 1.279691 Loss 19.066739, Accuracy 0.089%\n",
      "Epoch 5, Batch 938, LR 1.279988 Loss 19.066667, Accuracy 0.089%\n",
      "Epoch 5, Batch 939, LR 1.280284 Loss 19.066847, Accuracy 0.089%\n",
      "Epoch 5, Batch 940, LR 1.280581 Loss 19.066885, Accuracy 0.090%\n",
      "Epoch 5, Batch 941, LR 1.280878 Loss 19.066868, Accuracy 0.090%\n",
      "Epoch 5, Batch 942, LR 1.281175 Loss 19.066879, Accuracy 0.090%\n",
      "Epoch 5, Batch 943, LR 1.281472 Loss 19.066824, Accuracy 0.089%\n",
      "Epoch 5, Batch 944, LR 1.281769 Loss 19.066963, Accuracy 0.089%\n",
      "Epoch 5, Batch 945, LR 1.282065 Loss 19.066704, Accuracy 0.089%\n",
      "Epoch 5, Batch 946, LR 1.282362 Loss 19.066838, Accuracy 0.089%\n",
      "Epoch 5, Batch 947, LR 1.282659 Loss 19.066598, Accuracy 0.089%\n",
      "Epoch 5, Batch 948, LR 1.282956 Loss 19.066452, Accuracy 0.089%\n",
      "Epoch 5, Batch 949, LR 1.283253 Loss 19.066431, Accuracy 0.089%\n",
      "Epoch 5, Batch 950, LR 1.283550 Loss 19.066443, Accuracy 0.089%\n",
      "Epoch 5, Batch 951, LR 1.283847 Loss 19.066492, Accuracy 0.089%\n",
      "Epoch 5, Batch 952, LR 1.284144 Loss 19.066549, Accuracy 0.089%\n",
      "Epoch 5, Batch 953, LR 1.284440 Loss 19.066609, Accuracy 0.089%\n",
      "Epoch 5, Batch 954, LR 1.284737 Loss 19.066636, Accuracy 0.088%\n",
      "Epoch 5, Batch 955, LR 1.285034 Loss 19.066579, Accuracy 0.088%\n",
      "Epoch 5, Batch 956, LR 1.285331 Loss 19.066531, Accuracy 0.088%\n",
      "Epoch 5, Batch 957, LR 1.285628 Loss 19.066389, Accuracy 0.088%\n",
      "Epoch 5, Batch 958, LR 1.285925 Loss 19.066437, Accuracy 0.088%\n",
      "Epoch 5, Batch 959, LR 1.286222 Loss 19.066505, Accuracy 0.088%\n",
      "Epoch 5, Batch 960, LR 1.286519 Loss 19.066478, Accuracy 0.088%\n",
      "Epoch 5, Batch 961, LR 1.286815 Loss 19.066424, Accuracy 0.088%\n",
      "Epoch 5, Batch 962, LR 1.287112 Loss 19.066377, Accuracy 0.088%\n",
      "Epoch 5, Batch 963, LR 1.287409 Loss 19.066419, Accuracy 0.088%\n",
      "Epoch 5, Batch 964, LR 1.287706 Loss 19.066448, Accuracy 0.088%\n",
      "Epoch 5, Batch 965, LR 1.288003 Loss 19.066300, Accuracy 0.087%\n",
      "Epoch 5, Batch 966, LR 1.288300 Loss 19.066370, Accuracy 0.087%\n",
      "Epoch 5, Batch 967, LR 1.288597 Loss 19.066303, Accuracy 0.087%\n",
      "Epoch 5, Batch 968, LR 1.288894 Loss 19.066104, Accuracy 0.087%\n",
      "Epoch 5, Batch 969, LR 1.289191 Loss 19.065915, Accuracy 0.087%\n",
      "Epoch 5, Batch 970, LR 1.289487 Loss 19.065837, Accuracy 0.087%\n",
      "Epoch 5, Batch 971, LR 1.289784 Loss 19.065903, Accuracy 0.087%\n",
      "Epoch 5, Batch 972, LR 1.290081 Loss 19.066018, Accuracy 0.087%\n",
      "Epoch 5, Batch 973, LR 1.290378 Loss 19.066296, Accuracy 0.087%\n",
      "Epoch 5, Batch 974, LR 1.290675 Loss 19.066261, Accuracy 0.087%\n",
      "Epoch 5, Batch 975, LR 1.290972 Loss 19.066528, Accuracy 0.087%\n",
      "Epoch 5, Batch 976, LR 1.291269 Loss 19.066592, Accuracy 0.086%\n",
      "Epoch 5, Batch 977, LR 1.291566 Loss 19.066605, Accuracy 0.087%\n",
      "Epoch 5, Batch 978, LR 1.291863 Loss 19.066580, Accuracy 0.087%\n",
      "Epoch 5, Batch 979, LR 1.292160 Loss 19.066400, Accuracy 0.087%\n",
      "Epoch 5, Batch 980, LR 1.292457 Loss 19.066366, Accuracy 0.087%\n",
      "Epoch 5, Batch 981, LR 1.292753 Loss 19.066441, Accuracy 0.087%\n",
      "Epoch 5, Batch 982, LR 1.293050 Loss 19.066364, Accuracy 0.087%\n",
      "Epoch 5, Batch 983, LR 1.293347 Loss 19.066394, Accuracy 0.087%\n",
      "Epoch 5, Batch 984, LR 1.293644 Loss 19.066374, Accuracy 0.087%\n",
      "Epoch 5, Batch 985, LR 1.293941 Loss 19.066492, Accuracy 0.087%\n",
      "Epoch 5, Batch 986, LR 1.294238 Loss 19.066355, Accuracy 0.087%\n",
      "Epoch 5, Batch 987, LR 1.294535 Loss 19.066371, Accuracy 0.087%\n",
      "Epoch 5, Batch 988, LR 1.294832 Loss 19.066224, Accuracy 0.087%\n",
      "Epoch 5, Batch 989, LR 1.295129 Loss 19.066117, Accuracy 0.088%\n",
      "Epoch 5, Batch 990, LR 1.295426 Loss 19.066050, Accuracy 0.088%\n",
      "Epoch 5, Batch 991, LR 1.295723 Loss 19.066012, Accuracy 0.088%\n",
      "Epoch 5, Batch 992, LR 1.296020 Loss 19.065878, Accuracy 0.087%\n",
      "Epoch 5, Batch 993, LR 1.296316 Loss 19.065727, Accuracy 0.087%\n",
      "Epoch 5, Batch 994, LR 1.296613 Loss 19.065886, Accuracy 0.087%\n",
      "Epoch 5, Batch 995, LR 1.296910 Loss 19.065945, Accuracy 0.087%\n",
      "Epoch 5, Batch 996, LR 1.297207 Loss 19.065902, Accuracy 0.087%\n",
      "Epoch 5, Batch 997, LR 1.297504 Loss 19.065890, Accuracy 0.087%\n",
      "Epoch 5, Batch 998, LR 1.297801 Loss 19.065893, Accuracy 0.087%\n",
      "Epoch 5, Batch 999, LR 1.298098 Loss 19.065903, Accuracy 0.087%\n",
      "Epoch 5, Batch 1000, LR 1.298395 Loss 19.066076, Accuracy 0.087%\n",
      "Epoch 5, Batch 1001, LR 1.298692 Loss 19.066284, Accuracy 0.087%\n",
      "Epoch 5, Batch 1002, LR 1.298989 Loss 19.066076, Accuracy 0.087%\n",
      "Epoch 5, Batch 1003, LR 1.299286 Loss 19.066093, Accuracy 0.086%\n",
      "Epoch 5, Batch 1004, LR 1.299583 Loss 19.066071, Accuracy 0.086%\n",
      "Epoch 5, Batch 1005, LR 1.299880 Loss 19.065947, Accuracy 0.086%\n",
      "Epoch 5, Batch 1006, LR 1.300177 Loss 19.065974, Accuracy 0.086%\n",
      "Epoch 5, Batch 1007, LR 1.300474 Loss 19.065865, Accuracy 0.087%\n",
      "Epoch 5, Batch 1008, LR 1.300770 Loss 19.065862, Accuracy 0.087%\n",
      "Epoch 5, Batch 1009, LR 1.301067 Loss 19.065904, Accuracy 0.087%\n",
      "Epoch 5, Batch 1010, LR 1.301364 Loss 19.065862, Accuracy 0.087%\n",
      "Epoch 5, Batch 1011, LR 1.301661 Loss 19.065709, Accuracy 0.087%\n",
      "Epoch 5, Batch 1012, LR 1.301958 Loss 19.065834, Accuracy 0.087%\n",
      "Epoch 5, Batch 1013, LR 1.302255 Loss 19.065757, Accuracy 0.087%\n",
      "Epoch 5, Batch 1014, LR 1.302552 Loss 19.065917, Accuracy 0.087%\n",
      "Epoch 5, Batch 1015, LR 1.302849 Loss 19.065883, Accuracy 0.087%\n",
      "Epoch 5, Batch 1016, LR 1.303146 Loss 19.065831, Accuracy 0.088%\n",
      "Epoch 5, Batch 1017, LR 1.303443 Loss 19.065890, Accuracy 0.088%\n",
      "Epoch 5, Batch 1018, LR 1.303740 Loss 19.065928, Accuracy 0.087%\n",
      "Epoch 5, Batch 1019, LR 1.304037 Loss 19.065844, Accuracy 0.087%\n",
      "Epoch 5, Batch 1020, LR 1.304334 Loss 19.065709, Accuracy 0.087%\n",
      "Epoch 5, Batch 1021, LR 1.304631 Loss 19.065672, Accuracy 0.087%\n",
      "Epoch 5, Batch 1022, LR 1.304928 Loss 19.065827, Accuracy 0.087%\n",
      "Epoch 5, Batch 1023, LR 1.305225 Loss 19.065786, Accuracy 0.087%\n",
      "Epoch 5, Batch 1024, LR 1.305522 Loss 19.065705, Accuracy 0.087%\n",
      "Epoch 5, Batch 1025, LR 1.305819 Loss 19.065692, Accuracy 0.087%\n",
      "Epoch 5, Batch 1026, LR 1.306116 Loss 19.065757, Accuracy 0.087%\n",
      "Epoch 5, Batch 1027, LR 1.306412 Loss 19.065637, Accuracy 0.087%\n",
      "Epoch 5, Batch 1028, LR 1.306709 Loss 19.065587, Accuracy 0.087%\n",
      "Epoch 5, Batch 1029, LR 1.307006 Loss 19.065364, Accuracy 0.087%\n",
      "Epoch 5, Batch 1030, LR 1.307303 Loss 19.065466, Accuracy 0.087%\n",
      "Epoch 5, Batch 1031, LR 1.307600 Loss 19.065466, Accuracy 0.087%\n",
      "Epoch 5, Batch 1032, LR 1.307897 Loss 19.065504, Accuracy 0.089%\n",
      "Epoch 5, Batch 1033, LR 1.308194 Loss 19.065613, Accuracy 0.088%\n",
      "Epoch 5, Batch 1034, LR 1.308491 Loss 19.065678, Accuracy 0.088%\n",
      "Epoch 5, Batch 1035, LR 1.308788 Loss 19.065682, Accuracy 0.088%\n",
      "Epoch 5, Batch 1036, LR 1.309085 Loss 19.065609, Accuracy 0.088%\n",
      "Epoch 5, Batch 1037, LR 1.309382 Loss 19.065524, Accuracy 0.088%\n",
      "Epoch 5, Batch 1038, LR 1.309679 Loss 19.065539, Accuracy 0.088%\n",
      "Epoch 5, Batch 1039, LR 1.309976 Loss 19.065460, Accuracy 0.088%\n",
      "Epoch 5, Batch 1040, LR 1.310273 Loss 19.065533, Accuracy 0.088%\n",
      "Epoch 5, Batch 1041, LR 1.310570 Loss 19.065610, Accuracy 0.088%\n",
      "Epoch 5, Batch 1042, LR 1.310867 Loss 19.065565, Accuracy 0.088%\n",
      "Epoch 5, Batch 1043, LR 1.311164 Loss 19.065414, Accuracy 0.088%\n",
      "Epoch 5, Batch 1044, LR 1.311461 Loss 19.065079, Accuracy 0.088%\n",
      "Epoch 5, Batch 1045, LR 1.311758 Loss 19.065013, Accuracy 0.088%\n",
      "Epoch 5, Batch 1046, LR 1.312055 Loss 19.064791, Accuracy 0.089%\n",
      "Epoch 5, Batch 1047, LR 1.312352 Loss 19.064743, Accuracy 0.089%\n",
      "Epoch 5, Loss (train set) 19.064743, Accuracy (train set) 0.089%\n",
      "Epoch 6, Batch 1, LR 1.312648 Loss 19.125601, Accuracy 0.000%\n",
      "Epoch 6, Batch 2, LR 1.312945 Loss 19.116170, Accuracy 0.000%\n",
      "Epoch 6, Batch 3, LR 1.313242 Loss 19.103483, Accuracy 0.000%\n",
      "Epoch 6, Batch 4, LR 1.313539 Loss 19.144819, Accuracy 0.000%\n",
      "Epoch 6, Batch 5, LR 1.313836 Loss 19.118462, Accuracy 0.000%\n",
      "Epoch 6, Batch 6, LR 1.314133 Loss 19.097435, Accuracy 0.000%\n",
      "Epoch 6, Batch 7, LR 1.314430 Loss 19.074368, Accuracy 0.000%\n",
      "Epoch 6, Batch 8, LR 1.314727 Loss 19.087148, Accuracy 0.000%\n",
      "Epoch 6, Batch 9, LR 1.315024 Loss 19.092449, Accuracy 0.000%\n",
      "Epoch 6, Batch 10, LR 1.315321 Loss 19.069927, Accuracy 0.000%\n",
      "Epoch 6, Batch 11, LR 1.315618 Loss 19.068361, Accuracy 0.000%\n",
      "Epoch 6, Batch 12, LR 1.315915 Loss 19.053331, Accuracy 0.000%\n",
      "Epoch 6, Batch 13, LR 1.316212 Loss 19.062227, Accuracy 0.000%\n",
      "Epoch 6, Batch 14, LR 1.316509 Loss 19.058210, Accuracy 0.000%\n",
      "Epoch 6, Batch 15, LR 1.316806 Loss 19.059832, Accuracy 0.000%\n",
      "Epoch 6, Batch 16, LR 1.317103 Loss 19.062604, Accuracy 0.000%\n",
      "Epoch 6, Batch 17, LR 1.317400 Loss 19.047613, Accuracy 0.046%\n",
      "Epoch 6, Batch 18, LR 1.317697 Loss 19.048987, Accuracy 0.087%\n",
      "Epoch 6, Batch 19, LR 1.317994 Loss 19.051230, Accuracy 0.082%\n",
      "Epoch 6, Batch 20, LR 1.318291 Loss 19.042973, Accuracy 0.078%\n",
      "Epoch 6, Batch 21, LR 1.318588 Loss 19.045043, Accuracy 0.074%\n",
      "Epoch 6, Batch 22, LR 1.318884 Loss 19.054784, Accuracy 0.071%\n",
      "Epoch 6, Batch 23, LR 1.319181 Loss 19.048167, Accuracy 0.068%\n",
      "Epoch 6, Batch 24, LR 1.319478 Loss 19.054611, Accuracy 0.065%\n",
      "Epoch 6, Batch 25, LR 1.319775 Loss 19.051603, Accuracy 0.094%\n",
      "Epoch 6, Batch 26, LR 1.320072 Loss 19.053305, Accuracy 0.090%\n",
      "Epoch 6, Batch 27, LR 1.320369 Loss 19.055413, Accuracy 0.087%\n",
      "Epoch 6, Batch 28, LR 1.320666 Loss 19.057900, Accuracy 0.084%\n",
      "Epoch 6, Batch 29, LR 1.320963 Loss 19.056331, Accuracy 0.081%\n",
      "Epoch 6, Batch 30, LR 1.321260 Loss 19.055422, Accuracy 0.078%\n",
      "Epoch 6, Batch 31, LR 1.321557 Loss 19.057122, Accuracy 0.076%\n",
      "Epoch 6, Batch 32, LR 1.321854 Loss 19.063230, Accuracy 0.073%\n",
      "Epoch 6, Batch 33, LR 1.322151 Loss 19.065249, Accuracy 0.095%\n",
      "Epoch 6, Batch 34, LR 1.322448 Loss 19.066314, Accuracy 0.092%\n",
      "Epoch 6, Batch 35, LR 1.322745 Loss 19.065601, Accuracy 0.089%\n",
      "Epoch 6, Batch 36, LR 1.323042 Loss 19.063395, Accuracy 0.087%\n",
      "Epoch 6, Batch 37, LR 1.323339 Loss 19.065438, Accuracy 0.084%\n",
      "Epoch 6, Batch 38, LR 1.323636 Loss 19.071990, Accuracy 0.082%\n",
      "Epoch 6, Batch 39, LR 1.323933 Loss 19.071371, Accuracy 0.080%\n",
      "Epoch 6, Batch 40, LR 1.324230 Loss 19.075625, Accuracy 0.078%\n",
      "Epoch 6, Batch 41, LR 1.324526 Loss 19.080481, Accuracy 0.076%\n",
      "Epoch 6, Batch 42, LR 1.324823 Loss 19.083124, Accuracy 0.074%\n",
      "Epoch 6, Batch 43, LR 1.325120 Loss 19.078845, Accuracy 0.091%\n",
      "Epoch 6, Batch 44, LR 1.325417 Loss 19.078641, Accuracy 0.089%\n",
      "Epoch 6, Batch 45, LR 1.325714 Loss 19.077700, Accuracy 0.087%\n",
      "Epoch 6, Batch 46, LR 1.326011 Loss 19.078491, Accuracy 0.085%\n",
      "Epoch 6, Batch 47, LR 1.326308 Loss 19.079805, Accuracy 0.083%\n",
      "Epoch 6, Batch 48, LR 1.326605 Loss 19.080400, Accuracy 0.081%\n",
      "Epoch 6, Batch 49, LR 1.326902 Loss 19.080993, Accuracy 0.080%\n",
      "Epoch 6, Batch 50, LR 1.327199 Loss 19.080265, Accuracy 0.078%\n",
      "Epoch 6, Batch 51, LR 1.327496 Loss 19.078284, Accuracy 0.077%\n",
      "Epoch 6, Batch 52, LR 1.327793 Loss 19.078593, Accuracy 0.075%\n",
      "Epoch 6, Batch 53, LR 1.328090 Loss 19.078208, Accuracy 0.074%\n",
      "Epoch 6, Batch 54, LR 1.328387 Loss 19.078243, Accuracy 0.072%\n",
      "Epoch 6, Batch 55, LR 1.328684 Loss 19.078198, Accuracy 0.071%\n",
      "Epoch 6, Batch 56, LR 1.328980 Loss 19.077735, Accuracy 0.070%\n",
      "Epoch 6, Batch 57, LR 1.329277 Loss 19.078193, Accuracy 0.069%\n",
      "Epoch 6, Batch 58, LR 1.329574 Loss 19.077400, Accuracy 0.067%\n",
      "Epoch 6, Batch 59, LR 1.329871 Loss 19.077391, Accuracy 0.066%\n",
      "Epoch 6, Batch 60, LR 1.330168 Loss 19.078791, Accuracy 0.065%\n",
      "Epoch 6, Batch 61, LR 1.330465 Loss 19.079831, Accuracy 0.064%\n",
      "Epoch 6, Batch 62, LR 1.330762 Loss 19.081864, Accuracy 0.063%\n",
      "Epoch 6, Batch 63, LR 1.331059 Loss 19.083836, Accuracy 0.062%\n",
      "Epoch 6, Batch 64, LR 1.331356 Loss 19.080344, Accuracy 0.061%\n",
      "Epoch 6, Batch 65, LR 1.331653 Loss 19.079634, Accuracy 0.060%\n",
      "Epoch 6, Batch 66, LR 1.331950 Loss 19.079193, Accuracy 0.059%\n",
      "Epoch 6, Batch 67, LR 1.332247 Loss 19.076543, Accuracy 0.070%\n",
      "Epoch 6, Batch 68, LR 1.332543 Loss 19.076179, Accuracy 0.069%\n",
      "Epoch 6, Batch 69, LR 1.332840 Loss 19.075107, Accuracy 0.068%\n",
      "Epoch 6, Batch 70, LR 1.333137 Loss 19.074046, Accuracy 0.067%\n",
      "Epoch 6, Batch 71, LR 1.333434 Loss 19.075321, Accuracy 0.066%\n",
      "Epoch 6, Batch 72, LR 1.333731 Loss 19.076593, Accuracy 0.065%\n",
      "Epoch 6, Batch 73, LR 1.334028 Loss 19.076654, Accuracy 0.064%\n",
      "Epoch 6, Batch 74, LR 1.334325 Loss 19.075799, Accuracy 0.063%\n",
      "Epoch 6, Batch 75, LR 1.334622 Loss 19.076135, Accuracy 0.062%\n",
      "Epoch 6, Batch 76, LR 1.334919 Loss 19.074460, Accuracy 0.082%\n",
      "Epoch 6, Batch 77, LR 1.335216 Loss 19.070928, Accuracy 0.081%\n",
      "Epoch 6, Batch 78, LR 1.335513 Loss 19.072642, Accuracy 0.080%\n",
      "Epoch 6, Batch 79, LR 1.335809 Loss 19.072624, Accuracy 0.079%\n",
      "Epoch 6, Batch 80, LR 1.336106 Loss 19.071498, Accuracy 0.078%\n",
      "Epoch 6, Batch 81, LR 1.336403 Loss 19.070848, Accuracy 0.077%\n",
      "Epoch 6, Batch 82, LR 1.336700 Loss 19.071888, Accuracy 0.076%\n",
      "Epoch 6, Batch 83, LR 1.336997 Loss 19.069780, Accuracy 0.075%\n",
      "Epoch 6, Batch 84, LR 1.337294 Loss 19.067235, Accuracy 0.074%\n",
      "Epoch 6, Batch 85, LR 1.337591 Loss 19.066864, Accuracy 0.074%\n",
      "Epoch 6, Batch 86, LR 1.337888 Loss 19.065876, Accuracy 0.073%\n",
      "Epoch 6, Batch 87, LR 1.338185 Loss 19.066801, Accuracy 0.072%\n",
      "Epoch 6, Batch 88, LR 1.338481 Loss 19.065292, Accuracy 0.071%\n",
      "Epoch 6, Batch 89, LR 1.338778 Loss 19.065160, Accuracy 0.070%\n",
      "Epoch 6, Batch 90, LR 1.339075 Loss 19.065762, Accuracy 0.078%\n",
      "Epoch 6, Batch 91, LR 1.339372 Loss 19.065775, Accuracy 0.077%\n",
      "Epoch 6, Batch 92, LR 1.339669 Loss 19.066768, Accuracy 0.076%\n",
      "Epoch 6, Batch 93, LR 1.339966 Loss 19.066932, Accuracy 0.076%\n",
      "Epoch 6, Batch 94, LR 1.340263 Loss 19.067399, Accuracy 0.075%\n",
      "Epoch 6, Batch 95, LR 1.340560 Loss 19.068358, Accuracy 0.074%\n",
      "Epoch 6, Batch 96, LR 1.340856 Loss 19.068469, Accuracy 0.081%\n",
      "Epoch 6, Batch 97, LR 1.341153 Loss 19.068827, Accuracy 0.081%\n",
      "Epoch 6, Batch 98, LR 1.341450 Loss 19.069716, Accuracy 0.080%\n",
      "Epoch 6, Batch 99, LR 1.341747 Loss 19.067875, Accuracy 0.079%\n",
      "Epoch 6, Batch 100, LR 1.342044 Loss 19.069616, Accuracy 0.078%\n",
      "Epoch 6, Batch 101, LR 1.342341 Loss 19.068225, Accuracy 0.077%\n",
      "Epoch 6, Batch 102, LR 1.342638 Loss 19.067965, Accuracy 0.077%\n",
      "Epoch 6, Batch 103, LR 1.342935 Loss 19.067748, Accuracy 0.076%\n",
      "Epoch 6, Batch 104, LR 1.343231 Loss 19.065948, Accuracy 0.075%\n",
      "Epoch 6, Batch 105, LR 1.343528 Loss 19.064693, Accuracy 0.074%\n",
      "Epoch 6, Batch 106, LR 1.343825 Loss 19.062218, Accuracy 0.081%\n",
      "Epoch 6, Batch 107, LR 1.344122 Loss 19.062140, Accuracy 0.080%\n",
      "Epoch 6, Batch 108, LR 1.344419 Loss 19.060848, Accuracy 0.087%\n",
      "Epoch 6, Batch 109, LR 1.344716 Loss 19.059763, Accuracy 0.086%\n",
      "Epoch 6, Batch 110, LR 1.345012 Loss 19.060752, Accuracy 0.085%\n",
      "Epoch 6, Batch 111, LR 1.345309 Loss 19.060762, Accuracy 0.084%\n",
      "Epoch 6, Batch 112, LR 1.345606 Loss 19.059789, Accuracy 0.084%\n",
      "Epoch 6, Batch 113, LR 1.345903 Loss 19.061065, Accuracy 0.083%\n",
      "Epoch 6, Batch 114, LR 1.346200 Loss 19.060829, Accuracy 0.082%\n",
      "Epoch 6, Batch 115, LR 1.346497 Loss 19.060987, Accuracy 0.082%\n",
      "Epoch 6, Batch 116, LR 1.346793 Loss 19.061408, Accuracy 0.081%\n",
      "Epoch 6, Batch 117, LR 1.347090 Loss 19.061068, Accuracy 0.080%\n",
      "Epoch 6, Batch 118, LR 1.347387 Loss 19.061064, Accuracy 0.079%\n",
      "Epoch 6, Batch 119, LR 1.347684 Loss 19.060318, Accuracy 0.079%\n",
      "Epoch 6, Batch 120, LR 1.347981 Loss 19.060101, Accuracy 0.078%\n",
      "Epoch 6, Batch 121, LR 1.348278 Loss 19.060916, Accuracy 0.077%\n",
      "Epoch 6, Batch 122, LR 1.348574 Loss 19.060686, Accuracy 0.077%\n",
      "Epoch 6, Batch 123, LR 1.348871 Loss 19.061423, Accuracy 0.083%\n",
      "Epoch 6, Batch 124, LR 1.349168 Loss 19.061672, Accuracy 0.082%\n",
      "Epoch 6, Batch 125, LR 1.349465 Loss 19.059851, Accuracy 0.081%\n",
      "Epoch 6, Batch 126, LR 1.349762 Loss 19.061570, Accuracy 0.081%\n",
      "Epoch 6, Batch 127, LR 1.350058 Loss 19.060417, Accuracy 0.080%\n",
      "Epoch 6, Batch 128, LR 1.350355 Loss 19.059519, Accuracy 0.079%\n",
      "Epoch 6, Batch 129, LR 1.350652 Loss 19.058149, Accuracy 0.079%\n",
      "Epoch 6, Batch 130, LR 1.350949 Loss 19.058584, Accuracy 0.078%\n",
      "Epoch 6, Batch 131, LR 1.351246 Loss 19.058553, Accuracy 0.083%\n",
      "Epoch 6, Batch 132, LR 1.351542 Loss 19.058076, Accuracy 0.083%\n",
      "Epoch 6, Batch 133, LR 1.351839 Loss 19.056340, Accuracy 0.088%\n",
      "Epoch 6, Batch 134, LR 1.352136 Loss 19.056647, Accuracy 0.087%\n",
      "Epoch 6, Batch 135, LR 1.352433 Loss 19.056466, Accuracy 0.087%\n",
      "Epoch 6, Batch 136, LR 1.352730 Loss 19.057787, Accuracy 0.086%\n",
      "Epoch 6, Batch 137, LR 1.353026 Loss 19.057748, Accuracy 0.086%\n",
      "Epoch 6, Batch 138, LR 1.353323 Loss 19.058624, Accuracy 0.085%\n",
      "Epoch 6, Batch 139, LR 1.353620 Loss 19.058918, Accuracy 0.084%\n",
      "Epoch 6, Batch 140, LR 1.353917 Loss 19.060165, Accuracy 0.084%\n",
      "Epoch 6, Batch 141, LR 1.354214 Loss 19.059560, Accuracy 0.083%\n",
      "Epoch 6, Batch 142, LR 1.354510 Loss 19.059496, Accuracy 0.083%\n",
      "Epoch 6, Batch 143, LR 1.354807 Loss 19.059249, Accuracy 0.087%\n",
      "Epoch 6, Batch 144, LR 1.355104 Loss 19.058937, Accuracy 0.087%\n",
      "Epoch 6, Batch 145, LR 1.355401 Loss 19.059314, Accuracy 0.086%\n",
      "Epoch 6, Batch 146, LR 1.355697 Loss 19.059373, Accuracy 0.086%\n",
      "Epoch 6, Batch 147, LR 1.355994 Loss 19.059983, Accuracy 0.085%\n",
      "Epoch 6, Batch 148, LR 1.356291 Loss 19.059956, Accuracy 0.084%\n",
      "Epoch 6, Batch 149, LR 1.356588 Loss 19.060162, Accuracy 0.084%\n",
      "Epoch 6, Batch 150, LR 1.356884 Loss 19.059655, Accuracy 0.089%\n",
      "Epoch 6, Batch 151, LR 1.357181 Loss 19.059490, Accuracy 0.088%\n",
      "Epoch 6, Batch 152, LR 1.357478 Loss 19.059455, Accuracy 0.087%\n",
      "Epoch 6, Batch 153, LR 1.357775 Loss 19.057852, Accuracy 0.087%\n",
      "Epoch 6, Batch 154, LR 1.358071 Loss 19.057803, Accuracy 0.086%\n",
      "Epoch 6, Batch 155, LR 1.358368 Loss 19.059314, Accuracy 0.086%\n",
      "Epoch 6, Batch 156, LR 1.358665 Loss 19.058382, Accuracy 0.085%\n",
      "Epoch 6, Batch 157, LR 1.358962 Loss 19.057561, Accuracy 0.085%\n",
      "Epoch 6, Batch 158, LR 1.359258 Loss 19.058150, Accuracy 0.084%\n",
      "Epoch 6, Batch 159, LR 1.359555 Loss 19.057852, Accuracy 0.084%\n",
      "Epoch 6, Batch 160, LR 1.359852 Loss 19.057664, Accuracy 0.083%\n",
      "Epoch 6, Batch 161, LR 1.360148 Loss 19.058244, Accuracy 0.082%\n",
      "Epoch 6, Batch 162, LR 1.360445 Loss 19.056764, Accuracy 0.082%\n",
      "Epoch 6, Batch 163, LR 1.360742 Loss 19.057684, Accuracy 0.081%\n",
      "Epoch 6, Batch 164, LR 1.361039 Loss 19.057402, Accuracy 0.081%\n",
      "Epoch 6, Batch 165, LR 1.361335 Loss 19.057074, Accuracy 0.080%\n",
      "Epoch 6, Batch 166, LR 1.361632 Loss 19.056948, Accuracy 0.080%\n",
      "Epoch 6, Batch 167, LR 1.361929 Loss 19.057779, Accuracy 0.080%\n",
      "Epoch 6, Batch 168, LR 1.362225 Loss 19.057220, Accuracy 0.079%\n",
      "Epoch 6, Batch 169, LR 1.362522 Loss 19.056873, Accuracy 0.079%\n",
      "Epoch 6, Batch 170, LR 1.362819 Loss 19.057029, Accuracy 0.078%\n",
      "Epoch 6, Batch 171, LR 1.363115 Loss 19.057438, Accuracy 0.078%\n",
      "Epoch 6, Batch 172, LR 1.363412 Loss 19.058436, Accuracy 0.077%\n",
      "Epoch 6, Batch 173, LR 1.363709 Loss 19.059008, Accuracy 0.077%\n",
      "Epoch 6, Batch 174, LR 1.364005 Loss 19.058403, Accuracy 0.076%\n",
      "Epoch 6, Batch 175, LR 1.364302 Loss 19.058205, Accuracy 0.076%\n",
      "Epoch 6, Batch 176, LR 1.364599 Loss 19.058722, Accuracy 0.075%\n",
      "Epoch 6, Batch 177, LR 1.364895 Loss 19.058938, Accuracy 0.075%\n",
      "Epoch 6, Batch 178, LR 1.365192 Loss 19.059433, Accuracy 0.075%\n",
      "Epoch 6, Batch 179, LR 1.365489 Loss 19.059753, Accuracy 0.074%\n",
      "Epoch 6, Batch 180, LR 1.365785 Loss 19.059841, Accuracy 0.074%\n",
      "Epoch 6, Batch 181, LR 1.366082 Loss 19.060034, Accuracy 0.073%\n",
      "Epoch 6, Batch 182, LR 1.366379 Loss 19.060424, Accuracy 0.073%\n",
      "Epoch 6, Batch 183, LR 1.366675 Loss 19.059848, Accuracy 0.073%\n",
      "Epoch 6, Batch 184, LR 1.366972 Loss 19.059046, Accuracy 0.072%\n",
      "Epoch 6, Batch 185, LR 1.367269 Loss 19.059010, Accuracy 0.072%\n",
      "Epoch 6, Batch 186, LR 1.367565 Loss 19.059803, Accuracy 0.071%\n",
      "Epoch 6, Batch 187, LR 1.367862 Loss 19.060296, Accuracy 0.071%\n",
      "Epoch 6, Batch 188, LR 1.368159 Loss 19.061135, Accuracy 0.071%\n",
      "Epoch 6, Batch 189, LR 1.368455 Loss 19.060250, Accuracy 0.070%\n",
      "Epoch 6, Batch 190, LR 1.368752 Loss 19.059953, Accuracy 0.070%\n",
      "Epoch 6, Batch 191, LR 1.369048 Loss 19.059599, Accuracy 0.074%\n",
      "Epoch 6, Batch 192, LR 1.369345 Loss 19.060291, Accuracy 0.073%\n",
      "Epoch 6, Batch 193, LR 1.369642 Loss 19.061062, Accuracy 0.073%\n",
      "Epoch 6, Batch 194, LR 1.369938 Loss 19.061327, Accuracy 0.072%\n",
      "Epoch 6, Batch 195, LR 1.370235 Loss 19.061225, Accuracy 0.072%\n",
      "Epoch 6, Batch 196, LR 1.370532 Loss 19.062102, Accuracy 0.072%\n",
      "Epoch 6, Batch 197, LR 1.370828 Loss 19.060915, Accuracy 0.075%\n",
      "Epoch 6, Batch 198, LR 1.371125 Loss 19.060533, Accuracy 0.075%\n",
      "Epoch 6, Batch 199, LR 1.371421 Loss 19.061786, Accuracy 0.075%\n",
      "Epoch 6, Batch 200, LR 1.371718 Loss 19.062303, Accuracy 0.074%\n",
      "Epoch 6, Batch 201, LR 1.372014 Loss 19.062240, Accuracy 0.074%\n",
      "Epoch 6, Batch 202, LR 1.372311 Loss 19.062889, Accuracy 0.073%\n",
      "Epoch 6, Batch 203, LR 1.372608 Loss 19.063222, Accuracy 0.073%\n",
      "Epoch 6, Batch 204, LR 1.372904 Loss 19.063085, Accuracy 0.073%\n",
      "Epoch 6, Batch 205, LR 1.373201 Loss 19.063838, Accuracy 0.072%\n",
      "Epoch 6, Batch 206, LR 1.373497 Loss 19.064517, Accuracy 0.072%\n",
      "Epoch 6, Batch 207, LR 1.373794 Loss 19.064302, Accuracy 0.072%\n",
      "Epoch 6, Batch 208, LR 1.374090 Loss 19.065592, Accuracy 0.071%\n",
      "Epoch 6, Batch 209, LR 1.374387 Loss 19.065994, Accuracy 0.071%\n",
      "Epoch 6, Batch 210, LR 1.374684 Loss 19.066508, Accuracy 0.071%\n",
      "Epoch 6, Batch 211, LR 1.374980 Loss 19.066232, Accuracy 0.070%\n",
      "Epoch 6, Batch 212, LR 1.375277 Loss 19.067105, Accuracy 0.070%\n",
      "Epoch 6, Batch 213, LR 1.375573 Loss 19.066527, Accuracy 0.070%\n",
      "Epoch 6, Batch 214, LR 1.375870 Loss 19.066980, Accuracy 0.069%\n",
      "Epoch 6, Batch 215, LR 1.376166 Loss 19.067120, Accuracy 0.069%\n",
      "Epoch 6, Batch 216, LR 1.376463 Loss 19.067096, Accuracy 0.069%\n",
      "Epoch 6, Batch 217, LR 1.376759 Loss 19.066588, Accuracy 0.068%\n",
      "Epoch 6, Batch 218, LR 1.377056 Loss 19.065504, Accuracy 0.072%\n",
      "Epoch 6, Batch 219, LR 1.377352 Loss 19.065469, Accuracy 0.071%\n",
      "Epoch 6, Batch 220, LR 1.377649 Loss 19.065929, Accuracy 0.071%\n",
      "Epoch 6, Batch 221, LR 1.377945 Loss 19.066280, Accuracy 0.071%\n",
      "Epoch 6, Batch 222, LR 1.378242 Loss 19.066172, Accuracy 0.074%\n",
      "Epoch 6, Batch 223, LR 1.378538 Loss 19.066550, Accuracy 0.074%\n",
      "Epoch 6, Batch 224, LR 1.378835 Loss 19.067061, Accuracy 0.073%\n",
      "Epoch 6, Batch 225, LR 1.379131 Loss 19.066734, Accuracy 0.073%\n",
      "Epoch 6, Batch 226, LR 1.379428 Loss 19.067037, Accuracy 0.073%\n",
      "Epoch 6, Batch 227, LR 1.379724 Loss 19.067345, Accuracy 0.072%\n",
      "Epoch 6, Batch 228, LR 1.380021 Loss 19.068215, Accuracy 0.072%\n",
      "Epoch 6, Batch 229, LR 1.380317 Loss 19.068831, Accuracy 0.072%\n",
      "Epoch 6, Batch 230, LR 1.380614 Loss 19.069401, Accuracy 0.075%\n",
      "Epoch 6, Batch 231, LR 1.380910 Loss 19.070425, Accuracy 0.074%\n",
      "Epoch 6, Batch 232, LR 1.381207 Loss 19.071237, Accuracy 0.074%\n",
      "Epoch 6, Batch 233, LR 1.381503 Loss 19.070514, Accuracy 0.074%\n",
      "Epoch 6, Batch 234, LR 1.381800 Loss 19.069712, Accuracy 0.073%\n",
      "Epoch 6, Batch 235, LR 1.382096 Loss 19.069144, Accuracy 0.073%\n",
      "Epoch 6, Batch 236, LR 1.382392 Loss 19.068940, Accuracy 0.073%\n",
      "Epoch 6, Batch 237, LR 1.382689 Loss 19.069819, Accuracy 0.073%\n",
      "Epoch 6, Batch 238, LR 1.382985 Loss 19.069785, Accuracy 0.072%\n",
      "Epoch 6, Batch 239, LR 1.383282 Loss 19.070339, Accuracy 0.072%\n",
      "Epoch 6, Batch 240, LR 1.383578 Loss 19.070589, Accuracy 0.072%\n",
      "Epoch 6, Batch 241, LR 1.383875 Loss 19.070754, Accuracy 0.071%\n",
      "Epoch 6, Batch 242, LR 1.384171 Loss 19.070361, Accuracy 0.071%\n",
      "Epoch 6, Batch 243, LR 1.384467 Loss 19.071278, Accuracy 0.071%\n",
      "Epoch 6, Batch 244, LR 1.384764 Loss 19.070310, Accuracy 0.070%\n",
      "Epoch 6, Batch 245, LR 1.385060 Loss 19.070554, Accuracy 0.070%\n",
      "Epoch 6, Batch 246, LR 1.385357 Loss 19.070915, Accuracy 0.070%\n",
      "Epoch 6, Batch 247, LR 1.385653 Loss 19.072138, Accuracy 0.070%\n",
      "Epoch 6, Batch 248, LR 1.385949 Loss 19.072267, Accuracy 0.069%\n",
      "Epoch 6, Batch 249, LR 1.386246 Loss 19.071647, Accuracy 0.069%\n",
      "Epoch 6, Batch 250, LR 1.386542 Loss 19.071858, Accuracy 0.069%\n",
      "Epoch 6, Batch 251, LR 1.386839 Loss 19.070863, Accuracy 0.068%\n",
      "Epoch 6, Batch 252, LR 1.387135 Loss 19.071040, Accuracy 0.068%\n",
      "Epoch 6, Batch 253, LR 1.387431 Loss 19.071391, Accuracy 0.068%\n",
      "Epoch 6, Batch 254, LR 1.387728 Loss 19.071148, Accuracy 0.068%\n",
      "Epoch 6, Batch 255, LR 1.388024 Loss 19.071478, Accuracy 0.070%\n",
      "Epoch 6, Batch 256, LR 1.388320 Loss 19.071978, Accuracy 0.070%\n",
      "Epoch 6, Batch 257, LR 1.388617 Loss 19.071238, Accuracy 0.070%\n",
      "Epoch 6, Batch 258, LR 1.388913 Loss 19.071127, Accuracy 0.073%\n",
      "Epoch 6, Batch 259, LR 1.389209 Loss 19.070754, Accuracy 0.072%\n",
      "Epoch 6, Batch 260, LR 1.389506 Loss 19.070891, Accuracy 0.072%\n",
      "Epoch 6, Batch 261, LR 1.389802 Loss 19.070847, Accuracy 0.072%\n",
      "Epoch 6, Batch 262, LR 1.390098 Loss 19.071063, Accuracy 0.072%\n",
      "Epoch 6, Batch 263, LR 1.390395 Loss 19.071512, Accuracy 0.071%\n",
      "Epoch 6, Batch 264, LR 1.390691 Loss 19.071154, Accuracy 0.071%\n",
      "Epoch 6, Batch 265, LR 1.390987 Loss 19.071128, Accuracy 0.071%\n",
      "Epoch 6, Batch 266, LR 1.391284 Loss 19.070423, Accuracy 0.070%\n",
      "Epoch 6, Batch 267, LR 1.391580 Loss 19.070020, Accuracy 0.070%\n",
      "Epoch 6, Batch 268, LR 1.391876 Loss 19.069722, Accuracy 0.070%\n",
      "Epoch 6, Batch 269, LR 1.392172 Loss 19.069694, Accuracy 0.073%\n",
      "Epoch 6, Batch 270, LR 1.392469 Loss 19.069124, Accuracy 0.072%\n",
      "Epoch 6, Batch 271, LR 1.392765 Loss 19.069277, Accuracy 0.072%\n",
      "Epoch 6, Batch 272, LR 1.393061 Loss 19.068794, Accuracy 0.072%\n",
      "Epoch 6, Batch 273, LR 1.393358 Loss 19.068696, Accuracy 0.072%\n",
      "Epoch 6, Batch 274, LR 1.393654 Loss 19.068381, Accuracy 0.071%\n",
      "Epoch 6, Batch 275, LR 1.393950 Loss 19.068485, Accuracy 0.071%\n",
      "Epoch 6, Batch 276, LR 1.394246 Loss 19.068639, Accuracy 0.074%\n",
      "Epoch 6, Batch 277, LR 1.394543 Loss 19.068611, Accuracy 0.073%\n",
      "Epoch 6, Batch 278, LR 1.394839 Loss 19.069004, Accuracy 0.076%\n",
      "Epoch 6, Batch 279, LR 1.395135 Loss 19.069717, Accuracy 0.076%\n",
      "Epoch 6, Batch 280, LR 1.395431 Loss 19.070205, Accuracy 0.078%\n",
      "Epoch 6, Batch 281, LR 1.395727 Loss 19.070532, Accuracy 0.078%\n",
      "Epoch 6, Batch 282, LR 1.396024 Loss 19.070283, Accuracy 0.078%\n",
      "Epoch 6, Batch 283, LR 1.396320 Loss 19.070055, Accuracy 0.077%\n",
      "Epoch 6, Batch 284, LR 1.396616 Loss 19.070095, Accuracy 0.077%\n",
      "Epoch 6, Batch 285, LR 1.396912 Loss 19.069772, Accuracy 0.077%\n",
      "Epoch 6, Batch 286, LR 1.397209 Loss 19.069630, Accuracy 0.076%\n",
      "Epoch 6, Batch 287, LR 1.397505 Loss 19.069583, Accuracy 0.076%\n",
      "Epoch 6, Batch 288, LR 1.397801 Loss 19.069869, Accuracy 0.076%\n",
      "Epoch 6, Batch 289, LR 1.398097 Loss 19.069593, Accuracy 0.076%\n",
      "Epoch 6, Batch 290, LR 1.398393 Loss 19.070042, Accuracy 0.075%\n",
      "Epoch 6, Batch 291, LR 1.398689 Loss 19.070244, Accuracy 0.075%\n",
      "Epoch 6, Batch 292, LR 1.398986 Loss 19.070273, Accuracy 0.075%\n",
      "Epoch 6, Batch 293, LR 1.399282 Loss 19.070429, Accuracy 0.075%\n",
      "Epoch 6, Batch 294, LR 1.399578 Loss 19.070114, Accuracy 0.074%\n",
      "Epoch 6, Batch 295, LR 1.399874 Loss 19.069826, Accuracy 0.074%\n",
      "Epoch 6, Batch 296, LR 1.400170 Loss 19.069491, Accuracy 0.074%\n",
      "Epoch 6, Batch 297, LR 1.400466 Loss 19.069077, Accuracy 0.074%\n",
      "Epoch 6, Batch 298, LR 1.400763 Loss 19.068313, Accuracy 0.076%\n",
      "Epoch 6, Batch 299, LR 1.401059 Loss 19.068197, Accuracy 0.076%\n",
      "Epoch 6, Batch 300, LR 1.401355 Loss 19.067969, Accuracy 0.076%\n",
      "Epoch 6, Batch 301, LR 1.401651 Loss 19.067973, Accuracy 0.075%\n",
      "Epoch 6, Batch 302, LR 1.401947 Loss 19.068070, Accuracy 0.075%\n",
      "Epoch 6, Batch 303, LR 1.402243 Loss 19.067625, Accuracy 0.077%\n",
      "Epoch 6, Batch 304, LR 1.402539 Loss 19.067446, Accuracy 0.080%\n",
      "Epoch 6, Batch 305, LR 1.402835 Loss 19.067259, Accuracy 0.079%\n",
      "Epoch 6, Batch 306, LR 1.403131 Loss 19.066967, Accuracy 0.079%\n",
      "Epoch 6, Batch 307, LR 1.403428 Loss 19.067257, Accuracy 0.079%\n",
      "Epoch 6, Batch 308, LR 1.403724 Loss 19.067481, Accuracy 0.081%\n",
      "Epoch 6, Batch 309, LR 1.404020 Loss 19.067391, Accuracy 0.081%\n",
      "Epoch 6, Batch 310, LR 1.404316 Loss 19.067008, Accuracy 0.081%\n",
      "Epoch 6, Batch 311, LR 1.404612 Loss 19.066834, Accuracy 0.083%\n",
      "Epoch 6, Batch 312, LR 1.404908 Loss 19.066656, Accuracy 0.083%\n",
      "Epoch 6, Batch 313, LR 1.405204 Loss 19.066571, Accuracy 0.082%\n",
      "Epoch 6, Batch 314, LR 1.405500 Loss 19.066923, Accuracy 0.082%\n",
      "Epoch 6, Batch 315, LR 1.405796 Loss 19.067098, Accuracy 0.082%\n",
      "Epoch 6, Batch 316, LR 1.406092 Loss 19.067973, Accuracy 0.082%\n",
      "Epoch 6, Batch 317, LR 1.406388 Loss 19.067601, Accuracy 0.081%\n",
      "Epoch 6, Batch 318, LR 1.406684 Loss 19.067780, Accuracy 0.081%\n",
      "Epoch 6, Batch 319, LR 1.406980 Loss 19.067486, Accuracy 0.083%\n",
      "Epoch 6, Batch 320, LR 1.407276 Loss 19.067516, Accuracy 0.085%\n",
      "Epoch 6, Batch 321, LR 1.407572 Loss 19.066399, Accuracy 0.085%\n",
      "Epoch 6, Batch 322, LR 1.407868 Loss 19.066022, Accuracy 0.085%\n",
      "Epoch 6, Batch 323, LR 1.408164 Loss 19.065584, Accuracy 0.085%\n",
      "Epoch 6, Batch 324, LR 1.408460 Loss 19.065616, Accuracy 0.084%\n",
      "Epoch 6, Batch 325, LR 1.408756 Loss 19.065671, Accuracy 0.084%\n",
      "Epoch 6, Batch 326, LR 1.409052 Loss 19.065130, Accuracy 0.084%\n",
      "Epoch 6, Batch 327, LR 1.409348 Loss 19.065049, Accuracy 0.084%\n",
      "Epoch 6, Batch 328, LR 1.409644 Loss 19.065182, Accuracy 0.083%\n",
      "Epoch 6, Batch 329, LR 1.409940 Loss 19.065367, Accuracy 0.085%\n",
      "Epoch 6, Batch 330, LR 1.410236 Loss 19.065610, Accuracy 0.085%\n",
      "Epoch 6, Batch 331, LR 1.410532 Loss 19.065205, Accuracy 0.085%\n",
      "Epoch 6, Batch 332, LR 1.410828 Loss 19.065364, Accuracy 0.085%\n",
      "Epoch 6, Batch 333, LR 1.411124 Loss 19.065419, Accuracy 0.084%\n",
      "Epoch 6, Batch 334, LR 1.411420 Loss 19.065981, Accuracy 0.084%\n",
      "Epoch 6, Batch 335, LR 1.411716 Loss 19.065908, Accuracy 0.086%\n",
      "Epoch 6, Batch 336, LR 1.412011 Loss 19.066180, Accuracy 0.086%\n",
      "Epoch 6, Batch 337, LR 1.412307 Loss 19.066335, Accuracy 0.086%\n",
      "Epoch 6, Batch 338, LR 1.412603 Loss 19.065977, Accuracy 0.086%\n",
      "Epoch 6, Batch 339, LR 1.412899 Loss 19.065559, Accuracy 0.085%\n",
      "Epoch 6, Batch 340, LR 1.413195 Loss 19.065221, Accuracy 0.085%\n",
      "Epoch 6, Batch 341, LR 1.413491 Loss 19.064827, Accuracy 0.085%\n",
      "Epoch 6, Batch 342, LR 1.413787 Loss 19.065413, Accuracy 0.085%\n",
      "Epoch 6, Batch 343, LR 1.414083 Loss 19.065099, Accuracy 0.084%\n",
      "Epoch 6, Batch 344, LR 1.414379 Loss 19.064755, Accuracy 0.084%\n",
      "Epoch 6, Batch 345, LR 1.414674 Loss 19.064414, Accuracy 0.084%\n",
      "Epoch 6, Batch 346, LR 1.414970 Loss 19.064056, Accuracy 0.084%\n",
      "Epoch 6, Batch 347, LR 1.415266 Loss 19.063832, Accuracy 0.083%\n",
      "Epoch 6, Batch 348, LR 1.415562 Loss 19.064092, Accuracy 0.083%\n",
      "Epoch 6, Batch 349, LR 1.415858 Loss 19.064270, Accuracy 0.083%\n",
      "Epoch 6, Batch 350, LR 1.416154 Loss 19.064613, Accuracy 0.083%\n",
      "Epoch 6, Batch 351, LR 1.416449 Loss 19.064965, Accuracy 0.082%\n",
      "Epoch 6, Batch 352, LR 1.416745 Loss 19.064845, Accuracy 0.082%\n",
      "Epoch 6, Batch 353, LR 1.417041 Loss 19.065032, Accuracy 0.082%\n",
      "Epoch 6, Batch 354, LR 1.417337 Loss 19.064862, Accuracy 0.082%\n",
      "Epoch 6, Batch 355, LR 1.417633 Loss 19.064702, Accuracy 0.081%\n",
      "Epoch 6, Batch 356, LR 1.417928 Loss 19.065145, Accuracy 0.081%\n",
      "Epoch 6, Batch 357, LR 1.418224 Loss 19.064812, Accuracy 0.081%\n",
      "Epoch 6, Batch 358, LR 1.418520 Loss 19.065324, Accuracy 0.081%\n",
      "Epoch 6, Batch 359, LR 1.418816 Loss 19.065243, Accuracy 0.083%\n",
      "Epoch 6, Batch 360, LR 1.419111 Loss 19.065501, Accuracy 0.085%\n",
      "Epoch 6, Batch 361, LR 1.419407 Loss 19.065452, Accuracy 0.084%\n",
      "Epoch 6, Batch 362, LR 1.419703 Loss 19.065969, Accuracy 0.084%\n",
      "Epoch 6, Batch 363, LR 1.419999 Loss 19.066151, Accuracy 0.084%\n",
      "Epoch 6, Batch 364, LR 1.420294 Loss 19.066588, Accuracy 0.084%\n",
      "Epoch 6, Batch 365, LR 1.420590 Loss 19.066589, Accuracy 0.083%\n",
      "Epoch 6, Batch 366, LR 1.420886 Loss 19.066771, Accuracy 0.083%\n",
      "Epoch 6, Batch 367, LR 1.421182 Loss 19.067119, Accuracy 0.083%\n",
      "Epoch 6, Batch 368, LR 1.421477 Loss 19.066831, Accuracy 0.083%\n",
      "Epoch 6, Batch 369, LR 1.421773 Loss 19.066334, Accuracy 0.083%\n",
      "Epoch 6, Batch 370, LR 1.422069 Loss 19.065691, Accuracy 0.082%\n",
      "Epoch 6, Batch 371, LR 1.422364 Loss 19.065664, Accuracy 0.082%\n",
      "Epoch 6, Batch 372, LR 1.422660 Loss 19.065759, Accuracy 0.082%\n",
      "Epoch 6, Batch 373, LR 1.422956 Loss 19.066060, Accuracy 0.082%\n",
      "Epoch 6, Batch 374, LR 1.423251 Loss 19.066250, Accuracy 0.081%\n",
      "Epoch 6, Batch 375, LR 1.423547 Loss 19.066641, Accuracy 0.081%\n",
      "Epoch 6, Batch 376, LR 1.423843 Loss 19.066721, Accuracy 0.081%\n",
      "Epoch 6, Batch 377, LR 1.424138 Loss 19.066695, Accuracy 0.081%\n",
      "Epoch 6, Batch 378, LR 1.424434 Loss 19.066622, Accuracy 0.081%\n",
      "Epoch 6, Batch 379, LR 1.424730 Loss 19.066309, Accuracy 0.082%\n",
      "Epoch 6, Batch 380, LR 1.425025 Loss 19.066566, Accuracy 0.082%\n",
      "Epoch 6, Batch 381, LR 1.425321 Loss 19.066871, Accuracy 0.082%\n",
      "Epoch 6, Batch 382, LR 1.425616 Loss 19.066707, Accuracy 0.082%\n",
      "Epoch 6, Batch 383, LR 1.425912 Loss 19.066771, Accuracy 0.082%\n",
      "Epoch 6, Batch 384, LR 1.426208 Loss 19.066929, Accuracy 0.081%\n",
      "Epoch 6, Batch 385, LR 1.426503 Loss 19.066877, Accuracy 0.081%\n",
      "Epoch 6, Batch 386, LR 1.426799 Loss 19.066781, Accuracy 0.081%\n",
      "Epoch 6, Batch 387, LR 1.427094 Loss 19.066657, Accuracy 0.081%\n",
      "Epoch 6, Batch 388, LR 1.427390 Loss 19.066542, Accuracy 0.081%\n",
      "Epoch 6, Batch 389, LR 1.427685 Loss 19.066635, Accuracy 0.080%\n",
      "Epoch 6, Batch 390, LR 1.427981 Loss 19.066762, Accuracy 0.080%\n",
      "Epoch 6, Batch 391, LR 1.428277 Loss 19.067254, Accuracy 0.080%\n",
      "Epoch 6, Batch 392, LR 1.428572 Loss 19.067499, Accuracy 0.080%\n",
      "Epoch 6, Batch 393, LR 1.428868 Loss 19.067216, Accuracy 0.082%\n",
      "Epoch 6, Batch 394, LR 1.429163 Loss 19.067021, Accuracy 0.081%\n",
      "Epoch 6, Batch 395, LR 1.429459 Loss 19.066689, Accuracy 0.081%\n",
      "Epoch 6, Batch 396, LR 1.429754 Loss 19.066266, Accuracy 0.081%\n",
      "Epoch 6, Batch 397, LR 1.430050 Loss 19.066901, Accuracy 0.081%\n",
      "Epoch 6, Batch 398, LR 1.430345 Loss 19.066403, Accuracy 0.080%\n",
      "Epoch 6, Batch 399, LR 1.430641 Loss 19.066439, Accuracy 0.080%\n",
      "Epoch 6, Batch 400, LR 1.430936 Loss 19.066590, Accuracy 0.080%\n",
      "Epoch 6, Batch 401, LR 1.431232 Loss 19.066665, Accuracy 0.080%\n",
      "Epoch 6, Batch 402, LR 1.431527 Loss 19.066698, Accuracy 0.080%\n",
      "Epoch 6, Batch 403, LR 1.431823 Loss 19.066390, Accuracy 0.081%\n",
      "Epoch 6, Batch 404, LR 1.432118 Loss 19.066603, Accuracy 0.081%\n",
      "Epoch 6, Batch 405, LR 1.432413 Loss 19.067102, Accuracy 0.081%\n",
      "Epoch 6, Batch 406, LR 1.432709 Loss 19.066991, Accuracy 0.083%\n",
      "Epoch 6, Batch 407, LR 1.433004 Loss 19.066776, Accuracy 0.083%\n",
      "Epoch 6, Batch 408, LR 1.433300 Loss 19.066735, Accuracy 0.082%\n",
      "Epoch 6, Batch 409, LR 1.433595 Loss 19.066345, Accuracy 0.082%\n",
      "Epoch 6, Batch 410, LR 1.433891 Loss 19.066041, Accuracy 0.082%\n",
      "Epoch 6, Batch 411, LR 1.434186 Loss 19.066197, Accuracy 0.082%\n",
      "Epoch 6, Batch 412, LR 1.434481 Loss 19.066137, Accuracy 0.082%\n",
      "Epoch 6, Batch 413, LR 1.434777 Loss 19.065713, Accuracy 0.081%\n",
      "Epoch 6, Batch 414, LR 1.435072 Loss 19.065981, Accuracy 0.081%\n",
      "Epoch 6, Batch 415, LR 1.435367 Loss 19.066000, Accuracy 0.081%\n",
      "Epoch 6, Batch 416, LR 1.435663 Loss 19.065690, Accuracy 0.081%\n",
      "Epoch 6, Batch 417, LR 1.435958 Loss 19.065836, Accuracy 0.081%\n",
      "Epoch 6, Batch 418, LR 1.436253 Loss 19.065409, Accuracy 0.082%\n",
      "Epoch 6, Batch 419, LR 1.436549 Loss 19.065456, Accuracy 0.082%\n",
      "Epoch 6, Batch 420, LR 1.436844 Loss 19.065170, Accuracy 0.082%\n",
      "Epoch 6, Batch 421, LR 1.437139 Loss 19.065142, Accuracy 0.082%\n",
      "Epoch 6, Batch 422, LR 1.437435 Loss 19.065343, Accuracy 0.081%\n",
      "Epoch 6, Batch 423, LR 1.437730 Loss 19.064805, Accuracy 0.081%\n",
      "Epoch 6, Batch 424, LR 1.438025 Loss 19.064965, Accuracy 0.081%\n",
      "Epoch 6, Batch 425, LR 1.438321 Loss 19.065226, Accuracy 0.081%\n",
      "Epoch 6, Batch 426, LR 1.438616 Loss 19.065537, Accuracy 0.083%\n",
      "Epoch 6, Batch 427, LR 1.438911 Loss 19.065958, Accuracy 0.082%\n",
      "Epoch 6, Batch 428, LR 1.439206 Loss 19.065868, Accuracy 0.082%\n",
      "Epoch 6, Batch 429, LR 1.439502 Loss 19.066188, Accuracy 0.082%\n",
      "Epoch 6, Batch 430, LR 1.439797 Loss 19.066306, Accuracy 0.082%\n",
      "Epoch 6, Batch 431, LR 1.440092 Loss 19.066617, Accuracy 0.082%\n",
      "Epoch 6, Batch 432, LR 1.440387 Loss 19.066250, Accuracy 0.081%\n",
      "Epoch 6, Batch 433, LR 1.440683 Loss 19.066249, Accuracy 0.081%\n",
      "Epoch 6, Batch 434, LR 1.440978 Loss 19.066843, Accuracy 0.081%\n",
      "Epoch 6, Batch 435, LR 1.441273 Loss 19.066500, Accuracy 0.081%\n",
      "Epoch 6, Batch 436, LR 1.441568 Loss 19.066391, Accuracy 0.082%\n",
      "Epoch 6, Batch 437, LR 1.441863 Loss 19.066609, Accuracy 0.082%\n",
      "Epoch 6, Batch 438, LR 1.442159 Loss 19.066077, Accuracy 0.082%\n",
      "Epoch 6, Batch 439, LR 1.442454 Loss 19.065785, Accuracy 0.082%\n",
      "Epoch 6, Batch 440, LR 1.442749 Loss 19.065510, Accuracy 0.082%\n",
      "Epoch 6, Batch 441, LR 1.443044 Loss 19.065397, Accuracy 0.081%\n",
      "Epoch 6, Batch 442, LR 1.443339 Loss 19.065698, Accuracy 0.081%\n",
      "Epoch 6, Batch 443, LR 1.443634 Loss 19.065763, Accuracy 0.081%\n",
      "Epoch 6, Batch 444, LR 1.443930 Loss 19.066242, Accuracy 0.081%\n",
      "Epoch 6, Batch 445, LR 1.444225 Loss 19.066187, Accuracy 0.081%\n",
      "Epoch 6, Batch 446, LR 1.444520 Loss 19.066085, Accuracy 0.082%\n",
      "Epoch 6, Batch 447, LR 1.444815 Loss 19.066094, Accuracy 0.082%\n",
      "Epoch 6, Batch 448, LR 1.445110 Loss 19.066408, Accuracy 0.082%\n",
      "Epoch 6, Batch 449, LR 1.445405 Loss 19.066337, Accuracy 0.082%\n",
      "Epoch 6, Batch 450, LR 1.445700 Loss 19.066488, Accuracy 0.082%\n",
      "Epoch 6, Batch 451, LR 1.445995 Loss 19.066070, Accuracy 0.081%\n",
      "Epoch 6, Batch 452, LR 1.446290 Loss 19.066249, Accuracy 0.081%\n",
      "Epoch 6, Batch 453, LR 1.446585 Loss 19.066182, Accuracy 0.081%\n",
      "Epoch 6, Batch 454, LR 1.446880 Loss 19.066020, Accuracy 0.081%\n",
      "Epoch 6, Batch 455, LR 1.447176 Loss 19.066370, Accuracy 0.082%\n",
      "Epoch 6, Batch 456, LR 1.447471 Loss 19.066313, Accuracy 0.082%\n",
      "Epoch 6, Batch 457, LR 1.447766 Loss 19.066109, Accuracy 0.082%\n",
      "Epoch 6, Batch 458, LR 1.448061 Loss 19.066202, Accuracy 0.082%\n",
      "Epoch 6, Batch 459, LR 1.448356 Loss 19.066503, Accuracy 0.082%\n",
      "Epoch 6, Batch 460, LR 1.448651 Loss 19.066681, Accuracy 0.082%\n",
      "Epoch 6, Batch 461, LR 1.448946 Loss 19.066502, Accuracy 0.081%\n",
      "Epoch 6, Batch 462, LR 1.449241 Loss 19.066503, Accuracy 0.081%\n",
      "Epoch 6, Batch 463, LR 1.449536 Loss 19.067096, Accuracy 0.081%\n",
      "Epoch 6, Batch 464, LR 1.449831 Loss 19.067259, Accuracy 0.081%\n",
      "Epoch 6, Batch 465, LR 1.450125 Loss 19.067023, Accuracy 0.081%\n",
      "Epoch 6, Batch 466, LR 1.450420 Loss 19.067461, Accuracy 0.080%\n",
      "Epoch 6, Batch 467, LR 1.450715 Loss 19.067228, Accuracy 0.080%\n",
      "Epoch 6, Batch 468, LR 1.451010 Loss 19.067375, Accuracy 0.080%\n",
      "Epoch 6, Batch 469, LR 1.451305 Loss 19.067366, Accuracy 0.080%\n",
      "Epoch 6, Batch 470, LR 1.451600 Loss 19.067659, Accuracy 0.080%\n",
      "Epoch 6, Batch 471, LR 1.451895 Loss 19.067509, Accuracy 0.080%\n",
      "Epoch 6, Batch 472, LR 1.452190 Loss 19.067409, Accuracy 0.079%\n",
      "Epoch 6, Batch 473, LR 1.452485 Loss 19.067534, Accuracy 0.079%\n",
      "Epoch 6, Batch 474, LR 1.452780 Loss 19.067411, Accuracy 0.079%\n",
      "Epoch 6, Batch 475, LR 1.453075 Loss 19.067369, Accuracy 0.079%\n",
      "Epoch 6, Batch 476, LR 1.453369 Loss 19.067129, Accuracy 0.079%\n",
      "Epoch 6, Batch 477, LR 1.453664 Loss 19.066902, Accuracy 0.079%\n",
      "Epoch 6, Batch 478, LR 1.453959 Loss 19.066285, Accuracy 0.078%\n",
      "Epoch 6, Batch 479, LR 1.454254 Loss 19.066338, Accuracy 0.078%\n",
      "Epoch 6, Batch 480, LR 1.454549 Loss 19.066529, Accuracy 0.078%\n",
      "Epoch 6, Batch 481, LR 1.454844 Loss 19.066459, Accuracy 0.078%\n",
      "Epoch 6, Batch 482, LR 1.455138 Loss 19.066321, Accuracy 0.078%\n",
      "Epoch 6, Batch 483, LR 1.455433 Loss 19.066062, Accuracy 0.078%\n",
      "Epoch 6, Batch 484, LR 1.455728 Loss 19.066110, Accuracy 0.077%\n",
      "Epoch 6, Batch 485, LR 1.456023 Loss 19.066289, Accuracy 0.077%\n",
      "Epoch 6, Batch 486, LR 1.456318 Loss 19.066084, Accuracy 0.077%\n",
      "Epoch 6, Batch 487, LR 1.456612 Loss 19.065899, Accuracy 0.077%\n",
      "Epoch 6, Batch 488, LR 1.456907 Loss 19.066389, Accuracy 0.077%\n",
      "Epoch 6, Batch 489, LR 1.457202 Loss 19.066540, Accuracy 0.077%\n",
      "Epoch 6, Batch 490, LR 1.457497 Loss 19.066252, Accuracy 0.077%\n",
      "Epoch 6, Batch 491, LR 1.457791 Loss 19.066219, Accuracy 0.076%\n",
      "Epoch 6, Batch 492, LR 1.458086 Loss 19.066447, Accuracy 0.076%\n",
      "Epoch 6, Batch 493, LR 1.458381 Loss 19.066658, Accuracy 0.076%\n",
      "Epoch 6, Batch 494, LR 1.458675 Loss 19.066825, Accuracy 0.076%\n",
      "Epoch 6, Batch 495, LR 1.458970 Loss 19.067067, Accuracy 0.076%\n",
      "Epoch 6, Batch 496, LR 1.459265 Loss 19.066992, Accuracy 0.076%\n",
      "Epoch 6, Batch 497, LR 1.459559 Loss 19.066876, Accuracy 0.075%\n",
      "Epoch 6, Batch 498, LR 1.459854 Loss 19.066760, Accuracy 0.075%\n",
      "Epoch 6, Batch 499, LR 1.460149 Loss 19.066850, Accuracy 0.075%\n",
      "Epoch 6, Batch 500, LR 1.460443 Loss 19.066648, Accuracy 0.075%\n",
      "Epoch 6, Batch 501, LR 1.460738 Loss 19.066778, Accuracy 0.075%\n",
      "Epoch 6, Batch 502, LR 1.461033 Loss 19.067127, Accuracy 0.075%\n",
      "Epoch 6, Batch 503, LR 1.461327 Loss 19.067205, Accuracy 0.075%\n",
      "Epoch 6, Batch 504, LR 1.461622 Loss 19.067170, Accuracy 0.074%\n",
      "Epoch 6, Batch 505, LR 1.461917 Loss 19.066790, Accuracy 0.074%\n",
      "Epoch 6, Batch 506, LR 1.462211 Loss 19.067195, Accuracy 0.074%\n",
      "Epoch 6, Batch 507, LR 1.462506 Loss 19.066912, Accuracy 0.074%\n",
      "Epoch 6, Batch 508, LR 1.462800 Loss 19.066778, Accuracy 0.074%\n",
      "Epoch 6, Batch 509, LR 1.463095 Loss 19.066496, Accuracy 0.075%\n",
      "Epoch 6, Batch 510, LR 1.463389 Loss 19.066414, Accuracy 0.075%\n",
      "Epoch 6, Batch 511, LR 1.463684 Loss 19.066070, Accuracy 0.075%\n",
      "Epoch 6, Batch 512, LR 1.463978 Loss 19.066024, Accuracy 0.075%\n",
      "Epoch 6, Batch 513, LR 1.464273 Loss 19.066027, Accuracy 0.075%\n",
      "Epoch 6, Batch 514, LR 1.464568 Loss 19.065820, Accuracy 0.074%\n",
      "Epoch 6, Batch 515, LR 1.464862 Loss 19.065799, Accuracy 0.074%\n",
      "Epoch 6, Batch 516, LR 1.465157 Loss 19.065684, Accuracy 0.074%\n",
      "Epoch 6, Batch 517, LR 1.465451 Loss 19.065712, Accuracy 0.074%\n",
      "Epoch 6, Batch 518, LR 1.465745 Loss 19.065426, Accuracy 0.074%\n",
      "Epoch 6, Batch 519, LR 1.466040 Loss 19.065464, Accuracy 0.074%\n",
      "Epoch 6, Batch 520, LR 1.466334 Loss 19.065141, Accuracy 0.074%\n",
      "Epoch 6, Batch 521, LR 1.466629 Loss 19.065167, Accuracy 0.073%\n",
      "Epoch 6, Batch 522, LR 1.466923 Loss 19.065001, Accuracy 0.073%\n",
      "Epoch 6, Batch 523, LR 1.467218 Loss 19.064962, Accuracy 0.073%\n",
      "Epoch 6, Batch 524, LR 1.467512 Loss 19.065245, Accuracy 0.073%\n",
      "Epoch 6, Batch 525, LR 1.467807 Loss 19.065750, Accuracy 0.073%\n",
      "Epoch 6, Batch 526, LR 1.468101 Loss 19.065899, Accuracy 0.073%\n",
      "Epoch 6, Batch 527, LR 1.468395 Loss 19.065822, Accuracy 0.073%\n",
      "Epoch 6, Batch 528, LR 1.468690 Loss 19.065928, Accuracy 0.073%\n",
      "Epoch 6, Batch 529, LR 1.468984 Loss 19.066007, Accuracy 0.072%\n",
      "Epoch 6, Batch 530, LR 1.469278 Loss 19.065816, Accuracy 0.074%\n",
      "Epoch 6, Batch 531, LR 1.469573 Loss 19.066031, Accuracy 0.074%\n",
      "Epoch 6, Batch 532, LR 1.469867 Loss 19.065983, Accuracy 0.073%\n",
      "Epoch 6, Batch 533, LR 1.470161 Loss 19.066133, Accuracy 0.073%\n",
      "Epoch 6, Batch 534, LR 1.470456 Loss 19.066257, Accuracy 0.073%\n",
      "Epoch 6, Batch 535, LR 1.470750 Loss 19.066326, Accuracy 0.073%\n",
      "Epoch 6, Batch 536, LR 1.471044 Loss 19.066326, Accuracy 0.073%\n",
      "Epoch 6, Batch 537, LR 1.471339 Loss 19.066129, Accuracy 0.073%\n",
      "Epoch 6, Batch 538, LR 1.471633 Loss 19.066668, Accuracy 0.073%\n",
      "Epoch 6, Batch 539, LR 1.471927 Loss 19.066646, Accuracy 0.072%\n",
      "Epoch 6, Batch 540, LR 1.472221 Loss 19.066695, Accuracy 0.072%\n",
      "Epoch 6, Batch 541, LR 1.472516 Loss 19.066597, Accuracy 0.072%\n",
      "Epoch 6, Batch 542, LR 1.472810 Loss 19.066376, Accuracy 0.072%\n",
      "Epoch 6, Batch 543, LR 1.473104 Loss 19.066146, Accuracy 0.072%\n",
      "Epoch 6, Batch 544, LR 1.473398 Loss 19.066015, Accuracy 0.072%\n",
      "Epoch 6, Batch 545, LR 1.473693 Loss 19.066068, Accuracy 0.073%\n",
      "Epoch 6, Batch 546, LR 1.473987 Loss 19.066329, Accuracy 0.073%\n",
      "Epoch 6, Batch 547, LR 1.474281 Loss 19.066556, Accuracy 0.073%\n",
      "Epoch 6, Batch 548, LR 1.474575 Loss 19.066292, Accuracy 0.073%\n",
      "Epoch 6, Batch 549, LR 1.474869 Loss 19.066242, Accuracy 0.073%\n",
      "Epoch 6, Batch 550, LR 1.475164 Loss 19.066169, Accuracy 0.072%\n",
      "Epoch 6, Batch 551, LR 1.475458 Loss 19.066261, Accuracy 0.072%\n",
      "Epoch 6, Batch 552, LR 1.475752 Loss 19.066749, Accuracy 0.072%\n",
      "Epoch 6, Batch 553, LR 1.476046 Loss 19.066547, Accuracy 0.072%\n",
      "Epoch 6, Batch 554, LR 1.476340 Loss 19.066702, Accuracy 0.072%\n",
      "Epoch 6, Batch 555, LR 1.476634 Loss 19.066398, Accuracy 0.072%\n",
      "Epoch 6, Batch 556, LR 1.476928 Loss 19.066477, Accuracy 0.072%\n",
      "Epoch 6, Batch 557, LR 1.477222 Loss 19.066366, Accuracy 0.072%\n",
      "Epoch 6, Batch 558, LR 1.477516 Loss 19.066159, Accuracy 0.071%\n",
      "Epoch 6, Batch 559, LR 1.477811 Loss 19.066332, Accuracy 0.071%\n",
      "Epoch 6, Batch 560, LR 1.478105 Loss 19.066349, Accuracy 0.071%\n",
      "Epoch 6, Batch 561, LR 1.478399 Loss 19.066501, Accuracy 0.071%\n",
      "Epoch 6, Batch 562, LR 1.478693 Loss 19.066828, Accuracy 0.072%\n",
      "Epoch 6, Batch 563, LR 1.478987 Loss 19.066619, Accuracy 0.072%\n",
      "Epoch 6, Batch 564, LR 1.479281 Loss 19.066630, Accuracy 0.072%\n",
      "Epoch 6, Batch 565, LR 1.479575 Loss 19.066465, Accuracy 0.072%\n",
      "Epoch 6, Batch 566, LR 1.479869 Loss 19.066024, Accuracy 0.072%\n",
      "Epoch 6, Batch 567, LR 1.480163 Loss 19.065823, Accuracy 0.072%\n",
      "Epoch 6, Batch 568, LR 1.480457 Loss 19.065651, Accuracy 0.072%\n",
      "Epoch 6, Batch 569, LR 1.480751 Loss 19.065376, Accuracy 0.071%\n",
      "Epoch 6, Batch 570, LR 1.481045 Loss 19.065512, Accuracy 0.071%\n",
      "Epoch 6, Batch 571, LR 1.481339 Loss 19.065486, Accuracy 0.071%\n",
      "Epoch 6, Batch 572, LR 1.481632 Loss 19.065296, Accuracy 0.071%\n",
      "Epoch 6, Batch 573, LR 1.481926 Loss 19.065062, Accuracy 0.072%\n",
      "Epoch 6, Batch 574, LR 1.482220 Loss 19.064832, Accuracy 0.072%\n",
      "Epoch 6, Batch 575, LR 1.482514 Loss 19.064989, Accuracy 0.072%\n",
      "Epoch 6, Batch 576, LR 1.482808 Loss 19.064923, Accuracy 0.072%\n",
      "Epoch 6, Batch 577, LR 1.483102 Loss 19.064626, Accuracy 0.073%\n",
      "Epoch 6, Batch 578, LR 1.483396 Loss 19.064499, Accuracy 0.073%\n",
      "Epoch 6, Batch 579, LR 1.483690 Loss 19.064925, Accuracy 0.073%\n",
      "Epoch 6, Batch 580, LR 1.483984 Loss 19.064946, Accuracy 0.073%\n",
      "Epoch 6, Batch 581, LR 1.484277 Loss 19.064881, Accuracy 0.074%\n",
      "Epoch 6, Batch 582, LR 1.484571 Loss 19.064963, Accuracy 0.074%\n",
      "Epoch 6, Batch 583, LR 1.484865 Loss 19.064834, Accuracy 0.074%\n",
      "Epoch 6, Batch 584, LR 1.485159 Loss 19.065116, Accuracy 0.074%\n",
      "Epoch 6, Batch 585, LR 1.485453 Loss 19.064929, Accuracy 0.073%\n",
      "Epoch 6, Batch 586, LR 1.485746 Loss 19.064815, Accuracy 0.073%\n",
      "Epoch 6, Batch 587, LR 1.486040 Loss 19.064961, Accuracy 0.073%\n",
      "Epoch 6, Batch 588, LR 1.486334 Loss 19.064570, Accuracy 0.073%\n",
      "Epoch 6, Batch 589, LR 1.486628 Loss 19.064878, Accuracy 0.073%\n",
      "Epoch 6, Batch 590, LR 1.486921 Loss 19.064814, Accuracy 0.073%\n",
      "Epoch 6, Batch 591, LR 1.487215 Loss 19.064971, Accuracy 0.073%\n",
      "Epoch 6, Batch 592, LR 1.487509 Loss 19.065030, Accuracy 0.073%\n",
      "Epoch 6, Batch 593, LR 1.487803 Loss 19.065032, Accuracy 0.072%\n",
      "Epoch 6, Batch 594, LR 1.488096 Loss 19.065184, Accuracy 0.072%\n",
      "Epoch 6, Batch 595, LR 1.488390 Loss 19.064888, Accuracy 0.072%\n",
      "Epoch 6, Batch 596, LR 1.488684 Loss 19.064572, Accuracy 0.072%\n",
      "Epoch 6, Batch 597, LR 1.488977 Loss 19.064535, Accuracy 0.072%\n",
      "Epoch 6, Batch 598, LR 1.489271 Loss 19.064451, Accuracy 0.072%\n",
      "Epoch 6, Batch 599, LR 1.489565 Loss 19.064457, Accuracy 0.072%\n",
      "Epoch 6, Batch 600, LR 1.489858 Loss 19.064518, Accuracy 0.072%\n",
      "Epoch 6, Batch 601, LR 1.490152 Loss 19.064475, Accuracy 0.071%\n",
      "Epoch 6, Batch 602, LR 1.490445 Loss 19.064392, Accuracy 0.071%\n",
      "Epoch 6, Batch 603, LR 1.490739 Loss 19.064607, Accuracy 0.071%\n",
      "Epoch 6, Batch 604, LR 1.491033 Loss 19.064505, Accuracy 0.071%\n",
      "Epoch 6, Batch 605, LR 1.491326 Loss 19.064627, Accuracy 0.071%\n",
      "Epoch 6, Batch 606, LR 1.491620 Loss 19.064825, Accuracy 0.071%\n",
      "Epoch 6, Batch 607, LR 1.491913 Loss 19.064738, Accuracy 0.071%\n",
      "Epoch 6, Batch 608, LR 1.492207 Loss 19.064892, Accuracy 0.072%\n",
      "Epoch 6, Batch 609, LR 1.492500 Loss 19.064623, Accuracy 0.073%\n",
      "Epoch 6, Batch 610, LR 1.492794 Loss 19.064778, Accuracy 0.073%\n",
      "Epoch 6, Batch 611, LR 1.493087 Loss 19.064949, Accuracy 0.073%\n",
      "Epoch 6, Batch 612, LR 1.493381 Loss 19.064719, Accuracy 0.073%\n",
      "Epoch 6, Batch 613, LR 1.493674 Loss 19.064833, Accuracy 0.073%\n",
      "Epoch 6, Batch 614, LR 1.493968 Loss 19.064678, Accuracy 0.073%\n",
      "Epoch 6, Batch 615, LR 1.494261 Loss 19.064740, Accuracy 0.074%\n",
      "Epoch 6, Batch 616, LR 1.494555 Loss 19.064790, Accuracy 0.074%\n",
      "Epoch 6, Batch 617, LR 1.494848 Loss 19.064771, Accuracy 0.073%\n",
      "Epoch 6, Batch 618, LR 1.495142 Loss 19.064808, Accuracy 0.073%\n",
      "Epoch 6, Batch 619, LR 1.495435 Loss 19.064565, Accuracy 0.073%\n",
      "Epoch 6, Batch 620, LR 1.495728 Loss 19.064666, Accuracy 0.073%\n",
      "Epoch 6, Batch 621, LR 1.496022 Loss 19.064377, Accuracy 0.074%\n",
      "Epoch 6, Batch 622, LR 1.496315 Loss 19.064662, Accuracy 0.074%\n",
      "Epoch 6, Batch 623, LR 1.496609 Loss 19.064370, Accuracy 0.074%\n",
      "Epoch 6, Batch 624, LR 1.496902 Loss 19.064122, Accuracy 0.074%\n",
      "Epoch 6, Batch 625, LR 1.497195 Loss 19.064230, Accuracy 0.075%\n",
      "Epoch 6, Batch 626, LR 1.497489 Loss 19.064029, Accuracy 0.075%\n",
      "Epoch 6, Batch 627, LR 1.497782 Loss 19.063867, Accuracy 0.075%\n",
      "Epoch 6, Batch 628, LR 1.498075 Loss 19.063735, Accuracy 0.075%\n",
      "Epoch 6, Batch 629, LR 1.498369 Loss 19.063726, Accuracy 0.075%\n",
      "Epoch 6, Batch 630, LR 1.498662 Loss 19.063984, Accuracy 0.074%\n",
      "Epoch 6, Batch 631, LR 1.498955 Loss 19.063943, Accuracy 0.074%\n",
      "Epoch 6, Batch 632, LR 1.499248 Loss 19.063600, Accuracy 0.074%\n",
      "Epoch 6, Batch 633, LR 1.499542 Loss 19.063599, Accuracy 0.074%\n",
      "Epoch 6, Batch 634, LR 1.499835 Loss 19.063408, Accuracy 0.074%\n",
      "Epoch 6, Batch 635, LR 1.500128 Loss 19.063554, Accuracy 0.074%\n",
      "Epoch 6, Batch 636, LR 1.500421 Loss 19.063498, Accuracy 0.074%\n",
      "Epoch 6, Batch 637, LR 1.500715 Loss 19.063637, Accuracy 0.074%\n",
      "Epoch 6, Batch 638, LR 1.501008 Loss 19.063370, Accuracy 0.073%\n",
      "Epoch 6, Batch 639, LR 1.501301 Loss 19.063361, Accuracy 0.073%\n",
      "Epoch 6, Batch 640, LR 1.501594 Loss 19.063307, Accuracy 0.073%\n",
      "Epoch 6, Batch 641, LR 1.501887 Loss 19.063395, Accuracy 0.073%\n",
      "Epoch 6, Batch 642, LR 1.502180 Loss 19.063615, Accuracy 0.073%\n",
      "Epoch 6, Batch 643, LR 1.502474 Loss 19.063940, Accuracy 0.073%\n",
      "Epoch 6, Batch 644, LR 1.502767 Loss 19.064040, Accuracy 0.073%\n",
      "Epoch 6, Batch 645, LR 1.503060 Loss 19.063868, Accuracy 0.074%\n",
      "Epoch 6, Batch 646, LR 1.503353 Loss 19.063879, Accuracy 0.074%\n",
      "Epoch 6, Batch 647, LR 1.503646 Loss 19.063788, Accuracy 0.074%\n",
      "Epoch 6, Batch 648, LR 1.503939 Loss 19.063745, Accuracy 0.074%\n",
      "Epoch 6, Batch 649, LR 1.504232 Loss 19.063881, Accuracy 0.073%\n",
      "Epoch 6, Batch 650, LR 1.504525 Loss 19.063900, Accuracy 0.073%\n",
      "Epoch 6, Batch 651, LR 1.504818 Loss 19.063900, Accuracy 0.073%\n",
      "Epoch 6, Batch 652, LR 1.505111 Loss 19.063965, Accuracy 0.073%\n",
      "Epoch 6, Batch 653, LR 1.505404 Loss 19.064062, Accuracy 0.073%\n",
      "Epoch 6, Batch 654, LR 1.505697 Loss 19.064101, Accuracy 0.073%\n",
      "Epoch 6, Batch 655, LR 1.505990 Loss 19.064166, Accuracy 0.073%\n",
      "Epoch 6, Batch 656, LR 1.506283 Loss 19.064183, Accuracy 0.073%\n",
      "Epoch 6, Batch 657, LR 1.506576 Loss 19.064281, Accuracy 0.073%\n",
      "Epoch 6, Batch 658, LR 1.506869 Loss 19.064545, Accuracy 0.072%\n",
      "Epoch 6, Batch 659, LR 1.507162 Loss 19.064657, Accuracy 0.072%\n",
      "Epoch 6, Batch 660, LR 1.507455 Loss 19.064521, Accuracy 0.072%\n",
      "Epoch 6, Batch 661, LR 1.507748 Loss 19.064502, Accuracy 0.072%\n",
      "Epoch 6, Batch 662, LR 1.508041 Loss 19.064502, Accuracy 0.072%\n",
      "Epoch 6, Batch 663, LR 1.508334 Loss 19.064712, Accuracy 0.072%\n",
      "Epoch 6, Batch 664, LR 1.508627 Loss 19.064738, Accuracy 0.072%\n",
      "Epoch 6, Batch 665, LR 1.508920 Loss 19.064761, Accuracy 0.072%\n",
      "Epoch 6, Batch 666, LR 1.509212 Loss 19.064712, Accuracy 0.072%\n",
      "Epoch 6, Batch 667, LR 1.509505 Loss 19.064942, Accuracy 0.071%\n",
      "Epoch 6, Batch 668, LR 1.509798 Loss 19.064889, Accuracy 0.071%\n",
      "Epoch 6, Batch 669, LR 1.510091 Loss 19.064770, Accuracy 0.071%\n",
      "Epoch 6, Batch 670, LR 1.510384 Loss 19.064919, Accuracy 0.071%\n",
      "Epoch 6, Batch 671, LR 1.510676 Loss 19.064750, Accuracy 0.072%\n",
      "Epoch 6, Batch 672, LR 1.510969 Loss 19.064824, Accuracy 0.072%\n",
      "Epoch 6, Batch 673, LR 1.511262 Loss 19.064633, Accuracy 0.072%\n",
      "Epoch 6, Batch 674, LR 1.511555 Loss 19.064668, Accuracy 0.072%\n",
      "Epoch 6, Batch 675, LR 1.511848 Loss 19.064589, Accuracy 0.072%\n",
      "Epoch 6, Batch 676, LR 1.512140 Loss 19.064382, Accuracy 0.072%\n",
      "Epoch 6, Batch 677, LR 1.512433 Loss 19.064480, Accuracy 0.072%\n",
      "Epoch 6, Batch 678, LR 1.512726 Loss 19.064421, Accuracy 0.071%\n",
      "Epoch 6, Batch 679, LR 1.513018 Loss 19.064526, Accuracy 0.071%\n",
      "Epoch 6, Batch 680, LR 1.513311 Loss 19.064646, Accuracy 0.071%\n",
      "Epoch 6, Batch 681, LR 1.513604 Loss 19.064732, Accuracy 0.071%\n",
      "Epoch 6, Batch 682, LR 1.513896 Loss 19.064674, Accuracy 0.071%\n",
      "Epoch 6, Batch 683, LR 1.514189 Loss 19.064686, Accuracy 0.071%\n",
      "Epoch 6, Batch 684, LR 1.514482 Loss 19.064681, Accuracy 0.071%\n",
      "Epoch 6, Batch 685, LR 1.514774 Loss 19.064331, Accuracy 0.071%\n",
      "Epoch 6, Batch 686, LR 1.515067 Loss 19.064771, Accuracy 0.071%\n",
      "Epoch 6, Batch 687, LR 1.515360 Loss 19.064671, Accuracy 0.071%\n",
      "Epoch 6, Batch 688, LR 1.515652 Loss 19.064522, Accuracy 0.070%\n",
      "Epoch 6, Batch 689, LR 1.515945 Loss 19.064392, Accuracy 0.070%\n",
      "Epoch 6, Batch 690, LR 1.516237 Loss 19.064503, Accuracy 0.070%\n",
      "Epoch 6, Batch 691, LR 1.516530 Loss 19.064547, Accuracy 0.070%\n",
      "Epoch 6, Batch 692, LR 1.516822 Loss 19.064525, Accuracy 0.070%\n",
      "Epoch 6, Batch 693, LR 1.517115 Loss 19.064669, Accuracy 0.070%\n",
      "Epoch 6, Batch 694, LR 1.517407 Loss 19.064323, Accuracy 0.070%\n",
      "Epoch 6, Batch 695, LR 1.517700 Loss 19.064225, Accuracy 0.070%\n",
      "Epoch 6, Batch 696, LR 1.517992 Loss 19.064047, Accuracy 0.070%\n",
      "Epoch 6, Batch 697, LR 1.518285 Loss 19.064008, Accuracy 0.069%\n",
      "Epoch 6, Batch 698, LR 1.518577 Loss 19.063850, Accuracy 0.069%\n",
      "Epoch 6, Batch 699, LR 1.518870 Loss 19.063495, Accuracy 0.069%\n",
      "Epoch 6, Batch 700, LR 1.519162 Loss 19.063503, Accuracy 0.069%\n",
      "Epoch 6, Batch 701, LR 1.519455 Loss 19.063492, Accuracy 0.069%\n",
      "Epoch 6, Batch 702, LR 1.519747 Loss 19.063160, Accuracy 0.069%\n",
      "Epoch 6, Batch 703, LR 1.520039 Loss 19.063197, Accuracy 0.069%\n",
      "Epoch 6, Batch 704, LR 1.520332 Loss 19.063165, Accuracy 0.069%\n",
      "Epoch 6, Batch 705, LR 1.520624 Loss 19.062767, Accuracy 0.069%\n",
      "Epoch 6, Batch 706, LR 1.520916 Loss 19.062686, Accuracy 0.069%\n",
      "Epoch 6, Batch 707, LR 1.521209 Loss 19.062863, Accuracy 0.069%\n",
      "Epoch 6, Batch 708, LR 1.521501 Loss 19.062860, Accuracy 0.070%\n",
      "Epoch 6, Batch 709, LR 1.521793 Loss 19.062731, Accuracy 0.071%\n",
      "Epoch 6, Batch 710, LR 1.522086 Loss 19.062735, Accuracy 0.070%\n",
      "Epoch 6, Batch 711, LR 1.522378 Loss 19.062896, Accuracy 0.070%\n",
      "Epoch 6, Batch 712, LR 1.522670 Loss 19.063103, Accuracy 0.070%\n",
      "Epoch 6, Batch 713, LR 1.522963 Loss 19.063333, Accuracy 0.070%\n",
      "Epoch 6, Batch 714, LR 1.523255 Loss 19.063061, Accuracy 0.070%\n",
      "Epoch 6, Batch 715, LR 1.523547 Loss 19.062916, Accuracy 0.070%\n",
      "Epoch 6, Batch 716, LR 1.523839 Loss 19.062943, Accuracy 0.070%\n",
      "Epoch 6, Batch 717, LR 1.524131 Loss 19.063090, Accuracy 0.070%\n",
      "Epoch 6, Batch 718, LR 1.524424 Loss 19.062824, Accuracy 0.070%\n",
      "Epoch 6, Batch 719, LR 1.524716 Loss 19.062731, Accuracy 0.070%\n",
      "Epoch 6, Batch 720, LR 1.525008 Loss 19.062823, Accuracy 0.069%\n",
      "Epoch 6, Batch 721, LR 1.525300 Loss 19.062794, Accuracy 0.069%\n",
      "Epoch 6, Batch 722, LR 1.525592 Loss 19.062804, Accuracy 0.069%\n",
      "Epoch 6, Batch 723, LR 1.525884 Loss 19.062959, Accuracy 0.070%\n",
      "Epoch 6, Batch 724, LR 1.526176 Loss 19.063059, Accuracy 0.070%\n",
      "Epoch 6, Batch 725, LR 1.526469 Loss 19.063119, Accuracy 0.070%\n",
      "Epoch 6, Batch 726, LR 1.526761 Loss 19.062934, Accuracy 0.070%\n",
      "Epoch 6, Batch 727, LR 1.527053 Loss 19.062818, Accuracy 0.070%\n",
      "Epoch 6, Batch 728, LR 1.527345 Loss 19.062469, Accuracy 0.070%\n",
      "Epoch 6, Batch 729, LR 1.527637 Loss 19.062285, Accuracy 0.070%\n",
      "Epoch 6, Batch 730, LR 1.527929 Loss 19.062464, Accuracy 0.070%\n",
      "Epoch 6, Batch 731, LR 1.528221 Loss 19.062194, Accuracy 0.069%\n",
      "Epoch 6, Batch 732, LR 1.528513 Loss 19.062262, Accuracy 0.069%\n",
      "Epoch 6, Batch 733, LR 1.528805 Loss 19.062334, Accuracy 0.069%\n",
      "Epoch 6, Batch 734, LR 1.529097 Loss 19.062075, Accuracy 0.069%\n",
      "Epoch 6, Batch 735, LR 1.529389 Loss 19.062445, Accuracy 0.069%\n",
      "Epoch 6, Batch 736, LR 1.529681 Loss 19.062706, Accuracy 0.069%\n",
      "Epoch 6, Batch 737, LR 1.529973 Loss 19.062534, Accuracy 0.069%\n",
      "Epoch 6, Batch 738, LR 1.530265 Loss 19.062417, Accuracy 0.069%\n",
      "Epoch 6, Batch 739, LR 1.530557 Loss 19.062349, Accuracy 0.069%\n",
      "Epoch 6, Batch 740, LR 1.530848 Loss 19.062364, Accuracy 0.069%\n",
      "Epoch 6, Batch 741, LR 1.531140 Loss 19.062592, Accuracy 0.069%\n",
      "Epoch 6, Batch 742, LR 1.531432 Loss 19.062862, Accuracy 0.068%\n",
      "Epoch 6, Batch 743, LR 1.531724 Loss 19.062622, Accuracy 0.068%\n",
      "Epoch 6, Batch 744, LR 1.532016 Loss 19.062475, Accuracy 0.068%\n",
      "Epoch 6, Batch 745, LR 1.532308 Loss 19.062643, Accuracy 0.068%\n",
      "Epoch 6, Batch 746, LR 1.532600 Loss 19.062727, Accuracy 0.069%\n",
      "Epoch 6, Batch 747, LR 1.532891 Loss 19.062797, Accuracy 0.069%\n",
      "Epoch 6, Batch 748, LR 1.533183 Loss 19.062800, Accuracy 0.069%\n",
      "Epoch 6, Batch 749, LR 1.533475 Loss 19.062701, Accuracy 0.069%\n",
      "Epoch 6, Batch 750, LR 1.533767 Loss 19.062865, Accuracy 0.070%\n",
      "Epoch 6, Batch 751, LR 1.534058 Loss 19.063162, Accuracy 0.070%\n",
      "Epoch 6, Batch 752, LR 1.534350 Loss 19.063202, Accuracy 0.070%\n",
      "Epoch 6, Batch 753, LR 1.534642 Loss 19.063063, Accuracy 0.070%\n",
      "Epoch 6, Batch 754, LR 1.534934 Loss 19.062941, Accuracy 0.069%\n",
      "Epoch 6, Batch 755, LR 1.535225 Loss 19.063324, Accuracy 0.069%\n",
      "Epoch 6, Batch 756, LR 1.535517 Loss 19.063186, Accuracy 0.069%\n",
      "Epoch 6, Batch 757, LR 1.535809 Loss 19.063124, Accuracy 0.070%\n",
      "Epoch 6, Batch 758, LR 1.536100 Loss 19.063041, Accuracy 0.070%\n",
      "Epoch 6, Batch 759, LR 1.536392 Loss 19.063134, Accuracy 0.070%\n",
      "Epoch 6, Batch 760, LR 1.536684 Loss 19.062967, Accuracy 0.070%\n",
      "Epoch 6, Batch 761, LR 1.536975 Loss 19.063074, Accuracy 0.070%\n",
      "Epoch 6, Batch 762, LR 1.537267 Loss 19.062715, Accuracy 0.071%\n",
      "Epoch 6, Batch 763, LR 1.537558 Loss 19.062480, Accuracy 0.071%\n",
      "Epoch 6, Batch 764, LR 1.537850 Loss 19.062540, Accuracy 0.071%\n",
      "Epoch 6, Batch 765, LR 1.538141 Loss 19.062400, Accuracy 0.070%\n",
      "Epoch 6, Batch 766, LR 1.538433 Loss 19.062301, Accuracy 0.071%\n",
      "Epoch 6, Batch 767, LR 1.538725 Loss 19.062273, Accuracy 0.072%\n",
      "Epoch 6, Batch 768, LR 1.539016 Loss 19.062150, Accuracy 0.072%\n",
      "Epoch 6, Batch 769, LR 1.539308 Loss 19.062131, Accuracy 0.072%\n",
      "Epoch 6, Batch 770, LR 1.539599 Loss 19.062055, Accuracy 0.072%\n",
      "Epoch 6, Batch 771, LR 1.539890 Loss 19.062233, Accuracy 0.072%\n",
      "Epoch 6, Batch 772, LR 1.540182 Loss 19.062139, Accuracy 0.073%\n",
      "Epoch 6, Batch 773, LR 1.540473 Loss 19.061932, Accuracy 0.073%\n",
      "Epoch 6, Batch 774, LR 1.540765 Loss 19.062041, Accuracy 0.073%\n",
      "Epoch 6, Batch 775, LR 1.541056 Loss 19.061801, Accuracy 0.073%\n",
      "Epoch 6, Batch 776, LR 1.541348 Loss 19.061880, Accuracy 0.072%\n",
      "Epoch 6, Batch 777, LR 1.541639 Loss 19.062078, Accuracy 0.072%\n",
      "Epoch 6, Batch 778, LR 1.541930 Loss 19.062098, Accuracy 0.072%\n",
      "Epoch 6, Batch 779, LR 1.542222 Loss 19.062025, Accuracy 0.072%\n",
      "Epoch 6, Batch 780, LR 1.542513 Loss 19.062289, Accuracy 0.072%\n",
      "Epoch 6, Batch 781, LR 1.542804 Loss 19.062401, Accuracy 0.073%\n",
      "Epoch 6, Batch 782, LR 1.543096 Loss 19.062473, Accuracy 0.073%\n",
      "Epoch 6, Batch 783, LR 1.543387 Loss 19.062175, Accuracy 0.073%\n",
      "Epoch 6, Batch 784, LR 1.543678 Loss 19.062007, Accuracy 0.073%\n",
      "Epoch 6, Batch 785, LR 1.543970 Loss 19.062247, Accuracy 0.073%\n",
      "Epoch 6, Batch 786, LR 1.544261 Loss 19.062523, Accuracy 0.073%\n",
      "Epoch 6, Batch 787, LR 1.544552 Loss 19.062424, Accuracy 0.072%\n",
      "Epoch 6, Batch 788, LR 1.544843 Loss 19.062404, Accuracy 0.072%\n",
      "Epoch 6, Batch 789, LR 1.545134 Loss 19.062517, Accuracy 0.072%\n",
      "Epoch 6, Batch 790, LR 1.545426 Loss 19.062470, Accuracy 0.072%\n",
      "Epoch 6, Batch 791, LR 1.545717 Loss 19.062555, Accuracy 0.073%\n",
      "Epoch 6, Batch 792, LR 1.546008 Loss 19.062415, Accuracy 0.073%\n",
      "Epoch 6, Batch 793, LR 1.546299 Loss 19.062535, Accuracy 0.073%\n",
      "Epoch 6, Batch 794, LR 1.546590 Loss 19.062403, Accuracy 0.073%\n",
      "Epoch 6, Batch 795, LR 1.546881 Loss 19.062564, Accuracy 0.074%\n",
      "Epoch 6, Batch 796, LR 1.547172 Loss 19.062260, Accuracy 0.074%\n",
      "Epoch 6, Batch 797, LR 1.547464 Loss 19.062343, Accuracy 0.074%\n",
      "Epoch 6, Batch 798, LR 1.547755 Loss 19.062378, Accuracy 0.073%\n",
      "Epoch 6, Batch 799, LR 1.548046 Loss 19.062293, Accuracy 0.073%\n",
      "Epoch 6, Batch 800, LR 1.548337 Loss 19.062268, Accuracy 0.073%\n",
      "Epoch 6, Batch 801, LR 1.548628 Loss 19.062107, Accuracy 0.073%\n",
      "Epoch 6, Batch 802, LR 1.548919 Loss 19.062145, Accuracy 0.073%\n",
      "Epoch 6, Batch 803, LR 1.549210 Loss 19.061952, Accuracy 0.073%\n",
      "Epoch 6, Batch 804, LR 1.549501 Loss 19.062059, Accuracy 0.073%\n",
      "Epoch 6, Batch 805, LR 1.549792 Loss 19.062257, Accuracy 0.073%\n",
      "Epoch 6, Batch 806, LR 1.550083 Loss 19.062251, Accuracy 0.073%\n",
      "Epoch 6, Batch 807, LR 1.550374 Loss 19.062074, Accuracy 0.073%\n",
      "Epoch 6, Batch 808, LR 1.550665 Loss 19.062136, Accuracy 0.073%\n",
      "Epoch 6, Batch 809, LR 1.550956 Loss 19.062062, Accuracy 0.072%\n",
      "Epoch 6, Batch 810, LR 1.551246 Loss 19.061963, Accuracy 0.072%\n",
      "Epoch 6, Batch 811, LR 1.551537 Loss 19.062167, Accuracy 0.072%\n",
      "Epoch 6, Batch 812, LR 1.551828 Loss 19.062237, Accuracy 0.072%\n",
      "Epoch 6, Batch 813, LR 1.552119 Loss 19.062008, Accuracy 0.072%\n",
      "Epoch 6, Batch 814, LR 1.552410 Loss 19.061881, Accuracy 0.073%\n",
      "Epoch 6, Batch 815, LR 1.552701 Loss 19.061684, Accuracy 0.073%\n",
      "Epoch 6, Batch 816, LR 1.552991 Loss 19.061657, Accuracy 0.073%\n",
      "Epoch 6, Batch 817, LR 1.553282 Loss 19.061484, Accuracy 0.073%\n",
      "Epoch 6, Batch 818, LR 1.553573 Loss 19.061201, Accuracy 0.073%\n",
      "Epoch 6, Batch 819, LR 1.553864 Loss 19.061287, Accuracy 0.072%\n",
      "Epoch 6, Batch 820, LR 1.554155 Loss 19.061106, Accuracy 0.072%\n",
      "Epoch 6, Batch 821, LR 1.554445 Loss 19.061272, Accuracy 0.072%\n",
      "Epoch 6, Batch 822, LR 1.554736 Loss 19.061263, Accuracy 0.073%\n",
      "Epoch 6, Batch 823, LR 1.555027 Loss 19.061332, Accuracy 0.073%\n",
      "Epoch 6, Batch 824, LR 1.555317 Loss 19.061260, Accuracy 0.073%\n",
      "Epoch 6, Batch 825, LR 1.555608 Loss 19.061594, Accuracy 0.073%\n",
      "Epoch 6, Batch 826, LR 1.555899 Loss 19.061746, Accuracy 0.073%\n",
      "Epoch 6, Batch 827, LR 1.556189 Loss 19.061746, Accuracy 0.074%\n",
      "Epoch 6, Batch 828, LR 1.556480 Loss 19.061607, Accuracy 0.074%\n",
      "Epoch 6, Batch 829, LR 1.556771 Loss 19.061719, Accuracy 0.074%\n",
      "Epoch 6, Batch 830, LR 1.557061 Loss 19.061851, Accuracy 0.074%\n",
      "Epoch 6, Batch 831, LR 1.557352 Loss 19.061796, Accuracy 0.074%\n",
      "Epoch 6, Batch 832, LR 1.557642 Loss 19.062074, Accuracy 0.074%\n",
      "Epoch 6, Batch 833, LR 1.557933 Loss 19.062171, Accuracy 0.074%\n",
      "Epoch 6, Batch 834, LR 1.558223 Loss 19.061950, Accuracy 0.074%\n",
      "Epoch 6, Batch 835, LR 1.558514 Loss 19.061790, Accuracy 0.074%\n",
      "Epoch 6, Batch 836, LR 1.558804 Loss 19.061812, Accuracy 0.074%\n",
      "Epoch 6, Batch 837, LR 1.559095 Loss 19.061908, Accuracy 0.074%\n",
      "Epoch 6, Batch 838, LR 1.559385 Loss 19.061811, Accuracy 0.074%\n",
      "Epoch 6, Batch 839, LR 1.559676 Loss 19.062060, Accuracy 0.074%\n",
      "Epoch 6, Batch 840, LR 1.559966 Loss 19.062206, Accuracy 0.073%\n",
      "Epoch 6, Batch 841, LR 1.560257 Loss 19.062070, Accuracy 0.073%\n",
      "Epoch 6, Batch 842, LR 1.560547 Loss 19.061975, Accuracy 0.073%\n",
      "Epoch 6, Batch 843, LR 1.560838 Loss 19.061949, Accuracy 0.073%\n",
      "Epoch 6, Batch 844, LR 1.561128 Loss 19.061887, Accuracy 0.073%\n",
      "Epoch 6, Batch 845, LR 1.561418 Loss 19.062032, Accuracy 0.073%\n",
      "Epoch 6, Batch 846, LR 1.561709 Loss 19.061942, Accuracy 0.073%\n",
      "Epoch 6, Batch 847, LR 1.561999 Loss 19.062146, Accuracy 0.073%\n",
      "Epoch 6, Batch 848, LR 1.562289 Loss 19.062140, Accuracy 0.073%\n",
      "Epoch 6, Batch 849, LR 1.562580 Loss 19.062174, Accuracy 0.073%\n",
      "Epoch 6, Batch 850, LR 1.562870 Loss 19.062152, Accuracy 0.073%\n",
      "Epoch 6, Batch 851, LR 1.563160 Loss 19.062594, Accuracy 0.073%\n",
      "Epoch 6, Batch 852, LR 1.563450 Loss 19.062505, Accuracy 0.072%\n",
      "Epoch 6, Batch 853, LR 1.563741 Loss 19.062462, Accuracy 0.072%\n",
      "Epoch 6, Batch 854, LR 1.564031 Loss 19.062515, Accuracy 0.072%\n",
      "Epoch 6, Batch 855, LR 1.564321 Loss 19.062398, Accuracy 0.072%\n",
      "Epoch 6, Batch 856, LR 1.564611 Loss 19.062522, Accuracy 0.072%\n",
      "Epoch 6, Batch 857, LR 1.564901 Loss 19.062450, Accuracy 0.072%\n",
      "Epoch 6, Batch 858, LR 1.565192 Loss 19.062591, Accuracy 0.072%\n",
      "Epoch 6, Batch 859, LR 1.565482 Loss 19.062542, Accuracy 0.072%\n",
      "Epoch 6, Batch 860, LR 1.565772 Loss 19.062556, Accuracy 0.072%\n",
      "Epoch 6, Batch 861, LR 1.566062 Loss 19.062502, Accuracy 0.072%\n",
      "Epoch 6, Batch 862, LR 1.566352 Loss 19.062628, Accuracy 0.072%\n",
      "Epoch 6, Batch 863, LR 1.566642 Loss 19.062805, Accuracy 0.072%\n",
      "Epoch 6, Batch 864, LR 1.566932 Loss 19.062707, Accuracy 0.073%\n",
      "Epoch 6, Batch 865, LR 1.567222 Loss 19.062735, Accuracy 0.073%\n",
      "Epoch 6, Batch 866, LR 1.567512 Loss 19.062778, Accuracy 0.074%\n",
      "Epoch 6, Batch 867, LR 1.567802 Loss 19.062615, Accuracy 0.075%\n",
      "Epoch 6, Batch 868, LR 1.568092 Loss 19.062584, Accuracy 0.075%\n",
      "Epoch 6, Batch 869, LR 1.568382 Loss 19.062618, Accuracy 0.075%\n",
      "Epoch 6, Batch 870, LR 1.568672 Loss 19.062576, Accuracy 0.075%\n",
      "Epoch 6, Batch 871, LR 1.568962 Loss 19.062780, Accuracy 0.074%\n",
      "Epoch 6, Batch 872, LR 1.569252 Loss 19.062769, Accuracy 0.074%\n",
      "Epoch 6, Batch 873, LR 1.569542 Loss 19.062691, Accuracy 0.074%\n",
      "Epoch 6, Batch 874, LR 1.569832 Loss 19.062766, Accuracy 0.075%\n",
      "Epoch 6, Batch 875, LR 1.570122 Loss 19.062776, Accuracy 0.075%\n",
      "Epoch 6, Batch 876, LR 1.570412 Loss 19.062901, Accuracy 0.075%\n",
      "Epoch 6, Batch 877, LR 1.570702 Loss 19.062900, Accuracy 0.075%\n",
      "Epoch 6, Batch 878, LR 1.570992 Loss 19.062845, Accuracy 0.075%\n",
      "Epoch 6, Batch 879, LR 1.571281 Loss 19.063097, Accuracy 0.075%\n",
      "Epoch 6, Batch 880, LR 1.571571 Loss 19.063191, Accuracy 0.075%\n",
      "Epoch 6, Batch 881, LR 1.571861 Loss 19.063201, Accuracy 0.074%\n",
      "Epoch 6, Batch 882, LR 1.572151 Loss 19.063003, Accuracy 0.074%\n",
      "Epoch 6, Batch 883, LR 1.572441 Loss 19.063180, Accuracy 0.074%\n",
      "Epoch 6, Batch 884, LR 1.572730 Loss 19.063416, Accuracy 0.074%\n",
      "Epoch 6, Batch 885, LR 1.573020 Loss 19.063394, Accuracy 0.074%\n",
      "Epoch 6, Batch 886, LR 1.573310 Loss 19.063024, Accuracy 0.074%\n",
      "Epoch 6, Batch 887, LR 1.573599 Loss 19.063073, Accuracy 0.074%\n",
      "Epoch 6, Batch 888, LR 1.573889 Loss 19.063051, Accuracy 0.074%\n",
      "Epoch 6, Batch 889, LR 1.574179 Loss 19.063015, Accuracy 0.074%\n",
      "Epoch 6, Batch 890, LR 1.574468 Loss 19.063106, Accuracy 0.074%\n",
      "Epoch 6, Batch 891, LR 1.574758 Loss 19.063089, Accuracy 0.074%\n",
      "Epoch 6, Batch 892, LR 1.575048 Loss 19.062952, Accuracy 0.074%\n",
      "Epoch 6, Batch 893, LR 1.575337 Loss 19.063167, Accuracy 0.074%\n",
      "Epoch 6, Batch 894, LR 1.575627 Loss 19.063099, Accuracy 0.074%\n",
      "Epoch 6, Batch 895, LR 1.575916 Loss 19.063070, Accuracy 0.074%\n",
      "Epoch 6, Batch 896, LR 1.576206 Loss 19.063239, Accuracy 0.074%\n",
      "Epoch 6, Batch 897, LR 1.576495 Loss 19.063115, Accuracy 0.074%\n",
      "Epoch 6, Batch 898, LR 1.576785 Loss 19.063130, Accuracy 0.074%\n",
      "Epoch 6, Batch 899, LR 1.577074 Loss 19.063187, Accuracy 0.075%\n",
      "Epoch 6, Batch 900, LR 1.577364 Loss 19.063070, Accuracy 0.075%\n",
      "Epoch 6, Batch 901, LR 1.577653 Loss 19.063215, Accuracy 0.075%\n",
      "Epoch 6, Batch 902, LR 1.577943 Loss 19.063127, Accuracy 0.075%\n",
      "Epoch 6, Batch 903, LR 1.578232 Loss 19.062939, Accuracy 0.076%\n",
      "Epoch 6, Batch 904, LR 1.578522 Loss 19.063154, Accuracy 0.077%\n",
      "Epoch 6, Batch 905, LR 1.578811 Loss 19.063228, Accuracy 0.077%\n",
      "Epoch 6, Batch 906, LR 1.579100 Loss 19.063361, Accuracy 0.077%\n",
      "Epoch 6, Batch 907, LR 1.579390 Loss 19.063297, Accuracy 0.077%\n",
      "Epoch 6, Batch 908, LR 1.579679 Loss 19.063356, Accuracy 0.077%\n",
      "Epoch 6, Batch 909, LR 1.579969 Loss 19.063202, Accuracy 0.076%\n",
      "Epoch 6, Batch 910, LR 1.580258 Loss 19.063181, Accuracy 0.076%\n",
      "Epoch 6, Batch 911, LR 1.580547 Loss 19.062997, Accuracy 0.076%\n",
      "Epoch 6, Batch 912, LR 1.580836 Loss 19.062777, Accuracy 0.076%\n",
      "Epoch 6, Batch 913, LR 1.581126 Loss 19.062705, Accuracy 0.076%\n",
      "Epoch 6, Batch 914, LR 1.581415 Loss 19.062711, Accuracy 0.076%\n",
      "Epoch 6, Batch 915, LR 1.581704 Loss 19.062868, Accuracy 0.076%\n",
      "Epoch 6, Batch 916, LR 1.581993 Loss 19.062784, Accuracy 0.076%\n",
      "Epoch 6, Batch 917, LR 1.582283 Loss 19.062774, Accuracy 0.076%\n",
      "Epoch 6, Batch 918, LR 1.582572 Loss 19.062901, Accuracy 0.076%\n",
      "Epoch 6, Batch 919, LR 1.582861 Loss 19.063023, Accuracy 0.076%\n",
      "Epoch 6, Batch 920, LR 1.583150 Loss 19.063053, Accuracy 0.076%\n",
      "Epoch 6, Batch 921, LR 1.583439 Loss 19.062890, Accuracy 0.075%\n",
      "Epoch 6, Batch 922, LR 1.583728 Loss 19.062712, Accuracy 0.075%\n",
      "Epoch 6, Batch 923, LR 1.584017 Loss 19.062877, Accuracy 0.075%\n",
      "Epoch 6, Batch 924, LR 1.584307 Loss 19.062784, Accuracy 0.075%\n",
      "Epoch 6, Batch 925, LR 1.584596 Loss 19.062766, Accuracy 0.075%\n",
      "Epoch 6, Batch 926, LR 1.584885 Loss 19.062731, Accuracy 0.075%\n",
      "Epoch 6, Batch 927, LR 1.585174 Loss 19.062857, Accuracy 0.075%\n",
      "Epoch 6, Batch 928, LR 1.585463 Loss 19.062821, Accuracy 0.076%\n",
      "Epoch 6, Batch 929, LR 1.585752 Loss 19.062789, Accuracy 0.076%\n",
      "Epoch 6, Batch 930, LR 1.586041 Loss 19.062676, Accuracy 0.076%\n",
      "Epoch 6, Batch 931, LR 1.586330 Loss 19.062862, Accuracy 0.076%\n",
      "Epoch 6, Batch 932, LR 1.586619 Loss 19.062843, Accuracy 0.076%\n",
      "Epoch 6, Batch 933, LR 1.586907 Loss 19.062656, Accuracy 0.076%\n",
      "Epoch 6, Batch 934, LR 1.587196 Loss 19.062720, Accuracy 0.076%\n",
      "Epoch 6, Batch 935, LR 1.587485 Loss 19.062716, Accuracy 0.076%\n",
      "Epoch 6, Batch 936, LR 1.587774 Loss 19.062707, Accuracy 0.076%\n",
      "Epoch 6, Batch 937, LR 1.588063 Loss 19.062968, Accuracy 0.076%\n",
      "Epoch 6, Batch 938, LR 1.588352 Loss 19.063182, Accuracy 0.077%\n",
      "Epoch 6, Batch 939, LR 1.588641 Loss 19.063320, Accuracy 0.077%\n",
      "Epoch 6, Batch 940, LR 1.588929 Loss 19.063185, Accuracy 0.076%\n",
      "Epoch 6, Batch 941, LR 1.589218 Loss 19.063261, Accuracy 0.076%\n",
      "Epoch 6, Batch 942, LR 1.589507 Loss 19.063403, Accuracy 0.076%\n",
      "Epoch 6, Batch 943, LR 1.589796 Loss 19.063448, Accuracy 0.076%\n",
      "Epoch 6, Batch 944, LR 1.590085 Loss 19.063419, Accuracy 0.076%\n",
      "Epoch 6, Batch 945, LR 1.590373 Loss 19.063659, Accuracy 0.076%\n",
      "Epoch 6, Batch 946, LR 1.590662 Loss 19.063522, Accuracy 0.076%\n",
      "Epoch 6, Batch 947, LR 1.590951 Loss 19.063532, Accuracy 0.076%\n",
      "Epoch 6, Batch 948, LR 1.591239 Loss 19.063566, Accuracy 0.076%\n",
      "Epoch 6, Batch 949, LR 1.591528 Loss 19.063461, Accuracy 0.076%\n",
      "Epoch 6, Batch 950, LR 1.591817 Loss 19.063540, Accuracy 0.076%\n",
      "Epoch 6, Batch 951, LR 1.592105 Loss 19.063474, Accuracy 0.076%\n",
      "Epoch 6, Batch 952, LR 1.592394 Loss 19.063440, Accuracy 0.075%\n",
      "Epoch 6, Batch 953, LR 1.592682 Loss 19.063480, Accuracy 0.075%\n",
      "Epoch 6, Batch 954, LR 1.592971 Loss 19.063386, Accuracy 0.075%\n",
      "Epoch 6, Batch 955, LR 1.593259 Loss 19.063387, Accuracy 0.075%\n",
      "Epoch 6, Batch 956, LR 1.593548 Loss 19.063393, Accuracy 0.075%\n",
      "Epoch 6, Batch 957, LR 1.593836 Loss 19.063465, Accuracy 0.075%\n",
      "Epoch 6, Batch 958, LR 1.594125 Loss 19.063403, Accuracy 0.075%\n",
      "Epoch 6, Batch 959, LR 1.594413 Loss 19.063393, Accuracy 0.075%\n",
      "Epoch 6, Batch 960, LR 1.594702 Loss 19.063493, Accuracy 0.075%\n",
      "Epoch 6, Batch 961, LR 1.594990 Loss 19.063500, Accuracy 0.075%\n",
      "Epoch 6, Batch 962, LR 1.595279 Loss 19.063445, Accuracy 0.076%\n",
      "Epoch 6, Batch 963, LR 1.595567 Loss 19.063489, Accuracy 0.075%\n",
      "Epoch 6, Batch 964, LR 1.595856 Loss 19.063324, Accuracy 0.075%\n",
      "Epoch 6, Batch 965, LR 1.596144 Loss 19.063352, Accuracy 0.076%\n",
      "Epoch 6, Batch 966, LR 1.596432 Loss 19.063226, Accuracy 0.076%\n",
      "Epoch 6, Batch 967, LR 1.596721 Loss 19.063271, Accuracy 0.076%\n",
      "Epoch 6, Batch 968, LR 1.597009 Loss 19.063303, Accuracy 0.076%\n",
      "Epoch 6, Batch 969, LR 1.597297 Loss 19.063241, Accuracy 0.076%\n",
      "Epoch 6, Batch 970, LR 1.597586 Loss 19.063157, Accuracy 0.076%\n",
      "Epoch 6, Batch 971, LR 1.597874 Loss 19.063180, Accuracy 0.076%\n",
      "Epoch 6, Batch 972, LR 1.598162 Loss 19.063387, Accuracy 0.076%\n",
      "Epoch 6, Batch 973, LR 1.598450 Loss 19.063449, Accuracy 0.076%\n",
      "Epoch 6, Batch 974, LR 1.598738 Loss 19.063402, Accuracy 0.076%\n",
      "Epoch 6, Batch 975, LR 1.599027 Loss 19.063312, Accuracy 0.077%\n",
      "Epoch 6, Batch 976, LR 1.599315 Loss 19.063265, Accuracy 0.077%\n",
      "Epoch 6, Batch 977, LR 1.599603 Loss 19.063232, Accuracy 0.078%\n",
      "Epoch 6, Batch 978, LR 1.599891 Loss 19.063322, Accuracy 0.077%\n",
      "Epoch 6, Batch 979, LR 1.600179 Loss 19.063532, Accuracy 0.077%\n",
      "Epoch 6, Batch 980, LR 1.600467 Loss 19.063514, Accuracy 0.077%\n",
      "Epoch 6, Batch 981, LR 1.600755 Loss 19.063893, Accuracy 0.077%\n",
      "Epoch 6, Batch 982, LR 1.601043 Loss 19.064032, Accuracy 0.078%\n",
      "Epoch 6, Batch 983, LR 1.601332 Loss 19.064153, Accuracy 0.078%\n",
      "Epoch 6, Batch 984, LR 1.601620 Loss 19.064156, Accuracy 0.078%\n",
      "Epoch 6, Batch 985, LR 1.601908 Loss 19.064394, Accuracy 0.078%\n",
      "Epoch 6, Batch 986, LR 1.602196 Loss 19.064558, Accuracy 0.078%\n",
      "Epoch 6, Batch 987, LR 1.602484 Loss 19.064698, Accuracy 0.078%\n",
      "Epoch 6, Batch 988, LR 1.602771 Loss 19.064752, Accuracy 0.077%\n",
      "Epoch 6, Batch 989, LR 1.603059 Loss 19.064647, Accuracy 0.077%\n",
      "Epoch 6, Batch 990, LR 1.603347 Loss 19.064993, Accuracy 0.077%\n",
      "Epoch 6, Batch 991, LR 1.603635 Loss 19.065116, Accuracy 0.077%\n",
      "Epoch 6, Batch 992, LR 1.603923 Loss 19.064949, Accuracy 0.077%\n",
      "Epoch 6, Batch 993, LR 1.604211 Loss 19.064993, Accuracy 0.077%\n",
      "Epoch 6, Batch 994, LR 1.604499 Loss 19.065192, Accuracy 0.077%\n",
      "Epoch 6, Batch 995, LR 1.604787 Loss 19.065076, Accuracy 0.077%\n",
      "Epoch 6, Batch 996, LR 1.605074 Loss 19.065106, Accuracy 0.078%\n",
      "Epoch 6, Batch 997, LR 1.605362 Loss 19.065176, Accuracy 0.078%\n",
      "Epoch 6, Batch 998, LR 1.605650 Loss 19.065171, Accuracy 0.077%\n",
      "Epoch 6, Batch 999, LR 1.605938 Loss 19.064947, Accuracy 0.077%\n",
      "Epoch 6, Batch 1000, LR 1.606226 Loss 19.064915, Accuracy 0.077%\n",
      "Epoch 6, Batch 1001, LR 1.606513 Loss 19.064829, Accuracy 0.077%\n",
      "Epoch 6, Batch 1002, LR 1.606801 Loss 19.064838, Accuracy 0.077%\n",
      "Epoch 6, Batch 1003, LR 1.607089 Loss 19.064781, Accuracy 0.077%\n",
      "Epoch 6, Batch 1004, LR 1.607376 Loss 19.064884, Accuracy 0.077%\n",
      "Epoch 6, Batch 1005, LR 1.607664 Loss 19.064807, Accuracy 0.077%\n",
      "Epoch 6, Batch 1006, LR 1.607952 Loss 19.064744, Accuracy 0.077%\n",
      "Epoch 6, Batch 1007, LR 1.608239 Loss 19.064623, Accuracy 0.077%\n",
      "Epoch 6, Batch 1008, LR 1.608527 Loss 19.064400, Accuracy 0.077%\n",
      "Epoch 6, Batch 1009, LR 1.608814 Loss 19.064337, Accuracy 0.077%\n",
      "Epoch 6, Batch 1010, LR 1.609102 Loss 19.064294, Accuracy 0.077%\n",
      "Epoch 6, Batch 1011, LR 1.609389 Loss 19.064397, Accuracy 0.077%\n",
      "Epoch 6, Batch 1012, LR 1.609677 Loss 19.064514, Accuracy 0.076%\n",
      "Epoch 6, Batch 1013, LR 1.609964 Loss 19.064508, Accuracy 0.077%\n",
      "Epoch 6, Batch 1014, LR 1.610252 Loss 19.064460, Accuracy 0.077%\n",
      "Epoch 6, Batch 1015, LR 1.610539 Loss 19.064402, Accuracy 0.077%\n",
      "Epoch 6, Batch 1016, LR 1.610827 Loss 19.064428, Accuracy 0.078%\n",
      "Epoch 6, Batch 1017, LR 1.611114 Loss 19.064276, Accuracy 0.079%\n",
      "Epoch 6, Batch 1018, LR 1.611402 Loss 19.064266, Accuracy 0.079%\n",
      "Epoch 6, Batch 1019, LR 1.611689 Loss 19.064342, Accuracy 0.079%\n",
      "Epoch 6, Batch 1020, LR 1.611976 Loss 19.064403, Accuracy 0.079%\n",
      "Epoch 6, Batch 1021, LR 1.612264 Loss 19.064392, Accuracy 0.079%\n",
      "Epoch 6, Batch 1022, LR 1.612551 Loss 19.064153, Accuracy 0.079%\n",
      "Epoch 6, Batch 1023, LR 1.612838 Loss 19.064158, Accuracy 0.079%\n",
      "Epoch 6, Batch 1024, LR 1.613126 Loss 19.064174, Accuracy 0.079%\n",
      "Epoch 6, Batch 1025, LR 1.613413 Loss 19.064178, Accuracy 0.079%\n",
      "Epoch 6, Batch 1026, LR 1.613700 Loss 19.064071, Accuracy 0.078%\n",
      "Epoch 6, Batch 1027, LR 1.613987 Loss 19.064069, Accuracy 0.078%\n",
      "Epoch 6, Batch 1028, LR 1.614275 Loss 19.063978, Accuracy 0.078%\n",
      "Epoch 6, Batch 1029, LR 1.614562 Loss 19.064021, Accuracy 0.078%\n",
      "Epoch 6, Batch 1030, LR 1.614849 Loss 19.063909, Accuracy 0.078%\n",
      "Epoch 6, Batch 1031, LR 1.615136 Loss 19.063868, Accuracy 0.078%\n",
      "Epoch 6, Batch 1032, LR 1.615423 Loss 19.063863, Accuracy 0.078%\n",
      "Epoch 6, Batch 1033, LR 1.615710 Loss 19.063890, Accuracy 0.079%\n",
      "Epoch 6, Batch 1034, LR 1.615998 Loss 19.063846, Accuracy 0.079%\n",
      "Epoch 6, Batch 1035, LR 1.616285 Loss 19.063834, Accuracy 0.079%\n",
      "Epoch 6, Batch 1036, LR 1.616572 Loss 19.063783, Accuracy 0.078%\n",
      "Epoch 6, Batch 1037, LR 1.616859 Loss 19.063791, Accuracy 0.078%\n",
      "Epoch 6, Batch 1038, LR 1.617146 Loss 19.063929, Accuracy 0.078%\n",
      "Epoch 6, Batch 1039, LR 1.617433 Loss 19.064070, Accuracy 0.078%\n",
      "Epoch 6, Batch 1040, LR 1.617720 Loss 19.064080, Accuracy 0.078%\n",
      "Epoch 6, Batch 1041, LR 1.618007 Loss 19.064067, Accuracy 0.078%\n",
      "Epoch 6, Batch 1042, LR 1.618294 Loss 19.064300, Accuracy 0.078%\n",
      "Epoch 6, Batch 1043, LR 1.618581 Loss 19.064357, Accuracy 0.078%\n",
      "Epoch 6, Batch 1044, LR 1.618867 Loss 19.064362, Accuracy 0.078%\n",
      "Epoch 6, Batch 1045, LR 1.619154 Loss 19.064341, Accuracy 0.078%\n",
      "Epoch 6, Batch 1046, LR 1.619441 Loss 19.064424, Accuracy 0.078%\n",
      "Epoch 6, Batch 1047, LR 1.619728 Loss 19.064514, Accuracy 0.078%\n",
      "Epoch 6, Loss (train set) 19.064514, Accuracy (train set) 0.078%\n",
      "Epoch 7, Batch 1, LR 1.620015 Loss 19.107780, Accuracy 0.000%\n",
      "Epoch 7, Batch 2, LR 1.620302 Loss 19.070698, Accuracy 0.000%\n",
      "Epoch 7, Batch 3, LR 1.620589 Loss 18.977348, Accuracy 0.000%\n",
      "Epoch 7, Batch 4, LR 1.620875 Loss 18.960535, Accuracy 0.000%\n",
      "Epoch 7, Batch 5, LR 1.621162 Loss 18.971818, Accuracy 0.000%\n",
      "Epoch 7, Batch 6, LR 1.621449 Loss 18.976327, Accuracy 0.000%\n",
      "Epoch 7, Batch 7, LR 1.621736 Loss 18.993040, Accuracy 0.000%\n",
      "Epoch 7, Batch 8, LR 1.622022 Loss 19.004552, Accuracy 0.000%\n",
      "Epoch 7, Batch 9, LR 1.622309 Loss 19.021870, Accuracy 0.000%\n",
      "Epoch 7, Batch 10, LR 1.622596 Loss 19.038028, Accuracy 0.000%\n",
      "Epoch 7, Batch 11, LR 1.622882 Loss 19.034977, Accuracy 0.071%\n",
      "Epoch 7, Batch 12, LR 1.623169 Loss 19.042377, Accuracy 0.065%\n",
      "Epoch 7, Batch 13, LR 1.623455 Loss 19.050450, Accuracy 0.060%\n",
      "Epoch 7, Batch 14, LR 1.623742 Loss 19.061330, Accuracy 0.056%\n",
      "Epoch 7, Batch 15, LR 1.624029 Loss 19.065840, Accuracy 0.052%\n",
      "Epoch 7, Batch 16, LR 1.624315 Loss 19.043833, Accuracy 0.098%\n",
      "Epoch 7, Batch 17, LR 1.624602 Loss 19.041362, Accuracy 0.092%\n",
      "Epoch 7, Batch 18, LR 1.624888 Loss 19.035563, Accuracy 0.087%\n",
      "Epoch 7, Batch 19, LR 1.625175 Loss 19.040929, Accuracy 0.082%\n",
      "Epoch 7, Batch 20, LR 1.625461 Loss 19.045534, Accuracy 0.078%\n",
      "Epoch 7, Batch 21, LR 1.625748 Loss 19.041393, Accuracy 0.074%\n",
      "Epoch 7, Batch 22, LR 1.626034 Loss 19.044090, Accuracy 0.071%\n",
      "Epoch 7, Batch 23, LR 1.626320 Loss 19.036970, Accuracy 0.068%\n",
      "Epoch 7, Batch 24, LR 1.626607 Loss 19.035490, Accuracy 0.065%\n",
      "Epoch 7, Batch 25, LR 1.626893 Loss 19.043129, Accuracy 0.062%\n",
      "Epoch 7, Batch 26, LR 1.627180 Loss 19.045403, Accuracy 0.090%\n",
      "Epoch 7, Batch 27, LR 1.627466 Loss 19.049625, Accuracy 0.116%\n",
      "Epoch 7, Batch 28, LR 1.627752 Loss 19.044607, Accuracy 0.112%\n",
      "Epoch 7, Batch 29, LR 1.628038 Loss 19.046450, Accuracy 0.108%\n",
      "Epoch 7, Batch 30, LR 1.628325 Loss 19.043820, Accuracy 0.104%\n",
      "Epoch 7, Batch 31, LR 1.628611 Loss 19.048019, Accuracy 0.101%\n",
      "Epoch 7, Batch 32, LR 1.628897 Loss 19.051220, Accuracy 0.098%\n",
      "Epoch 7, Batch 33, LR 1.629183 Loss 19.058657, Accuracy 0.095%\n",
      "Epoch 7, Batch 34, LR 1.629470 Loss 19.059989, Accuracy 0.092%\n",
      "Epoch 7, Batch 35, LR 1.629756 Loss 19.063269, Accuracy 0.089%\n",
      "Epoch 7, Batch 36, LR 1.630042 Loss 19.066039, Accuracy 0.087%\n",
      "Epoch 7, Batch 37, LR 1.630328 Loss 19.066157, Accuracy 0.084%\n",
      "Epoch 7, Batch 38, LR 1.630614 Loss 19.063475, Accuracy 0.082%\n",
      "Epoch 7, Batch 39, LR 1.630900 Loss 19.062945, Accuracy 0.080%\n",
      "Epoch 7, Batch 40, LR 1.631186 Loss 19.068801, Accuracy 0.078%\n",
      "Epoch 7, Batch 41, LR 1.631472 Loss 19.067994, Accuracy 0.076%\n",
      "Epoch 7, Batch 42, LR 1.631758 Loss 19.067414, Accuracy 0.093%\n",
      "Epoch 7, Batch 43, LR 1.632044 Loss 19.065748, Accuracy 0.091%\n",
      "Epoch 7, Batch 44, LR 1.632330 Loss 19.065710, Accuracy 0.089%\n",
      "Epoch 7, Batch 45, LR 1.632616 Loss 19.065193, Accuracy 0.087%\n",
      "Epoch 7, Batch 46, LR 1.632902 Loss 19.065418, Accuracy 0.085%\n",
      "Epoch 7, Batch 47, LR 1.633188 Loss 19.067054, Accuracy 0.100%\n",
      "Epoch 7, Batch 48, LR 1.633474 Loss 19.069539, Accuracy 0.098%\n",
      "Epoch 7, Batch 49, LR 1.633760 Loss 19.068325, Accuracy 0.096%\n",
      "Epoch 7, Batch 50, LR 1.634046 Loss 19.068671, Accuracy 0.094%\n",
      "Epoch 7, Batch 51, LR 1.634332 Loss 19.068609, Accuracy 0.092%\n",
      "Epoch 7, Batch 52, LR 1.634618 Loss 19.068806, Accuracy 0.105%\n",
      "Epoch 7, Batch 53, LR 1.634903 Loss 19.069408, Accuracy 0.103%\n",
      "Epoch 7, Batch 54, LR 1.635189 Loss 19.068905, Accuracy 0.101%\n",
      "Epoch 7, Batch 55, LR 1.635475 Loss 19.071934, Accuracy 0.099%\n",
      "Epoch 7, Batch 56, LR 1.635761 Loss 19.072174, Accuracy 0.098%\n",
      "Epoch 7, Batch 57, LR 1.636046 Loss 19.071980, Accuracy 0.110%\n",
      "Epoch 7, Batch 58, LR 1.636332 Loss 19.072831, Accuracy 0.108%\n",
      "Epoch 7, Batch 59, LR 1.636618 Loss 19.075036, Accuracy 0.106%\n",
      "Epoch 7, Batch 60, LR 1.636904 Loss 19.073588, Accuracy 0.104%\n",
      "Epoch 7, Batch 61, LR 1.637189 Loss 19.075397, Accuracy 0.102%\n",
      "Epoch 7, Batch 62, LR 1.637475 Loss 19.075103, Accuracy 0.101%\n",
      "Epoch 7, Batch 63, LR 1.637760 Loss 19.078065, Accuracy 0.099%\n",
      "Epoch 7, Batch 64, LR 1.638046 Loss 19.081623, Accuracy 0.098%\n",
      "Epoch 7, Batch 65, LR 1.638332 Loss 19.080817, Accuracy 0.096%\n",
      "Epoch 7, Batch 66, LR 1.638617 Loss 19.078676, Accuracy 0.095%\n",
      "Epoch 7, Batch 67, LR 1.638903 Loss 19.076456, Accuracy 0.093%\n",
      "Epoch 7, Batch 68, LR 1.639188 Loss 19.078082, Accuracy 0.092%\n",
      "Epoch 7, Batch 69, LR 1.639474 Loss 19.074369, Accuracy 0.091%\n",
      "Epoch 7, Batch 70, LR 1.639759 Loss 19.074185, Accuracy 0.089%\n",
      "Epoch 7, Batch 71, LR 1.640045 Loss 19.073293, Accuracy 0.088%\n",
      "Epoch 7, Batch 72, LR 1.640330 Loss 19.074370, Accuracy 0.087%\n",
      "Epoch 7, Batch 73, LR 1.640615 Loss 19.073279, Accuracy 0.086%\n",
      "Epoch 7, Batch 74, LR 1.640901 Loss 19.074017, Accuracy 0.084%\n",
      "Epoch 7, Batch 75, LR 1.641186 Loss 19.075179, Accuracy 0.083%\n",
      "Epoch 7, Batch 76, LR 1.641471 Loss 19.072998, Accuracy 0.082%\n",
      "Epoch 7, Batch 77, LR 1.641757 Loss 19.070250, Accuracy 0.081%\n",
      "Epoch 7, Batch 78, LR 1.642042 Loss 19.069251, Accuracy 0.080%\n",
      "Epoch 7, Batch 79, LR 1.642327 Loss 19.068318, Accuracy 0.089%\n",
      "Epoch 7, Batch 80, LR 1.642613 Loss 19.067032, Accuracy 0.088%\n",
      "Epoch 7, Batch 81, LR 1.642898 Loss 19.067664, Accuracy 0.087%\n",
      "Epoch 7, Batch 82, LR 1.643183 Loss 19.068652, Accuracy 0.086%\n",
      "Epoch 7, Batch 83, LR 1.643468 Loss 19.067286, Accuracy 0.085%\n",
      "Epoch 7, Batch 84, LR 1.643753 Loss 19.065749, Accuracy 0.084%\n",
      "Epoch 7, Batch 85, LR 1.644039 Loss 19.067031, Accuracy 0.083%\n",
      "Epoch 7, Batch 86, LR 1.644324 Loss 19.066474, Accuracy 0.082%\n",
      "Epoch 7, Batch 87, LR 1.644609 Loss 19.065667, Accuracy 0.081%\n",
      "Epoch 7, Batch 88, LR 1.644894 Loss 19.064205, Accuracy 0.080%\n",
      "Epoch 7, Batch 89, LR 1.645179 Loss 19.065418, Accuracy 0.079%\n",
      "Epoch 7, Batch 90, LR 1.645464 Loss 19.068010, Accuracy 0.078%\n",
      "Epoch 7, Batch 91, LR 1.645749 Loss 19.068381, Accuracy 0.077%\n",
      "Epoch 7, Batch 92, LR 1.646034 Loss 19.068815, Accuracy 0.076%\n",
      "Epoch 7, Batch 93, LR 1.646319 Loss 19.070265, Accuracy 0.076%\n",
      "Epoch 7, Batch 94, LR 1.646604 Loss 19.071808, Accuracy 0.075%\n",
      "Epoch 7, Batch 95, LR 1.646889 Loss 19.074030, Accuracy 0.074%\n",
      "Epoch 7, Batch 96, LR 1.647174 Loss 19.073708, Accuracy 0.073%\n",
      "Epoch 7, Batch 97, LR 1.647459 Loss 19.072597, Accuracy 0.072%\n",
      "Epoch 7, Batch 98, LR 1.647744 Loss 19.070523, Accuracy 0.072%\n",
      "Epoch 7, Batch 99, LR 1.648029 Loss 19.070978, Accuracy 0.071%\n",
      "Epoch 7, Batch 100, LR 1.648313 Loss 19.069998, Accuracy 0.070%\n",
      "Epoch 7, Batch 101, LR 1.648598 Loss 19.069192, Accuracy 0.070%\n",
      "Epoch 7, Batch 102, LR 1.648883 Loss 19.067305, Accuracy 0.069%\n",
      "Epoch 7, Batch 103, LR 1.649168 Loss 19.067433, Accuracy 0.068%\n",
      "Epoch 7, Batch 104, LR 1.649453 Loss 19.066912, Accuracy 0.068%\n",
      "Epoch 7, Batch 105, LR 1.649737 Loss 19.067398, Accuracy 0.067%\n",
      "Epoch 7, Batch 106, LR 1.650022 Loss 19.068720, Accuracy 0.066%\n",
      "Epoch 7, Batch 107, LR 1.650307 Loss 19.067604, Accuracy 0.073%\n",
      "Epoch 7, Batch 108, LR 1.650591 Loss 19.067460, Accuracy 0.072%\n",
      "Epoch 7, Batch 109, LR 1.650876 Loss 19.068576, Accuracy 0.072%\n",
      "Epoch 7, Batch 110, LR 1.651161 Loss 19.070162, Accuracy 0.071%\n",
      "Epoch 7, Batch 111, LR 1.651445 Loss 19.069219, Accuracy 0.070%\n",
      "Epoch 7, Batch 112, LR 1.651730 Loss 19.068895, Accuracy 0.070%\n",
      "Epoch 7, Batch 113, LR 1.652015 Loss 19.068492, Accuracy 0.069%\n",
      "Epoch 7, Batch 114, LR 1.652299 Loss 19.069197, Accuracy 0.069%\n",
      "Epoch 7, Batch 115, LR 1.652584 Loss 19.069055, Accuracy 0.068%\n",
      "Epoch 7, Batch 116, LR 1.652868 Loss 19.068588, Accuracy 0.067%\n",
      "Epoch 7, Batch 117, LR 1.653153 Loss 19.070498, Accuracy 0.067%\n",
      "Epoch 7, Batch 118, LR 1.653437 Loss 19.072311, Accuracy 0.066%\n",
      "Epoch 7, Batch 119, LR 1.653721 Loss 19.073169, Accuracy 0.066%\n",
      "Epoch 7, Batch 120, LR 1.654006 Loss 19.071386, Accuracy 0.065%\n",
      "Epoch 7, Batch 121, LR 1.654290 Loss 19.070825, Accuracy 0.065%\n",
      "Epoch 7, Batch 122, LR 1.654575 Loss 19.068912, Accuracy 0.070%\n",
      "Epoch 7, Batch 123, LR 1.654859 Loss 19.068448, Accuracy 0.070%\n",
      "Epoch 7, Batch 124, LR 1.655143 Loss 19.067984, Accuracy 0.069%\n",
      "Epoch 7, Batch 125, LR 1.655428 Loss 19.069209, Accuracy 0.069%\n",
      "Epoch 7, Batch 126, LR 1.655712 Loss 19.068921, Accuracy 0.068%\n",
      "Epoch 7, Batch 127, LR 1.655996 Loss 19.068847, Accuracy 0.068%\n",
      "Epoch 7, Batch 128, LR 1.656281 Loss 19.070446, Accuracy 0.073%\n",
      "Epoch 7, Batch 129, LR 1.656565 Loss 19.071824, Accuracy 0.073%\n",
      "Epoch 7, Batch 130, LR 1.656849 Loss 19.071969, Accuracy 0.072%\n",
      "Epoch 7, Batch 131, LR 1.657133 Loss 19.071391, Accuracy 0.072%\n",
      "Epoch 7, Batch 132, LR 1.657417 Loss 19.071945, Accuracy 0.071%\n",
      "Epoch 7, Batch 133, LR 1.657701 Loss 19.072034, Accuracy 0.070%\n",
      "Epoch 7, Batch 134, LR 1.657986 Loss 19.070429, Accuracy 0.070%\n",
      "Epoch 7, Batch 135, LR 1.658270 Loss 19.071298, Accuracy 0.069%\n",
      "Epoch 7, Batch 136, LR 1.658554 Loss 19.070604, Accuracy 0.069%\n",
      "Epoch 7, Batch 137, LR 1.658838 Loss 19.069887, Accuracy 0.068%\n",
      "Epoch 7, Batch 138, LR 1.659122 Loss 19.069620, Accuracy 0.068%\n",
      "Epoch 7, Batch 139, LR 1.659406 Loss 19.068929, Accuracy 0.067%\n",
      "Epoch 7, Batch 140, LR 1.659690 Loss 19.069716, Accuracy 0.067%\n",
      "Epoch 7, Batch 141, LR 1.659974 Loss 19.069645, Accuracy 0.066%\n",
      "Epoch 7, Batch 142, LR 1.660258 Loss 19.069096, Accuracy 0.066%\n",
      "Epoch 7, Batch 143, LR 1.660542 Loss 19.068705, Accuracy 0.066%\n",
      "Epoch 7, Batch 144, LR 1.660826 Loss 19.067975, Accuracy 0.065%\n",
      "Epoch 7, Batch 145, LR 1.661109 Loss 19.067843, Accuracy 0.065%\n",
      "Epoch 7, Batch 146, LR 1.661393 Loss 19.067823, Accuracy 0.064%\n",
      "Epoch 7, Batch 147, LR 1.661677 Loss 19.066817, Accuracy 0.064%\n",
      "Epoch 7, Batch 148, LR 1.661961 Loss 19.067796, Accuracy 0.063%\n",
      "Epoch 7, Batch 149, LR 1.662245 Loss 19.068080, Accuracy 0.063%\n",
      "Epoch 7, Batch 150, LR 1.662529 Loss 19.068806, Accuracy 0.062%\n",
      "Epoch 7, Batch 151, LR 1.662812 Loss 19.067763, Accuracy 0.062%\n",
      "Epoch 7, Batch 152, LR 1.663096 Loss 19.067569, Accuracy 0.062%\n",
      "Epoch 7, Batch 153, LR 1.663380 Loss 19.067695, Accuracy 0.061%\n",
      "Epoch 7, Batch 154, LR 1.663663 Loss 19.066877, Accuracy 0.061%\n",
      "Epoch 7, Batch 155, LR 1.663947 Loss 19.067739, Accuracy 0.060%\n",
      "Epoch 7, Batch 156, LR 1.664231 Loss 19.066509, Accuracy 0.060%\n",
      "Epoch 7, Batch 157, LR 1.664514 Loss 19.066598, Accuracy 0.060%\n",
      "Epoch 7, Batch 158, LR 1.664798 Loss 19.066720, Accuracy 0.059%\n",
      "Epoch 7, Batch 159, LR 1.665081 Loss 19.066788, Accuracy 0.059%\n",
      "Epoch 7, Batch 160, LR 1.665365 Loss 19.067709, Accuracy 0.059%\n",
      "Epoch 7, Batch 161, LR 1.665649 Loss 19.067865, Accuracy 0.058%\n",
      "Epoch 7, Batch 162, LR 1.665932 Loss 19.067841, Accuracy 0.063%\n",
      "Epoch 7, Batch 163, LR 1.666216 Loss 19.065950, Accuracy 0.062%\n",
      "Epoch 7, Batch 164, LR 1.666499 Loss 19.066231, Accuracy 0.062%\n",
      "Epoch 7, Batch 165, LR 1.666782 Loss 19.067096, Accuracy 0.062%\n",
      "Epoch 7, Batch 166, LR 1.667066 Loss 19.066126, Accuracy 0.061%\n",
      "Epoch 7, Batch 167, LR 1.667349 Loss 19.065686, Accuracy 0.061%\n",
      "Epoch 7, Batch 168, LR 1.667633 Loss 19.065166, Accuracy 0.060%\n",
      "Epoch 7, Batch 169, LR 1.667916 Loss 19.066124, Accuracy 0.060%\n",
      "Epoch 7, Batch 170, LR 1.668199 Loss 19.065880, Accuracy 0.060%\n",
      "Epoch 7, Batch 171, LR 1.668483 Loss 19.067288, Accuracy 0.059%\n",
      "Epoch 7, Batch 172, LR 1.668766 Loss 19.068365, Accuracy 0.059%\n",
      "Epoch 7, Batch 173, LR 1.669049 Loss 19.069044, Accuracy 0.059%\n",
      "Epoch 7, Batch 174, LR 1.669332 Loss 19.068832, Accuracy 0.058%\n",
      "Epoch 7, Batch 175, LR 1.669616 Loss 19.068389, Accuracy 0.058%\n",
      "Epoch 7, Batch 176, LR 1.669899 Loss 19.068471, Accuracy 0.058%\n",
      "Epoch 7, Batch 177, LR 1.670182 Loss 19.067551, Accuracy 0.057%\n",
      "Epoch 7, Batch 178, LR 1.670465 Loss 19.066837, Accuracy 0.057%\n",
      "Epoch 7, Batch 179, LR 1.670748 Loss 19.067363, Accuracy 0.057%\n",
      "Epoch 7, Batch 180, LR 1.671031 Loss 19.066972, Accuracy 0.056%\n",
      "Epoch 7, Batch 181, LR 1.671315 Loss 19.067854, Accuracy 0.060%\n",
      "Epoch 7, Batch 182, LR 1.671598 Loss 19.067707, Accuracy 0.060%\n",
      "Epoch 7, Batch 183, LR 1.671881 Loss 19.068134, Accuracy 0.060%\n",
      "Epoch 7, Batch 184, LR 1.672164 Loss 19.068404, Accuracy 0.059%\n",
      "Epoch 7, Batch 185, LR 1.672447 Loss 19.068032, Accuracy 0.059%\n",
      "Epoch 7, Batch 186, LR 1.672730 Loss 19.067419, Accuracy 0.059%\n",
      "Epoch 7, Batch 187, LR 1.673013 Loss 19.068294, Accuracy 0.058%\n",
      "Epoch 7, Batch 188, LR 1.673295 Loss 19.068202, Accuracy 0.062%\n",
      "Epoch 7, Batch 189, LR 1.673578 Loss 19.068754, Accuracy 0.062%\n",
      "Epoch 7, Batch 190, LR 1.673861 Loss 19.069464, Accuracy 0.062%\n",
      "Epoch 7, Batch 191, LR 1.674144 Loss 19.069555, Accuracy 0.061%\n",
      "Epoch 7, Batch 192, LR 1.674427 Loss 19.069242, Accuracy 0.061%\n",
      "Epoch 7, Batch 193, LR 1.674710 Loss 19.069624, Accuracy 0.061%\n",
      "Epoch 7, Batch 194, LR 1.674993 Loss 19.068387, Accuracy 0.060%\n",
      "Epoch 7, Batch 195, LR 1.675275 Loss 19.068388, Accuracy 0.060%\n",
      "Epoch 7, Batch 196, LR 1.675558 Loss 19.067244, Accuracy 0.060%\n",
      "Epoch 7, Batch 197, LR 1.675841 Loss 19.067275, Accuracy 0.059%\n",
      "Epoch 7, Batch 198, LR 1.676124 Loss 19.066617, Accuracy 0.059%\n",
      "Epoch 7, Batch 199, LR 1.676406 Loss 19.065336, Accuracy 0.059%\n",
      "Epoch 7, Batch 200, LR 1.676689 Loss 19.065650, Accuracy 0.059%\n",
      "Epoch 7, Batch 201, LR 1.676971 Loss 19.065441, Accuracy 0.058%\n",
      "Epoch 7, Batch 202, LR 1.677254 Loss 19.065542, Accuracy 0.058%\n",
      "Epoch 7, Batch 203, LR 1.677537 Loss 19.066221, Accuracy 0.058%\n",
      "Epoch 7, Batch 204, LR 1.677819 Loss 19.066200, Accuracy 0.057%\n",
      "Epoch 7, Batch 205, LR 1.678102 Loss 19.065779, Accuracy 0.057%\n",
      "Epoch 7, Batch 206, LR 1.678384 Loss 19.065794, Accuracy 0.057%\n",
      "Epoch 7, Batch 207, LR 1.678667 Loss 19.065955, Accuracy 0.057%\n",
      "Epoch 7, Batch 208, LR 1.678949 Loss 19.066619, Accuracy 0.056%\n",
      "Epoch 7, Batch 209, LR 1.679232 Loss 19.066812, Accuracy 0.056%\n",
      "Epoch 7, Batch 210, LR 1.679514 Loss 19.066705, Accuracy 0.056%\n",
      "Epoch 7, Batch 211, LR 1.679797 Loss 19.067053, Accuracy 0.056%\n",
      "Epoch 7, Batch 212, LR 1.680079 Loss 19.067538, Accuracy 0.059%\n",
      "Epoch 7, Batch 213, LR 1.680361 Loss 19.067624, Accuracy 0.059%\n",
      "Epoch 7, Batch 214, LR 1.680644 Loss 19.067583, Accuracy 0.058%\n",
      "Epoch 7, Batch 215, LR 1.680926 Loss 19.067527, Accuracy 0.058%\n",
      "Epoch 7, Batch 216, LR 1.681208 Loss 19.066359, Accuracy 0.061%\n",
      "Epoch 7, Batch 217, LR 1.681491 Loss 19.066472, Accuracy 0.061%\n",
      "Epoch 7, Batch 218, LR 1.681773 Loss 19.065702, Accuracy 0.061%\n",
      "Epoch 7, Batch 219, LR 1.682055 Loss 19.065917, Accuracy 0.061%\n",
      "Epoch 7, Batch 220, LR 1.682337 Loss 19.066322, Accuracy 0.060%\n",
      "Epoch 7, Batch 221, LR 1.682619 Loss 19.066590, Accuracy 0.060%\n",
      "Epoch 7, Batch 222, LR 1.682901 Loss 19.066947, Accuracy 0.060%\n",
      "Epoch 7, Batch 223, LR 1.683184 Loss 19.067221, Accuracy 0.060%\n",
      "Epoch 7, Batch 224, LR 1.683466 Loss 19.067055, Accuracy 0.059%\n",
      "Epoch 7, Batch 225, LR 1.683748 Loss 19.066775, Accuracy 0.059%\n",
      "Epoch 7, Batch 226, LR 1.684030 Loss 19.067423, Accuracy 0.059%\n",
      "Epoch 7, Batch 227, LR 1.684312 Loss 19.067414, Accuracy 0.059%\n",
      "Epoch 7, Batch 228, LR 1.684594 Loss 19.067494, Accuracy 0.058%\n",
      "Epoch 7, Batch 229, LR 1.684876 Loss 19.068046, Accuracy 0.058%\n",
      "Epoch 7, Batch 230, LR 1.685158 Loss 19.067882, Accuracy 0.058%\n",
      "Epoch 7, Batch 231, LR 1.685440 Loss 19.068520, Accuracy 0.057%\n",
      "Epoch 7, Batch 232, LR 1.685722 Loss 19.067982, Accuracy 0.057%\n",
      "Epoch 7, Batch 233, LR 1.686004 Loss 19.067269, Accuracy 0.057%\n",
      "Epoch 7, Batch 234, LR 1.686285 Loss 19.067640, Accuracy 0.057%\n",
      "Epoch 7, Batch 235, LR 1.686567 Loss 19.066870, Accuracy 0.057%\n",
      "Epoch 7, Batch 236, LR 1.686849 Loss 19.066984, Accuracy 0.056%\n",
      "Epoch 7, Batch 237, LR 1.687131 Loss 19.067066, Accuracy 0.056%\n",
      "Epoch 7, Batch 238, LR 1.687413 Loss 19.067485, Accuracy 0.056%\n",
      "Epoch 7, Batch 239, LR 1.687695 Loss 19.066967, Accuracy 0.056%\n",
      "Epoch 7, Batch 240, LR 1.687976 Loss 19.067314, Accuracy 0.055%\n",
      "Epoch 7, Batch 241, LR 1.688258 Loss 19.067130, Accuracy 0.055%\n",
      "Epoch 7, Batch 242, LR 1.688540 Loss 19.067075, Accuracy 0.055%\n",
      "Epoch 7, Batch 243, LR 1.688821 Loss 19.067421, Accuracy 0.055%\n",
      "Epoch 7, Batch 244, LR 1.689103 Loss 19.066869, Accuracy 0.054%\n",
      "Epoch 7, Batch 245, LR 1.689385 Loss 19.067396, Accuracy 0.054%\n",
      "Epoch 7, Batch 246, LR 1.689666 Loss 19.067519, Accuracy 0.054%\n",
      "Epoch 7, Batch 247, LR 1.689948 Loss 19.068115, Accuracy 0.054%\n",
      "Epoch 7, Batch 248, LR 1.690229 Loss 19.067715, Accuracy 0.054%\n",
      "Epoch 7, Batch 249, LR 1.690511 Loss 19.067337, Accuracy 0.053%\n",
      "Epoch 7, Batch 250, LR 1.690792 Loss 19.067076, Accuracy 0.053%\n",
      "Epoch 7, Batch 251, LR 1.691074 Loss 19.065716, Accuracy 0.053%\n",
      "Epoch 7, Batch 252, LR 1.691355 Loss 19.066265, Accuracy 0.053%\n",
      "Epoch 7, Batch 253, LR 1.691637 Loss 19.066105, Accuracy 0.052%\n",
      "Epoch 7, Batch 254, LR 1.691918 Loss 19.065543, Accuracy 0.052%\n",
      "Epoch 7, Batch 255, LR 1.692199 Loss 19.064962, Accuracy 0.052%\n",
      "Epoch 7, Batch 256, LR 1.692481 Loss 19.064390, Accuracy 0.052%\n",
      "Epoch 7, Batch 257, LR 1.692762 Loss 19.063852, Accuracy 0.052%\n",
      "Epoch 7, Batch 258, LR 1.693043 Loss 19.063746, Accuracy 0.051%\n",
      "Epoch 7, Batch 259, LR 1.693325 Loss 19.064604, Accuracy 0.051%\n",
      "Epoch 7, Batch 260, LR 1.693606 Loss 19.065017, Accuracy 0.051%\n",
      "Epoch 7, Batch 261, LR 1.693887 Loss 19.065121, Accuracy 0.051%\n",
      "Epoch 7, Batch 262, LR 1.694168 Loss 19.065103, Accuracy 0.051%\n",
      "Epoch 7, Batch 263, LR 1.694450 Loss 19.065292, Accuracy 0.050%\n",
      "Epoch 7, Batch 264, LR 1.694731 Loss 19.065167, Accuracy 0.050%\n",
      "Epoch 7, Batch 265, LR 1.695012 Loss 19.065681, Accuracy 0.050%\n",
      "Epoch 7, Batch 266, LR 1.695293 Loss 19.065505, Accuracy 0.050%\n",
      "Epoch 7, Batch 267, LR 1.695574 Loss 19.066092, Accuracy 0.050%\n",
      "Epoch 7, Batch 268, LR 1.695855 Loss 19.066312, Accuracy 0.050%\n",
      "Epoch 7, Batch 269, LR 1.696136 Loss 19.065905, Accuracy 0.049%\n",
      "Epoch 7, Batch 270, LR 1.696417 Loss 19.065642, Accuracy 0.052%\n",
      "Epoch 7, Batch 271, LR 1.696698 Loss 19.065848, Accuracy 0.055%\n",
      "Epoch 7, Batch 272, LR 1.696979 Loss 19.065840, Accuracy 0.055%\n",
      "Epoch 7, Batch 273, LR 1.697260 Loss 19.065325, Accuracy 0.054%\n",
      "Epoch 7, Batch 274, LR 1.697541 Loss 19.065923, Accuracy 0.054%\n",
      "Epoch 7, Batch 275, LR 1.697822 Loss 19.066439, Accuracy 0.054%\n",
      "Epoch 7, Batch 276, LR 1.698103 Loss 19.066720, Accuracy 0.054%\n",
      "Epoch 7, Batch 277, LR 1.698384 Loss 19.067025, Accuracy 0.054%\n",
      "Epoch 7, Batch 278, LR 1.698664 Loss 19.067224, Accuracy 0.053%\n",
      "Epoch 7, Batch 279, LR 1.698945 Loss 19.067621, Accuracy 0.053%\n",
      "Epoch 7, Batch 280, LR 1.699226 Loss 19.068246, Accuracy 0.053%\n",
      "Epoch 7, Batch 281, LR 1.699507 Loss 19.068579, Accuracy 0.053%\n",
      "Epoch 7, Batch 282, LR 1.699788 Loss 19.068298, Accuracy 0.053%\n",
      "Epoch 7, Batch 283, LR 1.700068 Loss 19.068505, Accuracy 0.052%\n",
      "Epoch 7, Batch 284, LR 1.700349 Loss 19.068838, Accuracy 0.052%\n",
      "Epoch 7, Batch 285, LR 1.700630 Loss 19.068734, Accuracy 0.052%\n",
      "Epoch 7, Batch 286, LR 1.700910 Loss 19.068551, Accuracy 0.055%\n",
      "Epoch 7, Batch 287, LR 1.701191 Loss 19.068885, Accuracy 0.054%\n",
      "Epoch 7, Batch 288, LR 1.701471 Loss 19.069221, Accuracy 0.054%\n",
      "Epoch 7, Batch 289, LR 1.701752 Loss 19.069696, Accuracy 0.054%\n",
      "Epoch 7, Batch 290, LR 1.702033 Loss 19.070542, Accuracy 0.054%\n",
      "Epoch 7, Batch 291, LR 1.702313 Loss 19.070508, Accuracy 0.054%\n",
      "Epoch 7, Batch 292, LR 1.702594 Loss 19.070465, Accuracy 0.054%\n",
      "Epoch 7, Batch 293, LR 1.702874 Loss 19.069809, Accuracy 0.053%\n",
      "Epoch 7, Batch 294, LR 1.703154 Loss 19.070238, Accuracy 0.053%\n",
      "Epoch 7, Batch 295, LR 1.703435 Loss 19.070865, Accuracy 0.053%\n",
      "Epoch 7, Batch 296, LR 1.703715 Loss 19.070642, Accuracy 0.053%\n",
      "Epoch 7, Batch 297, LR 1.703996 Loss 19.070437, Accuracy 0.053%\n",
      "Epoch 7, Batch 298, LR 1.704276 Loss 19.070681, Accuracy 0.052%\n",
      "Epoch 7, Batch 299, LR 1.704556 Loss 19.070785, Accuracy 0.052%\n",
      "Epoch 7, Batch 300, LR 1.704837 Loss 19.071113, Accuracy 0.052%\n",
      "Epoch 7, Batch 301, LR 1.705117 Loss 19.070906, Accuracy 0.055%\n",
      "Epoch 7, Batch 302, LR 1.705397 Loss 19.070884, Accuracy 0.054%\n",
      "Epoch 7, Batch 303, LR 1.705677 Loss 19.070755, Accuracy 0.054%\n",
      "Epoch 7, Batch 304, LR 1.705957 Loss 19.070790, Accuracy 0.054%\n",
      "Epoch 7, Batch 305, LR 1.706238 Loss 19.071187, Accuracy 0.054%\n",
      "Epoch 7, Batch 306, LR 1.706518 Loss 19.071328, Accuracy 0.054%\n",
      "Epoch 7, Batch 307, LR 1.706798 Loss 19.071361, Accuracy 0.053%\n",
      "Epoch 7, Batch 308, LR 1.707078 Loss 19.071204, Accuracy 0.056%\n",
      "Epoch 7, Batch 309, LR 1.707358 Loss 19.071294, Accuracy 0.058%\n",
      "Epoch 7, Batch 310, LR 1.707638 Loss 19.071130, Accuracy 0.058%\n",
      "Epoch 7, Batch 311, LR 1.707918 Loss 19.071209, Accuracy 0.058%\n",
      "Epoch 7, Batch 312, LR 1.708198 Loss 19.071056, Accuracy 0.060%\n",
      "Epoch 7, Batch 313, LR 1.708478 Loss 19.071037, Accuracy 0.060%\n",
      "Epoch 7, Batch 314, LR 1.708758 Loss 19.071232, Accuracy 0.060%\n",
      "Epoch 7, Batch 315, LR 1.709038 Loss 19.070703, Accuracy 0.060%\n",
      "Epoch 7, Batch 316, LR 1.709318 Loss 19.070569, Accuracy 0.059%\n",
      "Epoch 7, Batch 317, LR 1.709598 Loss 19.070518, Accuracy 0.059%\n",
      "Epoch 7, Batch 318, LR 1.709878 Loss 19.070511, Accuracy 0.059%\n",
      "Epoch 7, Batch 319, LR 1.710157 Loss 19.070987, Accuracy 0.059%\n",
      "Epoch 7, Batch 320, LR 1.710437 Loss 19.070970, Accuracy 0.059%\n",
      "Epoch 7, Batch 321, LR 1.710717 Loss 19.070689, Accuracy 0.058%\n",
      "Epoch 7, Batch 322, LR 1.710997 Loss 19.071062, Accuracy 0.058%\n",
      "Epoch 7, Batch 323, LR 1.711276 Loss 19.070584, Accuracy 0.058%\n",
      "Epoch 7, Batch 324, LR 1.711556 Loss 19.070241, Accuracy 0.058%\n",
      "Epoch 7, Batch 325, LR 1.711836 Loss 19.070235, Accuracy 0.060%\n",
      "Epoch 7, Batch 326, LR 1.712115 Loss 19.069676, Accuracy 0.060%\n",
      "Epoch 7, Batch 327, LR 1.712395 Loss 19.070029, Accuracy 0.060%\n",
      "Epoch 7, Batch 328, LR 1.712675 Loss 19.070416, Accuracy 0.060%\n",
      "Epoch 7, Batch 329, LR 1.712954 Loss 19.070241, Accuracy 0.059%\n",
      "Epoch 7, Batch 330, LR 1.713234 Loss 19.070213, Accuracy 0.059%\n",
      "Epoch 7, Batch 331, LR 1.713513 Loss 19.070679, Accuracy 0.059%\n",
      "Epoch 7, Batch 332, LR 1.713793 Loss 19.070669, Accuracy 0.059%\n",
      "Epoch 7, Batch 333, LR 1.714072 Loss 19.070697, Accuracy 0.059%\n",
      "Epoch 7, Batch 334, LR 1.714352 Loss 19.070349, Accuracy 0.061%\n",
      "Epoch 7, Batch 335, LR 1.714631 Loss 19.070481, Accuracy 0.061%\n",
      "Epoch 7, Batch 336, LR 1.714911 Loss 19.070190, Accuracy 0.060%\n",
      "Epoch 7, Batch 337, LR 1.715190 Loss 19.070585, Accuracy 0.060%\n",
      "Epoch 7, Batch 338, LR 1.715469 Loss 19.070829, Accuracy 0.060%\n",
      "Epoch 7, Batch 339, LR 1.715749 Loss 19.070306, Accuracy 0.060%\n",
      "Epoch 7, Batch 340, LR 1.716028 Loss 19.070378, Accuracy 0.060%\n",
      "Epoch 7, Batch 341, LR 1.716307 Loss 19.070463, Accuracy 0.062%\n",
      "Epoch 7, Batch 342, LR 1.716586 Loss 19.069918, Accuracy 0.062%\n",
      "Epoch 7, Batch 343, LR 1.716866 Loss 19.070551, Accuracy 0.061%\n",
      "Epoch 7, Batch 344, LR 1.717145 Loss 19.070169, Accuracy 0.064%\n",
      "Epoch 7, Batch 345, LR 1.717424 Loss 19.070361, Accuracy 0.063%\n",
      "Epoch 7, Batch 346, LR 1.717703 Loss 19.070371, Accuracy 0.063%\n",
      "Epoch 7, Batch 347, LR 1.717982 Loss 19.070554, Accuracy 0.063%\n",
      "Epoch 7, Batch 348, LR 1.718261 Loss 19.070076, Accuracy 0.063%\n",
      "Epoch 7, Batch 349, LR 1.718540 Loss 19.069995, Accuracy 0.065%\n",
      "Epoch 7, Batch 350, LR 1.718819 Loss 19.069727, Accuracy 0.065%\n",
      "Epoch 7, Batch 351, LR 1.719098 Loss 19.070260, Accuracy 0.065%\n",
      "Epoch 7, Batch 352, LR 1.719377 Loss 19.069529, Accuracy 0.067%\n",
      "Epoch 7, Batch 353, LR 1.719656 Loss 19.069381, Accuracy 0.066%\n",
      "Epoch 7, Batch 354, LR 1.719935 Loss 19.069950, Accuracy 0.066%\n",
      "Epoch 7, Batch 355, LR 1.720214 Loss 19.070376, Accuracy 0.066%\n",
      "Epoch 7, Batch 356, LR 1.720493 Loss 19.070474, Accuracy 0.066%\n",
      "Epoch 7, Batch 357, LR 1.720772 Loss 19.070757, Accuracy 0.066%\n",
      "Epoch 7, Batch 358, LR 1.721051 Loss 19.070533, Accuracy 0.065%\n",
      "Epoch 7, Batch 359, LR 1.721330 Loss 19.070590, Accuracy 0.065%\n",
      "Epoch 7, Batch 360, LR 1.721608 Loss 19.070431, Accuracy 0.065%\n",
      "Epoch 7, Batch 361, LR 1.721887 Loss 19.069888, Accuracy 0.065%\n",
      "Epoch 7, Batch 362, LR 1.722166 Loss 19.069493, Accuracy 0.065%\n",
      "Epoch 7, Batch 363, LR 1.722445 Loss 19.069127, Accuracy 0.065%\n",
      "Epoch 7, Batch 364, LR 1.722723 Loss 19.069282, Accuracy 0.064%\n",
      "Epoch 7, Batch 365, LR 1.723002 Loss 19.068806, Accuracy 0.064%\n",
      "Epoch 7, Batch 366, LR 1.723281 Loss 19.068249, Accuracy 0.064%\n",
      "Epoch 7, Batch 367, LR 1.723559 Loss 19.068422, Accuracy 0.066%\n",
      "Epoch 7, Batch 368, LR 1.723838 Loss 19.068422, Accuracy 0.066%\n",
      "Epoch 7, Batch 369, LR 1.724116 Loss 19.068635, Accuracy 0.066%\n",
      "Epoch 7, Batch 370, LR 1.724395 Loss 19.068322, Accuracy 0.068%\n",
      "Epoch 7, Batch 371, LR 1.724673 Loss 19.068813, Accuracy 0.067%\n",
      "Epoch 7, Batch 372, LR 1.724952 Loss 19.068276, Accuracy 0.067%\n",
      "Epoch 7, Batch 373, LR 1.725230 Loss 19.068238, Accuracy 0.067%\n",
      "Epoch 7, Batch 374, LR 1.725509 Loss 19.068220, Accuracy 0.067%\n",
      "Epoch 7, Batch 375, LR 1.725787 Loss 19.068790, Accuracy 0.067%\n",
      "Epoch 7, Batch 376, LR 1.726066 Loss 19.068672, Accuracy 0.066%\n",
      "Epoch 7, Batch 377, LR 1.726344 Loss 19.069215, Accuracy 0.066%\n",
      "Epoch 7, Batch 378, LR 1.726622 Loss 19.069099, Accuracy 0.066%\n",
      "Epoch 7, Batch 379, LR 1.726901 Loss 19.069448, Accuracy 0.066%\n",
      "Epoch 7, Batch 380, LR 1.727179 Loss 19.069608, Accuracy 0.066%\n",
      "Epoch 7, Batch 381, LR 1.727457 Loss 19.069443, Accuracy 0.066%\n",
      "Epoch 7, Batch 382, LR 1.727735 Loss 19.068973, Accuracy 0.065%\n",
      "Epoch 7, Batch 383, LR 1.728014 Loss 19.068834, Accuracy 0.065%\n",
      "Epoch 7, Batch 384, LR 1.728292 Loss 19.068744, Accuracy 0.065%\n",
      "Epoch 7, Batch 385, LR 1.728570 Loss 19.069231, Accuracy 0.065%\n",
      "Epoch 7, Batch 386, LR 1.728848 Loss 19.068630, Accuracy 0.065%\n",
      "Epoch 7, Batch 387, LR 1.729126 Loss 19.068653, Accuracy 0.065%\n",
      "Epoch 7, Batch 388, LR 1.729404 Loss 19.068925, Accuracy 0.064%\n",
      "Epoch 7, Batch 389, LR 1.729682 Loss 19.069127, Accuracy 0.064%\n",
      "Epoch 7, Batch 390, LR 1.729960 Loss 19.069055, Accuracy 0.064%\n",
      "Epoch 7, Batch 391, LR 1.730238 Loss 19.068709, Accuracy 0.064%\n",
      "Epoch 7, Batch 392, LR 1.730516 Loss 19.068842, Accuracy 0.066%\n",
      "Epoch 7, Batch 393, LR 1.730794 Loss 19.068641, Accuracy 0.066%\n",
      "Epoch 7, Batch 394, LR 1.731072 Loss 19.068237, Accuracy 0.065%\n",
      "Epoch 7, Batch 395, LR 1.731350 Loss 19.067704, Accuracy 0.065%\n",
      "Epoch 7, Batch 396, LR 1.731628 Loss 19.068097, Accuracy 0.065%\n",
      "Epoch 7, Batch 397, LR 1.731906 Loss 19.068437, Accuracy 0.065%\n",
      "Epoch 7, Batch 398, LR 1.732183 Loss 19.068315, Accuracy 0.065%\n",
      "Epoch 7, Batch 399, LR 1.732461 Loss 19.068262, Accuracy 0.065%\n",
      "Epoch 7, Batch 400, LR 1.732739 Loss 19.067977, Accuracy 0.064%\n",
      "Epoch 7, Batch 401, LR 1.733017 Loss 19.068187, Accuracy 0.064%\n",
      "Epoch 7, Batch 402, LR 1.733294 Loss 19.068595, Accuracy 0.064%\n",
      "Epoch 7, Batch 403, LR 1.733572 Loss 19.068682, Accuracy 0.064%\n",
      "Epoch 7, Batch 404, LR 1.733850 Loss 19.069057, Accuracy 0.064%\n",
      "Epoch 7, Batch 405, LR 1.734127 Loss 19.068929, Accuracy 0.064%\n",
      "Epoch 7, Batch 406, LR 1.734405 Loss 19.069109, Accuracy 0.064%\n",
      "Epoch 7, Batch 407, LR 1.734682 Loss 19.069281, Accuracy 0.063%\n",
      "Epoch 7, Batch 408, LR 1.734960 Loss 19.069371, Accuracy 0.063%\n",
      "Epoch 7, Batch 409, LR 1.735237 Loss 19.069459, Accuracy 0.063%\n",
      "Epoch 7, Batch 410, LR 1.735515 Loss 19.069655, Accuracy 0.063%\n",
      "Epoch 7, Batch 411, LR 1.735792 Loss 19.069482, Accuracy 0.063%\n",
      "Epoch 7, Batch 412, LR 1.736070 Loss 19.069841, Accuracy 0.064%\n",
      "Epoch 7, Batch 413, LR 1.736347 Loss 19.069643, Accuracy 0.064%\n",
      "Epoch 7, Batch 414, LR 1.736625 Loss 19.069570, Accuracy 0.064%\n",
      "Epoch 7, Batch 415, LR 1.736902 Loss 19.069448, Accuracy 0.066%\n",
      "Epoch 7, Batch 416, LR 1.737179 Loss 19.069677, Accuracy 0.066%\n",
      "Epoch 7, Batch 417, LR 1.737457 Loss 19.070072, Accuracy 0.066%\n",
      "Epoch 7, Batch 418, LR 1.737734 Loss 19.069324, Accuracy 0.065%\n",
      "Epoch 7, Batch 419, LR 1.738011 Loss 19.069381, Accuracy 0.065%\n",
      "Epoch 7, Batch 420, LR 1.738288 Loss 19.069938, Accuracy 0.065%\n",
      "Epoch 7, Batch 421, LR 1.738566 Loss 19.070037, Accuracy 0.067%\n",
      "Epoch 7, Batch 422, LR 1.738843 Loss 19.070121, Accuracy 0.067%\n",
      "Epoch 7, Batch 423, LR 1.739120 Loss 19.070311, Accuracy 0.066%\n",
      "Epoch 7, Batch 424, LR 1.739397 Loss 19.070003, Accuracy 0.066%\n",
      "Epoch 7, Batch 425, LR 1.739674 Loss 19.070200, Accuracy 0.066%\n",
      "Epoch 7, Batch 426, LR 1.739951 Loss 19.070475, Accuracy 0.066%\n",
      "Epoch 7, Batch 427, LR 1.740228 Loss 19.070688, Accuracy 0.066%\n",
      "Epoch 7, Batch 428, LR 1.740505 Loss 19.070574, Accuracy 0.066%\n",
      "Epoch 7, Batch 429, LR 1.740782 Loss 19.070427, Accuracy 0.066%\n",
      "Epoch 7, Batch 430, LR 1.741059 Loss 19.070330, Accuracy 0.065%\n",
      "Epoch 7, Batch 431, LR 1.741336 Loss 19.070632, Accuracy 0.067%\n",
      "Epoch 7, Batch 432, LR 1.741613 Loss 19.071010, Accuracy 0.067%\n",
      "Epoch 7, Batch 433, LR 1.741890 Loss 19.071203, Accuracy 0.067%\n",
      "Epoch 7, Batch 434, LR 1.742167 Loss 19.071322, Accuracy 0.067%\n",
      "Epoch 7, Batch 435, LR 1.742443 Loss 19.071285, Accuracy 0.066%\n",
      "Epoch 7, Batch 436, LR 1.742720 Loss 19.071201, Accuracy 0.066%\n",
      "Epoch 7, Batch 437, LR 1.742997 Loss 19.071115, Accuracy 0.068%\n",
      "Epoch 7, Batch 438, LR 1.743274 Loss 19.070536, Accuracy 0.070%\n",
      "Epoch 7, Batch 439, LR 1.743550 Loss 19.070528, Accuracy 0.069%\n",
      "Epoch 7, Batch 440, LR 1.743827 Loss 19.070236, Accuracy 0.069%\n",
      "Epoch 7, Batch 441, LR 1.744104 Loss 19.070059, Accuracy 0.071%\n",
      "Epoch 7, Batch 442, LR 1.744380 Loss 19.069632, Accuracy 0.071%\n",
      "Epoch 7, Batch 443, LR 1.744657 Loss 19.069481, Accuracy 0.071%\n",
      "Epoch 7, Batch 444, LR 1.744934 Loss 19.069594, Accuracy 0.070%\n",
      "Epoch 7, Batch 445, LR 1.745210 Loss 19.069382, Accuracy 0.070%\n",
      "Epoch 7, Batch 446, LR 1.745487 Loss 19.069389, Accuracy 0.070%\n",
      "Epoch 7, Batch 447, LR 1.745763 Loss 19.069804, Accuracy 0.070%\n",
      "Epoch 7, Batch 448, LR 1.746040 Loss 19.070105, Accuracy 0.070%\n",
      "Epoch 7, Batch 449, LR 1.746316 Loss 19.070287, Accuracy 0.070%\n",
      "Epoch 7, Batch 450, LR 1.746593 Loss 19.070179, Accuracy 0.069%\n",
      "Epoch 7, Batch 451, LR 1.746869 Loss 19.069863, Accuracy 0.071%\n",
      "Epoch 7, Batch 452, LR 1.747145 Loss 19.069961, Accuracy 0.071%\n",
      "Epoch 7, Batch 453, LR 1.747422 Loss 19.069877, Accuracy 0.071%\n",
      "Epoch 7, Batch 454, LR 1.747698 Loss 19.069672, Accuracy 0.071%\n",
      "Epoch 7, Batch 455, LR 1.747974 Loss 19.069738, Accuracy 0.070%\n",
      "Epoch 7, Batch 456, LR 1.748250 Loss 19.069576, Accuracy 0.070%\n",
      "Epoch 7, Batch 457, LR 1.748527 Loss 19.069755, Accuracy 0.070%\n",
      "Epoch 7, Batch 458, LR 1.748803 Loss 19.069109, Accuracy 0.070%\n",
      "Epoch 7, Batch 459, LR 1.749079 Loss 19.069271, Accuracy 0.070%\n",
      "Epoch 7, Batch 460, LR 1.749355 Loss 19.069428, Accuracy 0.070%\n",
      "Epoch 7, Batch 461, LR 1.749631 Loss 19.069254, Accuracy 0.069%\n",
      "Epoch 7, Batch 462, LR 1.749907 Loss 19.069138, Accuracy 0.069%\n",
      "Epoch 7, Batch 463, LR 1.750183 Loss 19.069136, Accuracy 0.069%\n",
      "Epoch 7, Batch 464, LR 1.750460 Loss 19.069421, Accuracy 0.069%\n",
      "Epoch 7, Batch 465, LR 1.750736 Loss 19.068923, Accuracy 0.071%\n",
      "Epoch 7, Batch 466, LR 1.751012 Loss 19.069195, Accuracy 0.070%\n",
      "Epoch 7, Batch 467, LR 1.751287 Loss 19.068953, Accuracy 0.070%\n",
      "Epoch 7, Batch 468, LR 1.751563 Loss 19.068817, Accuracy 0.070%\n",
      "Epoch 7, Batch 469, LR 1.751839 Loss 19.068398, Accuracy 0.070%\n",
      "Epoch 7, Batch 470, LR 1.752115 Loss 19.068267, Accuracy 0.070%\n",
      "Epoch 7, Batch 471, LR 1.752391 Loss 19.068165, Accuracy 0.070%\n",
      "Epoch 7, Batch 472, LR 1.752667 Loss 19.068153, Accuracy 0.070%\n",
      "Epoch 7, Batch 473, LR 1.752943 Loss 19.068240, Accuracy 0.069%\n",
      "Epoch 7, Batch 474, LR 1.753218 Loss 19.067835, Accuracy 0.069%\n",
      "Epoch 7, Batch 475, LR 1.753494 Loss 19.067784, Accuracy 0.069%\n",
      "Epoch 7, Batch 476, LR 1.753770 Loss 19.067737, Accuracy 0.069%\n",
      "Epoch 7, Batch 477, LR 1.754045 Loss 19.067494, Accuracy 0.069%\n",
      "Epoch 7, Batch 478, LR 1.754321 Loss 19.067401, Accuracy 0.070%\n",
      "Epoch 7, Batch 479, LR 1.754597 Loss 19.067606, Accuracy 0.070%\n",
      "Epoch 7, Batch 480, LR 1.754872 Loss 19.067336, Accuracy 0.072%\n",
      "Epoch 7, Batch 481, LR 1.755148 Loss 19.067077, Accuracy 0.071%\n",
      "Epoch 7, Batch 482, LR 1.755423 Loss 19.066911, Accuracy 0.071%\n",
      "Epoch 7, Batch 483, LR 1.755699 Loss 19.066920, Accuracy 0.071%\n",
      "Epoch 7, Batch 484, LR 1.755974 Loss 19.066813, Accuracy 0.071%\n",
      "Epoch 7, Batch 485, LR 1.756250 Loss 19.066476, Accuracy 0.071%\n",
      "Epoch 7, Batch 486, LR 1.756525 Loss 19.066424, Accuracy 0.071%\n",
      "Epoch 7, Batch 487, LR 1.756801 Loss 19.066640, Accuracy 0.071%\n",
      "Epoch 7, Batch 488, LR 1.757076 Loss 19.066783, Accuracy 0.070%\n",
      "Epoch 7, Batch 489, LR 1.757351 Loss 19.066589, Accuracy 0.070%\n",
      "Epoch 7, Batch 490, LR 1.757627 Loss 19.066786, Accuracy 0.070%\n",
      "Epoch 7, Batch 491, LR 1.757902 Loss 19.066630, Accuracy 0.070%\n",
      "Epoch 7, Batch 492, LR 1.758177 Loss 19.066350, Accuracy 0.070%\n",
      "Epoch 7, Batch 493, LR 1.758453 Loss 19.066540, Accuracy 0.070%\n",
      "Epoch 7, Batch 494, LR 1.758728 Loss 19.066620, Accuracy 0.070%\n",
      "Epoch 7, Batch 495, LR 1.759003 Loss 19.066489, Accuracy 0.069%\n",
      "Epoch 7, Batch 496, LR 1.759278 Loss 19.066904, Accuracy 0.069%\n",
      "Epoch 7, Batch 497, LR 1.759553 Loss 19.066755, Accuracy 0.069%\n",
      "Epoch 7, Batch 498, LR 1.759828 Loss 19.066757, Accuracy 0.071%\n",
      "Epoch 7, Batch 499, LR 1.760103 Loss 19.066416, Accuracy 0.070%\n",
      "Epoch 7, Batch 500, LR 1.760378 Loss 19.066441, Accuracy 0.072%\n",
      "Epoch 7, Batch 501, LR 1.760653 Loss 19.066574, Accuracy 0.072%\n",
      "Epoch 7, Batch 502, LR 1.760928 Loss 19.066511, Accuracy 0.072%\n",
      "Epoch 7, Batch 503, LR 1.761203 Loss 19.066579, Accuracy 0.071%\n",
      "Epoch 7, Batch 504, LR 1.761478 Loss 19.066686, Accuracy 0.071%\n",
      "Epoch 7, Batch 505, LR 1.761753 Loss 19.066708, Accuracy 0.071%\n",
      "Epoch 7, Batch 506, LR 1.762028 Loss 19.066335, Accuracy 0.071%\n",
      "Epoch 7, Batch 507, LR 1.762303 Loss 19.066379, Accuracy 0.071%\n",
      "Epoch 7, Batch 508, LR 1.762578 Loss 19.066616, Accuracy 0.071%\n",
      "Epoch 7, Batch 509, LR 1.762852 Loss 19.066827, Accuracy 0.071%\n",
      "Epoch 7, Batch 510, LR 1.763127 Loss 19.066842, Accuracy 0.070%\n",
      "Epoch 7, Batch 511, LR 1.763402 Loss 19.066779, Accuracy 0.070%\n",
      "Epoch 7, Batch 512, LR 1.763677 Loss 19.066770, Accuracy 0.070%\n",
      "Epoch 7, Batch 513, LR 1.763951 Loss 19.066562, Accuracy 0.070%\n",
      "Epoch 7, Batch 514, LR 1.764226 Loss 19.066490, Accuracy 0.070%\n",
      "Epoch 7, Batch 515, LR 1.764501 Loss 19.066337, Accuracy 0.070%\n",
      "Epoch 7, Batch 516, LR 1.764775 Loss 19.066559, Accuracy 0.070%\n",
      "Epoch 7, Batch 517, LR 1.765050 Loss 19.066994, Accuracy 0.070%\n",
      "Epoch 7, Batch 518, LR 1.765324 Loss 19.067056, Accuracy 0.069%\n",
      "Epoch 7, Batch 519, LR 1.765599 Loss 19.066987, Accuracy 0.069%\n",
      "Epoch 7, Batch 520, LR 1.765873 Loss 19.066927, Accuracy 0.069%\n",
      "Epoch 7, Batch 521, LR 1.766148 Loss 19.066876, Accuracy 0.069%\n",
      "Epoch 7, Batch 522, LR 1.766422 Loss 19.066875, Accuracy 0.069%\n",
      "Epoch 7, Batch 523, LR 1.766697 Loss 19.066767, Accuracy 0.069%\n",
      "Epoch 7, Batch 524, LR 1.766971 Loss 19.066638, Accuracy 0.069%\n",
      "Epoch 7, Batch 525, LR 1.767245 Loss 19.066665, Accuracy 0.068%\n",
      "Epoch 7, Batch 526, LR 1.767520 Loss 19.067206, Accuracy 0.068%\n",
      "Epoch 7, Batch 527, LR 1.767794 Loss 19.066840, Accuracy 0.068%\n",
      "Epoch 7, Batch 528, LR 1.768068 Loss 19.067114, Accuracy 0.068%\n",
      "Epoch 7, Batch 529, LR 1.768342 Loss 19.067322, Accuracy 0.068%\n",
      "Epoch 7, Batch 530, LR 1.768616 Loss 19.067448, Accuracy 0.068%\n",
      "Epoch 7, Batch 531, LR 1.768891 Loss 19.067854, Accuracy 0.068%\n",
      "Epoch 7, Batch 532, LR 1.769165 Loss 19.067939, Accuracy 0.069%\n",
      "Epoch 7, Batch 533, LR 1.769439 Loss 19.068283, Accuracy 0.069%\n",
      "Epoch 7, Batch 534, LR 1.769713 Loss 19.068174, Accuracy 0.069%\n",
      "Epoch 7, Batch 535, LR 1.769987 Loss 19.068078, Accuracy 0.069%\n",
      "Epoch 7, Batch 536, LR 1.770261 Loss 19.068014, Accuracy 0.069%\n",
      "Epoch 7, Batch 537, LR 1.770535 Loss 19.067756, Accuracy 0.068%\n",
      "Epoch 7, Batch 538, LR 1.770809 Loss 19.067877, Accuracy 0.068%\n",
      "Epoch 7, Batch 539, LR 1.771083 Loss 19.067694, Accuracy 0.068%\n",
      "Epoch 7, Batch 540, LR 1.771357 Loss 19.067706, Accuracy 0.068%\n",
      "Epoch 7, Batch 541, LR 1.771631 Loss 19.067393, Accuracy 0.068%\n",
      "Epoch 7, Batch 542, LR 1.771904 Loss 19.067579, Accuracy 0.068%\n",
      "Epoch 7, Batch 543, LR 1.772178 Loss 19.067848, Accuracy 0.068%\n",
      "Epoch 7, Batch 544, LR 1.772452 Loss 19.067822, Accuracy 0.067%\n",
      "Epoch 7, Batch 545, LR 1.772726 Loss 19.067785, Accuracy 0.067%\n",
      "Epoch 7, Batch 546, LR 1.773000 Loss 19.068034, Accuracy 0.067%\n",
      "Epoch 7, Batch 547, LR 1.773273 Loss 19.068158, Accuracy 0.067%\n",
      "Epoch 7, Batch 548, LR 1.773547 Loss 19.068055, Accuracy 0.067%\n",
      "Epoch 7, Batch 549, LR 1.773821 Loss 19.068197, Accuracy 0.067%\n",
      "Epoch 7, Batch 550, LR 1.774094 Loss 19.068414, Accuracy 0.067%\n",
      "Epoch 7, Batch 551, LR 1.774368 Loss 19.068621, Accuracy 0.067%\n",
      "Epoch 7, Batch 552, LR 1.774641 Loss 19.068518, Accuracy 0.067%\n",
      "Epoch 7, Batch 553, LR 1.774915 Loss 19.068143, Accuracy 0.066%\n",
      "Epoch 7, Batch 554, LR 1.775188 Loss 19.068151, Accuracy 0.068%\n",
      "Epoch 7, Batch 555, LR 1.775462 Loss 19.068082, Accuracy 0.068%\n",
      "Epoch 7, Batch 556, LR 1.775735 Loss 19.068293, Accuracy 0.067%\n",
      "Epoch 7, Batch 557, LR 1.776009 Loss 19.068274, Accuracy 0.069%\n",
      "Epoch 7, Batch 558, LR 1.776282 Loss 19.068405, Accuracy 0.069%\n",
      "Epoch 7, Batch 559, LR 1.776555 Loss 19.068257, Accuracy 0.070%\n",
      "Epoch 7, Batch 560, LR 1.776829 Loss 19.068332, Accuracy 0.070%\n",
      "Epoch 7, Batch 561, LR 1.777102 Loss 19.068304, Accuracy 0.070%\n",
      "Epoch 7, Batch 562, LR 1.777375 Loss 19.068128, Accuracy 0.070%\n",
      "Epoch 7, Batch 563, LR 1.777649 Loss 19.068178, Accuracy 0.069%\n",
      "Epoch 7, Batch 564, LR 1.777922 Loss 19.068095, Accuracy 0.069%\n",
      "Epoch 7, Batch 565, LR 1.778195 Loss 19.067564, Accuracy 0.071%\n",
      "Epoch 7, Batch 566, LR 1.778468 Loss 19.067494, Accuracy 0.070%\n",
      "Epoch 7, Batch 567, LR 1.778741 Loss 19.067430, Accuracy 0.070%\n",
      "Epoch 7, Batch 568, LR 1.779014 Loss 19.067485, Accuracy 0.070%\n",
      "Epoch 7, Batch 569, LR 1.779287 Loss 19.067222, Accuracy 0.070%\n",
      "Epoch 7, Batch 570, LR 1.779560 Loss 19.067091, Accuracy 0.070%\n",
      "Epoch 7, Batch 571, LR 1.779833 Loss 19.066714, Accuracy 0.070%\n",
      "Epoch 7, Batch 572, LR 1.780106 Loss 19.066501, Accuracy 0.070%\n",
      "Epoch 7, Batch 573, LR 1.780379 Loss 19.066778, Accuracy 0.070%\n",
      "Epoch 7, Batch 574, LR 1.780652 Loss 19.066902, Accuracy 0.071%\n",
      "Epoch 7, Batch 575, LR 1.780925 Loss 19.066691, Accuracy 0.071%\n",
      "Epoch 7, Batch 576, LR 1.781198 Loss 19.066742, Accuracy 0.071%\n",
      "Epoch 7, Batch 577, LR 1.781471 Loss 19.067000, Accuracy 0.070%\n",
      "Epoch 7, Batch 578, LR 1.781744 Loss 19.067062, Accuracy 0.070%\n",
      "Epoch 7, Batch 579, LR 1.782016 Loss 19.067196, Accuracy 0.070%\n",
      "Epoch 7, Batch 580, LR 1.782289 Loss 19.067322, Accuracy 0.070%\n",
      "Epoch 7, Batch 581, LR 1.782562 Loss 19.067238, Accuracy 0.070%\n",
      "Epoch 7, Batch 582, LR 1.782835 Loss 19.067320, Accuracy 0.070%\n",
      "Epoch 7, Batch 583, LR 1.783107 Loss 19.067326, Accuracy 0.070%\n",
      "Epoch 7, Batch 584, LR 1.783380 Loss 19.067351, Accuracy 0.070%\n",
      "Epoch 7, Batch 585, LR 1.783652 Loss 19.067427, Accuracy 0.069%\n",
      "Epoch 7, Batch 586, LR 1.783925 Loss 19.067496, Accuracy 0.069%\n",
      "Epoch 7, Batch 587, LR 1.784198 Loss 19.067364, Accuracy 0.069%\n",
      "Epoch 7, Batch 588, LR 1.784470 Loss 19.067124, Accuracy 0.070%\n",
      "Epoch 7, Batch 589, LR 1.784743 Loss 19.066872, Accuracy 0.070%\n",
      "Epoch 7, Batch 590, LR 1.785015 Loss 19.067073, Accuracy 0.070%\n",
      "Epoch 7, Batch 591, LR 1.785287 Loss 19.067117, Accuracy 0.071%\n",
      "Epoch 7, Batch 592, LR 1.785560 Loss 19.067306, Accuracy 0.071%\n",
      "Epoch 7, Batch 593, LR 1.785832 Loss 19.067344, Accuracy 0.071%\n",
      "Epoch 7, Batch 594, LR 1.786105 Loss 19.067239, Accuracy 0.072%\n",
      "Epoch 7, Batch 595, LR 1.786377 Loss 19.067400, Accuracy 0.072%\n",
      "Epoch 7, Batch 596, LR 1.786649 Loss 19.067533, Accuracy 0.072%\n",
      "Epoch 7, Batch 597, LR 1.786921 Loss 19.067464, Accuracy 0.072%\n",
      "Epoch 7, Batch 598, LR 1.787194 Loss 19.067404, Accuracy 0.072%\n",
      "Epoch 7, Batch 599, LR 1.787466 Loss 19.067427, Accuracy 0.072%\n",
      "Epoch 7, Batch 600, LR 1.787738 Loss 19.067255, Accuracy 0.072%\n",
      "Epoch 7, Batch 601, LR 1.788010 Loss 19.067101, Accuracy 0.071%\n",
      "Epoch 7, Batch 602, LR 1.788282 Loss 19.067118, Accuracy 0.071%\n",
      "Epoch 7, Batch 603, LR 1.788554 Loss 19.067264, Accuracy 0.071%\n",
      "Epoch 7, Batch 604, LR 1.788826 Loss 19.066976, Accuracy 0.071%\n",
      "Epoch 7, Batch 605, LR 1.789098 Loss 19.067037, Accuracy 0.071%\n",
      "Epoch 7, Batch 606, LR 1.789370 Loss 19.067232, Accuracy 0.071%\n",
      "Epoch 7, Batch 607, LR 1.789642 Loss 19.066872, Accuracy 0.071%\n",
      "Epoch 7, Batch 608, LR 1.789914 Loss 19.066815, Accuracy 0.071%\n",
      "Epoch 7, Batch 609, LR 1.790186 Loss 19.066759, Accuracy 0.071%\n",
      "Epoch 7, Batch 610, LR 1.790458 Loss 19.066612, Accuracy 0.072%\n",
      "Epoch 7, Batch 611, LR 1.790730 Loss 19.066625, Accuracy 0.073%\n",
      "Epoch 7, Batch 612, LR 1.791001 Loss 19.066619, Accuracy 0.073%\n",
      "Epoch 7, Batch 613, LR 1.791273 Loss 19.066574, Accuracy 0.073%\n",
      "Epoch 7, Batch 614, LR 1.791545 Loss 19.066370, Accuracy 0.073%\n",
      "Epoch 7, Batch 615, LR 1.791817 Loss 19.066177, Accuracy 0.072%\n",
      "Epoch 7, Batch 616, LR 1.792088 Loss 19.066251, Accuracy 0.072%\n",
      "Epoch 7, Batch 617, LR 1.792360 Loss 19.066264, Accuracy 0.072%\n",
      "Epoch 7, Batch 618, LR 1.792632 Loss 19.066169, Accuracy 0.072%\n",
      "Epoch 7, Batch 619, LR 1.792903 Loss 19.066353, Accuracy 0.072%\n",
      "Epoch 7, Batch 620, LR 1.793175 Loss 19.066193, Accuracy 0.072%\n",
      "Epoch 7, Batch 621, LR 1.793446 Loss 19.066339, Accuracy 0.072%\n",
      "Epoch 7, Batch 622, LR 1.793718 Loss 19.066206, Accuracy 0.072%\n",
      "Epoch 7, Batch 623, LR 1.793989 Loss 19.066310, Accuracy 0.071%\n",
      "Epoch 7, Batch 624, LR 1.794261 Loss 19.066837, Accuracy 0.071%\n",
      "Epoch 7, Batch 625, LR 1.794532 Loss 19.066794, Accuracy 0.071%\n",
      "Epoch 7, Batch 626, LR 1.794803 Loss 19.066878, Accuracy 0.074%\n",
      "Epoch 7, Batch 627, LR 1.795075 Loss 19.066755, Accuracy 0.074%\n",
      "Epoch 7, Batch 628, LR 1.795346 Loss 19.066817, Accuracy 0.073%\n",
      "Epoch 7, Batch 629, LR 1.795617 Loss 19.066932, Accuracy 0.073%\n",
      "Epoch 7, Batch 630, LR 1.795889 Loss 19.067187, Accuracy 0.073%\n",
      "Epoch 7, Batch 631, LR 1.796160 Loss 19.067195, Accuracy 0.073%\n",
      "Epoch 7, Batch 632, LR 1.796431 Loss 19.067163, Accuracy 0.073%\n",
      "Epoch 7, Batch 633, LR 1.796702 Loss 19.067146, Accuracy 0.073%\n",
      "Epoch 7, Batch 634, LR 1.796973 Loss 19.067377, Accuracy 0.073%\n",
      "Epoch 7, Batch 635, LR 1.797244 Loss 19.067510, Accuracy 0.073%\n",
      "Epoch 7, Batch 636, LR 1.797515 Loss 19.067479, Accuracy 0.072%\n",
      "Epoch 7, Batch 637, LR 1.797786 Loss 19.067358, Accuracy 0.074%\n",
      "Epoch 7, Batch 638, LR 1.798058 Loss 19.067282, Accuracy 0.073%\n",
      "Epoch 7, Batch 639, LR 1.798328 Loss 19.067022, Accuracy 0.075%\n",
      "Epoch 7, Batch 640, LR 1.798599 Loss 19.066890, Accuracy 0.074%\n",
      "Epoch 7, Batch 641, LR 1.798870 Loss 19.066757, Accuracy 0.074%\n",
      "Epoch 7, Batch 642, LR 1.799141 Loss 19.066887, Accuracy 0.074%\n",
      "Epoch 7, Batch 643, LR 1.799412 Loss 19.067138, Accuracy 0.074%\n",
      "Epoch 7, Batch 644, LR 1.799683 Loss 19.067058, Accuracy 0.074%\n",
      "Epoch 7, Batch 645, LR 1.799954 Loss 19.067284, Accuracy 0.074%\n",
      "Epoch 7, Batch 646, LR 1.800225 Loss 19.067239, Accuracy 0.074%\n",
      "Epoch 7, Batch 647, LR 1.800495 Loss 19.067305, Accuracy 0.075%\n",
      "Epoch 7, Batch 648, LR 1.800766 Loss 19.067110, Accuracy 0.075%\n",
      "Epoch 7, Batch 649, LR 1.801037 Loss 19.067235, Accuracy 0.075%\n",
      "Epoch 7, Batch 650, LR 1.801307 Loss 19.066992, Accuracy 0.075%\n",
      "Epoch 7, Batch 651, LR 1.801578 Loss 19.066916, Accuracy 0.074%\n",
      "Epoch 7, Batch 652, LR 1.801848 Loss 19.067028, Accuracy 0.074%\n",
      "Epoch 7, Batch 653, LR 1.802119 Loss 19.066998, Accuracy 0.074%\n",
      "Epoch 7, Batch 654, LR 1.802390 Loss 19.067105, Accuracy 0.074%\n",
      "Epoch 7, Batch 655, LR 1.802660 Loss 19.066951, Accuracy 0.074%\n",
      "Epoch 7, Batch 656, LR 1.802930 Loss 19.067001, Accuracy 0.075%\n",
      "Epoch 7, Batch 657, LR 1.803201 Loss 19.066947, Accuracy 0.075%\n",
      "Epoch 7, Batch 658, LR 1.803471 Loss 19.066484, Accuracy 0.075%\n",
      "Epoch 7, Batch 659, LR 1.803742 Loss 19.066319, Accuracy 0.075%\n",
      "Epoch 7, Batch 660, LR 1.804012 Loss 19.066435, Accuracy 0.075%\n",
      "Epoch 7, Batch 661, LR 1.804282 Loss 19.066564, Accuracy 0.074%\n",
      "Epoch 7, Batch 662, LR 1.804553 Loss 19.066557, Accuracy 0.074%\n",
      "Epoch 7, Batch 663, LR 1.804823 Loss 19.066442, Accuracy 0.077%\n",
      "Epoch 7, Batch 664, LR 1.805093 Loss 19.066482, Accuracy 0.076%\n",
      "Epoch 7, Batch 665, LR 1.805363 Loss 19.066771, Accuracy 0.076%\n",
      "Epoch 7, Batch 666, LR 1.805633 Loss 19.066827, Accuracy 0.076%\n",
      "Epoch 7, Batch 667, LR 1.805904 Loss 19.066991, Accuracy 0.076%\n",
      "Epoch 7, Batch 668, LR 1.806174 Loss 19.066675, Accuracy 0.076%\n",
      "Epoch 7, Batch 669, LR 1.806444 Loss 19.066986, Accuracy 0.077%\n",
      "Epoch 7, Batch 670, LR 1.806714 Loss 19.067078, Accuracy 0.078%\n",
      "Epoch 7, Batch 671, LR 1.806984 Loss 19.066990, Accuracy 0.078%\n",
      "Epoch 7, Batch 672, LR 1.807254 Loss 19.067188, Accuracy 0.078%\n",
      "Epoch 7, Batch 673, LR 1.807524 Loss 19.067294, Accuracy 0.078%\n",
      "Epoch 7, Batch 674, LR 1.807794 Loss 19.067153, Accuracy 0.078%\n",
      "Epoch 7, Batch 675, LR 1.808063 Loss 19.067281, Accuracy 0.078%\n",
      "Epoch 7, Batch 676, LR 1.808333 Loss 19.067136, Accuracy 0.077%\n",
      "Epoch 7, Batch 677, LR 1.808603 Loss 19.067229, Accuracy 0.077%\n",
      "Epoch 7, Batch 678, LR 1.808873 Loss 19.067032, Accuracy 0.078%\n",
      "Epoch 7, Batch 679, LR 1.809143 Loss 19.067238, Accuracy 0.078%\n",
      "Epoch 7, Batch 680, LR 1.809412 Loss 19.067108, Accuracy 0.078%\n",
      "Epoch 7, Batch 681, LR 1.809682 Loss 19.067076, Accuracy 0.078%\n",
      "Epoch 7, Batch 682, LR 1.809952 Loss 19.067302, Accuracy 0.078%\n",
      "Epoch 7, Batch 683, LR 1.810221 Loss 19.067343, Accuracy 0.078%\n",
      "Epoch 7, Batch 684, LR 1.810491 Loss 19.067049, Accuracy 0.078%\n",
      "Epoch 7, Batch 685, LR 1.810760 Loss 19.066928, Accuracy 0.078%\n",
      "Epoch 7, Batch 686, LR 1.811030 Loss 19.067068, Accuracy 0.077%\n",
      "Epoch 7, Batch 687, LR 1.811300 Loss 19.066897, Accuracy 0.077%\n",
      "Epoch 7, Batch 688, LR 1.811569 Loss 19.066639, Accuracy 0.077%\n",
      "Epoch 7, Batch 689, LR 1.811838 Loss 19.066767, Accuracy 0.077%\n",
      "Epoch 7, Batch 690, LR 1.812108 Loss 19.066598, Accuracy 0.077%\n",
      "Epoch 7, Batch 691, LR 1.812377 Loss 19.066248, Accuracy 0.077%\n",
      "Epoch 7, Batch 692, LR 1.812647 Loss 19.066195, Accuracy 0.077%\n",
      "Epoch 7, Batch 693, LR 1.812916 Loss 19.066227, Accuracy 0.077%\n",
      "Epoch 7, Batch 694, LR 1.813185 Loss 19.066375, Accuracy 0.077%\n",
      "Epoch 7, Batch 695, LR 1.813454 Loss 19.066377, Accuracy 0.078%\n",
      "Epoch 7, Batch 696, LR 1.813724 Loss 19.066341, Accuracy 0.077%\n",
      "Epoch 7, Batch 697, LR 1.813993 Loss 19.066330, Accuracy 0.077%\n",
      "Epoch 7, Batch 698, LR 1.814262 Loss 19.066361, Accuracy 0.077%\n",
      "Epoch 7, Batch 699, LR 1.814531 Loss 19.066343, Accuracy 0.077%\n",
      "Epoch 7, Batch 700, LR 1.814800 Loss 19.066267, Accuracy 0.077%\n",
      "Epoch 7, Batch 701, LR 1.815069 Loss 19.066420, Accuracy 0.077%\n",
      "Epoch 7, Batch 702, LR 1.815338 Loss 19.066662, Accuracy 0.077%\n",
      "Epoch 7, Batch 703, LR 1.815607 Loss 19.066579, Accuracy 0.077%\n",
      "Epoch 7, Batch 704, LR 1.815876 Loss 19.066625, Accuracy 0.077%\n",
      "Epoch 7, Batch 705, LR 1.816145 Loss 19.066838, Accuracy 0.076%\n",
      "Epoch 7, Batch 706, LR 1.816414 Loss 19.066967, Accuracy 0.076%\n",
      "Epoch 7, Batch 707, LR 1.816683 Loss 19.066990, Accuracy 0.076%\n",
      "Epoch 7, Batch 708, LR 1.816952 Loss 19.066981, Accuracy 0.076%\n",
      "Epoch 7, Batch 709, LR 1.817221 Loss 19.066983, Accuracy 0.076%\n",
      "Epoch 7, Batch 710, LR 1.817489 Loss 19.067270, Accuracy 0.076%\n",
      "Epoch 7, Batch 711, LR 1.817758 Loss 19.067192, Accuracy 0.076%\n",
      "Epoch 7, Batch 712, LR 1.818027 Loss 19.067151, Accuracy 0.076%\n",
      "Epoch 7, Batch 713, LR 1.818296 Loss 19.066935, Accuracy 0.076%\n",
      "Epoch 7, Batch 714, LR 1.818564 Loss 19.067109, Accuracy 0.075%\n",
      "Epoch 7, Batch 715, LR 1.818833 Loss 19.067254, Accuracy 0.075%\n",
      "Epoch 7, Batch 716, LR 1.819101 Loss 19.067400, Accuracy 0.075%\n",
      "Epoch 7, Batch 717, LR 1.819370 Loss 19.067199, Accuracy 0.075%\n",
      "Epoch 7, Batch 718, LR 1.819639 Loss 19.067493, Accuracy 0.075%\n",
      "Epoch 7, Batch 719, LR 1.819907 Loss 19.067499, Accuracy 0.075%\n",
      "Epoch 7, Batch 720, LR 1.820176 Loss 19.067413, Accuracy 0.075%\n",
      "Epoch 7, Batch 721, LR 1.820444 Loss 19.067566, Accuracy 0.075%\n",
      "Epoch 7, Batch 722, LR 1.820712 Loss 19.067653, Accuracy 0.075%\n",
      "Epoch 7, Batch 723, LR 1.820981 Loss 19.067667, Accuracy 0.075%\n",
      "Epoch 7, Batch 724, LR 1.821249 Loss 19.067633, Accuracy 0.076%\n",
      "Epoch 7, Batch 725, LR 1.821517 Loss 19.067788, Accuracy 0.075%\n",
      "Epoch 7, Batch 726, LR 1.821786 Loss 19.067655, Accuracy 0.075%\n",
      "Epoch 7, Batch 727, LR 1.822054 Loss 19.067721, Accuracy 0.075%\n",
      "Epoch 7, Batch 728, LR 1.822322 Loss 19.067892, Accuracy 0.075%\n",
      "Epoch 7, Batch 729, LR 1.822590 Loss 19.067701, Accuracy 0.075%\n",
      "Epoch 7, Batch 730, LR 1.822858 Loss 19.067774, Accuracy 0.075%\n",
      "Epoch 7, Batch 731, LR 1.823127 Loss 19.067385, Accuracy 0.075%\n",
      "Epoch 7, Batch 732, LR 1.823395 Loss 19.067458, Accuracy 0.075%\n",
      "Epoch 7, Batch 733, LR 1.823663 Loss 19.067304, Accuracy 0.075%\n",
      "Epoch 7, Batch 734, LR 1.823931 Loss 19.067563, Accuracy 0.075%\n",
      "Epoch 7, Batch 735, LR 1.824199 Loss 19.067632, Accuracy 0.074%\n",
      "Epoch 7, Batch 736, LR 1.824467 Loss 19.067833, Accuracy 0.074%\n",
      "Epoch 7, Batch 737, LR 1.824735 Loss 19.068017, Accuracy 0.074%\n",
      "Epoch 7, Batch 738, LR 1.825002 Loss 19.068144, Accuracy 0.074%\n",
      "Epoch 7, Batch 739, LR 1.825270 Loss 19.068166, Accuracy 0.074%\n",
      "Epoch 7, Batch 740, LR 1.825538 Loss 19.068411, Accuracy 0.074%\n",
      "Epoch 7, Batch 741, LR 1.825806 Loss 19.068105, Accuracy 0.074%\n",
      "Epoch 7, Batch 742, LR 1.826074 Loss 19.068143, Accuracy 0.074%\n",
      "Epoch 7, Batch 743, LR 1.826341 Loss 19.068292, Accuracy 0.074%\n",
      "Epoch 7, Batch 744, LR 1.826609 Loss 19.068097, Accuracy 0.074%\n",
      "Epoch 7, Batch 745, LR 1.826877 Loss 19.067932, Accuracy 0.074%\n",
      "Epoch 7, Batch 746, LR 1.827144 Loss 19.067866, Accuracy 0.074%\n",
      "Epoch 7, Batch 747, LR 1.827412 Loss 19.067771, Accuracy 0.074%\n",
      "Epoch 7, Batch 748, LR 1.827680 Loss 19.067547, Accuracy 0.074%\n",
      "Epoch 7, Batch 749, LR 1.827947 Loss 19.067321, Accuracy 0.074%\n",
      "Epoch 7, Batch 750, LR 1.828215 Loss 19.067270, Accuracy 0.074%\n",
      "Epoch 7, Batch 751, LR 1.828482 Loss 19.067353, Accuracy 0.074%\n",
      "Epoch 7, Batch 752, LR 1.828750 Loss 19.067723, Accuracy 0.074%\n",
      "Epoch 7, Batch 753, LR 1.829017 Loss 19.067751, Accuracy 0.074%\n",
      "Epoch 7, Batch 754, LR 1.829284 Loss 19.067734, Accuracy 0.074%\n",
      "Epoch 7, Batch 755, LR 1.829552 Loss 19.067990, Accuracy 0.073%\n",
      "Epoch 7, Batch 756, LR 1.829819 Loss 19.068031, Accuracy 0.073%\n",
      "Epoch 7, Batch 757, LR 1.830086 Loss 19.068257, Accuracy 0.073%\n",
      "Epoch 7, Batch 758, LR 1.830354 Loss 19.067910, Accuracy 0.073%\n",
      "Epoch 7, Batch 759, LR 1.830621 Loss 19.067887, Accuracy 0.073%\n",
      "Epoch 7, Batch 760, LR 1.830888 Loss 19.067763, Accuracy 0.073%\n",
      "Epoch 7, Batch 761, LR 1.831155 Loss 19.067641, Accuracy 0.073%\n",
      "Epoch 7, Batch 762, LR 1.831422 Loss 19.067806, Accuracy 0.073%\n",
      "Epoch 7, Batch 763, LR 1.831689 Loss 19.067596, Accuracy 0.073%\n",
      "Epoch 7, Batch 764, LR 1.831956 Loss 19.067582, Accuracy 0.073%\n",
      "Epoch 7, Batch 765, LR 1.832223 Loss 19.067623, Accuracy 0.073%\n",
      "Epoch 7, Batch 766, LR 1.832490 Loss 19.067405, Accuracy 0.072%\n",
      "Epoch 7, Batch 767, LR 1.832757 Loss 19.067340, Accuracy 0.072%\n",
      "Epoch 7, Batch 768, LR 1.833024 Loss 19.067609, Accuracy 0.072%\n",
      "Epoch 7, Batch 769, LR 1.833291 Loss 19.067660, Accuracy 0.072%\n",
      "Epoch 7, Batch 770, LR 1.833558 Loss 19.067507, Accuracy 0.072%\n",
      "Epoch 7, Batch 771, LR 1.833825 Loss 19.067546, Accuracy 0.072%\n",
      "Epoch 7, Batch 772, LR 1.834092 Loss 19.067560, Accuracy 0.072%\n",
      "Epoch 7, Batch 773, LR 1.834358 Loss 19.067591, Accuracy 0.072%\n",
      "Epoch 7, Batch 774, LR 1.834625 Loss 19.067497, Accuracy 0.072%\n",
      "Epoch 7, Batch 775, LR 1.834892 Loss 19.067514, Accuracy 0.072%\n",
      "Epoch 7, Batch 776, LR 1.835158 Loss 19.067163, Accuracy 0.072%\n",
      "Epoch 7, Batch 777, LR 1.835425 Loss 19.067331, Accuracy 0.072%\n",
      "Epoch 7, Batch 778, LR 1.835692 Loss 19.067465, Accuracy 0.073%\n",
      "Epoch 7, Batch 779, LR 1.835958 Loss 19.067533, Accuracy 0.073%\n",
      "Epoch 7, Batch 780, LR 1.836225 Loss 19.067655, Accuracy 0.073%\n",
      "Epoch 7, Batch 781, LR 1.836491 Loss 19.067828, Accuracy 0.073%\n",
      "Epoch 7, Batch 782, LR 1.836758 Loss 19.067997, Accuracy 0.073%\n",
      "Epoch 7, Batch 783, LR 1.837024 Loss 19.068053, Accuracy 0.073%\n",
      "Epoch 7, Batch 784, LR 1.837291 Loss 19.068183, Accuracy 0.073%\n",
      "Epoch 7, Batch 785, LR 1.837557 Loss 19.068271, Accuracy 0.073%\n",
      "Epoch 7, Batch 786, LR 1.837823 Loss 19.068116, Accuracy 0.073%\n",
      "Epoch 7, Batch 787, LR 1.838090 Loss 19.067995, Accuracy 0.072%\n",
      "Epoch 7, Batch 788, LR 1.838356 Loss 19.068222, Accuracy 0.072%\n",
      "Epoch 7, Batch 789, LR 1.838622 Loss 19.068520, Accuracy 0.072%\n",
      "Epoch 7, Batch 790, LR 1.838888 Loss 19.068512, Accuracy 0.072%\n",
      "Epoch 7, Batch 791, LR 1.839154 Loss 19.068346, Accuracy 0.072%\n",
      "Epoch 7, Batch 792, LR 1.839421 Loss 19.068033, Accuracy 0.073%\n",
      "Epoch 7, Batch 793, LR 1.839687 Loss 19.068199, Accuracy 0.073%\n",
      "Epoch 7, Batch 794, LR 1.839953 Loss 19.068245, Accuracy 0.073%\n",
      "Epoch 7, Batch 795, LR 1.840219 Loss 19.068298, Accuracy 0.073%\n",
      "Epoch 7, Batch 796, LR 1.840485 Loss 19.068127, Accuracy 0.073%\n",
      "Epoch 7, Batch 797, LR 1.840751 Loss 19.068174, Accuracy 0.073%\n",
      "Epoch 7, Batch 798, LR 1.841017 Loss 19.068359, Accuracy 0.073%\n",
      "Epoch 7, Batch 799, LR 1.841283 Loss 19.068463, Accuracy 0.073%\n",
      "Epoch 7, Batch 800, LR 1.841548 Loss 19.068479, Accuracy 0.073%\n",
      "Epoch 7, Batch 801, LR 1.841814 Loss 19.068499, Accuracy 0.073%\n",
      "Epoch 7, Batch 802, LR 1.842080 Loss 19.068680, Accuracy 0.073%\n",
      "Epoch 7, Batch 803, LR 1.842346 Loss 19.068524, Accuracy 0.073%\n",
      "Epoch 7, Batch 804, LR 1.842612 Loss 19.068332, Accuracy 0.073%\n",
      "Epoch 7, Batch 805, LR 1.842877 Loss 19.068227, Accuracy 0.073%\n",
      "Epoch 7, Batch 806, LR 1.843143 Loss 19.068267, Accuracy 0.073%\n",
      "Epoch 7, Batch 807, LR 1.843409 Loss 19.067979, Accuracy 0.074%\n",
      "Epoch 7, Batch 808, LR 1.843674 Loss 19.068092, Accuracy 0.073%\n",
      "Epoch 7, Batch 809, LR 1.843940 Loss 19.067968, Accuracy 0.073%\n",
      "Epoch 7, Batch 810, LR 1.844205 Loss 19.067889, Accuracy 0.073%\n",
      "Epoch 7, Batch 811, LR 1.844471 Loss 19.068025, Accuracy 0.073%\n",
      "Epoch 7, Batch 812, LR 1.844736 Loss 19.068135, Accuracy 0.073%\n",
      "Epoch 7, Batch 813, LR 1.845002 Loss 19.068115, Accuracy 0.073%\n",
      "Epoch 7, Batch 814, LR 1.845267 Loss 19.068228, Accuracy 0.073%\n",
      "Epoch 7, Batch 815, LR 1.845533 Loss 19.068424, Accuracy 0.073%\n",
      "Epoch 7, Batch 816, LR 1.845798 Loss 19.068206, Accuracy 0.073%\n",
      "Epoch 7, Batch 817, LR 1.846063 Loss 19.067854, Accuracy 0.073%\n",
      "Epoch 7, Batch 818, LR 1.846328 Loss 19.067614, Accuracy 0.073%\n",
      "Epoch 7, Batch 819, LR 1.846594 Loss 19.067625, Accuracy 0.072%\n",
      "Epoch 7, Batch 820, LR 1.846859 Loss 19.067729, Accuracy 0.072%\n",
      "Epoch 7, Batch 821, LR 1.847124 Loss 19.067882, Accuracy 0.072%\n",
      "Epoch 7, Batch 822, LR 1.847389 Loss 19.067971, Accuracy 0.072%\n",
      "Epoch 7, Batch 823, LR 1.847654 Loss 19.067953, Accuracy 0.072%\n",
      "Epoch 7, Batch 824, LR 1.847919 Loss 19.067687, Accuracy 0.072%\n",
      "Epoch 7, Batch 825, LR 1.848184 Loss 19.067684, Accuracy 0.072%\n",
      "Epoch 7, Batch 826, LR 1.848449 Loss 19.067863, Accuracy 0.072%\n",
      "Epoch 7, Batch 827, LR 1.848714 Loss 19.067971, Accuracy 0.072%\n",
      "Epoch 7, Batch 828, LR 1.848979 Loss 19.068205, Accuracy 0.072%\n",
      "Epoch 7, Batch 829, LR 1.849244 Loss 19.068250, Accuracy 0.073%\n",
      "Epoch 7, Batch 830, LR 1.849509 Loss 19.068205, Accuracy 0.072%\n",
      "Epoch 7, Batch 831, LR 1.849774 Loss 19.068460, Accuracy 0.072%\n",
      "Epoch 7, Batch 832, LR 1.850039 Loss 19.068429, Accuracy 0.072%\n",
      "Epoch 7, Batch 833, LR 1.850304 Loss 19.068348, Accuracy 0.072%\n",
      "Epoch 7, Batch 834, LR 1.850568 Loss 19.068427, Accuracy 0.073%\n",
      "Epoch 7, Batch 835, LR 1.850833 Loss 19.068286, Accuracy 0.073%\n",
      "Epoch 7, Batch 836, LR 1.851098 Loss 19.068525, Accuracy 0.073%\n",
      "Epoch 7, Batch 837, LR 1.851362 Loss 19.068848, Accuracy 0.074%\n",
      "Epoch 7, Batch 838, LR 1.851627 Loss 19.068874, Accuracy 0.074%\n",
      "Epoch 7, Batch 839, LR 1.851892 Loss 19.068754, Accuracy 0.074%\n",
      "Epoch 7, Batch 840, LR 1.852156 Loss 19.068823, Accuracy 0.073%\n",
      "Epoch 7, Batch 841, LR 1.852421 Loss 19.068756, Accuracy 0.073%\n",
      "Epoch 7, Batch 842, LR 1.852685 Loss 19.068797, Accuracy 0.073%\n",
      "Epoch 7, Batch 843, LR 1.852949 Loss 19.069166, Accuracy 0.073%\n",
      "Epoch 7, Batch 844, LR 1.853214 Loss 19.069295, Accuracy 0.073%\n",
      "Epoch 7, Batch 845, LR 1.853478 Loss 19.069226, Accuracy 0.073%\n",
      "Epoch 7, Batch 846, LR 1.853743 Loss 19.069001, Accuracy 0.073%\n",
      "Epoch 7, Batch 847, LR 1.854007 Loss 19.068800, Accuracy 0.073%\n",
      "Epoch 7, Batch 848, LR 1.854271 Loss 19.068565, Accuracy 0.073%\n",
      "Epoch 7, Batch 849, LR 1.854535 Loss 19.068516, Accuracy 0.074%\n",
      "Epoch 7, Batch 850, LR 1.854800 Loss 19.068364, Accuracy 0.074%\n",
      "Epoch 7, Batch 851, LR 1.855064 Loss 19.068355, Accuracy 0.074%\n",
      "Epoch 7, Batch 852, LR 1.855328 Loss 19.068353, Accuracy 0.074%\n",
      "Epoch 7, Batch 853, LR 1.855592 Loss 19.068496, Accuracy 0.074%\n",
      "Epoch 7, Batch 854, LR 1.855856 Loss 19.068326, Accuracy 0.074%\n",
      "Epoch 7, Batch 855, LR 1.856120 Loss 19.068109, Accuracy 0.075%\n",
      "Epoch 7, Batch 856, LR 1.856384 Loss 19.068186, Accuracy 0.075%\n",
      "Epoch 7, Batch 857, LR 1.856648 Loss 19.068212, Accuracy 0.076%\n",
      "Epoch 7, Batch 858, LR 1.856912 Loss 19.068268, Accuracy 0.076%\n",
      "Epoch 7, Batch 859, LR 1.857176 Loss 19.068235, Accuracy 0.075%\n",
      "Epoch 7, Batch 860, LR 1.857440 Loss 19.068059, Accuracy 0.075%\n",
      "Epoch 7, Batch 861, LR 1.857703 Loss 19.068200, Accuracy 0.075%\n",
      "Epoch 7, Batch 862, LR 1.857967 Loss 19.067957, Accuracy 0.075%\n",
      "Epoch 7, Batch 863, LR 1.858231 Loss 19.067885, Accuracy 0.075%\n",
      "Epoch 7, Batch 864, LR 1.858495 Loss 19.067763, Accuracy 0.075%\n",
      "Epoch 7, Batch 865, LR 1.858758 Loss 19.067762, Accuracy 0.075%\n",
      "Epoch 7, Batch 866, LR 1.859022 Loss 19.067776, Accuracy 0.075%\n",
      "Epoch 7, Batch 867, LR 1.859286 Loss 19.068016, Accuracy 0.075%\n",
      "Epoch 7, Batch 868, LR 1.859549 Loss 19.068060, Accuracy 0.075%\n",
      "Epoch 7, Batch 869, LR 1.859813 Loss 19.068075, Accuracy 0.075%\n",
      "Epoch 7, Batch 870, LR 1.860076 Loss 19.068110, Accuracy 0.075%\n",
      "Epoch 7, Batch 871, LR 1.860340 Loss 19.068203, Accuracy 0.075%\n",
      "Epoch 7, Batch 872, LR 1.860603 Loss 19.068401, Accuracy 0.075%\n",
      "Epoch 7, Batch 873, LR 1.860867 Loss 19.068353, Accuracy 0.075%\n",
      "Epoch 7, Batch 874, LR 1.861130 Loss 19.068391, Accuracy 0.075%\n",
      "Epoch 7, Batch 875, LR 1.861393 Loss 19.068390, Accuracy 0.075%\n",
      "Epoch 7, Batch 876, LR 1.861657 Loss 19.068522, Accuracy 0.075%\n",
      "Epoch 7, Batch 877, LR 1.861920 Loss 19.068602, Accuracy 0.075%\n",
      "Epoch 7, Batch 878, LR 1.862183 Loss 19.068593, Accuracy 0.075%\n",
      "Epoch 7, Batch 879, LR 1.862446 Loss 19.068546, Accuracy 0.075%\n",
      "Epoch 7, Batch 880, LR 1.862710 Loss 19.068496, Accuracy 0.075%\n",
      "Epoch 7, Batch 881, LR 1.862973 Loss 19.068479, Accuracy 0.074%\n",
      "Epoch 7, Batch 882, LR 1.863236 Loss 19.068346, Accuracy 0.074%\n",
      "Epoch 7, Batch 883, LR 1.863499 Loss 19.068267, Accuracy 0.074%\n",
      "Epoch 7, Batch 884, LR 1.863762 Loss 19.068095, Accuracy 0.074%\n",
      "Epoch 7, Batch 885, LR 1.864025 Loss 19.067996, Accuracy 0.074%\n",
      "Epoch 7, Batch 886, LR 1.864288 Loss 19.068041, Accuracy 0.074%\n",
      "Epoch 7, Batch 887, LR 1.864551 Loss 19.067888, Accuracy 0.074%\n",
      "Epoch 7, Batch 888, LR 1.864814 Loss 19.067833, Accuracy 0.074%\n",
      "Epoch 7, Batch 889, LR 1.865077 Loss 19.067770, Accuracy 0.074%\n",
      "Epoch 7, Batch 890, LR 1.865339 Loss 19.067800, Accuracy 0.074%\n",
      "Epoch 7, Batch 891, LR 1.865602 Loss 19.067835, Accuracy 0.074%\n",
      "Epoch 7, Batch 892, LR 1.865865 Loss 19.067713, Accuracy 0.074%\n",
      "Epoch 7, Batch 893, LR 1.866128 Loss 19.067724, Accuracy 0.073%\n",
      "Epoch 7, Batch 894, LR 1.866390 Loss 19.067500, Accuracy 0.074%\n",
      "Epoch 7, Batch 895, LR 1.866653 Loss 19.067450, Accuracy 0.074%\n",
      "Epoch 7, Batch 896, LR 1.866916 Loss 19.067471, Accuracy 0.074%\n",
      "Epoch 7, Batch 897, LR 1.867178 Loss 19.067424, Accuracy 0.074%\n",
      "Epoch 7, Batch 898, LR 1.867441 Loss 19.067305, Accuracy 0.074%\n",
      "Epoch 7, Batch 899, LR 1.867703 Loss 19.067287, Accuracy 0.074%\n",
      "Epoch 7, Batch 900, LR 1.867966 Loss 19.067090, Accuracy 0.074%\n",
      "Epoch 7, Batch 901, LR 1.868228 Loss 19.067360, Accuracy 0.074%\n",
      "Epoch 7, Batch 902, LR 1.868491 Loss 19.067292, Accuracy 0.074%\n",
      "Epoch 7, Batch 903, LR 1.868753 Loss 19.067504, Accuracy 0.074%\n",
      "Epoch 7, Batch 904, LR 1.869015 Loss 19.067502, Accuracy 0.074%\n",
      "Epoch 7, Batch 905, LR 1.869278 Loss 19.067424, Accuracy 0.074%\n",
      "Epoch 7, Batch 906, LR 1.869540 Loss 19.067404, Accuracy 0.074%\n",
      "Epoch 7, Batch 907, LR 1.869802 Loss 19.067750, Accuracy 0.074%\n",
      "Epoch 7, Batch 908, LR 1.870064 Loss 19.067748, Accuracy 0.074%\n",
      "Epoch 7, Batch 909, LR 1.870327 Loss 19.067674, Accuracy 0.074%\n",
      "Epoch 7, Batch 910, LR 1.870589 Loss 19.067813, Accuracy 0.074%\n",
      "Epoch 7, Batch 911, LR 1.870851 Loss 19.067933, Accuracy 0.074%\n",
      "Epoch 7, Batch 912, LR 1.871113 Loss 19.067766, Accuracy 0.074%\n",
      "Epoch 7, Batch 913, LR 1.871375 Loss 19.067801, Accuracy 0.074%\n",
      "Epoch 7, Batch 914, LR 1.871637 Loss 19.067832, Accuracy 0.074%\n",
      "Epoch 7, Batch 915, LR 1.871899 Loss 19.067794, Accuracy 0.073%\n",
      "Epoch 7, Batch 916, LR 1.872161 Loss 19.067648, Accuracy 0.073%\n",
      "Epoch 7, Batch 917, LR 1.872423 Loss 19.067729, Accuracy 0.073%\n",
      "Epoch 7, Batch 918, LR 1.872685 Loss 19.067666, Accuracy 0.073%\n",
      "Epoch 7, Batch 919, LR 1.872946 Loss 19.067709, Accuracy 0.073%\n",
      "Epoch 7, Batch 920, LR 1.873208 Loss 19.067869, Accuracy 0.073%\n",
      "Epoch 7, Batch 921, LR 1.873470 Loss 19.067952, Accuracy 0.073%\n",
      "Epoch 7, Batch 922, LR 1.873732 Loss 19.067900, Accuracy 0.073%\n",
      "Epoch 7, Batch 923, LR 1.873993 Loss 19.067910, Accuracy 0.073%\n",
      "Epoch 7, Batch 924, LR 1.874255 Loss 19.067728, Accuracy 0.073%\n",
      "Epoch 7, Batch 925, LR 1.874517 Loss 19.067690, Accuracy 0.073%\n",
      "Epoch 7, Batch 926, LR 1.874778 Loss 19.067749, Accuracy 0.073%\n",
      "Epoch 7, Batch 927, LR 1.875040 Loss 19.067771, Accuracy 0.072%\n",
      "Epoch 7, Batch 928, LR 1.875301 Loss 19.067542, Accuracy 0.074%\n",
      "Epoch 7, Batch 929, LR 1.875563 Loss 19.067680, Accuracy 0.074%\n",
      "Epoch 7, Batch 930, LR 1.875824 Loss 19.067619, Accuracy 0.074%\n",
      "Epoch 7, Batch 931, LR 1.876086 Loss 19.067378, Accuracy 0.074%\n",
      "Epoch 7, Batch 932, LR 1.876347 Loss 19.067432, Accuracy 0.074%\n",
      "Epoch 7, Batch 933, LR 1.876608 Loss 19.067632, Accuracy 0.075%\n",
      "Epoch 7, Batch 934, LR 1.876870 Loss 19.067832, Accuracy 0.074%\n",
      "Epoch 7, Batch 935, LR 1.877131 Loss 19.068002, Accuracy 0.074%\n",
      "Epoch 7, Batch 936, LR 1.877392 Loss 19.068203, Accuracy 0.074%\n",
      "Epoch 7, Batch 937, LR 1.877653 Loss 19.068133, Accuracy 0.074%\n",
      "Epoch 7, Batch 938, LR 1.877914 Loss 19.068197, Accuracy 0.074%\n",
      "Epoch 7, Batch 939, LR 1.878175 Loss 19.068094, Accuracy 0.074%\n",
      "Epoch 7, Batch 940, LR 1.878437 Loss 19.068207, Accuracy 0.074%\n",
      "Epoch 7, Batch 941, LR 1.878698 Loss 19.068156, Accuracy 0.074%\n",
      "Epoch 7, Batch 942, LR 1.878959 Loss 19.068041, Accuracy 0.074%\n",
      "Epoch 7, Batch 943, LR 1.879220 Loss 19.068181, Accuracy 0.074%\n",
      "Epoch 7, Batch 944, LR 1.879480 Loss 19.068099, Accuracy 0.074%\n",
      "Epoch 7, Batch 945, LR 1.879741 Loss 19.068032, Accuracy 0.074%\n",
      "Epoch 7, Batch 946, LR 1.880002 Loss 19.068079, Accuracy 0.074%\n",
      "Epoch 7, Batch 947, LR 1.880263 Loss 19.068086, Accuracy 0.073%\n",
      "Epoch 7, Batch 948, LR 1.880524 Loss 19.068083, Accuracy 0.073%\n",
      "Epoch 7, Batch 949, LR 1.880785 Loss 19.068174, Accuracy 0.073%\n",
      "Epoch 7, Batch 950, LR 1.881045 Loss 19.068167, Accuracy 0.073%\n",
      "Epoch 7, Batch 951, LR 1.881306 Loss 19.068309, Accuracy 0.073%\n",
      "Epoch 7, Batch 952, LR 1.881567 Loss 19.068453, Accuracy 0.073%\n",
      "Epoch 7, Batch 953, LR 1.881827 Loss 19.068371, Accuracy 0.073%\n",
      "Epoch 7, Batch 954, LR 1.882088 Loss 19.068536, Accuracy 0.073%\n",
      "Epoch 7, Batch 955, LR 1.882348 Loss 19.068499, Accuracy 0.073%\n",
      "Epoch 7, Batch 956, LR 1.882609 Loss 19.068649, Accuracy 0.073%\n",
      "Epoch 7, Batch 957, LR 1.882869 Loss 19.068759, Accuracy 0.073%\n",
      "Epoch 7, Batch 958, LR 1.883130 Loss 19.068736, Accuracy 0.073%\n",
      "Epoch 7, Batch 959, LR 1.883390 Loss 19.068757, Accuracy 0.073%\n",
      "Epoch 7, Batch 960, LR 1.883651 Loss 19.068650, Accuracy 0.072%\n",
      "Epoch 7, Batch 961, LR 1.883911 Loss 19.068630, Accuracy 0.072%\n",
      "Epoch 7, Batch 962, LR 1.884171 Loss 19.068732, Accuracy 0.072%\n",
      "Epoch 7, Batch 963, LR 1.884432 Loss 19.068481, Accuracy 0.072%\n",
      "Epoch 7, Batch 964, LR 1.884692 Loss 19.068556, Accuracy 0.072%\n",
      "Epoch 7, Batch 965, LR 1.884952 Loss 19.068648, Accuracy 0.072%\n",
      "Epoch 7, Batch 966, LR 1.885212 Loss 19.068621, Accuracy 0.072%\n",
      "Epoch 7, Batch 967, LR 1.885472 Loss 19.068657, Accuracy 0.072%\n",
      "Epoch 7, Batch 968, LR 1.885732 Loss 19.068619, Accuracy 0.073%\n",
      "Epoch 7, Batch 969, LR 1.885992 Loss 19.068520, Accuracy 0.073%\n",
      "Epoch 7, Batch 970, LR 1.886252 Loss 19.068482, Accuracy 0.072%\n",
      "Epoch 7, Batch 971, LR 1.886512 Loss 19.068304, Accuracy 0.072%\n",
      "Epoch 7, Batch 972, LR 1.886772 Loss 19.068407, Accuracy 0.072%\n",
      "Epoch 7, Batch 973, LR 1.887032 Loss 19.068610, Accuracy 0.072%\n",
      "Epoch 7, Batch 974, LR 1.887292 Loss 19.068647, Accuracy 0.072%\n",
      "Epoch 7, Batch 975, LR 1.887552 Loss 19.068434, Accuracy 0.072%\n",
      "Epoch 7, Batch 976, LR 1.887812 Loss 19.068708, Accuracy 0.072%\n",
      "Epoch 7, Batch 977, LR 1.888071 Loss 19.068869, Accuracy 0.072%\n",
      "Epoch 7, Batch 978, LR 1.888331 Loss 19.068904, Accuracy 0.073%\n",
      "Epoch 7, Batch 979, LR 1.888591 Loss 19.069083, Accuracy 0.073%\n",
      "Epoch 7, Batch 980, LR 1.888851 Loss 19.069262, Accuracy 0.073%\n",
      "Epoch 7, Batch 981, LR 1.889110 Loss 19.069079, Accuracy 0.072%\n",
      "Epoch 7, Batch 982, LR 1.889370 Loss 19.069021, Accuracy 0.072%\n",
      "Epoch 7, Batch 983, LR 1.889629 Loss 19.069092, Accuracy 0.072%\n",
      "Epoch 7, Batch 984, LR 1.889889 Loss 19.068837, Accuracy 0.072%\n",
      "Epoch 7, Batch 985, LR 1.890148 Loss 19.069100, Accuracy 0.072%\n",
      "Epoch 7, Batch 986, LR 1.890408 Loss 19.069013, Accuracy 0.072%\n",
      "Epoch 7, Batch 987, LR 1.890667 Loss 19.069175, Accuracy 0.072%\n",
      "Epoch 7, Batch 988, LR 1.890926 Loss 19.069248, Accuracy 0.072%\n",
      "Epoch 7, Batch 989, LR 1.891186 Loss 19.069073, Accuracy 0.072%\n",
      "Epoch 7, Batch 990, LR 1.891445 Loss 19.068959, Accuracy 0.072%\n",
      "Epoch 7, Batch 991, LR 1.891704 Loss 19.068787, Accuracy 0.072%\n",
      "Epoch 7, Batch 992, LR 1.891964 Loss 19.068807, Accuracy 0.072%\n",
      "Epoch 7, Batch 993, LR 1.892223 Loss 19.068933, Accuracy 0.072%\n",
      "Epoch 7, Batch 994, LR 1.892482 Loss 19.069070, Accuracy 0.072%\n",
      "Epoch 7, Batch 995, LR 1.892741 Loss 19.069114, Accuracy 0.071%\n",
      "Epoch 7, Batch 996, LR 1.893000 Loss 19.069080, Accuracy 0.071%\n",
      "Epoch 7, Batch 997, LR 1.893259 Loss 19.068928, Accuracy 0.071%\n",
      "Epoch 7, Batch 998, LR 1.893518 Loss 19.068869, Accuracy 0.071%\n",
      "Epoch 7, Batch 999, LR 1.893777 Loss 19.068755, Accuracy 0.071%\n",
      "Epoch 7, Batch 1000, LR 1.894036 Loss 19.068709, Accuracy 0.071%\n",
      "Epoch 7, Batch 1001, LR 1.894295 Loss 19.068681, Accuracy 0.071%\n",
      "Epoch 7, Batch 1002, LR 1.894554 Loss 19.068499, Accuracy 0.071%\n",
      "Epoch 7, Batch 1003, LR 1.894813 Loss 19.068333, Accuracy 0.071%\n",
      "Epoch 7, Batch 1004, LR 1.895071 Loss 19.068241, Accuracy 0.071%\n",
      "Epoch 7, Batch 1005, LR 1.895330 Loss 19.067986, Accuracy 0.071%\n",
      "Epoch 7, Batch 1006, LR 1.895589 Loss 19.067829, Accuracy 0.071%\n",
      "Epoch 7, Batch 1007, LR 1.895847 Loss 19.067722, Accuracy 0.071%\n",
      "Epoch 7, Batch 1008, LR 1.896106 Loss 19.067652, Accuracy 0.071%\n",
      "Epoch 7, Batch 1009, LR 1.896365 Loss 19.067539, Accuracy 0.071%\n",
      "Epoch 7, Batch 1010, LR 1.896623 Loss 19.067655, Accuracy 0.071%\n",
      "Epoch 7, Batch 1011, LR 1.896882 Loss 19.067664, Accuracy 0.071%\n",
      "Epoch 7, Batch 1012, LR 1.897140 Loss 19.067670, Accuracy 0.071%\n",
      "Epoch 7, Batch 1013, LR 1.897399 Loss 19.067634, Accuracy 0.071%\n",
      "Epoch 7, Batch 1014, LR 1.897657 Loss 19.067508, Accuracy 0.071%\n",
      "Epoch 7, Batch 1015, LR 1.897916 Loss 19.067405, Accuracy 0.071%\n",
      "Epoch 7, Batch 1016, LR 1.898174 Loss 19.067282, Accuracy 0.071%\n",
      "Epoch 7, Batch 1017, LR 1.898432 Loss 19.067209, Accuracy 0.071%\n",
      "Epoch 7, Batch 1018, LR 1.898690 Loss 19.067296, Accuracy 0.071%\n",
      "Epoch 7, Batch 1019, LR 1.898949 Loss 19.067311, Accuracy 0.071%\n",
      "Epoch 7, Batch 1020, LR 1.899207 Loss 19.067398, Accuracy 0.070%\n",
      "Epoch 7, Batch 1021, LR 1.899465 Loss 19.067338, Accuracy 0.070%\n",
      "Epoch 7, Batch 1022, LR 1.899723 Loss 19.067207, Accuracy 0.070%\n",
      "Epoch 7, Batch 1023, LR 1.899981 Loss 19.067247, Accuracy 0.070%\n",
      "Epoch 7, Batch 1024, LR 1.900239 Loss 19.067073, Accuracy 0.070%\n",
      "Epoch 7, Batch 1025, LR 1.900497 Loss 19.066976, Accuracy 0.070%\n",
      "Epoch 7, Batch 1026, LR 1.900755 Loss 19.067060, Accuracy 0.070%\n",
      "Epoch 7, Batch 1027, LR 1.901013 Loss 19.066951, Accuracy 0.070%\n",
      "Epoch 7, Batch 1028, LR 1.901271 Loss 19.067066, Accuracy 0.070%\n",
      "Epoch 7, Batch 1029, LR 1.901529 Loss 19.066921, Accuracy 0.070%\n",
      "Epoch 7, Batch 1030, LR 1.901787 Loss 19.066758, Accuracy 0.070%\n",
      "Epoch 7, Batch 1031, LR 1.902045 Loss 19.066742, Accuracy 0.070%\n",
      "Epoch 7, Batch 1032, LR 1.902302 Loss 19.066877, Accuracy 0.070%\n",
      "Epoch 7, Batch 1033, LR 1.902560 Loss 19.066796, Accuracy 0.070%\n",
      "Epoch 7, Batch 1034, LR 1.902818 Loss 19.066896, Accuracy 0.070%\n",
      "Epoch 7, Batch 1035, LR 1.903075 Loss 19.066711, Accuracy 0.069%\n",
      "Epoch 7, Batch 1036, LR 1.903333 Loss 19.066609, Accuracy 0.069%\n",
      "Epoch 7, Batch 1037, LR 1.903591 Loss 19.066538, Accuracy 0.069%\n",
      "Epoch 7, Batch 1038, LR 1.903848 Loss 19.066836, Accuracy 0.069%\n",
      "Epoch 7, Batch 1039, LR 1.904106 Loss 19.066849, Accuracy 0.069%\n",
      "Epoch 7, Batch 1040, LR 1.904363 Loss 19.066770, Accuracy 0.069%\n",
      "Epoch 7, Batch 1041, LR 1.904621 Loss 19.066820, Accuracy 0.069%\n",
      "Epoch 7, Batch 1042, LR 1.904878 Loss 19.066887, Accuracy 0.069%\n",
      "Epoch 7, Batch 1043, LR 1.905135 Loss 19.066763, Accuracy 0.069%\n",
      "Epoch 7, Batch 1044, LR 1.905393 Loss 19.066735, Accuracy 0.069%\n",
      "Epoch 7, Batch 1045, LR 1.905650 Loss 19.066866, Accuracy 0.070%\n",
      "Epoch 7, Batch 1046, LR 1.905907 Loss 19.066828, Accuracy 0.069%\n",
      "Epoch 7, Batch 1047, LR 1.906164 Loss 19.066754, Accuracy 0.069%\n",
      "Epoch 7, Loss (train set) 19.066754, Accuracy (train set) 0.069%\n",
      "Epoch 8, Batch 1, LR 1.906421 Loss 18.922844, Accuracy 0.000%\n",
      "Epoch 8, Batch 2, LR 1.906679 Loss 18.978147, Accuracy 0.000%\n",
      "Epoch 8, Batch 3, LR 1.906936 Loss 19.021390, Accuracy 0.000%\n",
      "Epoch 8, Batch 4, LR 1.907193 Loss 18.957680, Accuracy 0.000%\n",
      "Epoch 8, Batch 5, LR 1.907450 Loss 19.004964, Accuracy 0.000%\n",
      "Epoch 8, Batch 6, LR 1.907707 Loss 19.029732, Accuracy 0.000%\n",
      "Epoch 8, Batch 7, LR 1.907964 Loss 18.997820, Accuracy 0.000%\n",
      "Epoch 8, Batch 8, LR 1.908221 Loss 19.004032, Accuracy 0.000%\n",
      "Epoch 8, Batch 9, LR 1.908477 Loss 18.997252, Accuracy 0.000%\n",
      "Epoch 8, Batch 10, LR 1.908734 Loss 19.000520, Accuracy 0.000%\n",
      "Epoch 8, Batch 11, LR 1.908991 Loss 19.001427, Accuracy 0.000%\n",
      "Epoch 8, Batch 12, LR 1.909248 Loss 19.006118, Accuracy 0.000%\n",
      "Epoch 8, Batch 13, LR 1.909505 Loss 19.026911, Accuracy 0.000%\n",
      "Epoch 8, Batch 14, LR 1.909761 Loss 19.009159, Accuracy 0.000%\n",
      "Epoch 8, Batch 15, LR 1.910018 Loss 19.006746, Accuracy 0.000%\n",
      "Epoch 8, Batch 16, LR 1.910274 Loss 19.009157, Accuracy 0.000%\n",
      "Epoch 8, Batch 17, LR 1.910531 Loss 19.007223, Accuracy 0.000%\n",
      "Epoch 8, Batch 18, LR 1.910788 Loss 19.013339, Accuracy 0.000%\n",
      "Epoch 8, Batch 19, LR 1.911044 Loss 19.011876, Accuracy 0.000%\n",
      "Epoch 8, Batch 20, LR 1.911300 Loss 19.012648, Accuracy 0.000%\n",
      "Epoch 8, Batch 21, LR 1.911557 Loss 19.016407, Accuracy 0.000%\n",
      "Epoch 8, Batch 22, LR 1.911813 Loss 19.015468, Accuracy 0.000%\n",
      "Epoch 8, Batch 23, LR 1.912070 Loss 19.011041, Accuracy 0.000%\n",
      "Epoch 8, Batch 24, LR 1.912326 Loss 19.012181, Accuracy 0.000%\n",
      "Epoch 8, Batch 25, LR 1.912582 Loss 19.011005, Accuracy 0.000%\n",
      "Epoch 8, Batch 26, LR 1.912838 Loss 19.015235, Accuracy 0.000%\n",
      "Epoch 8, Batch 27, LR 1.913095 Loss 19.019622, Accuracy 0.000%\n",
      "Epoch 8, Batch 28, LR 1.913351 Loss 19.022680, Accuracy 0.000%\n",
      "Epoch 8, Batch 29, LR 1.913607 Loss 19.023689, Accuracy 0.000%\n",
      "Epoch 8, Batch 30, LR 1.913863 Loss 19.034340, Accuracy 0.000%\n",
      "Epoch 8, Batch 31, LR 1.914119 Loss 19.034865, Accuracy 0.000%\n",
      "Epoch 8, Batch 32, LR 1.914375 Loss 19.037367, Accuracy 0.000%\n",
      "Epoch 8, Batch 33, LR 1.914631 Loss 19.035220, Accuracy 0.000%\n",
      "Epoch 8, Batch 34, LR 1.914887 Loss 19.034261, Accuracy 0.000%\n",
      "Epoch 8, Batch 35, LR 1.915143 Loss 19.037007, Accuracy 0.000%\n",
      "Epoch 8, Batch 36, LR 1.915399 Loss 19.036362, Accuracy 0.000%\n",
      "Epoch 8, Batch 37, LR 1.915654 Loss 19.039117, Accuracy 0.000%\n",
      "Epoch 8, Batch 38, LR 1.915910 Loss 19.036182, Accuracy 0.000%\n",
      "Epoch 8, Batch 39, LR 1.916166 Loss 19.031549, Accuracy 0.000%\n",
      "Epoch 8, Batch 40, LR 1.916422 Loss 19.036507, Accuracy 0.000%\n",
      "Epoch 8, Batch 41, LR 1.916677 Loss 19.039669, Accuracy 0.000%\n",
      "Epoch 8, Batch 42, LR 1.916933 Loss 19.044406, Accuracy 0.000%\n",
      "Epoch 8, Batch 43, LR 1.917189 Loss 19.040605, Accuracy 0.018%\n",
      "Epoch 8, Batch 44, LR 1.917444 Loss 19.039902, Accuracy 0.036%\n",
      "Epoch 8, Batch 45, LR 1.917700 Loss 19.039196, Accuracy 0.035%\n",
      "Epoch 8, Batch 46, LR 1.917955 Loss 19.038782, Accuracy 0.034%\n",
      "Epoch 8, Batch 47, LR 1.918211 Loss 19.041873, Accuracy 0.033%\n",
      "Epoch 8, Batch 48, LR 1.918466 Loss 19.041205, Accuracy 0.033%\n",
      "Epoch 8, Batch 49, LR 1.918721 Loss 19.040020, Accuracy 0.032%\n",
      "Epoch 8, Batch 50, LR 1.918977 Loss 19.041392, Accuracy 0.031%\n",
      "Epoch 8, Batch 51, LR 1.919232 Loss 19.042146, Accuracy 0.031%\n",
      "Epoch 8, Batch 52, LR 1.919487 Loss 19.039036, Accuracy 0.045%\n",
      "Epoch 8, Batch 53, LR 1.919742 Loss 19.041077, Accuracy 0.044%\n",
      "Epoch 8, Batch 54, LR 1.919998 Loss 19.042693, Accuracy 0.043%\n",
      "Epoch 8, Batch 55, LR 1.920253 Loss 19.041282, Accuracy 0.043%\n",
      "Epoch 8, Batch 56, LR 1.920508 Loss 19.041835, Accuracy 0.042%\n",
      "Epoch 8, Batch 57, LR 1.920763 Loss 19.039374, Accuracy 0.041%\n",
      "Epoch 8, Batch 58, LR 1.921018 Loss 19.040465, Accuracy 0.040%\n",
      "Epoch 8, Batch 59, LR 1.921273 Loss 19.040711, Accuracy 0.040%\n",
      "Epoch 8, Batch 60, LR 1.921528 Loss 19.044037, Accuracy 0.039%\n",
      "Epoch 8, Batch 61, LR 1.921783 Loss 19.044374, Accuracy 0.038%\n",
      "Epoch 8, Batch 62, LR 1.922038 Loss 19.042837, Accuracy 0.038%\n",
      "Epoch 8, Batch 63, LR 1.922292 Loss 19.042339, Accuracy 0.037%\n",
      "Epoch 8, Batch 64, LR 1.922547 Loss 19.043229, Accuracy 0.049%\n",
      "Epoch 8, Batch 65, LR 1.922802 Loss 19.041597, Accuracy 0.048%\n",
      "Epoch 8, Batch 66, LR 1.923057 Loss 19.042775, Accuracy 0.047%\n",
      "Epoch 8, Batch 67, LR 1.923311 Loss 19.042471, Accuracy 0.047%\n",
      "Epoch 8, Batch 68, LR 1.923566 Loss 19.046264, Accuracy 0.046%\n",
      "Epoch 8, Batch 69, LR 1.923821 Loss 19.043754, Accuracy 0.057%\n",
      "Epoch 8, Batch 70, LR 1.924075 Loss 19.045679, Accuracy 0.056%\n",
      "Epoch 8, Batch 71, LR 1.924330 Loss 19.042962, Accuracy 0.055%\n",
      "Epoch 8, Batch 72, LR 1.924584 Loss 19.040303, Accuracy 0.054%\n",
      "Epoch 8, Batch 73, LR 1.924839 Loss 19.039282, Accuracy 0.054%\n",
      "Epoch 8, Batch 74, LR 1.925093 Loss 19.036997, Accuracy 0.053%\n",
      "Epoch 8, Batch 75, LR 1.925347 Loss 19.037910, Accuracy 0.052%\n",
      "Epoch 8, Batch 76, LR 1.925602 Loss 19.036895, Accuracy 0.051%\n",
      "Epoch 8, Batch 77, LR 1.925856 Loss 19.037573, Accuracy 0.051%\n",
      "Epoch 8, Batch 78, LR 1.926110 Loss 19.038416, Accuracy 0.050%\n",
      "Epoch 8, Batch 79, LR 1.926365 Loss 19.036505, Accuracy 0.049%\n",
      "Epoch 8, Batch 80, LR 1.926619 Loss 19.034758, Accuracy 0.049%\n",
      "Epoch 8, Batch 81, LR 1.926873 Loss 19.033212, Accuracy 0.048%\n",
      "Epoch 8, Batch 82, LR 1.927127 Loss 19.034948, Accuracy 0.048%\n",
      "Epoch 8, Batch 83, LR 1.927381 Loss 19.037164, Accuracy 0.047%\n",
      "Epoch 8, Batch 84, LR 1.927635 Loss 19.037451, Accuracy 0.047%\n",
      "Epoch 8, Batch 85, LR 1.927889 Loss 19.036256, Accuracy 0.046%\n",
      "Epoch 8, Batch 86, LR 1.928143 Loss 19.037259, Accuracy 0.045%\n",
      "Epoch 8, Batch 87, LR 1.928397 Loss 19.035015, Accuracy 0.045%\n",
      "Epoch 8, Batch 88, LR 1.928651 Loss 19.036497, Accuracy 0.044%\n",
      "Epoch 8, Batch 89, LR 1.928905 Loss 19.036083, Accuracy 0.053%\n",
      "Epoch 8, Batch 90, LR 1.929158 Loss 19.037016, Accuracy 0.052%\n",
      "Epoch 8, Batch 91, LR 1.929412 Loss 19.037112, Accuracy 0.052%\n",
      "Epoch 8, Batch 92, LR 1.929666 Loss 19.037398, Accuracy 0.051%\n",
      "Epoch 8, Batch 93, LR 1.929920 Loss 19.037644, Accuracy 0.050%\n",
      "Epoch 8, Batch 94, LR 1.930173 Loss 19.038456, Accuracy 0.050%\n",
      "Epoch 8, Batch 95, LR 1.930427 Loss 19.039893, Accuracy 0.049%\n",
      "Epoch 8, Batch 96, LR 1.930680 Loss 19.041722, Accuracy 0.049%\n",
      "Epoch 8, Batch 97, LR 1.930934 Loss 19.041866, Accuracy 0.048%\n",
      "Epoch 8, Batch 98, LR 1.931187 Loss 19.043840, Accuracy 0.048%\n",
      "Epoch 8, Batch 99, LR 1.931441 Loss 19.044227, Accuracy 0.047%\n",
      "Epoch 8, Batch 100, LR 1.931694 Loss 19.044882, Accuracy 0.047%\n",
      "Epoch 8, Batch 101, LR 1.931948 Loss 19.044892, Accuracy 0.046%\n",
      "Epoch 8, Batch 102, LR 1.932201 Loss 19.043835, Accuracy 0.046%\n",
      "Epoch 8, Batch 103, LR 1.932454 Loss 19.044185, Accuracy 0.046%\n",
      "Epoch 8, Batch 104, LR 1.932707 Loss 19.044799, Accuracy 0.045%\n",
      "Epoch 8, Batch 105, LR 1.932961 Loss 19.044604, Accuracy 0.045%\n",
      "Epoch 8, Batch 106, LR 1.933214 Loss 19.044804, Accuracy 0.044%\n",
      "Epoch 8, Batch 107, LR 1.933467 Loss 19.045998, Accuracy 0.044%\n",
      "Epoch 8, Batch 108, LR 1.933720 Loss 19.046622, Accuracy 0.043%\n",
      "Epoch 8, Batch 109, LR 1.933973 Loss 19.047145, Accuracy 0.043%\n",
      "Epoch 8, Batch 110, LR 1.934226 Loss 19.047039, Accuracy 0.043%\n",
      "Epoch 8, Batch 111, LR 1.934479 Loss 19.048384, Accuracy 0.042%\n",
      "Epoch 8, Batch 112, LR 1.934732 Loss 19.049524, Accuracy 0.042%\n",
      "Epoch 8, Batch 113, LR 1.934985 Loss 19.048006, Accuracy 0.041%\n",
      "Epoch 8, Batch 114, LR 1.935238 Loss 19.047901, Accuracy 0.041%\n",
      "Epoch 8, Batch 115, LR 1.935491 Loss 19.047300, Accuracy 0.041%\n",
      "Epoch 8, Batch 116, LR 1.935744 Loss 19.048306, Accuracy 0.040%\n",
      "Epoch 8, Batch 117, LR 1.935996 Loss 19.049023, Accuracy 0.040%\n",
      "Epoch 8, Batch 118, LR 1.936249 Loss 19.047320, Accuracy 0.040%\n",
      "Epoch 8, Batch 119, LR 1.936502 Loss 19.049846, Accuracy 0.039%\n",
      "Epoch 8, Batch 120, LR 1.936754 Loss 19.050603, Accuracy 0.039%\n",
      "Epoch 8, Batch 121, LR 1.937007 Loss 19.050414, Accuracy 0.039%\n",
      "Epoch 8, Batch 122, LR 1.937259 Loss 19.050832, Accuracy 0.038%\n",
      "Epoch 8, Batch 123, LR 1.937512 Loss 19.050771, Accuracy 0.038%\n",
      "Epoch 8, Batch 124, LR 1.937764 Loss 19.050567, Accuracy 0.038%\n",
      "Epoch 8, Batch 125, LR 1.938017 Loss 19.048817, Accuracy 0.037%\n",
      "Epoch 8, Batch 126, LR 1.938269 Loss 19.050341, Accuracy 0.037%\n",
      "Epoch 8, Batch 127, LR 1.938522 Loss 19.048990, Accuracy 0.037%\n",
      "Epoch 8, Batch 128, LR 1.938774 Loss 19.051613, Accuracy 0.037%\n",
      "Epoch 8, Batch 129, LR 1.939026 Loss 19.050915, Accuracy 0.042%\n",
      "Epoch 8, Batch 130, LR 1.939278 Loss 19.050450, Accuracy 0.042%\n",
      "Epoch 8, Batch 131, LR 1.939531 Loss 19.049424, Accuracy 0.042%\n",
      "Epoch 8, Batch 132, LR 1.939783 Loss 19.049432, Accuracy 0.041%\n",
      "Epoch 8, Batch 133, LR 1.940035 Loss 19.049668, Accuracy 0.041%\n",
      "Epoch 8, Batch 134, LR 1.940287 Loss 19.048550, Accuracy 0.041%\n",
      "Epoch 8, Batch 135, LR 1.940539 Loss 19.050148, Accuracy 0.041%\n",
      "Epoch 8, Batch 136, LR 1.940791 Loss 19.050638, Accuracy 0.040%\n",
      "Epoch 8, Batch 137, LR 1.941043 Loss 19.050776, Accuracy 0.040%\n",
      "Epoch 8, Batch 138, LR 1.941295 Loss 19.048472, Accuracy 0.040%\n",
      "Epoch 8, Batch 139, LR 1.941547 Loss 19.047484, Accuracy 0.045%\n",
      "Epoch 8, Batch 140, LR 1.941799 Loss 19.047174, Accuracy 0.045%\n",
      "Epoch 8, Batch 141, LR 1.942051 Loss 19.048473, Accuracy 0.044%\n",
      "Epoch 8, Batch 142, LR 1.942302 Loss 19.049061, Accuracy 0.044%\n",
      "Epoch 8, Batch 143, LR 1.942554 Loss 19.050117, Accuracy 0.044%\n",
      "Epoch 8, Batch 144, LR 1.942806 Loss 19.050329, Accuracy 0.043%\n",
      "Epoch 8, Batch 145, LR 1.943057 Loss 19.050343, Accuracy 0.043%\n",
      "Epoch 8, Batch 146, LR 1.943309 Loss 19.049863, Accuracy 0.043%\n",
      "Epoch 8, Batch 147, LR 1.943561 Loss 19.048035, Accuracy 0.043%\n",
      "Epoch 8, Batch 148, LR 1.943812 Loss 19.048048, Accuracy 0.042%\n",
      "Epoch 8, Batch 149, LR 1.944064 Loss 19.047085, Accuracy 0.042%\n",
      "Epoch 8, Batch 150, LR 1.944315 Loss 19.047848, Accuracy 0.042%\n",
      "Epoch 8, Batch 151, LR 1.944566 Loss 19.048729, Accuracy 0.041%\n",
      "Epoch 8, Batch 152, LR 1.944818 Loss 19.049249, Accuracy 0.041%\n",
      "Epoch 8, Batch 153, LR 1.945069 Loss 19.049489, Accuracy 0.041%\n",
      "Epoch 8, Batch 154, LR 1.945320 Loss 19.049805, Accuracy 0.041%\n",
      "Epoch 8, Batch 155, LR 1.945572 Loss 19.049619, Accuracy 0.040%\n",
      "Epoch 8, Batch 156, LR 1.945823 Loss 19.049385, Accuracy 0.040%\n",
      "Epoch 8, Batch 157, LR 1.946074 Loss 19.051378, Accuracy 0.040%\n",
      "Epoch 8, Batch 158, LR 1.946325 Loss 19.050929, Accuracy 0.040%\n",
      "Epoch 8, Batch 159, LR 1.946576 Loss 19.051237, Accuracy 0.039%\n",
      "Epoch 8, Batch 160, LR 1.946827 Loss 19.051559, Accuracy 0.039%\n",
      "Epoch 8, Batch 161, LR 1.947078 Loss 19.050940, Accuracy 0.039%\n",
      "Epoch 8, Batch 162, LR 1.947329 Loss 19.052162, Accuracy 0.039%\n",
      "Epoch 8, Batch 163, LR 1.947580 Loss 19.052536, Accuracy 0.038%\n",
      "Epoch 8, Batch 164, LR 1.947831 Loss 19.052488, Accuracy 0.038%\n",
      "Epoch 8, Batch 165, LR 1.948082 Loss 19.052229, Accuracy 0.038%\n",
      "Epoch 8, Batch 166, LR 1.948333 Loss 19.051149, Accuracy 0.038%\n",
      "Epoch 8, Batch 167, LR 1.948584 Loss 19.051974, Accuracy 0.037%\n",
      "Epoch 8, Batch 168, LR 1.948834 Loss 19.051562, Accuracy 0.037%\n",
      "Epoch 8, Batch 169, LR 1.949085 Loss 19.052188, Accuracy 0.037%\n",
      "Epoch 8, Batch 170, LR 1.949336 Loss 19.051422, Accuracy 0.041%\n",
      "Epoch 8, Batch 171, LR 1.949586 Loss 19.050299, Accuracy 0.046%\n",
      "Epoch 8, Batch 172, LR 1.949837 Loss 19.051320, Accuracy 0.045%\n",
      "Epoch 8, Batch 173, LR 1.950088 Loss 19.051529, Accuracy 0.045%\n",
      "Epoch 8, Batch 174, LR 1.950338 Loss 19.052759, Accuracy 0.045%\n",
      "Epoch 8, Batch 175, LR 1.950589 Loss 19.052594, Accuracy 0.049%\n",
      "Epoch 8, Batch 176, LR 1.950839 Loss 19.051868, Accuracy 0.049%\n",
      "Epoch 8, Batch 177, LR 1.951089 Loss 19.050806, Accuracy 0.049%\n",
      "Epoch 8, Batch 178, LR 1.951340 Loss 19.051206, Accuracy 0.048%\n",
      "Epoch 8, Batch 179, LR 1.951590 Loss 19.051948, Accuracy 0.048%\n",
      "Epoch 8, Batch 180, LR 1.951840 Loss 19.051824, Accuracy 0.048%\n",
      "Epoch 8, Batch 181, LR 1.952090 Loss 19.052241, Accuracy 0.047%\n",
      "Epoch 8, Batch 182, LR 1.952341 Loss 19.052153, Accuracy 0.047%\n",
      "Epoch 8, Batch 183, LR 1.952591 Loss 19.052124, Accuracy 0.047%\n",
      "Epoch 8, Batch 184, LR 1.952841 Loss 19.051567, Accuracy 0.047%\n",
      "Epoch 8, Batch 185, LR 1.953091 Loss 19.052335, Accuracy 0.046%\n",
      "Epoch 8, Batch 186, LR 1.953341 Loss 19.050913, Accuracy 0.046%\n",
      "Epoch 8, Batch 187, LR 1.953591 Loss 19.051117, Accuracy 0.046%\n",
      "Epoch 8, Batch 188, LR 1.953841 Loss 19.051716, Accuracy 0.046%\n",
      "Epoch 8, Batch 189, LR 1.954091 Loss 19.052456, Accuracy 0.050%\n",
      "Epoch 8, Batch 190, LR 1.954341 Loss 19.052366, Accuracy 0.049%\n",
      "Epoch 8, Batch 191, LR 1.954590 Loss 19.052539, Accuracy 0.049%\n",
      "Epoch 8, Batch 192, LR 1.954840 Loss 19.052609, Accuracy 0.049%\n",
      "Epoch 8, Batch 193, LR 1.955090 Loss 19.052839, Accuracy 0.049%\n",
      "Epoch 8, Batch 194, LR 1.955340 Loss 19.052949, Accuracy 0.048%\n",
      "Epoch 8, Batch 195, LR 1.955589 Loss 19.054138, Accuracy 0.048%\n",
      "Epoch 8, Batch 196, LR 1.955839 Loss 19.053252, Accuracy 0.048%\n",
      "Epoch 8, Batch 197, LR 1.956089 Loss 19.053807, Accuracy 0.048%\n",
      "Epoch 8, Batch 198, LR 1.956338 Loss 19.053834, Accuracy 0.047%\n",
      "Epoch 8, Batch 199, LR 1.956588 Loss 19.055345, Accuracy 0.047%\n",
      "Epoch 8, Batch 200, LR 1.956837 Loss 19.055255, Accuracy 0.047%\n",
      "Epoch 8, Batch 201, LR 1.957086 Loss 19.055366, Accuracy 0.047%\n",
      "Epoch 8, Batch 202, LR 1.957336 Loss 19.055790, Accuracy 0.046%\n",
      "Epoch 8, Batch 203, LR 1.957585 Loss 19.056246, Accuracy 0.046%\n",
      "Epoch 8, Batch 204, LR 1.957834 Loss 19.056644, Accuracy 0.046%\n",
      "Epoch 8, Batch 205, LR 1.958084 Loss 19.056970, Accuracy 0.050%\n",
      "Epoch 8, Batch 206, LR 1.958333 Loss 19.058113, Accuracy 0.049%\n",
      "Epoch 8, Batch 207, LR 1.958582 Loss 19.057971, Accuracy 0.053%\n",
      "Epoch 8, Batch 208, LR 1.958831 Loss 19.057751, Accuracy 0.053%\n",
      "Epoch 8, Batch 209, LR 1.959080 Loss 19.058752, Accuracy 0.052%\n",
      "Epoch 8, Batch 210, LR 1.959329 Loss 19.058554, Accuracy 0.052%\n",
      "Epoch 8, Batch 211, LR 1.959578 Loss 19.058377, Accuracy 0.052%\n",
      "Epoch 8, Batch 212, LR 1.959827 Loss 19.058848, Accuracy 0.052%\n",
      "Epoch 8, Batch 213, LR 1.960076 Loss 19.058792, Accuracy 0.051%\n",
      "Epoch 8, Batch 214, LR 1.960325 Loss 19.058355, Accuracy 0.055%\n",
      "Epoch 8, Batch 215, LR 1.960574 Loss 19.058235, Accuracy 0.055%\n",
      "Epoch 8, Batch 216, LR 1.960823 Loss 19.057810, Accuracy 0.054%\n",
      "Epoch 8, Batch 217, LR 1.961072 Loss 19.058074, Accuracy 0.054%\n",
      "Epoch 8, Batch 218, LR 1.961320 Loss 19.058451, Accuracy 0.054%\n",
      "Epoch 8, Batch 219, LR 1.961569 Loss 19.059199, Accuracy 0.054%\n",
      "Epoch 8, Batch 220, LR 1.961818 Loss 19.060001, Accuracy 0.053%\n",
      "Epoch 8, Batch 221, LR 1.962066 Loss 19.060151, Accuracy 0.057%\n",
      "Epoch 8, Batch 222, LR 1.962315 Loss 19.060577, Accuracy 0.056%\n",
      "Epoch 8, Batch 223, LR 1.962563 Loss 19.060526, Accuracy 0.056%\n",
      "Epoch 8, Batch 224, LR 1.962812 Loss 19.060578, Accuracy 0.056%\n",
      "Epoch 8, Batch 225, LR 1.963060 Loss 19.060128, Accuracy 0.056%\n",
      "Epoch 8, Batch 226, LR 1.963309 Loss 19.059808, Accuracy 0.055%\n",
      "Epoch 8, Batch 227, LR 1.963557 Loss 19.059479, Accuracy 0.055%\n",
      "Epoch 8, Batch 228, LR 1.963805 Loss 19.060239, Accuracy 0.055%\n",
      "Epoch 8, Batch 229, LR 1.964054 Loss 19.059923, Accuracy 0.058%\n",
      "Epoch 8, Batch 230, LR 1.964302 Loss 19.059373, Accuracy 0.058%\n",
      "Epoch 8, Batch 231, LR 1.964550 Loss 19.060380, Accuracy 0.057%\n",
      "Epoch 8, Batch 232, LR 1.964798 Loss 19.060556, Accuracy 0.057%\n",
      "Epoch 8, Batch 233, LR 1.965047 Loss 19.060619, Accuracy 0.057%\n",
      "Epoch 8, Batch 234, LR 1.965295 Loss 19.060091, Accuracy 0.060%\n",
      "Epoch 8, Batch 235, LR 1.965543 Loss 19.060654, Accuracy 0.060%\n",
      "Epoch 8, Batch 236, LR 1.965791 Loss 19.061643, Accuracy 0.060%\n",
      "Epoch 8, Batch 237, LR 1.966039 Loss 19.061725, Accuracy 0.059%\n",
      "Epoch 8, Batch 238, LR 1.966287 Loss 19.061887, Accuracy 0.059%\n",
      "Epoch 8, Batch 239, LR 1.966534 Loss 19.062150, Accuracy 0.059%\n",
      "Epoch 8, Batch 240, LR 1.966782 Loss 19.061764, Accuracy 0.059%\n",
      "Epoch 8, Batch 241, LR 1.967030 Loss 19.062045, Accuracy 0.058%\n",
      "Epoch 8, Batch 242, LR 1.967278 Loss 19.062406, Accuracy 0.058%\n",
      "Epoch 8, Batch 243, LR 1.967525 Loss 19.062500, Accuracy 0.058%\n",
      "Epoch 8, Batch 244, LR 1.967773 Loss 19.062704, Accuracy 0.058%\n",
      "Epoch 8, Batch 245, LR 1.968021 Loss 19.062628, Accuracy 0.057%\n",
      "Epoch 8, Batch 246, LR 1.968268 Loss 19.062310, Accuracy 0.057%\n",
      "Epoch 8, Batch 247, LR 1.968516 Loss 19.062832, Accuracy 0.057%\n",
      "Epoch 8, Batch 248, LR 1.968763 Loss 19.063195, Accuracy 0.057%\n",
      "Epoch 8, Batch 249, LR 1.969011 Loss 19.063614, Accuracy 0.056%\n",
      "Epoch 8, Batch 250, LR 1.969258 Loss 19.064152, Accuracy 0.056%\n",
      "Epoch 8, Batch 251, LR 1.969506 Loss 19.063622, Accuracy 0.056%\n",
      "Epoch 8, Batch 252, LR 1.969753 Loss 19.064195, Accuracy 0.056%\n",
      "Epoch 8, Batch 253, LR 1.970000 Loss 19.064089, Accuracy 0.056%\n",
      "Epoch 8, Batch 254, LR 1.970248 Loss 19.064260, Accuracy 0.055%\n",
      "Epoch 8, Batch 255, LR 1.970495 Loss 19.064045, Accuracy 0.055%\n",
      "Epoch 8, Batch 256, LR 1.970742 Loss 19.064610, Accuracy 0.055%\n",
      "Epoch 8, Batch 257, LR 1.970989 Loss 19.064547, Accuracy 0.058%\n",
      "Epoch 8, Batch 258, LR 1.971236 Loss 19.064736, Accuracy 0.058%\n",
      "Epoch 8, Batch 259, LR 1.971483 Loss 19.063714, Accuracy 0.057%\n",
      "Epoch 8, Batch 260, LR 1.971730 Loss 19.064226, Accuracy 0.057%\n",
      "Epoch 8, Batch 261, LR 1.971977 Loss 19.064052, Accuracy 0.057%\n",
      "Epoch 8, Batch 262, LR 1.972224 Loss 19.063704, Accuracy 0.057%\n",
      "Epoch 8, Batch 263, LR 1.972471 Loss 19.064134, Accuracy 0.056%\n",
      "Epoch 8, Batch 264, LR 1.972718 Loss 19.063142, Accuracy 0.056%\n",
      "Epoch 8, Batch 265, LR 1.972965 Loss 19.063283, Accuracy 0.056%\n",
      "Epoch 8, Batch 266, LR 1.973212 Loss 19.063630, Accuracy 0.056%\n",
      "Epoch 8, Batch 267, LR 1.973458 Loss 19.063499, Accuracy 0.059%\n",
      "Epoch 8, Batch 268, LR 1.973705 Loss 19.063735, Accuracy 0.058%\n",
      "Epoch 8, Batch 269, LR 1.973952 Loss 19.063899, Accuracy 0.058%\n",
      "Epoch 8, Batch 270, LR 1.974198 Loss 19.063993, Accuracy 0.058%\n",
      "Epoch 8, Batch 271, LR 1.974445 Loss 19.063900, Accuracy 0.058%\n",
      "Epoch 8, Batch 272, LR 1.974691 Loss 19.064314, Accuracy 0.057%\n",
      "Epoch 8, Batch 273, LR 1.974938 Loss 19.062858, Accuracy 0.057%\n",
      "Epoch 8, Batch 274, LR 1.975184 Loss 19.062541, Accuracy 0.057%\n",
      "Epoch 8, Batch 275, LR 1.975431 Loss 19.062748, Accuracy 0.060%\n",
      "Epoch 8, Batch 276, LR 1.975677 Loss 19.061743, Accuracy 0.059%\n",
      "Epoch 8, Batch 277, LR 1.975923 Loss 19.062115, Accuracy 0.059%\n",
      "Epoch 8, Batch 278, LR 1.976170 Loss 19.062649, Accuracy 0.059%\n",
      "Epoch 8, Batch 279, LR 1.976416 Loss 19.062239, Accuracy 0.062%\n",
      "Epoch 8, Batch 280, LR 1.976662 Loss 19.061622, Accuracy 0.067%\n",
      "Epoch 8, Batch 281, LR 1.976908 Loss 19.062330, Accuracy 0.067%\n",
      "Epoch 8, Batch 282, LR 1.977154 Loss 19.061822, Accuracy 0.066%\n",
      "Epoch 8, Batch 283, LR 1.977400 Loss 19.062005, Accuracy 0.066%\n",
      "Epoch 8, Batch 284, LR 1.977646 Loss 19.061986, Accuracy 0.066%\n",
      "Epoch 8, Batch 285, LR 1.977892 Loss 19.061569, Accuracy 0.066%\n",
      "Epoch 8, Batch 286, LR 1.978138 Loss 19.061053, Accuracy 0.066%\n",
      "Epoch 8, Batch 287, LR 1.978384 Loss 19.061332, Accuracy 0.065%\n",
      "Epoch 8, Batch 288, LR 1.978630 Loss 19.061216, Accuracy 0.065%\n",
      "Epoch 8, Batch 289, LR 1.978876 Loss 19.061306, Accuracy 0.065%\n",
      "Epoch 8, Batch 290, LR 1.979122 Loss 19.061937, Accuracy 0.065%\n",
      "Epoch 8, Batch 291, LR 1.979367 Loss 19.061211, Accuracy 0.064%\n",
      "Epoch 8, Batch 292, LR 1.979613 Loss 19.061390, Accuracy 0.064%\n",
      "Epoch 8, Batch 293, LR 1.979859 Loss 19.061287, Accuracy 0.064%\n",
      "Epoch 8, Batch 294, LR 1.980104 Loss 19.060988, Accuracy 0.064%\n",
      "Epoch 8, Batch 295, LR 1.980350 Loss 19.060985, Accuracy 0.064%\n",
      "Epoch 8, Batch 296, LR 1.980595 Loss 19.060802, Accuracy 0.063%\n",
      "Epoch 8, Batch 297, LR 1.980841 Loss 19.060675, Accuracy 0.063%\n",
      "Epoch 8, Batch 298, LR 1.981086 Loss 19.060759, Accuracy 0.066%\n",
      "Epoch 8, Batch 299, LR 1.981332 Loss 19.060264, Accuracy 0.065%\n",
      "Epoch 8, Batch 300, LR 1.981577 Loss 19.060018, Accuracy 0.065%\n",
      "Epoch 8, Batch 301, LR 1.981822 Loss 19.060132, Accuracy 0.065%\n",
      "Epoch 8, Batch 302, LR 1.982068 Loss 19.060631, Accuracy 0.065%\n",
      "Epoch 8, Batch 303, LR 1.982313 Loss 19.060531, Accuracy 0.064%\n",
      "Epoch 8, Batch 304, LR 1.982558 Loss 19.060768, Accuracy 0.067%\n",
      "Epoch 8, Batch 305, LR 1.982803 Loss 19.060506, Accuracy 0.067%\n",
      "Epoch 8, Batch 306, LR 1.983048 Loss 19.060378, Accuracy 0.069%\n",
      "Epoch 8, Batch 307, LR 1.983293 Loss 19.060415, Accuracy 0.069%\n",
      "Epoch 8, Batch 308, LR 1.983538 Loss 19.060650, Accuracy 0.068%\n",
      "Epoch 8, Batch 309, LR 1.983783 Loss 19.061245, Accuracy 0.068%\n",
      "Epoch 8, Batch 310, LR 1.984028 Loss 19.061937, Accuracy 0.068%\n",
      "Epoch 8, Batch 311, LR 1.984273 Loss 19.061898, Accuracy 0.068%\n",
      "Epoch 8, Batch 312, LR 1.984518 Loss 19.061494, Accuracy 0.068%\n",
      "Epoch 8, Batch 313, LR 1.984763 Loss 19.061543, Accuracy 0.070%\n",
      "Epoch 8, Batch 314, LR 1.985007 Loss 19.060990, Accuracy 0.070%\n",
      "Epoch 8, Batch 315, LR 1.985252 Loss 19.060428, Accuracy 0.069%\n",
      "Epoch 8, Batch 316, LR 1.985497 Loss 19.060587, Accuracy 0.069%\n",
      "Epoch 8, Batch 317, LR 1.985742 Loss 19.061455, Accuracy 0.069%\n",
      "Epoch 8, Batch 318, LR 1.985986 Loss 19.061316, Accuracy 0.069%\n",
      "Epoch 8, Batch 319, LR 1.986231 Loss 19.061466, Accuracy 0.069%\n",
      "Epoch 8, Batch 320, LR 1.986475 Loss 19.061118, Accuracy 0.068%\n",
      "Epoch 8, Batch 321, LR 1.986720 Loss 19.061192, Accuracy 0.068%\n",
      "Epoch 8, Batch 322, LR 1.986964 Loss 19.061273, Accuracy 0.068%\n",
      "Epoch 8, Batch 323, LR 1.987208 Loss 19.061894, Accuracy 0.068%\n",
      "Epoch 8, Batch 324, LR 1.987453 Loss 19.061369, Accuracy 0.068%\n",
      "Epoch 8, Batch 325, LR 1.987697 Loss 19.061543, Accuracy 0.067%\n",
      "Epoch 8, Batch 326, LR 1.987941 Loss 19.061809, Accuracy 0.067%\n",
      "Epoch 8, Batch 327, LR 1.988186 Loss 19.062186, Accuracy 0.067%\n",
      "Epoch 8, Batch 328, LR 1.988430 Loss 19.061649, Accuracy 0.067%\n",
      "Epoch 8, Batch 329, LR 1.988674 Loss 19.061631, Accuracy 0.066%\n",
      "Epoch 8, Batch 330, LR 1.988918 Loss 19.061962, Accuracy 0.066%\n",
      "Epoch 8, Batch 331, LR 1.989162 Loss 19.061716, Accuracy 0.066%\n",
      "Epoch 8, Batch 332, LR 1.989406 Loss 19.061917, Accuracy 0.066%\n",
      "Epoch 8, Batch 333, LR 1.989650 Loss 19.061814, Accuracy 0.066%\n",
      "Epoch 8, Batch 334, LR 1.989894 Loss 19.061114, Accuracy 0.068%\n",
      "Epoch 8, Batch 335, LR 1.990138 Loss 19.060993, Accuracy 0.068%\n",
      "Epoch 8, Batch 336, LR 1.990382 Loss 19.060629, Accuracy 0.067%\n",
      "Epoch 8, Batch 337, LR 1.990625 Loss 19.060800, Accuracy 0.067%\n",
      "Epoch 8, Batch 338, LR 1.990869 Loss 19.060472, Accuracy 0.067%\n",
      "Epoch 8, Batch 339, LR 1.991113 Loss 19.060921, Accuracy 0.067%\n",
      "Epoch 8, Batch 340, LR 1.991357 Loss 19.060901, Accuracy 0.067%\n",
      "Epoch 8, Batch 341, LR 1.991600 Loss 19.061027, Accuracy 0.066%\n",
      "Epoch 8, Batch 342, LR 1.991844 Loss 19.061550, Accuracy 0.066%\n",
      "Epoch 8, Batch 343, LR 1.992087 Loss 19.061815, Accuracy 0.066%\n",
      "Epoch 8, Batch 344, LR 1.992331 Loss 19.061396, Accuracy 0.068%\n",
      "Epoch 8, Batch 345, LR 1.992574 Loss 19.061573, Accuracy 0.068%\n",
      "Epoch 8, Batch 346, LR 1.992818 Loss 19.061208, Accuracy 0.068%\n",
      "Epoch 8, Batch 347, LR 1.993061 Loss 19.061559, Accuracy 0.068%\n",
      "Epoch 8, Batch 348, LR 1.993304 Loss 19.061733, Accuracy 0.070%\n",
      "Epoch 8, Batch 349, LR 1.993548 Loss 19.061355, Accuracy 0.069%\n",
      "Epoch 8, Batch 350, LR 1.993791 Loss 19.061943, Accuracy 0.069%\n",
      "Epoch 8, Batch 351, LR 1.994034 Loss 19.062417, Accuracy 0.069%\n",
      "Epoch 8, Batch 352, LR 1.994277 Loss 19.062671, Accuracy 0.069%\n",
      "Epoch 8, Batch 353, LR 1.994520 Loss 19.062897, Accuracy 0.069%\n",
      "Epoch 8, Batch 354, LR 1.994763 Loss 19.062786, Accuracy 0.068%\n",
      "Epoch 8, Batch 355, LR 1.995007 Loss 19.062835, Accuracy 0.068%\n",
      "Epoch 8, Batch 356, LR 1.995249 Loss 19.062331, Accuracy 0.068%\n",
      "Epoch 8, Batch 357, LR 1.995492 Loss 19.062495, Accuracy 0.068%\n",
      "Epoch 8, Batch 358, LR 1.995735 Loss 19.062788, Accuracy 0.068%\n",
      "Epoch 8, Batch 359, LR 1.995978 Loss 19.062398, Accuracy 0.067%\n",
      "Epoch 8, Batch 360, LR 1.996221 Loss 19.062494, Accuracy 0.067%\n",
      "Epoch 8, Batch 361, LR 1.996464 Loss 19.062464, Accuracy 0.067%\n",
      "Epoch 8, Batch 362, LR 1.996707 Loss 19.062683, Accuracy 0.067%\n",
      "Epoch 8, Batch 363, LR 1.996949 Loss 19.062528, Accuracy 0.067%\n",
      "Epoch 8, Batch 364, LR 1.997192 Loss 19.062735, Accuracy 0.067%\n",
      "Epoch 8, Batch 365, LR 1.997434 Loss 19.062233, Accuracy 0.066%\n",
      "Epoch 8, Batch 366, LR 1.997677 Loss 19.062293, Accuracy 0.066%\n",
      "Epoch 8, Batch 367, LR 1.997920 Loss 19.061603, Accuracy 0.066%\n",
      "Epoch 8, Batch 368, LR 1.998162 Loss 19.061371, Accuracy 0.066%\n",
      "Epoch 8, Batch 369, LR 1.998404 Loss 19.061410, Accuracy 0.066%\n",
      "Epoch 8, Batch 370, LR 1.998647 Loss 19.061279, Accuracy 0.068%\n",
      "Epoch 8, Batch 371, LR 1.998889 Loss 19.061318, Accuracy 0.067%\n",
      "Epoch 8, Batch 372, LR 1.999131 Loss 19.060912, Accuracy 0.067%\n",
      "Epoch 8, Batch 373, LR 1.999374 Loss 19.061216, Accuracy 0.067%\n",
      "Epoch 8, Batch 374, LR 1.999616 Loss 19.061529, Accuracy 0.067%\n",
      "Epoch 8, Batch 375, LR 1.999858 Loss 19.061563, Accuracy 0.067%\n",
      "Epoch 8, Batch 376, LR 2.000100 Loss 19.061032, Accuracy 0.066%\n",
      "Epoch 8, Batch 377, LR 2.000342 Loss 19.060837, Accuracy 0.066%\n",
      "Epoch 8, Batch 378, LR 2.000584 Loss 19.060999, Accuracy 0.066%\n",
      "Epoch 8, Batch 379, LR 2.000826 Loss 19.060035, Accuracy 0.066%\n",
      "Epoch 8, Batch 380, LR 2.001068 Loss 19.059830, Accuracy 0.066%\n",
      "Epoch 8, Batch 381, LR 2.001310 Loss 19.060408, Accuracy 0.066%\n",
      "Epoch 8, Batch 382, LR 2.001552 Loss 19.060312, Accuracy 0.065%\n",
      "Epoch 8, Batch 383, LR 2.001794 Loss 19.060730, Accuracy 0.065%\n",
      "Epoch 8, Batch 384, LR 2.002036 Loss 19.061247, Accuracy 0.065%\n",
      "Epoch 8, Batch 385, LR 2.002277 Loss 19.061337, Accuracy 0.065%\n",
      "Epoch 8, Batch 386, LR 2.002519 Loss 19.061375, Accuracy 0.067%\n",
      "Epoch 8, Batch 387, LR 2.002761 Loss 19.061441, Accuracy 0.067%\n",
      "Epoch 8, Batch 388, LR 2.003002 Loss 19.061154, Accuracy 0.066%\n",
      "Epoch 8, Batch 389, LR 2.003244 Loss 19.061233, Accuracy 0.066%\n",
      "Epoch 8, Batch 390, LR 2.003486 Loss 19.061186, Accuracy 0.068%\n",
      "Epoch 8, Batch 391, LR 2.003727 Loss 19.061358, Accuracy 0.068%\n",
      "Epoch 8, Batch 392, LR 2.003968 Loss 19.061655, Accuracy 0.068%\n",
      "Epoch 8, Batch 393, LR 2.004210 Loss 19.061288, Accuracy 0.068%\n",
      "Epoch 8, Batch 394, LR 2.004451 Loss 19.061223, Accuracy 0.067%\n",
      "Epoch 8, Batch 395, LR 2.004693 Loss 19.061139, Accuracy 0.067%\n",
      "Epoch 8, Batch 396, LR 2.004934 Loss 19.061060, Accuracy 0.067%\n",
      "Epoch 8, Batch 397, LR 2.005175 Loss 19.061208, Accuracy 0.067%\n",
      "Epoch 8, Batch 398, LR 2.005416 Loss 19.061535, Accuracy 0.067%\n",
      "Epoch 8, Batch 399, LR 2.005657 Loss 19.062048, Accuracy 0.067%\n",
      "Epoch 8, Batch 400, LR 2.005898 Loss 19.062301, Accuracy 0.066%\n",
      "Epoch 8, Batch 401, LR 2.006139 Loss 19.062225, Accuracy 0.070%\n",
      "Epoch 8, Batch 402, LR 2.006380 Loss 19.062136, Accuracy 0.070%\n",
      "Epoch 8, Batch 403, LR 2.006621 Loss 19.062179, Accuracy 0.070%\n",
      "Epoch 8, Batch 404, LR 2.006862 Loss 19.062410, Accuracy 0.070%\n",
      "Epoch 8, Batch 405, LR 2.007103 Loss 19.062496, Accuracy 0.069%\n",
      "Epoch 8, Batch 406, LR 2.007344 Loss 19.062235, Accuracy 0.069%\n",
      "Epoch 8, Batch 407, LR 2.007585 Loss 19.062561, Accuracy 0.069%\n",
      "Epoch 8, Batch 408, LR 2.007826 Loss 19.062699, Accuracy 0.069%\n",
      "Epoch 8, Batch 409, LR 2.008066 Loss 19.062282, Accuracy 0.071%\n",
      "Epoch 8, Batch 410, LR 2.008307 Loss 19.063113, Accuracy 0.071%\n",
      "Epoch 8, Batch 411, LR 2.008548 Loss 19.063222, Accuracy 0.070%\n",
      "Epoch 8, Batch 412, LR 2.008788 Loss 19.063082, Accuracy 0.070%\n",
      "Epoch 8, Batch 413, LR 2.009029 Loss 19.063713, Accuracy 0.070%\n",
      "Epoch 8, Batch 414, LR 2.009269 Loss 19.063301, Accuracy 0.070%\n",
      "Epoch 8, Batch 415, LR 2.009510 Loss 19.063371, Accuracy 0.070%\n",
      "Epoch 8, Batch 416, LR 2.009750 Loss 19.063503, Accuracy 0.069%\n",
      "Epoch 8, Batch 417, LR 2.009990 Loss 19.062933, Accuracy 0.069%\n",
      "Epoch 8, Batch 418, LR 2.010231 Loss 19.063000, Accuracy 0.071%\n",
      "Epoch 8, Batch 419, LR 2.010471 Loss 19.062754, Accuracy 0.071%\n",
      "Epoch 8, Batch 420, LR 2.010711 Loss 19.062786, Accuracy 0.073%\n",
      "Epoch 8, Batch 421, LR 2.010951 Loss 19.063008, Accuracy 0.072%\n",
      "Epoch 8, Batch 422, LR 2.011192 Loss 19.062647, Accuracy 0.072%\n",
      "Epoch 8, Batch 423, LR 2.011432 Loss 19.062653, Accuracy 0.072%\n",
      "Epoch 8, Batch 424, LR 2.011672 Loss 19.062790, Accuracy 0.072%\n",
      "Epoch 8, Batch 425, LR 2.011912 Loss 19.062324, Accuracy 0.072%\n",
      "Epoch 8, Batch 426, LR 2.012152 Loss 19.062001, Accuracy 0.072%\n",
      "Epoch 8, Batch 427, LR 2.012392 Loss 19.061609, Accuracy 0.073%\n",
      "Epoch 8, Batch 428, LR 2.012631 Loss 19.061948, Accuracy 0.073%\n",
      "Epoch 8, Batch 429, LR 2.012871 Loss 19.062008, Accuracy 0.073%\n",
      "Epoch 8, Batch 430, LR 2.013111 Loss 19.062100, Accuracy 0.073%\n",
      "Epoch 8, Batch 431, LR 2.013351 Loss 19.062247, Accuracy 0.073%\n",
      "Epoch 8, Batch 432, LR 2.013590 Loss 19.062559, Accuracy 0.072%\n",
      "Epoch 8, Batch 433, LR 2.013830 Loss 19.062539, Accuracy 0.074%\n",
      "Epoch 8, Batch 434, LR 2.014070 Loss 19.062691, Accuracy 0.076%\n",
      "Epoch 8, Batch 435, LR 2.014309 Loss 19.062830, Accuracy 0.075%\n",
      "Epoch 8, Batch 436, LR 2.014549 Loss 19.063467, Accuracy 0.075%\n",
      "Epoch 8, Batch 437, LR 2.014788 Loss 19.063547, Accuracy 0.075%\n",
      "Epoch 8, Batch 438, LR 2.015028 Loss 19.063785, Accuracy 0.075%\n",
      "Epoch 8, Batch 439, LR 2.015267 Loss 19.063633, Accuracy 0.075%\n",
      "Epoch 8, Batch 440, LR 2.015506 Loss 19.063309, Accuracy 0.075%\n",
      "Epoch 8, Batch 441, LR 2.015746 Loss 19.063644, Accuracy 0.074%\n",
      "Epoch 8, Batch 442, LR 2.015985 Loss 19.063452, Accuracy 0.076%\n",
      "Epoch 8, Batch 443, LR 2.016224 Loss 19.063496, Accuracy 0.076%\n",
      "Epoch 8, Batch 444, LR 2.016463 Loss 19.063273, Accuracy 0.076%\n",
      "Epoch 8, Batch 445, LR 2.016703 Loss 19.063709, Accuracy 0.075%\n",
      "Epoch 8, Batch 446, LR 2.016942 Loss 19.063871, Accuracy 0.075%\n",
      "Epoch 8, Batch 447, LR 2.017181 Loss 19.063332, Accuracy 0.077%\n",
      "Epoch 8, Batch 448, LR 2.017420 Loss 19.063327, Accuracy 0.077%\n",
      "Epoch 8, Batch 449, LR 2.017659 Loss 19.063937, Accuracy 0.077%\n",
      "Epoch 8, Batch 450, LR 2.017898 Loss 19.063580, Accuracy 0.076%\n",
      "Epoch 8, Batch 451, LR 2.018136 Loss 19.063783, Accuracy 0.076%\n",
      "Epoch 8, Batch 452, LR 2.018375 Loss 19.063730, Accuracy 0.076%\n",
      "Epoch 8, Batch 453, LR 2.018614 Loss 19.062920, Accuracy 0.076%\n",
      "Epoch 8, Batch 454, LR 2.018853 Loss 19.063050, Accuracy 0.076%\n",
      "Epoch 8, Batch 455, LR 2.019091 Loss 19.062876, Accuracy 0.076%\n",
      "Epoch 8, Batch 456, LR 2.019330 Loss 19.062971, Accuracy 0.075%\n",
      "Epoch 8, Batch 457, LR 2.019569 Loss 19.063423, Accuracy 0.075%\n",
      "Epoch 8, Batch 458, LR 2.019807 Loss 19.063259, Accuracy 0.075%\n",
      "Epoch 8, Batch 459, LR 2.020046 Loss 19.063318, Accuracy 0.078%\n",
      "Epoch 8, Batch 460, LR 2.020284 Loss 19.063379, Accuracy 0.078%\n",
      "Epoch 8, Batch 461, LR 2.020523 Loss 19.063265, Accuracy 0.078%\n",
      "Epoch 8, Batch 462, LR 2.020761 Loss 19.063174, Accuracy 0.078%\n",
      "Epoch 8, Batch 463, LR 2.020999 Loss 19.062955, Accuracy 0.078%\n",
      "Epoch 8, Batch 464, LR 2.021238 Loss 19.062866, Accuracy 0.077%\n",
      "Epoch 8, Batch 465, LR 2.021476 Loss 19.062937, Accuracy 0.077%\n",
      "Epoch 8, Batch 466, LR 2.021714 Loss 19.063014, Accuracy 0.077%\n",
      "Epoch 8, Batch 467, LR 2.021952 Loss 19.063441, Accuracy 0.077%\n",
      "Epoch 8, Batch 468, LR 2.022190 Loss 19.063313, Accuracy 0.078%\n",
      "Epoch 8, Batch 469, LR 2.022428 Loss 19.063450, Accuracy 0.078%\n",
      "Epoch 8, Batch 470, LR 2.022666 Loss 19.063442, Accuracy 0.078%\n",
      "Epoch 8, Batch 471, LR 2.022904 Loss 19.063176, Accuracy 0.080%\n",
      "Epoch 8, Batch 472, LR 2.023142 Loss 19.062982, Accuracy 0.079%\n",
      "Epoch 8, Batch 473, LR 2.023380 Loss 19.062999, Accuracy 0.079%\n",
      "Epoch 8, Batch 474, LR 2.023618 Loss 19.062921, Accuracy 0.079%\n",
      "Epoch 8, Batch 475, LR 2.023856 Loss 19.062977, Accuracy 0.079%\n",
      "Epoch 8, Batch 476, LR 2.024094 Loss 19.063103, Accuracy 0.079%\n",
      "Epoch 8, Batch 477, LR 2.024331 Loss 19.063251, Accuracy 0.079%\n",
      "Epoch 8, Batch 478, LR 2.024569 Loss 19.063505, Accuracy 0.078%\n",
      "Epoch 8, Batch 479, LR 2.024807 Loss 19.063537, Accuracy 0.078%\n",
      "Epoch 8, Batch 480, LR 2.025044 Loss 19.063288, Accuracy 0.078%\n",
      "Epoch 8, Batch 481, LR 2.025282 Loss 19.063268, Accuracy 0.078%\n",
      "Epoch 8, Batch 482, LR 2.025519 Loss 19.063205, Accuracy 0.078%\n",
      "Epoch 8, Batch 483, LR 2.025757 Loss 19.063259, Accuracy 0.078%\n",
      "Epoch 8, Batch 484, LR 2.025994 Loss 19.063458, Accuracy 0.077%\n",
      "Epoch 8, Batch 485, LR 2.026231 Loss 19.063392, Accuracy 0.077%\n",
      "Epoch 8, Batch 486, LR 2.026469 Loss 19.063416, Accuracy 0.077%\n",
      "Epoch 8, Batch 487, LR 2.026706 Loss 19.063333, Accuracy 0.077%\n",
      "Epoch 8, Batch 488, LR 2.026943 Loss 19.063348, Accuracy 0.077%\n",
      "Epoch 8, Batch 489, LR 2.027180 Loss 19.063570, Accuracy 0.077%\n",
      "Epoch 8, Batch 490, LR 2.027418 Loss 19.063531, Accuracy 0.077%\n",
      "Epoch 8, Batch 491, LR 2.027655 Loss 19.063291, Accuracy 0.076%\n",
      "Epoch 8, Batch 492, LR 2.027892 Loss 19.063280, Accuracy 0.076%\n",
      "Epoch 8, Batch 493, LR 2.028129 Loss 19.062725, Accuracy 0.076%\n",
      "Epoch 8, Batch 494, LR 2.028366 Loss 19.063114, Accuracy 0.076%\n",
      "Epoch 8, Batch 495, LR 2.028602 Loss 19.063278, Accuracy 0.076%\n",
      "Epoch 8, Batch 496, LR 2.028839 Loss 19.063697, Accuracy 0.076%\n",
      "Epoch 8, Batch 497, LR 2.029076 Loss 19.063618, Accuracy 0.075%\n",
      "Epoch 8, Batch 498, LR 2.029313 Loss 19.063820, Accuracy 0.075%\n",
      "Epoch 8, Batch 499, LR 2.029550 Loss 19.063669, Accuracy 0.075%\n",
      "Epoch 8, Batch 500, LR 2.029786 Loss 19.063609, Accuracy 0.075%\n",
      "Epoch 8, Batch 501, LR 2.030023 Loss 19.063672, Accuracy 0.075%\n",
      "Epoch 8, Batch 502, LR 2.030260 Loss 19.064199, Accuracy 0.075%\n",
      "Epoch 8, Batch 503, LR 2.030496 Loss 19.064020, Accuracy 0.075%\n",
      "Epoch 8, Batch 504, LR 2.030733 Loss 19.063984, Accuracy 0.074%\n",
      "Epoch 8, Batch 505, LR 2.030969 Loss 19.064061, Accuracy 0.074%\n",
      "Epoch 8, Batch 506, LR 2.031206 Loss 19.063832, Accuracy 0.076%\n",
      "Epoch 8, Batch 507, LR 2.031442 Loss 19.064353, Accuracy 0.076%\n",
      "Epoch 8, Batch 508, LR 2.031678 Loss 19.064400, Accuracy 0.075%\n",
      "Epoch 8, Batch 509, LR 2.031914 Loss 19.064338, Accuracy 0.075%\n",
      "Epoch 8, Batch 510, LR 2.032151 Loss 19.063968, Accuracy 0.075%\n",
      "Epoch 8, Batch 511, LR 2.032387 Loss 19.063894, Accuracy 0.075%\n",
      "Epoch 8, Batch 512, LR 2.032623 Loss 19.063865, Accuracy 0.075%\n",
      "Epoch 8, Batch 513, LR 2.032859 Loss 19.063718, Accuracy 0.075%\n",
      "Epoch 8, Batch 514, LR 2.033095 Loss 19.063923, Accuracy 0.074%\n",
      "Epoch 8, Batch 515, LR 2.033331 Loss 19.063829, Accuracy 0.074%\n",
      "Epoch 8, Batch 516, LR 2.033567 Loss 19.063637, Accuracy 0.076%\n",
      "Epoch 8, Batch 517, LR 2.033803 Loss 19.063601, Accuracy 0.076%\n",
      "Epoch 8, Batch 518, LR 2.034039 Loss 19.063851, Accuracy 0.075%\n",
      "Epoch 8, Batch 519, LR 2.034275 Loss 19.064284, Accuracy 0.075%\n",
      "Epoch 8, Batch 520, LR 2.034511 Loss 19.064153, Accuracy 0.075%\n",
      "Epoch 8, Batch 521, LR 2.034746 Loss 19.063950, Accuracy 0.075%\n",
      "Epoch 8, Batch 522, LR 2.034982 Loss 19.063896, Accuracy 0.075%\n",
      "Epoch 8, Batch 523, LR 2.035218 Loss 19.063820, Accuracy 0.075%\n",
      "Epoch 8, Batch 524, LR 2.035453 Loss 19.063853, Accuracy 0.075%\n",
      "Epoch 8, Batch 525, LR 2.035689 Loss 19.063554, Accuracy 0.074%\n",
      "Epoch 8, Batch 526, LR 2.035924 Loss 19.063663, Accuracy 0.074%\n",
      "Epoch 8, Batch 527, LR 2.036160 Loss 19.063614, Accuracy 0.074%\n",
      "Epoch 8, Batch 528, LR 2.036395 Loss 19.063593, Accuracy 0.074%\n",
      "Epoch 8, Batch 529, LR 2.036631 Loss 19.063583, Accuracy 0.074%\n",
      "Epoch 8, Batch 530, LR 2.036866 Loss 19.063371, Accuracy 0.074%\n",
      "Epoch 8, Batch 531, LR 2.037101 Loss 19.063434, Accuracy 0.074%\n",
      "Epoch 8, Batch 532, LR 2.037336 Loss 19.063411, Accuracy 0.073%\n",
      "Epoch 8, Batch 533, LR 2.037572 Loss 19.063351, Accuracy 0.073%\n",
      "Epoch 8, Batch 534, LR 2.037807 Loss 19.063352, Accuracy 0.073%\n",
      "Epoch 8, Batch 535, LR 2.038042 Loss 19.063690, Accuracy 0.073%\n",
      "Epoch 8, Batch 536, LR 2.038277 Loss 19.063864, Accuracy 0.073%\n",
      "Epoch 8, Batch 537, LR 2.038512 Loss 19.063658, Accuracy 0.073%\n",
      "Epoch 8, Batch 538, LR 2.038747 Loss 19.063629, Accuracy 0.073%\n",
      "Epoch 8, Batch 539, LR 2.038982 Loss 19.063677, Accuracy 0.074%\n",
      "Epoch 8, Batch 540, LR 2.039217 Loss 19.063647, Accuracy 0.074%\n",
      "Epoch 8, Batch 541, LR 2.039452 Loss 19.063629, Accuracy 0.074%\n",
      "Epoch 8, Batch 542, LR 2.039686 Loss 19.063782, Accuracy 0.074%\n",
      "Epoch 8, Batch 543, LR 2.039921 Loss 19.063759, Accuracy 0.073%\n",
      "Epoch 8, Batch 544, LR 2.040156 Loss 19.063883, Accuracy 0.075%\n",
      "Epoch 8, Batch 545, LR 2.040390 Loss 19.063966, Accuracy 0.076%\n",
      "Epoch 8, Batch 546, LR 2.040625 Loss 19.064090, Accuracy 0.076%\n",
      "Epoch 8, Batch 547, LR 2.040860 Loss 19.063772, Accuracy 0.076%\n",
      "Epoch 8, Batch 548, LR 2.041094 Loss 19.064098, Accuracy 0.076%\n",
      "Epoch 8, Batch 549, LR 2.041329 Loss 19.064321, Accuracy 0.075%\n",
      "Epoch 8, Batch 550, LR 2.041563 Loss 19.064686, Accuracy 0.075%\n",
      "Epoch 8, Batch 551, LR 2.041797 Loss 19.064394, Accuracy 0.075%\n",
      "Epoch 8, Batch 552, LR 2.042032 Loss 19.064410, Accuracy 0.075%\n",
      "Epoch 8, Batch 553, LR 2.042266 Loss 19.064596, Accuracy 0.075%\n",
      "Epoch 8, Batch 554, LR 2.042500 Loss 19.063957, Accuracy 0.075%\n",
      "Epoch 8, Batch 555, LR 2.042734 Loss 19.063990, Accuracy 0.075%\n",
      "Epoch 8, Batch 556, LR 2.042969 Loss 19.064224, Accuracy 0.074%\n",
      "Epoch 8, Batch 557, LR 2.043203 Loss 19.064555, Accuracy 0.074%\n",
      "Epoch 8, Batch 558, LR 2.043437 Loss 19.064578, Accuracy 0.074%\n",
      "Epoch 8, Batch 559, LR 2.043671 Loss 19.064723, Accuracy 0.074%\n",
      "Epoch 8, Batch 560, LR 2.043905 Loss 19.064686, Accuracy 0.074%\n",
      "Epoch 8, Batch 561, LR 2.044139 Loss 19.065004, Accuracy 0.074%\n",
      "Epoch 8, Batch 562, LR 2.044373 Loss 19.065080, Accuracy 0.074%\n",
      "Epoch 8, Batch 563, LR 2.044606 Loss 19.064830, Accuracy 0.075%\n",
      "Epoch 8, Batch 564, LR 2.044840 Loss 19.064805, Accuracy 0.075%\n",
      "Epoch 8, Batch 565, LR 2.045074 Loss 19.065101, Accuracy 0.075%\n",
      "Epoch 8, Batch 566, LR 2.045308 Loss 19.065003, Accuracy 0.075%\n",
      "Epoch 8, Batch 567, LR 2.045541 Loss 19.065102, Accuracy 0.074%\n",
      "Epoch 8, Batch 568, LR 2.045775 Loss 19.064956, Accuracy 0.074%\n",
      "Epoch 8, Batch 569, LR 2.046008 Loss 19.065179, Accuracy 0.074%\n",
      "Epoch 8, Batch 570, LR 2.046242 Loss 19.065170, Accuracy 0.074%\n",
      "Epoch 8, Batch 571, LR 2.046475 Loss 19.065248, Accuracy 0.074%\n",
      "Epoch 8, Batch 572, LR 2.046709 Loss 19.065491, Accuracy 0.074%\n",
      "Epoch 8, Batch 573, LR 2.046942 Loss 19.065310, Accuracy 0.074%\n",
      "Epoch 8, Batch 574, LR 2.047175 Loss 19.065076, Accuracy 0.073%\n",
      "Epoch 8, Batch 575, LR 2.047409 Loss 19.064772, Accuracy 0.073%\n",
      "Epoch 8, Batch 576, LR 2.047642 Loss 19.064843, Accuracy 0.073%\n",
      "Epoch 8, Batch 577, LR 2.047875 Loss 19.064813, Accuracy 0.073%\n",
      "Epoch 8, Batch 578, LR 2.048108 Loss 19.064734, Accuracy 0.073%\n",
      "Epoch 8, Batch 579, LR 2.048341 Loss 19.064836, Accuracy 0.073%\n",
      "Epoch 8, Batch 580, LR 2.048574 Loss 19.064939, Accuracy 0.073%\n",
      "Epoch 8, Batch 581, LR 2.048807 Loss 19.065238, Accuracy 0.073%\n",
      "Epoch 8, Batch 582, LR 2.049040 Loss 19.065389, Accuracy 0.074%\n",
      "Epoch 8, Batch 583, LR 2.049273 Loss 19.065871, Accuracy 0.074%\n",
      "Epoch 8, Batch 584, LR 2.049506 Loss 19.065588, Accuracy 0.074%\n",
      "Epoch 8, Batch 585, LR 2.049739 Loss 19.065646, Accuracy 0.073%\n",
      "Epoch 8, Batch 586, LR 2.049972 Loss 19.065669, Accuracy 0.073%\n",
      "Epoch 8, Batch 587, LR 2.050205 Loss 19.065797, Accuracy 0.073%\n",
      "Epoch 8, Batch 588, LR 2.050437 Loss 19.065609, Accuracy 0.073%\n",
      "Epoch 8, Batch 589, LR 2.050670 Loss 19.065305, Accuracy 0.074%\n",
      "Epoch 8, Batch 590, LR 2.050902 Loss 19.065152, Accuracy 0.074%\n",
      "Epoch 8, Batch 591, LR 2.051135 Loss 19.065262, Accuracy 0.074%\n",
      "Epoch 8, Batch 592, LR 2.051367 Loss 19.065382, Accuracy 0.074%\n",
      "Epoch 8, Batch 593, LR 2.051600 Loss 19.065332, Accuracy 0.074%\n",
      "Epoch 8, Batch 594, LR 2.051832 Loss 19.065504, Accuracy 0.074%\n",
      "Epoch 8, Batch 595, LR 2.052065 Loss 19.065669, Accuracy 0.074%\n",
      "Epoch 8, Batch 596, LR 2.052297 Loss 19.065432, Accuracy 0.073%\n",
      "Epoch 8, Batch 597, LR 2.052529 Loss 19.065579, Accuracy 0.073%\n",
      "Epoch 8, Batch 598, LR 2.052761 Loss 19.065643, Accuracy 0.073%\n",
      "Epoch 8, Batch 599, LR 2.052994 Loss 19.065542, Accuracy 0.073%\n",
      "Epoch 8, Batch 600, LR 2.053226 Loss 19.065590, Accuracy 0.073%\n",
      "Epoch 8, Batch 601, LR 2.053458 Loss 19.065745, Accuracy 0.073%\n",
      "Epoch 8, Batch 602, LR 2.053690 Loss 19.065366, Accuracy 0.074%\n",
      "Epoch 8, Batch 603, LR 2.053922 Loss 19.065070, Accuracy 0.074%\n",
      "Epoch 8, Batch 604, LR 2.054154 Loss 19.065106, Accuracy 0.074%\n",
      "Epoch 8, Batch 605, LR 2.054386 Loss 19.064888, Accuracy 0.074%\n",
      "Epoch 8, Batch 606, LR 2.054618 Loss 19.064743, Accuracy 0.073%\n",
      "Epoch 8, Batch 607, LR 2.054849 Loss 19.064580, Accuracy 0.073%\n",
      "Epoch 8, Batch 608, LR 2.055081 Loss 19.064554, Accuracy 0.073%\n",
      "Epoch 8, Batch 609, LR 2.055313 Loss 19.065039, Accuracy 0.073%\n",
      "Epoch 8, Batch 610, LR 2.055544 Loss 19.064926, Accuracy 0.073%\n",
      "Epoch 8, Batch 611, LR 2.055776 Loss 19.064735, Accuracy 0.073%\n",
      "Epoch 8, Batch 612, LR 2.056008 Loss 19.064727, Accuracy 0.073%\n",
      "Epoch 8, Batch 613, LR 2.056239 Loss 19.064121, Accuracy 0.075%\n",
      "Epoch 8, Batch 614, LR 2.056471 Loss 19.064439, Accuracy 0.075%\n",
      "Epoch 8, Batch 615, LR 2.056702 Loss 19.064297, Accuracy 0.075%\n",
      "Epoch 8, Batch 616, LR 2.056933 Loss 19.064618, Accuracy 0.075%\n",
      "Epoch 8, Batch 617, LR 2.057165 Loss 19.064329, Accuracy 0.075%\n",
      "Epoch 8, Batch 618, LR 2.057396 Loss 19.064556, Accuracy 0.076%\n",
      "Epoch 8, Batch 619, LR 2.057627 Loss 19.064550, Accuracy 0.076%\n",
      "Epoch 8, Batch 620, LR 2.057859 Loss 19.064224, Accuracy 0.076%\n",
      "Epoch 8, Batch 621, LR 2.058090 Loss 19.064436, Accuracy 0.075%\n",
      "Epoch 8, Batch 622, LR 2.058321 Loss 19.064525, Accuracy 0.075%\n",
      "Epoch 8, Batch 623, LR 2.058552 Loss 19.064598, Accuracy 0.075%\n",
      "Epoch 8, Batch 624, LR 2.058783 Loss 19.064692, Accuracy 0.076%\n",
      "Epoch 8, Batch 625, LR 2.059014 Loss 19.064336, Accuracy 0.076%\n",
      "Epoch 8, Batch 626, LR 2.059245 Loss 19.064347, Accuracy 0.076%\n",
      "Epoch 8, Batch 627, LR 2.059476 Loss 19.064258, Accuracy 0.076%\n",
      "Epoch 8, Batch 628, LR 2.059706 Loss 19.063975, Accuracy 0.076%\n",
      "Epoch 8, Batch 629, LR 2.059937 Loss 19.064330, Accuracy 0.076%\n",
      "Epoch 8, Batch 630, LR 2.060168 Loss 19.064280, Accuracy 0.076%\n",
      "Epoch 8, Batch 631, LR 2.060399 Loss 19.064298, Accuracy 0.076%\n",
      "Epoch 8, Batch 632, LR 2.060629 Loss 19.064241, Accuracy 0.075%\n",
      "Epoch 8, Batch 633, LR 2.060860 Loss 19.064232, Accuracy 0.075%\n",
      "Epoch 8, Batch 634, LR 2.061090 Loss 19.064260, Accuracy 0.076%\n",
      "Epoch 8, Batch 635, LR 2.061321 Loss 19.064254, Accuracy 0.076%\n",
      "Epoch 8, Batch 636, LR 2.061551 Loss 19.064280, Accuracy 0.076%\n",
      "Epoch 8, Batch 637, LR 2.061782 Loss 19.064562, Accuracy 0.076%\n",
      "Epoch 8, Batch 638, LR 2.062012 Loss 19.064519, Accuracy 0.076%\n",
      "Epoch 8, Batch 639, LR 2.062242 Loss 19.064832, Accuracy 0.076%\n",
      "Epoch 8, Batch 640, LR 2.062473 Loss 19.064390, Accuracy 0.076%\n",
      "Epoch 8, Batch 641, LR 2.062703 Loss 19.064349, Accuracy 0.076%\n",
      "Epoch 8, Batch 642, LR 2.062933 Loss 19.064422, Accuracy 0.075%\n",
      "Epoch 8, Batch 643, LR 2.063163 Loss 19.064449, Accuracy 0.075%\n",
      "Epoch 8, Batch 644, LR 2.063393 Loss 19.064425, Accuracy 0.075%\n",
      "Epoch 8, Batch 645, LR 2.063623 Loss 19.064447, Accuracy 0.075%\n",
      "Epoch 8, Batch 646, LR 2.063853 Loss 19.064342, Accuracy 0.075%\n",
      "Epoch 8, Batch 647, LR 2.064083 Loss 19.064273, Accuracy 0.075%\n",
      "Epoch 8, Batch 648, LR 2.064313 Loss 19.064268, Accuracy 0.075%\n",
      "Epoch 8, Batch 649, LR 2.064543 Loss 19.064286, Accuracy 0.075%\n",
      "Epoch 8, Batch 650, LR 2.064773 Loss 19.064364, Accuracy 0.076%\n",
      "Epoch 8, Batch 651, LR 2.065002 Loss 19.064227, Accuracy 0.076%\n",
      "Epoch 8, Batch 652, LR 2.065232 Loss 19.064263, Accuracy 0.075%\n",
      "Epoch 8, Batch 653, LR 2.065462 Loss 19.063973, Accuracy 0.075%\n",
      "Epoch 8, Batch 654, LR 2.065691 Loss 19.063739, Accuracy 0.075%\n",
      "Epoch 8, Batch 655, LR 2.065921 Loss 19.063778, Accuracy 0.075%\n",
      "Epoch 8, Batch 656, LR 2.066150 Loss 19.063716, Accuracy 0.075%\n",
      "Epoch 8, Batch 657, LR 2.066380 Loss 19.063713, Accuracy 0.075%\n",
      "Epoch 8, Batch 658, LR 2.066609 Loss 19.063650, Accuracy 0.075%\n",
      "Epoch 8, Batch 659, LR 2.066839 Loss 19.063715, Accuracy 0.075%\n",
      "Epoch 8, Batch 660, LR 2.067068 Loss 19.063635, Accuracy 0.075%\n",
      "Epoch 8, Batch 661, LR 2.067297 Loss 19.063637, Accuracy 0.074%\n",
      "Epoch 8, Batch 662, LR 2.067527 Loss 19.063475, Accuracy 0.074%\n",
      "Epoch 8, Batch 663, LR 2.067756 Loss 19.063750, Accuracy 0.074%\n",
      "Epoch 8, Batch 664, LR 2.067985 Loss 19.063769, Accuracy 0.074%\n",
      "Epoch 8, Batch 665, LR 2.068214 Loss 19.063738, Accuracy 0.074%\n",
      "Epoch 8, Batch 666, LR 2.068443 Loss 19.063665, Accuracy 0.074%\n",
      "Epoch 8, Batch 667, LR 2.068672 Loss 19.063458, Accuracy 0.074%\n",
      "Epoch 8, Batch 668, LR 2.068901 Loss 19.063429, Accuracy 0.074%\n",
      "Epoch 8, Batch 669, LR 2.069130 Loss 19.063490, Accuracy 0.074%\n",
      "Epoch 8, Batch 670, LR 2.069359 Loss 19.063359, Accuracy 0.073%\n",
      "Epoch 8, Batch 671, LR 2.069587 Loss 19.063551, Accuracy 0.073%\n",
      "Epoch 8, Batch 672, LR 2.069816 Loss 19.063469, Accuracy 0.073%\n",
      "Epoch 8, Batch 673, LR 2.070045 Loss 19.063408, Accuracy 0.073%\n",
      "Epoch 8, Batch 674, LR 2.070274 Loss 19.063224, Accuracy 0.073%\n",
      "Epoch 8, Batch 675, LR 2.070502 Loss 19.063358, Accuracy 0.073%\n",
      "Epoch 8, Batch 676, LR 2.070731 Loss 19.063499, Accuracy 0.073%\n",
      "Epoch 8, Batch 677, LR 2.070959 Loss 19.063361, Accuracy 0.073%\n",
      "Epoch 8, Batch 678, LR 2.071188 Loss 19.063262, Accuracy 0.073%\n",
      "Epoch 8, Batch 679, LR 2.071416 Loss 19.063456, Accuracy 0.074%\n",
      "Epoch 8, Batch 680, LR 2.071644 Loss 19.063577, Accuracy 0.074%\n",
      "Epoch 8, Batch 681, LR 2.071873 Loss 19.063694, Accuracy 0.073%\n",
      "Epoch 8, Batch 682, LR 2.072101 Loss 19.063970, Accuracy 0.073%\n",
      "Epoch 8, Batch 683, LR 2.072329 Loss 19.063792, Accuracy 0.073%\n",
      "Epoch 8, Batch 684, LR 2.072558 Loss 19.063907, Accuracy 0.073%\n",
      "Epoch 8, Batch 685, LR 2.072786 Loss 19.063820, Accuracy 0.073%\n",
      "Epoch 8, Batch 686, LR 2.073014 Loss 19.063935, Accuracy 0.073%\n",
      "Epoch 8, Batch 687, LR 2.073242 Loss 19.064287, Accuracy 0.073%\n",
      "Epoch 8, Batch 688, LR 2.073470 Loss 19.064251, Accuracy 0.073%\n",
      "Epoch 8, Batch 689, LR 2.073698 Loss 19.064149, Accuracy 0.074%\n",
      "Epoch 8, Batch 690, LR 2.073926 Loss 19.064267, Accuracy 0.074%\n",
      "Epoch 8, Batch 691, LR 2.074153 Loss 19.064572, Accuracy 0.073%\n",
      "Epoch 8, Batch 692, LR 2.074381 Loss 19.064461, Accuracy 0.073%\n",
      "Epoch 8, Batch 693, LR 2.074609 Loss 19.064706, Accuracy 0.073%\n",
      "Epoch 8, Batch 694, LR 2.074837 Loss 19.064536, Accuracy 0.073%\n",
      "Epoch 8, Batch 695, LR 2.075064 Loss 19.064372, Accuracy 0.073%\n",
      "Epoch 8, Batch 696, LR 2.075292 Loss 19.064581, Accuracy 0.074%\n",
      "Epoch 8, Batch 697, LR 2.075520 Loss 19.064477, Accuracy 0.074%\n",
      "Epoch 8, Batch 698, LR 2.075747 Loss 19.064275, Accuracy 0.075%\n",
      "Epoch 8, Batch 699, LR 2.075975 Loss 19.064385, Accuracy 0.075%\n",
      "Epoch 8, Batch 700, LR 2.076202 Loss 19.064102, Accuracy 0.075%\n",
      "Epoch 8, Batch 701, LR 2.076429 Loss 19.064068, Accuracy 0.075%\n",
      "Epoch 8, Batch 702, LR 2.076657 Loss 19.063909, Accuracy 0.075%\n",
      "Epoch 8, Batch 703, LR 2.076884 Loss 19.063457, Accuracy 0.074%\n",
      "Epoch 8, Batch 704, LR 2.077111 Loss 19.063478, Accuracy 0.074%\n",
      "Epoch 8, Batch 705, LR 2.077338 Loss 19.063556, Accuracy 0.075%\n",
      "Epoch 8, Batch 706, LR 2.077566 Loss 19.063405, Accuracy 0.075%\n",
      "Epoch 8, Batch 707, LR 2.077793 Loss 19.063448, Accuracy 0.075%\n",
      "Epoch 8, Batch 708, LR 2.078020 Loss 19.063356, Accuracy 0.075%\n",
      "Epoch 8, Batch 709, LR 2.078247 Loss 19.063367, Accuracy 0.075%\n",
      "Epoch 8, Batch 710, LR 2.078474 Loss 19.063315, Accuracy 0.075%\n",
      "Epoch 8, Batch 711, LR 2.078700 Loss 19.063165, Accuracy 0.075%\n",
      "Epoch 8, Batch 712, LR 2.078927 Loss 19.062985, Accuracy 0.075%\n",
      "Epoch 8, Batch 713, LR 2.079154 Loss 19.062972, Accuracy 0.075%\n",
      "Epoch 8, Batch 714, LR 2.079381 Loss 19.062901, Accuracy 0.074%\n",
      "Epoch 8, Batch 715, LR 2.079608 Loss 19.062909, Accuracy 0.074%\n",
      "Epoch 8, Batch 716, LR 2.079834 Loss 19.063059, Accuracy 0.074%\n",
      "Epoch 8, Batch 717, LR 2.080061 Loss 19.063003, Accuracy 0.074%\n",
      "Epoch 8, Batch 718, LR 2.080287 Loss 19.063007, Accuracy 0.074%\n",
      "Epoch 8, Batch 719, LR 2.080514 Loss 19.063173, Accuracy 0.074%\n",
      "Epoch 8, Batch 720, LR 2.080740 Loss 19.063061, Accuracy 0.074%\n",
      "Epoch 8, Batch 721, LR 2.080967 Loss 19.063055, Accuracy 0.074%\n",
      "Epoch 8, Batch 722, LR 2.081193 Loss 19.063169, Accuracy 0.074%\n",
      "Epoch 8, Batch 723, LR 2.081419 Loss 19.063539, Accuracy 0.075%\n",
      "Epoch 8, Batch 724, LR 2.081646 Loss 19.063748, Accuracy 0.074%\n",
      "Epoch 8, Batch 725, LR 2.081872 Loss 19.063553, Accuracy 0.074%\n",
      "Epoch 8, Batch 726, LR 2.082098 Loss 19.063611, Accuracy 0.074%\n",
      "Epoch 8, Batch 727, LR 2.082324 Loss 19.063598, Accuracy 0.074%\n",
      "Epoch 8, Batch 728, LR 2.082550 Loss 19.063461, Accuracy 0.075%\n",
      "Epoch 8, Batch 729, LR 2.082776 Loss 19.063399, Accuracy 0.075%\n",
      "Epoch 8, Batch 730, LR 2.083002 Loss 19.063424, Accuracy 0.076%\n",
      "Epoch 8, Batch 731, LR 2.083228 Loss 19.063679, Accuracy 0.076%\n",
      "Epoch 8, Batch 732, LR 2.083454 Loss 19.063731, Accuracy 0.076%\n",
      "Epoch 8, Batch 733, LR 2.083680 Loss 19.063864, Accuracy 0.076%\n",
      "Epoch 8, Batch 734, LR 2.083906 Loss 19.063825, Accuracy 0.076%\n",
      "Epoch 8, Batch 735, LR 2.084132 Loss 19.063875, Accuracy 0.075%\n",
      "Epoch 8, Batch 736, LR 2.084357 Loss 19.064002, Accuracy 0.076%\n",
      "Epoch 8, Batch 737, LR 2.084583 Loss 19.064402, Accuracy 0.076%\n",
      "Epoch 8, Batch 738, LR 2.084809 Loss 19.064510, Accuracy 0.076%\n",
      "Epoch 8, Batch 739, LR 2.085034 Loss 19.064322, Accuracy 0.077%\n",
      "Epoch 8, Batch 740, LR 2.085260 Loss 19.064125, Accuracy 0.077%\n",
      "Epoch 8, Batch 741, LR 2.085485 Loss 19.064365, Accuracy 0.077%\n",
      "Epoch 8, Batch 742, LR 2.085710 Loss 19.064384, Accuracy 0.077%\n",
      "Epoch 8, Batch 743, LR 2.085936 Loss 19.064817, Accuracy 0.077%\n",
      "Epoch 8, Batch 744, LR 2.086161 Loss 19.064876, Accuracy 0.077%\n",
      "Epoch 8, Batch 745, LR 2.086386 Loss 19.064823, Accuracy 0.077%\n",
      "Epoch 8, Batch 746, LR 2.086612 Loss 19.064926, Accuracy 0.076%\n",
      "Epoch 8, Batch 747, LR 2.086837 Loss 19.064997, Accuracy 0.076%\n",
      "Epoch 8, Batch 748, LR 2.087062 Loss 19.064793, Accuracy 0.076%\n",
      "Epoch 8, Batch 749, LR 2.087287 Loss 19.064883, Accuracy 0.076%\n",
      "Epoch 8, Batch 750, LR 2.087512 Loss 19.065155, Accuracy 0.076%\n",
      "Epoch 8, Batch 751, LR 2.087737 Loss 19.064919, Accuracy 0.076%\n",
      "Epoch 8, Batch 752, LR 2.087962 Loss 19.064949, Accuracy 0.076%\n",
      "Epoch 8, Batch 753, LR 2.088187 Loss 19.065051, Accuracy 0.077%\n",
      "Epoch 8, Batch 754, LR 2.088411 Loss 19.065011, Accuracy 0.077%\n",
      "Epoch 8, Batch 755, LR 2.088636 Loss 19.064908, Accuracy 0.077%\n",
      "Epoch 8, Batch 756, LR 2.088861 Loss 19.064644, Accuracy 0.076%\n",
      "Epoch 8, Batch 757, LR 2.089086 Loss 19.064693, Accuracy 0.076%\n",
      "Epoch 8, Batch 758, LR 2.089310 Loss 19.064557, Accuracy 0.076%\n",
      "Epoch 8, Batch 759, LR 2.089535 Loss 19.064765, Accuracy 0.076%\n",
      "Epoch 8, Batch 760, LR 2.089759 Loss 19.064760, Accuracy 0.076%\n",
      "Epoch 8, Batch 761, LR 2.089984 Loss 19.064631, Accuracy 0.077%\n",
      "Epoch 8, Batch 762, LR 2.090208 Loss 19.064494, Accuracy 0.077%\n",
      "Epoch 8, Batch 763, LR 2.090433 Loss 19.064579, Accuracy 0.077%\n",
      "Epoch 8, Batch 764, LR 2.090657 Loss 19.064429, Accuracy 0.077%\n",
      "Epoch 8, Batch 765, LR 2.090881 Loss 19.064344, Accuracy 0.077%\n",
      "Epoch 8, Batch 766, LR 2.091106 Loss 19.064210, Accuracy 0.076%\n",
      "Epoch 8, Batch 767, LR 2.091330 Loss 19.064217, Accuracy 0.076%\n",
      "Epoch 8, Batch 768, LR 2.091554 Loss 19.064165, Accuracy 0.076%\n",
      "Epoch 8, Batch 769, LR 2.091778 Loss 19.064150, Accuracy 0.076%\n",
      "Epoch 8, Batch 770, LR 2.092002 Loss 19.064071, Accuracy 0.076%\n",
      "Epoch 8, Batch 771, LR 2.092226 Loss 19.064104, Accuracy 0.076%\n",
      "Epoch 8, Batch 772, LR 2.092450 Loss 19.063851, Accuracy 0.076%\n",
      "Epoch 8, Batch 773, LR 2.092674 Loss 19.064025, Accuracy 0.076%\n",
      "Epoch 8, Batch 774, LR 2.092898 Loss 19.063839, Accuracy 0.076%\n",
      "Epoch 8, Batch 775, LR 2.093122 Loss 19.063623, Accuracy 0.076%\n",
      "Epoch 8, Batch 776, LR 2.093345 Loss 19.063743, Accuracy 0.076%\n",
      "Epoch 8, Batch 777, LR 2.093569 Loss 19.063752, Accuracy 0.075%\n",
      "Epoch 8, Batch 778, LR 2.093793 Loss 19.063424, Accuracy 0.075%\n",
      "Epoch 8, Batch 779, LR 2.094016 Loss 19.063393, Accuracy 0.075%\n",
      "Epoch 8, Batch 780, LR 2.094240 Loss 19.063435, Accuracy 0.075%\n",
      "Epoch 8, Batch 781, LR 2.094463 Loss 19.063573, Accuracy 0.075%\n",
      "Epoch 8, Batch 782, LR 2.094687 Loss 19.063735, Accuracy 0.075%\n",
      "Epoch 8, Batch 783, LR 2.094910 Loss 19.063684, Accuracy 0.075%\n",
      "Epoch 8, Batch 784, LR 2.095134 Loss 19.063451, Accuracy 0.075%\n",
      "Epoch 8, Batch 785, LR 2.095357 Loss 19.063463, Accuracy 0.075%\n",
      "Epoch 8, Batch 786, LR 2.095580 Loss 19.063524, Accuracy 0.075%\n",
      "Epoch 8, Batch 787, LR 2.095803 Loss 19.063661, Accuracy 0.074%\n",
      "Epoch 8, Batch 788, LR 2.096027 Loss 19.063574, Accuracy 0.074%\n",
      "Epoch 8, Batch 789, LR 2.096250 Loss 19.063541, Accuracy 0.074%\n",
      "Epoch 8, Batch 790, LR 2.096473 Loss 19.063462, Accuracy 0.074%\n",
      "Epoch 8, Batch 791, LR 2.096696 Loss 19.063604, Accuracy 0.075%\n",
      "Epoch 8, Batch 792, LR 2.096919 Loss 19.063514, Accuracy 0.075%\n",
      "Epoch 8, Batch 793, LR 2.097142 Loss 19.063709, Accuracy 0.075%\n",
      "Epoch 8, Batch 794, LR 2.097364 Loss 19.063789, Accuracy 0.075%\n",
      "Epoch 8, Batch 795, LR 2.097587 Loss 19.063889, Accuracy 0.075%\n",
      "Epoch 8, Batch 796, LR 2.097810 Loss 19.063665, Accuracy 0.075%\n",
      "Epoch 8, Batch 797, LR 2.098033 Loss 19.063439, Accuracy 0.074%\n",
      "Epoch 8, Batch 798, LR 2.098255 Loss 19.063491, Accuracy 0.074%\n",
      "Epoch 8, Batch 799, LR 2.098478 Loss 19.063461, Accuracy 0.074%\n",
      "Epoch 8, Batch 800, LR 2.098701 Loss 19.063521, Accuracy 0.074%\n",
      "Epoch 8, Batch 801, LR 2.098923 Loss 19.063665, Accuracy 0.074%\n",
      "Epoch 8, Batch 802, LR 2.099146 Loss 19.063580, Accuracy 0.074%\n",
      "Epoch 8, Batch 803, LR 2.099368 Loss 19.063386, Accuracy 0.074%\n",
      "Epoch 8, Batch 804, LR 2.099590 Loss 19.063509, Accuracy 0.074%\n",
      "Epoch 8, Batch 805, LR 2.099813 Loss 19.063395, Accuracy 0.074%\n",
      "Epoch 8, Batch 806, LR 2.100035 Loss 19.063455, Accuracy 0.074%\n",
      "Epoch 8, Batch 807, LR 2.100257 Loss 19.063261, Accuracy 0.074%\n",
      "Epoch 8, Batch 808, LR 2.100480 Loss 19.063310, Accuracy 0.073%\n",
      "Epoch 8, Batch 809, LR 2.100702 Loss 19.063548, Accuracy 0.073%\n",
      "Epoch 8, Batch 810, LR 2.100924 Loss 19.063566, Accuracy 0.073%\n",
      "Epoch 8, Batch 811, LR 2.101146 Loss 19.063442, Accuracy 0.073%\n",
      "Epoch 8, Batch 812, LR 2.101368 Loss 19.063499, Accuracy 0.073%\n",
      "Epoch 8, Batch 813, LR 2.101590 Loss 19.063607, Accuracy 0.074%\n",
      "Epoch 8, Batch 814, LR 2.101812 Loss 19.063750, Accuracy 0.074%\n",
      "Epoch 8, Batch 815, LR 2.102033 Loss 19.063964, Accuracy 0.074%\n",
      "Epoch 8, Batch 816, LR 2.102255 Loss 19.063956, Accuracy 0.074%\n",
      "Epoch 8, Batch 817, LR 2.102477 Loss 19.063880, Accuracy 0.074%\n",
      "Epoch 8, Batch 818, LR 2.102699 Loss 19.063865, Accuracy 0.074%\n",
      "Epoch 8, Batch 819, LR 2.102920 Loss 19.063820, Accuracy 0.073%\n",
      "Epoch 8, Batch 820, LR 2.103142 Loss 19.063893, Accuracy 0.073%\n",
      "Epoch 8, Batch 821, LR 2.103363 Loss 19.064065, Accuracy 0.073%\n",
      "Epoch 8, Batch 822, LR 2.103585 Loss 19.063997, Accuracy 0.073%\n",
      "Epoch 8, Batch 823, LR 2.103806 Loss 19.063873, Accuracy 0.074%\n",
      "Epoch 8, Batch 824, LR 2.104028 Loss 19.063965, Accuracy 0.074%\n",
      "Epoch 8, Batch 825, LR 2.104249 Loss 19.064248, Accuracy 0.075%\n",
      "Epoch 8, Batch 826, LR 2.104470 Loss 19.064333, Accuracy 0.075%\n",
      "Epoch 8, Batch 827, LR 2.104692 Loss 19.064306, Accuracy 0.076%\n",
      "Epoch 8, Batch 828, LR 2.104913 Loss 19.064394, Accuracy 0.075%\n",
      "Epoch 8, Batch 829, LR 2.105134 Loss 19.064331, Accuracy 0.075%\n",
      "Epoch 8, Batch 830, LR 2.105355 Loss 19.064520, Accuracy 0.075%\n",
      "Epoch 8, Batch 831, LR 2.105576 Loss 19.064573, Accuracy 0.075%\n",
      "Epoch 8, Batch 832, LR 2.105797 Loss 19.064390, Accuracy 0.075%\n",
      "Epoch 8, Batch 833, LR 2.106018 Loss 19.064560, Accuracy 0.075%\n",
      "Epoch 8, Batch 834, LR 2.106239 Loss 19.064585, Accuracy 0.075%\n",
      "Epoch 8, Batch 835, LR 2.106460 Loss 19.064616, Accuracy 0.075%\n",
      "Epoch 8, Batch 836, LR 2.106681 Loss 19.064712, Accuracy 0.075%\n",
      "Epoch 8, Batch 837, LR 2.106901 Loss 19.064806, Accuracy 0.075%\n",
      "Epoch 8, Batch 838, LR 2.107122 Loss 19.064853, Accuracy 0.075%\n",
      "Epoch 8, Batch 839, LR 2.107343 Loss 19.064837, Accuracy 0.074%\n",
      "Epoch 8, Batch 840, LR 2.107563 Loss 19.064829, Accuracy 0.075%\n",
      "Epoch 8, Batch 841, LR 2.107784 Loss 19.065115, Accuracy 0.075%\n",
      "Epoch 8, Batch 842, LR 2.108004 Loss 19.064955, Accuracy 0.075%\n",
      "Epoch 8, Batch 843, LR 2.108225 Loss 19.064952, Accuracy 0.075%\n",
      "Epoch 8, Batch 844, LR 2.108445 Loss 19.064905, Accuracy 0.075%\n",
      "Epoch 8, Batch 845, LR 2.108666 Loss 19.065082, Accuracy 0.075%\n",
      "Epoch 8, Batch 846, LR 2.108886 Loss 19.064810, Accuracy 0.075%\n",
      "Epoch 8, Batch 847, LR 2.109106 Loss 19.064741, Accuracy 0.075%\n",
      "Epoch 8, Batch 848, LR 2.109326 Loss 19.064700, Accuracy 0.075%\n",
      "Epoch 8, Batch 849, LR 2.109546 Loss 19.064856, Accuracy 0.075%\n",
      "Epoch 8, Batch 850, LR 2.109767 Loss 19.064956, Accuracy 0.074%\n",
      "Epoch 8, Batch 851, LR 2.109987 Loss 19.064960, Accuracy 0.074%\n",
      "Epoch 8, Batch 852, LR 2.110207 Loss 19.064985, Accuracy 0.074%\n",
      "Epoch 8, Batch 853, LR 2.110427 Loss 19.065113, Accuracy 0.074%\n",
      "Epoch 8, Batch 854, LR 2.110646 Loss 19.065094, Accuracy 0.074%\n",
      "Epoch 8, Batch 855, LR 2.110866 Loss 19.065418, Accuracy 0.075%\n",
      "Epoch 8, Batch 856, LR 2.111086 Loss 19.065332, Accuracy 0.075%\n",
      "Epoch 8, Batch 857, LR 2.111306 Loss 19.065594, Accuracy 0.075%\n",
      "Epoch 8, Batch 858, LR 2.111526 Loss 19.065635, Accuracy 0.075%\n",
      "Epoch 8, Batch 859, LR 2.111745 Loss 19.065801, Accuracy 0.075%\n",
      "Epoch 8, Batch 860, LR 2.111965 Loss 19.065661, Accuracy 0.074%\n",
      "Epoch 8, Batch 861, LR 2.112184 Loss 19.065676, Accuracy 0.074%\n",
      "Epoch 8, Batch 862, LR 2.112404 Loss 19.065719, Accuracy 0.074%\n",
      "Epoch 8, Batch 863, LR 2.112623 Loss 19.065609, Accuracy 0.074%\n",
      "Epoch 8, Batch 864, LR 2.112843 Loss 19.065458, Accuracy 0.074%\n",
      "Epoch 8, Batch 865, LR 2.113062 Loss 19.065600, Accuracy 0.074%\n",
      "Epoch 8, Batch 866, LR 2.113281 Loss 19.065431, Accuracy 0.074%\n",
      "Epoch 8, Batch 867, LR 2.113501 Loss 19.065336, Accuracy 0.074%\n",
      "Epoch 8, Batch 868, LR 2.113720 Loss 19.065416, Accuracy 0.074%\n",
      "Epoch 8, Batch 869, LR 2.113939 Loss 19.065536, Accuracy 0.074%\n",
      "Epoch 8, Batch 870, LR 2.114158 Loss 19.065408, Accuracy 0.074%\n",
      "Epoch 8, Batch 871, LR 2.114377 Loss 19.065450, Accuracy 0.074%\n",
      "Epoch 8, Batch 872, LR 2.114596 Loss 19.065468, Accuracy 0.073%\n",
      "Epoch 8, Batch 873, LR 2.114815 Loss 19.065370, Accuracy 0.073%\n",
      "Epoch 8, Batch 874, LR 2.115034 Loss 19.065462, Accuracy 0.073%\n",
      "Epoch 8, Batch 875, LR 2.115253 Loss 19.065598, Accuracy 0.073%\n",
      "Epoch 8, Batch 876, LR 2.115472 Loss 19.065702, Accuracy 0.073%\n",
      "Epoch 8, Batch 877, LR 2.115690 Loss 19.065635, Accuracy 0.073%\n",
      "Epoch 8, Batch 878, LR 2.115909 Loss 19.065893, Accuracy 0.073%\n",
      "Epoch 8, Batch 879, LR 2.116128 Loss 19.065874, Accuracy 0.073%\n",
      "Epoch 8, Batch 880, LR 2.116346 Loss 19.065805, Accuracy 0.073%\n",
      "Epoch 8, Batch 881, LR 2.116565 Loss 19.065757, Accuracy 0.073%\n",
      "Epoch 8, Batch 882, LR 2.116783 Loss 19.065891, Accuracy 0.073%\n",
      "Epoch 8, Batch 883, LR 2.117002 Loss 19.065857, Accuracy 0.073%\n",
      "Epoch 8, Batch 884, LR 2.117220 Loss 19.065915, Accuracy 0.072%\n",
      "Epoch 8, Batch 885, LR 2.117439 Loss 19.065960, Accuracy 0.072%\n",
      "Epoch 8, Batch 886, LR 2.117657 Loss 19.066070, Accuracy 0.072%\n",
      "Epoch 8, Batch 887, LR 2.117875 Loss 19.066245, Accuracy 0.072%\n",
      "Epoch 8, Batch 888, LR 2.118093 Loss 19.066296, Accuracy 0.072%\n",
      "Epoch 8, Batch 889, LR 2.118311 Loss 19.066163, Accuracy 0.072%\n",
      "Epoch 8, Batch 890, LR 2.118530 Loss 19.066172, Accuracy 0.072%\n",
      "Epoch 8, Batch 891, LR 2.118748 Loss 19.066167, Accuracy 0.073%\n",
      "Epoch 8, Batch 892, LR 2.118966 Loss 19.065807, Accuracy 0.073%\n",
      "Epoch 8, Batch 893, LR 2.119184 Loss 19.066016, Accuracy 0.073%\n",
      "Epoch 8, Batch 894, LR 2.119401 Loss 19.065906, Accuracy 0.073%\n",
      "Epoch 8, Batch 895, LR 2.119619 Loss 19.066226, Accuracy 0.072%\n",
      "Epoch 8, Batch 896, LR 2.119837 Loss 19.066264, Accuracy 0.072%\n",
      "Epoch 8, Batch 897, LR 2.120055 Loss 19.066042, Accuracy 0.072%\n",
      "Epoch 8, Batch 898, LR 2.120273 Loss 19.066188, Accuracy 0.072%\n",
      "Epoch 8, Batch 899, LR 2.120490 Loss 19.066267, Accuracy 0.072%\n",
      "Epoch 8, Batch 900, LR 2.120708 Loss 19.066361, Accuracy 0.072%\n",
      "Epoch 8, Batch 901, LR 2.120925 Loss 19.066371, Accuracy 0.072%\n",
      "Epoch 8, Batch 902, LR 2.121143 Loss 19.066009, Accuracy 0.072%\n",
      "Epoch 8, Batch 903, LR 2.121360 Loss 19.066009, Accuracy 0.072%\n",
      "Epoch 8, Batch 904, LR 2.121578 Loss 19.066270, Accuracy 0.072%\n",
      "Epoch 8, Batch 905, LR 2.121795 Loss 19.066134, Accuracy 0.072%\n",
      "Epoch 8, Batch 906, LR 2.122012 Loss 19.066184, Accuracy 0.072%\n",
      "Epoch 8, Batch 907, LR 2.122229 Loss 19.066424, Accuracy 0.071%\n",
      "Epoch 8, Batch 908, LR 2.122447 Loss 19.066343, Accuracy 0.071%\n",
      "Epoch 8, Batch 909, LR 2.122664 Loss 19.066366, Accuracy 0.071%\n",
      "Epoch 8, Batch 910, LR 2.122881 Loss 19.066163, Accuracy 0.071%\n",
      "Epoch 8, Batch 911, LR 2.123098 Loss 19.066219, Accuracy 0.071%\n",
      "Epoch 8, Batch 912, LR 2.123315 Loss 19.066162, Accuracy 0.071%\n",
      "Epoch 8, Batch 913, LR 2.123532 Loss 19.066204, Accuracy 0.071%\n",
      "Epoch 8, Batch 914, LR 2.123749 Loss 19.066216, Accuracy 0.071%\n",
      "Epoch 8, Batch 915, LR 2.123966 Loss 19.066363, Accuracy 0.071%\n",
      "Epoch 8, Batch 916, LR 2.124182 Loss 19.066160, Accuracy 0.071%\n",
      "Epoch 8, Batch 917, LR 2.124399 Loss 19.066249, Accuracy 0.072%\n",
      "Epoch 8, Batch 918, LR 2.124616 Loss 19.066043, Accuracy 0.071%\n",
      "Epoch 8, Batch 919, LR 2.124832 Loss 19.066286, Accuracy 0.071%\n",
      "Epoch 8, Batch 920, LR 2.125049 Loss 19.065983, Accuracy 0.072%\n",
      "Epoch 8, Batch 921, LR 2.125265 Loss 19.066056, Accuracy 0.072%\n",
      "Epoch 8, Batch 922, LR 2.125482 Loss 19.065828, Accuracy 0.072%\n",
      "Epoch 8, Batch 923, LR 2.125698 Loss 19.065915, Accuracy 0.072%\n",
      "Epoch 8, Batch 924, LR 2.125915 Loss 19.065735, Accuracy 0.072%\n",
      "Epoch 8, Batch 925, LR 2.126131 Loss 19.065710, Accuracy 0.072%\n",
      "Epoch 8, Batch 926, LR 2.126347 Loss 19.065666, Accuracy 0.073%\n",
      "Epoch 8, Batch 927, LR 2.126564 Loss 19.065922, Accuracy 0.073%\n",
      "Epoch 8, Batch 928, LR 2.126780 Loss 19.065897, Accuracy 0.073%\n",
      "Epoch 8, Batch 929, LR 2.126996 Loss 19.065879, Accuracy 0.074%\n",
      "Epoch 8, Batch 930, LR 2.127212 Loss 19.065945, Accuracy 0.074%\n",
      "Epoch 8, Batch 931, LR 2.127428 Loss 19.065928, Accuracy 0.074%\n",
      "Epoch 8, Batch 932, LR 2.127644 Loss 19.066038, Accuracy 0.074%\n",
      "Epoch 8, Batch 933, LR 2.127860 Loss 19.065943, Accuracy 0.074%\n",
      "Epoch 8, Batch 934, LR 2.128076 Loss 19.065843, Accuracy 0.074%\n",
      "Epoch 8, Batch 935, LR 2.128292 Loss 19.065724, Accuracy 0.074%\n",
      "Epoch 8, Batch 936, LR 2.128507 Loss 19.065565, Accuracy 0.074%\n",
      "Epoch 8, Batch 937, LR 2.128723 Loss 19.065562, Accuracy 0.074%\n",
      "Epoch 8, Batch 938, LR 2.128939 Loss 19.065732, Accuracy 0.074%\n",
      "Epoch 8, Batch 939, LR 2.129154 Loss 19.065829, Accuracy 0.074%\n",
      "Epoch 8, Batch 940, LR 2.129370 Loss 19.065972, Accuracy 0.074%\n",
      "Epoch 8, Batch 941, LR 2.129585 Loss 19.066064, Accuracy 0.074%\n",
      "Epoch 8, Batch 942, LR 2.129801 Loss 19.065975, Accuracy 0.074%\n",
      "Epoch 8, Batch 943, LR 2.130016 Loss 19.066086, Accuracy 0.074%\n",
      "Epoch 8, Batch 944, LR 2.130232 Loss 19.065962, Accuracy 0.074%\n",
      "Epoch 8, Batch 945, LR 2.130447 Loss 19.065797, Accuracy 0.074%\n",
      "Epoch 8, Batch 946, LR 2.130662 Loss 19.065946, Accuracy 0.074%\n",
      "Epoch 8, Batch 947, LR 2.130877 Loss 19.065826, Accuracy 0.074%\n",
      "Epoch 8, Batch 948, LR 2.131092 Loss 19.065668, Accuracy 0.074%\n",
      "Epoch 8, Batch 949, LR 2.131308 Loss 19.065676, Accuracy 0.074%\n",
      "Epoch 8, Batch 950, LR 2.131523 Loss 19.065616, Accuracy 0.074%\n",
      "Epoch 8, Batch 951, LR 2.131738 Loss 19.065588, Accuracy 0.074%\n",
      "Epoch 8, Batch 952, LR 2.131953 Loss 19.065656, Accuracy 0.074%\n",
      "Epoch 8, Batch 953, LR 2.132167 Loss 19.065666, Accuracy 0.074%\n",
      "Epoch 8, Batch 954, LR 2.132382 Loss 19.065614, Accuracy 0.074%\n",
      "Epoch 8, Batch 955, LR 2.132597 Loss 19.065785, Accuracy 0.074%\n",
      "Epoch 8, Batch 956, LR 2.132812 Loss 19.066025, Accuracy 0.074%\n",
      "Epoch 8, Batch 957, LR 2.133027 Loss 19.065892, Accuracy 0.073%\n",
      "Epoch 8, Batch 958, LR 2.133241 Loss 19.066161, Accuracy 0.073%\n",
      "Epoch 8, Batch 959, LR 2.133456 Loss 19.066213, Accuracy 0.073%\n",
      "Epoch 8, Batch 960, LR 2.133670 Loss 19.066305, Accuracy 0.073%\n",
      "Epoch 8, Batch 961, LR 2.133885 Loss 19.066314, Accuracy 0.073%\n",
      "Epoch 8, Batch 962, LR 2.134099 Loss 19.066256, Accuracy 0.073%\n",
      "Epoch 8, Batch 963, LR 2.134314 Loss 19.066292, Accuracy 0.073%\n",
      "Epoch 8, Batch 964, LR 2.134528 Loss 19.066452, Accuracy 0.073%\n",
      "Epoch 8, Batch 965, LR 2.134742 Loss 19.066290, Accuracy 0.073%\n",
      "Epoch 8, Batch 966, LR 2.134956 Loss 19.066369, Accuracy 0.073%\n",
      "Epoch 8, Batch 967, LR 2.135171 Loss 19.066301, Accuracy 0.074%\n",
      "Epoch 8, Batch 968, LR 2.135385 Loss 19.066350, Accuracy 0.073%\n",
      "Epoch 8, Batch 969, LR 2.135599 Loss 19.066327, Accuracy 0.073%\n",
      "Epoch 8, Batch 970, LR 2.135813 Loss 19.066357, Accuracy 0.073%\n",
      "Epoch 8, Batch 971, LR 2.136027 Loss 19.066384, Accuracy 0.073%\n",
      "Epoch 8, Batch 972, LR 2.136241 Loss 19.066184, Accuracy 0.073%\n",
      "Epoch 8, Batch 973, LR 2.136455 Loss 19.066051, Accuracy 0.073%\n",
      "Epoch 8, Batch 974, LR 2.136668 Loss 19.065978, Accuracy 0.073%\n",
      "Epoch 8, Batch 975, LR 2.136882 Loss 19.065855, Accuracy 0.073%\n",
      "Epoch 8, Batch 976, LR 2.137096 Loss 19.065859, Accuracy 0.073%\n",
      "Epoch 8, Batch 977, LR 2.137309 Loss 19.065815, Accuracy 0.073%\n",
      "Epoch 8, Batch 978, LR 2.137523 Loss 19.065755, Accuracy 0.073%\n",
      "Epoch 8, Batch 979, LR 2.137737 Loss 19.065631, Accuracy 0.073%\n",
      "Epoch 8, Batch 980, LR 2.137950 Loss 19.065579, Accuracy 0.073%\n",
      "Epoch 8, Batch 981, LR 2.138164 Loss 19.065531, Accuracy 0.073%\n",
      "Epoch 8, Batch 982, LR 2.138377 Loss 19.065467, Accuracy 0.073%\n",
      "Epoch 8, Batch 983, LR 2.138590 Loss 19.065513, Accuracy 0.073%\n",
      "Epoch 8, Batch 984, LR 2.138804 Loss 19.065582, Accuracy 0.073%\n",
      "Epoch 8, Batch 985, LR 2.139017 Loss 19.065703, Accuracy 0.073%\n",
      "Epoch 8, Batch 986, LR 2.139230 Loss 19.065709, Accuracy 0.073%\n",
      "Epoch 8, Batch 987, LR 2.139443 Loss 19.065673, Accuracy 0.073%\n",
      "Epoch 8, Batch 988, LR 2.139656 Loss 19.065786, Accuracy 0.073%\n",
      "Epoch 8, Batch 989, LR 2.139869 Loss 19.065668, Accuracy 0.073%\n",
      "Epoch 8, Batch 990, LR 2.140082 Loss 19.065512, Accuracy 0.073%\n",
      "Epoch 8, Batch 991, LR 2.140295 Loss 19.065371, Accuracy 0.073%\n",
      "Epoch 8, Batch 992, LR 2.140508 Loss 19.065507, Accuracy 0.073%\n",
      "Epoch 8, Batch 993, LR 2.140721 Loss 19.065329, Accuracy 0.073%\n",
      "Epoch 8, Batch 994, LR 2.140934 Loss 19.065086, Accuracy 0.073%\n",
      "Epoch 8, Batch 995, LR 2.141147 Loss 19.065014, Accuracy 0.073%\n",
      "Epoch 8, Batch 996, LR 2.141359 Loss 19.065038, Accuracy 0.073%\n",
      "Epoch 8, Batch 997, LR 2.141572 Loss 19.065034, Accuracy 0.073%\n",
      "Epoch 8, Batch 998, LR 2.141784 Loss 19.065012, Accuracy 0.073%\n",
      "Epoch 8, Batch 999, LR 2.141997 Loss 19.065040, Accuracy 0.073%\n",
      "Epoch 8, Batch 1000, LR 2.142209 Loss 19.064957, Accuracy 0.073%\n",
      "Epoch 8, Batch 1001, LR 2.142422 Loss 19.064916, Accuracy 0.073%\n",
      "Epoch 8, Batch 1002, LR 2.142634 Loss 19.064783, Accuracy 0.073%\n",
      "Epoch 8, Batch 1003, LR 2.142847 Loss 19.064805, Accuracy 0.073%\n",
      "Epoch 8, Batch 1004, LR 2.143059 Loss 19.064677, Accuracy 0.073%\n",
      "Epoch 8, Batch 1005, LR 2.143271 Loss 19.064560, Accuracy 0.074%\n",
      "Epoch 8, Batch 1006, LR 2.143483 Loss 19.064554, Accuracy 0.074%\n",
      "Epoch 8, Batch 1007, LR 2.143695 Loss 19.064477, Accuracy 0.074%\n",
      "Epoch 8, Batch 1008, LR 2.143907 Loss 19.064533, Accuracy 0.074%\n",
      "Epoch 8, Batch 1009, LR 2.144119 Loss 19.064444, Accuracy 0.074%\n",
      "Epoch 8, Batch 1010, LR 2.144331 Loss 19.064380, Accuracy 0.073%\n",
      "Epoch 8, Batch 1011, LR 2.144543 Loss 19.064423, Accuracy 0.073%\n",
      "Epoch 8, Batch 1012, LR 2.144755 Loss 19.064486, Accuracy 0.073%\n",
      "Epoch 8, Batch 1013, LR 2.144967 Loss 19.064429, Accuracy 0.073%\n",
      "Epoch 8, Batch 1014, LR 2.145179 Loss 19.064395, Accuracy 0.073%\n",
      "Epoch 8, Batch 1015, LR 2.145390 Loss 19.064557, Accuracy 0.073%\n",
      "Epoch 8, Batch 1016, LR 2.145602 Loss 19.064612, Accuracy 0.073%\n",
      "Epoch 8, Batch 1017, LR 2.145813 Loss 19.064559, Accuracy 0.073%\n",
      "Epoch 8, Batch 1018, LR 2.146025 Loss 19.064646, Accuracy 0.073%\n",
      "Epoch 8, Batch 1019, LR 2.146237 Loss 19.064797, Accuracy 0.073%\n",
      "Epoch 8, Batch 1020, LR 2.146448 Loss 19.064829, Accuracy 0.073%\n",
      "Epoch 8, Batch 1021, LR 2.146659 Loss 19.064823, Accuracy 0.073%\n",
      "Epoch 8, Batch 1022, LR 2.146871 Loss 19.064710, Accuracy 0.073%\n",
      "Epoch 8, Batch 1023, LR 2.147082 Loss 19.064673, Accuracy 0.073%\n",
      "Epoch 8, Batch 1024, LR 2.147293 Loss 19.064625, Accuracy 0.072%\n",
      "Epoch 8, Batch 1025, LR 2.147504 Loss 19.064528, Accuracy 0.072%\n",
      "Epoch 8, Batch 1026, LR 2.147715 Loss 19.064377, Accuracy 0.072%\n",
      "Epoch 8, Batch 1027, LR 2.147926 Loss 19.064261, Accuracy 0.072%\n",
      "Epoch 8, Batch 1028, LR 2.148138 Loss 19.064452, Accuracy 0.072%\n",
      "Epoch 8, Batch 1029, LR 2.148348 Loss 19.064550, Accuracy 0.072%\n",
      "Epoch 8, Batch 1030, LR 2.148559 Loss 19.064394, Accuracy 0.073%\n",
      "Epoch 8, Batch 1031, LR 2.148770 Loss 19.064349, Accuracy 0.073%\n",
      "Epoch 8, Batch 1032, LR 2.148981 Loss 19.064299, Accuracy 0.073%\n",
      "Epoch 8, Batch 1033, LR 2.149192 Loss 19.064280, Accuracy 0.073%\n",
      "Epoch 8, Batch 1034, LR 2.149402 Loss 19.064413, Accuracy 0.073%\n",
      "Epoch 8, Batch 1035, LR 2.149613 Loss 19.064246, Accuracy 0.073%\n",
      "Epoch 8, Batch 1036, LR 2.149824 Loss 19.064237, Accuracy 0.074%\n",
      "Epoch 8, Batch 1037, LR 2.150034 Loss 19.064459, Accuracy 0.074%\n",
      "Epoch 8, Batch 1038, LR 2.150245 Loss 19.064673, Accuracy 0.074%\n",
      "Epoch 8, Batch 1039, LR 2.150455 Loss 19.064987, Accuracy 0.074%\n",
      "Epoch 8, Batch 1040, LR 2.150666 Loss 19.064781, Accuracy 0.074%\n",
      "Epoch 8, Batch 1041, LR 2.150876 Loss 19.064945, Accuracy 0.074%\n",
      "Epoch 8, Batch 1042, LR 2.151086 Loss 19.064981, Accuracy 0.073%\n",
      "Epoch 8, Batch 1043, LR 2.151296 Loss 19.064836, Accuracy 0.073%\n",
      "Epoch 8, Batch 1044, LR 2.151507 Loss 19.064898, Accuracy 0.073%\n",
      "Epoch 8, Batch 1045, LR 2.151717 Loss 19.065014, Accuracy 0.073%\n",
      "Epoch 8, Batch 1046, LR 2.151927 Loss 19.065202, Accuracy 0.073%\n",
      "Epoch 8, Batch 1047, LR 2.152137 Loss 19.065092, Accuracy 0.073%\n",
      "Epoch 8, Loss (train set) 19.065092, Accuracy (train set) 0.073%\n",
      "Epoch 9, Batch 1, LR 2.152347 Loss 19.344374, Accuracy 0.000%\n",
      "Epoch 9, Batch 2, LR 2.152557 Loss 19.156074, Accuracy 0.000%\n",
      "Epoch 9, Batch 3, LR 2.152767 Loss 19.142713, Accuracy 0.000%\n",
      "Epoch 9, Batch 4, LR 2.152976 Loss 19.070442, Accuracy 0.000%\n",
      "Epoch 9, Batch 5, LR 2.153186 Loss 19.092733, Accuracy 0.000%\n",
      "Epoch 9, Batch 6, LR 2.153396 Loss 19.118662, Accuracy 0.000%\n",
      "Epoch 9, Batch 7, LR 2.153605 Loss 19.120729, Accuracy 0.000%\n",
      "Epoch 9, Batch 8, LR 2.153815 Loss 19.126715, Accuracy 0.000%\n",
      "Epoch 9, Batch 9, LR 2.154025 Loss 19.114273, Accuracy 0.000%\n",
      "Epoch 9, Batch 10, LR 2.154234 Loss 19.128479, Accuracy 0.000%\n",
      "Epoch 9, Batch 11, LR 2.154444 Loss 19.122979, Accuracy 0.000%\n",
      "Epoch 9, Batch 12, LR 2.154653 Loss 19.119580, Accuracy 0.000%\n",
      "Epoch 9, Batch 13, LR 2.154862 Loss 19.109612, Accuracy 0.000%\n",
      "Epoch 9, Batch 14, LR 2.155072 Loss 19.111526, Accuracy 0.000%\n",
      "Epoch 9, Batch 15, LR 2.155281 Loss 19.112707, Accuracy 0.000%\n",
      "Epoch 9, Batch 16, LR 2.155490 Loss 19.107672, Accuracy 0.000%\n",
      "Epoch 9, Batch 17, LR 2.155699 Loss 19.090503, Accuracy 0.000%\n",
      "Epoch 9, Batch 18, LR 2.155908 Loss 19.085343, Accuracy 0.000%\n",
      "Epoch 9, Batch 19, LR 2.156117 Loss 19.078196, Accuracy 0.000%\n",
      "Epoch 9, Batch 20, LR 2.156326 Loss 19.081471, Accuracy 0.000%\n",
      "Epoch 9, Batch 21, LR 2.156535 Loss 19.082480, Accuracy 0.000%\n",
      "Epoch 9, Batch 22, LR 2.156744 Loss 19.097707, Accuracy 0.000%\n",
      "Epoch 9, Batch 23, LR 2.156953 Loss 19.097315, Accuracy 0.000%\n",
      "Epoch 9, Batch 24, LR 2.157161 Loss 19.088081, Accuracy 0.000%\n",
      "Epoch 9, Batch 25, LR 2.157370 Loss 19.084094, Accuracy 0.000%\n",
      "Epoch 9, Batch 26, LR 2.157579 Loss 19.085241, Accuracy 0.000%\n",
      "Epoch 9, Batch 27, LR 2.157787 Loss 19.092803, Accuracy 0.000%\n",
      "Epoch 9, Batch 28, LR 2.157996 Loss 19.093040, Accuracy 0.000%\n",
      "Epoch 9, Batch 29, LR 2.158204 Loss 19.090654, Accuracy 0.000%\n",
      "Epoch 9, Batch 30, LR 2.158413 Loss 19.093818, Accuracy 0.000%\n",
      "Epoch 9, Batch 31, LR 2.158621 Loss 19.090562, Accuracy 0.000%\n",
      "Epoch 9, Batch 32, LR 2.158830 Loss 19.088421, Accuracy 0.000%\n",
      "Epoch 9, Batch 33, LR 2.159038 Loss 19.085892, Accuracy 0.000%\n",
      "Epoch 9, Batch 34, LR 2.159246 Loss 19.089588, Accuracy 0.000%\n",
      "Epoch 9, Batch 35, LR 2.159454 Loss 19.092885, Accuracy 0.000%\n",
      "Epoch 9, Batch 36, LR 2.159662 Loss 19.088035, Accuracy 0.000%\n",
      "Epoch 9, Batch 37, LR 2.159870 Loss 19.084432, Accuracy 0.000%\n",
      "Epoch 9, Batch 38, LR 2.160078 Loss 19.081940, Accuracy 0.000%\n",
      "Epoch 9, Batch 39, LR 2.160286 Loss 19.074112, Accuracy 0.000%\n",
      "Epoch 9, Batch 40, LR 2.160494 Loss 19.071347, Accuracy 0.000%\n",
      "Epoch 9, Batch 41, LR 2.160702 Loss 19.063357, Accuracy 0.019%\n",
      "Epoch 9, Batch 42, LR 2.160910 Loss 19.062378, Accuracy 0.019%\n",
      "Epoch 9, Batch 43, LR 2.161118 Loss 19.062656, Accuracy 0.018%\n",
      "Epoch 9, Batch 44, LR 2.161325 Loss 19.062258, Accuracy 0.018%\n",
      "Epoch 9, Batch 45, LR 2.161533 Loss 19.062622, Accuracy 0.017%\n",
      "Epoch 9, Batch 46, LR 2.161741 Loss 19.065564, Accuracy 0.017%\n",
      "Epoch 9, Batch 47, LR 2.161948 Loss 19.068810, Accuracy 0.017%\n",
      "Epoch 9, Batch 48, LR 2.162156 Loss 19.069797, Accuracy 0.016%\n",
      "Epoch 9, Batch 49, LR 2.162363 Loss 19.069191, Accuracy 0.016%\n",
      "Epoch 9, Batch 50, LR 2.162570 Loss 19.063303, Accuracy 0.016%\n",
      "Epoch 9, Batch 51, LR 2.162778 Loss 19.061731, Accuracy 0.015%\n",
      "Epoch 9, Batch 52, LR 2.162985 Loss 19.064163, Accuracy 0.015%\n",
      "Epoch 9, Batch 53, LR 2.163192 Loss 19.062198, Accuracy 0.015%\n",
      "Epoch 9, Batch 54, LR 2.163399 Loss 19.060632, Accuracy 0.014%\n",
      "Epoch 9, Batch 55, LR 2.163607 Loss 19.064009, Accuracy 0.014%\n",
      "Epoch 9, Batch 56, LR 2.163814 Loss 19.062566, Accuracy 0.014%\n",
      "Epoch 9, Batch 57, LR 2.164021 Loss 19.059634, Accuracy 0.014%\n",
      "Epoch 9, Batch 58, LR 2.164228 Loss 19.057072, Accuracy 0.013%\n",
      "Epoch 9, Batch 59, LR 2.164434 Loss 19.056083, Accuracy 0.013%\n",
      "Epoch 9, Batch 60, LR 2.164641 Loss 19.056571, Accuracy 0.013%\n",
      "Epoch 9, Batch 61, LR 2.164848 Loss 19.058856, Accuracy 0.013%\n",
      "Epoch 9, Batch 62, LR 2.165055 Loss 19.053375, Accuracy 0.013%\n",
      "Epoch 9, Batch 63, LR 2.165262 Loss 19.051317, Accuracy 0.012%\n",
      "Epoch 9, Batch 64, LR 2.165468 Loss 19.053116, Accuracy 0.012%\n",
      "Epoch 9, Batch 65, LR 2.165675 Loss 19.053830, Accuracy 0.012%\n",
      "Epoch 9, Batch 66, LR 2.165881 Loss 19.052546, Accuracy 0.024%\n",
      "Epoch 9, Batch 67, LR 2.166088 Loss 19.053736, Accuracy 0.023%\n",
      "Epoch 9, Batch 68, LR 2.166294 Loss 19.053882, Accuracy 0.023%\n",
      "Epoch 9, Batch 69, LR 2.166501 Loss 19.053401, Accuracy 0.023%\n",
      "Epoch 9, Batch 70, LR 2.166707 Loss 19.055820, Accuracy 0.022%\n",
      "Epoch 9, Batch 71, LR 2.166913 Loss 19.055101, Accuracy 0.022%\n",
      "Epoch 9, Batch 72, LR 2.167119 Loss 19.056444, Accuracy 0.022%\n",
      "Epoch 9, Batch 73, LR 2.167325 Loss 19.055874, Accuracy 0.032%\n",
      "Epoch 9, Batch 74, LR 2.167532 Loss 19.057950, Accuracy 0.032%\n",
      "Epoch 9, Batch 75, LR 2.167738 Loss 19.058758, Accuracy 0.031%\n",
      "Epoch 9, Batch 76, LR 2.167944 Loss 19.057547, Accuracy 0.031%\n",
      "Epoch 9, Batch 77, LR 2.168150 Loss 19.053858, Accuracy 0.030%\n",
      "Epoch 9, Batch 78, LR 2.168355 Loss 19.054810, Accuracy 0.030%\n",
      "Epoch 9, Batch 79, LR 2.168561 Loss 19.054734, Accuracy 0.030%\n",
      "Epoch 9, Batch 80, LR 2.168767 Loss 19.054136, Accuracy 0.029%\n",
      "Epoch 9, Batch 81, LR 2.168973 Loss 19.055529, Accuracy 0.029%\n",
      "Epoch 9, Batch 82, LR 2.169178 Loss 19.058057, Accuracy 0.029%\n",
      "Epoch 9, Batch 83, LR 2.169384 Loss 19.058537, Accuracy 0.028%\n",
      "Epoch 9, Batch 84, LR 2.169590 Loss 19.060375, Accuracy 0.028%\n",
      "Epoch 9, Batch 85, LR 2.169795 Loss 19.061597, Accuracy 0.028%\n",
      "Epoch 9, Batch 86, LR 2.170001 Loss 19.063717, Accuracy 0.036%\n",
      "Epoch 9, Batch 87, LR 2.170206 Loss 19.065116, Accuracy 0.036%\n",
      "Epoch 9, Batch 88, LR 2.170411 Loss 19.064825, Accuracy 0.036%\n",
      "Epoch 9, Batch 89, LR 2.170617 Loss 19.065680, Accuracy 0.044%\n",
      "Epoch 9, Batch 90, LR 2.170822 Loss 19.066229, Accuracy 0.043%\n",
      "Epoch 9, Batch 91, LR 2.171027 Loss 19.066474, Accuracy 0.043%\n",
      "Epoch 9, Batch 92, LR 2.171232 Loss 19.066332, Accuracy 0.042%\n",
      "Epoch 9, Batch 93, LR 2.171437 Loss 19.067744, Accuracy 0.042%\n",
      "Epoch 9, Batch 94, LR 2.171642 Loss 19.067586, Accuracy 0.042%\n",
      "Epoch 9, Batch 95, LR 2.171847 Loss 19.069747, Accuracy 0.041%\n",
      "Epoch 9, Batch 96, LR 2.172052 Loss 19.070730, Accuracy 0.049%\n",
      "Epoch 9, Batch 97, LR 2.172257 Loss 19.071895, Accuracy 0.048%\n",
      "Epoch 9, Batch 98, LR 2.172462 Loss 19.072875, Accuracy 0.048%\n",
      "Epoch 9, Batch 99, LR 2.172667 Loss 19.075005, Accuracy 0.047%\n",
      "Epoch 9, Batch 100, LR 2.172871 Loss 19.075166, Accuracy 0.047%\n",
      "Epoch 9, Batch 101, LR 2.173076 Loss 19.075987, Accuracy 0.046%\n",
      "Epoch 9, Batch 102, LR 2.173281 Loss 19.074601, Accuracy 0.046%\n",
      "Epoch 9, Batch 103, LR 2.173485 Loss 19.073584, Accuracy 0.046%\n",
      "Epoch 9, Batch 104, LR 2.173690 Loss 19.074476, Accuracy 0.045%\n",
      "Epoch 9, Batch 105, LR 2.173894 Loss 19.074618, Accuracy 0.045%\n",
      "Epoch 9, Batch 106, LR 2.174098 Loss 19.075536, Accuracy 0.044%\n",
      "Epoch 9, Batch 107, LR 2.174303 Loss 19.074885, Accuracy 0.051%\n",
      "Epoch 9, Batch 108, LR 2.174507 Loss 19.075813, Accuracy 0.051%\n",
      "Epoch 9, Batch 109, LR 2.174711 Loss 19.073851, Accuracy 0.050%\n",
      "Epoch 9, Batch 110, LR 2.174915 Loss 19.073654, Accuracy 0.050%\n",
      "Epoch 9, Batch 111, LR 2.175119 Loss 19.074203, Accuracy 0.056%\n",
      "Epoch 9, Batch 112, LR 2.175324 Loss 19.074213, Accuracy 0.056%\n",
      "Epoch 9, Batch 113, LR 2.175528 Loss 19.074473, Accuracy 0.062%\n",
      "Epoch 9, Batch 114, LR 2.175731 Loss 19.073795, Accuracy 0.062%\n",
      "Epoch 9, Batch 115, LR 2.175935 Loss 19.076268, Accuracy 0.061%\n",
      "Epoch 9, Batch 116, LR 2.176139 Loss 19.076482, Accuracy 0.061%\n",
      "Epoch 9, Batch 117, LR 2.176343 Loss 19.076222, Accuracy 0.060%\n",
      "Epoch 9, Batch 118, LR 2.176547 Loss 19.074370, Accuracy 0.060%\n",
      "Epoch 9, Batch 119, LR 2.176750 Loss 19.072089, Accuracy 0.059%\n",
      "Epoch 9, Batch 120, LR 2.176954 Loss 19.070469, Accuracy 0.059%\n",
      "Epoch 9, Batch 121, LR 2.177158 Loss 19.071133, Accuracy 0.065%\n",
      "Epoch 9, Batch 122, LR 2.177361 Loss 19.070394, Accuracy 0.064%\n",
      "Epoch 9, Batch 123, LR 2.177565 Loss 19.069402, Accuracy 0.070%\n",
      "Epoch 9, Batch 124, LR 2.177768 Loss 19.068357, Accuracy 0.069%\n",
      "Epoch 9, Batch 125, LR 2.177971 Loss 19.066945, Accuracy 0.075%\n",
      "Epoch 9, Batch 126, LR 2.178175 Loss 19.066806, Accuracy 0.074%\n",
      "Epoch 9, Batch 127, LR 2.178378 Loss 19.067336, Accuracy 0.074%\n",
      "Epoch 9, Batch 128, LR 2.178581 Loss 19.069126, Accuracy 0.073%\n",
      "Epoch 9, Batch 129, LR 2.178784 Loss 19.068925, Accuracy 0.073%\n",
      "Epoch 9, Batch 130, LR 2.178987 Loss 19.068204, Accuracy 0.072%\n",
      "Epoch 9, Batch 131, LR 2.179190 Loss 19.066945, Accuracy 0.072%\n",
      "Epoch 9, Batch 132, LR 2.179393 Loss 19.066539, Accuracy 0.071%\n",
      "Epoch 9, Batch 133, LR 2.179596 Loss 19.066883, Accuracy 0.070%\n",
      "Epoch 9, Batch 134, LR 2.179799 Loss 19.066401, Accuracy 0.070%\n",
      "Epoch 9, Batch 135, LR 2.180002 Loss 19.067202, Accuracy 0.069%\n",
      "Epoch 9, Batch 136, LR 2.180205 Loss 19.066927, Accuracy 0.069%\n",
      "Epoch 9, Batch 137, LR 2.180407 Loss 19.067122, Accuracy 0.068%\n",
      "Epoch 9, Batch 138, LR 2.180610 Loss 19.067200, Accuracy 0.068%\n",
      "Epoch 9, Batch 139, LR 2.180813 Loss 19.066917, Accuracy 0.067%\n",
      "Epoch 9, Batch 140, LR 2.181015 Loss 19.067641, Accuracy 0.067%\n",
      "Epoch 9, Batch 141, LR 2.181218 Loss 19.066801, Accuracy 0.066%\n",
      "Epoch 9, Batch 142, LR 2.181420 Loss 19.067650, Accuracy 0.066%\n",
      "Epoch 9, Batch 143, LR 2.181622 Loss 19.068950, Accuracy 0.066%\n",
      "Epoch 9, Batch 144, LR 2.181825 Loss 19.067527, Accuracy 0.065%\n",
      "Epoch 9, Batch 145, LR 2.182027 Loss 19.066374, Accuracy 0.065%\n",
      "Epoch 9, Batch 146, LR 2.182229 Loss 19.066274, Accuracy 0.064%\n",
      "Epoch 9, Batch 147, LR 2.182431 Loss 19.065593, Accuracy 0.064%\n",
      "Epoch 9, Batch 148, LR 2.182634 Loss 19.066226, Accuracy 0.063%\n",
      "Epoch 9, Batch 149, LR 2.182836 Loss 19.066365, Accuracy 0.063%\n",
      "Epoch 9, Batch 150, LR 2.183038 Loss 19.067242, Accuracy 0.062%\n",
      "Epoch 9, Batch 151, LR 2.183240 Loss 19.066094, Accuracy 0.062%\n",
      "Epoch 9, Batch 152, LR 2.183441 Loss 19.065497, Accuracy 0.062%\n",
      "Epoch 9, Batch 153, LR 2.183643 Loss 19.064653, Accuracy 0.061%\n",
      "Epoch 9, Batch 154, LR 2.183845 Loss 19.064311, Accuracy 0.061%\n",
      "Epoch 9, Batch 155, LR 2.184047 Loss 19.063610, Accuracy 0.060%\n",
      "Epoch 9, Batch 156, LR 2.184248 Loss 19.062986, Accuracy 0.060%\n",
      "Epoch 9, Batch 157, LR 2.184450 Loss 19.062452, Accuracy 0.060%\n",
      "Epoch 9, Batch 158, LR 2.184652 Loss 19.061789, Accuracy 0.059%\n",
      "Epoch 9, Batch 159, LR 2.184853 Loss 19.062043, Accuracy 0.059%\n",
      "Epoch 9, Batch 160, LR 2.185055 Loss 19.063052, Accuracy 0.059%\n",
      "Epoch 9, Batch 161, LR 2.185256 Loss 19.062435, Accuracy 0.058%\n",
      "Epoch 9, Batch 162, LR 2.185457 Loss 19.062617, Accuracy 0.058%\n",
      "Epoch 9, Batch 163, LR 2.185659 Loss 19.063846, Accuracy 0.058%\n",
      "Epoch 9, Batch 164, LR 2.185860 Loss 19.063712, Accuracy 0.057%\n",
      "Epoch 9, Batch 165, LR 2.186061 Loss 19.064737, Accuracy 0.057%\n",
      "Epoch 9, Batch 166, LR 2.186262 Loss 19.063899, Accuracy 0.056%\n",
      "Epoch 9, Batch 167, LR 2.186463 Loss 19.062828, Accuracy 0.056%\n",
      "Epoch 9, Batch 168, LR 2.186664 Loss 19.062503, Accuracy 0.056%\n",
      "Epoch 9, Batch 169, LR 2.186865 Loss 19.063500, Accuracy 0.055%\n",
      "Epoch 9, Batch 170, LR 2.187066 Loss 19.063299, Accuracy 0.055%\n",
      "Epoch 9, Batch 171, LR 2.187267 Loss 19.063334, Accuracy 0.055%\n",
      "Epoch 9, Batch 172, LR 2.187468 Loss 19.063970, Accuracy 0.059%\n",
      "Epoch 9, Batch 173, LR 2.187668 Loss 19.064806, Accuracy 0.059%\n",
      "Epoch 9, Batch 174, LR 2.187869 Loss 19.064130, Accuracy 0.058%\n",
      "Epoch 9, Batch 175, LR 2.188070 Loss 19.063463, Accuracy 0.058%\n",
      "Epoch 9, Batch 176, LR 2.188270 Loss 19.063912, Accuracy 0.058%\n",
      "Epoch 9, Batch 177, LR 2.188471 Loss 19.063113, Accuracy 0.057%\n",
      "Epoch 9, Batch 178, LR 2.188671 Loss 19.062791, Accuracy 0.057%\n",
      "Epoch 9, Batch 179, LR 2.188872 Loss 19.061654, Accuracy 0.061%\n",
      "Epoch 9, Batch 180, LR 2.189072 Loss 19.062089, Accuracy 0.061%\n",
      "Epoch 9, Batch 181, LR 2.189272 Loss 19.062815, Accuracy 0.060%\n",
      "Epoch 9, Batch 182, LR 2.189473 Loss 19.062943, Accuracy 0.060%\n",
      "Epoch 9, Batch 183, LR 2.189673 Loss 19.063635, Accuracy 0.060%\n",
      "Epoch 9, Batch 184, LR 2.189873 Loss 19.063661, Accuracy 0.059%\n",
      "Epoch 9, Batch 185, LR 2.190073 Loss 19.063729, Accuracy 0.059%\n",
      "Epoch 9, Batch 186, LR 2.190273 Loss 19.063955, Accuracy 0.059%\n",
      "Epoch 9, Batch 187, LR 2.190473 Loss 19.064109, Accuracy 0.058%\n",
      "Epoch 9, Batch 188, LR 2.190673 Loss 19.064439, Accuracy 0.058%\n",
      "Epoch 9, Batch 189, LR 2.190873 Loss 19.065710, Accuracy 0.058%\n",
      "Epoch 9, Batch 190, LR 2.191073 Loss 19.066294, Accuracy 0.058%\n",
      "Epoch 9, Batch 191, LR 2.191272 Loss 19.066336, Accuracy 0.057%\n",
      "Epoch 9, Batch 192, LR 2.191472 Loss 19.065597, Accuracy 0.057%\n",
      "Epoch 9, Batch 193, LR 2.191672 Loss 19.066838, Accuracy 0.057%\n",
      "Epoch 9, Batch 194, LR 2.191871 Loss 19.066619, Accuracy 0.056%\n",
      "Epoch 9, Batch 195, LR 2.192071 Loss 19.068067, Accuracy 0.056%\n",
      "Epoch 9, Batch 196, LR 2.192270 Loss 19.068153, Accuracy 0.056%\n",
      "Epoch 9, Batch 197, LR 2.192470 Loss 19.067936, Accuracy 0.056%\n",
      "Epoch 9, Batch 198, LR 2.192669 Loss 19.068168, Accuracy 0.055%\n",
      "Epoch 9, Batch 199, LR 2.192869 Loss 19.069033, Accuracy 0.055%\n",
      "Epoch 9, Batch 200, LR 2.193068 Loss 19.069191, Accuracy 0.055%\n",
      "Epoch 9, Batch 201, LR 2.193267 Loss 19.068792, Accuracy 0.054%\n",
      "Epoch 9, Batch 202, LR 2.193466 Loss 19.068809, Accuracy 0.054%\n",
      "Epoch 9, Batch 203, LR 2.193665 Loss 19.069127, Accuracy 0.054%\n",
      "Epoch 9, Batch 204, LR 2.193864 Loss 19.069754, Accuracy 0.054%\n",
      "Epoch 9, Batch 205, LR 2.194063 Loss 19.069697, Accuracy 0.053%\n",
      "Epoch 9, Batch 206, LR 2.194262 Loss 19.069579, Accuracy 0.053%\n",
      "Epoch 9, Batch 207, LR 2.194461 Loss 19.069807, Accuracy 0.053%\n",
      "Epoch 9, Batch 208, LR 2.194660 Loss 19.069398, Accuracy 0.056%\n",
      "Epoch 9, Batch 209, LR 2.194859 Loss 19.069026, Accuracy 0.056%\n",
      "Epoch 9, Batch 210, LR 2.195057 Loss 19.068725, Accuracy 0.056%\n",
      "Epoch 9, Batch 211, LR 2.195256 Loss 19.069766, Accuracy 0.056%\n",
      "Epoch 9, Batch 212, LR 2.195455 Loss 19.070458, Accuracy 0.055%\n",
      "Epoch 9, Batch 213, LR 2.195653 Loss 19.070380, Accuracy 0.055%\n",
      "Epoch 9, Batch 214, LR 2.195852 Loss 19.070686, Accuracy 0.055%\n",
      "Epoch 9, Batch 215, LR 2.196050 Loss 19.070805, Accuracy 0.055%\n",
      "Epoch 9, Batch 216, LR 2.196248 Loss 19.070357, Accuracy 0.054%\n",
      "Epoch 9, Batch 217, LR 2.196447 Loss 19.071265, Accuracy 0.054%\n",
      "Epoch 9, Batch 218, LR 2.196645 Loss 19.070857, Accuracy 0.054%\n",
      "Epoch 9, Batch 219, LR 2.196843 Loss 19.070130, Accuracy 0.054%\n",
      "Epoch 9, Batch 220, LR 2.197041 Loss 19.069677, Accuracy 0.053%\n",
      "Epoch 9, Batch 221, LR 2.197239 Loss 19.069013, Accuracy 0.053%\n",
      "Epoch 9, Batch 222, LR 2.197438 Loss 19.069697, Accuracy 0.053%\n",
      "Epoch 9, Batch 223, LR 2.197636 Loss 19.069267, Accuracy 0.053%\n",
      "Epoch 9, Batch 224, LR 2.197833 Loss 19.069917, Accuracy 0.052%\n",
      "Epoch 9, Batch 225, LR 2.198031 Loss 19.070032, Accuracy 0.052%\n",
      "Epoch 9, Batch 226, LR 2.198229 Loss 19.070709, Accuracy 0.052%\n",
      "Epoch 9, Batch 227, LR 2.198427 Loss 19.070945, Accuracy 0.052%\n",
      "Epoch 9, Batch 228, LR 2.198625 Loss 19.070383, Accuracy 0.055%\n",
      "Epoch 9, Batch 229, LR 2.198822 Loss 19.069945, Accuracy 0.055%\n",
      "Epoch 9, Batch 230, LR 2.199020 Loss 19.070191, Accuracy 0.058%\n",
      "Epoch 9, Batch 231, LR 2.199217 Loss 19.069935, Accuracy 0.057%\n",
      "Epoch 9, Batch 232, LR 2.199415 Loss 19.069872, Accuracy 0.057%\n",
      "Epoch 9, Batch 233, LR 2.199612 Loss 19.069896, Accuracy 0.057%\n",
      "Epoch 9, Batch 234, LR 2.199810 Loss 19.069612, Accuracy 0.057%\n",
      "Epoch 9, Batch 235, LR 2.200007 Loss 19.069386, Accuracy 0.057%\n",
      "Epoch 9, Batch 236, LR 2.200204 Loss 19.069976, Accuracy 0.056%\n",
      "Epoch 9, Batch 237, LR 2.200402 Loss 19.069838, Accuracy 0.056%\n",
      "Epoch 9, Batch 238, LR 2.200599 Loss 19.069725, Accuracy 0.056%\n",
      "Epoch 9, Batch 239, LR 2.200796 Loss 19.069928, Accuracy 0.056%\n",
      "Epoch 9, Batch 240, LR 2.200993 Loss 19.069419, Accuracy 0.055%\n",
      "Epoch 9, Batch 241, LR 2.201190 Loss 19.069570, Accuracy 0.055%\n",
      "Epoch 9, Batch 242, LR 2.201387 Loss 19.069689, Accuracy 0.055%\n",
      "Epoch 9, Batch 243, LR 2.201584 Loss 19.070060, Accuracy 0.055%\n",
      "Epoch 9, Batch 244, LR 2.201781 Loss 19.070567, Accuracy 0.054%\n",
      "Epoch 9, Batch 245, LR 2.201977 Loss 19.070609, Accuracy 0.054%\n",
      "Epoch 9, Batch 246, LR 2.202174 Loss 19.070072, Accuracy 0.054%\n",
      "Epoch 9, Batch 247, LR 2.202371 Loss 19.069774, Accuracy 0.054%\n",
      "Epoch 9, Batch 248, LR 2.202567 Loss 19.069245, Accuracy 0.054%\n",
      "Epoch 9, Batch 249, LR 2.202764 Loss 19.069299, Accuracy 0.053%\n",
      "Epoch 9, Batch 250, LR 2.202960 Loss 19.069410, Accuracy 0.053%\n",
      "Epoch 9, Batch 251, LR 2.203157 Loss 19.069662, Accuracy 0.053%\n",
      "Epoch 9, Batch 252, LR 2.203353 Loss 19.069842, Accuracy 0.053%\n",
      "Epoch 9, Batch 253, LR 2.203549 Loss 19.070309, Accuracy 0.052%\n",
      "Epoch 9, Batch 254, LR 2.203746 Loss 19.070803, Accuracy 0.052%\n",
      "Epoch 9, Batch 255, LR 2.203942 Loss 19.071576, Accuracy 0.055%\n",
      "Epoch 9, Batch 256, LR 2.204138 Loss 19.071859, Accuracy 0.055%\n",
      "Epoch 9, Batch 257, LR 2.204334 Loss 19.072591, Accuracy 0.055%\n",
      "Epoch 9, Batch 258, LR 2.204530 Loss 19.072395, Accuracy 0.058%\n",
      "Epoch 9, Batch 259, LR 2.204726 Loss 19.072932, Accuracy 0.060%\n",
      "Epoch 9, Batch 260, LR 2.204922 Loss 19.072205, Accuracy 0.063%\n",
      "Epoch 9, Batch 261, LR 2.205118 Loss 19.071491, Accuracy 0.063%\n",
      "Epoch 9, Batch 262, LR 2.205314 Loss 19.071368, Accuracy 0.063%\n",
      "Epoch 9, Batch 263, LR 2.205510 Loss 19.071606, Accuracy 0.062%\n",
      "Epoch 9, Batch 264, LR 2.205705 Loss 19.071441, Accuracy 0.062%\n",
      "Epoch 9, Batch 265, LR 2.205901 Loss 19.071734, Accuracy 0.062%\n",
      "Epoch 9, Batch 266, LR 2.206097 Loss 19.072239, Accuracy 0.062%\n",
      "Epoch 9, Batch 267, LR 2.206292 Loss 19.072528, Accuracy 0.061%\n",
      "Epoch 9, Batch 268, LR 2.206488 Loss 19.072752, Accuracy 0.061%\n",
      "Epoch 9, Batch 269, LR 2.206683 Loss 19.072818, Accuracy 0.061%\n",
      "Epoch 9, Batch 270, LR 2.206878 Loss 19.072990, Accuracy 0.061%\n",
      "Epoch 9, Batch 271, LR 2.207074 Loss 19.072741, Accuracy 0.061%\n",
      "Epoch 9, Batch 272, LR 2.207269 Loss 19.072590, Accuracy 0.060%\n",
      "Epoch 9, Batch 273, LR 2.207464 Loss 19.072332, Accuracy 0.060%\n",
      "Epoch 9, Batch 274, LR 2.207659 Loss 19.072702, Accuracy 0.060%\n",
      "Epoch 9, Batch 275, LR 2.207854 Loss 19.072442, Accuracy 0.060%\n",
      "Epoch 9, Batch 276, LR 2.208049 Loss 19.072780, Accuracy 0.059%\n",
      "Epoch 9, Batch 277, LR 2.208244 Loss 19.072581, Accuracy 0.059%\n",
      "Epoch 9, Batch 278, LR 2.208439 Loss 19.072394, Accuracy 0.059%\n",
      "Epoch 9, Batch 279, LR 2.208634 Loss 19.072152, Accuracy 0.059%\n",
      "Epoch 9, Batch 280, LR 2.208829 Loss 19.072092, Accuracy 0.059%\n",
      "Epoch 9, Batch 281, LR 2.209024 Loss 19.071797, Accuracy 0.058%\n",
      "Epoch 9, Batch 282, LR 2.209219 Loss 19.070903, Accuracy 0.058%\n",
      "Epoch 9, Batch 283, LR 2.209413 Loss 19.071265, Accuracy 0.058%\n",
      "Epoch 9, Batch 284, LR 2.209608 Loss 19.071434, Accuracy 0.058%\n",
      "Epoch 9, Batch 285, LR 2.209802 Loss 19.070913, Accuracy 0.058%\n",
      "Epoch 9, Batch 286, LR 2.209997 Loss 19.071227, Accuracy 0.057%\n",
      "Epoch 9, Batch 287, LR 2.210191 Loss 19.071424, Accuracy 0.057%\n",
      "Epoch 9, Batch 288, LR 2.210386 Loss 19.071856, Accuracy 0.057%\n",
      "Epoch 9, Batch 289, LR 2.210580 Loss 19.071734, Accuracy 0.057%\n",
      "Epoch 9, Batch 290, LR 2.210774 Loss 19.070901, Accuracy 0.057%\n",
      "Epoch 9, Batch 291, LR 2.210968 Loss 19.070989, Accuracy 0.056%\n",
      "Epoch 9, Batch 292, LR 2.211163 Loss 19.070441, Accuracy 0.056%\n",
      "Epoch 9, Batch 293, LR 2.211357 Loss 19.070629, Accuracy 0.056%\n",
      "Epoch 9, Batch 294, LR 2.211551 Loss 19.071158, Accuracy 0.056%\n",
      "Epoch 9, Batch 295, LR 2.211745 Loss 19.070459, Accuracy 0.056%\n",
      "Epoch 9, Batch 296, LR 2.211939 Loss 19.071203, Accuracy 0.055%\n",
      "Epoch 9, Batch 297, LR 2.212132 Loss 19.070839, Accuracy 0.055%\n",
      "Epoch 9, Batch 298, LR 2.212326 Loss 19.070640, Accuracy 0.055%\n",
      "Epoch 9, Batch 299, LR 2.212520 Loss 19.070466, Accuracy 0.055%\n",
      "Epoch 9, Batch 300, LR 2.212714 Loss 19.070514, Accuracy 0.055%\n",
      "Epoch 9, Batch 301, LR 2.212907 Loss 19.070693, Accuracy 0.055%\n",
      "Epoch 9, Batch 302, LR 2.213101 Loss 19.070428, Accuracy 0.054%\n",
      "Epoch 9, Batch 303, LR 2.213294 Loss 19.070933, Accuracy 0.054%\n",
      "Epoch 9, Batch 304, LR 2.213488 Loss 19.070353, Accuracy 0.054%\n",
      "Epoch 9, Batch 305, LR 2.213681 Loss 19.070862, Accuracy 0.054%\n",
      "Epoch 9, Batch 306, LR 2.213875 Loss 19.070794, Accuracy 0.054%\n",
      "Epoch 9, Batch 307, LR 2.214068 Loss 19.071217, Accuracy 0.056%\n",
      "Epoch 9, Batch 308, LR 2.214261 Loss 19.071655, Accuracy 0.056%\n",
      "Epoch 9, Batch 309, LR 2.214454 Loss 19.071315, Accuracy 0.056%\n",
      "Epoch 9, Batch 310, LR 2.214647 Loss 19.071310, Accuracy 0.058%\n",
      "Epoch 9, Batch 311, LR 2.214841 Loss 19.071314, Accuracy 0.058%\n",
      "Epoch 9, Batch 312, LR 2.215034 Loss 19.071248, Accuracy 0.058%\n",
      "Epoch 9, Batch 313, LR 2.215226 Loss 19.070385, Accuracy 0.057%\n",
      "Epoch 9, Batch 314, LR 2.215419 Loss 19.070095, Accuracy 0.060%\n",
      "Epoch 9, Batch 315, LR 2.215612 Loss 19.070507, Accuracy 0.060%\n",
      "Epoch 9, Batch 316, LR 2.215805 Loss 19.069259, Accuracy 0.059%\n",
      "Epoch 9, Batch 317, LR 2.215998 Loss 19.069748, Accuracy 0.059%\n",
      "Epoch 9, Batch 318, LR 2.216190 Loss 19.069941, Accuracy 0.059%\n",
      "Epoch 9, Batch 319, LR 2.216383 Loss 19.070193, Accuracy 0.059%\n",
      "Epoch 9, Batch 320, LR 2.216576 Loss 19.070076, Accuracy 0.059%\n",
      "Epoch 9, Batch 321, LR 2.216768 Loss 19.070366, Accuracy 0.058%\n",
      "Epoch 9, Batch 322, LR 2.216961 Loss 19.070239, Accuracy 0.058%\n",
      "Epoch 9, Batch 323, LR 2.217153 Loss 19.069983, Accuracy 0.058%\n",
      "Epoch 9, Batch 324, LR 2.217345 Loss 19.070455, Accuracy 0.058%\n",
      "Epoch 9, Batch 325, LR 2.217538 Loss 19.070956, Accuracy 0.058%\n",
      "Epoch 9, Batch 326, LR 2.217730 Loss 19.070849, Accuracy 0.058%\n",
      "Epoch 9, Batch 327, LR 2.217922 Loss 19.070673, Accuracy 0.057%\n",
      "Epoch 9, Batch 328, LR 2.218114 Loss 19.070940, Accuracy 0.057%\n",
      "Epoch 9, Batch 329, LR 2.218306 Loss 19.071283, Accuracy 0.057%\n",
      "Epoch 9, Batch 330, LR 2.218498 Loss 19.071108, Accuracy 0.057%\n",
      "Epoch 9, Batch 331, LR 2.218690 Loss 19.070816, Accuracy 0.057%\n",
      "Epoch 9, Batch 332, LR 2.218882 Loss 19.070914, Accuracy 0.056%\n",
      "Epoch 9, Batch 333, LR 2.219074 Loss 19.071018, Accuracy 0.056%\n",
      "Epoch 9, Batch 334, LR 2.219266 Loss 19.070923, Accuracy 0.056%\n",
      "Epoch 9, Batch 335, LR 2.219457 Loss 19.070551, Accuracy 0.056%\n",
      "Epoch 9, Batch 336, LR 2.219649 Loss 19.069882, Accuracy 0.056%\n",
      "Epoch 9, Batch 337, LR 2.219841 Loss 19.069708, Accuracy 0.056%\n",
      "Epoch 9, Batch 338, LR 2.220032 Loss 19.069896, Accuracy 0.055%\n",
      "Epoch 9, Batch 339, LR 2.220224 Loss 19.069905, Accuracy 0.055%\n",
      "Epoch 9, Batch 340, LR 2.220415 Loss 19.070717, Accuracy 0.055%\n",
      "Epoch 9, Batch 341, LR 2.220606 Loss 19.070658, Accuracy 0.055%\n",
      "Epoch 9, Batch 342, LR 2.220798 Loss 19.070818, Accuracy 0.055%\n",
      "Epoch 9, Batch 343, LR 2.220989 Loss 19.071080, Accuracy 0.055%\n",
      "Epoch 9, Batch 344, LR 2.221180 Loss 19.071433, Accuracy 0.055%\n",
      "Epoch 9, Batch 345, LR 2.221371 Loss 19.071446, Accuracy 0.057%\n",
      "Epoch 9, Batch 346, LR 2.221562 Loss 19.071732, Accuracy 0.056%\n",
      "Epoch 9, Batch 347, LR 2.221753 Loss 19.071918, Accuracy 0.056%\n",
      "Epoch 9, Batch 348, LR 2.221944 Loss 19.071575, Accuracy 0.056%\n",
      "Epoch 9, Batch 349, LR 2.222135 Loss 19.071798, Accuracy 0.056%\n",
      "Epoch 9, Batch 350, LR 2.222326 Loss 19.072340, Accuracy 0.056%\n",
      "Epoch 9, Batch 351, LR 2.222517 Loss 19.071816, Accuracy 0.058%\n",
      "Epoch 9, Batch 352, LR 2.222708 Loss 19.071661, Accuracy 0.058%\n",
      "Epoch 9, Batch 353, LR 2.222898 Loss 19.072383, Accuracy 0.058%\n",
      "Epoch 9, Batch 354, LR 2.223089 Loss 19.072501, Accuracy 0.057%\n",
      "Epoch 9, Batch 355, LR 2.223280 Loss 19.072427, Accuracy 0.057%\n",
      "Epoch 9, Batch 356, LR 2.223470 Loss 19.072163, Accuracy 0.057%\n",
      "Epoch 9, Batch 357, LR 2.223661 Loss 19.072640, Accuracy 0.057%\n",
      "Epoch 9, Batch 358, LR 2.223851 Loss 19.072438, Accuracy 0.057%\n",
      "Epoch 9, Batch 359, LR 2.224041 Loss 19.072376, Accuracy 0.057%\n",
      "Epoch 9, Batch 360, LR 2.224232 Loss 19.072409, Accuracy 0.056%\n",
      "Epoch 9, Batch 361, LR 2.224422 Loss 19.072822, Accuracy 0.056%\n",
      "Epoch 9, Batch 362, LR 2.224612 Loss 19.073378, Accuracy 0.056%\n",
      "Epoch 9, Batch 363, LR 2.224802 Loss 19.073176, Accuracy 0.056%\n",
      "Epoch 9, Batch 364, LR 2.224992 Loss 19.073333, Accuracy 0.056%\n",
      "Epoch 9, Batch 365, LR 2.225182 Loss 19.072999, Accuracy 0.056%\n",
      "Epoch 9, Batch 366, LR 2.225372 Loss 19.072808, Accuracy 0.055%\n",
      "Epoch 9, Batch 367, LR 2.225562 Loss 19.073017, Accuracy 0.055%\n",
      "Epoch 9, Batch 368, LR 2.225752 Loss 19.073297, Accuracy 0.055%\n",
      "Epoch 9, Batch 369, LR 2.225942 Loss 19.073055, Accuracy 0.055%\n",
      "Epoch 9, Batch 370, LR 2.226132 Loss 19.073233, Accuracy 0.055%\n",
      "Epoch 9, Batch 371, LR 2.226321 Loss 19.073503, Accuracy 0.055%\n",
      "Epoch 9, Batch 372, LR 2.226511 Loss 19.073261, Accuracy 0.055%\n",
      "Epoch 9, Batch 373, LR 2.226700 Loss 19.073419, Accuracy 0.054%\n",
      "Epoch 9, Batch 374, LR 2.226890 Loss 19.073861, Accuracy 0.054%\n",
      "Epoch 9, Batch 375, LR 2.227079 Loss 19.074020, Accuracy 0.054%\n",
      "Epoch 9, Batch 376, LR 2.227269 Loss 19.074263, Accuracy 0.054%\n",
      "Epoch 9, Batch 377, LR 2.227458 Loss 19.074523, Accuracy 0.056%\n",
      "Epoch 9, Batch 378, LR 2.227647 Loss 19.074926, Accuracy 0.056%\n",
      "Epoch 9, Batch 379, LR 2.227836 Loss 19.074577, Accuracy 0.056%\n",
      "Epoch 9, Batch 380, LR 2.228026 Loss 19.074357, Accuracy 0.056%\n",
      "Epoch 9, Batch 381, LR 2.228215 Loss 19.074533, Accuracy 0.055%\n",
      "Epoch 9, Batch 382, LR 2.228404 Loss 19.074824, Accuracy 0.055%\n",
      "Epoch 9, Batch 383, LR 2.228593 Loss 19.074645, Accuracy 0.057%\n",
      "Epoch 9, Batch 384, LR 2.228782 Loss 19.074673, Accuracy 0.059%\n",
      "Epoch 9, Batch 385, LR 2.228970 Loss 19.075273, Accuracy 0.059%\n",
      "Epoch 9, Batch 386, LR 2.229159 Loss 19.075167, Accuracy 0.061%\n",
      "Epoch 9, Batch 387, LR 2.229348 Loss 19.074926, Accuracy 0.061%\n",
      "Epoch 9, Batch 388, LR 2.229537 Loss 19.074899, Accuracy 0.060%\n",
      "Epoch 9, Batch 389, LR 2.229725 Loss 19.074238, Accuracy 0.064%\n",
      "Epoch 9, Batch 390, LR 2.229914 Loss 19.074191, Accuracy 0.064%\n",
      "Epoch 9, Batch 391, LR 2.230102 Loss 19.074101, Accuracy 0.064%\n",
      "Epoch 9, Batch 392, LR 2.230291 Loss 19.074273, Accuracy 0.064%\n",
      "Epoch 9, Batch 393, LR 2.230479 Loss 19.074384, Accuracy 0.064%\n",
      "Epoch 9, Batch 394, LR 2.230668 Loss 19.074065, Accuracy 0.063%\n",
      "Epoch 9, Batch 395, LR 2.230856 Loss 19.074242, Accuracy 0.063%\n",
      "Epoch 9, Batch 396, LR 2.231044 Loss 19.074518, Accuracy 0.065%\n",
      "Epoch 9, Batch 397, LR 2.231232 Loss 19.074194, Accuracy 0.065%\n",
      "Epoch 9, Batch 398, LR 2.231421 Loss 19.074621, Accuracy 0.065%\n",
      "Epoch 9, Batch 399, LR 2.231609 Loss 19.074662, Accuracy 0.065%\n",
      "Epoch 9, Batch 400, LR 2.231797 Loss 19.074148, Accuracy 0.064%\n",
      "Epoch 9, Batch 401, LR 2.231985 Loss 19.073983, Accuracy 0.064%\n",
      "Epoch 9, Batch 402, LR 2.232172 Loss 19.073385, Accuracy 0.064%\n",
      "Epoch 9, Batch 403, LR 2.232360 Loss 19.073147, Accuracy 0.064%\n",
      "Epoch 9, Batch 404, LR 2.232548 Loss 19.073047, Accuracy 0.066%\n",
      "Epoch 9, Batch 405, LR 2.232736 Loss 19.073080, Accuracy 0.066%\n",
      "Epoch 9, Batch 406, LR 2.232923 Loss 19.073251, Accuracy 0.065%\n",
      "Epoch 9, Batch 407, LR 2.233111 Loss 19.073260, Accuracy 0.065%\n",
      "Epoch 9, Batch 408, LR 2.233299 Loss 19.073075, Accuracy 0.065%\n",
      "Epoch 9, Batch 409, LR 2.233486 Loss 19.072678, Accuracy 0.065%\n",
      "Epoch 9, Batch 410, LR 2.233673 Loss 19.072745, Accuracy 0.065%\n",
      "Epoch 9, Batch 411, LR 2.233861 Loss 19.072759, Accuracy 0.067%\n",
      "Epoch 9, Batch 412, LR 2.234048 Loss 19.072931, Accuracy 0.066%\n",
      "Epoch 9, Batch 413, LR 2.234235 Loss 19.073007, Accuracy 0.066%\n",
      "Epoch 9, Batch 414, LR 2.234423 Loss 19.073089, Accuracy 0.066%\n",
      "Epoch 9, Batch 415, LR 2.234610 Loss 19.072951, Accuracy 0.066%\n",
      "Epoch 9, Batch 416, LR 2.234797 Loss 19.073153, Accuracy 0.066%\n",
      "Epoch 9, Batch 417, LR 2.234984 Loss 19.072957, Accuracy 0.066%\n",
      "Epoch 9, Batch 418, LR 2.235171 Loss 19.072878, Accuracy 0.065%\n",
      "Epoch 9, Batch 419, LR 2.235358 Loss 19.073341, Accuracy 0.065%\n",
      "Epoch 9, Batch 420, LR 2.235545 Loss 19.073311, Accuracy 0.065%\n",
      "Epoch 9, Batch 421, LR 2.235731 Loss 19.072796, Accuracy 0.069%\n",
      "Epoch 9, Batch 422, LR 2.235918 Loss 19.073193, Accuracy 0.068%\n",
      "Epoch 9, Batch 423, LR 2.236105 Loss 19.072760, Accuracy 0.068%\n",
      "Epoch 9, Batch 424, LR 2.236291 Loss 19.072443, Accuracy 0.068%\n",
      "Epoch 9, Batch 425, LR 2.236478 Loss 19.072536, Accuracy 0.068%\n",
      "Epoch 9, Batch 426, LR 2.236664 Loss 19.072768, Accuracy 0.068%\n",
      "Epoch 9, Batch 427, LR 2.236851 Loss 19.072484, Accuracy 0.068%\n",
      "Epoch 9, Batch 428, LR 2.237037 Loss 19.072230, Accuracy 0.068%\n",
      "Epoch 9, Batch 429, LR 2.237224 Loss 19.071970, Accuracy 0.069%\n",
      "Epoch 9, Batch 430, LR 2.237410 Loss 19.071915, Accuracy 0.069%\n",
      "Epoch 9, Batch 431, LR 2.237596 Loss 19.071742, Accuracy 0.069%\n",
      "Epoch 9, Batch 432, LR 2.237782 Loss 19.071191, Accuracy 0.069%\n",
      "Epoch 9, Batch 433, LR 2.237968 Loss 19.071305, Accuracy 0.069%\n",
      "Epoch 9, Batch 434, LR 2.238154 Loss 19.071053, Accuracy 0.068%\n",
      "Epoch 9, Batch 435, LR 2.238340 Loss 19.070466, Accuracy 0.068%\n",
      "Epoch 9, Batch 436, LR 2.238526 Loss 19.070385, Accuracy 0.068%\n",
      "Epoch 9, Batch 437, LR 2.238712 Loss 19.070660, Accuracy 0.068%\n",
      "Epoch 9, Batch 438, LR 2.238898 Loss 19.070887, Accuracy 0.070%\n",
      "Epoch 9, Batch 439, LR 2.239084 Loss 19.071232, Accuracy 0.069%\n",
      "Epoch 9, Batch 440, LR 2.239269 Loss 19.071573, Accuracy 0.069%\n",
      "Epoch 9, Batch 441, LR 2.239455 Loss 19.072054, Accuracy 0.069%\n",
      "Epoch 9, Batch 442, LR 2.239641 Loss 19.071901, Accuracy 0.069%\n",
      "Epoch 9, Batch 443, LR 2.239826 Loss 19.072466, Accuracy 0.069%\n",
      "Epoch 9, Batch 444, LR 2.240012 Loss 19.071731, Accuracy 0.070%\n",
      "Epoch 9, Batch 445, LR 2.240197 Loss 19.071877, Accuracy 0.070%\n",
      "Epoch 9, Batch 446, LR 2.240382 Loss 19.072039, Accuracy 0.070%\n",
      "Epoch 9, Batch 447, LR 2.240568 Loss 19.072346, Accuracy 0.070%\n",
      "Epoch 9, Batch 448, LR 2.240753 Loss 19.072572, Accuracy 0.071%\n",
      "Epoch 9, Batch 449, LR 2.240938 Loss 19.072378, Accuracy 0.071%\n",
      "Epoch 9, Batch 450, LR 2.241123 Loss 19.072403, Accuracy 0.071%\n",
      "Epoch 9, Batch 451, LR 2.241308 Loss 19.072740, Accuracy 0.071%\n",
      "Epoch 9, Batch 452, LR 2.241493 Loss 19.072823, Accuracy 0.071%\n",
      "Epoch 9, Batch 453, LR 2.241678 Loss 19.072973, Accuracy 0.071%\n",
      "Epoch 9, Batch 454, LR 2.241863 Loss 19.072896, Accuracy 0.071%\n",
      "Epoch 9, Batch 455, LR 2.242048 Loss 19.072913, Accuracy 0.070%\n",
      "Epoch 9, Batch 456, LR 2.242233 Loss 19.072448, Accuracy 0.070%\n",
      "Epoch 9, Batch 457, LR 2.242417 Loss 19.073102, Accuracy 0.070%\n",
      "Epoch 9, Batch 458, LR 2.242602 Loss 19.073350, Accuracy 0.070%\n",
      "Epoch 9, Batch 459, LR 2.242787 Loss 19.073698, Accuracy 0.070%\n",
      "Epoch 9, Batch 460, LR 2.242971 Loss 19.073809, Accuracy 0.071%\n",
      "Epoch 9, Batch 461, LR 2.243156 Loss 19.074044, Accuracy 0.071%\n",
      "Epoch 9, Batch 462, LR 2.243340 Loss 19.074348, Accuracy 0.071%\n",
      "Epoch 9, Batch 463, LR 2.243524 Loss 19.074410, Accuracy 0.071%\n",
      "Epoch 9, Batch 464, LR 2.243709 Loss 19.074352, Accuracy 0.071%\n",
      "Epoch 9, Batch 465, LR 2.243893 Loss 19.074071, Accuracy 0.071%\n",
      "Epoch 9, Batch 466, LR 2.244077 Loss 19.073991, Accuracy 0.070%\n",
      "Epoch 9, Batch 467, LR 2.244261 Loss 19.073946, Accuracy 0.070%\n",
      "Epoch 9, Batch 468, LR 2.244445 Loss 19.073674, Accuracy 0.070%\n",
      "Epoch 9, Batch 469, LR 2.244629 Loss 19.073115, Accuracy 0.070%\n",
      "Epoch 9, Batch 470, LR 2.244813 Loss 19.073135, Accuracy 0.070%\n",
      "Epoch 9, Batch 471, LR 2.244997 Loss 19.073115, Accuracy 0.070%\n",
      "Epoch 9, Batch 472, LR 2.245181 Loss 19.073467, Accuracy 0.071%\n",
      "Epoch 9, Batch 473, LR 2.245365 Loss 19.073181, Accuracy 0.071%\n",
      "Epoch 9, Batch 474, LR 2.245549 Loss 19.073078, Accuracy 0.071%\n",
      "Epoch 9, Batch 475, LR 2.245732 Loss 19.073162, Accuracy 0.071%\n",
      "Epoch 9, Batch 476, LR 2.245916 Loss 19.073202, Accuracy 0.071%\n",
      "Epoch 9, Batch 477, LR 2.246099 Loss 19.072726, Accuracy 0.070%\n",
      "Epoch 9, Batch 478, LR 2.246283 Loss 19.072388, Accuracy 0.070%\n",
      "Epoch 9, Batch 479, LR 2.246466 Loss 19.072734, Accuracy 0.070%\n",
      "Epoch 9, Batch 480, LR 2.246650 Loss 19.072820, Accuracy 0.070%\n",
      "Epoch 9, Batch 481, LR 2.246833 Loss 19.072944, Accuracy 0.070%\n",
      "Epoch 9, Batch 482, LR 2.247016 Loss 19.072726, Accuracy 0.070%\n",
      "Epoch 9, Batch 483, LR 2.247199 Loss 19.072431, Accuracy 0.070%\n",
      "Epoch 9, Batch 484, LR 2.247383 Loss 19.072479, Accuracy 0.069%\n",
      "Epoch 9, Batch 485, LR 2.247566 Loss 19.072098, Accuracy 0.069%\n",
      "Epoch 9, Batch 486, LR 2.247749 Loss 19.072187, Accuracy 0.069%\n",
      "Epoch 9, Batch 487, LR 2.247932 Loss 19.072188, Accuracy 0.069%\n",
      "Epoch 9, Batch 488, LR 2.248114 Loss 19.072007, Accuracy 0.069%\n",
      "Epoch 9, Batch 489, LR 2.248297 Loss 19.072042, Accuracy 0.069%\n",
      "Epoch 9, Batch 490, LR 2.248480 Loss 19.072088, Accuracy 0.069%\n",
      "Epoch 9, Batch 491, LR 2.248663 Loss 19.071859, Accuracy 0.068%\n",
      "Epoch 9, Batch 492, LR 2.248845 Loss 19.071693, Accuracy 0.068%\n",
      "Epoch 9, Batch 493, LR 2.249028 Loss 19.071589, Accuracy 0.068%\n",
      "Epoch 9, Batch 494, LR 2.249211 Loss 19.071636, Accuracy 0.068%\n",
      "Epoch 9, Batch 495, LR 2.249393 Loss 19.071747, Accuracy 0.068%\n",
      "Epoch 9, Batch 496, LR 2.249576 Loss 19.071740, Accuracy 0.069%\n",
      "Epoch 9, Batch 497, LR 2.249758 Loss 19.071425, Accuracy 0.069%\n",
      "Epoch 9, Batch 498, LR 2.249940 Loss 19.071558, Accuracy 0.069%\n",
      "Epoch 9, Batch 499, LR 2.250122 Loss 19.071544, Accuracy 0.069%\n",
      "Epoch 9, Batch 500, LR 2.250305 Loss 19.071683, Accuracy 0.069%\n",
      "Epoch 9, Batch 501, LR 2.250487 Loss 19.071106, Accuracy 0.069%\n",
      "Epoch 9, Batch 502, LR 2.250669 Loss 19.070999, Accuracy 0.070%\n",
      "Epoch 9, Batch 503, LR 2.250851 Loss 19.070806, Accuracy 0.071%\n",
      "Epoch 9, Batch 504, LR 2.251033 Loss 19.070483, Accuracy 0.071%\n",
      "Epoch 9, Batch 505, LR 2.251215 Loss 19.070432, Accuracy 0.073%\n",
      "Epoch 9, Batch 506, LR 2.251397 Loss 19.070045, Accuracy 0.073%\n",
      "Epoch 9, Batch 507, LR 2.251578 Loss 19.070230, Accuracy 0.072%\n",
      "Epoch 9, Batch 508, LR 2.251760 Loss 19.070361, Accuracy 0.072%\n",
      "Epoch 9, Batch 509, LR 2.251942 Loss 19.070142, Accuracy 0.072%\n",
      "Epoch 9, Batch 510, LR 2.252123 Loss 19.069952, Accuracy 0.072%\n",
      "Epoch 9, Batch 511, LR 2.252305 Loss 19.070387, Accuracy 0.072%\n",
      "Epoch 9, Batch 512, LR 2.252486 Loss 19.070606, Accuracy 0.072%\n",
      "Epoch 9, Batch 513, LR 2.252668 Loss 19.070157, Accuracy 0.072%\n",
      "Epoch 9, Batch 514, LR 2.252849 Loss 19.070579, Accuracy 0.071%\n",
      "Epoch 9, Batch 515, LR 2.253031 Loss 19.070780, Accuracy 0.071%\n",
      "Epoch 9, Batch 516, LR 2.253212 Loss 19.070921, Accuracy 0.071%\n",
      "Epoch 9, Batch 517, LR 2.253393 Loss 19.070497, Accuracy 0.071%\n",
      "Epoch 9, Batch 518, LR 2.253574 Loss 19.070305, Accuracy 0.071%\n",
      "Epoch 9, Batch 519, LR 2.253755 Loss 19.070488, Accuracy 0.071%\n",
      "Epoch 9, Batch 520, LR 2.253936 Loss 19.070437, Accuracy 0.071%\n",
      "Epoch 9, Batch 521, LR 2.254117 Loss 19.070437, Accuracy 0.070%\n",
      "Epoch 9, Batch 522, LR 2.254298 Loss 19.070278, Accuracy 0.070%\n",
      "Epoch 9, Batch 523, LR 2.254479 Loss 19.070260, Accuracy 0.070%\n",
      "Epoch 9, Batch 524, LR 2.254660 Loss 19.070159, Accuracy 0.070%\n",
      "Epoch 9, Batch 525, LR 2.254841 Loss 19.069942, Accuracy 0.070%\n",
      "Epoch 9, Batch 526, LR 2.255021 Loss 19.069981, Accuracy 0.070%\n",
      "Epoch 9, Batch 527, LR 2.255202 Loss 19.069657, Accuracy 0.070%\n",
      "Epoch 9, Batch 528, LR 2.255382 Loss 19.069456, Accuracy 0.070%\n",
      "Epoch 9, Batch 529, LR 2.255563 Loss 19.069322, Accuracy 0.069%\n",
      "Epoch 9, Batch 530, LR 2.255743 Loss 19.068961, Accuracy 0.071%\n",
      "Epoch 9, Batch 531, LR 2.255924 Loss 19.068867, Accuracy 0.071%\n",
      "Epoch 9, Batch 532, LR 2.256104 Loss 19.068648, Accuracy 0.070%\n",
      "Epoch 9, Batch 533, LR 2.256284 Loss 19.068670, Accuracy 0.070%\n",
      "Epoch 9, Batch 534, LR 2.256464 Loss 19.068661, Accuracy 0.072%\n",
      "Epoch 9, Batch 535, LR 2.256645 Loss 19.069139, Accuracy 0.072%\n",
      "Epoch 9, Batch 536, LR 2.256825 Loss 19.069161, Accuracy 0.071%\n",
      "Epoch 9, Batch 537, LR 2.257005 Loss 19.069551, Accuracy 0.071%\n",
      "Epoch 9, Batch 538, LR 2.257185 Loss 19.069311, Accuracy 0.073%\n",
      "Epoch 9, Batch 539, LR 2.257365 Loss 19.069432, Accuracy 0.072%\n",
      "Epoch 9, Batch 540, LR 2.257544 Loss 19.069279, Accuracy 0.072%\n",
      "Epoch 9, Batch 541, LR 2.257724 Loss 19.069159, Accuracy 0.072%\n",
      "Epoch 9, Batch 542, LR 2.257904 Loss 19.068708, Accuracy 0.072%\n",
      "Epoch 9, Batch 543, LR 2.258084 Loss 19.068669, Accuracy 0.072%\n",
      "Epoch 9, Batch 544, LR 2.258263 Loss 19.068485, Accuracy 0.072%\n",
      "Epoch 9, Batch 545, LR 2.258443 Loss 19.068364, Accuracy 0.072%\n",
      "Epoch 9, Batch 546, LR 2.258622 Loss 19.068432, Accuracy 0.073%\n",
      "Epoch 9, Batch 547, LR 2.258802 Loss 19.068289, Accuracy 0.073%\n",
      "Epoch 9, Batch 548, LR 2.258981 Loss 19.068185, Accuracy 0.073%\n",
      "Epoch 9, Batch 549, LR 2.259160 Loss 19.068178, Accuracy 0.073%\n",
      "Epoch 9, Batch 550, LR 2.259340 Loss 19.067945, Accuracy 0.072%\n",
      "Epoch 9, Batch 551, LR 2.259519 Loss 19.068087, Accuracy 0.072%\n",
      "Epoch 9, Batch 552, LR 2.259698 Loss 19.068115, Accuracy 0.074%\n",
      "Epoch 9, Batch 553, LR 2.259877 Loss 19.068089, Accuracy 0.073%\n",
      "Epoch 9, Batch 554, LR 2.260056 Loss 19.068423, Accuracy 0.073%\n",
      "Epoch 9, Batch 555, LR 2.260235 Loss 19.068686, Accuracy 0.073%\n",
      "Epoch 9, Batch 556, LR 2.260414 Loss 19.068899, Accuracy 0.073%\n",
      "Epoch 9, Batch 557, LR 2.260593 Loss 19.069057, Accuracy 0.073%\n",
      "Epoch 9, Batch 558, LR 2.260771 Loss 19.069101, Accuracy 0.073%\n",
      "Epoch 9, Batch 559, LR 2.260950 Loss 19.069026, Accuracy 0.074%\n",
      "Epoch 9, Batch 560, LR 2.261129 Loss 19.069222, Accuracy 0.074%\n",
      "Epoch 9, Batch 561, LR 2.261307 Loss 19.069213, Accuracy 0.074%\n",
      "Epoch 9, Batch 562, LR 2.261486 Loss 19.069333, Accuracy 0.074%\n",
      "Epoch 9, Batch 563, LR 2.261664 Loss 19.069112, Accuracy 0.074%\n",
      "Epoch 9, Batch 564, LR 2.261843 Loss 19.069069, Accuracy 0.073%\n",
      "Epoch 9, Batch 565, LR 2.262021 Loss 19.069145, Accuracy 0.073%\n",
      "Epoch 9, Batch 566, LR 2.262200 Loss 19.069388, Accuracy 0.075%\n",
      "Epoch 9, Batch 567, LR 2.262378 Loss 19.069484, Accuracy 0.074%\n",
      "Epoch 9, Batch 568, LR 2.262556 Loss 19.069662, Accuracy 0.074%\n",
      "Epoch 9, Batch 569, LR 2.262734 Loss 19.069606, Accuracy 0.074%\n",
      "Epoch 9, Batch 570, LR 2.262912 Loss 19.069554, Accuracy 0.074%\n",
      "Epoch 9, Batch 571, LR 2.263090 Loss 19.069284, Accuracy 0.075%\n",
      "Epoch 9, Batch 572, LR 2.263268 Loss 19.069302, Accuracy 0.075%\n",
      "Epoch 9, Batch 573, LR 2.263446 Loss 19.069048, Accuracy 0.075%\n",
      "Epoch 9, Batch 574, LR 2.263624 Loss 19.068918, Accuracy 0.075%\n",
      "Epoch 9, Batch 575, LR 2.263802 Loss 19.068810, Accuracy 0.075%\n",
      "Epoch 9, Batch 576, LR 2.263979 Loss 19.069113, Accuracy 0.076%\n",
      "Epoch 9, Batch 577, LR 2.264157 Loss 19.069032, Accuracy 0.076%\n",
      "Epoch 9, Batch 578, LR 2.264335 Loss 19.068667, Accuracy 0.076%\n",
      "Epoch 9, Batch 579, LR 2.264512 Loss 19.068704, Accuracy 0.076%\n",
      "Epoch 9, Batch 580, LR 2.264690 Loss 19.068623, Accuracy 0.075%\n",
      "Epoch 9, Batch 581, LR 2.264867 Loss 19.069027, Accuracy 0.075%\n",
      "Epoch 9, Batch 582, LR 2.265044 Loss 19.069197, Accuracy 0.075%\n",
      "Epoch 9, Batch 583, LR 2.265222 Loss 19.069158, Accuracy 0.075%\n",
      "Epoch 9, Batch 584, LR 2.265399 Loss 19.069146, Accuracy 0.075%\n",
      "Epoch 9, Batch 585, LR 2.265576 Loss 19.069312, Accuracy 0.076%\n",
      "Epoch 9, Batch 586, LR 2.265753 Loss 19.069428, Accuracy 0.076%\n",
      "Epoch 9, Batch 587, LR 2.265930 Loss 19.069507, Accuracy 0.076%\n",
      "Epoch 9, Batch 588, LR 2.266107 Loss 19.069535, Accuracy 0.076%\n",
      "Epoch 9, Batch 589, LR 2.266284 Loss 19.069555, Accuracy 0.076%\n",
      "Epoch 9, Batch 590, LR 2.266461 Loss 19.069573, Accuracy 0.075%\n",
      "Epoch 9, Batch 591, LR 2.266638 Loss 19.069218, Accuracy 0.075%\n",
      "Epoch 9, Batch 592, LR 2.266815 Loss 19.069399, Accuracy 0.075%\n",
      "Epoch 9, Batch 593, LR 2.266991 Loss 19.069890, Accuracy 0.075%\n",
      "Epoch 9, Batch 594, LR 2.267168 Loss 19.069670, Accuracy 0.075%\n",
      "Epoch 9, Batch 595, LR 2.267344 Loss 19.069727, Accuracy 0.075%\n",
      "Epoch 9, Batch 596, LR 2.267521 Loss 19.069864, Accuracy 0.075%\n",
      "Epoch 9, Batch 597, LR 2.267697 Loss 19.069935, Accuracy 0.075%\n",
      "Epoch 9, Batch 598, LR 2.267874 Loss 19.069910, Accuracy 0.076%\n",
      "Epoch 9, Batch 599, LR 2.268050 Loss 19.069725, Accuracy 0.076%\n",
      "Epoch 9, Batch 600, LR 2.268226 Loss 19.069701, Accuracy 0.076%\n",
      "Epoch 9, Batch 601, LR 2.268403 Loss 19.069629, Accuracy 0.077%\n",
      "Epoch 9, Batch 602, LR 2.268579 Loss 19.069586, Accuracy 0.077%\n",
      "Epoch 9, Batch 603, LR 2.268755 Loss 19.069407, Accuracy 0.076%\n",
      "Epoch 9, Batch 604, LR 2.268931 Loss 19.069309, Accuracy 0.078%\n",
      "Epoch 9, Batch 605, LR 2.269107 Loss 19.069526, Accuracy 0.077%\n",
      "Epoch 9, Batch 606, LR 2.269283 Loss 19.069499, Accuracy 0.077%\n",
      "Epoch 9, Batch 607, LR 2.269459 Loss 19.069325, Accuracy 0.077%\n",
      "Epoch 9, Batch 608, LR 2.269634 Loss 19.068838, Accuracy 0.077%\n",
      "Epoch 9, Batch 609, LR 2.269810 Loss 19.069094, Accuracy 0.077%\n",
      "Epoch 9, Batch 610, LR 2.269986 Loss 19.069231, Accuracy 0.078%\n",
      "Epoch 9, Batch 611, LR 2.270162 Loss 19.069340, Accuracy 0.078%\n",
      "Epoch 9, Batch 612, LR 2.270337 Loss 19.069293, Accuracy 0.079%\n",
      "Epoch 9, Batch 613, LR 2.270513 Loss 19.069155, Accuracy 0.079%\n",
      "Epoch 9, Batch 614, LR 2.270688 Loss 19.068895, Accuracy 0.079%\n",
      "Epoch 9, Batch 615, LR 2.270863 Loss 19.069004, Accuracy 0.079%\n",
      "Epoch 9, Batch 616, LR 2.271039 Loss 19.068675, Accuracy 0.079%\n",
      "Epoch 9, Batch 617, LR 2.271214 Loss 19.068774, Accuracy 0.079%\n",
      "Epoch 9, Batch 618, LR 2.271389 Loss 19.068862, Accuracy 0.078%\n",
      "Epoch 9, Batch 619, LR 2.271564 Loss 19.069104, Accuracy 0.078%\n",
      "Epoch 9, Batch 620, LR 2.271739 Loss 19.069112, Accuracy 0.079%\n",
      "Epoch 9, Batch 621, LR 2.271914 Loss 19.069320, Accuracy 0.079%\n",
      "Epoch 9, Batch 622, LR 2.272089 Loss 19.069160, Accuracy 0.080%\n",
      "Epoch 9, Batch 623, LR 2.272264 Loss 19.069288, Accuracy 0.082%\n",
      "Epoch 9, Batch 624, LR 2.272439 Loss 19.069585, Accuracy 0.081%\n",
      "Epoch 9, Batch 625, LR 2.272614 Loss 19.069779, Accuracy 0.081%\n",
      "Epoch 9, Batch 626, LR 2.272789 Loss 19.069712, Accuracy 0.081%\n",
      "Epoch 9, Batch 627, LR 2.272963 Loss 19.069886, Accuracy 0.081%\n",
      "Epoch 9, Batch 628, LR 2.273138 Loss 19.069811, Accuracy 0.081%\n",
      "Epoch 9, Batch 629, LR 2.273312 Loss 19.069782, Accuracy 0.081%\n",
      "Epoch 9, Batch 630, LR 2.273487 Loss 19.069965, Accuracy 0.081%\n",
      "Epoch 9, Batch 631, LR 2.273661 Loss 19.070134, Accuracy 0.082%\n",
      "Epoch 9, Batch 632, LR 2.273836 Loss 19.070198, Accuracy 0.082%\n",
      "Epoch 9, Batch 633, LR 2.274010 Loss 19.070399, Accuracy 0.081%\n",
      "Epoch 9, Batch 634, LR 2.274184 Loss 19.070601, Accuracy 0.081%\n",
      "Epoch 9, Batch 635, LR 2.274358 Loss 19.070519, Accuracy 0.081%\n",
      "Epoch 9, Batch 636, LR 2.274532 Loss 19.070374, Accuracy 0.082%\n",
      "Epoch 9, Batch 637, LR 2.274707 Loss 19.070385, Accuracy 0.082%\n",
      "Epoch 9, Batch 638, LR 2.274881 Loss 19.070456, Accuracy 0.082%\n",
      "Epoch 9, Batch 639, LR 2.275054 Loss 19.070280, Accuracy 0.082%\n",
      "Epoch 9, Batch 640, LR 2.275228 Loss 19.069941, Accuracy 0.082%\n",
      "Epoch 9, Batch 641, LR 2.275402 Loss 19.069799, Accuracy 0.083%\n",
      "Epoch 9, Batch 642, LR 2.275576 Loss 19.069912, Accuracy 0.083%\n",
      "Epoch 9, Batch 643, LR 2.275750 Loss 19.070011, Accuracy 0.083%\n",
      "Epoch 9, Batch 644, LR 2.275923 Loss 19.070210, Accuracy 0.082%\n",
      "Epoch 9, Batch 645, LR 2.276097 Loss 19.070469, Accuracy 0.082%\n",
      "Epoch 9, Batch 646, LR 2.276270 Loss 19.070415, Accuracy 0.082%\n",
      "Epoch 9, Batch 647, LR 2.276444 Loss 19.070422, Accuracy 0.083%\n",
      "Epoch 9, Batch 648, LR 2.276617 Loss 19.070086, Accuracy 0.084%\n",
      "Epoch 9, Batch 649, LR 2.276791 Loss 19.070202, Accuracy 0.084%\n",
      "Epoch 9, Batch 650, LR 2.276964 Loss 19.070688, Accuracy 0.084%\n",
      "Epoch 9, Batch 651, LR 2.277137 Loss 19.070913, Accuracy 0.084%\n",
      "Epoch 9, Batch 652, LR 2.277310 Loss 19.070674, Accuracy 0.084%\n",
      "Epoch 9, Batch 653, LR 2.277483 Loss 19.070513, Accuracy 0.085%\n",
      "Epoch 9, Batch 654, LR 2.277656 Loss 19.070516, Accuracy 0.085%\n",
      "Epoch 9, Batch 655, LR 2.277829 Loss 19.070516, Accuracy 0.085%\n",
      "Epoch 9, Batch 656, LR 2.278002 Loss 19.070511, Accuracy 0.085%\n",
      "Epoch 9, Batch 657, LR 2.278175 Loss 19.070461, Accuracy 0.084%\n",
      "Epoch 9, Batch 658, LR 2.278348 Loss 19.070365, Accuracy 0.084%\n",
      "Epoch 9, Batch 659, LR 2.278521 Loss 19.070178, Accuracy 0.084%\n",
      "Epoch 9, Batch 660, LR 2.278693 Loss 19.070258, Accuracy 0.085%\n",
      "Epoch 9, Batch 661, LR 2.278866 Loss 19.070220, Accuracy 0.085%\n",
      "Epoch 9, Batch 662, LR 2.279038 Loss 19.070556, Accuracy 0.085%\n",
      "Epoch 9, Batch 663, LR 2.279211 Loss 19.070304, Accuracy 0.085%\n",
      "Epoch 9, Batch 664, LR 2.279383 Loss 19.070239, Accuracy 0.085%\n",
      "Epoch 9, Batch 665, LR 2.279556 Loss 19.070219, Accuracy 0.085%\n",
      "Epoch 9, Batch 666, LR 2.279728 Loss 19.070182, Accuracy 0.084%\n",
      "Epoch 9, Batch 667, LR 2.279900 Loss 19.070195, Accuracy 0.084%\n",
      "Epoch 9, Batch 668, LR 2.280072 Loss 19.070333, Accuracy 0.084%\n",
      "Epoch 9, Batch 669, LR 2.280245 Loss 19.070326, Accuracy 0.084%\n",
      "Epoch 9, Batch 670, LR 2.280417 Loss 19.070372, Accuracy 0.084%\n",
      "Epoch 9, Batch 671, LR 2.280589 Loss 19.070523, Accuracy 0.084%\n",
      "Epoch 9, Batch 672, LR 2.280761 Loss 19.070544, Accuracy 0.084%\n",
      "Epoch 9, Batch 673, LR 2.280933 Loss 19.070745, Accuracy 0.084%\n",
      "Epoch 9, Batch 674, LR 2.281104 Loss 19.070641, Accuracy 0.083%\n",
      "Epoch 9, Batch 675, LR 2.281276 Loss 19.070644, Accuracy 0.083%\n",
      "Epoch 9, Batch 676, LR 2.281448 Loss 19.070575, Accuracy 0.083%\n",
      "Epoch 9, Batch 677, LR 2.281619 Loss 19.070405, Accuracy 0.083%\n",
      "Epoch 9, Batch 678, LR 2.281791 Loss 19.070460, Accuracy 0.083%\n",
      "Epoch 9, Batch 679, LR 2.281963 Loss 19.070828, Accuracy 0.083%\n",
      "Epoch 9, Batch 680, LR 2.282134 Loss 19.070587, Accuracy 0.083%\n",
      "Epoch 9, Batch 681, LR 2.282305 Loss 19.070786, Accuracy 0.083%\n",
      "Epoch 9, Batch 682, LR 2.282477 Loss 19.070645, Accuracy 0.084%\n",
      "Epoch 9, Batch 683, LR 2.282648 Loss 19.070609, Accuracy 0.085%\n",
      "Epoch 9, Batch 684, LR 2.282819 Loss 19.070611, Accuracy 0.085%\n",
      "Epoch 9, Batch 685, LR 2.282990 Loss 19.070738, Accuracy 0.084%\n",
      "Epoch 9, Batch 686, LR 2.283161 Loss 19.070685, Accuracy 0.084%\n",
      "Epoch 9, Batch 687, LR 2.283333 Loss 19.070956, Accuracy 0.084%\n",
      "Epoch 9, Batch 688, LR 2.283504 Loss 19.071053, Accuracy 0.084%\n",
      "Epoch 9, Batch 689, LR 2.283674 Loss 19.071012, Accuracy 0.084%\n",
      "Epoch 9, Batch 690, LR 2.283845 Loss 19.071078, Accuracy 0.084%\n",
      "Epoch 9, Batch 691, LR 2.284016 Loss 19.070603, Accuracy 0.084%\n",
      "Epoch 9, Batch 692, LR 2.284187 Loss 19.070637, Accuracy 0.084%\n",
      "Epoch 9, Batch 693, LR 2.284357 Loss 19.070386, Accuracy 0.083%\n",
      "Epoch 9, Batch 694, LR 2.284528 Loss 19.070162, Accuracy 0.083%\n",
      "Epoch 9, Batch 695, LR 2.284699 Loss 19.070199, Accuracy 0.083%\n",
      "Epoch 9, Batch 696, LR 2.284869 Loss 19.070331, Accuracy 0.083%\n",
      "Epoch 9, Batch 697, LR 2.285040 Loss 19.070069, Accuracy 0.083%\n",
      "Epoch 9, Batch 698, LR 2.285210 Loss 19.070092, Accuracy 0.084%\n",
      "Epoch 9, Batch 699, LR 2.285380 Loss 19.069827, Accuracy 0.085%\n",
      "Epoch 9, Batch 700, LR 2.285550 Loss 19.069714, Accuracy 0.085%\n",
      "Epoch 9, Batch 701, LR 2.285721 Loss 19.069878, Accuracy 0.085%\n",
      "Epoch 9, Batch 702, LR 2.285891 Loss 19.070092, Accuracy 0.085%\n",
      "Epoch 9, Batch 703, LR 2.286061 Loss 19.069977, Accuracy 0.084%\n",
      "Epoch 9, Batch 704, LR 2.286231 Loss 19.070015, Accuracy 0.084%\n",
      "Epoch 9, Batch 705, LR 2.286401 Loss 19.070022, Accuracy 0.084%\n",
      "Epoch 9, Batch 706, LR 2.286571 Loss 19.070154, Accuracy 0.084%\n",
      "Epoch 9, Batch 707, LR 2.286741 Loss 19.070157, Accuracy 0.084%\n",
      "Epoch 9, Batch 708, LR 2.286910 Loss 19.070116, Accuracy 0.084%\n",
      "Epoch 9, Batch 709, LR 2.287080 Loss 19.070263, Accuracy 0.085%\n",
      "Epoch 9, Batch 710, LR 2.287250 Loss 19.070168, Accuracy 0.085%\n",
      "Epoch 9, Batch 711, LR 2.287419 Loss 19.070209, Accuracy 0.085%\n",
      "Epoch 9, Batch 712, LR 2.287589 Loss 19.070055, Accuracy 0.084%\n",
      "Epoch 9, Batch 713, LR 2.287758 Loss 19.070148, Accuracy 0.084%\n",
      "Epoch 9, Batch 714, LR 2.287928 Loss 19.070102, Accuracy 0.084%\n",
      "Epoch 9, Batch 715, LR 2.288097 Loss 19.070173, Accuracy 0.085%\n",
      "Epoch 9, Batch 716, LR 2.288266 Loss 19.070412, Accuracy 0.085%\n",
      "Epoch 9, Batch 717, LR 2.288435 Loss 19.070452, Accuracy 0.085%\n",
      "Epoch 9, Batch 718, LR 2.288605 Loss 19.070675, Accuracy 0.085%\n",
      "Epoch 9, Batch 719, LR 2.288774 Loss 19.070485, Accuracy 0.086%\n",
      "Epoch 9, Batch 720, LR 2.288943 Loss 19.070421, Accuracy 0.086%\n",
      "Epoch 9, Batch 721, LR 2.289112 Loss 19.070140, Accuracy 0.086%\n",
      "Epoch 9, Batch 722, LR 2.289281 Loss 19.070025, Accuracy 0.085%\n",
      "Epoch 9, Batch 723, LR 2.289449 Loss 19.069644, Accuracy 0.086%\n",
      "Epoch 9, Batch 724, LR 2.289618 Loss 19.069677, Accuracy 0.086%\n",
      "Epoch 9, Batch 725, LR 2.289787 Loss 19.069546, Accuracy 0.086%\n",
      "Epoch 9, Batch 726, LR 2.289956 Loss 19.069695, Accuracy 0.087%\n",
      "Epoch 9, Batch 727, LR 2.290124 Loss 19.069791, Accuracy 0.087%\n",
      "Epoch 9, Batch 728, LR 2.290293 Loss 19.069703, Accuracy 0.087%\n",
      "Epoch 9, Batch 729, LR 2.290461 Loss 19.069828, Accuracy 0.087%\n",
      "Epoch 9, Batch 730, LR 2.290630 Loss 19.069767, Accuracy 0.087%\n",
      "Epoch 9, Batch 731, LR 2.290798 Loss 19.069894, Accuracy 0.087%\n",
      "Epoch 9, Batch 732, LR 2.290966 Loss 19.070078, Accuracy 0.086%\n",
      "Epoch 9, Batch 733, LR 2.291134 Loss 19.069964, Accuracy 0.086%\n",
      "Epoch 9, Batch 734, LR 2.291303 Loss 19.069961, Accuracy 0.086%\n",
      "Epoch 9, Batch 735, LR 2.291471 Loss 19.069855, Accuracy 0.086%\n",
      "Epoch 9, Batch 736, LR 2.291639 Loss 19.070005, Accuracy 0.086%\n",
      "Epoch 9, Batch 737, LR 2.291807 Loss 19.070083, Accuracy 0.086%\n",
      "Epoch 9, Batch 738, LR 2.291975 Loss 19.070153, Accuracy 0.086%\n",
      "Epoch 9, Batch 739, LR 2.292143 Loss 19.070446, Accuracy 0.086%\n",
      "Epoch 9, Batch 740, LR 2.292310 Loss 19.070508, Accuracy 0.086%\n",
      "Epoch 9, Batch 741, LR 2.292478 Loss 19.070483, Accuracy 0.085%\n",
      "Epoch 9, Batch 742, LR 2.292646 Loss 19.070616, Accuracy 0.085%\n",
      "Epoch 9, Batch 743, LR 2.292813 Loss 19.070578, Accuracy 0.085%\n",
      "Epoch 9, Batch 744, LR 2.292981 Loss 19.070381, Accuracy 0.085%\n",
      "Epoch 9, Batch 745, LR 2.293148 Loss 19.070351, Accuracy 0.085%\n",
      "Epoch 9, Batch 746, LR 2.293316 Loss 19.070231, Accuracy 0.085%\n",
      "Epoch 9, Batch 747, LR 2.293483 Loss 19.070529, Accuracy 0.085%\n",
      "Epoch 9, Batch 748, LR 2.293651 Loss 19.070635, Accuracy 0.085%\n",
      "Epoch 9, Batch 749, LR 2.293818 Loss 19.070514, Accuracy 0.084%\n",
      "Epoch 9, Batch 750, LR 2.293985 Loss 19.070394, Accuracy 0.084%\n",
      "Epoch 9, Batch 751, LR 2.294152 Loss 19.070375, Accuracy 0.084%\n",
      "Epoch 9, Batch 752, LR 2.294319 Loss 19.070231, Accuracy 0.084%\n",
      "Epoch 9, Batch 753, LR 2.294486 Loss 19.070361, Accuracy 0.084%\n",
      "Epoch 9, Batch 754, LR 2.294653 Loss 19.070397, Accuracy 0.084%\n",
      "Epoch 9, Batch 755, LR 2.294820 Loss 19.070493, Accuracy 0.084%\n",
      "Epoch 9, Batch 756, LR 2.294987 Loss 19.070324, Accuracy 0.084%\n",
      "Epoch 9, Batch 757, LR 2.295154 Loss 19.070385, Accuracy 0.084%\n",
      "Epoch 9, Batch 758, LR 2.295320 Loss 19.070431, Accuracy 0.083%\n",
      "Epoch 9, Batch 759, LR 2.295487 Loss 19.070375, Accuracy 0.083%\n",
      "Epoch 9, Batch 760, LR 2.295654 Loss 19.070389, Accuracy 0.083%\n",
      "Epoch 9, Batch 761, LR 2.295820 Loss 19.070415, Accuracy 0.083%\n",
      "Epoch 9, Batch 762, LR 2.295987 Loss 19.070379, Accuracy 0.083%\n",
      "Epoch 9, Batch 763, LR 2.296153 Loss 19.070244, Accuracy 0.084%\n",
      "Epoch 9, Batch 764, LR 2.296319 Loss 19.070288, Accuracy 0.084%\n",
      "Epoch 9, Batch 765, LR 2.296486 Loss 19.070436, Accuracy 0.084%\n",
      "Epoch 9, Batch 766, LR 2.296652 Loss 19.070267, Accuracy 0.084%\n",
      "Epoch 9, Batch 767, LR 2.296818 Loss 19.070435, Accuracy 0.084%\n",
      "Epoch 9, Batch 768, LR 2.296984 Loss 19.070749, Accuracy 0.083%\n",
      "Epoch 9, Batch 769, LR 2.297150 Loss 19.070662, Accuracy 0.084%\n",
      "Epoch 9, Batch 770, LR 2.297316 Loss 19.070468, Accuracy 0.084%\n",
      "Epoch 9, Batch 771, LR 2.297482 Loss 19.070325, Accuracy 0.084%\n",
      "Epoch 9, Batch 772, LR 2.297648 Loss 19.070298, Accuracy 0.084%\n",
      "Epoch 9, Batch 773, LR 2.297813 Loss 19.070336, Accuracy 0.084%\n",
      "Epoch 9, Batch 774, LR 2.297979 Loss 19.070276, Accuracy 0.084%\n",
      "Epoch 9, Batch 775, LR 2.298145 Loss 19.070142, Accuracy 0.084%\n",
      "Epoch 9, Batch 776, LR 2.298310 Loss 19.069937, Accuracy 0.084%\n",
      "Epoch 9, Batch 777, LR 2.298476 Loss 19.069716, Accuracy 0.083%\n",
      "Epoch 9, Batch 778, LR 2.298641 Loss 19.069980, Accuracy 0.083%\n",
      "Epoch 9, Batch 779, LR 2.298807 Loss 19.069706, Accuracy 0.083%\n",
      "Epoch 9, Batch 780, LR 2.298972 Loss 19.069721, Accuracy 0.083%\n",
      "Epoch 9, Batch 781, LR 2.299137 Loss 19.069854, Accuracy 0.083%\n",
      "Epoch 9, Batch 782, LR 2.299303 Loss 19.069748, Accuracy 0.083%\n",
      "Epoch 9, Batch 783, LR 2.299468 Loss 19.069806, Accuracy 0.083%\n",
      "Epoch 9, Batch 784, LR 2.299633 Loss 19.069540, Accuracy 0.083%\n",
      "Epoch 9, Batch 785, LR 2.299798 Loss 19.069464, Accuracy 0.083%\n",
      "Epoch 9, Batch 786, LR 2.299963 Loss 19.069390, Accuracy 0.082%\n",
      "Epoch 9, Batch 787, LR 2.300128 Loss 19.069566, Accuracy 0.082%\n",
      "Epoch 9, Batch 788, LR 2.300293 Loss 19.069564, Accuracy 0.082%\n",
      "Epoch 9, Batch 789, LR 2.300458 Loss 19.069542, Accuracy 0.082%\n",
      "Epoch 9, Batch 790, LR 2.300622 Loss 19.069491, Accuracy 0.082%\n",
      "Epoch 9, Batch 791, LR 2.300787 Loss 19.069363, Accuracy 0.082%\n",
      "Epoch 9, Batch 792, LR 2.300952 Loss 19.069165, Accuracy 0.082%\n",
      "Epoch 9, Batch 793, LR 2.301116 Loss 19.069156, Accuracy 0.082%\n",
      "Epoch 9, Batch 794, LR 2.301281 Loss 19.069222, Accuracy 0.082%\n",
      "Epoch 9, Batch 795, LR 2.301445 Loss 19.069179, Accuracy 0.082%\n",
      "Epoch 9, Batch 796, LR 2.301609 Loss 19.069074, Accuracy 0.081%\n",
      "Epoch 9, Batch 797, LR 2.301774 Loss 19.068921, Accuracy 0.081%\n",
      "Epoch 9, Batch 798, LR 2.301938 Loss 19.068878, Accuracy 0.081%\n",
      "Epoch 9, Batch 799, LR 2.302102 Loss 19.068805, Accuracy 0.081%\n",
      "Epoch 9, Batch 800, LR 2.302266 Loss 19.068796, Accuracy 0.081%\n",
      "Epoch 9, Batch 801, LR 2.302430 Loss 19.068828, Accuracy 0.081%\n",
      "Epoch 9, Batch 802, LR 2.302594 Loss 19.068931, Accuracy 0.081%\n",
      "Epoch 9, Batch 803, LR 2.302758 Loss 19.068872, Accuracy 0.081%\n",
      "Epoch 9, Batch 804, LR 2.302922 Loss 19.069001, Accuracy 0.081%\n",
      "Epoch 9, Batch 805, LR 2.303086 Loss 19.068911, Accuracy 0.081%\n",
      "Epoch 9, Batch 806, LR 2.303249 Loss 19.069076, Accuracy 0.080%\n",
      "Epoch 9, Batch 807, LR 2.303413 Loss 19.069008, Accuracy 0.080%\n",
      "Epoch 9, Batch 808, LR 2.303577 Loss 19.068988, Accuracy 0.080%\n",
      "Epoch 9, Batch 809, LR 2.303740 Loss 19.069013, Accuracy 0.080%\n",
      "Epoch 9, Batch 810, LR 2.303904 Loss 19.069147, Accuracy 0.080%\n",
      "Epoch 9, Batch 811, LR 2.304067 Loss 19.069153, Accuracy 0.080%\n",
      "Epoch 9, Batch 812, LR 2.304231 Loss 19.069156, Accuracy 0.080%\n",
      "Epoch 9, Batch 813, LR 2.304394 Loss 19.069289, Accuracy 0.080%\n",
      "Epoch 9, Batch 814, LR 2.304557 Loss 19.069148, Accuracy 0.080%\n",
      "Epoch 9, Batch 815, LR 2.304720 Loss 19.069169, Accuracy 0.080%\n",
      "Epoch 9, Batch 816, LR 2.304883 Loss 19.068851, Accuracy 0.080%\n",
      "Epoch 9, Batch 817, LR 2.305047 Loss 19.068594, Accuracy 0.080%\n",
      "Epoch 9, Batch 818, LR 2.305210 Loss 19.068620, Accuracy 0.080%\n",
      "Epoch 9, Batch 819, LR 2.305372 Loss 19.068241, Accuracy 0.080%\n",
      "Epoch 9, Batch 820, LR 2.305535 Loss 19.068286, Accuracy 0.080%\n",
      "Epoch 9, Batch 821, LR 2.305698 Loss 19.068107, Accuracy 0.080%\n",
      "Epoch 9, Batch 822, LR 2.305861 Loss 19.067942, Accuracy 0.080%\n",
      "Epoch 9, Batch 823, LR 2.306024 Loss 19.068017, Accuracy 0.080%\n",
      "Epoch 9, Batch 824, LR 2.306186 Loss 19.067991, Accuracy 0.080%\n",
      "Epoch 9, Batch 825, LR 2.306349 Loss 19.067858, Accuracy 0.080%\n",
      "Epoch 9, Batch 826, LR 2.306511 Loss 19.067794, Accuracy 0.079%\n",
      "Epoch 9, Batch 827, LR 2.306674 Loss 19.067740, Accuracy 0.079%\n",
      "Epoch 9, Batch 828, LR 2.306836 Loss 19.067767, Accuracy 0.080%\n",
      "Epoch 9, Batch 829, LR 2.306998 Loss 19.067676, Accuracy 0.080%\n",
      "Epoch 9, Batch 830, LR 2.307161 Loss 19.067918, Accuracy 0.080%\n",
      "Epoch 9, Batch 831, LR 2.307323 Loss 19.067736, Accuracy 0.080%\n",
      "Epoch 9, Batch 832, LR 2.307485 Loss 19.067839, Accuracy 0.080%\n",
      "Epoch 9, Batch 833, LR 2.307647 Loss 19.067592, Accuracy 0.080%\n",
      "Epoch 9, Batch 834, LR 2.307809 Loss 19.067607, Accuracy 0.080%\n",
      "Epoch 9, Batch 835, LR 2.307971 Loss 19.067637, Accuracy 0.080%\n",
      "Epoch 9, Batch 836, LR 2.308133 Loss 19.067474, Accuracy 0.079%\n",
      "Epoch 9, Batch 837, LR 2.308295 Loss 19.067577, Accuracy 0.079%\n",
      "Epoch 9, Batch 838, LR 2.308456 Loss 19.067459, Accuracy 0.081%\n",
      "Epoch 9, Batch 839, LR 2.308618 Loss 19.067534, Accuracy 0.081%\n",
      "Epoch 9, Batch 840, LR 2.308780 Loss 19.067664, Accuracy 0.081%\n",
      "Epoch 9, Batch 841, LR 2.308941 Loss 19.067665, Accuracy 0.081%\n",
      "Epoch 9, Batch 842, LR 2.309103 Loss 19.067696, Accuracy 0.081%\n",
      "Epoch 9, Batch 843, LR 2.309264 Loss 19.067570, Accuracy 0.081%\n",
      "Epoch 9, Batch 844, LR 2.309426 Loss 19.067412, Accuracy 0.081%\n",
      "Epoch 9, Batch 845, LR 2.309587 Loss 19.067330, Accuracy 0.081%\n",
      "Epoch 9, Batch 846, LR 2.309748 Loss 19.067250, Accuracy 0.081%\n",
      "Epoch 9, Batch 847, LR 2.309909 Loss 19.067141, Accuracy 0.081%\n",
      "Epoch 9, Batch 848, LR 2.310070 Loss 19.067076, Accuracy 0.082%\n",
      "Epoch 9, Batch 849, LR 2.310231 Loss 19.067009, Accuracy 0.082%\n",
      "Epoch 9, Batch 850, LR 2.310392 Loss 19.066987, Accuracy 0.082%\n",
      "Epoch 9, Batch 851, LR 2.310553 Loss 19.066712, Accuracy 0.082%\n",
      "Epoch 9, Batch 852, LR 2.310714 Loss 19.066561, Accuracy 0.082%\n",
      "Epoch 9, Batch 853, LR 2.310875 Loss 19.066594, Accuracy 0.082%\n",
      "Epoch 9, Batch 854, LR 2.311036 Loss 19.066703, Accuracy 0.081%\n",
      "Epoch 9, Batch 855, LR 2.311197 Loss 19.066753, Accuracy 0.081%\n",
      "Epoch 9, Batch 856, LR 2.311357 Loss 19.066626, Accuracy 0.081%\n",
      "Epoch 9, Batch 857, LR 2.311518 Loss 19.066591, Accuracy 0.082%\n",
      "Epoch 9, Batch 858, LR 2.311678 Loss 19.066525, Accuracy 0.082%\n",
      "Epoch 9, Batch 859, LR 2.311839 Loss 19.066629, Accuracy 0.082%\n",
      "Epoch 9, Batch 860, LR 2.311999 Loss 19.066688, Accuracy 0.082%\n",
      "Epoch 9, Batch 861, LR 2.312159 Loss 19.066674, Accuracy 0.082%\n",
      "Epoch 9, Batch 862, LR 2.312320 Loss 19.066757, Accuracy 0.082%\n",
      "Epoch 9, Batch 863, LR 2.312480 Loss 19.066922, Accuracy 0.082%\n",
      "Epoch 9, Batch 864, LR 2.312640 Loss 19.066842, Accuracy 0.082%\n",
      "Epoch 9, Batch 865, LR 2.312800 Loss 19.066817, Accuracy 0.083%\n",
      "Epoch 9, Batch 866, LR 2.312960 Loss 19.066645, Accuracy 0.084%\n",
      "Epoch 9, Batch 867, LR 2.313120 Loss 19.066863, Accuracy 0.084%\n",
      "Epoch 9, Batch 868, LR 2.313280 Loss 19.066826, Accuracy 0.084%\n",
      "Epoch 9, Batch 869, LR 2.313440 Loss 19.066751, Accuracy 0.084%\n",
      "Epoch 9, Batch 870, LR 2.313599 Loss 19.066806, Accuracy 0.084%\n",
      "Epoch 9, Batch 871, LR 2.313759 Loss 19.066677, Accuracy 0.083%\n",
      "Epoch 9, Batch 872, LR 2.313919 Loss 19.066825, Accuracy 0.083%\n",
      "Epoch 9, Batch 873, LR 2.314078 Loss 19.066732, Accuracy 0.083%\n",
      "Epoch 9, Batch 874, LR 2.314238 Loss 19.066900, Accuracy 0.083%\n",
      "Epoch 9, Batch 875, LR 2.314397 Loss 19.066761, Accuracy 0.083%\n",
      "Epoch 9, Batch 876, LR 2.314557 Loss 19.066830, Accuracy 0.083%\n",
      "Epoch 9, Batch 877, LR 2.314716 Loss 19.066774, Accuracy 0.084%\n",
      "Epoch 9, Batch 878, LR 2.314875 Loss 19.066660, Accuracy 0.084%\n",
      "Epoch 9, Batch 879, LR 2.315034 Loss 19.066753, Accuracy 0.084%\n",
      "Epoch 9, Batch 880, LR 2.315194 Loss 19.066970, Accuracy 0.083%\n",
      "Epoch 9, Batch 881, LR 2.315353 Loss 19.066883, Accuracy 0.083%\n",
      "Epoch 9, Batch 882, LR 2.315512 Loss 19.067011, Accuracy 0.083%\n",
      "Epoch 9, Batch 883, LR 2.315671 Loss 19.066839, Accuracy 0.083%\n",
      "Epoch 9, Batch 884, LR 2.315829 Loss 19.066604, Accuracy 0.083%\n",
      "Epoch 9, Batch 885, LR 2.315988 Loss 19.066384, Accuracy 0.083%\n",
      "Epoch 9, Batch 886, LR 2.316147 Loss 19.066451, Accuracy 0.083%\n",
      "Epoch 9, Batch 887, LR 2.316306 Loss 19.066305, Accuracy 0.083%\n",
      "Epoch 9, Batch 888, LR 2.316464 Loss 19.066245, Accuracy 0.083%\n",
      "Epoch 9, Batch 889, LR 2.316623 Loss 19.066076, Accuracy 0.083%\n",
      "Epoch 9, Batch 890, LR 2.316781 Loss 19.065831, Accuracy 0.083%\n",
      "Epoch 9, Batch 891, LR 2.316940 Loss 19.065621, Accuracy 0.082%\n",
      "Epoch 9, Batch 892, LR 2.317098 Loss 19.065409, Accuracy 0.083%\n",
      "Epoch 9, Batch 893, LR 2.317256 Loss 19.065330, Accuracy 0.083%\n",
      "Epoch 9, Batch 894, LR 2.317415 Loss 19.065385, Accuracy 0.083%\n",
      "Epoch 9, Batch 895, LR 2.317573 Loss 19.065257, Accuracy 0.083%\n",
      "Epoch 9, Batch 896, LR 2.317731 Loss 19.065221, Accuracy 0.083%\n",
      "Epoch 9, Batch 897, LR 2.317889 Loss 19.064983, Accuracy 0.083%\n",
      "Epoch 9, Batch 898, LR 2.318047 Loss 19.065093, Accuracy 0.083%\n",
      "Epoch 9, Batch 899, LR 2.318205 Loss 19.064971, Accuracy 0.083%\n",
      "Epoch 9, Batch 900, LR 2.318363 Loss 19.064960, Accuracy 0.082%\n",
      "Epoch 9, Batch 901, LR 2.318521 Loss 19.064923, Accuracy 0.082%\n",
      "Epoch 9, Batch 902, LR 2.318678 Loss 19.064894, Accuracy 0.082%\n",
      "Epoch 9, Batch 903, LR 2.318836 Loss 19.064825, Accuracy 0.082%\n",
      "Epoch 9, Batch 904, LR 2.318994 Loss 19.064862, Accuracy 0.082%\n",
      "Epoch 9, Batch 905, LR 2.319151 Loss 19.065054, Accuracy 0.082%\n",
      "Epoch 9, Batch 906, LR 2.319309 Loss 19.065067, Accuracy 0.082%\n",
      "Epoch 9, Batch 907, LR 2.319466 Loss 19.065051, Accuracy 0.082%\n",
      "Epoch 9, Batch 908, LR 2.319624 Loss 19.065039, Accuracy 0.082%\n",
      "Epoch 9, Batch 909, LR 2.319781 Loss 19.064666, Accuracy 0.082%\n",
      "Epoch 9, Batch 910, LR 2.319938 Loss 19.064857, Accuracy 0.082%\n",
      "Epoch 9, Batch 911, LR 2.320095 Loss 19.065006, Accuracy 0.081%\n",
      "Epoch 9, Batch 912, LR 2.320252 Loss 19.064925, Accuracy 0.081%\n",
      "Epoch 9, Batch 913, LR 2.320409 Loss 19.065087, Accuracy 0.081%\n",
      "Epoch 9, Batch 914, LR 2.320566 Loss 19.065130, Accuracy 0.081%\n",
      "Epoch 9, Batch 915, LR 2.320723 Loss 19.065231, Accuracy 0.081%\n",
      "Epoch 9, Batch 916, LR 2.320880 Loss 19.065200, Accuracy 0.081%\n",
      "Epoch 9, Batch 917, LR 2.321037 Loss 19.065084, Accuracy 0.081%\n",
      "Epoch 9, Batch 918, LR 2.321194 Loss 19.065110, Accuracy 0.081%\n",
      "Epoch 9, Batch 919, LR 2.321350 Loss 19.065009, Accuracy 0.081%\n",
      "Epoch 9, Batch 920, LR 2.321507 Loss 19.064921, Accuracy 0.082%\n",
      "Epoch 9, Batch 921, LR 2.321664 Loss 19.065168, Accuracy 0.082%\n",
      "Epoch 9, Batch 922, LR 2.321820 Loss 19.065170, Accuracy 0.082%\n",
      "Epoch 9, Batch 923, LR 2.321976 Loss 19.064979, Accuracy 0.082%\n",
      "Epoch 9, Batch 924, LR 2.322133 Loss 19.065080, Accuracy 0.082%\n",
      "Epoch 9, Batch 925, LR 2.322289 Loss 19.065041, Accuracy 0.082%\n",
      "Epoch 9, Batch 926, LR 2.322445 Loss 19.064839, Accuracy 0.082%\n",
      "Epoch 9, Batch 927, LR 2.322602 Loss 19.064754, Accuracy 0.082%\n",
      "Epoch 9, Batch 928, LR 2.322758 Loss 19.064714, Accuracy 0.082%\n",
      "Epoch 9, Batch 929, LR 2.322914 Loss 19.064666, Accuracy 0.082%\n",
      "Epoch 9, Batch 930, LR 2.323070 Loss 19.064739, Accuracy 0.081%\n",
      "Epoch 9, Batch 931, LR 2.323226 Loss 19.064547, Accuracy 0.081%\n",
      "Epoch 9, Batch 932, LR 2.323381 Loss 19.064628, Accuracy 0.081%\n",
      "Epoch 9, Batch 933, LR 2.323537 Loss 19.064732, Accuracy 0.081%\n",
      "Epoch 9, Batch 934, LR 2.323693 Loss 19.064677, Accuracy 0.081%\n",
      "Epoch 9, Batch 935, LR 2.323849 Loss 19.064741, Accuracy 0.081%\n",
      "Epoch 9, Batch 936, LR 2.324004 Loss 19.064821, Accuracy 0.081%\n",
      "Epoch 9, Batch 937, LR 2.324160 Loss 19.064836, Accuracy 0.081%\n",
      "Epoch 9, Batch 938, LR 2.324315 Loss 19.064828, Accuracy 0.081%\n",
      "Epoch 9, Batch 939, LR 2.324471 Loss 19.064953, Accuracy 0.081%\n",
      "Epoch 9, Batch 940, LR 2.324626 Loss 19.064969, Accuracy 0.081%\n",
      "Epoch 9, Batch 941, LR 2.324781 Loss 19.064926, Accuracy 0.081%\n",
      "Epoch 9, Batch 942, LR 2.324936 Loss 19.064940, Accuracy 0.080%\n",
      "Epoch 9, Batch 943, LR 2.325092 Loss 19.064981, Accuracy 0.080%\n",
      "Epoch 9, Batch 944, LR 2.325247 Loss 19.064984, Accuracy 0.080%\n",
      "Epoch 9, Batch 945, LR 2.325402 Loss 19.064827, Accuracy 0.080%\n",
      "Epoch 9, Batch 946, LR 2.325557 Loss 19.064740, Accuracy 0.080%\n",
      "Epoch 9, Batch 947, LR 2.325712 Loss 19.064654, Accuracy 0.081%\n",
      "Epoch 9, Batch 948, LR 2.325866 Loss 19.065012, Accuracy 0.081%\n",
      "Epoch 9, Batch 949, LR 2.326021 Loss 19.064897, Accuracy 0.081%\n",
      "Epoch 9, Batch 950, LR 2.326176 Loss 19.064706, Accuracy 0.081%\n",
      "Epoch 9, Batch 951, LR 2.326331 Loss 19.064731, Accuracy 0.081%\n",
      "Epoch 9, Batch 952, LR 2.326485 Loss 19.064641, Accuracy 0.081%\n",
      "Epoch 9, Batch 953, LR 2.326640 Loss 19.064758, Accuracy 0.081%\n",
      "Epoch 9, Batch 954, LR 2.326794 Loss 19.064695, Accuracy 0.081%\n",
      "Epoch 9, Batch 955, LR 2.326949 Loss 19.064712, Accuracy 0.081%\n",
      "Epoch 9, Batch 956, LR 2.327103 Loss 19.064576, Accuracy 0.081%\n",
      "Epoch 9, Batch 957, LR 2.327257 Loss 19.064618, Accuracy 0.081%\n",
      "Epoch 9, Batch 958, LR 2.327411 Loss 19.064664, Accuracy 0.081%\n",
      "Epoch 9, Batch 959, LR 2.327566 Loss 19.064612, Accuracy 0.081%\n",
      "Epoch 9, Batch 960, LR 2.327720 Loss 19.064367, Accuracy 0.081%\n",
      "Epoch 9, Batch 961, LR 2.327874 Loss 19.064415, Accuracy 0.080%\n",
      "Epoch 9, Batch 962, LR 2.328028 Loss 19.064306, Accuracy 0.080%\n",
      "Epoch 9, Batch 963, LR 2.328181 Loss 19.064078, Accuracy 0.081%\n",
      "Epoch 9, Batch 964, LR 2.328335 Loss 19.063951, Accuracy 0.081%\n",
      "Epoch 9, Batch 965, LR 2.328489 Loss 19.063893, Accuracy 0.082%\n",
      "Epoch 9, Batch 966, LR 2.328643 Loss 19.063967, Accuracy 0.082%\n",
      "Epoch 9, Batch 967, LR 2.328796 Loss 19.064115, Accuracy 0.082%\n",
      "Epoch 9, Batch 968, LR 2.328950 Loss 19.063994, Accuracy 0.082%\n",
      "Epoch 9, Batch 969, LR 2.329103 Loss 19.063893, Accuracy 0.081%\n",
      "Epoch 9, Batch 970, LR 2.329257 Loss 19.064024, Accuracy 0.081%\n",
      "Epoch 9, Batch 971, LR 2.329410 Loss 19.064095, Accuracy 0.081%\n",
      "Epoch 9, Batch 972, LR 2.329564 Loss 19.064130, Accuracy 0.081%\n",
      "Epoch 9, Batch 973, LR 2.329717 Loss 19.063946, Accuracy 0.081%\n",
      "Epoch 9, Batch 974, LR 2.329870 Loss 19.064059, Accuracy 0.081%\n",
      "Epoch 9, Batch 975, LR 2.330023 Loss 19.064120, Accuracy 0.082%\n",
      "Epoch 9, Batch 976, LR 2.330176 Loss 19.064104, Accuracy 0.082%\n",
      "Epoch 9, Batch 977, LR 2.330329 Loss 19.064283, Accuracy 0.082%\n",
      "Epoch 9, Batch 978, LR 2.330482 Loss 19.064382, Accuracy 0.081%\n",
      "Epoch 9, Batch 979, LR 2.330635 Loss 19.064277, Accuracy 0.081%\n",
      "Epoch 9, Batch 980, LR 2.330788 Loss 19.064335, Accuracy 0.081%\n",
      "Epoch 9, Batch 981, LR 2.330941 Loss 19.064322, Accuracy 0.081%\n",
      "Epoch 9, Batch 982, LR 2.331093 Loss 19.064592, Accuracy 0.081%\n",
      "Epoch 9, Batch 983, LR 2.331246 Loss 19.064686, Accuracy 0.081%\n",
      "Epoch 9, Batch 984, LR 2.331398 Loss 19.064596, Accuracy 0.081%\n",
      "Epoch 9, Batch 985, LR 2.331551 Loss 19.064662, Accuracy 0.081%\n",
      "Epoch 9, Batch 986, LR 2.331703 Loss 19.064475, Accuracy 0.081%\n",
      "Epoch 9, Batch 987, LR 2.331856 Loss 19.064587, Accuracy 0.081%\n",
      "Epoch 9, Batch 988, LR 2.332008 Loss 19.064649, Accuracy 0.081%\n",
      "Epoch 9, Batch 989, LR 2.332160 Loss 19.064572, Accuracy 0.081%\n",
      "Epoch 9, Batch 990, LR 2.332312 Loss 19.064528, Accuracy 0.080%\n",
      "Epoch 9, Batch 991, LR 2.332465 Loss 19.064719, Accuracy 0.080%\n",
      "Epoch 9, Batch 992, LR 2.332617 Loss 19.064702, Accuracy 0.080%\n",
      "Epoch 9, Batch 993, LR 2.332769 Loss 19.064794, Accuracy 0.080%\n",
      "Epoch 9, Batch 994, LR 2.332920 Loss 19.064767, Accuracy 0.080%\n",
      "Epoch 9, Batch 995, LR 2.333072 Loss 19.064708, Accuracy 0.080%\n",
      "Epoch 9, Batch 996, LR 2.333224 Loss 19.064677, Accuracy 0.080%\n",
      "Epoch 9, Batch 997, LR 2.333376 Loss 19.064776, Accuracy 0.080%\n",
      "Epoch 9, Batch 998, LR 2.333527 Loss 19.064805, Accuracy 0.080%\n",
      "Epoch 9, Batch 999, LR 2.333679 Loss 19.064957, Accuracy 0.080%\n",
      "Epoch 9, Batch 1000, LR 2.333831 Loss 19.064978, Accuracy 0.080%\n",
      "Epoch 9, Batch 1001, LR 2.333982 Loss 19.065128, Accuracy 0.080%\n",
      "Epoch 9, Batch 1002, LR 2.334133 Loss 19.065127, Accuracy 0.080%\n",
      "Epoch 9, Batch 1003, LR 2.334285 Loss 19.064973, Accuracy 0.080%\n",
      "Epoch 9, Batch 1004, LR 2.334436 Loss 19.064820, Accuracy 0.081%\n",
      "Epoch 9, Batch 1005, LR 2.334587 Loss 19.064929, Accuracy 0.081%\n",
      "Epoch 9, Batch 1006, LR 2.334738 Loss 19.065000, Accuracy 0.081%\n",
      "Epoch 9, Batch 1007, LR 2.334890 Loss 19.064944, Accuracy 0.081%\n",
      "Epoch 9, Batch 1008, LR 2.335041 Loss 19.065189, Accuracy 0.081%\n",
      "Epoch 9, Batch 1009, LR 2.335192 Loss 19.065085, Accuracy 0.081%\n",
      "Epoch 9, Batch 1010, LR 2.335342 Loss 19.065113, Accuracy 0.080%\n",
      "Epoch 9, Batch 1011, LR 2.335493 Loss 19.065105, Accuracy 0.080%\n",
      "Epoch 9, Batch 1012, LR 2.335644 Loss 19.065177, Accuracy 0.080%\n",
      "Epoch 9, Batch 1013, LR 2.335795 Loss 19.065279, Accuracy 0.080%\n",
      "Epoch 9, Batch 1014, LR 2.335945 Loss 19.064949, Accuracy 0.081%\n",
      "Epoch 9, Batch 1015, LR 2.336096 Loss 19.065146, Accuracy 0.081%\n",
      "Epoch 9, Batch 1016, LR 2.336246 Loss 19.064895, Accuracy 0.081%\n",
      "Epoch 9, Batch 1017, LR 2.336397 Loss 19.064789, Accuracy 0.081%\n",
      "Epoch 9, Batch 1018, LR 2.336547 Loss 19.064743, Accuracy 0.081%\n",
      "Epoch 9, Batch 1019, LR 2.336698 Loss 19.064884, Accuracy 0.081%\n",
      "Epoch 9, Batch 1020, LR 2.336848 Loss 19.064926, Accuracy 0.080%\n",
      "Epoch 9, Batch 1021, LR 2.336998 Loss 19.064985, Accuracy 0.080%\n",
      "Epoch 9, Batch 1022, LR 2.337148 Loss 19.064777, Accuracy 0.080%\n",
      "Epoch 9, Batch 1023, LR 2.337298 Loss 19.064873, Accuracy 0.080%\n",
      "Epoch 9, Batch 1024, LR 2.337448 Loss 19.064828, Accuracy 0.080%\n",
      "Epoch 9, Batch 1025, LR 2.337598 Loss 19.064888, Accuracy 0.080%\n",
      "Epoch 9, Batch 1026, LR 2.337748 Loss 19.064978, Accuracy 0.080%\n",
      "Epoch 9, Batch 1027, LR 2.337898 Loss 19.064953, Accuracy 0.080%\n",
      "Epoch 9, Batch 1028, LR 2.338048 Loss 19.064830, Accuracy 0.080%\n",
      "Epoch 9, Batch 1029, LR 2.338197 Loss 19.064878, Accuracy 0.080%\n",
      "Epoch 9, Batch 1030, LR 2.338347 Loss 19.064914, Accuracy 0.080%\n",
      "Epoch 9, Batch 1031, LR 2.338496 Loss 19.065011, Accuracy 0.080%\n",
      "Epoch 9, Batch 1032, LR 2.338646 Loss 19.064782, Accuracy 0.080%\n",
      "Epoch 9, Batch 1033, LR 2.338795 Loss 19.064735, Accuracy 0.080%\n",
      "Epoch 9, Batch 1034, LR 2.338945 Loss 19.064669, Accuracy 0.080%\n",
      "Epoch 9, Batch 1035, LR 2.339094 Loss 19.064644, Accuracy 0.080%\n",
      "Epoch 9, Batch 1036, LR 2.339243 Loss 19.064595, Accuracy 0.081%\n",
      "Epoch 9, Batch 1037, LR 2.339392 Loss 19.064728, Accuracy 0.081%\n",
      "Epoch 9, Batch 1038, LR 2.339541 Loss 19.064552, Accuracy 0.081%\n",
      "Epoch 9, Batch 1039, LR 2.339690 Loss 19.064317, Accuracy 0.081%\n",
      "Epoch 9, Batch 1040, LR 2.339839 Loss 19.064292, Accuracy 0.081%\n",
      "Epoch 9, Batch 1041, LR 2.339988 Loss 19.064203, Accuracy 0.081%\n",
      "Epoch 9, Batch 1042, LR 2.340137 Loss 19.064311, Accuracy 0.081%\n",
      "Epoch 9, Batch 1043, LR 2.340286 Loss 19.064254, Accuracy 0.081%\n",
      "Epoch 9, Batch 1044, LR 2.340435 Loss 19.064426, Accuracy 0.081%\n",
      "Epoch 9, Batch 1045, LR 2.340583 Loss 19.064498, Accuracy 0.081%\n",
      "Epoch 9, Batch 1046, LR 2.340732 Loss 19.064386, Accuracy 0.081%\n",
      "Epoch 9, Batch 1047, LR 2.340880 Loss 19.064280, Accuracy 0.081%\n",
      "Epoch 9, Loss (train set) 19.064280, Accuracy (train set) 0.081%\n",
      "Epoch 9, Accuracy (validation set) 0.118%\n",
      "Epoch 10, Batch 1, LR 2.341029 Loss 19.015886, Accuracy 0.000%\n",
      "Epoch 10, Batch 2, LR 2.341177 Loss 19.038249, Accuracy 0.000%\n",
      "Epoch 10, Batch 3, LR 2.341326 Loss 19.005139, Accuracy 0.000%\n",
      "Epoch 10, Batch 4, LR 2.341474 Loss 18.976856, Accuracy 0.000%\n",
      "Epoch 10, Batch 5, LR 2.341622 Loss 19.033678, Accuracy 0.000%\n",
      "Epoch 10, Batch 6, LR 2.341770 Loss 19.024239, Accuracy 0.000%\n",
      "Epoch 10, Batch 7, LR 2.341918 Loss 19.011589, Accuracy 0.000%\n",
      "Epoch 10, Batch 8, LR 2.342066 Loss 19.015148, Accuracy 0.098%\n",
      "Epoch 10, Batch 9, LR 2.342214 Loss 19.025721, Accuracy 0.087%\n",
      "Epoch 10, Batch 10, LR 2.342362 Loss 19.024351, Accuracy 0.078%\n",
      "Epoch 10, Batch 11, LR 2.342510 Loss 19.031819, Accuracy 0.071%\n",
      "Epoch 10, Batch 12, LR 2.342658 Loss 19.023274, Accuracy 0.065%\n",
      "Epoch 10, Batch 13, LR 2.342805 Loss 19.041124, Accuracy 0.060%\n",
      "Epoch 10, Batch 14, LR 2.342953 Loss 19.050682, Accuracy 0.056%\n",
      "Epoch 10, Batch 15, LR 2.343101 Loss 19.050552, Accuracy 0.052%\n",
      "Epoch 10, Batch 16, LR 2.343248 Loss 19.052701, Accuracy 0.049%\n",
      "Epoch 10, Batch 17, LR 2.343395 Loss 19.053864, Accuracy 0.046%\n",
      "Epoch 10, Batch 18, LR 2.343543 Loss 19.060729, Accuracy 0.043%\n",
      "Epoch 10, Batch 19, LR 2.343690 Loss 19.057807, Accuracy 0.041%\n",
      "Epoch 10, Batch 20, LR 2.343837 Loss 19.057657, Accuracy 0.039%\n",
      "Epoch 10, Batch 21, LR 2.343984 Loss 19.065528, Accuracy 0.037%\n",
      "Epoch 10, Batch 22, LR 2.344132 Loss 19.058251, Accuracy 0.036%\n",
      "Epoch 10, Batch 23, LR 2.344279 Loss 19.057590, Accuracy 0.034%\n",
      "Epoch 10, Batch 24, LR 2.344426 Loss 19.061745, Accuracy 0.033%\n",
      "Epoch 10, Batch 25, LR 2.344573 Loss 19.062932, Accuracy 0.031%\n",
      "Epoch 10, Batch 26, LR 2.344719 Loss 19.068182, Accuracy 0.060%\n",
      "Epoch 10, Batch 27, LR 2.344866 Loss 19.069225, Accuracy 0.058%\n",
      "Epoch 10, Batch 28, LR 2.345013 Loss 19.066534, Accuracy 0.084%\n",
      "Epoch 10, Batch 29, LR 2.345159 Loss 19.072343, Accuracy 0.081%\n",
      "Epoch 10, Batch 30, LR 2.345306 Loss 19.065611, Accuracy 0.078%\n",
      "Epoch 10, Batch 31, LR 2.345453 Loss 19.060411, Accuracy 0.076%\n",
      "Epoch 10, Batch 32, LR 2.345599 Loss 19.059090, Accuracy 0.073%\n",
      "Epoch 10, Batch 33, LR 2.345745 Loss 19.060214, Accuracy 0.095%\n",
      "Epoch 10, Batch 34, LR 2.345892 Loss 19.065016, Accuracy 0.092%\n",
      "Epoch 10, Batch 35, LR 2.346038 Loss 19.077609, Accuracy 0.089%\n",
      "Epoch 10, Batch 36, LR 2.346184 Loss 19.073208, Accuracy 0.087%\n",
      "Epoch 10, Batch 37, LR 2.346330 Loss 19.071520, Accuracy 0.084%\n",
      "Epoch 10, Batch 38, LR 2.346476 Loss 19.069854, Accuracy 0.082%\n",
      "Epoch 10, Batch 39, LR 2.346622 Loss 19.071275, Accuracy 0.080%\n",
      "Epoch 10, Batch 40, LR 2.346768 Loss 19.066545, Accuracy 0.078%\n",
      "Epoch 10, Batch 41, LR 2.346914 Loss 19.061711, Accuracy 0.076%\n",
      "Epoch 10, Batch 42, LR 2.347060 Loss 19.059571, Accuracy 0.074%\n",
      "Epoch 10, Batch 43, LR 2.347206 Loss 19.059000, Accuracy 0.073%\n",
      "Epoch 10, Batch 44, LR 2.347352 Loss 19.063850, Accuracy 0.071%\n",
      "Epoch 10, Batch 45, LR 2.347497 Loss 19.065123, Accuracy 0.069%\n",
      "Epoch 10, Batch 46, LR 2.347643 Loss 19.064831, Accuracy 0.068%\n",
      "Epoch 10, Batch 47, LR 2.347788 Loss 19.064688, Accuracy 0.066%\n",
      "Epoch 10, Batch 48, LR 2.347934 Loss 19.064568, Accuracy 0.065%\n",
      "Epoch 10, Batch 49, LR 2.348079 Loss 19.066297, Accuracy 0.064%\n",
      "Epoch 10, Batch 50, LR 2.348224 Loss 19.066156, Accuracy 0.062%\n",
      "Epoch 10, Batch 51, LR 2.348369 Loss 19.067655, Accuracy 0.061%\n",
      "Epoch 10, Batch 52, LR 2.348515 Loss 19.066476, Accuracy 0.075%\n",
      "Epoch 10, Batch 53, LR 2.348660 Loss 19.065094, Accuracy 0.074%\n",
      "Epoch 10, Batch 54, LR 2.348805 Loss 19.064498, Accuracy 0.072%\n",
      "Epoch 10, Batch 55, LR 2.348950 Loss 19.066562, Accuracy 0.071%\n",
      "Epoch 10, Batch 56, LR 2.349095 Loss 19.064877, Accuracy 0.070%\n",
      "Epoch 10, Batch 57, LR 2.349239 Loss 19.061961, Accuracy 0.069%\n",
      "Epoch 10, Batch 58, LR 2.349384 Loss 19.062498, Accuracy 0.067%\n",
      "Epoch 10, Batch 59, LR 2.349529 Loss 19.062770, Accuracy 0.079%\n",
      "Epoch 10, Batch 60, LR 2.349674 Loss 19.067466, Accuracy 0.078%\n",
      "Epoch 10, Batch 61, LR 2.349818 Loss 19.067549, Accuracy 0.077%\n",
      "Epoch 10, Batch 62, LR 2.349963 Loss 19.067216, Accuracy 0.076%\n",
      "Epoch 10, Batch 63, LR 2.350107 Loss 19.070565, Accuracy 0.074%\n",
      "Epoch 10, Batch 64, LR 2.350252 Loss 19.069082, Accuracy 0.073%\n",
      "Epoch 10, Batch 65, LR 2.350396 Loss 19.066313, Accuracy 0.072%\n",
      "Epoch 10, Batch 66, LR 2.350540 Loss 19.066054, Accuracy 0.071%\n",
      "Epoch 10, Batch 67, LR 2.350684 Loss 19.066321, Accuracy 0.070%\n",
      "Epoch 10, Batch 68, LR 2.350828 Loss 19.067532, Accuracy 0.069%\n",
      "Epoch 10, Batch 69, LR 2.350973 Loss 19.068043, Accuracy 0.068%\n",
      "Epoch 10, Batch 70, LR 2.351117 Loss 19.068998, Accuracy 0.067%\n",
      "Epoch 10, Batch 71, LR 2.351260 Loss 19.070837, Accuracy 0.077%\n",
      "Epoch 10, Batch 72, LR 2.351404 Loss 19.071274, Accuracy 0.087%\n",
      "Epoch 10, Batch 73, LR 2.351548 Loss 19.071562, Accuracy 0.086%\n",
      "Epoch 10, Batch 74, LR 2.351692 Loss 19.070650, Accuracy 0.084%\n",
      "Epoch 10, Batch 75, LR 2.351836 Loss 19.069572, Accuracy 0.083%\n",
      "Epoch 10, Batch 76, LR 2.351979 Loss 19.068681, Accuracy 0.082%\n",
      "Epoch 10, Batch 77, LR 2.352123 Loss 19.066177, Accuracy 0.081%\n",
      "Epoch 10, Batch 78, LR 2.352266 Loss 19.064875, Accuracy 0.080%\n",
      "Epoch 10, Batch 79, LR 2.352410 Loss 19.065459, Accuracy 0.079%\n",
      "Epoch 10, Batch 80, LR 2.352553 Loss 19.066189, Accuracy 0.078%\n",
      "Epoch 10, Batch 81, LR 2.352696 Loss 19.066918, Accuracy 0.077%\n",
      "Epoch 10, Batch 82, LR 2.352839 Loss 19.070272, Accuracy 0.076%\n",
      "Epoch 10, Batch 83, LR 2.352983 Loss 19.066836, Accuracy 0.075%\n",
      "Epoch 10, Batch 84, LR 2.353126 Loss 19.067408, Accuracy 0.074%\n",
      "Epoch 10, Batch 85, LR 2.353269 Loss 19.067618, Accuracy 0.074%\n",
      "Epoch 10, Batch 86, LR 2.353412 Loss 19.067837, Accuracy 0.073%\n",
      "Epoch 10, Batch 87, LR 2.353554 Loss 19.069127, Accuracy 0.072%\n",
      "Epoch 10, Batch 88, LR 2.353697 Loss 19.068208, Accuracy 0.080%\n",
      "Epoch 10, Batch 89, LR 2.353840 Loss 19.071147, Accuracy 0.079%\n",
      "Epoch 10, Batch 90, LR 2.353983 Loss 19.070729, Accuracy 0.078%\n",
      "Epoch 10, Batch 91, LR 2.354125 Loss 19.070880, Accuracy 0.077%\n",
      "Epoch 10, Batch 92, LR 2.354268 Loss 19.073237, Accuracy 0.076%\n",
      "Epoch 10, Batch 93, LR 2.354410 Loss 19.072836, Accuracy 0.076%\n",
      "Epoch 10, Batch 94, LR 2.354553 Loss 19.072789, Accuracy 0.075%\n",
      "Epoch 10, Batch 95, LR 2.354695 Loss 19.075124, Accuracy 0.082%\n",
      "Epoch 10, Batch 96, LR 2.354838 Loss 19.074506, Accuracy 0.081%\n",
      "Epoch 10, Batch 97, LR 2.354980 Loss 19.073648, Accuracy 0.081%\n",
      "Epoch 10, Batch 98, LR 2.355122 Loss 19.071469, Accuracy 0.080%\n",
      "Epoch 10, Batch 99, LR 2.355264 Loss 19.070782, Accuracy 0.079%\n",
      "Epoch 10, Batch 100, LR 2.355406 Loss 19.071938, Accuracy 0.078%\n",
      "Epoch 10, Batch 101, LR 2.355548 Loss 19.072226, Accuracy 0.077%\n",
      "Epoch 10, Batch 102, LR 2.355690 Loss 19.071689, Accuracy 0.077%\n",
      "Epoch 10, Batch 103, LR 2.355832 Loss 19.072305, Accuracy 0.076%\n",
      "Epoch 10, Batch 104, LR 2.355974 Loss 19.071361, Accuracy 0.075%\n",
      "Epoch 10, Batch 105, LR 2.356115 Loss 19.072498, Accuracy 0.082%\n",
      "Epoch 10, Batch 106, LR 2.356257 Loss 19.073792, Accuracy 0.081%\n",
      "Epoch 10, Batch 107, LR 2.356399 Loss 19.074066, Accuracy 0.080%\n",
      "Epoch 10, Batch 108, LR 2.356540 Loss 19.072573, Accuracy 0.080%\n",
      "Epoch 10, Batch 109, LR 2.356682 Loss 19.072265, Accuracy 0.079%\n",
      "Epoch 10, Batch 110, LR 2.356823 Loss 19.070895, Accuracy 0.078%\n",
      "Epoch 10, Batch 111, LR 2.356964 Loss 19.070938, Accuracy 0.077%\n",
      "Epoch 10, Batch 112, LR 2.357106 Loss 19.070908, Accuracy 0.077%\n",
      "Epoch 10, Batch 113, LR 2.357247 Loss 19.071033, Accuracy 0.076%\n",
      "Epoch 10, Batch 114, LR 2.357388 Loss 19.072664, Accuracy 0.075%\n",
      "Epoch 10, Batch 115, LR 2.357529 Loss 19.072017, Accuracy 0.075%\n",
      "Epoch 10, Batch 116, LR 2.357670 Loss 19.072705, Accuracy 0.081%\n",
      "Epoch 10, Batch 117, LR 2.357811 Loss 19.074965, Accuracy 0.080%\n",
      "Epoch 10, Batch 118, LR 2.357952 Loss 19.075428, Accuracy 0.079%\n",
      "Epoch 10, Batch 119, LR 2.358093 Loss 19.074437, Accuracy 0.085%\n",
      "Epoch 10, Batch 120, LR 2.358233 Loss 19.074984, Accuracy 0.085%\n",
      "Epoch 10, Batch 121, LR 2.358374 Loss 19.073086, Accuracy 0.084%\n",
      "Epoch 10, Batch 122, LR 2.358515 Loss 19.072906, Accuracy 0.083%\n",
      "Epoch 10, Batch 123, LR 2.358655 Loss 19.070743, Accuracy 0.083%\n",
      "Epoch 10, Batch 124, LR 2.358796 Loss 19.070024, Accuracy 0.082%\n",
      "Epoch 10, Batch 125, LR 2.358936 Loss 19.069267, Accuracy 0.081%\n",
      "Epoch 10, Batch 126, LR 2.359076 Loss 19.068569, Accuracy 0.081%\n",
      "Epoch 10, Batch 127, LR 2.359217 Loss 19.068123, Accuracy 0.080%\n",
      "Epoch 10, Batch 128, LR 2.359357 Loss 19.069262, Accuracy 0.079%\n",
      "Epoch 10, Batch 129, LR 2.359497 Loss 19.069053, Accuracy 0.079%\n",
      "Epoch 10, Batch 130, LR 2.359637 Loss 19.068580, Accuracy 0.078%\n",
      "Epoch 10, Batch 131, LR 2.359777 Loss 19.069677, Accuracy 0.078%\n",
      "Epoch 10, Batch 132, LR 2.359917 Loss 19.069507, Accuracy 0.077%\n",
      "Epoch 10, Batch 133, LR 2.360057 Loss 19.068178, Accuracy 0.076%\n",
      "Epoch 10, Batch 134, LR 2.360197 Loss 19.066202, Accuracy 0.076%\n",
      "Epoch 10, Batch 135, LR 2.360337 Loss 19.066193, Accuracy 0.075%\n",
      "Epoch 10, Batch 136, LR 2.360476 Loss 19.066608, Accuracy 0.075%\n",
      "Epoch 10, Batch 137, LR 2.360616 Loss 19.066830, Accuracy 0.074%\n",
      "Epoch 10, Batch 138, LR 2.360755 Loss 19.065960, Accuracy 0.074%\n",
      "Epoch 10, Batch 139, LR 2.360895 Loss 19.067082, Accuracy 0.073%\n",
      "Epoch 10, Batch 140, LR 2.361034 Loss 19.065993, Accuracy 0.073%\n",
      "Epoch 10, Batch 141, LR 2.361174 Loss 19.067135, Accuracy 0.072%\n",
      "Epoch 10, Batch 142, LR 2.361313 Loss 19.067487, Accuracy 0.072%\n",
      "Epoch 10, Batch 143, LR 2.361452 Loss 19.067846, Accuracy 0.071%\n",
      "Epoch 10, Batch 144, LR 2.361591 Loss 19.066772, Accuracy 0.071%\n",
      "Epoch 10, Batch 145, LR 2.361731 Loss 19.064762, Accuracy 0.081%\n",
      "Epoch 10, Batch 146, LR 2.361870 Loss 19.064810, Accuracy 0.080%\n",
      "Epoch 10, Batch 147, LR 2.362009 Loss 19.064317, Accuracy 0.080%\n",
      "Epoch 10, Batch 148, LR 2.362147 Loss 19.064247, Accuracy 0.079%\n",
      "Epoch 10, Batch 149, LR 2.362286 Loss 19.064170, Accuracy 0.079%\n",
      "Epoch 10, Batch 150, LR 2.362425 Loss 19.064341, Accuracy 0.078%\n",
      "Epoch 10, Batch 151, LR 2.362564 Loss 19.063793, Accuracy 0.078%\n",
      "Epoch 10, Batch 152, LR 2.362702 Loss 19.064910, Accuracy 0.077%\n",
      "Epoch 10, Batch 153, LR 2.362841 Loss 19.062938, Accuracy 0.077%\n",
      "Epoch 10, Batch 154, LR 2.362979 Loss 19.062812, Accuracy 0.081%\n",
      "Epoch 10, Batch 155, LR 2.363118 Loss 19.062111, Accuracy 0.081%\n",
      "Epoch 10, Batch 156, LR 2.363256 Loss 19.062660, Accuracy 0.080%\n",
      "Epoch 10, Batch 157, LR 2.363395 Loss 19.062549, Accuracy 0.080%\n",
      "Epoch 10, Batch 158, LR 2.363533 Loss 19.061933, Accuracy 0.079%\n",
      "Epoch 10, Batch 159, LR 2.363671 Loss 19.062008, Accuracy 0.079%\n",
      "Epoch 10, Batch 160, LR 2.363809 Loss 19.061418, Accuracy 0.078%\n",
      "Epoch 10, Batch 161, LR 2.363947 Loss 19.061929, Accuracy 0.082%\n",
      "Epoch 10, Batch 162, LR 2.364085 Loss 19.062169, Accuracy 0.087%\n",
      "Epoch 10, Batch 163, LR 2.364223 Loss 19.062971, Accuracy 0.086%\n",
      "Epoch 10, Batch 164, LR 2.364361 Loss 19.061874, Accuracy 0.086%\n",
      "Epoch 10, Batch 165, LR 2.364499 Loss 19.062083, Accuracy 0.085%\n",
      "Epoch 10, Batch 166, LR 2.364636 Loss 19.062256, Accuracy 0.085%\n",
      "Epoch 10, Batch 167, LR 2.364774 Loss 19.062063, Accuracy 0.084%\n",
      "Epoch 10, Batch 168, LR 2.364912 Loss 19.062454, Accuracy 0.084%\n",
      "Epoch 10, Batch 169, LR 2.365049 Loss 19.063499, Accuracy 0.083%\n",
      "Epoch 10, Batch 170, LR 2.365187 Loss 19.064473, Accuracy 0.083%\n",
      "Epoch 10, Batch 171, LR 2.365324 Loss 19.064891, Accuracy 0.082%\n",
      "Epoch 10, Batch 172, LR 2.365461 Loss 19.065321, Accuracy 0.082%\n",
      "Epoch 10, Batch 173, LR 2.365599 Loss 19.065469, Accuracy 0.081%\n",
      "Epoch 10, Batch 174, LR 2.365736 Loss 19.065766, Accuracy 0.085%\n",
      "Epoch 10, Batch 175, LR 2.365873 Loss 19.065434, Accuracy 0.085%\n",
      "Epoch 10, Batch 176, LR 2.366010 Loss 19.066778, Accuracy 0.084%\n",
      "Epoch 10, Batch 177, LR 2.366147 Loss 19.066207, Accuracy 0.084%\n",
      "Epoch 10, Batch 178, LR 2.366284 Loss 19.065834, Accuracy 0.083%\n",
      "Epoch 10, Batch 179, LR 2.366421 Loss 19.065180, Accuracy 0.083%\n",
      "Epoch 10, Batch 180, LR 2.366558 Loss 19.066265, Accuracy 0.082%\n",
      "Epoch 10, Batch 181, LR 2.366694 Loss 19.066124, Accuracy 0.082%\n",
      "Epoch 10, Batch 182, LR 2.366831 Loss 19.065881, Accuracy 0.082%\n",
      "Epoch 10, Batch 183, LR 2.366968 Loss 19.066259, Accuracy 0.081%\n",
      "Epoch 10, Batch 184, LR 2.367104 Loss 19.066285, Accuracy 0.081%\n",
      "Epoch 10, Batch 185, LR 2.367241 Loss 19.065925, Accuracy 0.080%\n",
      "Epoch 10, Batch 186, LR 2.367377 Loss 19.065814, Accuracy 0.080%\n",
      "Epoch 10, Batch 187, LR 2.367513 Loss 19.065573, Accuracy 0.079%\n",
      "Epoch 10, Batch 188, LR 2.367650 Loss 19.064938, Accuracy 0.079%\n",
      "Epoch 10, Batch 189, LR 2.367786 Loss 19.065487, Accuracy 0.079%\n",
      "Epoch 10, Batch 190, LR 2.367922 Loss 19.065267, Accuracy 0.078%\n",
      "Epoch 10, Batch 191, LR 2.368058 Loss 19.064395, Accuracy 0.078%\n",
      "Epoch 10, Batch 192, LR 2.368194 Loss 19.063203, Accuracy 0.077%\n",
      "Epoch 10, Batch 193, LR 2.368330 Loss 19.063625, Accuracy 0.077%\n",
      "Epoch 10, Batch 194, LR 2.368466 Loss 19.062021, Accuracy 0.085%\n",
      "Epoch 10, Batch 195, LR 2.368602 Loss 19.060618, Accuracy 0.084%\n",
      "Epoch 10, Batch 196, LR 2.368737 Loss 19.060274, Accuracy 0.084%\n",
      "Epoch 10, Batch 197, LR 2.368873 Loss 19.060429, Accuracy 0.083%\n",
      "Epoch 10, Batch 198, LR 2.369009 Loss 19.060098, Accuracy 0.087%\n",
      "Epoch 10, Batch 199, LR 2.369144 Loss 19.060784, Accuracy 0.086%\n",
      "Epoch 10, Batch 200, LR 2.369280 Loss 19.060280, Accuracy 0.086%\n",
      "Epoch 10, Batch 201, LR 2.369415 Loss 19.060224, Accuracy 0.089%\n",
      "Epoch 10, Batch 202, LR 2.369551 Loss 19.060167, Accuracy 0.089%\n",
      "Epoch 10, Batch 203, LR 2.369686 Loss 19.060826, Accuracy 0.089%\n",
      "Epoch 10, Batch 204, LR 2.369821 Loss 19.059845, Accuracy 0.092%\n",
      "Epoch 10, Batch 205, LR 2.369956 Loss 19.060006, Accuracy 0.091%\n",
      "Epoch 10, Batch 206, LR 2.370091 Loss 19.060000, Accuracy 0.091%\n",
      "Epoch 10, Batch 207, LR 2.370226 Loss 19.060206, Accuracy 0.091%\n",
      "Epoch 10, Batch 208, LR 2.370361 Loss 19.060898, Accuracy 0.090%\n",
      "Epoch 10, Batch 209, LR 2.370496 Loss 19.061183, Accuracy 0.090%\n",
      "Epoch 10, Batch 210, LR 2.370631 Loss 19.061357, Accuracy 0.093%\n",
      "Epoch 10, Batch 211, LR 2.370766 Loss 19.061197, Accuracy 0.093%\n",
      "Epoch 10, Batch 212, LR 2.370900 Loss 19.060820, Accuracy 0.092%\n",
      "Epoch 10, Batch 213, LR 2.371035 Loss 19.061361, Accuracy 0.092%\n",
      "Epoch 10, Batch 214, LR 2.371170 Loss 19.061671, Accuracy 0.091%\n",
      "Epoch 10, Batch 215, LR 2.371304 Loss 19.061271, Accuracy 0.091%\n",
      "Epoch 10, Batch 216, LR 2.371438 Loss 19.062205, Accuracy 0.090%\n",
      "Epoch 10, Batch 217, LR 2.371573 Loss 19.061992, Accuracy 0.090%\n",
      "Epoch 10, Batch 218, LR 2.371707 Loss 19.061531, Accuracy 0.090%\n",
      "Epoch 10, Batch 219, LR 2.371841 Loss 19.061303, Accuracy 0.089%\n",
      "Epoch 10, Batch 220, LR 2.371975 Loss 19.062942, Accuracy 0.089%\n",
      "Epoch 10, Batch 221, LR 2.372110 Loss 19.063163, Accuracy 0.088%\n",
      "Epoch 10, Batch 222, LR 2.372244 Loss 19.062776, Accuracy 0.091%\n",
      "Epoch 10, Batch 223, LR 2.372378 Loss 19.063823, Accuracy 0.091%\n",
      "Epoch 10, Batch 224, LR 2.372511 Loss 19.063718, Accuracy 0.091%\n",
      "Epoch 10, Batch 225, LR 2.372645 Loss 19.064408, Accuracy 0.090%\n",
      "Epoch 10, Batch 226, LR 2.372779 Loss 19.064589, Accuracy 0.090%\n",
      "Epoch 10, Batch 227, LR 2.372913 Loss 19.064011, Accuracy 0.089%\n",
      "Epoch 10, Batch 228, LR 2.373046 Loss 19.064725, Accuracy 0.089%\n",
      "Epoch 10, Batch 229, LR 2.373180 Loss 19.064847, Accuracy 0.092%\n",
      "Epoch 10, Batch 230, LR 2.373313 Loss 19.064601, Accuracy 0.092%\n",
      "Epoch 10, Batch 231, LR 2.373447 Loss 19.065063, Accuracy 0.091%\n",
      "Epoch 10, Batch 232, LR 2.373580 Loss 19.065282, Accuracy 0.091%\n",
      "Epoch 10, Batch 233, LR 2.373713 Loss 19.065136, Accuracy 0.091%\n",
      "Epoch 10, Batch 234, LR 2.373847 Loss 19.064861, Accuracy 0.090%\n",
      "Epoch 10, Batch 235, LR 2.373980 Loss 19.065451, Accuracy 0.090%\n",
      "Epoch 10, Batch 236, LR 2.374113 Loss 19.065277, Accuracy 0.089%\n",
      "Epoch 10, Batch 237, LR 2.374246 Loss 19.065382, Accuracy 0.089%\n",
      "Epoch 10, Batch 238, LR 2.374379 Loss 19.065188, Accuracy 0.089%\n",
      "Epoch 10, Batch 239, LR 2.374512 Loss 19.064612, Accuracy 0.088%\n",
      "Epoch 10, Batch 240, LR 2.374645 Loss 19.065053, Accuracy 0.088%\n",
      "Epoch 10, Batch 241, LR 2.374777 Loss 19.065456, Accuracy 0.088%\n",
      "Epoch 10, Batch 242, LR 2.374910 Loss 19.065148, Accuracy 0.090%\n",
      "Epoch 10, Batch 243, LR 2.375043 Loss 19.065310, Accuracy 0.090%\n",
      "Epoch 10, Batch 244, LR 2.375175 Loss 19.065229, Accuracy 0.090%\n",
      "Epoch 10, Batch 245, LR 2.375308 Loss 19.065650, Accuracy 0.089%\n",
      "Epoch 10, Batch 246, LR 2.375440 Loss 19.065563, Accuracy 0.089%\n",
      "Epoch 10, Batch 247, LR 2.375573 Loss 19.065888, Accuracy 0.089%\n",
      "Epoch 10, Batch 248, LR 2.375705 Loss 19.065083, Accuracy 0.088%\n",
      "Epoch 10, Batch 249, LR 2.375837 Loss 19.064923, Accuracy 0.088%\n",
      "Epoch 10, Batch 250, LR 2.375969 Loss 19.064909, Accuracy 0.087%\n",
      "Epoch 10, Batch 251, LR 2.376101 Loss 19.064559, Accuracy 0.087%\n",
      "Epoch 10, Batch 252, LR 2.376233 Loss 19.065690, Accuracy 0.087%\n",
      "Epoch 10, Batch 253, LR 2.376365 Loss 19.065568, Accuracy 0.086%\n",
      "Epoch 10, Batch 254, LR 2.376497 Loss 19.066168, Accuracy 0.086%\n",
      "Epoch 10, Batch 255, LR 2.376629 Loss 19.066142, Accuracy 0.086%\n",
      "Epoch 10, Batch 256, LR 2.376761 Loss 19.065924, Accuracy 0.085%\n",
      "Epoch 10, Batch 257, LR 2.376893 Loss 19.065630, Accuracy 0.085%\n",
      "Epoch 10, Batch 258, LR 2.377024 Loss 19.065626, Accuracy 0.085%\n",
      "Epoch 10, Batch 259, LR 2.377156 Loss 19.065822, Accuracy 0.084%\n",
      "Epoch 10, Batch 260, LR 2.377287 Loss 19.066334, Accuracy 0.084%\n",
      "Epoch 10, Batch 261, LR 2.377419 Loss 19.065800, Accuracy 0.084%\n",
      "Epoch 10, Batch 262, LR 2.377550 Loss 19.065471, Accuracy 0.083%\n",
      "Epoch 10, Batch 263, LR 2.377681 Loss 19.065351, Accuracy 0.083%\n",
      "Epoch 10, Batch 264, LR 2.377813 Loss 19.065778, Accuracy 0.083%\n",
      "Epoch 10, Batch 265, LR 2.377944 Loss 19.065046, Accuracy 0.083%\n",
      "Epoch 10, Batch 266, LR 2.378075 Loss 19.064048, Accuracy 0.082%\n",
      "Epoch 10, Batch 267, LR 2.378206 Loss 19.064703, Accuracy 0.082%\n",
      "Epoch 10, Batch 268, LR 2.378337 Loss 19.064884, Accuracy 0.082%\n",
      "Epoch 10, Batch 269, LR 2.378468 Loss 19.064449, Accuracy 0.081%\n",
      "Epoch 10, Batch 270, LR 2.378599 Loss 19.063971, Accuracy 0.081%\n",
      "Epoch 10, Batch 271, LR 2.378729 Loss 19.063664, Accuracy 0.081%\n",
      "Epoch 10, Batch 272, LR 2.378860 Loss 19.063599, Accuracy 0.080%\n",
      "Epoch 10, Batch 273, LR 2.378991 Loss 19.063976, Accuracy 0.080%\n",
      "Epoch 10, Batch 274, LR 2.379121 Loss 19.063459, Accuracy 0.080%\n",
      "Epoch 10, Batch 275, LR 2.379252 Loss 19.063693, Accuracy 0.080%\n",
      "Epoch 10, Batch 276, LR 2.379382 Loss 19.063889, Accuracy 0.079%\n",
      "Epoch 10, Batch 277, LR 2.379513 Loss 19.063669, Accuracy 0.079%\n",
      "Epoch 10, Batch 278, LR 2.379643 Loss 19.063709, Accuracy 0.079%\n",
      "Epoch 10, Batch 279, LR 2.379773 Loss 19.064088, Accuracy 0.078%\n",
      "Epoch 10, Batch 280, LR 2.379903 Loss 19.063499, Accuracy 0.078%\n",
      "Epoch 10, Batch 281, LR 2.380033 Loss 19.064037, Accuracy 0.078%\n",
      "Epoch 10, Batch 282, LR 2.380163 Loss 19.064132, Accuracy 0.078%\n",
      "Epoch 10, Batch 283, LR 2.380293 Loss 19.064160, Accuracy 0.080%\n",
      "Epoch 10, Batch 284, LR 2.380423 Loss 19.064382, Accuracy 0.080%\n",
      "Epoch 10, Batch 285, LR 2.380553 Loss 19.064924, Accuracy 0.079%\n",
      "Epoch 10, Batch 286, LR 2.380683 Loss 19.064672, Accuracy 0.079%\n",
      "Epoch 10, Batch 287, LR 2.380813 Loss 19.064936, Accuracy 0.079%\n",
      "Epoch 10, Batch 288, LR 2.380942 Loss 19.064744, Accuracy 0.079%\n",
      "Epoch 10, Batch 289, LR 2.381072 Loss 19.064718, Accuracy 0.078%\n",
      "Epoch 10, Batch 290, LR 2.381201 Loss 19.065491, Accuracy 0.078%\n",
      "Epoch 10, Batch 291, LR 2.381331 Loss 19.065540, Accuracy 0.078%\n",
      "Epoch 10, Batch 292, LR 2.381460 Loss 19.065294, Accuracy 0.078%\n",
      "Epoch 10, Batch 293, LR 2.381589 Loss 19.065765, Accuracy 0.077%\n",
      "Epoch 10, Batch 294, LR 2.381719 Loss 19.065726, Accuracy 0.077%\n",
      "Epoch 10, Batch 295, LR 2.381848 Loss 19.065886, Accuracy 0.077%\n",
      "Epoch 10, Batch 296, LR 2.381977 Loss 19.066353, Accuracy 0.077%\n",
      "Epoch 10, Batch 297, LR 2.382106 Loss 19.066021, Accuracy 0.076%\n",
      "Epoch 10, Batch 298, LR 2.382235 Loss 19.066673, Accuracy 0.079%\n",
      "Epoch 10, Batch 299, LR 2.382364 Loss 19.066731, Accuracy 0.078%\n",
      "Epoch 10, Batch 300, LR 2.382493 Loss 19.066531, Accuracy 0.078%\n",
      "Epoch 10, Batch 301, LR 2.382621 Loss 19.066566, Accuracy 0.078%\n",
      "Epoch 10, Batch 302, LR 2.382750 Loss 19.066580, Accuracy 0.078%\n",
      "Epoch 10, Batch 303, LR 2.382879 Loss 19.066220, Accuracy 0.077%\n",
      "Epoch 10, Batch 304, LR 2.383007 Loss 19.065467, Accuracy 0.077%\n",
      "Epoch 10, Batch 305, LR 2.383136 Loss 19.065050, Accuracy 0.077%\n",
      "Epoch 10, Batch 306, LR 2.383264 Loss 19.065024, Accuracy 0.077%\n",
      "Epoch 10, Batch 307, LR 2.383392 Loss 19.064934, Accuracy 0.076%\n",
      "Epoch 10, Batch 308, LR 2.383521 Loss 19.065038, Accuracy 0.076%\n",
      "Epoch 10, Batch 309, LR 2.383649 Loss 19.064979, Accuracy 0.076%\n",
      "Epoch 10, Batch 310, LR 2.383777 Loss 19.065128, Accuracy 0.078%\n",
      "Epoch 10, Batch 311, LR 2.383905 Loss 19.065476, Accuracy 0.078%\n",
      "Epoch 10, Batch 312, LR 2.384033 Loss 19.065469, Accuracy 0.078%\n",
      "Epoch 10, Batch 313, LR 2.384161 Loss 19.065476, Accuracy 0.077%\n",
      "Epoch 10, Batch 314, LR 2.384289 Loss 19.065663, Accuracy 0.077%\n",
      "Epoch 10, Batch 315, LR 2.384417 Loss 19.065846, Accuracy 0.077%\n",
      "Epoch 10, Batch 316, LR 2.384545 Loss 19.065566, Accuracy 0.077%\n",
      "Epoch 10, Batch 317, LR 2.384672 Loss 19.065349, Accuracy 0.076%\n",
      "Epoch 10, Batch 318, LR 2.384800 Loss 19.065073, Accuracy 0.076%\n",
      "Epoch 10, Batch 319, LR 2.384928 Loss 19.065209, Accuracy 0.076%\n",
      "Epoch 10, Batch 320, LR 2.385055 Loss 19.065025, Accuracy 0.076%\n",
      "Epoch 10, Batch 321, LR 2.385182 Loss 19.064976, Accuracy 0.075%\n",
      "Epoch 10, Batch 322, LR 2.385310 Loss 19.064560, Accuracy 0.075%\n",
      "Epoch 10, Batch 323, LR 2.385437 Loss 19.064327, Accuracy 0.075%\n",
      "Epoch 10, Batch 324, LR 2.385564 Loss 19.064543, Accuracy 0.075%\n",
      "Epoch 10, Batch 325, LR 2.385692 Loss 19.064690, Accuracy 0.075%\n",
      "Epoch 10, Batch 326, LR 2.385819 Loss 19.064211, Accuracy 0.074%\n",
      "Epoch 10, Batch 327, LR 2.385946 Loss 19.064684, Accuracy 0.074%\n",
      "Epoch 10, Batch 328, LR 2.386073 Loss 19.064527, Accuracy 0.074%\n",
      "Epoch 10, Batch 329, LR 2.386199 Loss 19.064933, Accuracy 0.074%\n",
      "Epoch 10, Batch 330, LR 2.386326 Loss 19.064539, Accuracy 0.073%\n",
      "Epoch 10, Batch 331, LR 2.386453 Loss 19.063842, Accuracy 0.073%\n",
      "Epoch 10, Batch 332, LR 2.386580 Loss 19.063705, Accuracy 0.073%\n",
      "Epoch 10, Batch 333, LR 2.386706 Loss 19.063429, Accuracy 0.073%\n",
      "Epoch 10, Batch 334, LR 2.386833 Loss 19.063662, Accuracy 0.073%\n",
      "Epoch 10, Batch 335, LR 2.386959 Loss 19.063772, Accuracy 0.072%\n",
      "Epoch 10, Batch 336, LR 2.387086 Loss 19.063026, Accuracy 0.074%\n",
      "Epoch 10, Batch 337, LR 2.387212 Loss 19.063309, Accuracy 0.074%\n",
      "Epoch 10, Batch 338, LR 2.387338 Loss 19.062680, Accuracy 0.074%\n",
      "Epoch 10, Batch 339, LR 2.387465 Loss 19.062384, Accuracy 0.074%\n",
      "Epoch 10, Batch 340, LR 2.387591 Loss 19.062195, Accuracy 0.074%\n",
      "Epoch 10, Batch 341, LR 2.387717 Loss 19.062252, Accuracy 0.073%\n",
      "Epoch 10, Batch 342, LR 2.387843 Loss 19.061862, Accuracy 0.073%\n",
      "Epoch 10, Batch 343, LR 2.387969 Loss 19.061196, Accuracy 0.073%\n",
      "Epoch 10, Batch 344, LR 2.388095 Loss 19.061051, Accuracy 0.073%\n",
      "Epoch 10, Batch 345, LR 2.388220 Loss 19.060434, Accuracy 0.072%\n",
      "Epoch 10, Batch 346, LR 2.388346 Loss 19.060215, Accuracy 0.072%\n",
      "Epoch 10, Batch 347, LR 2.388472 Loss 19.060302, Accuracy 0.072%\n",
      "Epoch 10, Batch 348, LR 2.388598 Loss 19.060142, Accuracy 0.072%\n",
      "Epoch 10, Batch 349, LR 2.388723 Loss 19.060201, Accuracy 0.072%\n",
      "Epoch 10, Batch 350, LR 2.388849 Loss 19.060560, Accuracy 0.071%\n",
      "Epoch 10, Batch 351, LR 2.388974 Loss 19.060815, Accuracy 0.073%\n",
      "Epoch 10, Batch 352, LR 2.389099 Loss 19.060951, Accuracy 0.073%\n",
      "Epoch 10, Batch 353, LR 2.389225 Loss 19.061075, Accuracy 0.073%\n",
      "Epoch 10, Batch 354, LR 2.389350 Loss 19.061196, Accuracy 0.073%\n",
      "Epoch 10, Batch 355, LR 2.389475 Loss 19.061284, Accuracy 0.073%\n",
      "Epoch 10, Batch 356, LR 2.389600 Loss 19.061859, Accuracy 0.072%\n",
      "Epoch 10, Batch 357, LR 2.389725 Loss 19.062089, Accuracy 0.072%\n",
      "Epoch 10, Batch 358, LR 2.389850 Loss 19.061754, Accuracy 0.072%\n",
      "Epoch 10, Batch 359, LR 2.389975 Loss 19.061745, Accuracy 0.072%\n",
      "Epoch 10, Batch 360, LR 2.390100 Loss 19.062139, Accuracy 0.072%\n",
      "Epoch 10, Batch 361, LR 2.390224 Loss 19.061991, Accuracy 0.071%\n",
      "Epoch 10, Batch 362, LR 2.390349 Loss 19.061642, Accuracy 0.071%\n",
      "Epoch 10, Batch 363, LR 2.390474 Loss 19.061332, Accuracy 0.071%\n",
      "Epoch 10, Batch 364, LR 2.390598 Loss 19.061086, Accuracy 0.071%\n",
      "Epoch 10, Batch 365, LR 2.390723 Loss 19.061623, Accuracy 0.071%\n",
      "Epoch 10, Batch 366, LR 2.390847 Loss 19.061306, Accuracy 0.070%\n",
      "Epoch 10, Batch 367, LR 2.390971 Loss 19.061927, Accuracy 0.070%\n",
      "Epoch 10, Batch 368, LR 2.391096 Loss 19.061797, Accuracy 0.070%\n",
      "Epoch 10, Batch 369, LR 2.391220 Loss 19.061784, Accuracy 0.072%\n",
      "Epoch 10, Batch 370, LR 2.391344 Loss 19.061609, Accuracy 0.072%\n",
      "Epoch 10, Batch 371, LR 2.391468 Loss 19.061519, Accuracy 0.072%\n",
      "Epoch 10, Batch 372, LR 2.391592 Loss 19.062054, Accuracy 0.071%\n",
      "Epoch 10, Batch 373, LR 2.391716 Loss 19.062148, Accuracy 0.071%\n",
      "Epoch 10, Batch 374, LR 2.391840 Loss 19.062459, Accuracy 0.071%\n",
      "Epoch 10, Batch 375, LR 2.391963 Loss 19.062322, Accuracy 0.071%\n",
      "Epoch 10, Batch 376, LR 2.392087 Loss 19.061937, Accuracy 0.071%\n",
      "Epoch 10, Batch 377, LR 2.392211 Loss 19.062334, Accuracy 0.070%\n",
      "Epoch 10, Batch 378, LR 2.392334 Loss 19.062905, Accuracy 0.070%\n",
      "Epoch 10, Batch 379, LR 2.392458 Loss 19.062597, Accuracy 0.072%\n",
      "Epoch 10, Batch 380, LR 2.392581 Loss 19.062605, Accuracy 0.072%\n",
      "Epoch 10, Batch 381, LR 2.392705 Loss 19.062483, Accuracy 0.072%\n",
      "Epoch 10, Batch 382, LR 2.392828 Loss 19.062671, Accuracy 0.072%\n",
      "Epoch 10, Batch 383, LR 2.392951 Loss 19.062732, Accuracy 0.071%\n",
      "Epoch 10, Batch 384, LR 2.393075 Loss 19.062565, Accuracy 0.071%\n",
      "Epoch 10, Batch 385, LR 2.393198 Loss 19.062146, Accuracy 0.071%\n",
      "Epoch 10, Batch 386, LR 2.393321 Loss 19.062040, Accuracy 0.071%\n",
      "Epoch 10, Batch 387, LR 2.393444 Loss 19.061595, Accuracy 0.071%\n",
      "Epoch 10, Batch 388, LR 2.393567 Loss 19.061355, Accuracy 0.070%\n",
      "Epoch 10, Batch 389, LR 2.393689 Loss 19.061147, Accuracy 0.070%\n",
      "Epoch 10, Batch 390, LR 2.393812 Loss 19.061319, Accuracy 0.072%\n",
      "Epoch 10, Batch 391, LR 2.393935 Loss 19.060508, Accuracy 0.074%\n",
      "Epoch 10, Batch 392, LR 2.394058 Loss 19.060541, Accuracy 0.074%\n",
      "Epoch 10, Batch 393, LR 2.394180 Loss 19.060773, Accuracy 0.076%\n",
      "Epoch 10, Batch 394, LR 2.394303 Loss 19.060428, Accuracy 0.075%\n",
      "Epoch 10, Batch 395, LR 2.394425 Loss 19.060612, Accuracy 0.075%\n",
      "Epoch 10, Batch 396, LR 2.394547 Loss 19.060550, Accuracy 0.075%\n",
      "Epoch 10, Batch 397, LR 2.394670 Loss 19.060809, Accuracy 0.075%\n",
      "Epoch 10, Batch 398, LR 2.394792 Loss 19.060643, Accuracy 0.075%\n",
      "Epoch 10, Batch 399, LR 2.394914 Loss 19.060255, Accuracy 0.074%\n",
      "Epoch 10, Batch 400, LR 2.395036 Loss 19.060454, Accuracy 0.074%\n",
      "Epoch 10, Batch 401, LR 2.395158 Loss 19.060610, Accuracy 0.074%\n",
      "Epoch 10, Batch 402, LR 2.395280 Loss 19.060621, Accuracy 0.074%\n",
      "Epoch 10, Batch 403, LR 2.395402 Loss 19.060598, Accuracy 0.074%\n",
      "Epoch 10, Batch 404, LR 2.395524 Loss 19.060483, Accuracy 0.073%\n",
      "Epoch 10, Batch 405, LR 2.395646 Loss 19.060648, Accuracy 0.073%\n",
      "Epoch 10, Batch 406, LR 2.395767 Loss 19.060638, Accuracy 0.073%\n",
      "Epoch 10, Batch 407, LR 2.395889 Loss 19.059983, Accuracy 0.073%\n",
      "Epoch 10, Batch 408, LR 2.396011 Loss 19.060004, Accuracy 0.073%\n",
      "Epoch 10, Batch 409, LR 2.396132 Loss 19.060277, Accuracy 0.073%\n",
      "Epoch 10, Batch 410, LR 2.396254 Loss 19.059751, Accuracy 0.074%\n",
      "Epoch 10, Batch 411, LR 2.396375 Loss 19.060006, Accuracy 0.074%\n",
      "Epoch 10, Batch 412, LR 2.396496 Loss 19.060240, Accuracy 0.074%\n",
      "Epoch 10, Batch 413, LR 2.396617 Loss 19.060516, Accuracy 0.074%\n",
      "Epoch 10, Batch 414, LR 2.396739 Loss 19.060604, Accuracy 0.074%\n",
      "Epoch 10, Batch 415, LR 2.396860 Loss 19.060599, Accuracy 0.073%\n",
      "Epoch 10, Batch 416, LR 2.396981 Loss 19.060265, Accuracy 0.073%\n",
      "Epoch 10, Batch 417, LR 2.397102 Loss 19.060492, Accuracy 0.073%\n",
      "Epoch 10, Batch 418, LR 2.397222 Loss 19.060563, Accuracy 0.073%\n",
      "Epoch 10, Batch 419, LR 2.397343 Loss 19.060611, Accuracy 0.073%\n",
      "Epoch 10, Batch 420, LR 2.397464 Loss 19.060587, Accuracy 0.073%\n",
      "Epoch 10, Batch 421, LR 2.397585 Loss 19.060578, Accuracy 0.072%\n",
      "Epoch 10, Batch 422, LR 2.397705 Loss 19.060559, Accuracy 0.072%\n",
      "Epoch 10, Batch 423, LR 2.397826 Loss 19.060517, Accuracy 0.072%\n",
      "Epoch 10, Batch 424, LR 2.397946 Loss 19.060855, Accuracy 0.072%\n",
      "Epoch 10, Batch 425, LR 2.398067 Loss 19.060427, Accuracy 0.072%\n",
      "Epoch 10, Batch 426, LR 2.398187 Loss 19.060778, Accuracy 0.072%\n",
      "Epoch 10, Batch 427, LR 2.398307 Loss 19.060592, Accuracy 0.071%\n",
      "Epoch 10, Batch 428, LR 2.398428 Loss 19.060609, Accuracy 0.071%\n",
      "Epoch 10, Batch 429, LR 2.398548 Loss 19.060408, Accuracy 0.071%\n",
      "Epoch 10, Batch 430, LR 2.398668 Loss 19.060670, Accuracy 0.071%\n",
      "Epoch 10, Batch 431, LR 2.398788 Loss 19.060889, Accuracy 0.073%\n",
      "Epoch 10, Batch 432, LR 2.398908 Loss 19.061056, Accuracy 0.072%\n",
      "Epoch 10, Batch 433, LR 2.399028 Loss 19.061526, Accuracy 0.072%\n",
      "Epoch 10, Batch 434, LR 2.399147 Loss 19.061759, Accuracy 0.072%\n",
      "Epoch 10, Batch 435, LR 2.399267 Loss 19.061604, Accuracy 0.072%\n",
      "Epoch 10, Batch 436, LR 2.399387 Loss 19.061798, Accuracy 0.072%\n",
      "Epoch 10, Batch 437, LR 2.399506 Loss 19.061318, Accuracy 0.072%\n",
      "Epoch 10, Batch 438, LR 2.399626 Loss 19.061332, Accuracy 0.071%\n",
      "Epoch 10, Batch 439, LR 2.399745 Loss 19.061218, Accuracy 0.071%\n",
      "Epoch 10, Batch 440, LR 2.399865 Loss 19.061108, Accuracy 0.073%\n",
      "Epoch 10, Batch 441, LR 2.399984 Loss 19.061203, Accuracy 0.073%\n",
      "Epoch 10, Batch 442, LR 2.400103 Loss 19.061145, Accuracy 0.074%\n",
      "Epoch 10, Batch 443, LR 2.400222 Loss 19.061269, Accuracy 0.076%\n",
      "Epoch 10, Batch 444, LR 2.400342 Loss 19.061446, Accuracy 0.076%\n",
      "Epoch 10, Batch 445, LR 2.400461 Loss 19.061449, Accuracy 0.075%\n",
      "Epoch 10, Batch 446, LR 2.400580 Loss 19.061202, Accuracy 0.075%\n",
      "Epoch 10, Batch 447, LR 2.400698 Loss 19.061322, Accuracy 0.075%\n",
      "Epoch 10, Batch 448, LR 2.400817 Loss 19.061583, Accuracy 0.075%\n",
      "Epoch 10, Batch 449, LR 2.400936 Loss 19.061285, Accuracy 0.075%\n",
      "Epoch 10, Batch 450, LR 2.401055 Loss 19.061073, Accuracy 0.075%\n",
      "Epoch 10, Batch 451, LR 2.401173 Loss 19.061014, Accuracy 0.074%\n",
      "Epoch 10, Batch 452, LR 2.401292 Loss 19.060934, Accuracy 0.074%\n",
      "Epoch 10, Batch 453, LR 2.401410 Loss 19.060739, Accuracy 0.074%\n",
      "Epoch 10, Batch 454, LR 2.401529 Loss 19.060576, Accuracy 0.076%\n",
      "Epoch 10, Batch 455, LR 2.401647 Loss 19.060673, Accuracy 0.076%\n",
      "Epoch 10, Batch 456, LR 2.401766 Loss 19.060836, Accuracy 0.075%\n",
      "Epoch 10, Batch 457, LR 2.401884 Loss 19.061358, Accuracy 0.075%\n",
      "Epoch 10, Batch 458, LR 2.402002 Loss 19.061371, Accuracy 0.075%\n",
      "Epoch 10, Batch 459, LR 2.402120 Loss 19.060868, Accuracy 0.080%\n",
      "Epoch 10, Batch 460, LR 2.402238 Loss 19.060898, Accuracy 0.080%\n",
      "Epoch 10, Batch 461, LR 2.402356 Loss 19.061239, Accuracy 0.080%\n",
      "Epoch 10, Batch 462, LR 2.402474 Loss 19.061732, Accuracy 0.079%\n",
      "Epoch 10, Batch 463, LR 2.402592 Loss 19.061648, Accuracy 0.079%\n",
      "Epoch 10, Batch 464, LR 2.402709 Loss 19.061840, Accuracy 0.079%\n",
      "Epoch 10, Batch 465, LR 2.402827 Loss 19.061723, Accuracy 0.079%\n",
      "Epoch 10, Batch 466, LR 2.402945 Loss 19.061722, Accuracy 0.079%\n",
      "Epoch 10, Batch 467, LR 2.403062 Loss 19.061393, Accuracy 0.079%\n",
      "Epoch 10, Batch 468, LR 2.403180 Loss 19.061336, Accuracy 0.078%\n",
      "Epoch 10, Batch 469, LR 2.403297 Loss 19.061442, Accuracy 0.078%\n",
      "Epoch 10, Batch 470, LR 2.403415 Loss 19.061500, Accuracy 0.078%\n",
      "Epoch 10, Batch 471, LR 2.403532 Loss 19.061623, Accuracy 0.078%\n",
      "Epoch 10, Batch 472, LR 2.403649 Loss 19.061872, Accuracy 0.079%\n",
      "Epoch 10, Batch 473, LR 2.403766 Loss 19.061790, Accuracy 0.079%\n",
      "Epoch 10, Batch 474, LR 2.403883 Loss 19.061833, Accuracy 0.079%\n",
      "Epoch 10, Batch 475, LR 2.404000 Loss 19.061798, Accuracy 0.079%\n",
      "Epoch 10, Batch 476, LR 2.404117 Loss 19.061702, Accuracy 0.079%\n",
      "Epoch 10, Batch 477, LR 2.404234 Loss 19.061351, Accuracy 0.079%\n",
      "Epoch 10, Batch 478, LR 2.404351 Loss 19.061395, Accuracy 0.078%\n",
      "Epoch 10, Batch 479, LR 2.404468 Loss 19.061416, Accuracy 0.078%\n",
      "Epoch 10, Batch 480, LR 2.404584 Loss 19.061486, Accuracy 0.078%\n",
      "Epoch 10, Batch 481, LR 2.404701 Loss 19.061335, Accuracy 0.078%\n",
      "Epoch 10, Batch 482, LR 2.404817 Loss 19.061841, Accuracy 0.078%\n",
      "Epoch 10, Batch 483, LR 2.404934 Loss 19.062106, Accuracy 0.078%\n",
      "Epoch 10, Batch 484, LR 2.405050 Loss 19.062291, Accuracy 0.077%\n",
      "Epoch 10, Batch 485, LR 2.405167 Loss 19.062150, Accuracy 0.077%\n",
      "Epoch 10, Batch 486, LR 2.405283 Loss 19.062245, Accuracy 0.077%\n",
      "Epoch 10, Batch 487, LR 2.405399 Loss 19.062343, Accuracy 0.077%\n",
      "Epoch 10, Batch 488, LR 2.405515 Loss 19.062116, Accuracy 0.077%\n",
      "Epoch 10, Batch 489, LR 2.405631 Loss 19.061802, Accuracy 0.077%\n",
      "Epoch 10, Batch 490, LR 2.405747 Loss 19.061712, Accuracy 0.077%\n",
      "Epoch 10, Batch 491, LR 2.405863 Loss 19.061992, Accuracy 0.076%\n",
      "Epoch 10, Batch 492, LR 2.405979 Loss 19.062187, Accuracy 0.076%\n",
      "Epoch 10, Batch 493, LR 2.406095 Loss 19.062074, Accuracy 0.076%\n",
      "Epoch 10, Batch 494, LR 2.406210 Loss 19.062315, Accuracy 0.076%\n",
      "Epoch 10, Batch 495, LR 2.406326 Loss 19.062416, Accuracy 0.076%\n",
      "Epoch 10, Batch 496, LR 2.406441 Loss 19.062757, Accuracy 0.076%\n",
      "Epoch 10, Batch 497, LR 2.406557 Loss 19.063020, Accuracy 0.075%\n",
      "Epoch 10, Batch 498, LR 2.406672 Loss 19.062870, Accuracy 0.075%\n",
      "Epoch 10, Batch 499, LR 2.406788 Loss 19.062701, Accuracy 0.075%\n",
      "Epoch 10, Batch 500, LR 2.406903 Loss 19.062640, Accuracy 0.075%\n",
      "Epoch 10, Batch 501, LR 2.407018 Loss 19.062627, Accuracy 0.075%\n",
      "Epoch 10, Batch 502, LR 2.407133 Loss 19.062533, Accuracy 0.075%\n",
      "Epoch 10, Batch 503, LR 2.407249 Loss 19.062553, Accuracy 0.075%\n",
      "Epoch 10, Batch 504, LR 2.407364 Loss 19.062466, Accuracy 0.074%\n",
      "Epoch 10, Batch 505, LR 2.407479 Loss 19.062194, Accuracy 0.074%\n",
      "Epoch 10, Batch 506, LR 2.407593 Loss 19.062524, Accuracy 0.074%\n",
      "Epoch 10, Batch 507, LR 2.407708 Loss 19.062601, Accuracy 0.074%\n",
      "Epoch 10, Batch 508, LR 2.407823 Loss 19.062915, Accuracy 0.074%\n",
      "Epoch 10, Batch 509, LR 2.407938 Loss 19.062716, Accuracy 0.074%\n",
      "Epoch 10, Batch 510, LR 2.408052 Loss 19.062748, Accuracy 0.074%\n",
      "Epoch 10, Batch 511, LR 2.408167 Loss 19.062896, Accuracy 0.073%\n",
      "Epoch 10, Batch 512, LR 2.408281 Loss 19.063091, Accuracy 0.073%\n",
      "Epoch 10, Batch 513, LR 2.408396 Loss 19.063181, Accuracy 0.073%\n",
      "Epoch 10, Batch 514, LR 2.408510 Loss 19.062957, Accuracy 0.073%\n",
      "Epoch 10, Batch 515, LR 2.408624 Loss 19.063050, Accuracy 0.073%\n",
      "Epoch 10, Batch 516, LR 2.408738 Loss 19.062851, Accuracy 0.073%\n",
      "Epoch 10, Batch 517, LR 2.408853 Loss 19.062461, Accuracy 0.073%\n",
      "Epoch 10, Batch 518, LR 2.408967 Loss 19.062270, Accuracy 0.072%\n",
      "Epoch 10, Batch 519, LR 2.409081 Loss 19.062342, Accuracy 0.072%\n",
      "Epoch 10, Batch 520, LR 2.409195 Loss 19.062345, Accuracy 0.072%\n",
      "Epoch 10, Batch 521, LR 2.409308 Loss 19.062001, Accuracy 0.072%\n",
      "Epoch 10, Batch 522, LR 2.409422 Loss 19.061898, Accuracy 0.072%\n",
      "Epoch 10, Batch 523, LR 2.409536 Loss 19.062144, Accuracy 0.072%\n",
      "Epoch 10, Batch 524, LR 2.409650 Loss 19.062356, Accuracy 0.072%\n",
      "Epoch 10, Batch 525, LR 2.409763 Loss 19.062254, Accuracy 0.073%\n",
      "Epoch 10, Batch 526, LR 2.409877 Loss 19.062341, Accuracy 0.073%\n",
      "Epoch 10, Batch 527, LR 2.409990 Loss 19.062053, Accuracy 0.076%\n",
      "Epoch 10, Batch 528, LR 2.410103 Loss 19.061747, Accuracy 0.075%\n",
      "Epoch 10, Batch 529, LR 2.410217 Loss 19.061863, Accuracy 0.075%\n",
      "Epoch 10, Batch 530, LR 2.410330 Loss 19.061640, Accuracy 0.077%\n",
      "Epoch 10, Batch 531, LR 2.410443 Loss 19.061714, Accuracy 0.077%\n",
      "Epoch 10, Batch 532, LR 2.410556 Loss 19.061620, Accuracy 0.076%\n",
      "Epoch 10, Batch 533, LR 2.410669 Loss 19.061375, Accuracy 0.076%\n",
      "Epoch 10, Batch 534, LR 2.410782 Loss 19.061393, Accuracy 0.076%\n",
      "Epoch 10, Batch 535, LR 2.410895 Loss 19.062179, Accuracy 0.076%\n",
      "Epoch 10, Batch 536, LR 2.411008 Loss 19.061999, Accuracy 0.076%\n",
      "Epoch 10, Batch 537, LR 2.411121 Loss 19.062198, Accuracy 0.076%\n",
      "Epoch 10, Batch 538, LR 2.411233 Loss 19.062321, Accuracy 0.076%\n",
      "Epoch 10, Batch 539, LR 2.411346 Loss 19.062517, Accuracy 0.075%\n",
      "Epoch 10, Batch 540, LR 2.411459 Loss 19.062503, Accuracy 0.075%\n",
      "Epoch 10, Batch 541, LR 2.411571 Loss 19.062541, Accuracy 0.075%\n",
      "Epoch 10, Batch 542, LR 2.411683 Loss 19.062676, Accuracy 0.075%\n",
      "Epoch 10, Batch 543, LR 2.411796 Loss 19.062723, Accuracy 0.075%\n",
      "Epoch 10, Batch 544, LR 2.411908 Loss 19.062510, Accuracy 0.075%\n",
      "Epoch 10, Batch 545, LR 2.412020 Loss 19.062438, Accuracy 0.076%\n",
      "Epoch 10, Batch 546, LR 2.412132 Loss 19.062131, Accuracy 0.076%\n",
      "Epoch 10, Batch 547, LR 2.412245 Loss 19.062348, Accuracy 0.076%\n",
      "Epoch 10, Batch 548, LR 2.412357 Loss 19.062070, Accuracy 0.076%\n",
      "Epoch 10, Batch 549, LR 2.412468 Loss 19.061560, Accuracy 0.075%\n",
      "Epoch 10, Batch 550, LR 2.412580 Loss 19.061712, Accuracy 0.075%\n",
      "Epoch 10, Batch 551, LR 2.412692 Loss 19.061666, Accuracy 0.075%\n",
      "Epoch 10, Batch 552, LR 2.412804 Loss 19.061479, Accuracy 0.075%\n",
      "Epoch 10, Batch 553, LR 2.412915 Loss 19.061714, Accuracy 0.075%\n",
      "Epoch 10, Batch 554, LR 2.413027 Loss 19.061984, Accuracy 0.075%\n",
      "Epoch 10, Batch 555, LR 2.413139 Loss 19.061889, Accuracy 0.075%\n",
      "Epoch 10, Batch 556, LR 2.413250 Loss 19.061944, Accuracy 0.076%\n",
      "Epoch 10, Batch 557, LR 2.413361 Loss 19.062147, Accuracy 0.076%\n",
      "Epoch 10, Batch 558, LR 2.413473 Loss 19.062109, Accuracy 0.077%\n",
      "Epoch 10, Batch 559, LR 2.413584 Loss 19.061976, Accuracy 0.077%\n",
      "Epoch 10, Batch 560, LR 2.413695 Loss 19.062142, Accuracy 0.077%\n",
      "Epoch 10, Batch 561, LR 2.413806 Loss 19.061923, Accuracy 0.077%\n",
      "Epoch 10, Batch 562, LR 2.413917 Loss 19.061613, Accuracy 0.076%\n",
      "Epoch 10, Batch 563, LR 2.414028 Loss 19.062100, Accuracy 0.076%\n",
      "Epoch 10, Batch 564, LR 2.414139 Loss 19.062226, Accuracy 0.076%\n",
      "Epoch 10, Batch 565, LR 2.414250 Loss 19.062462, Accuracy 0.076%\n",
      "Epoch 10, Batch 566, LR 2.414361 Loss 19.062767, Accuracy 0.076%\n",
      "Epoch 10, Batch 567, LR 2.414471 Loss 19.062682, Accuracy 0.076%\n",
      "Epoch 10, Batch 568, LR 2.414582 Loss 19.063049, Accuracy 0.076%\n",
      "Epoch 10, Batch 569, LR 2.414693 Loss 19.063137, Accuracy 0.076%\n",
      "Epoch 10, Batch 570, LR 2.414803 Loss 19.062703, Accuracy 0.075%\n",
      "Epoch 10, Batch 571, LR 2.414914 Loss 19.062525, Accuracy 0.075%\n",
      "Epoch 10, Batch 572, LR 2.415024 Loss 19.062387, Accuracy 0.076%\n",
      "Epoch 10, Batch 573, LR 2.415134 Loss 19.062321, Accuracy 0.076%\n",
      "Epoch 10, Batch 574, LR 2.415244 Loss 19.062054, Accuracy 0.076%\n",
      "Epoch 10, Batch 575, LR 2.415354 Loss 19.061989, Accuracy 0.076%\n",
      "Epoch 10, Batch 576, LR 2.415465 Loss 19.061924, Accuracy 0.076%\n",
      "Epoch 10, Batch 577, LR 2.415575 Loss 19.061860, Accuracy 0.076%\n",
      "Epoch 10, Batch 578, LR 2.415684 Loss 19.061947, Accuracy 0.076%\n",
      "Epoch 10, Batch 579, LR 2.415794 Loss 19.061849, Accuracy 0.076%\n",
      "Epoch 10, Batch 580, LR 2.415904 Loss 19.061636, Accuracy 0.075%\n",
      "Epoch 10, Batch 581, LR 2.416014 Loss 19.061473, Accuracy 0.075%\n",
      "Epoch 10, Batch 582, LR 2.416124 Loss 19.061647, Accuracy 0.075%\n",
      "Epoch 10, Batch 583, LR 2.416233 Loss 19.061928, Accuracy 0.075%\n",
      "Epoch 10, Batch 584, LR 2.416343 Loss 19.061928, Accuracy 0.075%\n",
      "Epoch 10, Batch 585, LR 2.416452 Loss 19.061866, Accuracy 0.075%\n",
      "Epoch 10, Batch 586, LR 2.416561 Loss 19.061898, Accuracy 0.075%\n",
      "Epoch 10, Batch 587, LR 2.416671 Loss 19.061695, Accuracy 0.075%\n",
      "Epoch 10, Batch 588, LR 2.416780 Loss 19.061736, Accuracy 0.074%\n",
      "Epoch 10, Batch 589, LR 2.416889 Loss 19.061876, Accuracy 0.074%\n",
      "Epoch 10, Batch 590, LR 2.416998 Loss 19.061798, Accuracy 0.074%\n",
      "Epoch 10, Batch 591, LR 2.417107 Loss 19.061633, Accuracy 0.074%\n",
      "Epoch 10, Batch 592, LR 2.417216 Loss 19.061481, Accuracy 0.074%\n",
      "Epoch 10, Batch 593, LR 2.417325 Loss 19.061470, Accuracy 0.074%\n",
      "Epoch 10, Batch 594, LR 2.417434 Loss 19.061294, Accuracy 0.074%\n",
      "Epoch 10, Batch 595, LR 2.417543 Loss 19.061251, Accuracy 0.074%\n",
      "Epoch 10, Batch 596, LR 2.417651 Loss 19.060900, Accuracy 0.073%\n",
      "Epoch 10, Batch 597, LR 2.417760 Loss 19.061224, Accuracy 0.073%\n",
      "Epoch 10, Batch 598, LR 2.417869 Loss 19.061586, Accuracy 0.073%\n",
      "Epoch 10, Batch 599, LR 2.417977 Loss 19.061670, Accuracy 0.073%\n",
      "Epoch 10, Batch 600, LR 2.418086 Loss 19.061435, Accuracy 0.073%\n",
      "Epoch 10, Batch 601, LR 2.418194 Loss 19.061483, Accuracy 0.073%\n",
      "Epoch 10, Batch 602, LR 2.418302 Loss 19.061689, Accuracy 0.073%\n",
      "Epoch 10, Batch 603, LR 2.418410 Loss 19.061407, Accuracy 0.073%\n",
      "Epoch 10, Batch 604, LR 2.418519 Loss 19.061447, Accuracy 0.072%\n",
      "Epoch 10, Batch 605, LR 2.418627 Loss 19.061602, Accuracy 0.072%\n",
      "Epoch 10, Batch 606, LR 2.418735 Loss 19.061955, Accuracy 0.072%\n",
      "Epoch 10, Batch 607, LR 2.418843 Loss 19.062140, Accuracy 0.072%\n",
      "Epoch 10, Batch 608, LR 2.418950 Loss 19.061948, Accuracy 0.072%\n",
      "Epoch 10, Batch 609, LR 2.419058 Loss 19.061659, Accuracy 0.072%\n",
      "Epoch 10, Batch 610, LR 2.419166 Loss 19.061589, Accuracy 0.072%\n",
      "Epoch 10, Batch 611, LR 2.419274 Loss 19.061691, Accuracy 0.072%\n",
      "Epoch 10, Batch 612, LR 2.419381 Loss 19.061788, Accuracy 0.071%\n",
      "Epoch 10, Batch 613, LR 2.419489 Loss 19.061836, Accuracy 0.071%\n",
      "Epoch 10, Batch 614, LR 2.419596 Loss 19.061937, Accuracy 0.071%\n",
      "Epoch 10, Batch 615, LR 2.419703 Loss 19.062152, Accuracy 0.071%\n",
      "Epoch 10, Batch 616, LR 2.419811 Loss 19.062433, Accuracy 0.071%\n",
      "Epoch 10, Batch 617, LR 2.419918 Loss 19.062459, Accuracy 0.072%\n",
      "Epoch 10, Batch 618, LR 2.420025 Loss 19.062598, Accuracy 0.072%\n",
      "Epoch 10, Batch 619, LR 2.420132 Loss 19.062807, Accuracy 0.073%\n",
      "Epoch 10, Batch 620, LR 2.420239 Loss 19.062654, Accuracy 0.074%\n",
      "Epoch 10, Batch 621, LR 2.420346 Loss 19.062652, Accuracy 0.074%\n",
      "Epoch 10, Batch 622, LR 2.420453 Loss 19.062851, Accuracy 0.075%\n",
      "Epoch 10, Batch 623, LR 2.420560 Loss 19.062872, Accuracy 0.075%\n",
      "Epoch 10, Batch 624, LR 2.420667 Loss 19.062683, Accuracy 0.075%\n",
      "Epoch 10, Batch 625, LR 2.420773 Loss 19.062707, Accuracy 0.075%\n",
      "Epoch 10, Batch 626, LR 2.420880 Loss 19.062578, Accuracy 0.075%\n",
      "Epoch 10, Batch 627, LR 2.420987 Loss 19.062344, Accuracy 0.075%\n",
      "Epoch 10, Batch 628, LR 2.421093 Loss 19.062137, Accuracy 0.075%\n",
      "Epoch 10, Batch 629, LR 2.421199 Loss 19.062159, Accuracy 0.075%\n",
      "Epoch 10, Batch 630, LR 2.421306 Loss 19.062184, Accuracy 0.074%\n",
      "Epoch 10, Batch 631, LR 2.421412 Loss 19.062345, Accuracy 0.074%\n",
      "Epoch 10, Batch 632, LR 2.421518 Loss 19.062251, Accuracy 0.074%\n",
      "Epoch 10, Batch 633, LR 2.421624 Loss 19.062148, Accuracy 0.074%\n",
      "Epoch 10, Batch 634, LR 2.421730 Loss 19.062258, Accuracy 0.074%\n",
      "Epoch 10, Batch 635, LR 2.421836 Loss 19.062079, Accuracy 0.074%\n",
      "Epoch 10, Batch 636, LR 2.421942 Loss 19.062181, Accuracy 0.074%\n",
      "Epoch 10, Batch 637, LR 2.422048 Loss 19.062598, Accuracy 0.074%\n",
      "Epoch 10, Batch 638, LR 2.422154 Loss 19.062611, Accuracy 0.075%\n",
      "Epoch 10, Batch 639, LR 2.422260 Loss 19.062304, Accuracy 0.075%\n",
      "Epoch 10, Batch 640, LR 2.422365 Loss 19.062277, Accuracy 0.074%\n",
      "Epoch 10, Batch 641, LR 2.422471 Loss 19.062338, Accuracy 0.074%\n",
      "Epoch 10, Batch 642, LR 2.422576 Loss 19.062514, Accuracy 0.074%\n",
      "Epoch 10, Batch 643, LR 2.422682 Loss 19.062318, Accuracy 0.074%\n",
      "Epoch 10, Batch 644, LR 2.422787 Loss 19.062303, Accuracy 0.074%\n",
      "Epoch 10, Batch 645, LR 2.422893 Loss 19.062428, Accuracy 0.074%\n",
      "Epoch 10, Batch 646, LR 2.422998 Loss 19.062519, Accuracy 0.074%\n",
      "Epoch 10, Batch 647, LR 2.423103 Loss 19.062487, Accuracy 0.074%\n",
      "Epoch 10, Batch 648, LR 2.423208 Loss 19.062519, Accuracy 0.074%\n",
      "Epoch 10, Batch 649, LR 2.423313 Loss 19.062553, Accuracy 0.073%\n",
      "Epoch 10, Batch 650, LR 2.423418 Loss 19.062667, Accuracy 0.073%\n",
      "Epoch 10, Batch 651, LR 2.423523 Loss 19.062685, Accuracy 0.073%\n",
      "Epoch 10, Batch 652, LR 2.423628 Loss 19.063153, Accuracy 0.073%\n",
      "Epoch 10, Batch 653, LR 2.423732 Loss 19.062860, Accuracy 0.073%\n",
      "Epoch 10, Batch 654, LR 2.423837 Loss 19.062825, Accuracy 0.073%\n",
      "Epoch 10, Batch 655, LR 2.423942 Loss 19.062821, Accuracy 0.073%\n",
      "Epoch 10, Batch 656, LR 2.424046 Loss 19.062941, Accuracy 0.073%\n",
      "Epoch 10, Batch 657, LR 2.424151 Loss 19.062860, Accuracy 0.073%\n",
      "Epoch 10, Batch 658, LR 2.424255 Loss 19.062619, Accuracy 0.072%\n",
      "Epoch 10, Batch 659, LR 2.424359 Loss 19.062671, Accuracy 0.072%\n",
      "Epoch 10, Batch 660, LR 2.424464 Loss 19.062814, Accuracy 0.072%\n",
      "Epoch 10, Batch 661, LR 2.424568 Loss 19.063231, Accuracy 0.072%\n",
      "Epoch 10, Batch 662, LR 2.424672 Loss 19.063221, Accuracy 0.072%\n",
      "Epoch 10, Batch 663, LR 2.424776 Loss 19.063243, Accuracy 0.073%\n",
      "Epoch 10, Batch 664, LR 2.424880 Loss 19.062966, Accuracy 0.073%\n",
      "Epoch 10, Batch 665, LR 2.424984 Loss 19.062514, Accuracy 0.073%\n",
      "Epoch 10, Batch 666, LR 2.425088 Loss 19.062269, Accuracy 0.073%\n",
      "Epoch 10, Batch 667, LR 2.425191 Loss 19.062066, Accuracy 0.073%\n",
      "Epoch 10, Batch 668, LR 2.425295 Loss 19.062159, Accuracy 0.073%\n",
      "Epoch 10, Batch 669, LR 2.425399 Loss 19.061972, Accuracy 0.072%\n",
      "Epoch 10, Batch 670, LR 2.425502 Loss 19.061794, Accuracy 0.072%\n",
      "Epoch 10, Batch 671, LR 2.425606 Loss 19.061782, Accuracy 0.072%\n",
      "Epoch 10, Batch 672, LR 2.425709 Loss 19.061585, Accuracy 0.072%\n",
      "Epoch 10, Batch 673, LR 2.425813 Loss 19.061715, Accuracy 0.072%\n",
      "Epoch 10, Batch 674, LR 2.425916 Loss 19.061600, Accuracy 0.072%\n",
      "Epoch 10, Batch 675, LR 2.426019 Loss 19.061825, Accuracy 0.072%\n",
      "Epoch 10, Batch 676, LR 2.426122 Loss 19.062342, Accuracy 0.072%\n",
      "Epoch 10, Batch 677, LR 2.426225 Loss 19.062203, Accuracy 0.072%\n",
      "Epoch 10, Batch 678, LR 2.426328 Loss 19.062378, Accuracy 0.071%\n",
      "Epoch 10, Batch 679, LR 2.426431 Loss 19.062268, Accuracy 0.074%\n",
      "Epoch 10, Batch 680, LR 2.426534 Loss 19.062353, Accuracy 0.075%\n",
      "Epoch 10, Batch 681, LR 2.426637 Loss 19.062274, Accuracy 0.075%\n",
      "Epoch 10, Batch 682, LR 2.426740 Loss 19.062246, Accuracy 0.074%\n",
      "Epoch 10, Batch 683, LR 2.426842 Loss 19.062017, Accuracy 0.074%\n",
      "Epoch 10, Batch 684, LR 2.426945 Loss 19.061878, Accuracy 0.074%\n",
      "Epoch 10, Batch 685, LR 2.427047 Loss 19.061725, Accuracy 0.074%\n",
      "Epoch 10, Batch 686, LR 2.427150 Loss 19.062013, Accuracy 0.075%\n",
      "Epoch 10, Batch 687, LR 2.427252 Loss 19.061898, Accuracy 0.075%\n",
      "Epoch 10, Batch 688, LR 2.427354 Loss 19.061972, Accuracy 0.075%\n",
      "Epoch 10, Batch 689, LR 2.427457 Loss 19.061910, Accuracy 0.075%\n",
      "Epoch 10, Batch 690, LR 2.427559 Loss 19.062004, Accuracy 0.075%\n",
      "Epoch 10, Batch 691, LR 2.427661 Loss 19.062273, Accuracy 0.075%\n",
      "Epoch 10, Batch 692, LR 2.427763 Loss 19.062337, Accuracy 0.075%\n",
      "Epoch 10, Batch 693, LR 2.427865 Loss 19.062636, Accuracy 0.074%\n",
      "Epoch 10, Batch 694, LR 2.427967 Loss 19.062336, Accuracy 0.075%\n",
      "Epoch 10, Batch 695, LR 2.428069 Loss 19.062272, Accuracy 0.075%\n",
      "Epoch 10, Batch 696, LR 2.428170 Loss 19.062786, Accuracy 0.075%\n",
      "Epoch 10, Batch 697, LR 2.428272 Loss 19.063050, Accuracy 0.075%\n",
      "Epoch 10, Batch 698, LR 2.428374 Loss 19.063092, Accuracy 0.075%\n",
      "Epoch 10, Batch 699, LR 2.428475 Loss 19.063151, Accuracy 0.076%\n",
      "Epoch 10, Batch 700, LR 2.428577 Loss 19.063296, Accuracy 0.077%\n",
      "Epoch 10, Batch 701, LR 2.428678 Loss 19.063223, Accuracy 0.078%\n",
      "Epoch 10, Batch 702, LR 2.428779 Loss 19.063050, Accuracy 0.080%\n",
      "Epoch 10, Batch 703, LR 2.428881 Loss 19.063121, Accuracy 0.080%\n",
      "Epoch 10, Batch 704, LR 2.428982 Loss 19.063047, Accuracy 0.080%\n",
      "Epoch 10, Batch 705, LR 2.429083 Loss 19.062947, Accuracy 0.080%\n",
      "Epoch 10, Batch 706, LR 2.429184 Loss 19.063171, Accuracy 0.080%\n",
      "Epoch 10, Batch 707, LR 2.429285 Loss 19.063218, Accuracy 0.080%\n",
      "Epoch 10, Batch 708, LR 2.429386 Loss 19.063166, Accuracy 0.079%\n",
      "Epoch 10, Batch 709, LR 2.429487 Loss 19.063031, Accuracy 0.079%\n",
      "Epoch 10, Batch 710, LR 2.429588 Loss 19.062864, Accuracy 0.079%\n",
      "Epoch 10, Batch 711, LR 2.429688 Loss 19.062907, Accuracy 0.079%\n",
      "Epoch 10, Batch 712, LR 2.429789 Loss 19.063122, Accuracy 0.079%\n",
      "Epoch 10, Batch 713, LR 2.429889 Loss 19.063032, Accuracy 0.079%\n",
      "Epoch 10, Batch 714, LR 2.429990 Loss 19.063142, Accuracy 0.079%\n",
      "Epoch 10, Batch 715, LR 2.430090 Loss 19.062992, Accuracy 0.079%\n",
      "Epoch 10, Batch 716, LR 2.430191 Loss 19.062652, Accuracy 0.079%\n",
      "Epoch 10, Batch 717, LR 2.430291 Loss 19.062930, Accuracy 0.078%\n",
      "Epoch 10, Batch 718, LR 2.430391 Loss 19.063291, Accuracy 0.078%\n",
      "Epoch 10, Batch 719, LR 2.430491 Loss 19.063152, Accuracy 0.078%\n",
      "Epoch 10, Batch 720, LR 2.430591 Loss 19.062971, Accuracy 0.078%\n",
      "Epoch 10, Batch 721, LR 2.430691 Loss 19.062942, Accuracy 0.078%\n",
      "Epoch 10, Batch 722, LR 2.430791 Loss 19.063093, Accuracy 0.078%\n",
      "Epoch 10, Batch 723, LR 2.430891 Loss 19.063200, Accuracy 0.078%\n",
      "Epoch 10, Batch 724, LR 2.430991 Loss 19.063008, Accuracy 0.078%\n",
      "Epoch 10, Batch 725, LR 2.431091 Loss 19.062878, Accuracy 0.079%\n",
      "Epoch 10, Batch 726, LR 2.431190 Loss 19.062955, Accuracy 0.079%\n",
      "Epoch 10, Batch 727, LR 2.431290 Loss 19.062939, Accuracy 0.078%\n",
      "Epoch 10, Batch 728, LR 2.431389 Loss 19.062790, Accuracy 0.078%\n",
      "Epoch 10, Batch 729, LR 2.431489 Loss 19.062892, Accuracy 0.078%\n",
      "Epoch 10, Batch 730, LR 2.431588 Loss 19.062918, Accuracy 0.078%\n",
      "Epoch 10, Batch 731, LR 2.431688 Loss 19.062954, Accuracy 0.078%\n",
      "Epoch 10, Batch 732, LR 2.431787 Loss 19.063245, Accuracy 0.078%\n",
      "Epoch 10, Batch 733, LR 2.431886 Loss 19.063387, Accuracy 0.078%\n",
      "Epoch 10, Batch 734, LR 2.431985 Loss 19.063546, Accuracy 0.078%\n",
      "Epoch 10, Batch 735, LR 2.432084 Loss 19.063611, Accuracy 0.078%\n",
      "Epoch 10, Batch 736, LR 2.432183 Loss 19.063838, Accuracy 0.077%\n",
      "Epoch 10, Batch 737, LR 2.432282 Loss 19.064034, Accuracy 0.077%\n",
      "Epoch 10, Batch 738, LR 2.432381 Loss 19.064155, Accuracy 0.077%\n",
      "Epoch 10, Batch 739, LR 2.432479 Loss 19.064320, Accuracy 0.077%\n",
      "Epoch 10, Batch 740, LR 2.432578 Loss 19.064329, Accuracy 0.077%\n",
      "Epoch 10, Batch 741, LR 2.432677 Loss 19.064503, Accuracy 0.077%\n",
      "Epoch 10, Batch 742, LR 2.432775 Loss 19.064637, Accuracy 0.077%\n",
      "Epoch 10, Batch 743, LR 2.432874 Loss 19.064716, Accuracy 0.077%\n",
      "Epoch 10, Batch 744, LR 2.432972 Loss 19.064578, Accuracy 0.077%\n",
      "Epoch 10, Batch 745, LR 2.433070 Loss 19.064264, Accuracy 0.077%\n",
      "Epoch 10, Batch 746, LR 2.433169 Loss 19.064743, Accuracy 0.076%\n",
      "Epoch 10, Batch 747, LR 2.433267 Loss 19.064519, Accuracy 0.076%\n",
      "Epoch 10, Batch 748, LR 2.433365 Loss 19.064573, Accuracy 0.076%\n",
      "Epoch 10, Batch 749, LR 2.433463 Loss 19.064667, Accuracy 0.076%\n",
      "Epoch 10, Batch 750, LR 2.433561 Loss 19.064685, Accuracy 0.076%\n",
      "Epoch 10, Batch 751, LR 2.433659 Loss 19.064975, Accuracy 0.076%\n",
      "Epoch 10, Batch 752, LR 2.433757 Loss 19.064959, Accuracy 0.076%\n",
      "Epoch 10, Batch 753, LR 2.433854 Loss 19.065054, Accuracy 0.077%\n",
      "Epoch 10, Batch 754, LR 2.433952 Loss 19.065155, Accuracy 0.077%\n",
      "Epoch 10, Batch 755, LR 2.434050 Loss 19.065432, Accuracy 0.077%\n",
      "Epoch 10, Batch 756, LR 2.434147 Loss 19.065490, Accuracy 0.076%\n",
      "Epoch 10, Batch 757, LR 2.434245 Loss 19.065499, Accuracy 0.077%\n",
      "Epoch 10, Batch 758, LR 2.434342 Loss 19.065360, Accuracy 0.077%\n",
      "Epoch 10, Batch 759, LR 2.434440 Loss 19.065423, Accuracy 0.077%\n",
      "Epoch 10, Batch 760, LR 2.434537 Loss 19.065380, Accuracy 0.077%\n",
      "Epoch 10, Batch 761, LR 2.434634 Loss 19.065436, Accuracy 0.077%\n",
      "Epoch 10, Batch 762, LR 2.434731 Loss 19.065509, Accuracy 0.077%\n",
      "Epoch 10, Batch 763, LR 2.434828 Loss 19.065480, Accuracy 0.077%\n",
      "Epoch 10, Batch 764, LR 2.434925 Loss 19.065501, Accuracy 0.077%\n",
      "Epoch 10, Batch 765, LR 2.435022 Loss 19.065623, Accuracy 0.077%\n",
      "Epoch 10, Batch 766, LR 2.435119 Loss 19.065574, Accuracy 0.076%\n",
      "Epoch 10, Batch 767, LR 2.435216 Loss 19.065811, Accuracy 0.076%\n",
      "Epoch 10, Batch 768, LR 2.435312 Loss 19.065827, Accuracy 0.076%\n",
      "Epoch 10, Batch 769, LR 2.435409 Loss 19.065849, Accuracy 0.076%\n",
      "Epoch 10, Batch 770, LR 2.435506 Loss 19.065855, Accuracy 0.076%\n",
      "Epoch 10, Batch 771, LR 2.435602 Loss 19.065656, Accuracy 0.076%\n",
      "Epoch 10, Batch 772, LR 2.435699 Loss 19.065726, Accuracy 0.076%\n",
      "Epoch 10, Batch 773, LR 2.435795 Loss 19.065824, Accuracy 0.076%\n",
      "Epoch 10, Batch 774, LR 2.435891 Loss 19.065691, Accuracy 0.076%\n",
      "Epoch 10, Batch 775, LR 2.435987 Loss 19.065638, Accuracy 0.076%\n",
      "Epoch 10, Batch 776, LR 2.436084 Loss 19.065749, Accuracy 0.076%\n",
      "Epoch 10, Batch 777, LR 2.436180 Loss 19.065951, Accuracy 0.075%\n",
      "Epoch 10, Batch 778, LR 2.436276 Loss 19.065793, Accuracy 0.075%\n",
      "Epoch 10, Batch 779, LR 2.436372 Loss 19.065641, Accuracy 0.075%\n",
      "Epoch 10, Batch 780, LR 2.436467 Loss 19.065288, Accuracy 0.075%\n",
      "Epoch 10, Batch 781, LR 2.436563 Loss 19.065383, Accuracy 0.075%\n",
      "Epoch 10, Batch 782, LR 2.436659 Loss 19.065506, Accuracy 0.075%\n",
      "Epoch 10, Batch 783, LR 2.436755 Loss 19.065462, Accuracy 0.075%\n",
      "Epoch 10, Batch 784, LR 2.436850 Loss 19.065478, Accuracy 0.075%\n",
      "Epoch 10, Batch 785, LR 2.436946 Loss 19.065468, Accuracy 0.075%\n",
      "Epoch 10, Batch 786, LR 2.437041 Loss 19.065256, Accuracy 0.075%\n",
      "Epoch 10, Batch 787, LR 2.437137 Loss 19.065238, Accuracy 0.074%\n",
      "Epoch 10, Batch 788, LR 2.437232 Loss 19.065212, Accuracy 0.074%\n",
      "Epoch 10, Batch 789, LR 2.437327 Loss 19.065255, Accuracy 0.074%\n",
      "Epoch 10, Batch 790, LR 2.437422 Loss 19.065247, Accuracy 0.074%\n",
      "Epoch 10, Batch 791, LR 2.437517 Loss 19.065107, Accuracy 0.074%\n",
      "Epoch 10, Batch 792, LR 2.437612 Loss 19.065447, Accuracy 0.074%\n",
      "Epoch 10, Batch 793, LR 2.437707 Loss 19.065286, Accuracy 0.074%\n",
      "Epoch 10, Batch 794, LR 2.437802 Loss 19.065442, Accuracy 0.074%\n",
      "Epoch 10, Batch 795, LR 2.437897 Loss 19.065574, Accuracy 0.074%\n",
      "Epoch 10, Batch 796, LR 2.437992 Loss 19.065452, Accuracy 0.074%\n",
      "Epoch 10, Batch 797, LR 2.438086 Loss 19.065615, Accuracy 0.074%\n",
      "Epoch 10, Batch 798, LR 2.438181 Loss 19.065933, Accuracy 0.073%\n",
      "Epoch 10, Batch 799, LR 2.438276 Loss 19.066014, Accuracy 0.073%\n",
      "Epoch 10, Batch 800, LR 2.438370 Loss 19.066132, Accuracy 0.073%\n",
      "Epoch 10, Batch 801, LR 2.438464 Loss 19.065916, Accuracy 0.073%\n",
      "Epoch 10, Batch 802, LR 2.438559 Loss 19.065877, Accuracy 0.073%\n",
      "Epoch 10, Batch 803, LR 2.438653 Loss 19.065754, Accuracy 0.073%\n",
      "Epoch 10, Batch 804, LR 2.438747 Loss 19.065599, Accuracy 0.073%\n",
      "Epoch 10, Batch 805, LR 2.438841 Loss 19.065713, Accuracy 0.073%\n",
      "Epoch 10, Batch 806, LR 2.438935 Loss 19.066030, Accuracy 0.073%\n",
      "Epoch 10, Batch 807, LR 2.439029 Loss 19.066090, Accuracy 0.074%\n",
      "Epoch 10, Batch 808, LR 2.439123 Loss 19.066165, Accuracy 0.073%\n",
      "Epoch 10, Batch 809, LR 2.439217 Loss 19.066159, Accuracy 0.073%\n",
      "Epoch 10, Batch 810, LR 2.439311 Loss 19.066045, Accuracy 0.073%\n",
      "Epoch 10, Batch 811, LR 2.439404 Loss 19.066165, Accuracy 0.074%\n",
      "Epoch 10, Batch 812, LR 2.439498 Loss 19.066323, Accuracy 0.074%\n",
      "Epoch 10, Batch 813, LR 2.439592 Loss 19.066348, Accuracy 0.074%\n",
      "Epoch 10, Batch 814, LR 2.439685 Loss 19.066380, Accuracy 0.074%\n",
      "Epoch 10, Batch 815, LR 2.439778 Loss 19.066378, Accuracy 0.074%\n",
      "Epoch 10, Batch 816, LR 2.439872 Loss 19.066405, Accuracy 0.074%\n",
      "Epoch 10, Batch 817, LR 2.439965 Loss 19.066596, Accuracy 0.074%\n",
      "Epoch 10, Batch 818, LR 2.440058 Loss 19.066661, Accuracy 0.074%\n",
      "Epoch 10, Batch 819, LR 2.440151 Loss 19.066464, Accuracy 0.073%\n",
      "Epoch 10, Batch 820, LR 2.440244 Loss 19.066604, Accuracy 0.073%\n",
      "Epoch 10, Batch 821, LR 2.440337 Loss 19.066666, Accuracy 0.073%\n",
      "Epoch 10, Batch 822, LR 2.440430 Loss 19.066627, Accuracy 0.074%\n",
      "Epoch 10, Batch 823, LR 2.440523 Loss 19.066740, Accuracy 0.074%\n",
      "Epoch 10, Batch 824, LR 2.440616 Loss 19.066621, Accuracy 0.074%\n",
      "Epoch 10, Batch 825, LR 2.440709 Loss 19.066696, Accuracy 0.074%\n",
      "Epoch 10, Batch 826, LR 2.440801 Loss 19.066548, Accuracy 0.074%\n",
      "Epoch 10, Batch 827, LR 2.440894 Loss 19.066550, Accuracy 0.074%\n",
      "Epoch 10, Batch 828, LR 2.440986 Loss 19.066546, Accuracy 0.074%\n",
      "Epoch 10, Batch 829, LR 2.441079 Loss 19.066412, Accuracy 0.074%\n",
      "Epoch 10, Batch 830, LR 2.441171 Loss 19.066484, Accuracy 0.074%\n",
      "Epoch 10, Batch 831, LR 2.441263 Loss 19.066343, Accuracy 0.074%\n",
      "Epoch 10, Batch 832, LR 2.441355 Loss 19.066395, Accuracy 0.075%\n",
      "Epoch 10, Batch 833, LR 2.441448 Loss 19.066311, Accuracy 0.075%\n",
      "Epoch 10, Batch 834, LR 2.441540 Loss 19.066388, Accuracy 0.075%\n",
      "Epoch 10, Batch 835, LR 2.441632 Loss 19.066345, Accuracy 0.075%\n",
      "Epoch 10, Batch 836, LR 2.441724 Loss 19.066321, Accuracy 0.075%\n",
      "Epoch 10, Batch 837, LR 2.441815 Loss 19.066351, Accuracy 0.075%\n",
      "Epoch 10, Batch 838, LR 2.441907 Loss 19.066248, Accuracy 0.076%\n",
      "Epoch 10, Batch 839, LR 2.441999 Loss 19.066107, Accuracy 0.075%\n",
      "Epoch 10, Batch 840, LR 2.442090 Loss 19.065974, Accuracy 0.076%\n",
      "Epoch 10, Batch 841, LR 2.442182 Loss 19.066009, Accuracy 0.076%\n",
      "Epoch 10, Batch 842, LR 2.442274 Loss 19.066126, Accuracy 0.076%\n",
      "Epoch 10, Batch 843, LR 2.442365 Loss 19.066040, Accuracy 0.076%\n",
      "Epoch 10, Batch 844, LR 2.442456 Loss 19.066073, Accuracy 0.076%\n",
      "Epoch 10, Batch 845, LR 2.442548 Loss 19.066184, Accuracy 0.076%\n",
      "Epoch 10, Batch 846, LR 2.442639 Loss 19.066298, Accuracy 0.076%\n",
      "Epoch 10, Batch 847, LR 2.442730 Loss 19.066329, Accuracy 0.076%\n",
      "Epoch 10, Batch 848, LR 2.442821 Loss 19.066455, Accuracy 0.076%\n",
      "Epoch 10, Batch 849, LR 2.442912 Loss 19.066540, Accuracy 0.075%\n",
      "Epoch 10, Batch 850, LR 2.443003 Loss 19.066425, Accuracy 0.075%\n",
      "Epoch 10, Batch 851, LR 2.443094 Loss 19.066609, Accuracy 0.075%\n",
      "Epoch 10, Batch 852, LR 2.443185 Loss 19.066478, Accuracy 0.075%\n",
      "Epoch 10, Batch 853, LR 2.443275 Loss 19.066492, Accuracy 0.075%\n",
      "Epoch 10, Batch 854, LR 2.443366 Loss 19.066589, Accuracy 0.075%\n",
      "Epoch 10, Batch 855, LR 2.443457 Loss 19.066740, Accuracy 0.075%\n",
      "Epoch 10, Batch 856, LR 2.443547 Loss 19.067123, Accuracy 0.075%\n",
      "Epoch 10, Batch 857, LR 2.443637 Loss 19.067053, Accuracy 0.075%\n",
      "Epoch 10, Batch 858, LR 2.443728 Loss 19.066967, Accuracy 0.075%\n",
      "Epoch 10, Batch 859, LR 2.443818 Loss 19.067238, Accuracy 0.075%\n",
      "Epoch 10, Batch 860, LR 2.443908 Loss 19.067348, Accuracy 0.075%\n",
      "Epoch 10, Batch 861, LR 2.443999 Loss 19.067144, Accuracy 0.075%\n",
      "Epoch 10, Batch 862, LR 2.444089 Loss 19.067182, Accuracy 0.075%\n",
      "Epoch 10, Batch 863, LR 2.444179 Loss 19.067260, Accuracy 0.075%\n",
      "Epoch 10, Batch 864, LR 2.444269 Loss 19.067257, Accuracy 0.075%\n",
      "Epoch 10, Batch 865, LR 2.444358 Loss 19.067109, Accuracy 0.075%\n",
      "Epoch 10, Batch 866, LR 2.444448 Loss 19.066914, Accuracy 0.075%\n",
      "Epoch 10, Batch 867, LR 2.444538 Loss 19.066864, Accuracy 0.075%\n",
      "Epoch 10, Batch 868, LR 2.444628 Loss 19.066928, Accuracy 0.075%\n",
      "Epoch 10, Batch 869, LR 2.444717 Loss 19.066791, Accuracy 0.076%\n",
      "Epoch 10, Batch 870, LR 2.444807 Loss 19.066901, Accuracy 0.075%\n",
      "Epoch 10, Batch 871, LR 2.444896 Loss 19.066795, Accuracy 0.075%\n",
      "Epoch 10, Batch 872, LR 2.444986 Loss 19.066757, Accuracy 0.075%\n",
      "Epoch 10, Batch 873, LR 2.445075 Loss 19.066814, Accuracy 0.075%\n",
      "Epoch 10, Batch 874, LR 2.445164 Loss 19.066836, Accuracy 0.075%\n",
      "Epoch 10, Batch 875, LR 2.445253 Loss 19.066948, Accuracy 0.075%\n",
      "Epoch 10, Batch 876, LR 2.445342 Loss 19.066827, Accuracy 0.075%\n",
      "Epoch 10, Batch 877, LR 2.445431 Loss 19.066962, Accuracy 0.076%\n",
      "Epoch 10, Batch 878, LR 2.445520 Loss 19.066684, Accuracy 0.077%\n",
      "Epoch 10, Batch 879, LR 2.445609 Loss 19.066628, Accuracy 0.077%\n",
      "Epoch 10, Batch 880, LR 2.445698 Loss 19.066668, Accuracy 0.077%\n",
      "Epoch 10, Batch 881, LR 2.445787 Loss 19.066707, Accuracy 0.077%\n",
      "Epoch 10, Batch 882, LR 2.445875 Loss 19.066847, Accuracy 0.077%\n",
      "Epoch 10, Batch 883, LR 2.445964 Loss 19.066831, Accuracy 0.077%\n",
      "Epoch 10, Batch 884, LR 2.446052 Loss 19.066762, Accuracy 0.077%\n",
      "Epoch 10, Batch 885, LR 2.446141 Loss 19.066677, Accuracy 0.077%\n",
      "Epoch 10, Batch 886, LR 2.446229 Loss 19.066652, Accuracy 0.077%\n",
      "Epoch 10, Batch 887, LR 2.446318 Loss 19.066711, Accuracy 0.077%\n",
      "Epoch 10, Batch 888, LR 2.446406 Loss 19.066579, Accuracy 0.077%\n",
      "Epoch 10, Batch 889, LR 2.446494 Loss 19.066583, Accuracy 0.076%\n",
      "Epoch 10, Batch 890, LR 2.446582 Loss 19.066685, Accuracy 0.076%\n",
      "Epoch 10, Batch 891, LR 2.446670 Loss 19.066971, Accuracy 0.076%\n",
      "Epoch 10, Batch 892, LR 2.446758 Loss 19.066808, Accuracy 0.076%\n",
      "Epoch 10, Batch 893, LR 2.446846 Loss 19.066912, Accuracy 0.076%\n",
      "Epoch 10, Batch 894, LR 2.446934 Loss 19.067082, Accuracy 0.076%\n",
      "Epoch 10, Batch 895, LR 2.447022 Loss 19.067115, Accuracy 0.076%\n",
      "Epoch 10, Batch 896, LR 2.447109 Loss 19.067004, Accuracy 0.076%\n",
      "Epoch 10, Batch 897, LR 2.447197 Loss 19.066786, Accuracy 0.076%\n",
      "Epoch 10, Batch 898, LR 2.447284 Loss 19.066658, Accuracy 0.077%\n",
      "Epoch 10, Batch 899, LR 2.447372 Loss 19.066672, Accuracy 0.076%\n",
      "Epoch 10, Batch 900, LR 2.447459 Loss 19.066957, Accuracy 0.076%\n",
      "Epoch 10, Batch 901, LR 2.447547 Loss 19.066894, Accuracy 0.077%\n",
      "Epoch 10, Batch 902, LR 2.447634 Loss 19.066946, Accuracy 0.077%\n",
      "Epoch 10, Batch 903, LR 2.447721 Loss 19.067001, Accuracy 0.077%\n",
      "Epoch 10, Batch 904, LR 2.447808 Loss 19.067168, Accuracy 0.078%\n",
      "Epoch 10, Batch 905, LR 2.447895 Loss 19.067269, Accuracy 0.078%\n",
      "Epoch 10, Batch 906, LR 2.447982 Loss 19.067280, Accuracy 0.078%\n",
      "Epoch 10, Batch 907, LR 2.448069 Loss 19.067378, Accuracy 0.078%\n",
      "Epoch 10, Batch 908, LR 2.448156 Loss 19.067364, Accuracy 0.078%\n",
      "Epoch 10, Batch 909, LR 2.448243 Loss 19.067610, Accuracy 0.078%\n",
      "Epoch 10, Batch 910, LR 2.448329 Loss 19.067238, Accuracy 0.078%\n",
      "Epoch 10, Batch 911, LR 2.448416 Loss 19.067300, Accuracy 0.078%\n",
      "Epoch 10, Batch 912, LR 2.448502 Loss 19.067539, Accuracy 0.078%\n",
      "Epoch 10, Batch 913, LR 2.448589 Loss 19.067568, Accuracy 0.078%\n",
      "Epoch 10, Batch 914, LR 2.448675 Loss 19.067725, Accuracy 0.078%\n",
      "Epoch 10, Batch 915, LR 2.448762 Loss 19.067712, Accuracy 0.078%\n",
      "Epoch 10, Batch 916, LR 2.448848 Loss 19.067710, Accuracy 0.078%\n",
      "Epoch 10, Batch 917, LR 2.448934 Loss 19.067516, Accuracy 0.078%\n",
      "Epoch 10, Batch 918, LR 2.449020 Loss 19.067636, Accuracy 0.077%\n",
      "Epoch 10, Batch 919, LR 2.449106 Loss 19.067667, Accuracy 0.077%\n",
      "Epoch 10, Batch 920, LR 2.449192 Loss 19.067705, Accuracy 0.077%\n",
      "Epoch 10, Batch 921, LR 2.449278 Loss 19.067735, Accuracy 0.077%\n",
      "Epoch 10, Batch 922, LR 2.449364 Loss 19.067904, Accuracy 0.077%\n",
      "Epoch 10, Batch 923, LR 2.449450 Loss 19.067921, Accuracy 0.077%\n",
      "Epoch 10, Batch 924, LR 2.449535 Loss 19.067833, Accuracy 0.077%\n",
      "Epoch 10, Batch 925, LR 2.449621 Loss 19.067991, Accuracy 0.077%\n",
      "Epoch 10, Batch 926, LR 2.449706 Loss 19.067853, Accuracy 0.077%\n",
      "Epoch 10, Batch 927, LR 2.449792 Loss 19.067737, Accuracy 0.077%\n",
      "Epoch 10, Batch 928, LR 2.449877 Loss 19.067548, Accuracy 0.077%\n",
      "Epoch 10, Batch 929, LR 2.449963 Loss 19.067423, Accuracy 0.077%\n",
      "Epoch 10, Batch 930, LR 2.450048 Loss 19.067394, Accuracy 0.077%\n",
      "Epoch 10, Batch 931, LR 2.450133 Loss 19.067586, Accuracy 0.077%\n",
      "Epoch 10, Batch 932, LR 2.450218 Loss 19.067678, Accuracy 0.077%\n",
      "Epoch 10, Batch 933, LR 2.450303 Loss 19.067754, Accuracy 0.079%\n",
      "Epoch 10, Batch 934, LR 2.450388 Loss 19.067891, Accuracy 0.079%\n",
      "Epoch 10, Batch 935, LR 2.450473 Loss 19.067610, Accuracy 0.079%\n",
      "Epoch 10, Batch 936, LR 2.450558 Loss 19.067691, Accuracy 0.078%\n",
      "Epoch 10, Batch 937, LR 2.450643 Loss 19.067603, Accuracy 0.078%\n",
      "Epoch 10, Batch 938, LR 2.450727 Loss 19.067678, Accuracy 0.078%\n",
      "Epoch 10, Batch 939, LR 2.450812 Loss 19.067465, Accuracy 0.078%\n",
      "Epoch 10, Batch 940, LR 2.450897 Loss 19.067335, Accuracy 0.078%\n",
      "Epoch 10, Batch 941, LR 2.450981 Loss 19.067358, Accuracy 0.078%\n",
      "Epoch 10, Batch 942, LR 2.451065 Loss 19.067328, Accuracy 0.078%\n",
      "Epoch 10, Batch 943, LR 2.451150 Loss 19.067391, Accuracy 0.078%\n",
      "Epoch 10, Batch 944, LR 2.451234 Loss 19.067337, Accuracy 0.078%\n",
      "Epoch 10, Batch 945, LR 2.451318 Loss 19.067178, Accuracy 0.079%\n",
      "Epoch 10, Batch 946, LR 2.451402 Loss 19.067184, Accuracy 0.078%\n",
      "Epoch 10, Batch 947, LR 2.451486 Loss 19.067127, Accuracy 0.078%\n",
      "Epoch 10, Batch 948, LR 2.451570 Loss 19.067234, Accuracy 0.078%\n",
      "Epoch 10, Batch 949, LR 2.451654 Loss 19.067239, Accuracy 0.078%\n",
      "Epoch 10, Batch 950, LR 2.451738 Loss 19.067131, Accuracy 0.078%\n",
      "Epoch 10, Batch 951, LR 2.451822 Loss 19.067065, Accuracy 0.078%\n",
      "Epoch 10, Batch 952, LR 2.451906 Loss 19.067110, Accuracy 0.078%\n",
      "Epoch 10, Batch 953, LR 2.451989 Loss 19.067012, Accuracy 0.078%\n",
      "Epoch 10, Batch 954, LR 2.452073 Loss 19.067210, Accuracy 0.078%\n",
      "Epoch 10, Batch 955, LR 2.452156 Loss 19.067112, Accuracy 0.078%\n",
      "Epoch 10, Batch 956, LR 2.452240 Loss 19.067046, Accuracy 0.078%\n",
      "Epoch 10, Batch 957, LR 2.452323 Loss 19.067036, Accuracy 0.078%\n",
      "Epoch 10, Batch 958, LR 2.452406 Loss 19.066972, Accuracy 0.078%\n",
      "Epoch 10, Batch 959, LR 2.452489 Loss 19.067085, Accuracy 0.078%\n",
      "Epoch 10, Batch 960, LR 2.452572 Loss 19.066997, Accuracy 0.078%\n",
      "Epoch 10, Batch 961, LR 2.452656 Loss 19.066801, Accuracy 0.078%\n",
      "Epoch 10, Batch 962, LR 2.452739 Loss 19.066813, Accuracy 0.078%\n",
      "Epoch 10, Batch 963, LR 2.452821 Loss 19.066878, Accuracy 0.078%\n",
      "Epoch 10, Batch 964, LR 2.452904 Loss 19.067016, Accuracy 0.078%\n",
      "Epoch 10, Batch 965, LR 2.452987 Loss 19.067136, Accuracy 0.078%\n",
      "Epoch 10, Batch 966, LR 2.453070 Loss 19.067005, Accuracy 0.078%\n",
      "Epoch 10, Batch 967, LR 2.453152 Loss 19.067122, Accuracy 0.078%\n",
      "Epoch 10, Batch 968, LR 2.453235 Loss 19.067420, Accuracy 0.077%\n",
      "Epoch 10, Batch 969, LR 2.453317 Loss 19.067397, Accuracy 0.077%\n",
      "Epoch 10, Batch 970, LR 2.453400 Loss 19.067264, Accuracy 0.077%\n",
      "Epoch 10, Batch 971, LR 2.453482 Loss 19.067130, Accuracy 0.077%\n",
      "Epoch 10, Batch 972, LR 2.453564 Loss 19.067121, Accuracy 0.077%\n",
      "Epoch 10, Batch 973, LR 2.453647 Loss 19.067145, Accuracy 0.077%\n",
      "Epoch 10, Batch 974, LR 2.453729 Loss 19.067113, Accuracy 0.077%\n",
      "Epoch 10, Batch 975, LR 2.453811 Loss 19.067283, Accuracy 0.077%\n",
      "Epoch 10, Batch 976, LR 2.453893 Loss 19.067187, Accuracy 0.078%\n",
      "Epoch 10, Batch 977, LR 2.453975 Loss 19.066927, Accuracy 0.078%\n",
      "Epoch 10, Batch 978, LR 2.454056 Loss 19.066922, Accuracy 0.078%\n",
      "Epoch 10, Batch 979, LR 2.454138 Loss 19.066860, Accuracy 0.078%\n",
      "Epoch 10, Batch 980, LR 2.454220 Loss 19.067024, Accuracy 0.078%\n",
      "Epoch 10, Batch 981, LR 2.454302 Loss 19.067112, Accuracy 0.078%\n",
      "Epoch 10, Batch 982, LR 2.454383 Loss 19.067168, Accuracy 0.078%\n",
      "Epoch 10, Batch 983, LR 2.454465 Loss 19.067128, Accuracy 0.078%\n",
      "Epoch 10, Batch 984, LR 2.454546 Loss 19.067032, Accuracy 0.078%\n",
      "Epoch 10, Batch 985, LR 2.454627 Loss 19.067129, Accuracy 0.078%\n",
      "Epoch 10, Batch 986, LR 2.454709 Loss 19.067076, Accuracy 0.078%\n",
      "Epoch 10, Batch 987, LR 2.454790 Loss 19.067132, Accuracy 0.078%\n",
      "Epoch 10, Batch 988, LR 2.454871 Loss 19.066886, Accuracy 0.077%\n",
      "Epoch 10, Batch 989, LR 2.454952 Loss 19.066733, Accuracy 0.077%\n",
      "Epoch 10, Batch 990, LR 2.455033 Loss 19.066671, Accuracy 0.077%\n",
      "Epoch 10, Batch 991, LR 2.455114 Loss 19.066773, Accuracy 0.078%\n",
      "Epoch 10, Batch 992, LR 2.455195 Loss 19.066824, Accuracy 0.078%\n",
      "Epoch 10, Batch 993, LR 2.455275 Loss 19.066871, Accuracy 0.078%\n",
      "Epoch 10, Batch 994, LR 2.455356 Loss 19.066844, Accuracy 0.078%\n",
      "Epoch 10, Batch 995, LR 2.455437 Loss 19.066756, Accuracy 0.078%\n",
      "Epoch 10, Batch 996, LR 2.455517 Loss 19.066817, Accuracy 0.078%\n",
      "Epoch 10, Batch 997, LR 2.455598 Loss 19.066777, Accuracy 0.078%\n",
      "Epoch 10, Batch 998, LR 2.455678 Loss 19.066476, Accuracy 0.077%\n",
      "Epoch 10, Batch 999, LR 2.455759 Loss 19.066413, Accuracy 0.077%\n",
      "Epoch 10, Batch 1000, LR 2.455839 Loss 19.066467, Accuracy 0.077%\n",
      "Epoch 10, Batch 1001, LR 2.455919 Loss 19.066573, Accuracy 0.077%\n",
      "Epoch 10, Batch 1002, LR 2.455999 Loss 19.066721, Accuracy 0.077%\n",
      "Epoch 10, Batch 1003, LR 2.456079 Loss 19.066734, Accuracy 0.077%\n",
      "Epoch 10, Batch 1004, LR 2.456159 Loss 19.066660, Accuracy 0.077%\n",
      "Epoch 10, Batch 1005, LR 2.456239 Loss 19.066661, Accuracy 0.077%\n",
      "Epoch 10, Batch 1006, LR 2.456319 Loss 19.066628, Accuracy 0.077%\n",
      "Epoch 10, Batch 1007, LR 2.456399 Loss 19.066482, Accuracy 0.077%\n",
      "Epoch 10, Batch 1008, LR 2.456478 Loss 19.066632, Accuracy 0.077%\n",
      "Epoch 10, Batch 1009, LR 2.456558 Loss 19.066690, Accuracy 0.077%\n",
      "Epoch 10, Batch 1010, LR 2.456638 Loss 19.066655, Accuracy 0.077%\n",
      "Epoch 10, Batch 1011, LR 2.456717 Loss 19.066860, Accuracy 0.077%\n",
      "Epoch 10, Batch 1012, LR 2.456796 Loss 19.066908, Accuracy 0.076%\n",
      "Epoch 10, Batch 1013, LR 2.456876 Loss 19.067006, Accuracy 0.076%\n",
      "Epoch 10, Batch 1014, LR 2.456955 Loss 19.066973, Accuracy 0.076%\n",
      "Epoch 10, Batch 1015, LR 2.457034 Loss 19.066744, Accuracy 0.076%\n",
      "Epoch 10, Batch 1016, LR 2.457113 Loss 19.066636, Accuracy 0.077%\n",
      "Epoch 10, Batch 1017, LR 2.457192 Loss 19.066590, Accuracy 0.077%\n",
      "Epoch 10, Batch 1018, LR 2.457271 Loss 19.066451, Accuracy 0.077%\n",
      "Epoch 10, Batch 1019, LR 2.457350 Loss 19.066587, Accuracy 0.077%\n",
      "Epoch 10, Batch 1020, LR 2.457429 Loss 19.066516, Accuracy 0.077%\n",
      "Epoch 10, Batch 1021, LR 2.457508 Loss 19.066575, Accuracy 0.077%\n",
      "Epoch 10, Batch 1022, LR 2.457587 Loss 19.066811, Accuracy 0.077%\n",
      "Epoch 10, Batch 1023, LR 2.457665 Loss 19.066868, Accuracy 0.077%\n",
      "Epoch 10, Batch 1024, LR 2.457744 Loss 19.066607, Accuracy 0.077%\n",
      "Epoch 10, Batch 1025, LR 2.457822 Loss 19.066696, Accuracy 0.077%\n",
      "Epoch 10, Batch 1026, LR 2.457901 Loss 19.066936, Accuracy 0.077%\n",
      "Epoch 10, Batch 1027, LR 2.457979 Loss 19.067016, Accuracy 0.077%\n",
      "Epoch 10, Batch 1028, LR 2.458057 Loss 19.067046, Accuracy 0.077%\n",
      "Epoch 10, Batch 1029, LR 2.458135 Loss 19.067069, Accuracy 0.077%\n",
      "Epoch 10, Batch 1030, LR 2.458213 Loss 19.067149, Accuracy 0.077%\n",
      "Epoch 10, Batch 1031, LR 2.458292 Loss 19.067225, Accuracy 0.077%\n",
      "Epoch 10, Batch 1032, LR 2.458369 Loss 19.067014, Accuracy 0.076%\n",
      "Epoch 10, Batch 1033, LR 2.458447 Loss 19.066945, Accuracy 0.076%\n",
      "Epoch 10, Batch 1034, LR 2.458525 Loss 19.067140, Accuracy 0.076%\n",
      "Epoch 10, Batch 1035, LR 2.458603 Loss 19.067320, Accuracy 0.076%\n",
      "Epoch 10, Batch 1036, LR 2.458681 Loss 19.067297, Accuracy 0.076%\n",
      "Epoch 10, Batch 1037, LR 2.458758 Loss 19.067334, Accuracy 0.076%\n",
      "Epoch 10, Batch 1038, LR 2.458836 Loss 19.067192, Accuracy 0.076%\n",
      "Epoch 10, Batch 1039, LR 2.458913 Loss 19.067110, Accuracy 0.076%\n",
      "Epoch 10, Batch 1040, LR 2.458991 Loss 19.067097, Accuracy 0.076%\n",
      "Epoch 10, Batch 1041, LR 2.459068 Loss 19.067071, Accuracy 0.077%\n",
      "Epoch 10, Batch 1042, LR 2.459145 Loss 19.067008, Accuracy 0.076%\n",
      "Epoch 10, Batch 1043, LR 2.459222 Loss 19.066945, Accuracy 0.076%\n",
      "Epoch 10, Batch 1044, LR 2.459300 Loss 19.066951, Accuracy 0.076%\n",
      "Epoch 10, Batch 1045, LR 2.459377 Loss 19.066860, Accuracy 0.077%\n",
      "Epoch 10, Batch 1046, LR 2.459454 Loss 19.066879, Accuracy 0.077%\n",
      "Epoch 10, Batch 1047, LR 2.459531 Loss 19.066863, Accuracy 0.077%\n",
      "Epoch 10, Loss (train set) 19.066863, Accuracy (train set) 0.077%\n",
      "Epoch 11, Batch 1, LR 2.459607 Loss 19.038433, Accuracy 0.000%\n",
      "Epoch 11, Batch 2, LR 2.459684 Loss 19.027784, Accuracy 0.000%\n",
      "Epoch 11, Batch 3, LR 2.459761 Loss 19.017412, Accuracy 0.000%\n",
      "Epoch 11, Batch 4, LR 2.459837 Loss 19.051779, Accuracy 0.000%\n",
      "Epoch 11, Batch 5, LR 2.459914 Loss 19.058540, Accuracy 0.000%\n",
      "Epoch 11, Batch 6, LR 2.459990 Loss 19.067264, Accuracy 0.000%\n",
      "Epoch 11, Batch 7, LR 2.460067 Loss 19.067885, Accuracy 0.000%\n",
      "Epoch 11, Batch 8, LR 2.460143 Loss 19.075729, Accuracy 0.098%\n",
      "Epoch 11, Batch 9, LR 2.460219 Loss 19.067859, Accuracy 0.087%\n",
      "Epoch 11, Batch 10, LR 2.460296 Loss 19.062207, Accuracy 0.078%\n",
      "Epoch 11, Batch 11, LR 2.460372 Loss 19.071288, Accuracy 0.071%\n",
      "Epoch 11, Batch 12, LR 2.460448 Loss 19.080959, Accuracy 0.065%\n",
      "Epoch 11, Batch 13, LR 2.460524 Loss 19.081072, Accuracy 0.060%\n",
      "Epoch 11, Batch 14, LR 2.460600 Loss 19.081440, Accuracy 0.056%\n",
      "Epoch 11, Batch 15, LR 2.460675 Loss 19.082437, Accuracy 0.104%\n",
      "Epoch 11, Batch 16, LR 2.460751 Loss 19.071053, Accuracy 0.098%\n",
      "Epoch 11, Batch 17, LR 2.460827 Loss 19.081074, Accuracy 0.138%\n",
      "Epoch 11, Batch 18, LR 2.460902 Loss 19.070506, Accuracy 0.130%\n",
      "Epoch 11, Batch 19, LR 2.460978 Loss 19.065368, Accuracy 0.164%\n",
      "Epoch 11, Batch 20, LR 2.461053 Loss 19.067024, Accuracy 0.156%\n",
      "Epoch 11, Batch 21, LR 2.461129 Loss 19.052567, Accuracy 0.149%\n",
      "Epoch 11, Batch 22, LR 2.461204 Loss 19.050751, Accuracy 0.142%\n",
      "Epoch 11, Batch 23, LR 2.461279 Loss 19.056524, Accuracy 0.136%\n",
      "Epoch 11, Batch 24, LR 2.461355 Loss 19.046553, Accuracy 0.130%\n",
      "Epoch 11, Batch 25, LR 2.461430 Loss 19.038596, Accuracy 0.125%\n",
      "Epoch 11, Batch 26, LR 2.461505 Loss 19.036187, Accuracy 0.120%\n",
      "Epoch 11, Batch 27, LR 2.461580 Loss 19.032239, Accuracy 0.116%\n",
      "Epoch 11, Batch 28, LR 2.461655 Loss 19.030697, Accuracy 0.112%\n",
      "Epoch 11, Batch 29, LR 2.461729 Loss 19.034946, Accuracy 0.108%\n",
      "Epoch 11, Batch 30, LR 2.461804 Loss 19.035489, Accuracy 0.104%\n",
      "Epoch 11, Batch 31, LR 2.461879 Loss 19.039370, Accuracy 0.101%\n",
      "Epoch 11, Batch 32, LR 2.461953 Loss 19.031209, Accuracy 0.098%\n",
      "Epoch 11, Batch 33, LR 2.462028 Loss 19.035109, Accuracy 0.095%\n",
      "Epoch 11, Batch 34, LR 2.462102 Loss 19.035647, Accuracy 0.092%\n",
      "Epoch 11, Batch 35, LR 2.462177 Loss 19.036609, Accuracy 0.089%\n",
      "Epoch 11, Batch 36, LR 2.462251 Loss 19.042963, Accuracy 0.087%\n",
      "Epoch 11, Batch 37, LR 2.462325 Loss 19.042642, Accuracy 0.084%\n",
      "Epoch 11, Batch 38, LR 2.462399 Loss 19.038622, Accuracy 0.082%\n",
      "Epoch 11, Batch 39, LR 2.462474 Loss 19.040331, Accuracy 0.100%\n",
      "Epoch 11, Batch 40, LR 2.462548 Loss 19.040020, Accuracy 0.098%\n",
      "Epoch 11, Batch 41, LR 2.462622 Loss 19.041282, Accuracy 0.095%\n",
      "Epoch 11, Batch 42, LR 2.462695 Loss 19.040931, Accuracy 0.093%\n",
      "Epoch 11, Batch 43, LR 2.462769 Loss 19.040048, Accuracy 0.091%\n",
      "Epoch 11, Batch 44, LR 2.462843 Loss 19.040036, Accuracy 0.089%\n",
      "Epoch 11, Batch 45, LR 2.462917 Loss 19.037723, Accuracy 0.087%\n",
      "Epoch 11, Batch 46, LR 2.462990 Loss 19.034643, Accuracy 0.085%\n",
      "Epoch 11, Batch 47, LR 2.463064 Loss 19.036733, Accuracy 0.083%\n",
      "Epoch 11, Batch 48, LR 2.463137 Loss 19.036943, Accuracy 0.081%\n",
      "Epoch 11, Batch 49, LR 2.463211 Loss 19.037091, Accuracy 0.080%\n",
      "Epoch 11, Batch 50, LR 2.463284 Loss 19.040749, Accuracy 0.078%\n",
      "Epoch 11, Batch 51, LR 2.463357 Loss 19.044832, Accuracy 0.077%\n",
      "Epoch 11, Batch 52, LR 2.463430 Loss 19.044579, Accuracy 0.075%\n",
      "Epoch 11, Batch 53, LR 2.463503 Loss 19.045964, Accuracy 0.074%\n",
      "Epoch 11, Batch 54, LR 2.463576 Loss 19.048308, Accuracy 0.072%\n",
      "Epoch 11, Batch 55, LR 2.463649 Loss 19.048237, Accuracy 0.071%\n",
      "Epoch 11, Batch 56, LR 2.463722 Loss 19.047494, Accuracy 0.070%\n",
      "Epoch 11, Batch 57, LR 2.463795 Loss 19.046447, Accuracy 0.082%\n",
      "Epoch 11, Batch 58, LR 2.463868 Loss 19.049103, Accuracy 0.081%\n",
      "Epoch 11, Batch 59, LR 2.463940 Loss 19.046325, Accuracy 0.093%\n",
      "Epoch 11, Batch 60, LR 2.464013 Loss 19.045198, Accuracy 0.091%\n",
      "Epoch 11, Batch 61, LR 2.464086 Loss 19.042027, Accuracy 0.090%\n",
      "Epoch 11, Batch 62, LR 2.464158 Loss 19.044938, Accuracy 0.088%\n",
      "Epoch 11, Batch 63, LR 2.464230 Loss 19.045562, Accuracy 0.087%\n",
      "Epoch 11, Batch 64, LR 2.464303 Loss 19.047537, Accuracy 0.085%\n",
      "Epoch 11, Batch 65, LR 2.464375 Loss 19.047835, Accuracy 0.084%\n",
      "Epoch 11, Batch 66, LR 2.464447 Loss 19.051462, Accuracy 0.083%\n",
      "Epoch 11, Batch 67, LR 2.464519 Loss 19.049044, Accuracy 0.082%\n",
      "Epoch 11, Batch 68, LR 2.464591 Loss 19.052596, Accuracy 0.080%\n",
      "Epoch 11, Batch 69, LR 2.464663 Loss 19.057749, Accuracy 0.079%\n",
      "Epoch 11, Batch 70, LR 2.464735 Loss 19.057286, Accuracy 0.078%\n",
      "Epoch 11, Batch 71, LR 2.464807 Loss 19.057634, Accuracy 0.077%\n",
      "Epoch 11, Batch 72, LR 2.464878 Loss 19.057938, Accuracy 0.076%\n",
      "Epoch 11, Batch 73, LR 2.464950 Loss 19.057606, Accuracy 0.075%\n",
      "Epoch 11, Batch 74, LR 2.465022 Loss 19.055599, Accuracy 0.074%\n",
      "Epoch 11, Batch 75, LR 2.465093 Loss 19.056109, Accuracy 0.073%\n",
      "Epoch 11, Batch 76, LR 2.465165 Loss 19.055381, Accuracy 0.072%\n",
      "Epoch 11, Batch 77, LR 2.465236 Loss 19.055674, Accuracy 0.071%\n",
      "Epoch 11, Batch 78, LR 2.465307 Loss 19.058723, Accuracy 0.070%\n",
      "Epoch 11, Batch 79, LR 2.465379 Loss 19.060041, Accuracy 0.069%\n",
      "Epoch 11, Batch 80, LR 2.465450 Loss 19.060276, Accuracy 0.068%\n",
      "Epoch 11, Batch 81, LR 2.465521 Loss 19.062240, Accuracy 0.068%\n",
      "Epoch 11, Batch 82, LR 2.465592 Loss 19.061532, Accuracy 0.067%\n",
      "Epoch 11, Batch 83, LR 2.465663 Loss 19.061989, Accuracy 0.066%\n",
      "Epoch 11, Batch 84, LR 2.465734 Loss 19.061371, Accuracy 0.065%\n",
      "Epoch 11, Batch 85, LR 2.465804 Loss 19.062737, Accuracy 0.064%\n",
      "Epoch 11, Batch 86, LR 2.465875 Loss 19.062229, Accuracy 0.064%\n",
      "Epoch 11, Batch 87, LR 2.465946 Loss 19.062350, Accuracy 0.063%\n",
      "Epoch 11, Batch 88, LR 2.466016 Loss 19.061774, Accuracy 0.062%\n",
      "Epoch 11, Batch 89, LR 2.466087 Loss 19.061337, Accuracy 0.061%\n",
      "Epoch 11, Batch 90, LR 2.466157 Loss 19.063358, Accuracy 0.061%\n",
      "Epoch 11, Batch 91, LR 2.466228 Loss 19.063479, Accuracy 0.060%\n",
      "Epoch 11, Batch 92, LR 2.466298 Loss 19.063993, Accuracy 0.059%\n",
      "Epoch 11, Batch 93, LR 2.466368 Loss 19.061840, Accuracy 0.059%\n",
      "Epoch 11, Batch 94, LR 2.466438 Loss 19.063563, Accuracy 0.058%\n",
      "Epoch 11, Batch 95, LR 2.466508 Loss 19.063246, Accuracy 0.066%\n",
      "Epoch 11, Batch 96, LR 2.466578 Loss 19.064653, Accuracy 0.065%\n",
      "Epoch 11, Batch 97, LR 2.466648 Loss 19.065308, Accuracy 0.064%\n",
      "Epoch 11, Batch 98, LR 2.466718 Loss 19.067107, Accuracy 0.064%\n",
      "Epoch 11, Batch 99, LR 2.466788 Loss 19.065728, Accuracy 0.063%\n",
      "Epoch 11, Batch 100, LR 2.466857 Loss 19.064999, Accuracy 0.062%\n",
      "Epoch 11, Batch 101, LR 2.466927 Loss 19.064169, Accuracy 0.062%\n",
      "Epoch 11, Batch 102, LR 2.466997 Loss 19.063293, Accuracy 0.061%\n",
      "Epoch 11, Batch 103, LR 2.467066 Loss 19.062680, Accuracy 0.061%\n",
      "Epoch 11, Batch 104, LR 2.467136 Loss 19.062075, Accuracy 0.068%\n",
      "Epoch 11, Batch 105, LR 2.467205 Loss 19.061688, Accuracy 0.067%\n",
      "Epoch 11, Batch 106, LR 2.467274 Loss 19.060925, Accuracy 0.066%\n",
      "Epoch 11, Batch 107, LR 2.467343 Loss 19.060412, Accuracy 0.066%\n",
      "Epoch 11, Batch 108, LR 2.467412 Loss 19.060386, Accuracy 0.065%\n",
      "Epoch 11, Batch 109, LR 2.467482 Loss 19.059277, Accuracy 0.072%\n",
      "Epoch 11, Batch 110, LR 2.467551 Loss 19.058842, Accuracy 0.071%\n",
      "Epoch 11, Batch 111, LR 2.467619 Loss 19.058535, Accuracy 0.070%\n",
      "Epoch 11, Batch 112, LR 2.467688 Loss 19.059959, Accuracy 0.070%\n",
      "Epoch 11, Batch 113, LR 2.467757 Loss 19.061693, Accuracy 0.069%\n",
      "Epoch 11, Batch 114, LR 2.467826 Loss 19.063146, Accuracy 0.069%\n",
      "Epoch 11, Batch 115, LR 2.467894 Loss 19.062483, Accuracy 0.068%\n",
      "Epoch 11, Batch 116, LR 2.467963 Loss 19.063102, Accuracy 0.067%\n",
      "Epoch 11, Batch 117, LR 2.468031 Loss 19.064476, Accuracy 0.073%\n",
      "Epoch 11, Batch 118, LR 2.468100 Loss 19.063357, Accuracy 0.073%\n",
      "Epoch 11, Batch 119, LR 2.468168 Loss 19.061245, Accuracy 0.072%\n",
      "Epoch 11, Batch 120, LR 2.468236 Loss 19.061087, Accuracy 0.072%\n",
      "Epoch 11, Batch 121, LR 2.468305 Loss 19.060202, Accuracy 0.071%\n",
      "Epoch 11, Batch 122, LR 2.468373 Loss 19.061831, Accuracy 0.070%\n",
      "Epoch 11, Batch 123, LR 2.468441 Loss 19.062289, Accuracy 0.070%\n",
      "Epoch 11, Batch 124, LR 2.468509 Loss 19.061850, Accuracy 0.069%\n",
      "Epoch 11, Batch 125, LR 2.468577 Loss 19.061439, Accuracy 0.069%\n",
      "Epoch 11, Batch 126, LR 2.468644 Loss 19.062212, Accuracy 0.068%\n",
      "Epoch 11, Batch 127, LR 2.468712 Loss 19.062422, Accuracy 0.068%\n",
      "Epoch 11, Batch 128, LR 2.468780 Loss 19.062432, Accuracy 0.067%\n",
      "Epoch 11, Batch 129, LR 2.468847 Loss 19.061023, Accuracy 0.067%\n",
      "Epoch 11, Batch 130, LR 2.468915 Loss 19.062261, Accuracy 0.066%\n",
      "Epoch 11, Batch 131, LR 2.468982 Loss 19.063666, Accuracy 0.066%\n",
      "Epoch 11, Batch 132, LR 2.469050 Loss 19.063476, Accuracy 0.065%\n",
      "Epoch 11, Batch 133, LR 2.469117 Loss 19.062820, Accuracy 0.065%\n",
      "Epoch 11, Batch 134, LR 2.469184 Loss 19.062835, Accuracy 0.064%\n",
      "Epoch 11, Batch 135, LR 2.469252 Loss 19.063478, Accuracy 0.064%\n",
      "Epoch 11, Batch 136, LR 2.469319 Loss 19.063070, Accuracy 0.063%\n",
      "Epoch 11, Batch 137, LR 2.469386 Loss 19.062291, Accuracy 0.063%\n",
      "Epoch 11, Batch 138, LR 2.469453 Loss 19.061630, Accuracy 0.062%\n",
      "Epoch 11, Batch 139, LR 2.469520 Loss 19.061847, Accuracy 0.062%\n",
      "Epoch 11, Batch 140, LR 2.469586 Loss 19.062490, Accuracy 0.061%\n",
      "Epoch 11, Batch 141, LR 2.469653 Loss 19.062023, Accuracy 0.061%\n",
      "Epoch 11, Batch 142, LR 2.469720 Loss 19.061453, Accuracy 0.061%\n",
      "Epoch 11, Batch 143, LR 2.469786 Loss 19.062991, Accuracy 0.060%\n",
      "Epoch 11, Batch 144, LR 2.469853 Loss 19.063574, Accuracy 0.060%\n",
      "Epoch 11, Batch 145, LR 2.469919 Loss 19.064283, Accuracy 0.059%\n",
      "Epoch 11, Batch 146, LR 2.469986 Loss 19.062905, Accuracy 0.064%\n",
      "Epoch 11, Batch 147, LR 2.470052 Loss 19.062781, Accuracy 0.069%\n",
      "Epoch 11, Batch 148, LR 2.470118 Loss 19.063455, Accuracy 0.069%\n",
      "Epoch 11, Batch 149, LR 2.470184 Loss 19.062562, Accuracy 0.068%\n",
      "Epoch 11, Batch 150, LR 2.470250 Loss 19.063895, Accuracy 0.068%\n",
      "Epoch 11, Batch 151, LR 2.470317 Loss 19.063310, Accuracy 0.067%\n",
      "Epoch 11, Batch 152, LR 2.470382 Loss 19.063357, Accuracy 0.067%\n",
      "Epoch 11, Batch 153, LR 2.470448 Loss 19.063223, Accuracy 0.066%\n",
      "Epoch 11, Batch 154, LR 2.470514 Loss 19.064450, Accuracy 0.066%\n",
      "Epoch 11, Batch 155, LR 2.470580 Loss 19.064513, Accuracy 0.071%\n",
      "Epoch 11, Batch 156, LR 2.470646 Loss 19.064655, Accuracy 0.070%\n",
      "Epoch 11, Batch 157, LR 2.470711 Loss 19.066208, Accuracy 0.070%\n",
      "Epoch 11, Batch 158, LR 2.470777 Loss 19.067029, Accuracy 0.069%\n",
      "Epoch 11, Batch 159, LR 2.470842 Loss 19.067336, Accuracy 0.069%\n",
      "Epoch 11, Batch 160, LR 2.470907 Loss 19.066092, Accuracy 0.068%\n",
      "Epoch 11, Batch 161, LR 2.470973 Loss 19.067293, Accuracy 0.068%\n",
      "Epoch 11, Batch 162, LR 2.471038 Loss 19.067447, Accuracy 0.068%\n",
      "Epoch 11, Batch 163, LR 2.471103 Loss 19.067540, Accuracy 0.067%\n",
      "Epoch 11, Batch 164, LR 2.471168 Loss 19.068335, Accuracy 0.067%\n",
      "Epoch 11, Batch 165, LR 2.471233 Loss 19.067830, Accuracy 0.066%\n",
      "Epoch 11, Batch 166, LR 2.471298 Loss 19.068169, Accuracy 0.066%\n",
      "Epoch 11, Batch 167, LR 2.471363 Loss 19.070305, Accuracy 0.065%\n",
      "Epoch 11, Batch 168, LR 2.471428 Loss 19.070676, Accuracy 0.065%\n",
      "Epoch 11, Batch 169, LR 2.471492 Loss 19.070601, Accuracy 0.069%\n",
      "Epoch 11, Batch 170, LR 2.471557 Loss 19.069605, Accuracy 0.069%\n",
      "Epoch 11, Batch 171, LR 2.471622 Loss 19.069950, Accuracy 0.069%\n",
      "Epoch 11, Batch 172, LR 2.471686 Loss 19.070066, Accuracy 0.068%\n",
      "Epoch 11, Batch 173, LR 2.471751 Loss 19.070816, Accuracy 0.068%\n",
      "Epoch 11, Batch 174, LR 2.471815 Loss 19.070482, Accuracy 0.067%\n",
      "Epoch 11, Batch 175, LR 2.471879 Loss 19.070202, Accuracy 0.071%\n",
      "Epoch 11, Batch 176, LR 2.471943 Loss 19.068870, Accuracy 0.075%\n",
      "Epoch 11, Batch 177, LR 2.472008 Loss 19.069243, Accuracy 0.075%\n",
      "Epoch 11, Batch 178, LR 2.472072 Loss 19.069760, Accuracy 0.075%\n",
      "Epoch 11, Batch 179, LR 2.472136 Loss 19.070616, Accuracy 0.074%\n",
      "Epoch 11, Batch 180, LR 2.472199 Loss 19.069719, Accuracy 0.074%\n",
      "Epoch 11, Batch 181, LR 2.472263 Loss 19.068832, Accuracy 0.073%\n",
      "Epoch 11, Batch 182, LR 2.472327 Loss 19.068770, Accuracy 0.073%\n",
      "Epoch 11, Batch 183, LR 2.472391 Loss 19.067397, Accuracy 0.073%\n",
      "Epoch 11, Batch 184, LR 2.472454 Loss 19.067425, Accuracy 0.072%\n",
      "Epoch 11, Batch 185, LR 2.472518 Loss 19.067704, Accuracy 0.080%\n",
      "Epoch 11, Batch 186, LR 2.472581 Loss 19.067378, Accuracy 0.080%\n",
      "Epoch 11, Batch 187, LR 2.472645 Loss 19.067718, Accuracy 0.079%\n",
      "Epoch 11, Batch 188, LR 2.472708 Loss 19.068688, Accuracy 0.079%\n",
      "Epoch 11, Batch 189, LR 2.472771 Loss 19.068028, Accuracy 0.079%\n",
      "Epoch 11, Batch 190, LR 2.472835 Loss 19.068579, Accuracy 0.078%\n",
      "Epoch 11, Batch 191, LR 2.472898 Loss 19.068912, Accuracy 0.078%\n",
      "Epoch 11, Batch 192, LR 2.472961 Loss 19.069659, Accuracy 0.077%\n",
      "Epoch 11, Batch 193, LR 2.473024 Loss 19.069050, Accuracy 0.077%\n",
      "Epoch 11, Batch 194, LR 2.473087 Loss 19.068519, Accuracy 0.077%\n",
      "Epoch 11, Batch 195, LR 2.473150 Loss 19.068983, Accuracy 0.076%\n",
      "Epoch 11, Batch 196, LR 2.473212 Loss 19.069084, Accuracy 0.076%\n",
      "Epoch 11, Batch 197, LR 2.473275 Loss 19.069004, Accuracy 0.075%\n",
      "Epoch 11, Batch 198, LR 2.473338 Loss 19.068552, Accuracy 0.075%\n",
      "Epoch 11, Batch 199, LR 2.473400 Loss 19.067030, Accuracy 0.075%\n",
      "Epoch 11, Batch 200, LR 2.473463 Loss 19.067138, Accuracy 0.074%\n",
      "Epoch 11, Batch 201, LR 2.473525 Loss 19.067517, Accuracy 0.074%\n",
      "Epoch 11, Batch 202, LR 2.473587 Loss 19.068269, Accuracy 0.073%\n",
      "Epoch 11, Batch 203, LR 2.473650 Loss 19.067780, Accuracy 0.073%\n",
      "Epoch 11, Batch 204, LR 2.473712 Loss 19.067589, Accuracy 0.073%\n",
      "Epoch 11, Batch 205, LR 2.473774 Loss 19.067140, Accuracy 0.072%\n",
      "Epoch 11, Batch 206, LR 2.473836 Loss 19.068143, Accuracy 0.072%\n",
      "Epoch 11, Batch 207, LR 2.473898 Loss 19.067689, Accuracy 0.072%\n",
      "Epoch 11, Batch 208, LR 2.473960 Loss 19.066672, Accuracy 0.071%\n",
      "Epoch 11, Batch 209, LR 2.474021 Loss 19.067902, Accuracy 0.071%\n",
      "Epoch 11, Batch 210, LR 2.474083 Loss 19.067845, Accuracy 0.071%\n",
      "Epoch 11, Batch 211, LR 2.474145 Loss 19.068234, Accuracy 0.070%\n",
      "Epoch 11, Batch 212, LR 2.474206 Loss 19.067713, Accuracy 0.070%\n",
      "Epoch 11, Batch 213, LR 2.474268 Loss 19.067637, Accuracy 0.070%\n",
      "Epoch 11, Batch 214, LR 2.474329 Loss 19.067631, Accuracy 0.069%\n",
      "Epoch 11, Batch 215, LR 2.474391 Loss 19.067746, Accuracy 0.069%\n",
      "Epoch 11, Batch 216, LR 2.474452 Loss 19.067706, Accuracy 0.069%\n",
      "Epoch 11, Batch 217, LR 2.474513 Loss 19.067859, Accuracy 0.068%\n",
      "Epoch 11, Batch 218, LR 2.474574 Loss 19.067779, Accuracy 0.072%\n",
      "Epoch 11, Batch 219, LR 2.474636 Loss 19.067866, Accuracy 0.071%\n",
      "Epoch 11, Batch 220, LR 2.474697 Loss 19.067376, Accuracy 0.075%\n",
      "Epoch 11, Batch 221, LR 2.474758 Loss 19.067205, Accuracy 0.074%\n",
      "Epoch 11, Batch 222, LR 2.474818 Loss 19.068066, Accuracy 0.074%\n",
      "Epoch 11, Batch 223, LR 2.474879 Loss 19.068017, Accuracy 0.074%\n",
      "Epoch 11, Batch 224, LR 2.474940 Loss 19.067564, Accuracy 0.073%\n",
      "Epoch 11, Batch 225, LR 2.475001 Loss 19.067831, Accuracy 0.073%\n",
      "Epoch 11, Batch 226, LR 2.475061 Loss 19.068797, Accuracy 0.073%\n",
      "Epoch 11, Batch 227, LR 2.475122 Loss 19.068817, Accuracy 0.072%\n",
      "Epoch 11, Batch 228, LR 2.475182 Loss 19.068613, Accuracy 0.072%\n",
      "Epoch 11, Batch 229, LR 2.475242 Loss 19.068281, Accuracy 0.072%\n",
      "Epoch 11, Batch 230, LR 2.475303 Loss 19.068717, Accuracy 0.071%\n",
      "Epoch 11, Batch 231, LR 2.475363 Loss 19.069092, Accuracy 0.071%\n",
      "Epoch 11, Batch 232, LR 2.475423 Loss 19.069689, Accuracy 0.071%\n",
      "Epoch 11, Batch 233, LR 2.475483 Loss 19.070156, Accuracy 0.074%\n",
      "Epoch 11, Batch 234, LR 2.475543 Loss 19.069819, Accuracy 0.073%\n",
      "Epoch 11, Batch 235, LR 2.475603 Loss 19.069827, Accuracy 0.073%\n",
      "Epoch 11, Batch 236, LR 2.475663 Loss 19.070328, Accuracy 0.073%\n",
      "Epoch 11, Batch 237, LR 2.475723 Loss 19.070163, Accuracy 0.073%\n",
      "Epoch 11, Batch 238, LR 2.475782 Loss 19.070103, Accuracy 0.072%\n",
      "Epoch 11, Batch 239, LR 2.475842 Loss 19.070348, Accuracy 0.072%\n",
      "Epoch 11, Batch 240, LR 2.475902 Loss 19.069820, Accuracy 0.075%\n",
      "Epoch 11, Batch 241, LR 2.475961 Loss 19.070046, Accuracy 0.075%\n",
      "Epoch 11, Batch 242, LR 2.476020 Loss 19.070335, Accuracy 0.077%\n",
      "Epoch 11, Batch 243, LR 2.476080 Loss 19.070206, Accuracy 0.077%\n",
      "Epoch 11, Batch 244, LR 2.476139 Loss 19.070383, Accuracy 0.077%\n",
      "Epoch 11, Batch 245, LR 2.476198 Loss 19.070975, Accuracy 0.077%\n",
      "Epoch 11, Batch 246, LR 2.476257 Loss 19.071292, Accuracy 0.079%\n",
      "Epoch 11, Batch 247, LR 2.476316 Loss 19.071103, Accuracy 0.079%\n",
      "Epoch 11, Batch 248, LR 2.476375 Loss 19.070444, Accuracy 0.085%\n",
      "Epoch 11, Batch 249, LR 2.476434 Loss 19.070739, Accuracy 0.085%\n",
      "Epoch 11, Batch 250, LR 2.476493 Loss 19.070452, Accuracy 0.084%\n",
      "Epoch 11, Batch 251, LR 2.476552 Loss 19.070415, Accuracy 0.084%\n",
      "Epoch 11, Batch 252, LR 2.476611 Loss 19.070080, Accuracy 0.084%\n",
      "Epoch 11, Batch 253, LR 2.476669 Loss 19.070558, Accuracy 0.083%\n",
      "Epoch 11, Batch 254, LR 2.476728 Loss 19.070311, Accuracy 0.083%\n",
      "Epoch 11, Batch 255, LR 2.476786 Loss 19.069607, Accuracy 0.086%\n",
      "Epoch 11, Batch 256, LR 2.476845 Loss 19.069939, Accuracy 0.085%\n",
      "Epoch 11, Batch 257, LR 2.476903 Loss 19.069481, Accuracy 0.085%\n",
      "Epoch 11, Batch 258, LR 2.476961 Loss 19.069845, Accuracy 0.085%\n",
      "Epoch 11, Batch 259, LR 2.477019 Loss 19.069307, Accuracy 0.084%\n",
      "Epoch 11, Batch 260, LR 2.477077 Loss 19.069354, Accuracy 0.084%\n",
      "Epoch 11, Batch 261, LR 2.477135 Loss 19.068626, Accuracy 0.084%\n",
      "Epoch 11, Batch 262, LR 2.477193 Loss 19.069412, Accuracy 0.083%\n",
      "Epoch 11, Batch 263, LR 2.477251 Loss 19.069851, Accuracy 0.083%\n",
      "Epoch 11, Batch 264, LR 2.477309 Loss 19.070727, Accuracy 0.083%\n",
      "Epoch 11, Batch 265, LR 2.477367 Loss 19.070930, Accuracy 0.083%\n",
      "Epoch 11, Batch 266, LR 2.477424 Loss 19.070855, Accuracy 0.082%\n",
      "Epoch 11, Batch 267, LR 2.477482 Loss 19.071620, Accuracy 0.082%\n",
      "Epoch 11, Batch 268, LR 2.477540 Loss 19.071622, Accuracy 0.082%\n",
      "Epoch 11, Batch 269, LR 2.477597 Loss 19.070587, Accuracy 0.081%\n",
      "Epoch 11, Batch 270, LR 2.477654 Loss 19.070236, Accuracy 0.084%\n",
      "Epoch 11, Batch 271, LR 2.477712 Loss 19.070543, Accuracy 0.084%\n",
      "Epoch 11, Batch 272, LR 2.477769 Loss 19.070321, Accuracy 0.083%\n",
      "Epoch 11, Batch 273, LR 2.477826 Loss 19.070154, Accuracy 0.083%\n",
      "Epoch 11, Batch 274, LR 2.477883 Loss 19.069830, Accuracy 0.083%\n",
      "Epoch 11, Batch 275, LR 2.477940 Loss 19.069657, Accuracy 0.082%\n",
      "Epoch 11, Batch 276, LR 2.477997 Loss 19.069795, Accuracy 0.082%\n",
      "Epoch 11, Batch 277, LR 2.478054 Loss 19.069720, Accuracy 0.082%\n",
      "Epoch 11, Batch 278, LR 2.478111 Loss 19.069410, Accuracy 0.081%\n",
      "Epoch 11, Batch 279, LR 2.478167 Loss 19.069458, Accuracy 0.081%\n",
      "Epoch 11, Batch 280, LR 2.478224 Loss 19.069757, Accuracy 0.081%\n",
      "Epoch 11, Batch 281, LR 2.478281 Loss 19.069556, Accuracy 0.081%\n",
      "Epoch 11, Batch 282, LR 2.478337 Loss 19.069038, Accuracy 0.080%\n",
      "Epoch 11, Batch 283, LR 2.478394 Loss 19.069349, Accuracy 0.080%\n",
      "Epoch 11, Batch 284, LR 2.478450 Loss 19.068845, Accuracy 0.080%\n",
      "Epoch 11, Batch 285, LR 2.478506 Loss 19.068914, Accuracy 0.079%\n",
      "Epoch 11, Batch 286, LR 2.478562 Loss 19.068439, Accuracy 0.079%\n",
      "Epoch 11, Batch 287, LR 2.478619 Loss 19.067923, Accuracy 0.079%\n",
      "Epoch 11, Batch 288, LR 2.478675 Loss 19.067952, Accuracy 0.079%\n",
      "Epoch 11, Batch 289, LR 2.478731 Loss 19.067856, Accuracy 0.078%\n",
      "Epoch 11, Batch 290, LR 2.478787 Loss 19.068083, Accuracy 0.078%\n",
      "Epoch 11, Batch 291, LR 2.478842 Loss 19.068548, Accuracy 0.078%\n",
      "Epoch 11, Batch 292, LR 2.478898 Loss 19.068706, Accuracy 0.078%\n",
      "Epoch 11, Batch 293, LR 2.478954 Loss 19.068302, Accuracy 0.077%\n",
      "Epoch 11, Batch 294, LR 2.479009 Loss 19.067836, Accuracy 0.077%\n",
      "Epoch 11, Batch 295, LR 2.479065 Loss 19.067986, Accuracy 0.077%\n",
      "Epoch 11, Batch 296, LR 2.479121 Loss 19.067878, Accuracy 0.077%\n",
      "Epoch 11, Batch 297, LR 2.479176 Loss 19.068569, Accuracy 0.076%\n",
      "Epoch 11, Batch 298, LR 2.479231 Loss 19.068483, Accuracy 0.076%\n",
      "Epoch 11, Batch 299, LR 2.479287 Loss 19.068475, Accuracy 0.076%\n",
      "Epoch 11, Batch 300, LR 2.479342 Loss 19.069027, Accuracy 0.076%\n",
      "Epoch 11, Batch 301, LR 2.479397 Loss 19.068709, Accuracy 0.075%\n",
      "Epoch 11, Batch 302, LR 2.479452 Loss 19.068578, Accuracy 0.075%\n",
      "Epoch 11, Batch 303, LR 2.479507 Loss 19.068560, Accuracy 0.075%\n",
      "Epoch 11, Batch 304, LR 2.479562 Loss 19.068600, Accuracy 0.075%\n",
      "Epoch 11, Batch 305, LR 2.479617 Loss 19.068378, Accuracy 0.074%\n",
      "Epoch 11, Batch 306, LR 2.479671 Loss 19.068046, Accuracy 0.074%\n",
      "Epoch 11, Batch 307, LR 2.479726 Loss 19.067977, Accuracy 0.074%\n",
      "Epoch 11, Batch 308, LR 2.479781 Loss 19.068627, Accuracy 0.074%\n",
      "Epoch 11, Batch 309, LR 2.479835 Loss 19.068370, Accuracy 0.073%\n",
      "Epoch 11, Batch 310, LR 2.479890 Loss 19.068283, Accuracy 0.073%\n",
      "Epoch 11, Batch 311, LR 2.479944 Loss 19.068537, Accuracy 0.073%\n",
      "Epoch 11, Batch 312, LR 2.479998 Loss 19.067880, Accuracy 0.073%\n",
      "Epoch 11, Batch 313, LR 2.480052 Loss 19.068111, Accuracy 0.075%\n",
      "Epoch 11, Batch 314, LR 2.480107 Loss 19.068364, Accuracy 0.075%\n",
      "Epoch 11, Batch 315, LR 2.480161 Loss 19.068257, Accuracy 0.074%\n",
      "Epoch 11, Batch 316, LR 2.480215 Loss 19.068369, Accuracy 0.074%\n",
      "Epoch 11, Batch 317, LR 2.480269 Loss 19.068319, Accuracy 0.074%\n",
      "Epoch 11, Batch 318, LR 2.480323 Loss 19.068980, Accuracy 0.074%\n",
      "Epoch 11, Batch 319, LR 2.480376 Loss 19.068895, Accuracy 0.073%\n",
      "Epoch 11, Batch 320, LR 2.480430 Loss 19.068982, Accuracy 0.073%\n",
      "Epoch 11, Batch 321, LR 2.480484 Loss 19.068924, Accuracy 0.073%\n",
      "Epoch 11, Batch 322, LR 2.480537 Loss 19.069115, Accuracy 0.073%\n",
      "Epoch 11, Batch 323, LR 2.480591 Loss 19.068633, Accuracy 0.073%\n",
      "Epoch 11, Batch 324, LR 2.480644 Loss 19.069028, Accuracy 0.075%\n",
      "Epoch 11, Batch 325, LR 2.480698 Loss 19.068096, Accuracy 0.075%\n",
      "Epoch 11, Batch 326, LR 2.480751 Loss 19.067461, Accuracy 0.077%\n",
      "Epoch 11, Batch 327, LR 2.480804 Loss 19.067928, Accuracy 0.076%\n",
      "Epoch 11, Batch 328, LR 2.480857 Loss 19.067716, Accuracy 0.076%\n",
      "Epoch 11, Batch 329, LR 2.480910 Loss 19.067455, Accuracy 0.076%\n",
      "Epoch 11, Batch 330, LR 2.480963 Loss 19.067484, Accuracy 0.076%\n",
      "Epoch 11, Batch 331, LR 2.481016 Loss 19.066935, Accuracy 0.076%\n",
      "Epoch 11, Batch 332, LR 2.481069 Loss 19.066680, Accuracy 0.075%\n",
      "Epoch 11, Batch 333, LR 2.481122 Loss 19.067685, Accuracy 0.075%\n",
      "Epoch 11, Batch 334, LR 2.481175 Loss 19.067528, Accuracy 0.075%\n",
      "Epoch 11, Batch 335, LR 2.481227 Loss 19.067290, Accuracy 0.075%\n",
      "Epoch 11, Batch 336, LR 2.481280 Loss 19.067453, Accuracy 0.074%\n",
      "Epoch 11, Batch 337, LR 2.481332 Loss 19.067686, Accuracy 0.074%\n",
      "Epoch 11, Batch 338, LR 2.481385 Loss 19.067700, Accuracy 0.076%\n",
      "Epoch 11, Batch 339, LR 2.481437 Loss 19.067375, Accuracy 0.076%\n",
      "Epoch 11, Batch 340, LR 2.481489 Loss 19.067049, Accuracy 0.076%\n",
      "Epoch 11, Batch 341, LR 2.481541 Loss 19.066635, Accuracy 0.076%\n",
      "Epoch 11, Batch 342, LR 2.481594 Loss 19.066126, Accuracy 0.075%\n",
      "Epoch 11, Batch 343, LR 2.481646 Loss 19.066380, Accuracy 0.075%\n",
      "Epoch 11, Batch 344, LR 2.481698 Loss 19.066944, Accuracy 0.075%\n",
      "Epoch 11, Batch 345, LR 2.481749 Loss 19.067289, Accuracy 0.075%\n",
      "Epoch 11, Batch 346, LR 2.481801 Loss 19.067283, Accuracy 0.075%\n",
      "Epoch 11, Batch 347, LR 2.481853 Loss 19.066771, Accuracy 0.074%\n",
      "Epoch 11, Batch 348, LR 2.481905 Loss 19.066474, Accuracy 0.074%\n",
      "Epoch 11, Batch 349, LR 2.481956 Loss 19.066860, Accuracy 0.074%\n",
      "Epoch 11, Batch 350, LR 2.482008 Loss 19.066828, Accuracy 0.074%\n",
      "Epoch 11, Batch 351, LR 2.482059 Loss 19.066928, Accuracy 0.073%\n",
      "Epoch 11, Batch 352, LR 2.482111 Loss 19.067038, Accuracy 0.073%\n",
      "Epoch 11, Batch 353, LR 2.482162 Loss 19.066600, Accuracy 0.073%\n",
      "Epoch 11, Batch 354, LR 2.482213 Loss 19.067116, Accuracy 0.073%\n",
      "Epoch 11, Batch 355, LR 2.482264 Loss 19.066609, Accuracy 0.073%\n",
      "Epoch 11, Batch 356, LR 2.482316 Loss 19.066738, Accuracy 0.072%\n",
      "Epoch 11, Batch 357, LR 2.482367 Loss 19.067089, Accuracy 0.072%\n",
      "Epoch 11, Batch 358, LR 2.482418 Loss 19.067015, Accuracy 0.072%\n",
      "Epoch 11, Batch 359, LR 2.482468 Loss 19.067033, Accuracy 0.072%\n",
      "Epoch 11, Batch 360, LR 2.482519 Loss 19.066681, Accuracy 0.074%\n",
      "Epoch 11, Batch 361, LR 2.482570 Loss 19.066696, Accuracy 0.076%\n",
      "Epoch 11, Batch 362, LR 2.482621 Loss 19.066238, Accuracy 0.076%\n",
      "Epoch 11, Batch 363, LR 2.482671 Loss 19.066468, Accuracy 0.075%\n",
      "Epoch 11, Batch 364, LR 2.482722 Loss 19.066857, Accuracy 0.075%\n",
      "Epoch 11, Batch 365, LR 2.482772 Loss 19.066601, Accuracy 0.077%\n",
      "Epoch 11, Batch 366, LR 2.482822 Loss 19.066068, Accuracy 0.077%\n",
      "Epoch 11, Batch 367, LR 2.482873 Loss 19.066293, Accuracy 0.077%\n",
      "Epoch 11, Batch 368, LR 2.482923 Loss 19.065659, Accuracy 0.076%\n",
      "Epoch 11, Batch 369, LR 2.482973 Loss 19.066011, Accuracy 0.076%\n",
      "Epoch 11, Batch 370, LR 2.483023 Loss 19.065634, Accuracy 0.076%\n",
      "Epoch 11, Batch 371, LR 2.483073 Loss 19.066064, Accuracy 0.076%\n",
      "Epoch 11, Batch 372, LR 2.483123 Loss 19.066139, Accuracy 0.076%\n",
      "Epoch 11, Batch 373, LR 2.483173 Loss 19.065951, Accuracy 0.075%\n",
      "Epoch 11, Batch 374, LR 2.483223 Loss 19.066167, Accuracy 0.075%\n",
      "Epoch 11, Batch 375, LR 2.483272 Loss 19.065774, Accuracy 0.075%\n",
      "Epoch 11, Batch 376, LR 2.483322 Loss 19.066026, Accuracy 0.075%\n",
      "Epoch 11, Batch 377, LR 2.483372 Loss 19.066488, Accuracy 0.075%\n",
      "Epoch 11, Batch 378, LR 2.483421 Loss 19.066690, Accuracy 0.076%\n",
      "Epoch 11, Batch 379, LR 2.483471 Loss 19.066909, Accuracy 0.076%\n",
      "Epoch 11, Batch 380, LR 2.483520 Loss 19.067263, Accuracy 0.076%\n",
      "Epoch 11, Batch 381, LR 2.483569 Loss 19.067138, Accuracy 0.076%\n",
      "Epoch 11, Batch 382, LR 2.483618 Loss 19.066988, Accuracy 0.076%\n",
      "Epoch 11, Batch 383, LR 2.483667 Loss 19.066705, Accuracy 0.075%\n",
      "Epoch 11, Batch 384, LR 2.483716 Loss 19.066644, Accuracy 0.075%\n",
      "Epoch 11, Batch 385, LR 2.483765 Loss 19.065704, Accuracy 0.075%\n",
      "Epoch 11, Batch 386, LR 2.483814 Loss 19.066168, Accuracy 0.075%\n",
      "Epoch 11, Batch 387, LR 2.483863 Loss 19.066414, Accuracy 0.075%\n",
      "Epoch 11, Batch 388, LR 2.483912 Loss 19.066911, Accuracy 0.075%\n",
      "Epoch 11, Batch 389, LR 2.483961 Loss 19.067398, Accuracy 0.074%\n",
      "Epoch 11, Batch 390, LR 2.484009 Loss 19.067529, Accuracy 0.074%\n",
      "Epoch 11, Batch 391, LR 2.484058 Loss 19.067854, Accuracy 0.074%\n",
      "Epoch 11, Batch 392, LR 2.484106 Loss 19.067995, Accuracy 0.074%\n",
      "Epoch 11, Batch 393, LR 2.484155 Loss 19.068314, Accuracy 0.074%\n",
      "Epoch 11, Batch 394, LR 2.484203 Loss 19.068568, Accuracy 0.073%\n",
      "Epoch 11, Batch 395, LR 2.484251 Loss 19.068147, Accuracy 0.077%\n",
      "Epoch 11, Batch 396, LR 2.484299 Loss 19.068208, Accuracy 0.077%\n",
      "Epoch 11, Batch 397, LR 2.484347 Loss 19.068497, Accuracy 0.077%\n",
      "Epoch 11, Batch 398, LR 2.484395 Loss 19.068139, Accuracy 0.077%\n",
      "Epoch 11, Batch 399, LR 2.484443 Loss 19.068387, Accuracy 0.078%\n",
      "Epoch 11, Batch 400, LR 2.484491 Loss 19.068576, Accuracy 0.078%\n",
      "Epoch 11, Batch 401, LR 2.484539 Loss 19.069062, Accuracy 0.078%\n",
      "Epoch 11, Batch 402, LR 2.484587 Loss 19.069496, Accuracy 0.078%\n",
      "Epoch 11, Batch 403, LR 2.484634 Loss 19.069749, Accuracy 0.078%\n",
      "Epoch 11, Batch 404, LR 2.484682 Loss 19.069789, Accuracy 0.077%\n",
      "Epoch 11, Batch 405, LR 2.484729 Loss 19.070170, Accuracy 0.077%\n",
      "Epoch 11, Batch 406, LR 2.484777 Loss 19.069929, Accuracy 0.077%\n",
      "Epoch 11, Batch 407, LR 2.484824 Loss 19.069576, Accuracy 0.077%\n",
      "Epoch 11, Batch 408, LR 2.484872 Loss 19.069739, Accuracy 0.077%\n",
      "Epoch 11, Batch 409, LR 2.484919 Loss 19.070477, Accuracy 0.076%\n",
      "Epoch 11, Batch 410, LR 2.484966 Loss 19.070314, Accuracy 0.076%\n",
      "Epoch 11, Batch 411, LR 2.485013 Loss 19.070680, Accuracy 0.076%\n",
      "Epoch 11, Batch 412, LR 2.485060 Loss 19.070563, Accuracy 0.076%\n",
      "Epoch 11, Batch 413, LR 2.485107 Loss 19.070533, Accuracy 0.076%\n",
      "Epoch 11, Batch 414, LR 2.485154 Loss 19.070784, Accuracy 0.075%\n",
      "Epoch 11, Batch 415, LR 2.485201 Loss 19.071228, Accuracy 0.075%\n",
      "Epoch 11, Batch 416, LR 2.485247 Loss 19.071029, Accuracy 0.075%\n",
      "Epoch 11, Batch 417, LR 2.485294 Loss 19.071061, Accuracy 0.075%\n",
      "Epoch 11, Batch 418, LR 2.485340 Loss 19.070482, Accuracy 0.075%\n",
      "Epoch 11, Batch 419, LR 2.485387 Loss 19.070343, Accuracy 0.075%\n",
      "Epoch 11, Batch 420, LR 2.485433 Loss 19.070478, Accuracy 0.074%\n",
      "Epoch 11, Batch 421, LR 2.485480 Loss 19.070482, Accuracy 0.074%\n",
      "Epoch 11, Batch 422, LR 2.485526 Loss 19.070069, Accuracy 0.074%\n",
      "Epoch 11, Batch 423, LR 2.485572 Loss 19.070343, Accuracy 0.074%\n",
      "Epoch 11, Batch 424, LR 2.485618 Loss 19.070800, Accuracy 0.074%\n",
      "Epoch 11, Batch 425, LR 2.485664 Loss 19.070668, Accuracy 0.074%\n",
      "Epoch 11, Batch 426, LR 2.485710 Loss 19.070589, Accuracy 0.075%\n",
      "Epoch 11, Batch 427, LR 2.485756 Loss 19.070278, Accuracy 0.077%\n",
      "Epoch 11, Batch 428, LR 2.485802 Loss 19.069381, Accuracy 0.077%\n",
      "Epoch 11, Batch 429, LR 2.485848 Loss 19.069322, Accuracy 0.076%\n",
      "Epoch 11, Batch 430, LR 2.485893 Loss 19.068933, Accuracy 0.076%\n",
      "Epoch 11, Batch 431, LR 2.485939 Loss 19.069102, Accuracy 0.076%\n",
      "Epoch 11, Batch 432, LR 2.485984 Loss 19.069403, Accuracy 0.076%\n",
      "Epoch 11, Batch 433, LR 2.486030 Loss 19.068955, Accuracy 0.076%\n",
      "Epoch 11, Batch 434, LR 2.486075 Loss 19.069283, Accuracy 0.076%\n",
      "Epoch 11, Batch 435, LR 2.486121 Loss 19.069644, Accuracy 0.075%\n",
      "Epoch 11, Batch 436, LR 2.486166 Loss 19.069745, Accuracy 0.075%\n",
      "Epoch 11, Batch 437, LR 2.486211 Loss 19.069499, Accuracy 0.075%\n",
      "Epoch 11, Batch 438, LR 2.486256 Loss 19.069700, Accuracy 0.077%\n",
      "Epoch 11, Batch 439, LR 2.486301 Loss 19.069494, Accuracy 0.077%\n",
      "Epoch 11, Batch 440, LR 2.486346 Loss 19.068800, Accuracy 0.078%\n",
      "Epoch 11, Batch 441, LR 2.486391 Loss 19.069206, Accuracy 0.078%\n",
      "Epoch 11, Batch 442, LR 2.486436 Loss 19.068836, Accuracy 0.078%\n",
      "Epoch 11, Batch 443, LR 2.486480 Loss 19.068327, Accuracy 0.078%\n",
      "Epoch 11, Batch 444, LR 2.486525 Loss 19.068723, Accuracy 0.077%\n",
      "Epoch 11, Batch 445, LR 2.486570 Loss 19.068521, Accuracy 0.077%\n",
      "Epoch 11, Batch 446, LR 2.486614 Loss 19.068546, Accuracy 0.077%\n",
      "Epoch 11, Batch 447, LR 2.486659 Loss 19.068921, Accuracy 0.077%\n",
      "Epoch 11, Batch 448, LR 2.486703 Loss 19.068590, Accuracy 0.077%\n",
      "Epoch 11, Batch 449, LR 2.486747 Loss 19.068357, Accuracy 0.077%\n",
      "Epoch 11, Batch 450, LR 2.486791 Loss 19.067865, Accuracy 0.076%\n",
      "Epoch 11, Batch 451, LR 2.486835 Loss 19.067705, Accuracy 0.078%\n",
      "Epoch 11, Batch 452, LR 2.486880 Loss 19.067755, Accuracy 0.078%\n",
      "Epoch 11, Batch 453, LR 2.486924 Loss 19.067654, Accuracy 0.079%\n",
      "Epoch 11, Batch 454, LR 2.486967 Loss 19.067453, Accuracy 0.081%\n",
      "Epoch 11, Batch 455, LR 2.487011 Loss 19.067631, Accuracy 0.081%\n",
      "Epoch 11, Batch 456, LR 2.487055 Loss 19.067676, Accuracy 0.081%\n",
      "Epoch 11, Batch 457, LR 2.487099 Loss 19.067791, Accuracy 0.080%\n",
      "Epoch 11, Batch 458, LR 2.487142 Loss 19.068073, Accuracy 0.080%\n",
      "Epoch 11, Batch 459, LR 2.487186 Loss 19.068064, Accuracy 0.080%\n",
      "Epoch 11, Batch 460, LR 2.487229 Loss 19.068038, Accuracy 0.080%\n",
      "Epoch 11, Batch 461, LR 2.487273 Loss 19.067941, Accuracy 0.080%\n",
      "Epoch 11, Batch 462, LR 2.487316 Loss 19.068134, Accuracy 0.079%\n",
      "Epoch 11, Batch 463, LR 2.487359 Loss 19.068268, Accuracy 0.079%\n",
      "Epoch 11, Batch 464, LR 2.487402 Loss 19.068605, Accuracy 0.079%\n",
      "Epoch 11, Batch 465, LR 2.487446 Loss 19.068747, Accuracy 0.079%\n",
      "Epoch 11, Batch 466, LR 2.487489 Loss 19.068772, Accuracy 0.079%\n",
      "Epoch 11, Batch 467, LR 2.487532 Loss 19.068860, Accuracy 0.079%\n",
      "Epoch 11, Batch 468, LR 2.487574 Loss 19.068757, Accuracy 0.078%\n",
      "Epoch 11, Batch 469, LR 2.487617 Loss 19.068539, Accuracy 0.080%\n",
      "Epoch 11, Batch 470, LR 2.487660 Loss 19.068361, Accuracy 0.080%\n",
      "Epoch 11, Batch 471, LR 2.487703 Loss 19.068515, Accuracy 0.080%\n",
      "Epoch 11, Batch 472, LR 2.487745 Loss 19.068629, Accuracy 0.079%\n",
      "Epoch 11, Batch 473, LR 2.487788 Loss 19.068127, Accuracy 0.079%\n",
      "Epoch 11, Batch 474, LR 2.487830 Loss 19.067860, Accuracy 0.079%\n",
      "Epoch 11, Batch 475, LR 2.487873 Loss 19.067357, Accuracy 0.079%\n",
      "Epoch 11, Batch 476, LR 2.487915 Loss 19.067857, Accuracy 0.079%\n",
      "Epoch 11, Batch 477, LR 2.487957 Loss 19.068037, Accuracy 0.079%\n",
      "Epoch 11, Batch 478, LR 2.487999 Loss 19.067621, Accuracy 0.080%\n",
      "Epoch 11, Batch 479, LR 2.488041 Loss 19.067739, Accuracy 0.080%\n",
      "Epoch 11, Batch 480, LR 2.488083 Loss 19.068261, Accuracy 0.081%\n",
      "Epoch 11, Batch 481, LR 2.488125 Loss 19.068311, Accuracy 0.081%\n",
      "Epoch 11, Batch 482, LR 2.488167 Loss 19.068094, Accuracy 0.083%\n",
      "Epoch 11, Batch 483, LR 2.488209 Loss 19.067822, Accuracy 0.082%\n",
      "Epoch 11, Batch 484, LR 2.488251 Loss 19.067825, Accuracy 0.082%\n",
      "Epoch 11, Batch 485, LR 2.488292 Loss 19.068321, Accuracy 0.082%\n",
      "Epoch 11, Batch 486, LR 2.488334 Loss 19.068553, Accuracy 0.082%\n",
      "Epoch 11, Batch 487, LR 2.488375 Loss 19.068896, Accuracy 0.082%\n",
      "Epoch 11, Batch 488, LR 2.488417 Loss 19.068846, Accuracy 0.082%\n",
      "Epoch 11, Batch 489, LR 2.488458 Loss 19.068736, Accuracy 0.083%\n",
      "Epoch 11, Batch 490, LR 2.488499 Loss 19.068946, Accuracy 0.083%\n",
      "Epoch 11, Batch 491, LR 2.488540 Loss 19.069250, Accuracy 0.083%\n",
      "Epoch 11, Batch 492, LR 2.488582 Loss 19.069495, Accuracy 0.083%\n",
      "Epoch 11, Batch 493, LR 2.488623 Loss 19.069506, Accuracy 0.082%\n",
      "Epoch 11, Batch 494, LR 2.488664 Loss 19.069311, Accuracy 0.082%\n",
      "Epoch 11, Batch 495, LR 2.488705 Loss 19.069198, Accuracy 0.082%\n",
      "Epoch 11, Batch 496, LR 2.488745 Loss 19.069012, Accuracy 0.082%\n",
      "Epoch 11, Batch 497, LR 2.488786 Loss 19.069015, Accuracy 0.083%\n",
      "Epoch 11, Batch 498, LR 2.488827 Loss 19.068590, Accuracy 0.083%\n",
      "Epoch 11, Batch 499, LR 2.488867 Loss 19.068484, Accuracy 0.083%\n",
      "Epoch 11, Batch 500, LR 2.488908 Loss 19.068622, Accuracy 0.083%\n",
      "Epoch 11, Batch 501, LR 2.488948 Loss 19.068668, Accuracy 0.083%\n",
      "Epoch 11, Batch 502, LR 2.488989 Loss 19.068945, Accuracy 0.082%\n",
      "Epoch 11, Batch 503, LR 2.489029 Loss 19.068943, Accuracy 0.085%\n",
      "Epoch 11, Batch 504, LR 2.489069 Loss 19.068824, Accuracy 0.085%\n",
      "Epoch 11, Batch 505, LR 2.489109 Loss 19.068882, Accuracy 0.087%\n",
      "Epoch 11, Batch 506, LR 2.489150 Loss 19.069037, Accuracy 0.086%\n",
      "Epoch 11, Batch 507, LR 2.489190 Loss 19.068657, Accuracy 0.086%\n",
      "Epoch 11, Batch 508, LR 2.489229 Loss 19.068223, Accuracy 0.086%\n",
      "Epoch 11, Batch 509, LR 2.489269 Loss 19.068222, Accuracy 0.086%\n",
      "Epoch 11, Batch 510, LR 2.489309 Loss 19.068083, Accuracy 0.086%\n",
      "Epoch 11, Batch 511, LR 2.489349 Loss 19.068125, Accuracy 0.086%\n",
      "Epoch 11, Batch 512, LR 2.489389 Loss 19.068228, Accuracy 0.085%\n",
      "Epoch 11, Batch 513, LR 2.489428 Loss 19.068609, Accuracy 0.085%\n",
      "Epoch 11, Batch 514, LR 2.489468 Loss 19.068207, Accuracy 0.087%\n",
      "Epoch 11, Batch 515, LR 2.489507 Loss 19.068032, Accuracy 0.086%\n",
      "Epoch 11, Batch 516, LR 2.489546 Loss 19.068059, Accuracy 0.088%\n",
      "Epoch 11, Batch 517, LR 2.489586 Loss 19.068377, Accuracy 0.088%\n",
      "Epoch 11, Batch 518, LR 2.489625 Loss 19.068045, Accuracy 0.087%\n",
      "Epoch 11, Batch 519, LR 2.489664 Loss 19.068181, Accuracy 0.087%\n",
      "Epoch 11, Batch 520, LR 2.489703 Loss 19.068241, Accuracy 0.087%\n",
      "Epoch 11, Batch 521, LR 2.489742 Loss 19.068510, Accuracy 0.087%\n",
      "Epoch 11, Batch 522, LR 2.489781 Loss 19.068745, Accuracy 0.087%\n",
      "Epoch 11, Batch 523, LR 2.489820 Loss 19.068457, Accuracy 0.087%\n",
      "Epoch 11, Batch 524, LR 2.489859 Loss 19.068683, Accuracy 0.086%\n",
      "Epoch 11, Batch 525, LR 2.489897 Loss 19.068692, Accuracy 0.086%\n",
      "Epoch 11, Batch 526, LR 2.489936 Loss 19.068801, Accuracy 0.088%\n",
      "Epoch 11, Batch 527, LR 2.489974 Loss 19.068500, Accuracy 0.087%\n",
      "Epoch 11, Batch 528, LR 2.490013 Loss 19.068546, Accuracy 0.087%\n",
      "Epoch 11, Batch 529, LR 2.490051 Loss 19.068693, Accuracy 0.087%\n",
      "Epoch 11, Batch 530, LR 2.490090 Loss 19.068748, Accuracy 0.087%\n",
      "Epoch 11, Batch 531, LR 2.490128 Loss 19.068935, Accuracy 0.087%\n",
      "Epoch 11, Batch 532, LR 2.490166 Loss 19.068646, Accuracy 0.087%\n",
      "Epoch 11, Batch 533, LR 2.490204 Loss 19.068921, Accuracy 0.086%\n",
      "Epoch 11, Batch 534, LR 2.490242 Loss 19.068990, Accuracy 0.086%\n",
      "Epoch 11, Batch 535, LR 2.490280 Loss 19.068874, Accuracy 0.086%\n",
      "Epoch 11, Batch 536, LR 2.490318 Loss 19.068786, Accuracy 0.086%\n",
      "Epoch 11, Batch 537, LR 2.490356 Loss 19.069119, Accuracy 0.086%\n",
      "Epoch 11, Batch 538, LR 2.490393 Loss 19.068977, Accuracy 0.087%\n",
      "Epoch 11, Batch 539, LR 2.490431 Loss 19.068991, Accuracy 0.087%\n",
      "Epoch 11, Batch 540, LR 2.490469 Loss 19.068793, Accuracy 0.087%\n",
      "Epoch 11, Batch 541, LR 2.490506 Loss 19.068734, Accuracy 0.087%\n",
      "Epoch 11, Batch 542, LR 2.490544 Loss 19.068766, Accuracy 0.088%\n",
      "Epoch 11, Batch 543, LR 2.490581 Loss 19.068569, Accuracy 0.088%\n",
      "Epoch 11, Batch 544, LR 2.490618 Loss 19.068532, Accuracy 0.088%\n",
      "Epoch 11, Batch 545, LR 2.490656 Loss 19.068304, Accuracy 0.087%\n",
      "Epoch 11, Batch 546, LR 2.490693 Loss 19.068488, Accuracy 0.090%\n",
      "Epoch 11, Batch 547, LR 2.490730 Loss 19.068565, Accuracy 0.090%\n",
      "Epoch 11, Batch 548, LR 2.490767 Loss 19.068528, Accuracy 0.090%\n",
      "Epoch 11, Batch 549, LR 2.490804 Loss 19.068380, Accuracy 0.091%\n",
      "Epoch 11, Batch 550, LR 2.490841 Loss 19.068434, Accuracy 0.091%\n",
      "Epoch 11, Batch 551, LR 2.490877 Loss 19.068268, Accuracy 0.091%\n",
      "Epoch 11, Batch 552, LR 2.490914 Loss 19.068425, Accuracy 0.091%\n",
      "Epoch 11, Batch 553, LR 2.490951 Loss 19.068263, Accuracy 0.090%\n",
      "Epoch 11, Batch 554, LR 2.490987 Loss 19.068050, Accuracy 0.090%\n",
      "Epoch 11, Batch 555, LR 2.491024 Loss 19.067688, Accuracy 0.090%\n",
      "Epoch 11, Batch 556, LR 2.491060 Loss 19.067677, Accuracy 0.090%\n",
      "Epoch 11, Batch 557, LR 2.491096 Loss 19.068119, Accuracy 0.090%\n",
      "Epoch 11, Batch 558, LR 2.491133 Loss 19.068072, Accuracy 0.090%\n",
      "Epoch 11, Batch 559, LR 2.491169 Loss 19.067537, Accuracy 0.089%\n",
      "Epoch 11, Batch 560, LR 2.491205 Loss 19.067801, Accuracy 0.089%\n",
      "Epoch 11, Batch 561, LR 2.491241 Loss 19.067390, Accuracy 0.089%\n",
      "Epoch 11, Batch 562, LR 2.491277 Loss 19.067412, Accuracy 0.089%\n",
      "Epoch 11, Batch 563, LR 2.491313 Loss 19.067379, Accuracy 0.089%\n",
      "Epoch 11, Batch 564, LR 2.491349 Loss 19.067427, Accuracy 0.089%\n",
      "Epoch 11, Batch 565, LR 2.491384 Loss 19.067622, Accuracy 0.088%\n",
      "Epoch 11, Batch 566, LR 2.491420 Loss 19.067743, Accuracy 0.088%\n",
      "Epoch 11, Batch 567, LR 2.491456 Loss 19.067515, Accuracy 0.088%\n",
      "Epoch 11, Batch 568, LR 2.491491 Loss 19.067535, Accuracy 0.088%\n",
      "Epoch 11, Batch 569, LR 2.491527 Loss 19.067241, Accuracy 0.089%\n",
      "Epoch 11, Batch 570, LR 2.491562 Loss 19.066820, Accuracy 0.089%\n",
      "Epoch 11, Batch 571, LR 2.491597 Loss 19.066655, Accuracy 0.089%\n",
      "Epoch 11, Batch 572, LR 2.491633 Loss 19.066534, Accuracy 0.089%\n",
      "Epoch 11, Batch 573, LR 2.491668 Loss 19.066046, Accuracy 0.089%\n",
      "Epoch 11, Batch 574, LR 2.491703 Loss 19.066076, Accuracy 0.090%\n",
      "Epoch 11, Batch 575, LR 2.491738 Loss 19.065926, Accuracy 0.090%\n",
      "Epoch 11, Batch 576, LR 2.491773 Loss 19.066120, Accuracy 0.090%\n",
      "Epoch 11, Batch 577, LR 2.491808 Loss 19.066453, Accuracy 0.089%\n",
      "Epoch 11, Batch 578, LR 2.491842 Loss 19.066379, Accuracy 0.089%\n",
      "Epoch 11, Batch 579, LR 2.491877 Loss 19.066485, Accuracy 0.089%\n",
      "Epoch 11, Batch 580, LR 2.491912 Loss 19.066891, Accuracy 0.089%\n",
      "Epoch 11, Batch 581, LR 2.491946 Loss 19.066741, Accuracy 0.089%\n",
      "Epoch 11, Batch 582, LR 2.491981 Loss 19.066964, Accuracy 0.089%\n",
      "Epoch 11, Batch 583, LR 2.492015 Loss 19.066763, Accuracy 0.088%\n",
      "Epoch 11, Batch 584, LR 2.492050 Loss 19.066598, Accuracy 0.088%\n",
      "Epoch 11, Batch 585, LR 2.492084 Loss 19.066478, Accuracy 0.088%\n",
      "Epoch 11, Batch 586, LR 2.492118 Loss 19.066487, Accuracy 0.088%\n",
      "Epoch 11, Batch 587, LR 2.492152 Loss 19.066414, Accuracy 0.088%\n",
      "Epoch 11, Batch 588, LR 2.492186 Loss 19.066265, Accuracy 0.088%\n",
      "Epoch 11, Batch 589, LR 2.492220 Loss 19.066233, Accuracy 0.088%\n",
      "Epoch 11, Batch 590, LR 2.492254 Loss 19.066261, Accuracy 0.087%\n",
      "Epoch 11, Batch 591, LR 2.492288 Loss 19.066383, Accuracy 0.087%\n",
      "Epoch 11, Batch 592, LR 2.492322 Loss 19.066343, Accuracy 0.087%\n",
      "Epoch 11, Batch 593, LR 2.492355 Loss 19.066562, Accuracy 0.087%\n",
      "Epoch 11, Batch 594, LR 2.492389 Loss 19.066544, Accuracy 0.087%\n",
      "Epoch 11, Batch 595, LR 2.492422 Loss 19.066695, Accuracy 0.087%\n",
      "Epoch 11, Batch 596, LR 2.492456 Loss 19.066565, Accuracy 0.087%\n",
      "Epoch 11, Batch 597, LR 2.492489 Loss 19.066160, Accuracy 0.086%\n",
      "Epoch 11, Batch 598, LR 2.492523 Loss 19.066355, Accuracy 0.086%\n",
      "Epoch 11, Batch 599, LR 2.492556 Loss 19.066503, Accuracy 0.086%\n",
      "Epoch 11, Batch 600, LR 2.492589 Loss 19.066392, Accuracy 0.086%\n",
      "Epoch 11, Batch 601, LR 2.492622 Loss 19.066237, Accuracy 0.086%\n",
      "Epoch 11, Batch 602, LR 2.492655 Loss 19.066252, Accuracy 0.086%\n",
      "Epoch 11, Batch 603, LR 2.492688 Loss 19.066017, Accuracy 0.087%\n",
      "Epoch 11, Batch 604, LR 2.492721 Loss 19.065968, Accuracy 0.087%\n",
      "Epoch 11, Batch 605, LR 2.492754 Loss 19.065729, Accuracy 0.087%\n",
      "Epoch 11, Batch 606, LR 2.492786 Loss 19.065606, Accuracy 0.086%\n",
      "Epoch 11, Batch 607, LR 2.492819 Loss 19.065700, Accuracy 0.086%\n",
      "Epoch 11, Batch 608, LR 2.492852 Loss 19.065496, Accuracy 0.086%\n",
      "Epoch 11, Batch 609, LR 2.492884 Loss 19.065584, Accuracy 0.086%\n",
      "Epoch 11, Batch 610, LR 2.492917 Loss 19.065659, Accuracy 0.086%\n",
      "Epoch 11, Batch 611, LR 2.492949 Loss 19.065550, Accuracy 0.086%\n",
      "Epoch 11, Batch 612, LR 2.492981 Loss 19.065281, Accuracy 0.087%\n",
      "Epoch 11, Batch 613, LR 2.493013 Loss 19.065195, Accuracy 0.087%\n",
      "Epoch 11, Batch 614, LR 2.493045 Loss 19.065301, Accuracy 0.087%\n",
      "Epoch 11, Batch 615, LR 2.493078 Loss 19.065284, Accuracy 0.086%\n",
      "Epoch 11, Batch 616, LR 2.493109 Loss 19.065281, Accuracy 0.086%\n",
      "Epoch 11, Batch 617, LR 2.493141 Loss 19.065303, Accuracy 0.086%\n",
      "Epoch 11, Batch 618, LR 2.493173 Loss 19.065305, Accuracy 0.086%\n",
      "Epoch 11, Batch 619, LR 2.493205 Loss 19.065358, Accuracy 0.086%\n",
      "Epoch 11, Batch 620, LR 2.493237 Loss 19.065409, Accuracy 0.086%\n",
      "Epoch 11, Batch 621, LR 2.493268 Loss 19.065388, Accuracy 0.086%\n",
      "Epoch 11, Batch 622, LR 2.493300 Loss 19.065753, Accuracy 0.085%\n",
      "Epoch 11, Batch 623, LR 2.493331 Loss 19.065388, Accuracy 0.085%\n",
      "Epoch 11, Batch 624, LR 2.493363 Loss 19.065387, Accuracy 0.085%\n",
      "Epoch 11, Batch 625, LR 2.493394 Loss 19.064992, Accuracy 0.085%\n",
      "Epoch 11, Batch 626, LR 2.493425 Loss 19.064898, Accuracy 0.085%\n",
      "Epoch 11, Batch 627, LR 2.493456 Loss 19.064846, Accuracy 0.085%\n",
      "Epoch 11, Batch 628, LR 2.493488 Loss 19.064938, Accuracy 0.085%\n",
      "Epoch 11, Batch 629, LR 2.493519 Loss 19.064832, Accuracy 0.084%\n",
      "Epoch 11, Batch 630, LR 2.493549 Loss 19.064675, Accuracy 0.084%\n",
      "Epoch 11, Batch 631, LR 2.493580 Loss 19.064772, Accuracy 0.085%\n",
      "Epoch 11, Batch 632, LR 2.493611 Loss 19.064725, Accuracy 0.085%\n",
      "Epoch 11, Batch 633, LR 2.493642 Loss 19.064695, Accuracy 0.085%\n",
      "Epoch 11, Batch 634, LR 2.493673 Loss 19.064756, Accuracy 0.085%\n",
      "Epoch 11, Batch 635, LR 2.493703 Loss 19.064729, Accuracy 0.085%\n",
      "Epoch 11, Batch 636, LR 2.493734 Loss 19.064782, Accuracy 0.085%\n",
      "Epoch 11, Batch 637, LR 2.493764 Loss 19.064928, Accuracy 0.086%\n",
      "Epoch 11, Batch 638, LR 2.493794 Loss 19.065007, Accuracy 0.086%\n",
      "Epoch 11, Batch 639, LR 2.493825 Loss 19.065106, Accuracy 0.086%\n",
      "Epoch 11, Batch 640, LR 2.493855 Loss 19.065206, Accuracy 0.085%\n",
      "Epoch 11, Batch 641, LR 2.493885 Loss 19.065164, Accuracy 0.087%\n",
      "Epoch 11, Batch 642, LR 2.493915 Loss 19.065013, Accuracy 0.086%\n",
      "Epoch 11, Batch 643, LR 2.493945 Loss 19.064819, Accuracy 0.086%\n",
      "Epoch 11, Batch 644, LR 2.493975 Loss 19.064820, Accuracy 0.086%\n",
      "Epoch 11, Batch 645, LR 2.494005 Loss 19.064951, Accuracy 0.086%\n",
      "Epoch 11, Batch 646, LR 2.494035 Loss 19.065150, Accuracy 0.086%\n",
      "Epoch 11, Batch 647, LR 2.494064 Loss 19.065429, Accuracy 0.086%\n",
      "Epoch 11, Batch 648, LR 2.494094 Loss 19.065171, Accuracy 0.086%\n",
      "Epoch 11, Batch 649, LR 2.494123 Loss 19.065227, Accuracy 0.085%\n",
      "Epoch 11, Batch 650, LR 2.494153 Loss 19.065146, Accuracy 0.085%\n",
      "Epoch 11, Batch 651, LR 2.494182 Loss 19.065256, Accuracy 0.085%\n",
      "Epoch 11, Batch 652, LR 2.494212 Loss 19.065227, Accuracy 0.085%\n",
      "Epoch 11, Batch 653, LR 2.494241 Loss 19.065393, Accuracy 0.085%\n",
      "Epoch 11, Batch 654, LR 2.494270 Loss 19.065383, Accuracy 0.085%\n",
      "Epoch 11, Batch 655, LR 2.494299 Loss 19.065302, Accuracy 0.085%\n",
      "Epoch 11, Batch 656, LR 2.494328 Loss 19.065184, Accuracy 0.085%\n",
      "Epoch 11, Batch 657, LR 2.494357 Loss 19.064926, Accuracy 0.084%\n",
      "Epoch 11, Batch 658, LR 2.494386 Loss 19.064913, Accuracy 0.084%\n",
      "Epoch 11, Batch 659, LR 2.494415 Loss 19.065084, Accuracy 0.084%\n",
      "Epoch 11, Batch 660, LR 2.494444 Loss 19.065033, Accuracy 0.084%\n",
      "Epoch 11, Batch 661, LR 2.494472 Loss 19.065033, Accuracy 0.084%\n",
      "Epoch 11, Batch 662, LR 2.494501 Loss 19.064906, Accuracy 0.085%\n",
      "Epoch 11, Batch 663, LR 2.494529 Loss 19.065017, Accuracy 0.085%\n",
      "Epoch 11, Batch 664, LR 2.494558 Loss 19.065001, Accuracy 0.085%\n",
      "Epoch 11, Batch 665, LR 2.494586 Loss 19.064836, Accuracy 0.085%\n",
      "Epoch 11, Batch 666, LR 2.494614 Loss 19.065138, Accuracy 0.086%\n",
      "Epoch 11, Batch 667, LR 2.494643 Loss 19.065291, Accuracy 0.086%\n",
      "Epoch 11, Batch 668, LR 2.494671 Loss 19.065318, Accuracy 0.085%\n",
      "Epoch 11, Batch 669, LR 2.494699 Loss 19.065389, Accuracy 0.085%\n",
      "Epoch 11, Batch 670, LR 2.494727 Loss 19.065586, Accuracy 0.085%\n",
      "Epoch 11, Batch 671, LR 2.494755 Loss 19.065740, Accuracy 0.085%\n",
      "Epoch 11, Batch 672, LR 2.494783 Loss 19.065642, Accuracy 0.085%\n",
      "Epoch 11, Batch 673, LR 2.494810 Loss 19.065589, Accuracy 0.085%\n",
      "Epoch 11, Batch 674, LR 2.494838 Loss 19.065558, Accuracy 0.085%\n",
      "Epoch 11, Batch 675, LR 2.494866 Loss 19.065585, Accuracy 0.084%\n",
      "Epoch 11, Batch 676, LR 2.494893 Loss 19.065884, Accuracy 0.084%\n",
      "Epoch 11, Batch 677, LR 2.494921 Loss 19.065879, Accuracy 0.084%\n",
      "Epoch 11, Batch 678, LR 2.494948 Loss 19.065743, Accuracy 0.084%\n",
      "Epoch 11, Batch 679, LR 2.494975 Loss 19.065948, Accuracy 0.084%\n",
      "Epoch 11, Batch 680, LR 2.495003 Loss 19.065859, Accuracy 0.085%\n",
      "Epoch 11, Batch 681, LR 2.495030 Loss 19.065812, Accuracy 0.085%\n",
      "Epoch 11, Batch 682, LR 2.495057 Loss 19.065839, Accuracy 0.085%\n",
      "Epoch 11, Batch 683, LR 2.495084 Loss 19.065565, Accuracy 0.085%\n",
      "Epoch 11, Batch 684, LR 2.495111 Loss 19.065463, Accuracy 0.085%\n",
      "Epoch 11, Batch 685, LR 2.495138 Loss 19.065374, Accuracy 0.084%\n",
      "Epoch 11, Batch 686, LR 2.495165 Loss 19.065325, Accuracy 0.084%\n",
      "Epoch 11, Batch 687, LR 2.495191 Loss 19.065429, Accuracy 0.084%\n",
      "Epoch 11, Batch 688, LR 2.495218 Loss 19.065437, Accuracy 0.084%\n",
      "Epoch 11, Batch 689, LR 2.495245 Loss 19.065553, Accuracy 0.084%\n",
      "Epoch 11, Batch 690, LR 2.495271 Loss 19.065423, Accuracy 0.084%\n",
      "Epoch 11, Batch 691, LR 2.495297 Loss 19.065369, Accuracy 0.084%\n",
      "Epoch 11, Batch 692, LR 2.495324 Loss 19.065404, Accuracy 0.084%\n",
      "Epoch 11, Batch 693, LR 2.495350 Loss 19.065452, Accuracy 0.083%\n",
      "Epoch 11, Batch 694, LR 2.495376 Loss 19.065550, Accuracy 0.083%\n",
      "Epoch 11, Batch 695, LR 2.495403 Loss 19.065517, Accuracy 0.083%\n",
      "Epoch 11, Batch 696, LR 2.495429 Loss 19.065680, Accuracy 0.083%\n",
      "Epoch 11, Batch 697, LR 2.495455 Loss 19.065832, Accuracy 0.083%\n",
      "Epoch 11, Batch 698, LR 2.495480 Loss 19.065765, Accuracy 0.083%\n",
      "Epoch 11, Batch 699, LR 2.495506 Loss 19.065729, Accuracy 0.083%\n",
      "Epoch 11, Batch 700, LR 2.495532 Loss 19.065509, Accuracy 0.083%\n",
      "Epoch 11, Batch 701, LR 2.495558 Loss 19.065659, Accuracy 0.082%\n",
      "Epoch 11, Batch 702, LR 2.495583 Loss 19.065520, Accuracy 0.082%\n",
      "Epoch 11, Batch 703, LR 2.495609 Loss 19.065585, Accuracy 0.082%\n",
      "Epoch 11, Batch 704, LR 2.495634 Loss 19.065504, Accuracy 0.083%\n",
      "Epoch 11, Batch 705, LR 2.495660 Loss 19.065406, Accuracy 0.083%\n",
      "Epoch 11, Batch 706, LR 2.495685 Loss 19.065299, Accuracy 0.083%\n",
      "Epoch 11, Batch 707, LR 2.495710 Loss 19.065279, Accuracy 0.083%\n",
      "Epoch 11, Batch 708, LR 2.495736 Loss 19.065133, Accuracy 0.083%\n",
      "Epoch 11, Batch 709, LR 2.495761 Loss 19.065400, Accuracy 0.083%\n",
      "Epoch 11, Batch 710, LR 2.495786 Loss 19.065567, Accuracy 0.083%\n",
      "Epoch 11, Batch 711, LR 2.495811 Loss 19.065594, Accuracy 0.082%\n",
      "Epoch 11, Batch 712, LR 2.495836 Loss 19.065740, Accuracy 0.082%\n",
      "Epoch 11, Batch 713, LR 2.495860 Loss 19.065665, Accuracy 0.085%\n",
      "Epoch 11, Batch 714, LR 2.495885 Loss 19.065630, Accuracy 0.085%\n",
      "Epoch 11, Batch 715, LR 2.495910 Loss 19.065674, Accuracy 0.085%\n",
      "Epoch 11, Batch 716, LR 2.495934 Loss 19.065627, Accuracy 0.085%\n",
      "Epoch 11, Batch 717, LR 2.495959 Loss 19.065616, Accuracy 0.085%\n",
      "Epoch 11, Batch 718, LR 2.495983 Loss 19.065712, Accuracy 0.085%\n",
      "Epoch 11, Batch 719, LR 2.496008 Loss 19.065758, Accuracy 0.085%\n",
      "Epoch 11, Batch 720, LR 2.496032 Loss 19.065698, Accuracy 0.085%\n",
      "Epoch 11, Batch 721, LR 2.496056 Loss 19.065726, Accuracy 0.085%\n",
      "Epoch 11, Batch 722, LR 2.496080 Loss 19.065618, Accuracy 0.084%\n",
      "Epoch 11, Batch 723, LR 2.496104 Loss 19.065743, Accuracy 0.084%\n",
      "Epoch 11, Batch 724, LR 2.496128 Loss 19.065642, Accuracy 0.084%\n",
      "Epoch 11, Batch 725, LR 2.496152 Loss 19.065335, Accuracy 0.084%\n",
      "Epoch 11, Batch 726, LR 2.496176 Loss 19.065114, Accuracy 0.084%\n",
      "Epoch 11, Batch 727, LR 2.496200 Loss 19.065143, Accuracy 0.084%\n",
      "Epoch 11, Batch 728, LR 2.496224 Loss 19.065100, Accuracy 0.084%\n",
      "Epoch 11, Batch 729, LR 2.496247 Loss 19.065077, Accuracy 0.084%\n",
      "Epoch 11, Batch 730, LR 2.496271 Loss 19.064932, Accuracy 0.083%\n",
      "Epoch 11, Batch 731, LR 2.496294 Loss 19.064935, Accuracy 0.083%\n",
      "Epoch 11, Batch 732, LR 2.496318 Loss 19.064833, Accuracy 0.083%\n",
      "Epoch 11, Batch 733, LR 2.496341 Loss 19.064936, Accuracy 0.083%\n",
      "Epoch 11, Batch 734, LR 2.496364 Loss 19.065318, Accuracy 0.083%\n",
      "Epoch 11, Batch 735, LR 2.496388 Loss 19.065011, Accuracy 0.083%\n",
      "Epoch 11, Batch 736, LR 2.496411 Loss 19.065038, Accuracy 0.083%\n",
      "Epoch 11, Batch 737, LR 2.496434 Loss 19.064827, Accuracy 0.085%\n",
      "Epoch 11, Batch 738, LR 2.496457 Loss 19.064823, Accuracy 0.085%\n",
      "Epoch 11, Batch 739, LR 2.496480 Loss 19.064738, Accuracy 0.086%\n",
      "Epoch 11, Batch 740, LR 2.496502 Loss 19.065214, Accuracy 0.086%\n",
      "Epoch 11, Batch 741, LR 2.496525 Loss 19.065370, Accuracy 0.085%\n",
      "Epoch 11, Batch 742, LR 2.496548 Loss 19.065524, Accuracy 0.085%\n",
      "Epoch 11, Batch 743, LR 2.496570 Loss 19.065750, Accuracy 0.085%\n",
      "Epoch 11, Batch 744, LR 2.496593 Loss 19.065713, Accuracy 0.085%\n",
      "Epoch 11, Batch 745, LR 2.496615 Loss 19.065852, Accuracy 0.085%\n",
      "Epoch 11, Batch 746, LR 2.496638 Loss 19.065648, Accuracy 0.085%\n",
      "Epoch 11, Batch 747, LR 2.496660 Loss 19.065454, Accuracy 0.085%\n",
      "Epoch 11, Batch 748, LR 2.496682 Loss 19.065383, Accuracy 0.085%\n",
      "Epoch 11, Batch 749, LR 2.496704 Loss 19.065606, Accuracy 0.084%\n",
      "Epoch 11, Batch 750, LR 2.496726 Loss 19.065751, Accuracy 0.085%\n",
      "Epoch 11, Batch 751, LR 2.496748 Loss 19.065538, Accuracy 0.085%\n",
      "Epoch 11, Batch 752, LR 2.496770 Loss 19.065579, Accuracy 0.085%\n",
      "Epoch 11, Batch 753, LR 2.496792 Loss 19.065631, Accuracy 0.085%\n",
      "Epoch 11, Batch 754, LR 2.496814 Loss 19.065471, Accuracy 0.085%\n",
      "Epoch 11, Batch 755, LR 2.496836 Loss 19.065435, Accuracy 0.085%\n",
      "Epoch 11, Batch 756, LR 2.496857 Loss 19.065300, Accuracy 0.085%\n",
      "Epoch 11, Batch 757, LR 2.496879 Loss 19.065320, Accuracy 0.085%\n",
      "Epoch 11, Batch 758, LR 2.496900 Loss 19.064925, Accuracy 0.086%\n",
      "Epoch 11, Batch 759, LR 2.496922 Loss 19.064866, Accuracy 0.085%\n",
      "Epoch 11, Batch 760, LR 2.496943 Loss 19.064912, Accuracy 0.085%\n",
      "Epoch 11, Batch 761, LR 2.496964 Loss 19.064890, Accuracy 0.085%\n",
      "Epoch 11, Batch 762, LR 2.496985 Loss 19.064898, Accuracy 0.085%\n",
      "Epoch 11, Batch 763, LR 2.497007 Loss 19.065015, Accuracy 0.085%\n",
      "Epoch 11, Batch 764, LR 2.497028 Loss 19.065252, Accuracy 0.085%\n",
      "Epoch 11, Batch 765, LR 2.497049 Loss 19.065493, Accuracy 0.085%\n",
      "Epoch 11, Batch 766, LR 2.497069 Loss 19.065188, Accuracy 0.085%\n",
      "Epoch 11, Batch 767, LR 2.497090 Loss 19.065355, Accuracy 0.085%\n",
      "Epoch 11, Batch 768, LR 2.497111 Loss 19.065501, Accuracy 0.084%\n",
      "Epoch 11, Batch 769, LR 2.497132 Loss 19.065706, Accuracy 0.084%\n",
      "Epoch 11, Batch 770, LR 2.497152 Loss 19.065667, Accuracy 0.084%\n",
      "Epoch 11, Batch 771, LR 2.497173 Loss 19.065639, Accuracy 0.084%\n",
      "Epoch 11, Batch 772, LR 2.497193 Loss 19.065841, Accuracy 0.084%\n",
      "Epoch 11, Batch 773, LR 2.497214 Loss 19.065793, Accuracy 0.084%\n",
      "Epoch 11, Batch 774, LR 2.497234 Loss 19.065709, Accuracy 0.084%\n",
      "Epoch 11, Batch 775, LR 2.497254 Loss 19.065657, Accuracy 0.084%\n",
      "Epoch 11, Batch 776, LR 2.497274 Loss 19.065514, Accuracy 0.084%\n",
      "Epoch 11, Batch 777, LR 2.497294 Loss 19.065457, Accuracy 0.083%\n",
      "Epoch 11, Batch 778, LR 2.497314 Loss 19.065360, Accuracy 0.083%\n",
      "Epoch 11, Batch 779, LR 2.497334 Loss 19.065238, Accuracy 0.083%\n",
      "Epoch 11, Batch 780, LR 2.497354 Loss 19.065322, Accuracy 0.083%\n",
      "Epoch 11, Batch 781, LR 2.497374 Loss 19.064970, Accuracy 0.083%\n",
      "Epoch 11, Batch 782, LR 2.497394 Loss 19.065139, Accuracy 0.083%\n",
      "Epoch 11, Batch 783, LR 2.497413 Loss 19.065145, Accuracy 0.083%\n",
      "Epoch 11, Batch 784, LR 2.497433 Loss 19.064915, Accuracy 0.084%\n",
      "Epoch 11, Batch 785, LR 2.497452 Loss 19.064774, Accuracy 0.084%\n",
      "Epoch 11, Batch 786, LR 2.497472 Loss 19.064629, Accuracy 0.083%\n",
      "Epoch 11, Batch 787, LR 2.497491 Loss 19.064401, Accuracy 0.084%\n",
      "Epoch 11, Batch 788, LR 2.497510 Loss 19.064231, Accuracy 0.085%\n",
      "Epoch 11, Batch 789, LR 2.497529 Loss 19.064277, Accuracy 0.085%\n",
      "Epoch 11, Batch 790, LR 2.497548 Loss 19.064021, Accuracy 0.085%\n",
      "Epoch 11, Batch 791, LR 2.497568 Loss 19.064078, Accuracy 0.086%\n",
      "Epoch 11, Batch 792, LR 2.497586 Loss 19.063956, Accuracy 0.086%\n",
      "Epoch 11, Batch 793, LR 2.497605 Loss 19.064005, Accuracy 0.087%\n",
      "Epoch 11, Batch 794, LR 2.497624 Loss 19.063901, Accuracy 0.087%\n",
      "Epoch 11, Batch 795, LR 2.497643 Loss 19.063844, Accuracy 0.086%\n",
      "Epoch 11, Batch 796, LR 2.497662 Loss 19.063956, Accuracy 0.086%\n",
      "Epoch 11, Batch 797, LR 2.497680 Loss 19.063944, Accuracy 0.086%\n",
      "Epoch 11, Batch 798, LR 2.497699 Loss 19.064078, Accuracy 0.087%\n",
      "Epoch 11, Batch 799, LR 2.497717 Loss 19.064073, Accuracy 0.087%\n",
      "Epoch 11, Batch 800, LR 2.497735 Loss 19.063866, Accuracy 0.087%\n",
      "Epoch 11, Batch 801, LR 2.497754 Loss 19.063720, Accuracy 0.087%\n",
      "Epoch 11, Batch 802, LR 2.497772 Loss 19.063719, Accuracy 0.087%\n",
      "Epoch 11, Batch 803, LR 2.497790 Loss 19.063882, Accuracy 0.087%\n",
      "Epoch 11, Batch 804, LR 2.497808 Loss 19.063770, Accuracy 0.086%\n",
      "Epoch 11, Batch 805, LR 2.497826 Loss 19.063861, Accuracy 0.086%\n",
      "Epoch 11, Batch 806, LR 2.497844 Loss 19.063912, Accuracy 0.086%\n",
      "Epoch 11, Batch 807, LR 2.497862 Loss 19.063733, Accuracy 0.086%\n",
      "Epoch 11, Batch 808, LR 2.497880 Loss 19.064013, Accuracy 0.086%\n",
      "Epoch 11, Batch 809, LR 2.497897 Loss 19.064176, Accuracy 0.086%\n",
      "Epoch 11, Batch 810, LR 2.497915 Loss 19.064099, Accuracy 0.086%\n",
      "Epoch 11, Batch 811, LR 2.497933 Loss 19.064080, Accuracy 0.086%\n",
      "Epoch 11, Batch 812, LR 2.497950 Loss 19.064050, Accuracy 0.086%\n",
      "Epoch 11, Batch 813, LR 2.497968 Loss 19.063794, Accuracy 0.086%\n",
      "Epoch 11, Batch 814, LR 2.497985 Loss 19.063793, Accuracy 0.085%\n",
      "Epoch 11, Batch 815, LR 2.498002 Loss 19.064022, Accuracy 0.085%\n",
      "Epoch 11, Batch 816, LR 2.498019 Loss 19.063806, Accuracy 0.085%\n",
      "Epoch 11, Batch 817, LR 2.498036 Loss 19.063461, Accuracy 0.085%\n",
      "Epoch 11, Batch 818, LR 2.498053 Loss 19.063484, Accuracy 0.085%\n",
      "Epoch 11, Batch 819, LR 2.498070 Loss 19.063420, Accuracy 0.085%\n",
      "Epoch 11, Batch 820, LR 2.498087 Loss 19.063583, Accuracy 0.085%\n",
      "Epoch 11, Batch 821, LR 2.498104 Loss 19.063408, Accuracy 0.085%\n",
      "Epoch 11, Batch 822, LR 2.498121 Loss 19.063363, Accuracy 0.085%\n",
      "Epoch 11, Batch 823, LR 2.498137 Loss 19.063528, Accuracy 0.084%\n",
      "Epoch 11, Batch 824, LR 2.498154 Loss 19.063500, Accuracy 0.084%\n",
      "Epoch 11, Batch 825, LR 2.498171 Loss 19.063506, Accuracy 0.085%\n",
      "Epoch 11, Batch 826, LR 2.498187 Loss 19.063342, Accuracy 0.085%\n",
      "Epoch 11, Batch 827, LR 2.498203 Loss 19.063190, Accuracy 0.085%\n",
      "Epoch 11, Batch 828, LR 2.498220 Loss 19.063069, Accuracy 0.085%\n",
      "Epoch 11, Batch 829, LR 2.498236 Loss 19.063098, Accuracy 0.085%\n",
      "Epoch 11, Batch 830, LR 2.498252 Loss 19.063167, Accuracy 0.085%\n",
      "Epoch 11, Batch 831, LR 2.498268 Loss 19.063373, Accuracy 0.085%\n",
      "Epoch 11, Batch 832, LR 2.498284 Loss 19.063382, Accuracy 0.085%\n",
      "Epoch 11, Batch 833, LR 2.498300 Loss 19.063514, Accuracy 0.084%\n",
      "Epoch 11, Batch 834, LR 2.498316 Loss 19.063399, Accuracy 0.084%\n",
      "Epoch 11, Batch 835, LR 2.498332 Loss 19.063204, Accuracy 0.084%\n",
      "Epoch 11, Batch 836, LR 2.498347 Loss 19.063415, Accuracy 0.084%\n",
      "Epoch 11, Batch 837, LR 2.498363 Loss 19.063640, Accuracy 0.085%\n",
      "Epoch 11, Batch 838, LR 2.498379 Loss 19.063578, Accuracy 0.085%\n",
      "Epoch 11, Batch 839, LR 2.498394 Loss 19.063643, Accuracy 0.085%\n",
      "Epoch 11, Batch 840, LR 2.498409 Loss 19.063944, Accuracy 0.085%\n",
      "Epoch 11, Batch 841, LR 2.498425 Loss 19.063750, Accuracy 0.085%\n",
      "Epoch 11, Batch 842, LR 2.498440 Loss 19.063722, Accuracy 0.084%\n",
      "Epoch 11, Batch 843, LR 2.498455 Loss 19.063955, Accuracy 0.084%\n",
      "Epoch 11, Batch 844, LR 2.498470 Loss 19.063710, Accuracy 0.084%\n",
      "Epoch 11, Batch 845, LR 2.498485 Loss 19.063878, Accuracy 0.084%\n",
      "Epoch 11, Batch 846, LR 2.498500 Loss 19.063909, Accuracy 0.084%\n",
      "Epoch 11, Batch 847, LR 2.498515 Loss 19.063841, Accuracy 0.084%\n",
      "Epoch 11, Batch 848, LR 2.498530 Loss 19.063756, Accuracy 0.084%\n",
      "Epoch 11, Batch 849, LR 2.498545 Loss 19.063697, Accuracy 0.085%\n",
      "Epoch 11, Batch 850, LR 2.498559 Loss 19.063650, Accuracy 0.085%\n",
      "Epoch 11, Batch 851, LR 2.498574 Loss 19.063565, Accuracy 0.085%\n",
      "Epoch 11, Batch 852, LR 2.498588 Loss 19.063386, Accuracy 0.085%\n",
      "Epoch 11, Batch 853, LR 2.498603 Loss 19.063411, Accuracy 0.085%\n",
      "Epoch 11, Batch 854, LR 2.498617 Loss 19.063461, Accuracy 0.086%\n",
      "Epoch 11, Batch 855, LR 2.498632 Loss 19.063519, Accuracy 0.086%\n",
      "Epoch 11, Batch 856, LR 2.498646 Loss 19.063564, Accuracy 0.086%\n",
      "Epoch 11, Batch 857, LR 2.498660 Loss 19.063435, Accuracy 0.086%\n",
      "Epoch 11, Batch 858, LR 2.498674 Loss 19.063303, Accuracy 0.086%\n",
      "Epoch 11, Batch 859, LR 2.498688 Loss 19.063221, Accuracy 0.085%\n",
      "Epoch 11, Batch 860, LR 2.498702 Loss 19.063294, Accuracy 0.085%\n",
      "Epoch 11, Batch 861, LR 2.498716 Loss 19.063183, Accuracy 0.085%\n",
      "Epoch 11, Batch 862, LR 2.498729 Loss 19.062962, Accuracy 0.085%\n",
      "Epoch 11, Batch 863, LR 2.498743 Loss 19.062906, Accuracy 0.085%\n",
      "Epoch 11, Batch 864, LR 2.498757 Loss 19.063051, Accuracy 0.085%\n",
      "Epoch 11, Batch 865, LR 2.498770 Loss 19.062729, Accuracy 0.085%\n",
      "Epoch 11, Batch 866, LR 2.498784 Loss 19.062605, Accuracy 0.085%\n",
      "Epoch 11, Batch 867, LR 2.498797 Loss 19.062520, Accuracy 0.085%\n",
      "Epoch 11, Batch 868, LR 2.498811 Loss 19.062447, Accuracy 0.085%\n",
      "Epoch 11, Batch 869, LR 2.498824 Loss 19.062332, Accuracy 0.085%\n",
      "Epoch 11, Batch 870, LR 2.498837 Loss 19.062060, Accuracy 0.084%\n",
      "Epoch 11, Batch 871, LR 2.498850 Loss 19.061957, Accuracy 0.084%\n",
      "Epoch 11, Batch 872, LR 2.498863 Loss 19.062129, Accuracy 0.084%\n",
      "Epoch 11, Batch 873, LR 2.498876 Loss 19.062131, Accuracy 0.084%\n",
      "Epoch 11, Batch 874, LR 2.498889 Loss 19.062095, Accuracy 0.084%\n",
      "Epoch 11, Batch 875, LR 2.498902 Loss 19.062057, Accuracy 0.084%\n",
      "Epoch 11, Batch 876, LR 2.498914 Loss 19.062225, Accuracy 0.084%\n",
      "Epoch 11, Batch 877, LR 2.498927 Loss 19.062273, Accuracy 0.084%\n",
      "Epoch 11, Batch 878, LR 2.498940 Loss 19.062461, Accuracy 0.084%\n",
      "Epoch 11, Batch 879, LR 2.498952 Loss 19.062365, Accuracy 0.084%\n",
      "Epoch 11, Batch 880, LR 2.498965 Loss 19.062123, Accuracy 0.083%\n",
      "Epoch 11, Batch 881, LR 2.498977 Loss 19.062357, Accuracy 0.083%\n",
      "Epoch 11, Batch 882, LR 2.498989 Loss 19.062153, Accuracy 0.083%\n",
      "Epoch 11, Batch 883, LR 2.499002 Loss 19.062339, Accuracy 0.083%\n",
      "Epoch 11, Batch 884, LR 2.499014 Loss 19.062199, Accuracy 0.083%\n",
      "Epoch 11, Batch 885, LR 2.499026 Loss 19.062075, Accuracy 0.083%\n",
      "Epoch 11, Batch 886, LR 2.499038 Loss 19.062092, Accuracy 0.083%\n",
      "Epoch 11, Batch 887, LR 2.499050 Loss 19.061950, Accuracy 0.083%\n",
      "Epoch 11, Batch 888, LR 2.499061 Loss 19.061993, Accuracy 0.083%\n",
      "Epoch 11, Batch 889, LR 2.499073 Loss 19.061867, Accuracy 0.083%\n",
      "Epoch 11, Batch 890, LR 2.499085 Loss 19.062072, Accuracy 0.083%\n",
      "Epoch 11, Batch 891, LR 2.499097 Loss 19.062249, Accuracy 0.082%\n",
      "Epoch 11, Batch 892, LR 2.499108 Loss 19.062280, Accuracy 0.082%\n",
      "Epoch 11, Batch 893, LR 2.499120 Loss 19.062312, Accuracy 0.082%\n",
      "Epoch 11, Batch 894, LR 2.499131 Loss 19.062130, Accuracy 0.082%\n",
      "Epoch 11, Batch 895, LR 2.499142 Loss 19.062011, Accuracy 0.083%\n",
      "Epoch 11, Batch 896, LR 2.499154 Loss 19.061881, Accuracy 0.083%\n",
      "Epoch 11, Batch 897, LR 2.499165 Loss 19.061856, Accuracy 0.083%\n",
      "Epoch 11, Batch 898, LR 2.499176 Loss 19.061772, Accuracy 0.083%\n",
      "Epoch 11, Batch 899, LR 2.499187 Loss 19.061725, Accuracy 0.083%\n",
      "Epoch 11, Batch 900, LR 2.499198 Loss 19.061553, Accuracy 0.082%\n",
      "Epoch 11, Batch 901, LR 2.499209 Loss 19.061609, Accuracy 0.082%\n",
      "Epoch 11, Batch 902, LR 2.499219 Loss 19.061654, Accuracy 0.082%\n",
      "Epoch 11, Batch 903, LR 2.499230 Loss 19.061600, Accuracy 0.082%\n",
      "Epoch 11, Batch 904, LR 2.499241 Loss 19.061621, Accuracy 0.082%\n",
      "Epoch 11, Batch 905, LR 2.499251 Loss 19.061489, Accuracy 0.082%\n",
      "Epoch 11, Batch 906, LR 2.499262 Loss 19.061486, Accuracy 0.082%\n",
      "Epoch 11, Batch 907, LR 2.499272 Loss 19.061359, Accuracy 0.082%\n",
      "Epoch 11, Batch 908, LR 2.499283 Loss 19.061212, Accuracy 0.082%\n",
      "Epoch 11, Batch 909, LR 2.499293 Loss 19.061051, Accuracy 0.082%\n",
      "Epoch 11, Batch 910, LR 2.499303 Loss 19.060858, Accuracy 0.082%\n",
      "Epoch 11, Batch 911, LR 2.499313 Loss 19.060864, Accuracy 0.081%\n",
      "Epoch 11, Batch 912, LR 2.499323 Loss 19.060844, Accuracy 0.081%\n",
      "Epoch 11, Batch 913, LR 2.499333 Loss 19.060681, Accuracy 0.081%\n",
      "Epoch 11, Batch 914, LR 2.499343 Loss 19.060766, Accuracy 0.081%\n",
      "Epoch 11, Batch 915, LR 2.499353 Loss 19.060719, Accuracy 0.081%\n",
      "Epoch 11, Batch 916, LR 2.499363 Loss 19.060482, Accuracy 0.081%\n",
      "Epoch 11, Batch 917, LR 2.499373 Loss 19.060552, Accuracy 0.081%\n",
      "Epoch 11, Batch 918, LR 2.499382 Loss 19.060635, Accuracy 0.081%\n",
      "Epoch 11, Batch 919, LR 2.499392 Loss 19.060645, Accuracy 0.081%\n",
      "Epoch 11, Batch 920, LR 2.499401 Loss 19.060596, Accuracy 0.081%\n",
      "Epoch 11, Batch 921, LR 2.499411 Loss 19.060780, Accuracy 0.081%\n",
      "Epoch 11, Batch 922, LR 2.499420 Loss 19.060820, Accuracy 0.080%\n",
      "Epoch 11, Batch 923, LR 2.499429 Loss 19.060850, Accuracy 0.080%\n",
      "Epoch 11, Batch 924, LR 2.499438 Loss 19.060621, Accuracy 0.080%\n",
      "Epoch 11, Batch 925, LR 2.499447 Loss 19.060638, Accuracy 0.080%\n",
      "Epoch 11, Batch 926, LR 2.499456 Loss 19.060547, Accuracy 0.080%\n",
      "Epoch 11, Batch 927, LR 2.499465 Loss 19.060647, Accuracy 0.080%\n",
      "Epoch 11, Batch 928, LR 2.499474 Loss 19.060714, Accuracy 0.080%\n",
      "Epoch 11, Batch 929, LR 2.499483 Loss 19.060892, Accuracy 0.080%\n",
      "Epoch 11, Batch 930, LR 2.499492 Loss 19.060941, Accuracy 0.080%\n",
      "Epoch 11, Batch 931, LR 2.499500 Loss 19.060914, Accuracy 0.081%\n",
      "Epoch 11, Batch 932, LR 2.499509 Loss 19.060798, Accuracy 0.080%\n",
      "Epoch 11, Batch 933, LR 2.499518 Loss 19.060575, Accuracy 0.080%\n",
      "Epoch 11, Batch 934, LR 2.499526 Loss 19.060532, Accuracy 0.080%\n",
      "Epoch 11, Batch 935, LR 2.499534 Loss 19.060507, Accuracy 0.080%\n",
      "Epoch 11, Batch 936, LR 2.499543 Loss 19.060528, Accuracy 0.081%\n",
      "Epoch 11, Batch 937, LR 2.499551 Loss 19.060231, Accuracy 0.082%\n",
      "Epoch 11, Batch 938, LR 2.499559 Loss 19.060100, Accuracy 0.082%\n",
      "Epoch 11, Batch 939, LR 2.499567 Loss 19.059761, Accuracy 0.082%\n",
      "Epoch 11, Batch 940, LR 2.499575 Loss 19.059763, Accuracy 0.081%\n",
      "Epoch 11, Batch 941, LR 2.499583 Loss 19.059838, Accuracy 0.081%\n",
      "Epoch 11, Batch 942, LR 2.499591 Loss 19.059683, Accuracy 0.081%\n",
      "Epoch 11, Batch 943, LR 2.499598 Loss 19.059714, Accuracy 0.081%\n",
      "Epoch 11, Batch 944, LR 2.499606 Loss 19.059618, Accuracy 0.081%\n",
      "Epoch 11, Batch 945, LR 2.499614 Loss 19.059858, Accuracy 0.081%\n",
      "Epoch 11, Batch 946, LR 2.499621 Loss 19.060083, Accuracy 0.081%\n",
      "Epoch 11, Batch 947, LR 2.499629 Loss 19.060118, Accuracy 0.082%\n",
      "Epoch 11, Batch 948, LR 2.499636 Loss 19.060174, Accuracy 0.082%\n",
      "Epoch 11, Batch 949, LR 2.499643 Loss 19.060065, Accuracy 0.082%\n",
      "Epoch 11, Batch 950, LR 2.499651 Loss 19.060179, Accuracy 0.081%\n",
      "Epoch 11, Batch 951, LR 2.499658 Loss 19.060067, Accuracy 0.081%\n",
      "Epoch 11, Batch 952, LR 2.499665 Loss 19.059659, Accuracy 0.081%\n",
      "Epoch 11, Batch 953, LR 2.499672 Loss 19.059622, Accuracy 0.081%\n",
      "Epoch 11, Batch 954, LR 2.499679 Loss 19.059594, Accuracy 0.081%\n",
      "Epoch 11, Batch 955, LR 2.499686 Loss 19.059547, Accuracy 0.081%\n",
      "Epoch 11, Batch 956, LR 2.499693 Loss 19.059634, Accuracy 0.081%\n",
      "Epoch 11, Batch 957, LR 2.499699 Loss 19.059890, Accuracy 0.081%\n",
      "Epoch 11, Batch 958, LR 2.499706 Loss 19.059781, Accuracy 0.081%\n",
      "Epoch 11, Batch 959, LR 2.499712 Loss 19.059603, Accuracy 0.081%\n",
      "Epoch 11, Batch 960, LR 2.499719 Loss 19.059350, Accuracy 0.081%\n",
      "Epoch 11, Batch 961, LR 2.499725 Loss 19.059371, Accuracy 0.080%\n",
      "Epoch 11, Batch 962, LR 2.499732 Loss 19.059449, Accuracy 0.081%\n",
      "Epoch 11, Batch 963, LR 2.499738 Loss 19.059225, Accuracy 0.081%\n",
      "Epoch 11, Batch 964, LR 2.499744 Loss 19.058979, Accuracy 0.082%\n",
      "Epoch 11, Batch 965, LR 2.499750 Loss 19.058948, Accuracy 0.082%\n",
      "Epoch 11, Batch 966, LR 2.499756 Loss 19.058666, Accuracy 0.082%\n",
      "Epoch 11, Batch 967, LR 2.499762 Loss 19.058664, Accuracy 0.082%\n",
      "Epoch 11, Batch 968, LR 2.499768 Loss 19.058623, Accuracy 0.082%\n",
      "Epoch 11, Batch 969, LR 2.499774 Loss 19.058724, Accuracy 0.081%\n",
      "Epoch 11, Batch 970, LR 2.499780 Loss 19.058639, Accuracy 0.081%\n",
      "Epoch 11, Batch 971, LR 2.499786 Loss 19.058540, Accuracy 0.081%\n",
      "Epoch 11, Batch 972, LR 2.499791 Loss 19.058556, Accuracy 0.082%\n",
      "Epoch 11, Batch 973, LR 2.499797 Loss 19.058469, Accuracy 0.083%\n",
      "Epoch 11, Batch 974, LR 2.499802 Loss 19.058671, Accuracy 0.083%\n",
      "Epoch 11, Batch 975, LR 2.499808 Loss 19.058568, Accuracy 0.083%\n",
      "Epoch 11, Batch 976, LR 2.499813 Loss 19.058371, Accuracy 0.082%\n",
      "Epoch 11, Batch 977, LR 2.499818 Loss 19.058328, Accuracy 0.082%\n",
      "Epoch 11, Batch 978, LR 2.499823 Loss 19.058498, Accuracy 0.083%\n",
      "Epoch 11, Batch 979, LR 2.499828 Loss 19.058425, Accuracy 0.083%\n",
      "Epoch 11, Batch 980, LR 2.499833 Loss 19.058285, Accuracy 0.083%\n",
      "Epoch 11, Batch 981, LR 2.499838 Loss 19.058255, Accuracy 0.083%\n",
      "Epoch 11, Batch 982, LR 2.499843 Loss 19.058328, Accuracy 0.083%\n",
      "Epoch 11, Batch 983, LR 2.499848 Loss 19.058295, Accuracy 0.083%\n",
      "Epoch 11, Batch 984, LR 2.499853 Loss 19.058518, Accuracy 0.083%\n",
      "Epoch 11, Batch 985, LR 2.499857 Loss 19.058652, Accuracy 0.082%\n",
      "Epoch 11, Batch 986, LR 2.499862 Loss 19.058663, Accuracy 0.082%\n",
      "Epoch 11, Batch 987, LR 2.499866 Loss 19.058618, Accuracy 0.082%\n",
      "Epoch 11, Batch 988, LR 2.499871 Loss 19.058531, Accuracy 0.083%\n",
      "Epoch 11, Batch 989, LR 2.499875 Loss 19.058798, Accuracy 0.083%\n",
      "Epoch 11, Batch 990, LR 2.499879 Loss 19.058804, Accuracy 0.083%\n",
      "Epoch 11, Batch 991, LR 2.499884 Loss 19.058920, Accuracy 0.083%\n",
      "Epoch 11, Batch 992, LR 2.499888 Loss 19.058967, Accuracy 0.083%\n",
      "Epoch 11, Batch 993, LR 2.499892 Loss 19.058872, Accuracy 0.083%\n",
      "Epoch 11, Batch 994, LR 2.499896 Loss 19.058850, Accuracy 0.083%\n",
      "Epoch 11, Batch 995, LR 2.499900 Loss 19.058985, Accuracy 0.082%\n",
      "Epoch 11, Batch 996, LR 2.499903 Loss 19.058935, Accuracy 0.083%\n",
      "Epoch 11, Batch 997, LR 2.499907 Loss 19.058942, Accuracy 0.083%\n",
      "Epoch 11, Batch 998, LR 2.499911 Loss 19.058984, Accuracy 0.083%\n",
      "Epoch 11, Batch 999, LR 2.499914 Loss 19.058920, Accuracy 0.083%\n",
      "Epoch 11, Batch 1000, LR 2.499918 Loss 19.059012, Accuracy 0.083%\n",
      "Epoch 11, Batch 1001, LR 2.499921 Loss 19.058852, Accuracy 0.083%\n",
      "Epoch 11, Batch 1002, LR 2.499925 Loss 19.058889, Accuracy 0.083%\n",
      "Epoch 11, Batch 1003, LR 2.499928 Loss 19.058820, Accuracy 0.083%\n",
      "Epoch 11, Batch 1004, LR 2.499931 Loss 19.058839, Accuracy 0.083%\n",
      "Epoch 11, Batch 1005, LR 2.499935 Loss 19.058757, Accuracy 0.083%\n",
      "Epoch 11, Batch 1006, LR 2.499938 Loss 19.058888, Accuracy 0.083%\n",
      "Epoch 11, Batch 1007, LR 2.499941 Loss 19.058925, Accuracy 0.083%\n",
      "Epoch 11, Batch 1008, LR 2.499944 Loss 19.058793, Accuracy 0.083%\n",
      "Epoch 11, Batch 1009, LR 2.499946 Loss 19.058829, Accuracy 0.083%\n",
      "Epoch 11, Batch 1010, LR 2.499949 Loss 19.058811, Accuracy 0.083%\n",
      "Epoch 11, Batch 1011, LR 2.499952 Loss 19.058869, Accuracy 0.083%\n",
      "Epoch 11, Batch 1012, LR 2.499955 Loss 19.058799, Accuracy 0.083%\n",
      "Epoch 11, Batch 1013, LR 2.499957 Loss 19.059053, Accuracy 0.083%\n",
      "Epoch 11, Batch 1014, LR 2.499960 Loss 19.059169, Accuracy 0.082%\n",
      "Epoch 11, Batch 1015, LR 2.499962 Loss 19.059100, Accuracy 0.082%\n",
      "Epoch 11, Batch 1016, LR 2.499964 Loss 19.059110, Accuracy 0.082%\n",
      "Epoch 11, Batch 1017, LR 2.499967 Loss 19.059272, Accuracy 0.082%\n",
      "Epoch 11, Batch 1018, LR 2.499969 Loss 19.059413, Accuracy 0.082%\n",
      "Epoch 11, Batch 1019, LR 2.499971 Loss 19.059319, Accuracy 0.082%\n",
      "Epoch 11, Batch 1020, LR 2.499973 Loss 19.059385, Accuracy 0.082%\n",
      "Epoch 11, Batch 1021, LR 2.499975 Loss 19.059558, Accuracy 0.082%\n",
      "Epoch 11, Batch 1022, LR 2.499977 Loss 19.059598, Accuracy 0.083%\n",
      "Epoch 11, Batch 1023, LR 2.499979 Loss 19.059808, Accuracy 0.082%\n",
      "Epoch 11, Batch 1024, LR 2.499980 Loss 19.059680, Accuracy 0.082%\n",
      "Epoch 11, Batch 1025, LR 2.499982 Loss 19.059735, Accuracy 0.082%\n",
      "Epoch 11, Batch 1026, LR 2.499984 Loss 19.059716, Accuracy 0.082%\n",
      "Epoch 11, Batch 1027, LR 2.499985 Loss 19.059685, Accuracy 0.082%\n",
      "Epoch 11, Batch 1028, LR 2.499987 Loss 19.059634, Accuracy 0.082%\n",
      "Epoch 11, Batch 1029, LR 2.499988 Loss 19.059717, Accuracy 0.082%\n",
      "Epoch 11, Batch 1030, LR 2.499989 Loss 19.059543, Accuracy 0.082%\n",
      "Epoch 11, Batch 1031, LR 2.499990 Loss 19.059573, Accuracy 0.082%\n",
      "Epoch 11, Batch 1032, LR 2.499992 Loss 19.059588, Accuracy 0.082%\n",
      "Epoch 11, Batch 1033, LR 2.499993 Loss 19.059523, Accuracy 0.082%\n",
      "Epoch 11, Batch 1034, LR 2.499994 Loss 19.059510, Accuracy 0.082%\n",
      "Epoch 11, Batch 1035, LR 2.499995 Loss 19.059557, Accuracy 0.082%\n",
      "Epoch 11, Batch 1036, LR 2.499996 Loss 19.059574, Accuracy 0.081%\n",
      "Epoch 11, Batch 1037, LR 2.499996 Loss 19.059435, Accuracy 0.082%\n",
      "Epoch 11, Batch 1038, LR 2.499997 Loss 19.059444, Accuracy 0.082%\n",
      "Epoch 11, Batch 1039, LR 2.499998 Loss 19.059397, Accuracy 0.082%\n",
      "Epoch 11, Batch 1040, LR 2.499998 Loss 19.059572, Accuracy 0.082%\n",
      "Epoch 11, Batch 1041, LR 2.499999 Loss 19.059468, Accuracy 0.082%\n",
      "Epoch 11, Batch 1042, LR 2.499999 Loss 19.059874, Accuracy 0.082%\n",
      "Epoch 11, Batch 1043, LR 2.499999 Loss 19.059903, Accuracy 0.082%\n",
      "Epoch 11, Batch 1044, LR 2.500000 Loss 19.059967, Accuracy 0.082%\n",
      "Epoch 11, Batch 1045, LR 2.500000 Loss 19.059971, Accuracy 0.081%\n",
      "Epoch 11, Batch 1046, LR 2.500000 Loss 19.060110, Accuracy 0.082%\n",
      "Epoch 11, Batch 1047, LR 2.500000 Loss 19.060081, Accuracy 0.082%\n",
      "Epoch 11, Loss (train set) 19.060081, Accuracy (train set) 0.082%\n",
      "Epoch 12, Batch 1, LR 2.500000 Loss 18.986837, Accuracy 0.000%\n",
      "Epoch 12, Batch 2, LR 2.500000 Loss 18.978990, Accuracy 0.391%\n",
      "Epoch 12, Batch 3, LR 2.500000 Loss 19.012709, Accuracy 0.260%\n",
      "Epoch 12, Batch 4, LR 2.500000 Loss 19.016985, Accuracy 0.195%\n",
      "Epoch 12, Batch 5, LR 2.500000 Loss 19.000206, Accuracy 0.156%\n",
      "Epoch 12, Batch 6, LR 2.500000 Loss 18.977473, Accuracy 0.130%\n",
      "Epoch 12, Batch 7, LR 2.500000 Loss 18.997068, Accuracy 0.112%\n",
      "Epoch 12, Batch 8, LR 2.500000 Loss 19.000978, Accuracy 0.098%\n",
      "Epoch 12, Batch 9, LR 2.499999 Loss 18.995121, Accuracy 0.087%\n",
      "Epoch 12, Batch 10, LR 2.499999 Loss 18.991933, Accuracy 0.078%\n",
      "Epoch 12, Batch 11, LR 2.499999 Loss 19.012254, Accuracy 0.071%\n",
      "Epoch 12, Batch 12, LR 2.499999 Loss 19.017846, Accuracy 0.065%\n",
      "Epoch 12, Batch 13, LR 2.499999 Loss 19.039012, Accuracy 0.060%\n",
      "Epoch 12, Batch 14, LR 2.499999 Loss 19.039385, Accuracy 0.056%\n",
      "Epoch 12, Batch 15, LR 2.499998 Loss 19.043973, Accuracy 0.104%\n",
      "Epoch 12, Batch 16, LR 2.499998 Loss 19.036600, Accuracy 0.098%\n",
      "Epoch 12, Batch 17, LR 2.499998 Loss 19.046202, Accuracy 0.092%\n",
      "Epoch 12, Batch 18, LR 2.499998 Loss 19.036361, Accuracy 0.087%\n",
      "Epoch 12, Batch 19, LR 2.499997 Loss 19.041829, Accuracy 0.082%\n",
      "Epoch 12, Batch 20, LR 2.499997 Loss 19.050257, Accuracy 0.078%\n",
      "Epoch 12, Batch 21, LR 2.499997 Loss 19.046628, Accuracy 0.074%\n",
      "Epoch 12, Batch 22, LR 2.499997 Loss 19.042328, Accuracy 0.071%\n",
      "Epoch 12, Batch 23, LR 2.499996 Loss 19.048363, Accuracy 0.068%\n",
      "Epoch 12, Batch 24, LR 2.499996 Loss 19.052749, Accuracy 0.065%\n",
      "Epoch 12, Batch 25, LR 2.499996 Loss 19.054707, Accuracy 0.062%\n",
      "Epoch 12, Batch 26, LR 2.499995 Loss 19.061466, Accuracy 0.060%\n",
      "Epoch 12, Batch 27, LR 2.499995 Loss 19.071059, Accuracy 0.058%\n",
      "Epoch 12, Batch 28, LR 2.499994 Loss 19.065149, Accuracy 0.056%\n",
      "Epoch 12, Batch 29, LR 2.499994 Loss 19.060933, Accuracy 0.054%\n",
      "Epoch 12, Batch 30, LR 2.499994 Loss 19.065897, Accuracy 0.052%\n",
      "Epoch 12, Batch 31, LR 2.499993 Loss 19.068065, Accuracy 0.076%\n",
      "Epoch 12, Batch 32, LR 2.499993 Loss 19.072743, Accuracy 0.073%\n",
      "Epoch 12, Batch 33, LR 2.499992 Loss 19.071793, Accuracy 0.071%\n",
      "Epoch 12, Batch 34, LR 2.499992 Loss 19.068057, Accuracy 0.069%\n",
      "Epoch 12, Batch 35, LR 2.499991 Loss 19.067263, Accuracy 0.067%\n",
      "Epoch 12, Batch 36, LR 2.499991 Loss 19.064112, Accuracy 0.065%\n",
      "Epoch 12, Batch 37, LR 2.499990 Loss 19.064028, Accuracy 0.063%\n",
      "Epoch 12, Batch 38, LR 2.499990 Loss 19.060502, Accuracy 0.062%\n",
      "Epoch 12, Batch 39, LR 2.499989 Loss 19.060132, Accuracy 0.060%\n",
      "Epoch 12, Batch 40, LR 2.499989 Loss 19.053532, Accuracy 0.059%\n",
      "Epoch 12, Batch 41, LR 2.499988 Loss 19.049972, Accuracy 0.057%\n",
      "Epoch 12, Batch 42, LR 2.499987 Loss 19.052110, Accuracy 0.056%\n",
      "Epoch 12, Batch 43, LR 2.499987 Loss 19.054856, Accuracy 0.055%\n",
      "Epoch 12, Batch 44, LR 2.499986 Loss 19.057396, Accuracy 0.053%\n",
      "Epoch 12, Batch 45, LR 2.499985 Loss 19.055031, Accuracy 0.052%\n",
      "Epoch 12, Batch 46, LR 2.499985 Loss 19.056688, Accuracy 0.051%\n",
      "Epoch 12, Batch 47, LR 2.499984 Loss 19.057585, Accuracy 0.050%\n",
      "Epoch 12, Batch 48, LR 2.499983 Loss 19.060108, Accuracy 0.049%\n",
      "Epoch 12, Batch 49, LR 2.499983 Loss 19.063821, Accuracy 0.048%\n",
      "Epoch 12, Batch 50, LR 2.499982 Loss 19.063814, Accuracy 0.047%\n",
      "Epoch 12, Batch 51, LR 2.499981 Loss 19.063152, Accuracy 0.046%\n",
      "Epoch 12, Batch 52, LR 2.499981 Loss 19.063237, Accuracy 0.060%\n",
      "Epoch 12, Batch 53, LR 2.499980 Loss 19.062604, Accuracy 0.059%\n",
      "Epoch 12, Batch 54, LR 2.499979 Loss 19.062428, Accuracy 0.058%\n",
      "Epoch 12, Batch 55, LR 2.499978 Loss 19.061545, Accuracy 0.057%\n",
      "Epoch 12, Batch 56, LR 2.499977 Loss 19.060955, Accuracy 0.056%\n",
      "Epoch 12, Batch 57, LR 2.499977 Loss 19.060239, Accuracy 0.055%\n",
      "Epoch 12, Batch 58, LR 2.499976 Loss 19.060377, Accuracy 0.054%\n",
      "Epoch 12, Batch 59, LR 2.499975 Loss 19.060805, Accuracy 0.053%\n",
      "Epoch 12, Batch 60, LR 2.499974 Loss 19.060005, Accuracy 0.052%\n",
      "Epoch 12, Batch 61, LR 2.499973 Loss 19.060934, Accuracy 0.051%\n",
      "Epoch 12, Batch 62, LR 2.499972 Loss 19.059991, Accuracy 0.063%\n",
      "Epoch 12, Batch 63, LR 2.499972 Loss 19.061355, Accuracy 0.062%\n",
      "Epoch 12, Batch 64, LR 2.499971 Loss 19.062095, Accuracy 0.061%\n",
      "Epoch 12, Batch 65, LR 2.499970 Loss 19.061324, Accuracy 0.060%\n",
      "Epoch 12, Batch 66, LR 2.499969 Loss 19.061553, Accuracy 0.059%\n",
      "Epoch 12, Batch 67, LR 2.499968 Loss 19.062078, Accuracy 0.058%\n",
      "Epoch 12, Batch 68, LR 2.499967 Loss 19.061921, Accuracy 0.057%\n",
      "Epoch 12, Batch 69, LR 2.499966 Loss 19.060953, Accuracy 0.057%\n",
      "Epoch 12, Batch 70, LR 2.499965 Loss 19.060016, Accuracy 0.056%\n",
      "Epoch 12, Batch 71, LR 2.499964 Loss 19.061542, Accuracy 0.055%\n",
      "Epoch 12, Batch 72, LR 2.499963 Loss 19.059110, Accuracy 0.054%\n",
      "Epoch 12, Batch 73, LR 2.499962 Loss 19.057258, Accuracy 0.064%\n",
      "Epoch 12, Batch 74, LR 2.499961 Loss 19.058460, Accuracy 0.063%\n",
      "Epoch 12, Batch 75, LR 2.499960 Loss 19.055499, Accuracy 0.062%\n",
      "Epoch 12, Batch 76, LR 2.499959 Loss 19.054635, Accuracy 0.062%\n",
      "Epoch 12, Batch 77, LR 2.499957 Loss 19.053896, Accuracy 0.061%\n",
      "Epoch 12, Batch 78, LR 2.499956 Loss 19.054000, Accuracy 0.060%\n",
      "Epoch 12, Batch 79, LR 2.499955 Loss 19.053915, Accuracy 0.059%\n",
      "Epoch 12, Batch 80, LR 2.499954 Loss 19.053292, Accuracy 0.059%\n",
      "Epoch 12, Batch 81, LR 2.499953 Loss 19.055469, Accuracy 0.058%\n",
      "Epoch 12, Batch 82, LR 2.499952 Loss 19.056425, Accuracy 0.057%\n",
      "Epoch 12, Batch 83, LR 2.499951 Loss 19.056871, Accuracy 0.056%\n",
      "Epoch 12, Batch 84, LR 2.499949 Loss 19.055283, Accuracy 0.056%\n",
      "Epoch 12, Batch 85, LR 2.499948 Loss 19.054083, Accuracy 0.055%\n",
      "Epoch 12, Batch 86, LR 2.499947 Loss 19.054474, Accuracy 0.055%\n",
      "Epoch 12, Batch 87, LR 2.499946 Loss 19.056749, Accuracy 0.063%\n",
      "Epoch 12, Batch 88, LR 2.499944 Loss 19.057677, Accuracy 0.062%\n",
      "Epoch 12, Batch 89, LR 2.499943 Loss 19.057289, Accuracy 0.061%\n",
      "Epoch 12, Batch 90, LR 2.499942 Loss 19.058377, Accuracy 0.061%\n",
      "Epoch 12, Batch 91, LR 2.499941 Loss 19.058880, Accuracy 0.060%\n",
      "Epoch 12, Batch 92, LR 2.499939 Loss 19.060415, Accuracy 0.059%\n",
      "Epoch 12, Batch 93, LR 2.499938 Loss 19.060713, Accuracy 0.059%\n",
      "Epoch 12, Batch 94, LR 2.499937 Loss 19.060793, Accuracy 0.058%\n",
      "Epoch 12, Batch 95, LR 2.499935 Loss 19.059229, Accuracy 0.058%\n",
      "Epoch 12, Batch 96, LR 2.499934 Loss 19.058202, Accuracy 0.065%\n",
      "Epoch 12, Batch 97, LR 2.499932 Loss 19.054482, Accuracy 0.064%\n",
      "Epoch 12, Batch 98, LR 2.499931 Loss 19.053483, Accuracy 0.064%\n",
      "Epoch 12, Batch 99, LR 2.499930 Loss 19.052522, Accuracy 0.063%\n",
      "Epoch 12, Batch 100, LR 2.499928 Loss 19.051123, Accuracy 0.062%\n",
      "Epoch 12, Batch 101, LR 2.499927 Loss 19.050125, Accuracy 0.070%\n",
      "Epoch 12, Batch 102, LR 2.499925 Loss 19.049600, Accuracy 0.069%\n",
      "Epoch 12, Batch 103, LR 2.499924 Loss 19.048843, Accuracy 0.068%\n",
      "Epoch 12, Batch 104, LR 2.499922 Loss 19.050748, Accuracy 0.068%\n",
      "Epoch 12, Batch 105, LR 2.499921 Loss 19.051296, Accuracy 0.067%\n",
      "Epoch 12, Batch 106, LR 2.499919 Loss 19.051424, Accuracy 0.066%\n",
      "Epoch 12, Batch 107, LR 2.499918 Loss 19.049687, Accuracy 0.073%\n",
      "Epoch 12, Batch 108, LR 2.499916 Loss 19.051213, Accuracy 0.072%\n",
      "Epoch 12, Batch 109, LR 2.499915 Loss 19.052089, Accuracy 0.072%\n",
      "Epoch 12, Batch 110, LR 2.499913 Loss 19.054033, Accuracy 0.071%\n",
      "Epoch 12, Batch 111, LR 2.499912 Loss 19.055003, Accuracy 0.070%\n",
      "Epoch 12, Batch 112, LR 2.499910 Loss 19.053816, Accuracy 0.077%\n",
      "Epoch 12, Batch 113, LR 2.499908 Loss 19.054038, Accuracy 0.076%\n",
      "Epoch 12, Batch 114, LR 2.499907 Loss 19.054644, Accuracy 0.075%\n",
      "Epoch 12, Batch 115, LR 2.499905 Loss 19.053015, Accuracy 0.075%\n",
      "Epoch 12, Batch 116, LR 2.499903 Loss 19.054796, Accuracy 0.074%\n",
      "Epoch 12, Batch 117, LR 2.499902 Loss 19.056003, Accuracy 0.073%\n",
      "Epoch 12, Batch 118, LR 2.499900 Loss 19.054415, Accuracy 0.073%\n",
      "Epoch 12, Batch 119, LR 2.499898 Loss 19.053114, Accuracy 0.072%\n",
      "Epoch 12, Batch 120, LR 2.499897 Loss 19.052402, Accuracy 0.072%\n",
      "Epoch 12, Batch 121, LR 2.499895 Loss 19.052195, Accuracy 0.071%\n",
      "Epoch 12, Batch 122, LR 2.499893 Loss 19.051568, Accuracy 0.070%\n",
      "Epoch 12, Batch 123, LR 2.499891 Loss 19.051924, Accuracy 0.070%\n",
      "Epoch 12, Batch 124, LR 2.499890 Loss 19.052612, Accuracy 0.069%\n",
      "Epoch 12, Batch 125, LR 2.499888 Loss 19.053794, Accuracy 0.069%\n",
      "Epoch 12, Batch 126, LR 2.499886 Loss 19.052809, Accuracy 0.068%\n",
      "Epoch 12, Batch 127, LR 2.499884 Loss 19.052766, Accuracy 0.068%\n",
      "Epoch 12, Batch 128, LR 2.499882 Loss 19.052603, Accuracy 0.073%\n",
      "Epoch 12, Batch 129, LR 2.499881 Loss 19.052963, Accuracy 0.073%\n",
      "Epoch 12, Batch 130, LR 2.499879 Loss 19.054399, Accuracy 0.072%\n",
      "Epoch 12, Batch 131, LR 2.499877 Loss 19.054998, Accuracy 0.072%\n",
      "Epoch 12, Batch 132, LR 2.499875 Loss 19.054650, Accuracy 0.071%\n",
      "Epoch 12, Batch 133, LR 2.499873 Loss 19.055276, Accuracy 0.070%\n",
      "Epoch 12, Batch 134, LR 2.499871 Loss 19.057869, Accuracy 0.070%\n",
      "Epoch 12, Batch 135, LR 2.499869 Loss 19.058043, Accuracy 0.069%\n",
      "Epoch 12, Batch 136, LR 2.499867 Loss 19.059379, Accuracy 0.069%\n",
      "Epoch 12, Batch 137, LR 2.499865 Loss 19.059139, Accuracy 0.068%\n",
      "Epoch 12, Batch 138, LR 2.499863 Loss 19.058202, Accuracy 0.068%\n",
      "Epoch 12, Batch 139, LR 2.499861 Loss 19.058932, Accuracy 0.067%\n",
      "Epoch 12, Batch 140, LR 2.499859 Loss 19.060176, Accuracy 0.073%\n",
      "Epoch 12, Batch 141, LR 2.499857 Loss 19.061795, Accuracy 0.072%\n",
      "Epoch 12, Batch 142, LR 2.499855 Loss 19.061125, Accuracy 0.072%\n",
      "Epoch 12, Batch 143, LR 2.499853 Loss 19.060159, Accuracy 0.071%\n",
      "Epoch 12, Batch 144, LR 2.499851 Loss 19.062260, Accuracy 0.071%\n",
      "Epoch 12, Batch 145, LR 2.499849 Loss 19.062229, Accuracy 0.070%\n",
      "Epoch 12, Batch 146, LR 2.499847 Loss 19.062816, Accuracy 0.070%\n",
      "Epoch 12, Batch 147, LR 2.499845 Loss 19.062187, Accuracy 0.069%\n",
      "Epoch 12, Batch 148, LR 2.499843 Loss 19.061489, Accuracy 0.069%\n",
      "Epoch 12, Batch 149, LR 2.499841 Loss 19.061451, Accuracy 0.073%\n",
      "Epoch 12, Batch 150, LR 2.499839 Loss 19.063809, Accuracy 0.073%\n",
      "Epoch 12, Batch 151, LR 2.499836 Loss 19.065639, Accuracy 0.072%\n",
      "Epoch 12, Batch 152, LR 2.499834 Loss 19.065305, Accuracy 0.077%\n",
      "Epoch 12, Batch 153, LR 2.499832 Loss 19.065057, Accuracy 0.077%\n",
      "Epoch 12, Batch 154, LR 2.499830 Loss 19.065485, Accuracy 0.076%\n",
      "Epoch 12, Batch 155, LR 2.499828 Loss 19.067286, Accuracy 0.076%\n",
      "Epoch 12, Batch 156, LR 2.499825 Loss 19.067599, Accuracy 0.075%\n",
      "Epoch 12, Batch 157, LR 2.499823 Loss 19.068341, Accuracy 0.075%\n",
      "Epoch 12, Batch 158, LR 2.499821 Loss 19.068140, Accuracy 0.074%\n",
      "Epoch 12, Batch 159, LR 2.499819 Loss 19.067561, Accuracy 0.074%\n",
      "Epoch 12, Batch 160, LR 2.499816 Loss 19.065883, Accuracy 0.073%\n",
      "Epoch 12, Batch 161, LR 2.499814 Loss 19.064383, Accuracy 0.073%\n",
      "Epoch 12, Batch 162, LR 2.499812 Loss 19.064585, Accuracy 0.072%\n",
      "Epoch 12, Batch 163, LR 2.499809 Loss 19.063719, Accuracy 0.077%\n",
      "Epoch 12, Batch 164, LR 2.499807 Loss 19.063127, Accuracy 0.076%\n",
      "Epoch 12, Batch 165, LR 2.499805 Loss 19.063285, Accuracy 0.076%\n",
      "Epoch 12, Batch 166, LR 2.499802 Loss 19.063309, Accuracy 0.075%\n",
      "Epoch 12, Batch 167, LR 2.499800 Loss 19.063787, Accuracy 0.075%\n",
      "Epoch 12, Batch 168, LR 2.499797 Loss 19.064184, Accuracy 0.074%\n",
      "Epoch 12, Batch 169, LR 2.499795 Loss 19.064784, Accuracy 0.079%\n",
      "Epoch 12, Batch 170, LR 2.499793 Loss 19.064373, Accuracy 0.078%\n",
      "Epoch 12, Batch 171, LR 2.499790 Loss 19.063738, Accuracy 0.078%\n",
      "Epoch 12, Batch 172, LR 2.499788 Loss 19.064733, Accuracy 0.077%\n",
      "Epoch 12, Batch 173, LR 2.499785 Loss 19.064357, Accuracy 0.077%\n",
      "Epoch 12, Batch 174, LR 2.499783 Loss 19.064748, Accuracy 0.076%\n",
      "Epoch 12, Batch 175, LR 2.499780 Loss 19.064467, Accuracy 0.076%\n",
      "Epoch 12, Batch 176, LR 2.499778 Loss 19.064830, Accuracy 0.075%\n",
      "Epoch 12, Batch 177, LR 2.499775 Loss 19.064701, Accuracy 0.075%\n",
      "Epoch 12, Batch 178, LR 2.499773 Loss 19.065873, Accuracy 0.079%\n",
      "Epoch 12, Batch 179, LR 2.499770 Loss 19.065456, Accuracy 0.079%\n",
      "Epoch 12, Batch 180, LR 2.499767 Loss 19.064626, Accuracy 0.078%\n",
      "Epoch 12, Batch 181, LR 2.499765 Loss 19.063711, Accuracy 0.078%\n",
      "Epoch 12, Batch 182, LR 2.499762 Loss 19.063464, Accuracy 0.077%\n",
      "Epoch 12, Batch 183, LR 2.499760 Loss 19.064149, Accuracy 0.081%\n",
      "Epoch 12, Batch 184, LR 2.499757 Loss 19.064842, Accuracy 0.081%\n",
      "Epoch 12, Batch 185, LR 2.499754 Loss 19.064177, Accuracy 0.080%\n",
      "Epoch 12, Batch 186, LR 2.499752 Loss 19.063869, Accuracy 0.080%\n",
      "Epoch 12, Batch 187, LR 2.499749 Loss 19.063762, Accuracy 0.079%\n",
      "Epoch 12, Batch 188, LR 2.499746 Loss 19.064500, Accuracy 0.079%\n",
      "Epoch 12, Batch 189, LR 2.499744 Loss 19.063888, Accuracy 0.079%\n",
      "Epoch 12, Batch 190, LR 2.499741 Loss 19.065066, Accuracy 0.078%\n",
      "Epoch 12, Batch 191, LR 2.499738 Loss 19.064474, Accuracy 0.078%\n",
      "Epoch 12, Batch 192, LR 2.499735 Loss 19.064154, Accuracy 0.077%\n",
      "Epoch 12, Batch 193, LR 2.499733 Loss 19.064697, Accuracy 0.077%\n",
      "Epoch 12, Batch 194, LR 2.499730 Loss 19.064032, Accuracy 0.077%\n",
      "Epoch 12, Batch 195, LR 2.499727 Loss 19.063879, Accuracy 0.076%\n",
      "Epoch 12, Batch 196, LR 2.499724 Loss 19.064618, Accuracy 0.076%\n",
      "Epoch 12, Batch 197, LR 2.499721 Loss 19.064782, Accuracy 0.075%\n",
      "Epoch 12, Batch 198, LR 2.499719 Loss 19.064620, Accuracy 0.075%\n",
      "Epoch 12, Batch 199, LR 2.499716 Loss 19.064982, Accuracy 0.075%\n",
      "Epoch 12, Batch 200, LR 2.499713 Loss 19.064837, Accuracy 0.074%\n",
      "Epoch 12, Batch 201, LR 2.499710 Loss 19.064078, Accuracy 0.074%\n",
      "Epoch 12, Batch 202, LR 2.499707 Loss 19.064180, Accuracy 0.073%\n",
      "Epoch 12, Batch 203, LR 2.499704 Loss 19.064798, Accuracy 0.073%\n",
      "Epoch 12, Batch 204, LR 2.499701 Loss 19.065343, Accuracy 0.073%\n",
      "Epoch 12, Batch 205, LR 2.499698 Loss 19.065251, Accuracy 0.072%\n",
      "Epoch 12, Batch 206, LR 2.499695 Loss 19.064747, Accuracy 0.072%\n",
      "Epoch 12, Batch 207, LR 2.499692 Loss 19.064358, Accuracy 0.072%\n",
      "Epoch 12, Batch 208, LR 2.499689 Loss 19.063233, Accuracy 0.071%\n",
      "Epoch 12, Batch 209, LR 2.499686 Loss 19.063329, Accuracy 0.071%\n",
      "Epoch 12, Batch 210, LR 2.499683 Loss 19.063121, Accuracy 0.071%\n",
      "Epoch 12, Batch 211, LR 2.499680 Loss 19.062774, Accuracy 0.070%\n",
      "Epoch 12, Batch 212, LR 2.499677 Loss 19.062147, Accuracy 0.074%\n",
      "Epoch 12, Batch 213, LR 2.499674 Loss 19.061606, Accuracy 0.073%\n",
      "Epoch 12, Batch 214, LR 2.499671 Loss 19.062613, Accuracy 0.073%\n",
      "Epoch 12, Batch 215, LR 2.499668 Loss 19.062757, Accuracy 0.073%\n",
      "Epoch 12, Batch 216, LR 2.499665 Loss 19.063379, Accuracy 0.072%\n",
      "Epoch 12, Batch 217, LR 2.499662 Loss 19.064026, Accuracy 0.072%\n",
      "Epoch 12, Batch 218, LR 2.499659 Loss 19.064417, Accuracy 0.079%\n",
      "Epoch 12, Batch 219, LR 2.499656 Loss 19.064002, Accuracy 0.078%\n",
      "Epoch 12, Batch 220, LR 2.499653 Loss 19.064455, Accuracy 0.078%\n",
      "Epoch 12, Batch 221, LR 2.499649 Loss 19.064803, Accuracy 0.078%\n",
      "Epoch 12, Batch 222, LR 2.499646 Loss 19.064967, Accuracy 0.077%\n",
      "Epoch 12, Batch 223, LR 2.499643 Loss 19.065740, Accuracy 0.077%\n",
      "Epoch 12, Batch 224, LR 2.499640 Loss 19.065114, Accuracy 0.077%\n",
      "Epoch 12, Batch 225, LR 2.499637 Loss 19.064848, Accuracy 0.076%\n",
      "Epoch 12, Batch 226, LR 2.499633 Loss 19.065091, Accuracy 0.076%\n",
      "Epoch 12, Batch 227, LR 2.499630 Loss 19.065889, Accuracy 0.076%\n",
      "Epoch 12, Batch 228, LR 2.499627 Loss 19.065885, Accuracy 0.075%\n",
      "Epoch 12, Batch 229, LR 2.499624 Loss 19.065989, Accuracy 0.075%\n",
      "Epoch 12, Batch 230, LR 2.499620 Loss 19.066427, Accuracy 0.075%\n",
      "Epoch 12, Batch 231, LR 2.499617 Loss 19.066714, Accuracy 0.074%\n",
      "Epoch 12, Batch 232, LR 2.499614 Loss 19.067083, Accuracy 0.074%\n",
      "Epoch 12, Batch 233, LR 2.499610 Loss 19.067157, Accuracy 0.074%\n",
      "Epoch 12, Batch 234, LR 2.499607 Loss 19.066463, Accuracy 0.073%\n",
      "Epoch 12, Batch 235, LR 2.499604 Loss 19.066155, Accuracy 0.073%\n",
      "Epoch 12, Batch 236, LR 2.499600 Loss 19.067081, Accuracy 0.073%\n",
      "Epoch 12, Batch 237, LR 2.499597 Loss 19.067170, Accuracy 0.073%\n",
      "Epoch 12, Batch 238, LR 2.499593 Loss 19.067004, Accuracy 0.072%\n",
      "Epoch 12, Batch 239, LR 2.499590 Loss 19.067914, Accuracy 0.072%\n",
      "Epoch 12, Batch 240, LR 2.499587 Loss 19.068208, Accuracy 0.072%\n",
      "Epoch 12, Batch 241, LR 2.499583 Loss 19.068762, Accuracy 0.071%\n",
      "Epoch 12, Batch 242, LR 2.499580 Loss 19.069504, Accuracy 0.071%\n",
      "Epoch 12, Batch 243, LR 2.499576 Loss 19.069644, Accuracy 0.071%\n",
      "Epoch 12, Batch 244, LR 2.499573 Loss 19.069555, Accuracy 0.070%\n",
      "Epoch 12, Batch 245, LR 2.499569 Loss 19.069159, Accuracy 0.070%\n",
      "Epoch 12, Batch 246, LR 2.499566 Loss 19.069643, Accuracy 0.070%\n",
      "Epoch 12, Batch 247, LR 2.499562 Loss 19.069066, Accuracy 0.070%\n",
      "Epoch 12, Batch 248, LR 2.499559 Loss 19.068799, Accuracy 0.069%\n",
      "Epoch 12, Batch 249, LR 2.499555 Loss 19.069127, Accuracy 0.069%\n",
      "Epoch 12, Batch 250, LR 2.499551 Loss 19.069163, Accuracy 0.069%\n",
      "Epoch 12, Batch 251, LR 2.499548 Loss 19.068275, Accuracy 0.068%\n",
      "Epoch 12, Batch 252, LR 2.499544 Loss 19.068387, Accuracy 0.068%\n",
      "Epoch 12, Batch 253, LR 2.499541 Loss 19.067798, Accuracy 0.068%\n",
      "Epoch 12, Batch 254, LR 2.499537 Loss 19.067546, Accuracy 0.068%\n",
      "Epoch 12, Batch 255, LR 2.499533 Loss 19.067530, Accuracy 0.067%\n",
      "Epoch 12, Batch 256, LR 2.499530 Loss 19.067720, Accuracy 0.067%\n",
      "Epoch 12, Batch 257, LR 2.499526 Loss 19.067191, Accuracy 0.067%\n",
      "Epoch 12, Batch 258, LR 2.499522 Loss 19.067527, Accuracy 0.067%\n",
      "Epoch 12, Batch 259, LR 2.499519 Loss 19.067514, Accuracy 0.066%\n",
      "Epoch 12, Batch 260, LR 2.499515 Loss 19.067320, Accuracy 0.066%\n",
      "Epoch 12, Batch 261, LR 2.499511 Loss 19.067784, Accuracy 0.066%\n",
      "Epoch 12, Batch 262, LR 2.499507 Loss 19.067942, Accuracy 0.066%\n",
      "Epoch 12, Batch 263, LR 2.499504 Loss 19.067568, Accuracy 0.065%\n",
      "Epoch 12, Batch 264, LR 2.499500 Loss 19.067400, Accuracy 0.068%\n",
      "Epoch 12, Batch 265, LR 2.499496 Loss 19.067410, Accuracy 0.068%\n",
      "Epoch 12, Batch 266, LR 2.499492 Loss 19.067185, Accuracy 0.068%\n",
      "Epoch 12, Batch 267, LR 2.499488 Loss 19.067717, Accuracy 0.067%\n",
      "Epoch 12, Batch 268, LR 2.499485 Loss 19.068095, Accuracy 0.067%\n",
      "Epoch 12, Batch 269, LR 2.499481 Loss 19.067308, Accuracy 0.067%\n",
      "Epoch 12, Batch 270, LR 2.499477 Loss 19.066643, Accuracy 0.067%\n",
      "Epoch 12, Batch 271, LR 2.499473 Loss 19.066834, Accuracy 0.066%\n",
      "Epoch 12, Batch 272, LR 2.499469 Loss 19.066838, Accuracy 0.066%\n",
      "Epoch 12, Batch 273, LR 2.499465 Loss 19.066927, Accuracy 0.066%\n",
      "Epoch 12, Batch 274, LR 2.499461 Loss 19.066270, Accuracy 0.066%\n",
      "Epoch 12, Batch 275, LR 2.499457 Loss 19.065835, Accuracy 0.065%\n",
      "Epoch 12, Batch 276, LR 2.499453 Loss 19.065519, Accuracy 0.065%\n",
      "Epoch 12, Batch 277, LR 2.499449 Loss 19.065157, Accuracy 0.065%\n",
      "Epoch 12, Batch 278, LR 2.499445 Loss 19.064974, Accuracy 0.065%\n",
      "Epoch 12, Batch 279, LR 2.499441 Loss 19.065028, Accuracy 0.064%\n",
      "Epoch 12, Batch 280, LR 2.499437 Loss 19.065197, Accuracy 0.064%\n",
      "Epoch 12, Batch 281, LR 2.499433 Loss 19.064753, Accuracy 0.064%\n",
      "Epoch 12, Batch 282, LR 2.499429 Loss 19.065410, Accuracy 0.064%\n",
      "Epoch 12, Batch 283, LR 2.499425 Loss 19.065288, Accuracy 0.063%\n",
      "Epoch 12, Batch 284, LR 2.499421 Loss 19.065710, Accuracy 0.063%\n",
      "Epoch 12, Batch 285, LR 2.499417 Loss 19.065130, Accuracy 0.063%\n",
      "Epoch 12, Batch 286, LR 2.499413 Loss 19.064725, Accuracy 0.063%\n",
      "Epoch 12, Batch 287, LR 2.499409 Loss 19.064729, Accuracy 0.063%\n",
      "Epoch 12, Batch 288, LR 2.499405 Loss 19.065077, Accuracy 0.062%\n",
      "Epoch 12, Batch 289, LR 2.499401 Loss 19.065263, Accuracy 0.062%\n",
      "Epoch 12, Batch 290, LR 2.499396 Loss 19.065191, Accuracy 0.062%\n",
      "Epoch 12, Batch 291, LR 2.499392 Loss 19.064371, Accuracy 0.064%\n",
      "Epoch 12, Batch 292, LR 2.499388 Loss 19.064307, Accuracy 0.064%\n",
      "Epoch 12, Batch 293, LR 2.499384 Loss 19.064277, Accuracy 0.064%\n",
      "Epoch 12, Batch 294, LR 2.499380 Loss 19.064105, Accuracy 0.064%\n",
      "Epoch 12, Batch 295, LR 2.499375 Loss 19.063434, Accuracy 0.064%\n",
      "Epoch 12, Batch 296, LR 2.499371 Loss 19.063210, Accuracy 0.063%\n",
      "Epoch 12, Batch 297, LR 2.499367 Loss 19.062987, Accuracy 0.063%\n",
      "Epoch 12, Batch 298, LR 2.499363 Loss 19.063939, Accuracy 0.063%\n",
      "Epoch 12, Batch 299, LR 2.499358 Loss 19.064176, Accuracy 0.063%\n",
      "Epoch 12, Batch 300, LR 2.499354 Loss 19.064464, Accuracy 0.062%\n",
      "Epoch 12, Batch 301, LR 2.499350 Loss 19.064311, Accuracy 0.062%\n",
      "Epoch 12, Batch 302, LR 2.499345 Loss 19.064452, Accuracy 0.062%\n",
      "Epoch 12, Batch 303, LR 2.499341 Loss 19.064572, Accuracy 0.062%\n",
      "Epoch 12, Batch 304, LR 2.499337 Loss 19.064353, Accuracy 0.062%\n",
      "Epoch 12, Batch 305, LR 2.499332 Loss 19.064945, Accuracy 0.061%\n",
      "Epoch 12, Batch 306, LR 2.499328 Loss 19.065205, Accuracy 0.061%\n",
      "Epoch 12, Batch 307, LR 2.499324 Loss 19.065579, Accuracy 0.061%\n",
      "Epoch 12, Batch 308, LR 2.499319 Loss 19.066191, Accuracy 0.061%\n",
      "Epoch 12, Batch 309, LR 2.499315 Loss 19.066597, Accuracy 0.061%\n",
      "Epoch 12, Batch 310, LR 2.499310 Loss 19.066871, Accuracy 0.060%\n",
      "Epoch 12, Batch 311, LR 2.499306 Loss 19.066695, Accuracy 0.060%\n",
      "Epoch 12, Batch 312, LR 2.499301 Loss 19.066161, Accuracy 0.060%\n",
      "Epoch 12, Batch 313, LR 2.499297 Loss 19.065701, Accuracy 0.060%\n",
      "Epoch 12, Batch 314, LR 2.499292 Loss 19.064968, Accuracy 0.060%\n",
      "Epoch 12, Batch 315, LR 2.499288 Loss 19.064884, Accuracy 0.060%\n",
      "Epoch 12, Batch 316, LR 2.499283 Loss 19.065399, Accuracy 0.062%\n",
      "Epoch 12, Batch 317, LR 2.499279 Loss 19.065558, Accuracy 0.062%\n",
      "Epoch 12, Batch 318, LR 2.499274 Loss 19.065752, Accuracy 0.061%\n",
      "Epoch 12, Batch 319, LR 2.499270 Loss 19.066210, Accuracy 0.061%\n",
      "Epoch 12, Batch 320, LR 2.499265 Loss 19.065619, Accuracy 0.063%\n",
      "Epoch 12, Batch 321, LR 2.499261 Loss 19.065104, Accuracy 0.063%\n",
      "Epoch 12, Batch 322, LR 2.499256 Loss 19.065607, Accuracy 0.063%\n",
      "Epoch 12, Batch 323, LR 2.499251 Loss 19.065441, Accuracy 0.063%\n",
      "Epoch 12, Batch 324, LR 2.499247 Loss 19.064749, Accuracy 0.063%\n",
      "Epoch 12, Batch 325, LR 2.499242 Loss 19.065007, Accuracy 0.062%\n",
      "Epoch 12, Batch 326, LR 2.499237 Loss 19.065079, Accuracy 0.062%\n",
      "Epoch 12, Batch 327, LR 2.499233 Loss 19.065175, Accuracy 0.062%\n",
      "Epoch 12, Batch 328, LR 2.499228 Loss 19.064576, Accuracy 0.062%\n",
      "Epoch 12, Batch 329, LR 2.499223 Loss 19.064530, Accuracy 0.062%\n",
      "Epoch 12, Batch 330, LR 2.499218 Loss 19.064411, Accuracy 0.062%\n",
      "Epoch 12, Batch 331, LR 2.499214 Loss 19.063314, Accuracy 0.064%\n",
      "Epoch 12, Batch 332, LR 2.499209 Loss 19.062787, Accuracy 0.064%\n",
      "Epoch 12, Batch 333, LR 2.499204 Loss 19.062130, Accuracy 0.063%\n",
      "Epoch 12, Batch 334, LR 2.499199 Loss 19.061516, Accuracy 0.063%\n",
      "Epoch 12, Batch 335, LR 2.499195 Loss 19.061423, Accuracy 0.063%\n",
      "Epoch 12, Batch 336, LR 2.499190 Loss 19.061805, Accuracy 0.063%\n",
      "Epoch 12, Batch 337, LR 2.499185 Loss 19.061496, Accuracy 0.063%\n",
      "Epoch 12, Batch 338, LR 2.499180 Loss 19.061331, Accuracy 0.062%\n",
      "Epoch 12, Batch 339, LR 2.499175 Loss 19.061415, Accuracy 0.062%\n",
      "Epoch 12, Batch 340, LR 2.499170 Loss 19.061170, Accuracy 0.062%\n",
      "Epoch 12, Batch 341, LR 2.499165 Loss 19.061397, Accuracy 0.062%\n",
      "Epoch 12, Batch 342, LR 2.499161 Loss 19.061506, Accuracy 0.062%\n",
      "Epoch 12, Batch 343, LR 2.499156 Loss 19.061248, Accuracy 0.061%\n",
      "Epoch 12, Batch 344, LR 2.499151 Loss 19.060978, Accuracy 0.061%\n",
      "Epoch 12, Batch 345, LR 2.499146 Loss 19.060976, Accuracy 0.061%\n",
      "Epoch 12, Batch 346, LR 2.499141 Loss 19.061151, Accuracy 0.061%\n",
      "Epoch 12, Batch 347, LR 2.499136 Loss 19.060766, Accuracy 0.063%\n",
      "Epoch 12, Batch 348, LR 2.499131 Loss 19.060189, Accuracy 0.063%\n",
      "Epoch 12, Batch 349, LR 2.499126 Loss 19.060142, Accuracy 0.063%\n",
      "Epoch 12, Batch 350, LR 2.499121 Loss 19.059951, Accuracy 0.062%\n",
      "Epoch 12, Batch 351, LR 2.499116 Loss 19.060580, Accuracy 0.062%\n",
      "Epoch 12, Batch 352, LR 2.499111 Loss 19.060626, Accuracy 0.062%\n",
      "Epoch 12, Batch 353, LR 2.499106 Loss 19.060582, Accuracy 0.062%\n",
      "Epoch 12, Batch 354, LR 2.499101 Loss 19.060381, Accuracy 0.062%\n",
      "Epoch 12, Batch 355, LR 2.499096 Loss 19.061181, Accuracy 0.062%\n",
      "Epoch 12, Batch 356, LR 2.499090 Loss 19.061301, Accuracy 0.061%\n",
      "Epoch 12, Batch 357, LR 2.499085 Loss 19.061535, Accuracy 0.061%\n",
      "Epoch 12, Batch 358, LR 2.499080 Loss 19.061518, Accuracy 0.061%\n",
      "Epoch 12, Batch 359, LR 2.499075 Loss 19.061887, Accuracy 0.061%\n",
      "Epoch 12, Batch 360, LR 2.499070 Loss 19.062047, Accuracy 0.061%\n",
      "Epoch 12, Batch 361, LR 2.499065 Loss 19.061916, Accuracy 0.061%\n",
      "Epoch 12, Batch 362, LR 2.499060 Loss 19.062309, Accuracy 0.060%\n",
      "Epoch 12, Batch 363, LR 2.499054 Loss 19.062375, Accuracy 0.060%\n",
      "Epoch 12, Batch 364, LR 2.499049 Loss 19.061738, Accuracy 0.062%\n",
      "Epoch 12, Batch 365, LR 2.499044 Loss 19.061517, Accuracy 0.062%\n",
      "Epoch 12, Batch 366, LR 2.499039 Loss 19.061321, Accuracy 0.062%\n",
      "Epoch 12, Batch 367, LR 2.499033 Loss 19.061477, Accuracy 0.062%\n",
      "Epoch 12, Batch 368, LR 2.499028 Loss 19.061251, Accuracy 0.062%\n",
      "Epoch 12, Batch 369, LR 2.499023 Loss 19.061222, Accuracy 0.061%\n",
      "Epoch 12, Batch 370, LR 2.499018 Loss 19.060674, Accuracy 0.061%\n",
      "Epoch 12, Batch 371, LR 2.499012 Loss 19.060575, Accuracy 0.061%\n",
      "Epoch 12, Batch 372, LR 2.499007 Loss 19.060348, Accuracy 0.063%\n",
      "Epoch 12, Batch 373, LR 2.499002 Loss 19.060656, Accuracy 0.063%\n",
      "Epoch 12, Batch 374, LR 2.498996 Loss 19.060351, Accuracy 0.063%\n",
      "Epoch 12, Batch 375, LR 2.498991 Loss 19.060259, Accuracy 0.062%\n",
      "Epoch 12, Batch 376, LR 2.498985 Loss 19.060270, Accuracy 0.062%\n",
      "Epoch 12, Batch 377, LR 2.498980 Loss 19.060576, Accuracy 0.062%\n",
      "Epoch 12, Batch 378, LR 2.498975 Loss 19.060389, Accuracy 0.062%\n",
      "Epoch 12, Batch 379, LR 2.498969 Loss 19.059847, Accuracy 0.062%\n",
      "Epoch 12, Batch 380, LR 2.498964 Loss 19.059421, Accuracy 0.062%\n",
      "Epoch 12, Batch 381, LR 2.498958 Loss 19.059433, Accuracy 0.062%\n",
      "Epoch 12, Batch 382, LR 2.498953 Loss 19.059540, Accuracy 0.061%\n",
      "Epoch 12, Batch 383, LR 2.498947 Loss 19.059688, Accuracy 0.061%\n",
      "Epoch 12, Batch 384, LR 2.498942 Loss 19.060092, Accuracy 0.061%\n",
      "Epoch 12, Batch 385, LR 2.498936 Loss 19.060007, Accuracy 0.061%\n",
      "Epoch 12, Batch 386, LR 2.498931 Loss 19.060198, Accuracy 0.061%\n",
      "Epoch 12, Batch 387, LR 2.498925 Loss 19.060447, Accuracy 0.063%\n",
      "Epoch 12, Batch 388, LR 2.498920 Loss 19.060972, Accuracy 0.062%\n",
      "Epoch 12, Batch 389, LR 2.498914 Loss 19.061253, Accuracy 0.062%\n",
      "Epoch 12, Batch 390, LR 2.498908 Loss 19.060685, Accuracy 0.064%\n",
      "Epoch 12, Batch 391, LR 2.498903 Loss 19.060966, Accuracy 0.064%\n",
      "Epoch 12, Batch 392, LR 2.498897 Loss 19.060883, Accuracy 0.064%\n",
      "Epoch 12, Batch 393, LR 2.498892 Loss 19.060816, Accuracy 0.064%\n",
      "Epoch 12, Batch 394, LR 2.498886 Loss 19.060968, Accuracy 0.063%\n",
      "Epoch 12, Batch 395, LR 2.498880 Loss 19.060918, Accuracy 0.063%\n",
      "Epoch 12, Batch 396, LR 2.498875 Loss 19.061244, Accuracy 0.063%\n",
      "Epoch 12, Batch 397, LR 2.498869 Loss 19.062139, Accuracy 0.063%\n",
      "Epoch 12, Batch 398, LR 2.498863 Loss 19.062020, Accuracy 0.063%\n",
      "Epoch 12, Batch 399, LR 2.498858 Loss 19.061717, Accuracy 0.063%\n",
      "Epoch 12, Batch 400, LR 2.498852 Loss 19.061402, Accuracy 0.062%\n",
      "Epoch 12, Batch 401, LR 2.498846 Loss 19.061343, Accuracy 0.062%\n",
      "Epoch 12, Batch 402, LR 2.498840 Loss 19.061153, Accuracy 0.062%\n",
      "Epoch 12, Batch 403, LR 2.498835 Loss 19.060964, Accuracy 0.062%\n",
      "Epoch 12, Batch 404, LR 2.498829 Loss 19.061084, Accuracy 0.062%\n",
      "Epoch 12, Batch 405, LR 2.498823 Loss 19.060385, Accuracy 0.062%\n",
      "Epoch 12, Batch 406, LR 2.498817 Loss 19.060779, Accuracy 0.062%\n",
      "Epoch 12, Batch 407, LR 2.498811 Loss 19.060947, Accuracy 0.061%\n",
      "Epoch 12, Batch 408, LR 2.498805 Loss 19.061184, Accuracy 0.061%\n",
      "Epoch 12, Batch 409, LR 2.498800 Loss 19.061289, Accuracy 0.061%\n",
      "Epoch 12, Batch 410, LR 2.498794 Loss 19.061492, Accuracy 0.061%\n",
      "Epoch 12, Batch 411, LR 2.498788 Loss 19.061465, Accuracy 0.061%\n",
      "Epoch 12, Batch 412, LR 2.498782 Loss 19.061873, Accuracy 0.061%\n",
      "Epoch 12, Batch 413, LR 2.498776 Loss 19.061837, Accuracy 0.061%\n",
      "Epoch 12, Batch 414, LR 2.498770 Loss 19.062062, Accuracy 0.060%\n",
      "Epoch 12, Batch 415, LR 2.498764 Loss 19.061390, Accuracy 0.060%\n",
      "Epoch 12, Batch 416, LR 2.498758 Loss 19.061549, Accuracy 0.060%\n",
      "Epoch 12, Batch 417, LR 2.498752 Loss 19.061562, Accuracy 0.060%\n",
      "Epoch 12, Batch 418, LR 2.498746 Loss 19.061028, Accuracy 0.060%\n",
      "Epoch 12, Batch 419, LR 2.498740 Loss 19.060729, Accuracy 0.063%\n",
      "Epoch 12, Batch 420, LR 2.498734 Loss 19.060581, Accuracy 0.063%\n",
      "Epoch 12, Batch 421, LR 2.498728 Loss 19.060831, Accuracy 0.063%\n",
      "Epoch 12, Batch 422, LR 2.498722 Loss 19.061030, Accuracy 0.063%\n",
      "Epoch 12, Batch 423, LR 2.498716 Loss 19.061176, Accuracy 0.063%\n",
      "Epoch 12, Batch 424, LR 2.498710 Loss 19.061282, Accuracy 0.063%\n",
      "Epoch 12, Batch 425, LR 2.498704 Loss 19.061142, Accuracy 0.062%\n",
      "Epoch 12, Batch 426, LR 2.498698 Loss 19.061187, Accuracy 0.062%\n",
      "Epoch 12, Batch 427, LR 2.498692 Loss 19.061375, Accuracy 0.062%\n",
      "Epoch 12, Batch 428, LR 2.498685 Loss 19.061553, Accuracy 0.062%\n",
      "Epoch 12, Batch 429, LR 2.498679 Loss 19.061672, Accuracy 0.062%\n",
      "Epoch 12, Batch 430, LR 2.498673 Loss 19.061993, Accuracy 0.062%\n",
      "Epoch 12, Batch 431, LR 2.498667 Loss 19.062065, Accuracy 0.062%\n",
      "Epoch 12, Batch 432, LR 2.498661 Loss 19.062194, Accuracy 0.063%\n",
      "Epoch 12, Batch 433, LR 2.498655 Loss 19.062345, Accuracy 0.063%\n",
      "Epoch 12, Batch 434, LR 2.498648 Loss 19.061855, Accuracy 0.063%\n",
      "Epoch 12, Batch 435, LR 2.498642 Loss 19.062012, Accuracy 0.063%\n",
      "Epoch 12, Batch 436, LR 2.498636 Loss 19.062342, Accuracy 0.063%\n",
      "Epoch 12, Batch 437, LR 2.498630 Loss 19.062291, Accuracy 0.063%\n",
      "Epoch 12, Batch 438, LR 2.498623 Loss 19.062434, Accuracy 0.062%\n",
      "Epoch 12, Batch 439, LR 2.498617 Loss 19.062433, Accuracy 0.062%\n",
      "Epoch 12, Batch 440, LR 2.498611 Loss 19.062555, Accuracy 0.062%\n",
      "Epoch 12, Batch 441, LR 2.498604 Loss 19.062501, Accuracy 0.062%\n",
      "Epoch 12, Batch 442, LR 2.498598 Loss 19.062432, Accuracy 0.062%\n",
      "Epoch 12, Batch 443, LR 2.498592 Loss 19.062402, Accuracy 0.062%\n",
      "Epoch 12, Batch 444, LR 2.498585 Loss 19.062385, Accuracy 0.062%\n",
      "Epoch 12, Batch 445, LR 2.498579 Loss 19.061976, Accuracy 0.061%\n",
      "Epoch 12, Batch 446, LR 2.498573 Loss 19.061813, Accuracy 0.061%\n",
      "Epoch 12, Batch 447, LR 2.498566 Loss 19.061430, Accuracy 0.061%\n",
      "Epoch 12, Batch 448, LR 2.498560 Loss 19.061329, Accuracy 0.061%\n",
      "Epoch 12, Batch 449, LR 2.498553 Loss 19.061573, Accuracy 0.063%\n",
      "Epoch 12, Batch 450, LR 2.498547 Loss 19.061605, Accuracy 0.062%\n",
      "Epoch 12, Batch 451, LR 2.498540 Loss 19.061851, Accuracy 0.062%\n",
      "Epoch 12, Batch 452, LR 2.498534 Loss 19.061807, Accuracy 0.064%\n",
      "Epoch 12, Batch 453, LR 2.498527 Loss 19.062048, Accuracy 0.064%\n",
      "Epoch 12, Batch 454, LR 2.498521 Loss 19.062374, Accuracy 0.064%\n",
      "Epoch 12, Batch 455, LR 2.498514 Loss 19.062343, Accuracy 0.065%\n",
      "Epoch 12, Batch 456, LR 2.498508 Loss 19.062588, Accuracy 0.065%\n",
      "Epoch 12, Batch 457, LR 2.498501 Loss 19.062781, Accuracy 0.065%\n",
      "Epoch 12, Batch 458, LR 2.498495 Loss 19.063239, Accuracy 0.065%\n",
      "Epoch 12, Batch 459, LR 2.498488 Loss 19.063388, Accuracy 0.065%\n",
      "Epoch 12, Batch 460, LR 2.498482 Loss 19.063524, Accuracy 0.065%\n",
      "Epoch 12, Batch 461, LR 2.498475 Loss 19.063615, Accuracy 0.064%\n",
      "Epoch 12, Batch 462, LR 2.498468 Loss 19.063256, Accuracy 0.064%\n",
      "Epoch 12, Batch 463, LR 2.498462 Loss 19.063234, Accuracy 0.064%\n",
      "Epoch 12, Batch 464, LR 2.498455 Loss 19.063231, Accuracy 0.066%\n",
      "Epoch 12, Batch 465, LR 2.498448 Loss 19.063005, Accuracy 0.066%\n",
      "Epoch 12, Batch 466, LR 2.498442 Loss 19.062779, Accuracy 0.067%\n",
      "Epoch 12, Batch 467, LR 2.498435 Loss 19.062958, Accuracy 0.067%\n",
      "Epoch 12, Batch 468, LR 2.498428 Loss 19.063398, Accuracy 0.067%\n",
      "Epoch 12, Batch 469, LR 2.498422 Loss 19.063017, Accuracy 0.067%\n",
      "Epoch 12, Batch 470, LR 2.498415 Loss 19.062729, Accuracy 0.066%\n",
      "Epoch 12, Batch 471, LR 2.498408 Loss 19.062456, Accuracy 0.066%\n",
      "Epoch 12, Batch 472, LR 2.498401 Loss 19.062872, Accuracy 0.066%\n",
      "Epoch 12, Batch 473, LR 2.498395 Loss 19.062331, Accuracy 0.066%\n",
      "Epoch 12, Batch 474, LR 2.498388 Loss 19.062093, Accuracy 0.066%\n",
      "Epoch 12, Batch 475, LR 2.498381 Loss 19.062021, Accuracy 0.067%\n",
      "Epoch 12, Batch 476, LR 2.498374 Loss 19.061951, Accuracy 0.067%\n",
      "Epoch 12, Batch 477, LR 2.498367 Loss 19.061981, Accuracy 0.067%\n",
      "Epoch 12, Batch 478, LR 2.498360 Loss 19.061839, Accuracy 0.067%\n",
      "Epoch 12, Batch 479, LR 2.498354 Loss 19.061597, Accuracy 0.067%\n",
      "Epoch 12, Batch 480, LR 2.498347 Loss 19.061376, Accuracy 0.067%\n",
      "Epoch 12, Batch 481, LR 2.498340 Loss 19.061387, Accuracy 0.067%\n",
      "Epoch 12, Batch 482, LR 2.498333 Loss 19.061782, Accuracy 0.068%\n",
      "Epoch 12, Batch 483, LR 2.498326 Loss 19.061838, Accuracy 0.068%\n",
      "Epoch 12, Batch 484, LR 2.498319 Loss 19.061722, Accuracy 0.068%\n",
      "Epoch 12, Batch 485, LR 2.498312 Loss 19.061910, Accuracy 0.068%\n",
      "Epoch 12, Batch 486, LR 2.498305 Loss 19.062097, Accuracy 0.068%\n",
      "Epoch 12, Batch 487, LR 2.498298 Loss 19.062460, Accuracy 0.067%\n",
      "Epoch 12, Batch 488, LR 2.498291 Loss 19.061801, Accuracy 0.067%\n",
      "Epoch 12, Batch 489, LR 2.498284 Loss 19.062064, Accuracy 0.067%\n",
      "Epoch 12, Batch 490, LR 2.498277 Loss 19.062274, Accuracy 0.067%\n",
      "Epoch 12, Batch 491, LR 2.498270 Loss 19.062407, Accuracy 0.067%\n",
      "Epoch 12, Batch 492, LR 2.498263 Loss 19.061861, Accuracy 0.067%\n",
      "Epoch 12, Batch 493, LR 2.498256 Loss 19.062202, Accuracy 0.067%\n",
      "Epoch 12, Batch 494, LR 2.498249 Loss 19.062230, Accuracy 0.068%\n",
      "Epoch 12, Batch 495, LR 2.498242 Loss 19.062168, Accuracy 0.068%\n",
      "Epoch 12, Batch 496, LR 2.498235 Loss 19.062130, Accuracy 0.068%\n",
      "Epoch 12, Batch 497, LR 2.498228 Loss 19.062371, Accuracy 0.068%\n",
      "Epoch 12, Batch 498, LR 2.498220 Loss 19.062389, Accuracy 0.067%\n",
      "Epoch 12, Batch 499, LR 2.498213 Loss 19.062296, Accuracy 0.067%\n",
      "Epoch 12, Batch 500, LR 2.498206 Loss 19.062184, Accuracy 0.067%\n",
      "Epoch 12, Batch 501, LR 2.498199 Loss 19.061848, Accuracy 0.067%\n",
      "Epoch 12, Batch 502, LR 2.498192 Loss 19.061816, Accuracy 0.067%\n",
      "Epoch 12, Batch 503, LR 2.498184 Loss 19.061755, Accuracy 0.067%\n",
      "Epoch 12, Batch 504, LR 2.498177 Loss 19.061703, Accuracy 0.067%\n",
      "Epoch 12, Batch 505, LR 2.498170 Loss 19.062286, Accuracy 0.067%\n",
      "Epoch 12, Batch 506, LR 2.498163 Loss 19.062546, Accuracy 0.066%\n",
      "Epoch 12, Batch 507, LR 2.498156 Loss 19.062877, Accuracy 0.066%\n",
      "Epoch 12, Batch 508, LR 2.498148 Loss 19.062753, Accuracy 0.066%\n",
      "Epoch 12, Batch 509, LR 2.498141 Loss 19.062406, Accuracy 0.066%\n",
      "Epoch 12, Batch 510, LR 2.498134 Loss 19.062382, Accuracy 0.066%\n",
      "Epoch 12, Batch 511, LR 2.498126 Loss 19.062384, Accuracy 0.066%\n",
      "Epoch 12, Batch 512, LR 2.498119 Loss 19.062186, Accuracy 0.066%\n",
      "Epoch 12, Batch 513, LR 2.498112 Loss 19.062038, Accuracy 0.065%\n",
      "Epoch 12, Batch 514, LR 2.498104 Loss 19.062272, Accuracy 0.065%\n",
      "Epoch 12, Batch 515, LR 2.498097 Loss 19.062355, Accuracy 0.065%\n",
      "Epoch 12, Batch 516, LR 2.498089 Loss 19.062379, Accuracy 0.065%\n",
      "Epoch 12, Batch 517, LR 2.498082 Loss 19.062556, Accuracy 0.065%\n",
      "Epoch 12, Batch 518, LR 2.498075 Loss 19.062534, Accuracy 0.065%\n",
      "Epoch 12, Batch 519, LR 2.498067 Loss 19.063084, Accuracy 0.065%\n",
      "Epoch 12, Batch 520, LR 2.498060 Loss 19.062956, Accuracy 0.065%\n",
      "Epoch 12, Batch 521, LR 2.498052 Loss 19.062777, Accuracy 0.064%\n",
      "Epoch 12, Batch 522, LR 2.498045 Loss 19.063122, Accuracy 0.064%\n",
      "Epoch 12, Batch 523, LR 2.498037 Loss 19.062997, Accuracy 0.064%\n",
      "Epoch 12, Batch 524, LR 2.498030 Loss 19.063128, Accuracy 0.064%\n",
      "Epoch 12, Batch 525, LR 2.498022 Loss 19.063193, Accuracy 0.064%\n",
      "Epoch 12, Batch 526, LR 2.498015 Loss 19.063202, Accuracy 0.064%\n",
      "Epoch 12, Batch 527, LR 2.498007 Loss 19.063097, Accuracy 0.064%\n",
      "Epoch 12, Batch 528, LR 2.498000 Loss 19.063106, Accuracy 0.064%\n",
      "Epoch 12, Batch 529, LR 2.497992 Loss 19.062854, Accuracy 0.065%\n",
      "Epoch 12, Batch 530, LR 2.497984 Loss 19.062548, Accuracy 0.065%\n",
      "Epoch 12, Batch 531, LR 2.497977 Loss 19.062639, Accuracy 0.065%\n",
      "Epoch 12, Batch 532, LR 2.497969 Loss 19.062865, Accuracy 0.065%\n",
      "Epoch 12, Batch 533, LR 2.497962 Loss 19.062979, Accuracy 0.064%\n",
      "Epoch 12, Batch 534, LR 2.497954 Loss 19.063073, Accuracy 0.064%\n",
      "Epoch 12, Batch 535, LR 2.497946 Loss 19.062952, Accuracy 0.064%\n",
      "Epoch 12, Batch 536, LR 2.497939 Loss 19.062643, Accuracy 0.064%\n",
      "Epoch 12, Batch 537, LR 2.497931 Loss 19.062345, Accuracy 0.064%\n",
      "Epoch 12, Batch 538, LR 2.497923 Loss 19.062390, Accuracy 0.064%\n",
      "Epoch 12, Batch 539, LR 2.497915 Loss 19.062625, Accuracy 0.064%\n",
      "Epoch 12, Batch 540, LR 2.497908 Loss 19.062809, Accuracy 0.064%\n",
      "Epoch 12, Batch 541, LR 2.497900 Loss 19.062795, Accuracy 0.064%\n",
      "Epoch 12, Batch 542, LR 2.497892 Loss 19.062949, Accuracy 0.063%\n",
      "Epoch 12, Batch 543, LR 2.497884 Loss 19.063123, Accuracy 0.063%\n",
      "Epoch 12, Batch 544, LR 2.497877 Loss 19.063173, Accuracy 0.063%\n",
      "Epoch 12, Batch 545, LR 2.497869 Loss 19.063550, Accuracy 0.065%\n",
      "Epoch 12, Batch 546, LR 2.497861 Loss 19.063550, Accuracy 0.064%\n",
      "Epoch 12, Batch 547, LR 2.497853 Loss 19.063737, Accuracy 0.064%\n",
      "Epoch 12, Batch 548, LR 2.497845 Loss 19.063534, Accuracy 0.064%\n",
      "Epoch 12, Batch 549, LR 2.497837 Loss 19.063632, Accuracy 0.064%\n",
      "Epoch 12, Batch 550, LR 2.497829 Loss 19.063759, Accuracy 0.064%\n",
      "Epoch 12, Batch 551, LR 2.497822 Loss 19.063682, Accuracy 0.064%\n",
      "Epoch 12, Batch 552, LR 2.497814 Loss 19.063725, Accuracy 0.064%\n",
      "Epoch 12, Batch 553, LR 2.497806 Loss 19.063880, Accuracy 0.064%\n",
      "Epoch 12, Batch 554, LR 2.497798 Loss 19.064068, Accuracy 0.063%\n",
      "Epoch 12, Batch 555, LR 2.497790 Loss 19.063936, Accuracy 0.063%\n",
      "Epoch 12, Batch 556, LR 2.497782 Loss 19.064402, Accuracy 0.063%\n",
      "Epoch 12, Batch 557, LR 2.497774 Loss 19.064384, Accuracy 0.063%\n",
      "Epoch 12, Batch 558, LR 2.497766 Loss 19.064148, Accuracy 0.063%\n",
      "Epoch 12, Batch 559, LR 2.497758 Loss 19.064269, Accuracy 0.063%\n",
      "Epoch 12, Batch 560, LR 2.497750 Loss 19.064869, Accuracy 0.063%\n",
      "Epoch 12, Batch 561, LR 2.497742 Loss 19.064706, Accuracy 0.063%\n",
      "Epoch 12, Batch 562, LR 2.497734 Loss 19.064331, Accuracy 0.063%\n",
      "Epoch 12, Batch 563, LR 2.497726 Loss 19.064457, Accuracy 0.064%\n",
      "Epoch 12, Batch 564, LR 2.497718 Loss 19.064089, Accuracy 0.064%\n",
      "Epoch 12, Batch 565, LR 2.497709 Loss 19.064302, Accuracy 0.064%\n",
      "Epoch 12, Batch 566, LR 2.497701 Loss 19.064242, Accuracy 0.063%\n",
      "Epoch 12, Batch 567, LR 2.497693 Loss 19.064547, Accuracy 0.063%\n",
      "Epoch 12, Batch 568, LR 2.497685 Loss 19.064482, Accuracy 0.063%\n",
      "Epoch 12, Batch 569, LR 2.497677 Loss 19.064564, Accuracy 0.063%\n",
      "Epoch 12, Batch 570, LR 2.497669 Loss 19.064782, Accuracy 0.063%\n",
      "Epoch 12, Batch 571, LR 2.497661 Loss 19.064831, Accuracy 0.063%\n",
      "Epoch 12, Batch 572, LR 2.497652 Loss 19.064958, Accuracy 0.063%\n",
      "Epoch 12, Batch 573, LR 2.497644 Loss 19.064677, Accuracy 0.063%\n",
      "Epoch 12, Batch 574, LR 2.497636 Loss 19.064538, Accuracy 0.063%\n",
      "Epoch 12, Batch 575, LR 2.497628 Loss 19.064391, Accuracy 0.062%\n",
      "Epoch 12, Batch 576, LR 2.497619 Loss 19.064636, Accuracy 0.062%\n",
      "Epoch 12, Batch 577, LR 2.497611 Loss 19.064713, Accuracy 0.062%\n",
      "Epoch 12, Batch 578, LR 2.497603 Loss 19.064587, Accuracy 0.062%\n",
      "Epoch 12, Batch 579, LR 2.497595 Loss 19.064730, Accuracy 0.062%\n",
      "Epoch 12, Batch 580, LR 2.497586 Loss 19.064346, Accuracy 0.063%\n",
      "Epoch 12, Batch 581, LR 2.497578 Loss 19.064345, Accuracy 0.063%\n",
      "Epoch 12, Batch 582, LR 2.497570 Loss 19.064404, Accuracy 0.063%\n",
      "Epoch 12, Batch 583, LR 2.497561 Loss 19.064586, Accuracy 0.063%\n",
      "Epoch 12, Batch 584, LR 2.497553 Loss 19.064196, Accuracy 0.063%\n",
      "Epoch 12, Batch 585, LR 2.497545 Loss 19.064171, Accuracy 0.063%\n",
      "Epoch 12, Batch 586, LR 2.497536 Loss 19.064051, Accuracy 0.063%\n",
      "Epoch 12, Batch 587, LR 2.497528 Loss 19.064096, Accuracy 0.063%\n",
      "Epoch 12, Batch 588, LR 2.497519 Loss 19.063972, Accuracy 0.062%\n",
      "Epoch 12, Batch 589, LR 2.497511 Loss 19.063603, Accuracy 0.062%\n",
      "Epoch 12, Batch 590, LR 2.497502 Loss 19.063507, Accuracy 0.062%\n",
      "Epoch 12, Batch 591, LR 2.497494 Loss 19.063583, Accuracy 0.062%\n",
      "Epoch 12, Batch 592, LR 2.497485 Loss 19.063701, Accuracy 0.062%\n",
      "Epoch 12, Batch 593, LR 2.497477 Loss 19.063949, Accuracy 0.062%\n",
      "Epoch 12, Batch 594, LR 2.497468 Loss 19.063938, Accuracy 0.062%\n",
      "Epoch 12, Batch 595, LR 2.497460 Loss 19.063773, Accuracy 0.062%\n",
      "Epoch 12, Batch 596, LR 2.497451 Loss 19.063854, Accuracy 0.062%\n",
      "Epoch 12, Batch 597, LR 2.497443 Loss 19.064120, Accuracy 0.062%\n",
      "Epoch 12, Batch 598, LR 2.497434 Loss 19.064274, Accuracy 0.061%\n",
      "Epoch 12, Batch 599, LR 2.497426 Loss 19.064355, Accuracy 0.063%\n",
      "Epoch 12, Batch 600, LR 2.497417 Loss 19.064227, Accuracy 0.062%\n",
      "Epoch 12, Batch 601, LR 2.497408 Loss 19.064260, Accuracy 0.062%\n",
      "Epoch 12, Batch 602, LR 2.497400 Loss 19.064416, Accuracy 0.062%\n",
      "Epoch 12, Batch 603, LR 2.497391 Loss 19.064077, Accuracy 0.063%\n",
      "Epoch 12, Batch 604, LR 2.497382 Loss 19.064165, Accuracy 0.063%\n",
      "Epoch 12, Batch 605, LR 2.497374 Loss 19.063964, Accuracy 0.063%\n",
      "Epoch 12, Batch 606, LR 2.497365 Loss 19.064274, Accuracy 0.063%\n",
      "Epoch 12, Batch 607, LR 2.497356 Loss 19.064240, Accuracy 0.063%\n",
      "Epoch 12, Batch 608, LR 2.497348 Loss 19.063814, Accuracy 0.063%\n",
      "Epoch 12, Batch 609, LR 2.497339 Loss 19.063889, Accuracy 0.063%\n",
      "Epoch 12, Batch 610, LR 2.497330 Loss 19.063486, Accuracy 0.063%\n",
      "Epoch 12, Batch 611, LR 2.497321 Loss 19.063515, Accuracy 0.063%\n",
      "Epoch 12, Batch 612, LR 2.497313 Loss 19.063464, Accuracy 0.063%\n",
      "Epoch 12, Batch 613, LR 2.497304 Loss 19.063124, Accuracy 0.062%\n",
      "Epoch 12, Batch 614, LR 2.497295 Loss 19.063534, Accuracy 0.062%\n",
      "Epoch 12, Batch 615, LR 2.497286 Loss 19.063473, Accuracy 0.062%\n",
      "Epoch 12, Batch 616, LR 2.497277 Loss 19.063211, Accuracy 0.062%\n",
      "Epoch 12, Batch 617, LR 2.497269 Loss 19.063461, Accuracy 0.062%\n",
      "Epoch 12, Batch 618, LR 2.497260 Loss 19.063563, Accuracy 0.062%\n",
      "Epoch 12, Batch 619, LR 2.497251 Loss 19.063606, Accuracy 0.062%\n",
      "Epoch 12, Batch 620, LR 2.497242 Loss 19.063388, Accuracy 0.062%\n",
      "Epoch 12, Batch 621, LR 2.497233 Loss 19.063584, Accuracy 0.062%\n",
      "Epoch 12, Batch 622, LR 2.497224 Loss 19.063655, Accuracy 0.062%\n",
      "Epoch 12, Batch 623, LR 2.497215 Loss 19.063891, Accuracy 0.063%\n",
      "Epoch 12, Batch 624, LR 2.497206 Loss 19.063943, Accuracy 0.063%\n",
      "Epoch 12, Batch 625, LR 2.497197 Loss 19.063776, Accuracy 0.062%\n",
      "Epoch 12, Batch 626, LR 2.497188 Loss 19.063696, Accuracy 0.062%\n",
      "Epoch 12, Batch 627, LR 2.497179 Loss 19.063530, Accuracy 0.062%\n",
      "Epoch 12, Batch 628, LR 2.497170 Loss 19.063644, Accuracy 0.062%\n",
      "Epoch 12, Batch 629, LR 2.497161 Loss 19.063424, Accuracy 0.062%\n",
      "Epoch 12, Batch 630, LR 2.497152 Loss 19.063356, Accuracy 0.063%\n",
      "Epoch 12, Batch 631, LR 2.497143 Loss 19.063040, Accuracy 0.063%\n",
      "Epoch 12, Batch 632, LR 2.497134 Loss 19.062935, Accuracy 0.063%\n",
      "Epoch 12, Batch 633, LR 2.497125 Loss 19.063130, Accuracy 0.063%\n",
      "Epoch 12, Batch 634, LR 2.497116 Loss 19.063102, Accuracy 0.063%\n",
      "Epoch 12, Batch 635, LR 2.497107 Loss 19.063158, Accuracy 0.064%\n",
      "Epoch 12, Batch 636, LR 2.497098 Loss 19.063362, Accuracy 0.064%\n",
      "Epoch 12, Batch 637, LR 2.497089 Loss 19.063200, Accuracy 0.065%\n",
      "Epoch 12, Batch 638, LR 2.497080 Loss 19.063399, Accuracy 0.065%\n",
      "Epoch 12, Batch 639, LR 2.497070 Loss 19.063382, Accuracy 0.065%\n",
      "Epoch 12, Batch 640, LR 2.497061 Loss 19.063472, Accuracy 0.065%\n",
      "Epoch 12, Batch 641, LR 2.497052 Loss 19.063241, Accuracy 0.065%\n",
      "Epoch 12, Batch 642, LR 2.497043 Loss 19.063457, Accuracy 0.064%\n",
      "Epoch 12, Batch 643, LR 2.497034 Loss 19.063652, Accuracy 0.064%\n",
      "Epoch 12, Batch 644, LR 2.497024 Loss 19.063869, Accuracy 0.066%\n",
      "Epoch 12, Batch 645, LR 2.497015 Loss 19.063853, Accuracy 0.065%\n",
      "Epoch 12, Batch 646, LR 2.497006 Loss 19.063591, Accuracy 0.065%\n",
      "Epoch 12, Batch 647, LR 2.496997 Loss 19.063553, Accuracy 0.065%\n",
      "Epoch 12, Batch 648, LR 2.496987 Loss 19.063498, Accuracy 0.065%\n",
      "Epoch 12, Batch 649, LR 2.496978 Loss 19.063368, Accuracy 0.065%\n",
      "Epoch 12, Batch 650, LR 2.496969 Loss 19.063287, Accuracy 0.065%\n",
      "Epoch 12, Batch 651, LR 2.496959 Loss 19.063482, Accuracy 0.065%\n",
      "Epoch 12, Batch 652, LR 2.496950 Loss 19.063426, Accuracy 0.065%\n",
      "Epoch 12, Batch 653, LR 2.496941 Loss 19.063570, Accuracy 0.065%\n",
      "Epoch 12, Batch 654, LR 2.496931 Loss 19.064064, Accuracy 0.065%\n",
      "Epoch 12, Batch 655, LR 2.496922 Loss 19.064149, Accuracy 0.064%\n",
      "Epoch 12, Batch 656, LR 2.496913 Loss 19.063943, Accuracy 0.064%\n",
      "Epoch 12, Batch 657, LR 2.496903 Loss 19.064207, Accuracy 0.064%\n",
      "Epoch 12, Batch 658, LR 2.496894 Loss 19.063991, Accuracy 0.065%\n",
      "Epoch 12, Batch 659, LR 2.496884 Loss 19.063680, Accuracy 0.065%\n",
      "Epoch 12, Batch 660, LR 2.496875 Loss 19.063570, Accuracy 0.065%\n",
      "Epoch 12, Batch 661, LR 2.496865 Loss 19.063667, Accuracy 0.065%\n",
      "Epoch 12, Batch 662, LR 2.496856 Loss 19.063302, Accuracy 0.066%\n",
      "Epoch 12, Batch 663, LR 2.496846 Loss 19.063247, Accuracy 0.066%\n",
      "Epoch 12, Batch 664, LR 2.496837 Loss 19.062950, Accuracy 0.066%\n",
      "Epoch 12, Batch 665, LR 2.496827 Loss 19.062977, Accuracy 0.066%\n",
      "Epoch 12, Batch 666, LR 2.496818 Loss 19.062793, Accuracy 0.066%\n",
      "Epoch 12, Batch 667, LR 2.496808 Loss 19.062725, Accuracy 0.066%\n",
      "Epoch 12, Batch 668, LR 2.496799 Loss 19.062643, Accuracy 0.065%\n",
      "Epoch 12, Batch 669, LR 2.496789 Loss 19.062717, Accuracy 0.065%\n",
      "Epoch 12, Batch 670, LR 2.496779 Loss 19.062850, Accuracy 0.065%\n",
      "Epoch 12, Batch 671, LR 2.496770 Loss 19.062829, Accuracy 0.065%\n",
      "Epoch 12, Batch 672, LR 2.496760 Loss 19.062679, Accuracy 0.065%\n",
      "Epoch 12, Batch 673, LR 2.496751 Loss 19.062640, Accuracy 0.065%\n",
      "Epoch 12, Batch 674, LR 2.496741 Loss 19.062587, Accuracy 0.066%\n",
      "Epoch 12, Batch 675, LR 2.496731 Loss 19.062971, Accuracy 0.066%\n",
      "Epoch 12, Batch 676, LR 2.496722 Loss 19.063035, Accuracy 0.067%\n",
      "Epoch 12, Batch 677, LR 2.496712 Loss 19.062908, Accuracy 0.067%\n",
      "Epoch 12, Batch 678, LR 2.496702 Loss 19.063291, Accuracy 0.067%\n",
      "Epoch 12, Batch 679, LR 2.496692 Loss 19.063548, Accuracy 0.068%\n",
      "Epoch 12, Batch 680, LR 2.496683 Loss 19.063270, Accuracy 0.069%\n",
      "Epoch 12, Batch 681, LR 2.496673 Loss 19.063616, Accuracy 0.070%\n",
      "Epoch 12, Batch 682, LR 2.496663 Loss 19.063649, Accuracy 0.070%\n",
      "Epoch 12, Batch 683, LR 2.496653 Loss 19.063756, Accuracy 0.070%\n",
      "Epoch 12, Batch 684, LR 2.496644 Loss 19.064054, Accuracy 0.070%\n",
      "Epoch 12, Batch 685, LR 2.496634 Loss 19.063881, Accuracy 0.071%\n",
      "Epoch 12, Batch 686, LR 2.496624 Loss 19.063950, Accuracy 0.072%\n",
      "Epoch 12, Batch 687, LR 2.496614 Loss 19.063887, Accuracy 0.072%\n",
      "Epoch 12, Batch 688, LR 2.496604 Loss 19.063961, Accuracy 0.072%\n",
      "Epoch 12, Batch 689, LR 2.496594 Loss 19.063975, Accuracy 0.071%\n",
      "Epoch 12, Batch 690, LR 2.496584 Loss 19.063720, Accuracy 0.071%\n",
      "Epoch 12, Batch 691, LR 2.496574 Loss 19.063710, Accuracy 0.071%\n",
      "Epoch 12, Batch 692, LR 2.496565 Loss 19.063746, Accuracy 0.071%\n",
      "Epoch 12, Batch 693, LR 2.496555 Loss 19.063654, Accuracy 0.071%\n",
      "Epoch 12, Batch 694, LR 2.496545 Loss 19.063654, Accuracy 0.071%\n",
      "Epoch 12, Batch 695, LR 2.496535 Loss 19.063733, Accuracy 0.071%\n",
      "Epoch 12, Batch 696, LR 2.496525 Loss 19.063486, Accuracy 0.071%\n",
      "Epoch 12, Batch 697, LR 2.496515 Loss 19.063678, Accuracy 0.071%\n",
      "Epoch 12, Batch 698, LR 2.496505 Loss 19.063392, Accuracy 0.071%\n",
      "Epoch 12, Batch 699, LR 2.496495 Loss 19.063292, Accuracy 0.070%\n",
      "Epoch 12, Batch 700, LR 2.496485 Loss 19.063240, Accuracy 0.070%\n",
      "Epoch 12, Batch 701, LR 2.496475 Loss 19.063070, Accuracy 0.070%\n",
      "Epoch 12, Batch 702, LR 2.496465 Loss 19.062890, Accuracy 0.070%\n",
      "Epoch 12, Batch 703, LR 2.496455 Loss 19.062988, Accuracy 0.070%\n",
      "Epoch 12, Batch 704, LR 2.496444 Loss 19.063129, Accuracy 0.070%\n",
      "Epoch 12, Batch 705, LR 2.496434 Loss 19.062865, Accuracy 0.070%\n",
      "Epoch 12, Batch 706, LR 2.496424 Loss 19.062923, Accuracy 0.070%\n",
      "Epoch 12, Batch 707, LR 2.496414 Loss 19.062918, Accuracy 0.070%\n",
      "Epoch 12, Batch 708, LR 2.496404 Loss 19.062997, Accuracy 0.070%\n",
      "Epoch 12, Batch 709, LR 2.496394 Loss 19.062956, Accuracy 0.069%\n",
      "Epoch 12, Batch 710, LR 2.496384 Loss 19.063050, Accuracy 0.069%\n",
      "Epoch 12, Batch 711, LR 2.496373 Loss 19.063101, Accuracy 0.069%\n",
      "Epoch 12, Batch 712, LR 2.496363 Loss 19.063080, Accuracy 0.069%\n",
      "Epoch 12, Batch 713, LR 2.496353 Loss 19.063227, Accuracy 0.069%\n",
      "Epoch 12, Batch 714, LR 2.496343 Loss 19.063011, Accuracy 0.069%\n",
      "Epoch 12, Batch 715, LR 2.496333 Loss 19.063108, Accuracy 0.069%\n",
      "Epoch 12, Batch 716, LR 2.496322 Loss 19.062849, Accuracy 0.069%\n",
      "Epoch 12, Batch 717, LR 2.496312 Loss 19.062749, Accuracy 0.069%\n",
      "Epoch 12, Batch 718, LR 2.496302 Loss 19.062797, Accuracy 0.069%\n",
      "Epoch 12, Batch 719, LR 2.496291 Loss 19.062825, Accuracy 0.068%\n",
      "Epoch 12, Batch 720, LR 2.496281 Loss 19.062924, Accuracy 0.068%\n",
      "Epoch 12, Batch 721, LR 2.496271 Loss 19.063306, Accuracy 0.068%\n",
      "Epoch 12, Batch 722, LR 2.496260 Loss 19.063180, Accuracy 0.068%\n",
      "Epoch 12, Batch 723, LR 2.496250 Loss 19.063382, Accuracy 0.068%\n",
      "Epoch 12, Batch 724, LR 2.496240 Loss 19.063044, Accuracy 0.068%\n",
      "Epoch 12, Batch 725, LR 2.496229 Loss 19.062903, Accuracy 0.068%\n",
      "Epoch 12, Batch 726, LR 2.496219 Loss 19.062880, Accuracy 0.068%\n",
      "Epoch 12, Batch 727, LR 2.496208 Loss 19.062891, Accuracy 0.068%\n",
      "Epoch 12, Batch 728, LR 2.496198 Loss 19.063114, Accuracy 0.068%\n",
      "Epoch 12, Batch 729, LR 2.496188 Loss 19.063014, Accuracy 0.069%\n",
      "Epoch 12, Batch 730, LR 2.496177 Loss 19.062884, Accuracy 0.068%\n",
      "Epoch 12, Batch 731, LR 2.496167 Loss 19.063159, Accuracy 0.068%\n",
      "Epoch 12, Batch 732, LR 2.496156 Loss 19.062994, Accuracy 0.068%\n",
      "Epoch 12, Batch 733, LR 2.496146 Loss 19.062905, Accuracy 0.069%\n",
      "Epoch 12, Batch 734, LR 2.496135 Loss 19.063130, Accuracy 0.069%\n",
      "Epoch 12, Batch 735, LR 2.496125 Loss 19.062901, Accuracy 0.069%\n",
      "Epoch 12, Batch 736, LR 2.496114 Loss 19.062927, Accuracy 0.069%\n",
      "Epoch 12, Batch 737, LR 2.496103 Loss 19.063293, Accuracy 0.069%\n",
      "Epoch 12, Batch 738, LR 2.496093 Loss 19.063311, Accuracy 0.069%\n",
      "Epoch 12, Batch 739, LR 2.496082 Loss 19.063199, Accuracy 0.069%\n",
      "Epoch 12, Batch 740, LR 2.496072 Loss 19.063268, Accuracy 0.069%\n",
      "Epoch 12, Batch 741, LR 2.496061 Loss 19.063133, Accuracy 0.069%\n",
      "Epoch 12, Batch 742, LR 2.496050 Loss 19.062705, Accuracy 0.068%\n",
      "Epoch 12, Batch 743, LR 2.496040 Loss 19.062664, Accuracy 0.068%\n",
      "Epoch 12, Batch 744, LR 2.496029 Loss 19.062867, Accuracy 0.068%\n",
      "Epoch 12, Batch 745, LR 2.496018 Loss 19.063040, Accuracy 0.068%\n",
      "Epoch 12, Batch 746, LR 2.496008 Loss 19.063036, Accuracy 0.068%\n",
      "Epoch 12, Batch 747, LR 2.495997 Loss 19.063274, Accuracy 0.069%\n",
      "Epoch 12, Batch 748, LR 2.495986 Loss 19.063080, Accuracy 0.069%\n",
      "Epoch 12, Batch 749, LR 2.495976 Loss 19.063054, Accuracy 0.069%\n",
      "Epoch 12, Batch 750, LR 2.495965 Loss 19.063140, Accuracy 0.069%\n",
      "Epoch 12, Batch 751, LR 2.495954 Loss 19.062973, Accuracy 0.070%\n",
      "Epoch 12, Batch 752, LR 2.495943 Loss 19.062870, Accuracy 0.070%\n",
      "Epoch 12, Batch 753, LR 2.495933 Loss 19.063053, Accuracy 0.071%\n",
      "Epoch 12, Batch 754, LR 2.495922 Loss 19.062825, Accuracy 0.070%\n",
      "Epoch 12, Batch 755, LR 2.495911 Loss 19.062993, Accuracy 0.070%\n",
      "Epoch 12, Batch 756, LR 2.495900 Loss 19.062939, Accuracy 0.070%\n",
      "Epoch 12, Batch 757, LR 2.495889 Loss 19.062960, Accuracy 0.070%\n",
      "Epoch 12, Batch 758, LR 2.495878 Loss 19.062981, Accuracy 0.070%\n",
      "Epoch 12, Batch 759, LR 2.495868 Loss 19.063038, Accuracy 0.070%\n",
      "Epoch 12, Batch 760, LR 2.495857 Loss 19.063204, Accuracy 0.070%\n",
      "Epoch 12, Batch 761, LR 2.495846 Loss 19.063455, Accuracy 0.070%\n",
      "Epoch 12, Batch 762, LR 2.495835 Loss 19.063576, Accuracy 0.070%\n",
      "Epoch 12, Batch 763, LR 2.495824 Loss 19.063874, Accuracy 0.070%\n",
      "Epoch 12, Batch 764, LR 2.495813 Loss 19.064145, Accuracy 0.070%\n",
      "Epoch 12, Batch 765, LR 2.495802 Loss 19.064030, Accuracy 0.069%\n",
      "Epoch 12, Batch 766, LR 2.495791 Loss 19.064131, Accuracy 0.069%\n",
      "Epoch 12, Batch 767, LR 2.495780 Loss 19.063940, Accuracy 0.070%\n",
      "Epoch 12, Batch 768, LR 2.495769 Loss 19.063850, Accuracy 0.070%\n",
      "Epoch 12, Batch 769, LR 2.495758 Loss 19.063899, Accuracy 0.070%\n",
      "Epoch 12, Batch 770, LR 2.495747 Loss 19.063925, Accuracy 0.070%\n",
      "Epoch 12, Batch 771, LR 2.495736 Loss 19.063989, Accuracy 0.070%\n",
      "Epoch 12, Batch 772, LR 2.495725 Loss 19.063939, Accuracy 0.070%\n",
      "Epoch 12, Batch 773, LR 2.495714 Loss 19.064171, Accuracy 0.070%\n",
      "Epoch 12, Batch 774, LR 2.495703 Loss 19.064099, Accuracy 0.070%\n",
      "Epoch 12, Batch 775, LR 2.495692 Loss 19.064087, Accuracy 0.070%\n",
      "Epoch 12, Batch 776, LR 2.495680 Loss 19.063892, Accuracy 0.070%\n",
      "Epoch 12, Batch 777, LR 2.495669 Loss 19.063963, Accuracy 0.070%\n",
      "Epoch 12, Batch 778, LR 2.495658 Loss 19.064001, Accuracy 0.070%\n",
      "Epoch 12, Batch 779, LR 2.495647 Loss 19.063912, Accuracy 0.070%\n",
      "Epoch 12, Batch 780, LR 2.495636 Loss 19.063764, Accuracy 0.071%\n",
      "Epoch 12, Batch 781, LR 2.495625 Loss 19.064180, Accuracy 0.071%\n",
      "Epoch 12, Batch 782, LR 2.495613 Loss 19.064137, Accuracy 0.071%\n",
      "Epoch 12, Batch 783, LR 2.495602 Loss 19.064372, Accuracy 0.071%\n",
      "Epoch 12, Batch 784, LR 2.495591 Loss 19.064059, Accuracy 0.071%\n",
      "Epoch 12, Batch 785, LR 2.495580 Loss 19.063786, Accuracy 0.071%\n",
      "Epoch 12, Batch 786, LR 2.495568 Loss 19.063759, Accuracy 0.071%\n",
      "Epoch 12, Batch 787, LR 2.495557 Loss 19.063595, Accuracy 0.070%\n",
      "Epoch 12, Batch 788, LR 2.495546 Loss 19.063635, Accuracy 0.070%\n",
      "Epoch 12, Batch 789, LR 2.495535 Loss 19.063631, Accuracy 0.070%\n",
      "Epoch 12, Batch 790, LR 2.495523 Loss 19.063506, Accuracy 0.071%\n",
      "Epoch 12, Batch 791, LR 2.495512 Loss 19.063645, Accuracy 0.071%\n",
      "Epoch 12, Batch 792, LR 2.495501 Loss 19.063795, Accuracy 0.071%\n",
      "Epoch 12, Batch 793, LR 2.495489 Loss 19.063835, Accuracy 0.071%\n",
      "Epoch 12, Batch 794, LR 2.495478 Loss 19.063806, Accuracy 0.071%\n",
      "Epoch 12, Batch 795, LR 2.495466 Loss 19.063865, Accuracy 0.071%\n",
      "Epoch 12, Batch 796, LR 2.495455 Loss 19.064090, Accuracy 0.072%\n",
      "Epoch 12, Batch 797, LR 2.495444 Loss 19.064123, Accuracy 0.072%\n",
      "Epoch 12, Batch 798, LR 2.495432 Loss 19.064204, Accuracy 0.071%\n",
      "Epoch 12, Batch 799, LR 2.495421 Loss 19.064087, Accuracy 0.071%\n",
      "Epoch 12, Batch 800, LR 2.495409 Loss 19.064121, Accuracy 0.071%\n",
      "Epoch 12, Batch 801, LR 2.495398 Loss 19.064284, Accuracy 0.072%\n",
      "Epoch 12, Batch 802, LR 2.495386 Loss 19.064434, Accuracy 0.072%\n",
      "Epoch 12, Batch 803, LR 2.495375 Loss 19.064681, Accuracy 0.072%\n",
      "Epoch 12, Batch 804, LR 2.495363 Loss 19.064577, Accuracy 0.072%\n",
      "Epoch 12, Batch 805, LR 2.495352 Loss 19.064536, Accuracy 0.072%\n",
      "Epoch 12, Batch 806, LR 2.495340 Loss 19.064288, Accuracy 0.073%\n",
      "Epoch 12, Batch 807, LR 2.495329 Loss 19.064435, Accuracy 0.073%\n",
      "Epoch 12, Batch 808, LR 2.495317 Loss 19.064439, Accuracy 0.073%\n",
      "Epoch 12, Batch 809, LR 2.495305 Loss 19.064423, Accuracy 0.072%\n",
      "Epoch 12, Batch 810, LR 2.495294 Loss 19.064286, Accuracy 0.072%\n",
      "Epoch 12, Batch 811, LR 2.495282 Loss 19.064430, Accuracy 0.072%\n",
      "Epoch 12, Batch 812, LR 2.495271 Loss 19.064515, Accuracy 0.072%\n",
      "Epoch 12, Batch 813, LR 2.495259 Loss 19.064463, Accuracy 0.072%\n",
      "Epoch 12, Batch 814, LR 2.495247 Loss 19.064363, Accuracy 0.072%\n",
      "Epoch 12, Batch 815, LR 2.495236 Loss 19.064456, Accuracy 0.072%\n",
      "Epoch 12, Batch 816, LR 2.495224 Loss 19.064209, Accuracy 0.073%\n",
      "Epoch 12, Batch 817, LR 2.495212 Loss 19.064386, Accuracy 0.073%\n",
      "Epoch 12, Batch 818, LR 2.495200 Loss 19.064544, Accuracy 0.073%\n",
      "Epoch 12, Batch 819, LR 2.495189 Loss 19.064526, Accuracy 0.072%\n",
      "Epoch 12, Batch 820, LR 2.495177 Loss 19.064532, Accuracy 0.072%\n",
      "Epoch 12, Batch 821, LR 2.495165 Loss 19.064463, Accuracy 0.072%\n",
      "Epoch 12, Batch 822, LR 2.495153 Loss 19.064596, Accuracy 0.072%\n",
      "Epoch 12, Batch 823, LR 2.495142 Loss 19.065042, Accuracy 0.072%\n",
      "Epoch 12, Batch 824, LR 2.495130 Loss 19.064920, Accuracy 0.073%\n",
      "Epoch 12, Batch 825, LR 2.495118 Loss 19.064813, Accuracy 0.073%\n",
      "Epoch 12, Batch 826, LR 2.495106 Loss 19.065001, Accuracy 0.073%\n",
      "Epoch 12, Batch 827, LR 2.495094 Loss 19.064939, Accuracy 0.073%\n",
      "Epoch 12, Batch 828, LR 2.495083 Loss 19.064832, Accuracy 0.073%\n",
      "Epoch 12, Batch 829, LR 2.495071 Loss 19.064860, Accuracy 0.073%\n",
      "Epoch 12, Batch 830, LR 2.495059 Loss 19.065049, Accuracy 0.072%\n",
      "Epoch 12, Batch 831, LR 2.495047 Loss 19.065221, Accuracy 0.072%\n",
      "Epoch 12, Batch 832, LR 2.495035 Loss 19.065061, Accuracy 0.072%\n",
      "Epoch 12, Batch 833, LR 2.495023 Loss 19.064952, Accuracy 0.072%\n",
      "Epoch 12, Batch 834, LR 2.495011 Loss 19.064776, Accuracy 0.072%\n",
      "Epoch 12, Batch 835, LR 2.494999 Loss 19.064813, Accuracy 0.072%\n",
      "Epoch 12, Batch 836, LR 2.494987 Loss 19.064734, Accuracy 0.072%\n",
      "Epoch 12, Batch 837, LR 2.494975 Loss 19.064602, Accuracy 0.072%\n",
      "Epoch 12, Batch 838, LR 2.494963 Loss 19.064466, Accuracy 0.073%\n",
      "Epoch 12, Batch 839, LR 2.494951 Loss 19.064553, Accuracy 0.073%\n",
      "Epoch 12, Batch 840, LR 2.494939 Loss 19.064357, Accuracy 0.073%\n",
      "Epoch 12, Batch 841, LR 2.494927 Loss 19.064372, Accuracy 0.073%\n",
      "Epoch 12, Batch 842, LR 2.494915 Loss 19.064427, Accuracy 0.073%\n",
      "Epoch 12, Batch 843, LR 2.494903 Loss 19.064386, Accuracy 0.073%\n",
      "Epoch 12, Batch 844, LR 2.494891 Loss 19.064272, Accuracy 0.073%\n",
      "Epoch 12, Batch 845, LR 2.494879 Loss 19.064319, Accuracy 0.073%\n",
      "Epoch 12, Batch 846, LR 2.494867 Loss 19.064258, Accuracy 0.073%\n",
      "Epoch 12, Batch 847, LR 2.494854 Loss 19.064219, Accuracy 0.073%\n",
      "Epoch 12, Batch 848, LR 2.494842 Loss 19.064226, Accuracy 0.073%\n",
      "Epoch 12, Batch 849, LR 2.494830 Loss 19.064403, Accuracy 0.073%\n",
      "Epoch 12, Batch 850, LR 2.494818 Loss 19.064292, Accuracy 0.073%\n",
      "Epoch 12, Batch 851, LR 2.494806 Loss 19.064608, Accuracy 0.073%\n",
      "Epoch 12, Batch 852, LR 2.494794 Loss 19.064783, Accuracy 0.073%\n",
      "Epoch 12, Batch 853, LR 2.494781 Loss 19.064907, Accuracy 0.074%\n",
      "Epoch 12, Batch 854, LR 2.494769 Loss 19.064858, Accuracy 0.074%\n",
      "Epoch 12, Batch 855, LR 2.494757 Loss 19.064831, Accuracy 0.074%\n",
      "Epoch 12, Batch 856, LR 2.494745 Loss 19.064908, Accuracy 0.074%\n",
      "Epoch 12, Batch 857, LR 2.494732 Loss 19.064660, Accuracy 0.075%\n",
      "Epoch 12, Batch 858, LR 2.494720 Loss 19.064521, Accuracy 0.075%\n",
      "Epoch 12, Batch 859, LR 2.494708 Loss 19.064422, Accuracy 0.075%\n",
      "Epoch 12, Batch 860, LR 2.494695 Loss 19.064386, Accuracy 0.074%\n",
      "Epoch 12, Batch 861, LR 2.494683 Loss 19.064407, Accuracy 0.074%\n",
      "Epoch 12, Batch 862, LR 2.494671 Loss 19.064423, Accuracy 0.074%\n",
      "Epoch 12, Batch 863, LR 2.494658 Loss 19.064455, Accuracy 0.074%\n",
      "Epoch 12, Batch 864, LR 2.494646 Loss 19.064448, Accuracy 0.074%\n",
      "Epoch 12, Batch 865, LR 2.494634 Loss 19.064344, Accuracy 0.074%\n",
      "Epoch 12, Batch 866, LR 2.494621 Loss 19.064358, Accuracy 0.074%\n",
      "Epoch 12, Batch 867, LR 2.494609 Loss 19.064157, Accuracy 0.074%\n",
      "Epoch 12, Batch 868, LR 2.494596 Loss 19.064305, Accuracy 0.074%\n",
      "Epoch 12, Batch 869, LR 2.494584 Loss 19.064013, Accuracy 0.075%\n",
      "Epoch 12, Batch 870, LR 2.494571 Loss 19.063826, Accuracy 0.075%\n",
      "Epoch 12, Batch 871, LR 2.494559 Loss 19.063824, Accuracy 0.074%\n",
      "Epoch 12, Batch 872, LR 2.494546 Loss 19.063834, Accuracy 0.074%\n",
      "Epoch 12, Batch 873, LR 2.494534 Loss 19.063969, Accuracy 0.074%\n",
      "Epoch 12, Batch 874, LR 2.494521 Loss 19.064208, Accuracy 0.074%\n",
      "Epoch 12, Batch 875, LR 2.494509 Loss 19.064175, Accuracy 0.074%\n",
      "Epoch 12, Batch 876, LR 2.494496 Loss 19.064108, Accuracy 0.074%\n",
      "Epoch 12, Batch 877, LR 2.494484 Loss 19.063791, Accuracy 0.074%\n",
      "Epoch 12, Batch 878, LR 2.494471 Loss 19.063865, Accuracy 0.074%\n",
      "Epoch 12, Batch 879, LR 2.494459 Loss 19.063927, Accuracy 0.075%\n",
      "Epoch 12, Batch 880, LR 2.494446 Loss 19.063871, Accuracy 0.075%\n",
      "Epoch 12, Batch 881, LR 2.494433 Loss 19.063886, Accuracy 0.074%\n",
      "Epoch 12, Batch 882, LR 2.494421 Loss 19.063708, Accuracy 0.074%\n",
      "Epoch 12, Batch 883, LR 2.494408 Loss 19.063812, Accuracy 0.074%\n",
      "Epoch 12, Batch 884, LR 2.494395 Loss 19.063894, Accuracy 0.074%\n",
      "Epoch 12, Batch 885, LR 2.494383 Loss 19.063779, Accuracy 0.074%\n",
      "Epoch 12, Batch 886, LR 2.494370 Loss 19.064002, Accuracy 0.074%\n",
      "Epoch 12, Batch 887, LR 2.494357 Loss 19.063840, Accuracy 0.074%\n",
      "Epoch 12, Batch 888, LR 2.494345 Loss 19.063545, Accuracy 0.074%\n",
      "Epoch 12, Batch 889, LR 2.494332 Loss 19.063636, Accuracy 0.074%\n",
      "Epoch 12, Batch 890, LR 2.494319 Loss 19.063677, Accuracy 0.075%\n",
      "Epoch 12, Batch 891, LR 2.494306 Loss 19.063586, Accuracy 0.075%\n",
      "Epoch 12, Batch 892, LR 2.494294 Loss 19.063556, Accuracy 0.075%\n",
      "Epoch 12, Batch 893, LR 2.494281 Loss 19.063524, Accuracy 0.075%\n",
      "Epoch 12, Batch 894, LR 2.494268 Loss 19.063577, Accuracy 0.075%\n",
      "Epoch 12, Batch 895, LR 2.494255 Loss 19.063638, Accuracy 0.075%\n",
      "Epoch 12, Batch 896, LR 2.494242 Loss 19.063443, Accuracy 0.075%\n",
      "Epoch 12, Batch 897, LR 2.494229 Loss 19.063523, Accuracy 0.076%\n",
      "Epoch 12, Batch 898, LR 2.494217 Loss 19.063592, Accuracy 0.076%\n",
      "Epoch 12, Batch 899, LR 2.494204 Loss 19.063779, Accuracy 0.076%\n",
      "Epoch 12, Batch 900, LR 2.494191 Loss 19.063881, Accuracy 0.076%\n",
      "Epoch 12, Batch 901, LR 2.494178 Loss 19.063920, Accuracy 0.075%\n",
      "Epoch 12, Batch 902, LR 2.494165 Loss 19.063950, Accuracy 0.075%\n",
      "Epoch 12, Batch 903, LR 2.494152 Loss 19.064074, Accuracy 0.075%\n",
      "Epoch 12, Batch 904, LR 2.494139 Loss 19.064168, Accuracy 0.075%\n",
      "Epoch 12, Batch 905, LR 2.494126 Loss 19.063952, Accuracy 0.075%\n",
      "Epoch 12, Batch 906, LR 2.494113 Loss 19.063935, Accuracy 0.075%\n",
      "Epoch 12, Batch 907, LR 2.494100 Loss 19.064015, Accuracy 0.075%\n",
      "Epoch 12, Batch 908, LR 2.494087 Loss 19.063952, Accuracy 0.075%\n",
      "Epoch 12, Batch 909, LR 2.494074 Loss 19.064013, Accuracy 0.075%\n",
      "Epoch 12, Batch 910, LR 2.494061 Loss 19.064050, Accuracy 0.075%\n",
      "Epoch 12, Batch 911, LR 2.494048 Loss 19.064231, Accuracy 0.075%\n",
      "Epoch 12, Batch 912, LR 2.494035 Loss 19.064012, Accuracy 0.075%\n",
      "Epoch 12, Batch 913, LR 2.494022 Loss 19.063888, Accuracy 0.074%\n",
      "Epoch 12, Batch 914, LR 2.494009 Loss 19.063832, Accuracy 0.074%\n",
      "Epoch 12, Batch 915, LR 2.493996 Loss 19.063779, Accuracy 0.074%\n",
      "Epoch 12, Batch 916, LR 2.493983 Loss 19.063797, Accuracy 0.074%\n",
      "Epoch 12, Batch 917, LR 2.493969 Loss 19.063918, Accuracy 0.074%\n",
      "Epoch 12, Batch 918, LR 2.493956 Loss 19.063771, Accuracy 0.074%\n",
      "Epoch 12, Batch 919, LR 2.493943 Loss 19.063679, Accuracy 0.074%\n",
      "Epoch 12, Batch 920, LR 2.493930 Loss 19.063799, Accuracy 0.074%\n",
      "Epoch 12, Batch 921, LR 2.493917 Loss 19.063715, Accuracy 0.074%\n",
      "Epoch 12, Batch 922, LR 2.493904 Loss 19.063523, Accuracy 0.075%\n",
      "Epoch 12, Batch 923, LR 2.493890 Loss 19.063462, Accuracy 0.074%\n",
      "Epoch 12, Batch 924, LR 2.493877 Loss 19.063471, Accuracy 0.074%\n",
      "Epoch 12, Batch 925, LR 2.493864 Loss 19.063420, Accuracy 0.074%\n",
      "Epoch 12, Batch 926, LR 2.493851 Loss 19.063388, Accuracy 0.074%\n",
      "Epoch 12, Batch 927, LR 2.493837 Loss 19.063252, Accuracy 0.075%\n",
      "Epoch 12, Batch 928, LR 2.493824 Loss 19.063136, Accuracy 0.075%\n",
      "Epoch 12, Batch 929, LR 2.493811 Loss 19.063357, Accuracy 0.075%\n",
      "Epoch 12, Batch 930, LR 2.493797 Loss 19.063388, Accuracy 0.075%\n",
      "Epoch 12, Batch 931, LR 2.493784 Loss 19.063268, Accuracy 0.075%\n",
      "Epoch 12, Batch 932, LR 2.493771 Loss 19.063678, Accuracy 0.075%\n",
      "Epoch 12, Batch 933, LR 2.493757 Loss 19.063624, Accuracy 0.075%\n",
      "Epoch 12, Batch 934, LR 2.493744 Loss 19.063505, Accuracy 0.075%\n",
      "Epoch 12, Batch 935, LR 2.493731 Loss 19.063477, Accuracy 0.075%\n",
      "Epoch 12, Batch 936, LR 2.493717 Loss 19.063319, Accuracy 0.075%\n",
      "Epoch 12, Batch 937, LR 2.493704 Loss 19.063303, Accuracy 0.075%\n",
      "Epoch 12, Batch 938, LR 2.493690 Loss 19.063274, Accuracy 0.075%\n",
      "Epoch 12, Batch 939, LR 2.493677 Loss 19.063309, Accuracy 0.075%\n",
      "Epoch 12, Batch 940, LR 2.493663 Loss 19.063466, Accuracy 0.075%\n",
      "Epoch 12, Batch 941, LR 2.493650 Loss 19.063387, Accuracy 0.075%\n",
      "Epoch 12, Batch 942, LR 2.493636 Loss 19.063836, Accuracy 0.075%\n",
      "Epoch 12, Batch 943, LR 2.493623 Loss 19.063826, Accuracy 0.075%\n",
      "Epoch 12, Batch 944, LR 2.493609 Loss 19.063879, Accuracy 0.074%\n",
      "Epoch 12, Batch 945, LR 2.493596 Loss 19.063833, Accuracy 0.074%\n",
      "Epoch 12, Batch 946, LR 2.493582 Loss 19.063759, Accuracy 0.074%\n",
      "Epoch 12, Batch 947, LR 2.493569 Loss 19.063834, Accuracy 0.075%\n",
      "Epoch 12, Batch 948, LR 2.493555 Loss 19.063839, Accuracy 0.076%\n",
      "Epoch 12, Batch 949, LR 2.493542 Loss 19.063953, Accuracy 0.076%\n",
      "Epoch 12, Batch 950, LR 2.493528 Loss 19.064009, Accuracy 0.076%\n",
      "Epoch 12, Batch 951, LR 2.493514 Loss 19.063857, Accuracy 0.076%\n",
      "Epoch 12, Batch 952, LR 2.493501 Loss 19.063878, Accuracy 0.075%\n",
      "Epoch 12, Batch 953, LR 2.493487 Loss 19.063909, Accuracy 0.075%\n",
      "Epoch 12, Batch 954, LR 2.493473 Loss 19.064006, Accuracy 0.075%\n",
      "Epoch 12, Batch 955, LR 2.493460 Loss 19.064203, Accuracy 0.075%\n",
      "Epoch 12, Batch 956, LR 2.493446 Loss 19.064334, Accuracy 0.075%\n",
      "Epoch 12, Batch 957, LR 2.493432 Loss 19.064472, Accuracy 0.075%\n",
      "Epoch 12, Batch 958, LR 2.493419 Loss 19.064368, Accuracy 0.075%\n",
      "Epoch 12, Batch 959, LR 2.493405 Loss 19.064442, Accuracy 0.075%\n",
      "Epoch 12, Batch 960, LR 2.493391 Loss 19.064489, Accuracy 0.075%\n",
      "Epoch 12, Batch 961, LR 2.493377 Loss 19.064605, Accuracy 0.075%\n",
      "Epoch 12, Batch 962, LR 2.493364 Loss 19.064594, Accuracy 0.075%\n",
      "Epoch 12, Batch 963, LR 2.493350 Loss 19.064670, Accuracy 0.075%\n",
      "Epoch 12, Batch 964, LR 2.493336 Loss 19.064583, Accuracy 0.075%\n",
      "Epoch 12, Batch 965, LR 2.493322 Loss 19.064446, Accuracy 0.074%\n",
      "Epoch 12, Batch 966, LR 2.493308 Loss 19.064357, Accuracy 0.074%\n",
      "Epoch 12, Batch 967, LR 2.493294 Loss 19.064301, Accuracy 0.074%\n",
      "Epoch 12, Batch 968, LR 2.493281 Loss 19.064416, Accuracy 0.075%\n",
      "Epoch 12, Batch 969, LR 2.493267 Loss 19.064398, Accuracy 0.075%\n",
      "Epoch 12, Batch 970, LR 2.493253 Loss 19.064421, Accuracy 0.075%\n",
      "Epoch 12, Batch 971, LR 2.493239 Loss 19.064256, Accuracy 0.075%\n",
      "Epoch 12, Batch 972, LR 2.493225 Loss 19.064519, Accuracy 0.075%\n",
      "Epoch 12, Batch 973, LR 2.493211 Loss 19.064625, Accuracy 0.075%\n",
      "Epoch 12, Batch 974, LR 2.493197 Loss 19.064725, Accuracy 0.075%\n",
      "Epoch 12, Batch 975, LR 2.493183 Loss 19.064711, Accuracy 0.075%\n",
      "Epoch 12, Batch 976, LR 2.493169 Loss 19.064832, Accuracy 0.074%\n",
      "Epoch 12, Batch 977, LR 2.493155 Loss 19.064920, Accuracy 0.074%\n",
      "Epoch 12, Batch 978, LR 2.493141 Loss 19.064632, Accuracy 0.075%\n",
      "Epoch 12, Batch 979, LR 2.493127 Loss 19.064526, Accuracy 0.075%\n",
      "Epoch 12, Batch 980, LR 2.493113 Loss 19.064646, Accuracy 0.075%\n",
      "Epoch 12, Batch 981, LR 2.493099 Loss 19.064464, Accuracy 0.075%\n",
      "Epoch 12, Batch 982, LR 2.493085 Loss 19.064356, Accuracy 0.075%\n",
      "Epoch 12, Batch 983, LR 2.493071 Loss 19.064324, Accuracy 0.075%\n",
      "Epoch 12, Batch 984, LR 2.493057 Loss 19.064384, Accuracy 0.075%\n",
      "Epoch 12, Batch 985, LR 2.493043 Loss 19.064573, Accuracy 0.075%\n",
      "Epoch 12, Batch 986, LR 2.493029 Loss 19.064612, Accuracy 0.074%\n",
      "Epoch 12, Batch 987, LR 2.493014 Loss 19.064526, Accuracy 0.074%\n",
      "Epoch 12, Batch 988, LR 2.493000 Loss 19.064525, Accuracy 0.076%\n",
      "Epoch 12, Batch 989, LR 2.492986 Loss 19.064209, Accuracy 0.076%\n",
      "Epoch 12, Batch 990, LR 2.492972 Loss 19.064235, Accuracy 0.076%\n",
      "Epoch 12, Batch 991, LR 2.492958 Loss 19.064453, Accuracy 0.076%\n",
      "Epoch 12, Batch 992, LR 2.492944 Loss 19.064273, Accuracy 0.076%\n",
      "Epoch 12, Batch 993, LR 2.492929 Loss 19.064211, Accuracy 0.076%\n",
      "Epoch 12, Batch 994, LR 2.492915 Loss 19.064115, Accuracy 0.075%\n",
      "Epoch 12, Batch 995, LR 2.492901 Loss 19.064006, Accuracy 0.075%\n",
      "Epoch 12, Batch 996, LR 2.492887 Loss 19.064125, Accuracy 0.075%\n",
      "Epoch 12, Batch 997, LR 2.492872 Loss 19.064257, Accuracy 0.075%\n",
      "Epoch 12, Batch 998, LR 2.492858 Loss 19.064450, Accuracy 0.075%\n",
      "Epoch 12, Batch 999, LR 2.492844 Loss 19.064564, Accuracy 0.075%\n",
      "Epoch 12, Batch 1000, LR 2.492829 Loss 19.064683, Accuracy 0.076%\n",
      "Epoch 12, Batch 1001, LR 2.492815 Loss 19.064731, Accuracy 0.076%\n",
      "Epoch 12, Batch 1002, LR 2.492801 Loss 19.064844, Accuracy 0.076%\n",
      "Epoch 12, Batch 1003, LR 2.492786 Loss 19.064778, Accuracy 0.076%\n",
      "Epoch 12, Batch 1004, LR 2.492772 Loss 19.064829, Accuracy 0.075%\n",
      "Epoch 12, Batch 1005, LR 2.492758 Loss 19.064878, Accuracy 0.075%\n",
      "Epoch 12, Batch 1006, LR 2.492743 Loss 19.064840, Accuracy 0.075%\n",
      "Epoch 12, Batch 1007, LR 2.492729 Loss 19.064864, Accuracy 0.075%\n",
      "Epoch 12, Batch 1008, LR 2.492714 Loss 19.064947, Accuracy 0.075%\n",
      "Epoch 12, Batch 1009, LR 2.492700 Loss 19.065192, Accuracy 0.075%\n",
      "Epoch 12, Batch 1010, LR 2.492685 Loss 19.065397, Accuracy 0.075%\n",
      "Epoch 12, Batch 1011, LR 2.492671 Loss 19.065514, Accuracy 0.075%\n",
      "Epoch 12, Batch 1012, LR 2.492656 Loss 19.065495, Accuracy 0.075%\n",
      "Epoch 12, Batch 1013, LR 2.492642 Loss 19.065482, Accuracy 0.075%\n",
      "Epoch 12, Batch 1014, LR 2.492627 Loss 19.065507, Accuracy 0.075%\n",
      "Epoch 12, Batch 1015, LR 2.492613 Loss 19.065456, Accuracy 0.075%\n",
      "Epoch 12, Batch 1016, LR 2.492598 Loss 19.065329, Accuracy 0.075%\n",
      "Epoch 12, Batch 1017, LR 2.492584 Loss 19.065437, Accuracy 0.075%\n",
      "Epoch 12, Batch 1018, LR 2.492569 Loss 19.065313, Accuracy 0.074%\n",
      "Epoch 12, Batch 1019, LR 2.492555 Loss 19.065326, Accuracy 0.074%\n",
      "Epoch 12, Batch 1020, LR 2.492540 Loss 19.065142, Accuracy 0.074%\n",
      "Epoch 12, Batch 1021, LR 2.492525 Loss 19.065274, Accuracy 0.074%\n",
      "Epoch 12, Batch 1022, LR 2.492511 Loss 19.065240, Accuracy 0.074%\n",
      "Epoch 12, Batch 1023, LR 2.492496 Loss 19.065165, Accuracy 0.074%\n",
      "Epoch 12, Batch 1024, LR 2.492481 Loss 19.065026, Accuracy 0.074%\n",
      "Epoch 12, Batch 1025, LR 2.492467 Loss 19.065065, Accuracy 0.075%\n",
      "Epoch 12, Batch 1026, LR 2.492452 Loss 19.064796, Accuracy 0.075%\n",
      "Epoch 12, Batch 1027, LR 2.492437 Loss 19.064693, Accuracy 0.076%\n",
      "Epoch 12, Batch 1028, LR 2.492423 Loss 19.064613, Accuracy 0.076%\n",
      "Epoch 12, Batch 1029, LR 2.492408 Loss 19.064579, Accuracy 0.076%\n",
      "Epoch 12, Batch 1030, LR 2.492393 Loss 19.064611, Accuracy 0.076%\n",
      "Epoch 12, Batch 1031, LR 2.492378 Loss 19.064797, Accuracy 0.076%\n",
      "Epoch 12, Batch 1032, LR 2.492364 Loss 19.064909, Accuracy 0.076%\n",
      "Epoch 12, Batch 1033, LR 2.492349 Loss 19.064871, Accuracy 0.076%\n",
      "Epoch 12, Batch 1034, LR 2.492334 Loss 19.064909, Accuracy 0.076%\n",
      "Epoch 12, Batch 1035, LR 2.492319 Loss 19.064763, Accuracy 0.075%\n",
      "Epoch 12, Batch 1036, LR 2.492304 Loss 19.064719, Accuracy 0.075%\n",
      "Epoch 12, Batch 1037, LR 2.492290 Loss 19.064690, Accuracy 0.075%\n",
      "Epoch 12, Batch 1038, LR 2.492275 Loss 19.064788, Accuracy 0.075%\n",
      "Epoch 12, Batch 1039, LR 2.492260 Loss 19.064647, Accuracy 0.075%\n",
      "Epoch 12, Batch 1040, LR 2.492245 Loss 19.064761, Accuracy 0.075%\n",
      "Epoch 12, Batch 1041, LR 2.492230 Loss 19.064743, Accuracy 0.075%\n",
      "Epoch 12, Batch 1042, LR 2.492215 Loss 19.064560, Accuracy 0.075%\n",
      "Epoch 12, Batch 1043, LR 2.492200 Loss 19.064810, Accuracy 0.076%\n",
      "Epoch 12, Batch 1044, LR 2.492185 Loss 19.064689, Accuracy 0.076%\n",
      "Epoch 12, Batch 1045, LR 2.492170 Loss 19.064779, Accuracy 0.076%\n",
      "Epoch 12, Batch 1046, LR 2.492155 Loss 19.064730, Accuracy 0.076%\n",
      "Epoch 12, Batch 1047, LR 2.492140 Loss 19.064716, Accuracy 0.076%\n",
      "Epoch 12, Loss (train set) 19.064716, Accuracy (train set) 0.076%\n",
      "Epoch 13, Batch 1, LR 2.492125 Loss 18.990719, Accuracy 0.000%\n",
      "Epoch 13, Batch 2, LR 2.492110 Loss 19.054836, Accuracy 0.000%\n",
      "Epoch 13, Batch 3, LR 2.492095 Loss 19.035135, Accuracy 0.000%\n",
      "Epoch 13, Batch 4, LR 2.492080 Loss 19.069838, Accuracy 0.000%\n",
      "Epoch 13, Batch 5, LR 2.492065 Loss 19.079744, Accuracy 0.000%\n",
      "Epoch 13, Batch 6, LR 2.492050 Loss 19.071135, Accuracy 0.000%\n",
      "Epoch 13, Batch 7, LR 2.492035 Loss 19.082002, Accuracy 0.000%\n",
      "Epoch 13, Batch 8, LR 2.492020 Loss 19.056438, Accuracy 0.098%\n",
      "Epoch 13, Batch 9, LR 2.492005 Loss 19.036292, Accuracy 0.087%\n",
      "Epoch 13, Batch 10, LR 2.491990 Loss 19.028317, Accuracy 0.078%\n",
      "Epoch 13, Batch 11, LR 2.491974 Loss 19.021549, Accuracy 0.142%\n",
      "Epoch 13, Batch 12, LR 2.491959 Loss 19.021622, Accuracy 0.130%\n",
      "Epoch 13, Batch 13, LR 2.491944 Loss 19.015056, Accuracy 0.120%\n",
      "Epoch 13, Batch 14, LR 2.491929 Loss 19.009371, Accuracy 0.112%\n",
      "Epoch 13, Batch 15, LR 2.491914 Loss 19.011871, Accuracy 0.104%\n",
      "Epoch 13, Batch 16, LR 2.491899 Loss 19.009649, Accuracy 0.098%\n",
      "Epoch 13, Batch 17, LR 2.491883 Loss 19.000140, Accuracy 0.092%\n",
      "Epoch 13, Batch 18, LR 2.491868 Loss 19.002392, Accuracy 0.087%\n",
      "Epoch 13, Batch 19, LR 2.491853 Loss 18.998211, Accuracy 0.082%\n",
      "Epoch 13, Batch 20, LR 2.491837 Loss 18.990790, Accuracy 0.078%\n",
      "Epoch 13, Batch 21, LR 2.491822 Loss 18.998262, Accuracy 0.074%\n",
      "Epoch 13, Batch 22, LR 2.491807 Loss 19.002134, Accuracy 0.071%\n",
      "Epoch 13, Batch 23, LR 2.491792 Loss 19.011511, Accuracy 0.068%\n",
      "Epoch 13, Batch 24, LR 2.491776 Loss 19.018365, Accuracy 0.065%\n",
      "Epoch 13, Batch 25, LR 2.491761 Loss 19.011706, Accuracy 0.062%\n",
      "Epoch 13, Batch 26, LR 2.491746 Loss 19.015979, Accuracy 0.060%\n",
      "Epoch 13, Batch 27, LR 2.491730 Loss 19.018105, Accuracy 0.058%\n",
      "Epoch 13, Batch 28, LR 2.491715 Loss 19.012025, Accuracy 0.056%\n",
      "Epoch 13, Batch 29, LR 2.491699 Loss 19.013493, Accuracy 0.054%\n",
      "Epoch 13, Batch 30, LR 2.491684 Loss 19.016154, Accuracy 0.052%\n",
      "Epoch 13, Batch 31, LR 2.491669 Loss 19.017680, Accuracy 0.050%\n",
      "Epoch 13, Batch 32, LR 2.491653 Loss 19.017505, Accuracy 0.049%\n",
      "Epoch 13, Batch 33, LR 2.491638 Loss 19.024786, Accuracy 0.047%\n",
      "Epoch 13, Batch 34, LR 2.491622 Loss 19.032907, Accuracy 0.046%\n",
      "Epoch 13, Batch 35, LR 2.491607 Loss 19.035052, Accuracy 0.045%\n",
      "Epoch 13, Batch 36, LR 2.491591 Loss 19.035068, Accuracy 0.043%\n",
      "Epoch 13, Batch 37, LR 2.491576 Loss 19.036614, Accuracy 0.042%\n",
      "Epoch 13, Batch 38, LR 2.491560 Loss 19.040865, Accuracy 0.041%\n",
      "Epoch 13, Batch 39, LR 2.491545 Loss 19.035784, Accuracy 0.040%\n",
      "Epoch 13, Batch 40, LR 2.491529 Loss 19.040146, Accuracy 0.039%\n",
      "Epoch 13, Batch 41, LR 2.491513 Loss 19.041668, Accuracy 0.038%\n",
      "Epoch 13, Batch 42, LR 2.491498 Loss 19.038224, Accuracy 0.037%\n",
      "Epoch 13, Batch 43, LR 2.491482 Loss 19.037683, Accuracy 0.036%\n",
      "Epoch 13, Batch 44, LR 2.491467 Loss 19.036670, Accuracy 0.036%\n",
      "Epoch 13, Batch 45, LR 2.491451 Loss 19.039979, Accuracy 0.035%\n",
      "Epoch 13, Batch 46, LR 2.491435 Loss 19.038055, Accuracy 0.034%\n",
      "Epoch 13, Batch 47, LR 2.491420 Loss 19.037513, Accuracy 0.033%\n",
      "Epoch 13, Batch 48, LR 2.491404 Loss 19.043537, Accuracy 0.033%\n",
      "Epoch 13, Batch 49, LR 2.491388 Loss 19.044814, Accuracy 0.032%\n",
      "Epoch 13, Batch 50, LR 2.491373 Loss 19.047748, Accuracy 0.031%\n",
      "Epoch 13, Batch 51, LR 2.491357 Loss 19.049121, Accuracy 0.031%\n",
      "Epoch 13, Batch 52, LR 2.491341 Loss 19.052082, Accuracy 0.030%\n",
      "Epoch 13, Batch 53, LR 2.491325 Loss 19.054052, Accuracy 0.029%\n",
      "Epoch 13, Batch 54, LR 2.491310 Loss 19.056069, Accuracy 0.029%\n",
      "Epoch 13, Batch 55, LR 2.491294 Loss 19.056590, Accuracy 0.028%\n",
      "Epoch 13, Batch 56, LR 2.491278 Loss 19.054834, Accuracy 0.042%\n",
      "Epoch 13, Batch 57, LR 2.491262 Loss 19.056915, Accuracy 0.041%\n",
      "Epoch 13, Batch 58, LR 2.491246 Loss 19.053623, Accuracy 0.040%\n",
      "Epoch 13, Batch 59, LR 2.491231 Loss 19.054460, Accuracy 0.040%\n",
      "Epoch 13, Batch 60, LR 2.491215 Loss 19.055914, Accuracy 0.039%\n",
      "Epoch 13, Batch 61, LR 2.491199 Loss 19.055970, Accuracy 0.038%\n",
      "Epoch 13, Batch 62, LR 2.491183 Loss 19.055532, Accuracy 0.038%\n",
      "Epoch 13, Batch 63, LR 2.491167 Loss 19.056626, Accuracy 0.037%\n",
      "Epoch 13, Batch 64, LR 2.491151 Loss 19.056264, Accuracy 0.037%\n",
      "Epoch 13, Batch 65, LR 2.491135 Loss 19.056554, Accuracy 0.036%\n",
      "Epoch 13, Batch 66, LR 2.491119 Loss 19.057911, Accuracy 0.036%\n",
      "Epoch 13, Batch 67, LR 2.491103 Loss 19.057686, Accuracy 0.035%\n",
      "Epoch 13, Batch 68, LR 2.491087 Loss 19.060228, Accuracy 0.034%\n",
      "Epoch 13, Batch 69, LR 2.491071 Loss 19.059577, Accuracy 0.034%\n",
      "Epoch 13, Batch 70, LR 2.491056 Loss 19.059051, Accuracy 0.033%\n",
      "Epoch 13, Batch 71, LR 2.491040 Loss 19.059123, Accuracy 0.033%\n",
      "Epoch 13, Batch 72, LR 2.491023 Loss 19.059862, Accuracy 0.033%\n",
      "Epoch 13, Batch 73, LR 2.491007 Loss 19.058149, Accuracy 0.032%\n",
      "Epoch 13, Batch 74, LR 2.490991 Loss 19.059275, Accuracy 0.032%\n",
      "Epoch 13, Batch 75, LR 2.490975 Loss 19.062219, Accuracy 0.031%\n",
      "Epoch 13, Batch 76, LR 2.490959 Loss 19.062948, Accuracy 0.031%\n",
      "Epoch 13, Batch 77, LR 2.490943 Loss 19.064649, Accuracy 0.030%\n",
      "Epoch 13, Batch 78, LR 2.490927 Loss 19.065028, Accuracy 0.030%\n",
      "Epoch 13, Batch 79, LR 2.490911 Loss 19.062551, Accuracy 0.030%\n",
      "Epoch 13, Batch 80, LR 2.490895 Loss 19.060463, Accuracy 0.029%\n",
      "Epoch 13, Batch 81, LR 2.490879 Loss 19.058856, Accuracy 0.029%\n",
      "Epoch 13, Batch 82, LR 2.490863 Loss 19.058441, Accuracy 0.029%\n",
      "Epoch 13, Batch 83, LR 2.490846 Loss 19.059546, Accuracy 0.028%\n",
      "Epoch 13, Batch 84, LR 2.490830 Loss 19.060172, Accuracy 0.028%\n",
      "Epoch 13, Batch 85, LR 2.490814 Loss 19.061190, Accuracy 0.037%\n",
      "Epoch 13, Batch 86, LR 2.490798 Loss 19.059436, Accuracy 0.036%\n",
      "Epoch 13, Batch 87, LR 2.490782 Loss 19.057689, Accuracy 0.036%\n",
      "Epoch 13, Batch 88, LR 2.490765 Loss 19.057782, Accuracy 0.036%\n",
      "Epoch 13, Batch 89, LR 2.490749 Loss 19.057254, Accuracy 0.035%\n",
      "Epoch 13, Batch 90, LR 2.490733 Loss 19.057885, Accuracy 0.035%\n",
      "Epoch 13, Batch 91, LR 2.490716 Loss 19.058280, Accuracy 0.034%\n",
      "Epoch 13, Batch 92, LR 2.490700 Loss 19.057407, Accuracy 0.034%\n",
      "Epoch 13, Batch 93, LR 2.490684 Loss 19.056457, Accuracy 0.034%\n",
      "Epoch 13, Batch 94, LR 2.490667 Loss 19.059424, Accuracy 0.033%\n",
      "Epoch 13, Batch 95, LR 2.490651 Loss 19.058214, Accuracy 0.033%\n",
      "Epoch 13, Batch 96, LR 2.490635 Loss 19.060237, Accuracy 0.041%\n",
      "Epoch 13, Batch 97, LR 2.490618 Loss 19.060546, Accuracy 0.040%\n",
      "Epoch 13, Batch 98, LR 2.490602 Loss 19.061176, Accuracy 0.040%\n",
      "Epoch 13, Batch 99, LR 2.490586 Loss 19.062012, Accuracy 0.039%\n",
      "Epoch 13, Batch 100, LR 2.490569 Loss 19.062664, Accuracy 0.039%\n",
      "Epoch 13, Batch 101, LR 2.490553 Loss 19.063682, Accuracy 0.039%\n",
      "Epoch 13, Batch 102, LR 2.490536 Loss 19.062825, Accuracy 0.038%\n",
      "Epoch 13, Batch 103, LR 2.490520 Loss 19.061684, Accuracy 0.038%\n",
      "Epoch 13, Batch 104, LR 2.490503 Loss 19.061948, Accuracy 0.038%\n",
      "Epoch 13, Batch 105, LR 2.490487 Loss 19.061890, Accuracy 0.037%\n",
      "Epoch 13, Batch 106, LR 2.490470 Loss 19.061289, Accuracy 0.037%\n",
      "Epoch 13, Batch 107, LR 2.490454 Loss 19.061638, Accuracy 0.037%\n",
      "Epoch 13, Batch 108, LR 2.490437 Loss 19.060117, Accuracy 0.036%\n",
      "Epoch 13, Batch 109, LR 2.490421 Loss 19.057840, Accuracy 0.036%\n",
      "Epoch 13, Batch 110, LR 2.490404 Loss 19.059339, Accuracy 0.036%\n",
      "Epoch 13, Batch 111, LR 2.490388 Loss 19.058394, Accuracy 0.035%\n",
      "Epoch 13, Batch 112, LR 2.490371 Loss 19.056942, Accuracy 0.035%\n",
      "Epoch 13, Batch 113, LR 2.490354 Loss 19.055030, Accuracy 0.035%\n",
      "Epoch 13, Batch 114, LR 2.490338 Loss 19.054833, Accuracy 0.034%\n",
      "Epoch 13, Batch 115, LR 2.490321 Loss 19.055153, Accuracy 0.034%\n",
      "Epoch 13, Batch 116, LR 2.490305 Loss 19.054555, Accuracy 0.040%\n",
      "Epoch 13, Batch 117, LR 2.490288 Loss 19.054599, Accuracy 0.040%\n",
      "Epoch 13, Batch 118, LR 2.490271 Loss 19.054068, Accuracy 0.040%\n",
      "Epoch 13, Batch 119, LR 2.490255 Loss 19.053910, Accuracy 0.039%\n",
      "Epoch 13, Batch 120, LR 2.490238 Loss 19.054074, Accuracy 0.039%\n",
      "Epoch 13, Batch 121, LR 2.490221 Loss 19.054091, Accuracy 0.039%\n",
      "Epoch 13, Batch 122, LR 2.490204 Loss 19.053982, Accuracy 0.038%\n",
      "Epoch 13, Batch 123, LR 2.490188 Loss 19.053708, Accuracy 0.038%\n",
      "Epoch 13, Batch 124, LR 2.490171 Loss 19.052798, Accuracy 0.044%\n",
      "Epoch 13, Batch 125, LR 2.490154 Loss 19.052948, Accuracy 0.044%\n",
      "Epoch 13, Batch 126, LR 2.490137 Loss 19.053021, Accuracy 0.043%\n",
      "Epoch 13, Batch 127, LR 2.490121 Loss 19.054567, Accuracy 0.043%\n",
      "Epoch 13, Batch 128, LR 2.490104 Loss 19.057948, Accuracy 0.043%\n",
      "Epoch 13, Batch 129, LR 2.490087 Loss 19.057680, Accuracy 0.048%\n",
      "Epoch 13, Batch 130, LR 2.490070 Loss 19.060034, Accuracy 0.048%\n",
      "Epoch 13, Batch 131, LR 2.490053 Loss 19.058543, Accuracy 0.048%\n",
      "Epoch 13, Batch 132, LR 2.490036 Loss 19.059587, Accuracy 0.047%\n",
      "Epoch 13, Batch 133, LR 2.490019 Loss 19.060640, Accuracy 0.047%\n",
      "Epoch 13, Batch 134, LR 2.490003 Loss 19.060296, Accuracy 0.047%\n",
      "Epoch 13, Batch 135, LR 2.489986 Loss 19.061148, Accuracy 0.046%\n",
      "Epoch 13, Batch 136, LR 2.489969 Loss 19.060757, Accuracy 0.052%\n",
      "Epoch 13, Batch 137, LR 2.489952 Loss 19.058847, Accuracy 0.051%\n",
      "Epoch 13, Batch 138, LR 2.489935 Loss 19.057216, Accuracy 0.051%\n",
      "Epoch 13, Batch 139, LR 2.489918 Loss 19.056718, Accuracy 0.051%\n",
      "Epoch 13, Batch 140, LR 2.489901 Loss 19.057354, Accuracy 0.050%\n",
      "Epoch 13, Batch 141, LR 2.489884 Loss 19.058562, Accuracy 0.050%\n",
      "Epoch 13, Batch 142, LR 2.489867 Loss 19.058652, Accuracy 0.050%\n",
      "Epoch 13, Batch 143, LR 2.489850 Loss 19.058046, Accuracy 0.049%\n",
      "Epoch 13, Batch 144, LR 2.489833 Loss 19.058466, Accuracy 0.049%\n",
      "Epoch 13, Batch 145, LR 2.489816 Loss 19.057679, Accuracy 0.048%\n",
      "Epoch 13, Batch 146, LR 2.489799 Loss 19.057668, Accuracy 0.048%\n",
      "Epoch 13, Batch 147, LR 2.489782 Loss 19.057924, Accuracy 0.048%\n",
      "Epoch 13, Batch 148, LR 2.489764 Loss 19.058867, Accuracy 0.048%\n",
      "Epoch 13, Batch 149, LR 2.489747 Loss 19.059424, Accuracy 0.047%\n",
      "Epoch 13, Batch 150, LR 2.489730 Loss 19.059282, Accuracy 0.047%\n",
      "Epoch 13, Batch 151, LR 2.489713 Loss 19.058968, Accuracy 0.047%\n",
      "Epoch 13, Batch 152, LR 2.489696 Loss 19.059255, Accuracy 0.046%\n",
      "Epoch 13, Batch 153, LR 2.489679 Loss 19.058789, Accuracy 0.046%\n",
      "Epoch 13, Batch 154, LR 2.489662 Loss 19.060108, Accuracy 0.046%\n",
      "Epoch 13, Batch 155, LR 2.489644 Loss 19.060505, Accuracy 0.050%\n",
      "Epoch 13, Batch 156, LR 2.489627 Loss 19.061304, Accuracy 0.050%\n",
      "Epoch 13, Batch 157, LR 2.489610 Loss 19.061249, Accuracy 0.050%\n",
      "Epoch 13, Batch 158, LR 2.489593 Loss 19.061390, Accuracy 0.049%\n",
      "Epoch 13, Batch 159, LR 2.489575 Loss 19.061693, Accuracy 0.049%\n",
      "Epoch 13, Batch 160, LR 2.489558 Loss 19.061004, Accuracy 0.049%\n",
      "Epoch 13, Batch 161, LR 2.489541 Loss 19.060194, Accuracy 0.053%\n",
      "Epoch 13, Batch 162, LR 2.489524 Loss 19.061116, Accuracy 0.053%\n",
      "Epoch 13, Batch 163, LR 2.489506 Loss 19.060784, Accuracy 0.053%\n",
      "Epoch 13, Batch 164, LR 2.489489 Loss 19.060584, Accuracy 0.052%\n",
      "Epoch 13, Batch 165, LR 2.489472 Loss 19.061149, Accuracy 0.052%\n",
      "Epoch 13, Batch 166, LR 2.489454 Loss 19.060452, Accuracy 0.052%\n",
      "Epoch 13, Batch 167, LR 2.489437 Loss 19.059596, Accuracy 0.051%\n",
      "Epoch 13, Batch 168, LR 2.489419 Loss 19.059283, Accuracy 0.051%\n",
      "Epoch 13, Batch 169, LR 2.489402 Loss 19.058794, Accuracy 0.051%\n",
      "Epoch 13, Batch 170, LR 2.489385 Loss 19.057946, Accuracy 0.051%\n",
      "Epoch 13, Batch 171, LR 2.489367 Loss 19.058145, Accuracy 0.050%\n",
      "Epoch 13, Batch 172, LR 2.489350 Loss 19.057411, Accuracy 0.050%\n",
      "Epoch 13, Batch 173, LR 2.489332 Loss 19.057735, Accuracy 0.050%\n",
      "Epoch 13, Batch 174, LR 2.489315 Loss 19.056443, Accuracy 0.049%\n",
      "Epoch 13, Batch 175, LR 2.489297 Loss 19.057773, Accuracy 0.049%\n",
      "Epoch 13, Batch 176, LR 2.489280 Loss 19.057517, Accuracy 0.049%\n",
      "Epoch 13, Batch 177, LR 2.489262 Loss 19.057485, Accuracy 0.049%\n",
      "Epoch 13, Batch 178, LR 2.489245 Loss 19.056608, Accuracy 0.048%\n",
      "Epoch 13, Batch 179, LR 2.489227 Loss 19.056501, Accuracy 0.048%\n",
      "Epoch 13, Batch 180, LR 2.489210 Loss 19.056530, Accuracy 0.048%\n",
      "Epoch 13, Batch 181, LR 2.489192 Loss 19.056169, Accuracy 0.047%\n",
      "Epoch 13, Batch 182, LR 2.489175 Loss 19.055597, Accuracy 0.047%\n",
      "Epoch 13, Batch 183, LR 2.489157 Loss 19.055757, Accuracy 0.047%\n",
      "Epoch 13, Batch 184, LR 2.489139 Loss 19.056429, Accuracy 0.047%\n",
      "Epoch 13, Batch 185, LR 2.489122 Loss 19.057279, Accuracy 0.046%\n",
      "Epoch 13, Batch 186, LR 2.489104 Loss 19.057099, Accuracy 0.046%\n",
      "Epoch 13, Batch 187, LR 2.489086 Loss 19.057316, Accuracy 0.046%\n",
      "Epoch 13, Batch 188, LR 2.489069 Loss 19.057190, Accuracy 0.046%\n",
      "Epoch 13, Batch 189, LR 2.489051 Loss 19.056453, Accuracy 0.045%\n",
      "Epoch 13, Batch 190, LR 2.489033 Loss 19.057889, Accuracy 0.049%\n",
      "Epoch 13, Batch 191, LR 2.489016 Loss 19.058147, Accuracy 0.049%\n",
      "Epoch 13, Batch 192, LR 2.488998 Loss 19.058352, Accuracy 0.049%\n",
      "Epoch 13, Batch 193, LR 2.488980 Loss 19.058648, Accuracy 0.049%\n",
      "Epoch 13, Batch 194, LR 2.488962 Loss 19.058987, Accuracy 0.048%\n",
      "Epoch 13, Batch 195, LR 2.488945 Loss 19.059336, Accuracy 0.048%\n",
      "Epoch 13, Batch 196, LR 2.488927 Loss 19.059398, Accuracy 0.048%\n",
      "Epoch 13, Batch 197, LR 2.488909 Loss 19.058755, Accuracy 0.048%\n",
      "Epoch 13, Batch 198, LR 2.488891 Loss 19.059400, Accuracy 0.047%\n",
      "Epoch 13, Batch 199, LR 2.488873 Loss 19.060462, Accuracy 0.047%\n",
      "Epoch 13, Batch 200, LR 2.488856 Loss 19.061012, Accuracy 0.047%\n",
      "Epoch 13, Batch 201, LR 2.488838 Loss 19.060835, Accuracy 0.047%\n",
      "Epoch 13, Batch 202, LR 2.488820 Loss 19.061065, Accuracy 0.046%\n",
      "Epoch 13, Batch 203, LR 2.488802 Loss 19.060744, Accuracy 0.046%\n",
      "Epoch 13, Batch 204, LR 2.488784 Loss 19.060217, Accuracy 0.050%\n",
      "Epoch 13, Batch 205, LR 2.488766 Loss 19.060687, Accuracy 0.050%\n",
      "Epoch 13, Batch 206, LR 2.488748 Loss 19.060058, Accuracy 0.049%\n",
      "Epoch 13, Batch 207, LR 2.488730 Loss 19.060777, Accuracy 0.053%\n",
      "Epoch 13, Batch 208, LR 2.488712 Loss 19.060766, Accuracy 0.053%\n",
      "Epoch 13, Batch 209, LR 2.488694 Loss 19.061960, Accuracy 0.052%\n",
      "Epoch 13, Batch 210, LR 2.488676 Loss 19.062278, Accuracy 0.052%\n",
      "Epoch 13, Batch 211, LR 2.488658 Loss 19.061522, Accuracy 0.052%\n",
      "Epoch 13, Batch 212, LR 2.488640 Loss 19.061214, Accuracy 0.052%\n",
      "Epoch 13, Batch 213, LR 2.488622 Loss 19.061761, Accuracy 0.051%\n",
      "Epoch 13, Batch 214, LR 2.488604 Loss 19.062381, Accuracy 0.051%\n",
      "Epoch 13, Batch 215, LR 2.488586 Loss 19.062396, Accuracy 0.051%\n",
      "Epoch 13, Batch 216, LR 2.488568 Loss 19.062273, Accuracy 0.051%\n",
      "Epoch 13, Batch 217, LR 2.488550 Loss 19.062843, Accuracy 0.050%\n",
      "Epoch 13, Batch 218, LR 2.488532 Loss 19.063083, Accuracy 0.050%\n",
      "Epoch 13, Batch 219, LR 2.488514 Loss 19.062912, Accuracy 0.050%\n",
      "Epoch 13, Batch 220, LR 2.488496 Loss 19.061244, Accuracy 0.053%\n",
      "Epoch 13, Batch 221, LR 2.488478 Loss 19.061010, Accuracy 0.053%\n",
      "Epoch 13, Batch 222, LR 2.488460 Loss 19.060739, Accuracy 0.053%\n",
      "Epoch 13, Batch 223, LR 2.488441 Loss 19.059913, Accuracy 0.053%\n",
      "Epoch 13, Batch 224, LR 2.488423 Loss 19.060921, Accuracy 0.052%\n",
      "Epoch 13, Batch 225, LR 2.488405 Loss 19.060874, Accuracy 0.052%\n",
      "Epoch 13, Batch 226, LR 2.488387 Loss 19.061650, Accuracy 0.052%\n",
      "Epoch 13, Batch 227, LR 2.488369 Loss 19.060618, Accuracy 0.052%\n",
      "Epoch 13, Batch 228, LR 2.488350 Loss 19.059957, Accuracy 0.051%\n",
      "Epoch 13, Batch 229, LR 2.488332 Loss 19.060335, Accuracy 0.055%\n",
      "Epoch 13, Batch 230, LR 2.488314 Loss 19.060851, Accuracy 0.054%\n",
      "Epoch 13, Batch 231, LR 2.488296 Loss 19.060632, Accuracy 0.054%\n",
      "Epoch 13, Batch 232, LR 2.488277 Loss 19.061226, Accuracy 0.054%\n",
      "Epoch 13, Batch 233, LR 2.488259 Loss 19.060935, Accuracy 0.054%\n",
      "Epoch 13, Batch 234, LR 2.488241 Loss 19.060429, Accuracy 0.053%\n",
      "Epoch 13, Batch 235, LR 2.488222 Loss 19.060282, Accuracy 0.053%\n",
      "Epoch 13, Batch 236, LR 2.488204 Loss 19.060566, Accuracy 0.053%\n",
      "Epoch 13, Batch 237, LR 2.488186 Loss 19.059933, Accuracy 0.053%\n",
      "Epoch 13, Batch 238, LR 2.488167 Loss 19.060906, Accuracy 0.053%\n",
      "Epoch 13, Batch 239, LR 2.488149 Loss 19.061020, Accuracy 0.052%\n",
      "Epoch 13, Batch 240, LR 2.488130 Loss 19.061002, Accuracy 0.052%\n",
      "Epoch 13, Batch 241, LR 2.488112 Loss 19.061811, Accuracy 0.052%\n",
      "Epoch 13, Batch 242, LR 2.488094 Loss 19.061337, Accuracy 0.052%\n",
      "Epoch 13, Batch 243, LR 2.488075 Loss 19.062201, Accuracy 0.051%\n",
      "Epoch 13, Batch 244, LR 2.488057 Loss 19.061575, Accuracy 0.051%\n",
      "Epoch 13, Batch 245, LR 2.488038 Loss 19.061264, Accuracy 0.051%\n",
      "Epoch 13, Batch 246, LR 2.488020 Loss 19.061968, Accuracy 0.051%\n",
      "Epoch 13, Batch 247, LR 2.488001 Loss 19.062793, Accuracy 0.051%\n",
      "Epoch 13, Batch 248, LR 2.487983 Loss 19.062665, Accuracy 0.050%\n",
      "Epoch 13, Batch 249, LR 2.487964 Loss 19.062257, Accuracy 0.050%\n",
      "Epoch 13, Batch 250, LR 2.487946 Loss 19.062347, Accuracy 0.050%\n",
      "Epoch 13, Batch 251, LR 2.487927 Loss 19.062555, Accuracy 0.053%\n",
      "Epoch 13, Batch 252, LR 2.487908 Loss 19.061770, Accuracy 0.053%\n",
      "Epoch 13, Batch 253, LR 2.487890 Loss 19.061689, Accuracy 0.052%\n",
      "Epoch 13, Batch 254, LR 2.487871 Loss 19.061723, Accuracy 0.052%\n",
      "Epoch 13, Batch 255, LR 2.487853 Loss 19.061239, Accuracy 0.052%\n",
      "Epoch 13, Batch 256, LR 2.487834 Loss 19.061293, Accuracy 0.052%\n",
      "Epoch 13, Batch 257, LR 2.487815 Loss 19.061270, Accuracy 0.052%\n",
      "Epoch 13, Batch 258, LR 2.487797 Loss 19.060765, Accuracy 0.051%\n",
      "Epoch 13, Batch 259, LR 2.487778 Loss 19.060619, Accuracy 0.051%\n",
      "Epoch 13, Batch 260, LR 2.487759 Loss 19.061268, Accuracy 0.051%\n",
      "Epoch 13, Batch 261, LR 2.487741 Loss 19.061093, Accuracy 0.051%\n",
      "Epoch 13, Batch 262, LR 2.487722 Loss 19.062222, Accuracy 0.051%\n",
      "Epoch 13, Batch 263, LR 2.487703 Loss 19.062542, Accuracy 0.050%\n",
      "Epoch 13, Batch 264, LR 2.487684 Loss 19.062372, Accuracy 0.053%\n",
      "Epoch 13, Batch 265, LR 2.487666 Loss 19.062019, Accuracy 0.053%\n",
      "Epoch 13, Batch 266, LR 2.487647 Loss 19.061960, Accuracy 0.053%\n",
      "Epoch 13, Batch 267, LR 2.487628 Loss 19.061962, Accuracy 0.053%\n",
      "Epoch 13, Batch 268, LR 2.487609 Loss 19.062422, Accuracy 0.052%\n",
      "Epoch 13, Batch 269, LR 2.487590 Loss 19.062437, Accuracy 0.052%\n",
      "Epoch 13, Batch 270, LR 2.487572 Loss 19.063040, Accuracy 0.052%\n",
      "Epoch 13, Batch 271, LR 2.487553 Loss 19.062718, Accuracy 0.055%\n",
      "Epoch 13, Batch 272, LR 2.487534 Loss 19.063219, Accuracy 0.055%\n",
      "Epoch 13, Batch 273, LR 2.487515 Loss 19.062437, Accuracy 0.054%\n",
      "Epoch 13, Batch 274, LR 2.487496 Loss 19.062616, Accuracy 0.054%\n",
      "Epoch 13, Batch 275, LR 2.487477 Loss 19.062448, Accuracy 0.054%\n",
      "Epoch 13, Batch 276, LR 2.487458 Loss 19.061949, Accuracy 0.054%\n",
      "Epoch 13, Batch 277, LR 2.487439 Loss 19.062479, Accuracy 0.054%\n",
      "Epoch 13, Batch 278, LR 2.487420 Loss 19.061552, Accuracy 0.053%\n",
      "Epoch 13, Batch 279, LR 2.487401 Loss 19.061487, Accuracy 0.053%\n",
      "Epoch 13, Batch 280, LR 2.487382 Loss 19.061650, Accuracy 0.053%\n",
      "Epoch 13, Batch 281, LR 2.487363 Loss 19.061450, Accuracy 0.053%\n",
      "Epoch 13, Batch 282, LR 2.487344 Loss 19.061404, Accuracy 0.053%\n",
      "Epoch 13, Batch 283, LR 2.487325 Loss 19.061759, Accuracy 0.052%\n",
      "Epoch 13, Batch 284, LR 2.487306 Loss 19.062016, Accuracy 0.052%\n",
      "Epoch 13, Batch 285, LR 2.487287 Loss 19.062028, Accuracy 0.055%\n",
      "Epoch 13, Batch 286, LR 2.487268 Loss 19.061500, Accuracy 0.055%\n",
      "Epoch 13, Batch 287, LR 2.487249 Loss 19.061450, Accuracy 0.054%\n",
      "Epoch 13, Batch 288, LR 2.487230 Loss 19.062101, Accuracy 0.054%\n",
      "Epoch 13, Batch 289, LR 2.487211 Loss 19.062392, Accuracy 0.054%\n",
      "Epoch 13, Batch 290, LR 2.487192 Loss 19.061652, Accuracy 0.059%\n",
      "Epoch 13, Batch 291, LR 2.487173 Loss 19.062646, Accuracy 0.059%\n",
      "Epoch 13, Batch 292, LR 2.487154 Loss 19.061738, Accuracy 0.062%\n",
      "Epoch 13, Batch 293, LR 2.487134 Loss 19.061514, Accuracy 0.064%\n",
      "Epoch 13, Batch 294, LR 2.487115 Loss 19.062240, Accuracy 0.064%\n",
      "Epoch 13, Batch 295, LR 2.487096 Loss 19.062495, Accuracy 0.064%\n",
      "Epoch 13, Batch 296, LR 2.487077 Loss 19.063177, Accuracy 0.063%\n",
      "Epoch 13, Batch 297, LR 2.487058 Loss 19.063382, Accuracy 0.063%\n",
      "Epoch 13, Batch 298, LR 2.487038 Loss 19.063474, Accuracy 0.063%\n",
      "Epoch 13, Batch 299, LR 2.487019 Loss 19.063860, Accuracy 0.063%\n",
      "Epoch 13, Batch 300, LR 2.487000 Loss 19.063999, Accuracy 0.062%\n",
      "Epoch 13, Batch 301, LR 2.486981 Loss 19.063291, Accuracy 0.062%\n",
      "Epoch 13, Batch 302, LR 2.486961 Loss 19.062613, Accuracy 0.062%\n",
      "Epoch 13, Batch 303, LR 2.486942 Loss 19.062359, Accuracy 0.062%\n",
      "Epoch 13, Batch 304, LR 2.486923 Loss 19.062222, Accuracy 0.062%\n",
      "Epoch 13, Batch 305, LR 2.486903 Loss 19.061947, Accuracy 0.061%\n",
      "Epoch 13, Batch 306, LR 2.486884 Loss 19.062188, Accuracy 0.061%\n",
      "Epoch 13, Batch 307, LR 2.486865 Loss 19.062496, Accuracy 0.061%\n",
      "Epoch 13, Batch 308, LR 2.486845 Loss 19.062306, Accuracy 0.061%\n",
      "Epoch 13, Batch 309, LR 2.486826 Loss 19.061558, Accuracy 0.061%\n",
      "Epoch 13, Batch 310, LR 2.486806 Loss 19.061936, Accuracy 0.063%\n",
      "Epoch 13, Batch 311, LR 2.486787 Loss 19.061687, Accuracy 0.063%\n",
      "Epoch 13, Batch 312, LR 2.486768 Loss 19.061888, Accuracy 0.063%\n",
      "Epoch 13, Batch 313, LR 2.486748 Loss 19.061979, Accuracy 0.062%\n",
      "Epoch 13, Batch 314, LR 2.486729 Loss 19.061699, Accuracy 0.062%\n",
      "Epoch 13, Batch 315, LR 2.486709 Loss 19.061335, Accuracy 0.062%\n",
      "Epoch 13, Batch 316, LR 2.486690 Loss 19.061607, Accuracy 0.062%\n",
      "Epoch 13, Batch 317, LR 2.486670 Loss 19.061186, Accuracy 0.062%\n",
      "Epoch 13, Batch 318, LR 2.486651 Loss 19.061796, Accuracy 0.061%\n",
      "Epoch 13, Batch 319, LR 2.486631 Loss 19.062296, Accuracy 0.061%\n",
      "Epoch 13, Batch 320, LR 2.486612 Loss 19.062191, Accuracy 0.061%\n",
      "Epoch 13, Batch 321, LR 2.486592 Loss 19.061918, Accuracy 0.061%\n",
      "Epoch 13, Batch 322, LR 2.486572 Loss 19.062396, Accuracy 0.061%\n",
      "Epoch 13, Batch 323, LR 2.486553 Loss 19.062098, Accuracy 0.060%\n",
      "Epoch 13, Batch 324, LR 2.486533 Loss 19.061682, Accuracy 0.060%\n",
      "Epoch 13, Batch 325, LR 2.486514 Loss 19.061258, Accuracy 0.060%\n",
      "Epoch 13, Batch 326, LR 2.486494 Loss 19.060929, Accuracy 0.060%\n",
      "Epoch 13, Batch 327, LR 2.486474 Loss 19.062038, Accuracy 0.060%\n",
      "Epoch 13, Batch 328, LR 2.486455 Loss 19.061759, Accuracy 0.060%\n",
      "Epoch 13, Batch 329, LR 2.486435 Loss 19.061986, Accuracy 0.059%\n",
      "Epoch 13, Batch 330, LR 2.486415 Loss 19.061526, Accuracy 0.062%\n",
      "Epoch 13, Batch 331, LR 2.486396 Loss 19.061289, Accuracy 0.064%\n",
      "Epoch 13, Batch 332, LR 2.486376 Loss 19.061755, Accuracy 0.064%\n",
      "Epoch 13, Batch 333, LR 2.486356 Loss 19.061984, Accuracy 0.063%\n",
      "Epoch 13, Batch 334, LR 2.486336 Loss 19.061827, Accuracy 0.065%\n",
      "Epoch 13, Batch 335, LR 2.486317 Loss 19.062211, Accuracy 0.068%\n",
      "Epoch 13, Batch 336, LR 2.486297 Loss 19.061900, Accuracy 0.067%\n",
      "Epoch 13, Batch 337, LR 2.486277 Loss 19.061880, Accuracy 0.067%\n",
      "Epoch 13, Batch 338, LR 2.486257 Loss 19.062279, Accuracy 0.067%\n",
      "Epoch 13, Batch 339, LR 2.486238 Loss 19.062395, Accuracy 0.067%\n",
      "Epoch 13, Batch 340, LR 2.486218 Loss 19.062210, Accuracy 0.067%\n",
      "Epoch 13, Batch 341, LR 2.486198 Loss 19.061922, Accuracy 0.066%\n",
      "Epoch 13, Batch 342, LR 2.486178 Loss 19.062172, Accuracy 0.066%\n",
      "Epoch 13, Batch 343, LR 2.486158 Loss 19.062392, Accuracy 0.066%\n",
      "Epoch 13, Batch 344, LR 2.486138 Loss 19.062644, Accuracy 0.066%\n",
      "Epoch 13, Batch 345, LR 2.486118 Loss 19.062907, Accuracy 0.066%\n",
      "Epoch 13, Batch 346, LR 2.486098 Loss 19.063094, Accuracy 0.065%\n",
      "Epoch 13, Batch 347, LR 2.486079 Loss 19.063397, Accuracy 0.065%\n",
      "Epoch 13, Batch 348, LR 2.486059 Loss 19.063331, Accuracy 0.065%\n",
      "Epoch 13, Batch 349, LR 2.486039 Loss 19.062922, Accuracy 0.065%\n",
      "Epoch 13, Batch 350, LR 2.486019 Loss 19.062754, Accuracy 0.065%\n",
      "Epoch 13, Batch 351, LR 2.485999 Loss 19.062676, Accuracy 0.065%\n",
      "Epoch 13, Batch 352, LR 2.485979 Loss 19.063129, Accuracy 0.064%\n",
      "Epoch 13, Batch 353, LR 2.485959 Loss 19.063028, Accuracy 0.064%\n",
      "Epoch 13, Batch 354, LR 2.485939 Loss 19.062878, Accuracy 0.066%\n",
      "Epoch 13, Batch 355, LR 2.485919 Loss 19.062342, Accuracy 0.066%\n",
      "Epoch 13, Batch 356, LR 2.485899 Loss 19.062349, Accuracy 0.066%\n",
      "Epoch 13, Batch 357, LR 2.485878 Loss 19.062364, Accuracy 0.066%\n",
      "Epoch 13, Batch 358, LR 2.485858 Loss 19.062859, Accuracy 0.065%\n",
      "Epoch 13, Batch 359, LR 2.485838 Loss 19.063193, Accuracy 0.065%\n",
      "Epoch 13, Batch 360, LR 2.485818 Loss 19.063566, Accuracy 0.065%\n",
      "Epoch 13, Batch 361, LR 2.485798 Loss 19.063574, Accuracy 0.065%\n",
      "Epoch 13, Batch 362, LR 2.485778 Loss 19.063114, Accuracy 0.065%\n",
      "Epoch 13, Batch 363, LR 2.485758 Loss 19.063281, Accuracy 0.065%\n",
      "Epoch 13, Batch 364, LR 2.485738 Loss 19.063435, Accuracy 0.064%\n",
      "Epoch 13, Batch 365, LR 2.485717 Loss 19.063310, Accuracy 0.064%\n",
      "Epoch 13, Batch 366, LR 2.485697 Loss 19.063078, Accuracy 0.064%\n",
      "Epoch 13, Batch 367, LR 2.485677 Loss 19.063253, Accuracy 0.064%\n",
      "Epoch 13, Batch 368, LR 2.485657 Loss 19.063290, Accuracy 0.064%\n",
      "Epoch 13, Batch 369, LR 2.485636 Loss 19.063481, Accuracy 0.064%\n",
      "Epoch 13, Batch 370, LR 2.485616 Loss 19.064294, Accuracy 0.063%\n",
      "Epoch 13, Batch 371, LR 2.485596 Loss 19.064332, Accuracy 0.063%\n",
      "Epoch 13, Batch 372, LR 2.485576 Loss 19.064283, Accuracy 0.063%\n",
      "Epoch 13, Batch 373, LR 2.485555 Loss 19.064423, Accuracy 0.063%\n",
      "Epoch 13, Batch 374, LR 2.485535 Loss 19.064290, Accuracy 0.063%\n",
      "Epoch 13, Batch 375, LR 2.485515 Loss 19.064324, Accuracy 0.062%\n",
      "Epoch 13, Batch 376, LR 2.485494 Loss 19.064818, Accuracy 0.062%\n",
      "Epoch 13, Batch 377, LR 2.485474 Loss 19.065308, Accuracy 0.062%\n",
      "Epoch 13, Batch 378, LR 2.485454 Loss 19.065056, Accuracy 0.062%\n",
      "Epoch 13, Batch 379, LR 2.485433 Loss 19.065277, Accuracy 0.062%\n",
      "Epoch 13, Batch 380, LR 2.485413 Loss 19.065619, Accuracy 0.062%\n",
      "Epoch 13, Batch 381, LR 2.485392 Loss 19.065525, Accuracy 0.062%\n",
      "Epoch 13, Batch 382, LR 2.485372 Loss 19.065213, Accuracy 0.061%\n",
      "Epoch 13, Batch 383, LR 2.485352 Loss 19.065622, Accuracy 0.061%\n",
      "Epoch 13, Batch 384, LR 2.485331 Loss 19.065642, Accuracy 0.061%\n",
      "Epoch 13, Batch 385, LR 2.485311 Loss 19.065236, Accuracy 0.061%\n",
      "Epoch 13, Batch 386, LR 2.485290 Loss 19.065005, Accuracy 0.061%\n",
      "Epoch 13, Batch 387, LR 2.485270 Loss 19.065212, Accuracy 0.061%\n",
      "Epoch 13, Batch 388, LR 2.485249 Loss 19.064582, Accuracy 0.060%\n",
      "Epoch 13, Batch 389, LR 2.485229 Loss 19.064887, Accuracy 0.060%\n",
      "Epoch 13, Batch 390, LR 2.485208 Loss 19.065049, Accuracy 0.060%\n",
      "Epoch 13, Batch 391, LR 2.485188 Loss 19.065445, Accuracy 0.060%\n",
      "Epoch 13, Batch 392, LR 2.485167 Loss 19.065406, Accuracy 0.060%\n",
      "Epoch 13, Batch 393, LR 2.485146 Loss 19.064996, Accuracy 0.060%\n",
      "Epoch 13, Batch 394, LR 2.485126 Loss 19.064791, Accuracy 0.061%\n",
      "Epoch 13, Batch 395, LR 2.485105 Loss 19.064641, Accuracy 0.061%\n",
      "Epoch 13, Batch 396, LR 2.485085 Loss 19.064629, Accuracy 0.061%\n",
      "Epoch 13, Batch 397, LR 2.485064 Loss 19.064134, Accuracy 0.061%\n",
      "Epoch 13, Batch 398, LR 2.485043 Loss 19.064395, Accuracy 0.061%\n",
      "Epoch 13, Batch 399, LR 2.485023 Loss 19.064176, Accuracy 0.061%\n",
      "Epoch 13, Batch 400, LR 2.485002 Loss 19.063860, Accuracy 0.061%\n",
      "Epoch 13, Batch 401, LR 2.484981 Loss 19.063683, Accuracy 0.060%\n",
      "Epoch 13, Batch 402, LR 2.484961 Loss 19.063236, Accuracy 0.062%\n",
      "Epoch 13, Batch 403, LR 2.484940 Loss 19.063370, Accuracy 0.062%\n",
      "Epoch 13, Batch 404, LR 2.484919 Loss 19.063310, Accuracy 0.062%\n",
      "Epoch 13, Batch 405, LR 2.484898 Loss 19.063163, Accuracy 0.062%\n",
      "Epoch 13, Batch 406, LR 2.484878 Loss 19.063376, Accuracy 0.062%\n",
      "Epoch 13, Batch 407, LR 2.484857 Loss 19.063126, Accuracy 0.061%\n",
      "Epoch 13, Batch 408, LR 2.484836 Loss 19.063263, Accuracy 0.061%\n",
      "Epoch 13, Batch 409, LR 2.484815 Loss 19.063343, Accuracy 0.061%\n",
      "Epoch 13, Batch 410, LR 2.484794 Loss 19.063299, Accuracy 0.061%\n",
      "Epoch 13, Batch 411, LR 2.484774 Loss 19.063352, Accuracy 0.061%\n",
      "Epoch 13, Batch 412, LR 2.484753 Loss 19.063354, Accuracy 0.061%\n",
      "Epoch 13, Batch 413, LR 2.484732 Loss 19.064151, Accuracy 0.061%\n",
      "Epoch 13, Batch 414, LR 2.484711 Loss 19.064200, Accuracy 0.060%\n",
      "Epoch 13, Batch 415, LR 2.484690 Loss 19.064216, Accuracy 0.060%\n",
      "Epoch 13, Batch 416, LR 2.484669 Loss 19.064109, Accuracy 0.060%\n",
      "Epoch 13, Batch 417, LR 2.484648 Loss 19.064547, Accuracy 0.060%\n",
      "Epoch 13, Batch 418, LR 2.484627 Loss 19.064757, Accuracy 0.060%\n",
      "Epoch 13, Batch 419, LR 2.484606 Loss 19.064976, Accuracy 0.060%\n",
      "Epoch 13, Batch 420, LR 2.484585 Loss 19.065239, Accuracy 0.060%\n",
      "Epoch 13, Batch 421, LR 2.484564 Loss 19.065386, Accuracy 0.059%\n",
      "Epoch 13, Batch 422, LR 2.484543 Loss 19.065659, Accuracy 0.059%\n",
      "Epoch 13, Batch 423, LR 2.484522 Loss 19.065594, Accuracy 0.059%\n",
      "Epoch 13, Batch 424, LR 2.484501 Loss 19.065264, Accuracy 0.061%\n",
      "Epoch 13, Batch 425, LR 2.484480 Loss 19.065659, Accuracy 0.061%\n",
      "Epoch 13, Batch 426, LR 2.484459 Loss 19.065551, Accuracy 0.061%\n",
      "Epoch 13, Batch 427, LR 2.484438 Loss 19.066023, Accuracy 0.062%\n",
      "Epoch 13, Batch 428, LR 2.484417 Loss 19.066389, Accuracy 0.062%\n",
      "Epoch 13, Batch 429, LR 2.484396 Loss 19.066426, Accuracy 0.062%\n",
      "Epoch 13, Batch 430, LR 2.484375 Loss 19.065967, Accuracy 0.062%\n",
      "Epoch 13, Batch 431, LR 2.484354 Loss 19.066104, Accuracy 0.062%\n",
      "Epoch 13, Batch 432, LR 2.484333 Loss 19.065695, Accuracy 0.061%\n",
      "Epoch 13, Batch 433, LR 2.484312 Loss 19.066142, Accuracy 0.061%\n",
      "Epoch 13, Batch 434, LR 2.484290 Loss 19.065972, Accuracy 0.061%\n",
      "Epoch 13, Batch 435, LR 2.484269 Loss 19.065860, Accuracy 0.063%\n",
      "Epoch 13, Batch 436, LR 2.484248 Loss 19.065758, Accuracy 0.063%\n",
      "Epoch 13, Batch 437, LR 2.484227 Loss 19.065629, Accuracy 0.063%\n",
      "Epoch 13, Batch 438, LR 2.484206 Loss 19.065371, Accuracy 0.064%\n",
      "Epoch 13, Batch 439, LR 2.484184 Loss 19.065111, Accuracy 0.064%\n",
      "Epoch 13, Batch 440, LR 2.484163 Loss 19.064905, Accuracy 0.064%\n",
      "Epoch 13, Batch 441, LR 2.484142 Loss 19.064628, Accuracy 0.064%\n",
      "Epoch 13, Batch 442, LR 2.484121 Loss 19.064580, Accuracy 0.064%\n",
      "Epoch 13, Batch 443, LR 2.484099 Loss 19.064704, Accuracy 0.063%\n",
      "Epoch 13, Batch 444, LR 2.484078 Loss 19.065004, Accuracy 0.063%\n",
      "Epoch 13, Batch 445, LR 2.484057 Loss 19.064909, Accuracy 0.065%\n",
      "Epoch 13, Batch 446, LR 2.484035 Loss 19.064588, Accuracy 0.065%\n",
      "Epoch 13, Batch 447, LR 2.484014 Loss 19.064421, Accuracy 0.065%\n",
      "Epoch 13, Batch 448, LR 2.483993 Loss 19.064590, Accuracy 0.066%\n",
      "Epoch 13, Batch 449, LR 2.483971 Loss 19.064299, Accuracy 0.066%\n",
      "Epoch 13, Batch 450, LR 2.483950 Loss 19.063909, Accuracy 0.066%\n",
      "Epoch 13, Batch 451, LR 2.483928 Loss 19.064026, Accuracy 0.066%\n",
      "Epoch 13, Batch 452, LR 2.483907 Loss 19.064157, Accuracy 0.067%\n",
      "Epoch 13, Batch 453, LR 2.483886 Loss 19.064200, Accuracy 0.067%\n",
      "Epoch 13, Batch 454, LR 2.483864 Loss 19.063919, Accuracy 0.067%\n",
      "Epoch 13, Batch 455, LR 2.483843 Loss 19.063689, Accuracy 0.067%\n",
      "Epoch 13, Batch 456, LR 2.483821 Loss 19.063387, Accuracy 0.067%\n",
      "Epoch 13, Batch 457, LR 2.483800 Loss 19.062796, Accuracy 0.068%\n",
      "Epoch 13, Batch 458, LR 2.483778 Loss 19.062736, Accuracy 0.068%\n",
      "Epoch 13, Batch 459, LR 2.483757 Loss 19.063149, Accuracy 0.068%\n",
      "Epoch 13, Batch 460, LR 2.483735 Loss 19.063515, Accuracy 0.068%\n",
      "Epoch 13, Batch 461, LR 2.483714 Loss 19.062979, Accuracy 0.071%\n",
      "Epoch 13, Batch 462, LR 2.483692 Loss 19.063202, Accuracy 0.071%\n",
      "Epoch 13, Batch 463, LR 2.483670 Loss 19.063197, Accuracy 0.071%\n",
      "Epoch 13, Batch 464, LR 2.483649 Loss 19.063211, Accuracy 0.071%\n",
      "Epoch 13, Batch 465, LR 2.483627 Loss 19.063621, Accuracy 0.071%\n",
      "Epoch 13, Batch 466, LR 2.483606 Loss 19.063491, Accuracy 0.070%\n",
      "Epoch 13, Batch 467, LR 2.483584 Loss 19.063204, Accuracy 0.070%\n",
      "Epoch 13, Batch 468, LR 2.483562 Loss 19.063203, Accuracy 0.070%\n",
      "Epoch 13, Batch 469, LR 2.483541 Loss 19.063181, Accuracy 0.070%\n",
      "Epoch 13, Batch 470, LR 2.483519 Loss 19.062958, Accuracy 0.070%\n",
      "Epoch 13, Batch 471, LR 2.483497 Loss 19.062567, Accuracy 0.071%\n",
      "Epoch 13, Batch 472, LR 2.483476 Loss 19.062310, Accuracy 0.071%\n",
      "Epoch 13, Batch 473, LR 2.483454 Loss 19.062084, Accuracy 0.071%\n",
      "Epoch 13, Batch 474, LR 2.483432 Loss 19.061909, Accuracy 0.071%\n",
      "Epoch 13, Batch 475, LR 2.483410 Loss 19.062271, Accuracy 0.071%\n",
      "Epoch 13, Batch 476, LR 2.483389 Loss 19.062322, Accuracy 0.071%\n",
      "Epoch 13, Batch 477, LR 2.483367 Loss 19.062796, Accuracy 0.070%\n",
      "Epoch 13, Batch 478, LR 2.483345 Loss 19.062881, Accuracy 0.070%\n",
      "Epoch 13, Batch 479, LR 2.483323 Loss 19.062671, Accuracy 0.070%\n",
      "Epoch 13, Batch 480, LR 2.483302 Loss 19.062827, Accuracy 0.070%\n",
      "Epoch 13, Batch 481, LR 2.483280 Loss 19.063006, Accuracy 0.071%\n",
      "Epoch 13, Batch 482, LR 2.483258 Loss 19.063160, Accuracy 0.071%\n",
      "Epoch 13, Batch 483, LR 2.483236 Loss 19.062836, Accuracy 0.071%\n",
      "Epoch 13, Batch 484, LR 2.483214 Loss 19.063225, Accuracy 0.071%\n",
      "Epoch 13, Batch 485, LR 2.483192 Loss 19.063139, Accuracy 0.071%\n",
      "Epoch 13, Batch 486, LR 2.483170 Loss 19.063200, Accuracy 0.071%\n",
      "Epoch 13, Batch 487, LR 2.483148 Loss 19.063433, Accuracy 0.072%\n",
      "Epoch 13, Batch 488, LR 2.483126 Loss 19.063666, Accuracy 0.072%\n",
      "Epoch 13, Batch 489, LR 2.483105 Loss 19.064032, Accuracy 0.072%\n",
      "Epoch 13, Batch 490, LR 2.483083 Loss 19.063883, Accuracy 0.072%\n",
      "Epoch 13, Batch 491, LR 2.483061 Loss 19.064105, Accuracy 0.073%\n",
      "Epoch 13, Batch 492, LR 2.483039 Loss 19.064070, Accuracy 0.075%\n",
      "Epoch 13, Batch 493, LR 2.483017 Loss 19.063925, Accuracy 0.074%\n",
      "Epoch 13, Batch 494, LR 2.482995 Loss 19.063869, Accuracy 0.074%\n",
      "Epoch 13, Batch 495, LR 2.482973 Loss 19.064034, Accuracy 0.074%\n",
      "Epoch 13, Batch 496, LR 2.482951 Loss 19.063928, Accuracy 0.074%\n",
      "Epoch 13, Batch 497, LR 2.482928 Loss 19.063917, Accuracy 0.074%\n",
      "Epoch 13, Batch 498, LR 2.482906 Loss 19.064003, Accuracy 0.075%\n",
      "Epoch 13, Batch 499, LR 2.482884 Loss 19.063859, Accuracy 0.075%\n",
      "Epoch 13, Batch 500, LR 2.482862 Loss 19.063717, Accuracy 0.075%\n",
      "Epoch 13, Batch 501, LR 2.482840 Loss 19.063609, Accuracy 0.075%\n",
      "Epoch 13, Batch 502, LR 2.482818 Loss 19.063930, Accuracy 0.076%\n",
      "Epoch 13, Batch 503, LR 2.482796 Loss 19.063929, Accuracy 0.076%\n",
      "Epoch 13, Batch 504, LR 2.482774 Loss 19.064155, Accuracy 0.076%\n",
      "Epoch 13, Batch 505, LR 2.482752 Loss 19.064113, Accuracy 0.076%\n",
      "Epoch 13, Batch 506, LR 2.482729 Loss 19.063967, Accuracy 0.077%\n",
      "Epoch 13, Batch 507, LR 2.482707 Loss 19.064044, Accuracy 0.077%\n",
      "Epoch 13, Batch 508, LR 2.482685 Loss 19.064057, Accuracy 0.077%\n",
      "Epoch 13, Batch 509, LR 2.482663 Loss 19.064428, Accuracy 0.077%\n",
      "Epoch 13, Batch 510, LR 2.482640 Loss 19.064510, Accuracy 0.077%\n",
      "Epoch 13, Batch 511, LR 2.482618 Loss 19.064632, Accuracy 0.076%\n",
      "Epoch 13, Batch 512, LR 2.482596 Loss 19.064606, Accuracy 0.076%\n",
      "Epoch 13, Batch 513, LR 2.482574 Loss 19.064291, Accuracy 0.076%\n",
      "Epoch 13, Batch 514, LR 2.482551 Loss 19.064237, Accuracy 0.076%\n",
      "Epoch 13, Batch 515, LR 2.482529 Loss 19.064518, Accuracy 0.076%\n",
      "Epoch 13, Batch 516, LR 2.482507 Loss 19.064245, Accuracy 0.076%\n",
      "Epoch 13, Batch 517, LR 2.482484 Loss 19.064344, Accuracy 0.076%\n",
      "Epoch 13, Batch 518, LR 2.482462 Loss 19.064488, Accuracy 0.075%\n",
      "Epoch 13, Batch 519, LR 2.482440 Loss 19.064433, Accuracy 0.075%\n",
      "Epoch 13, Batch 520, LR 2.482417 Loss 19.064172, Accuracy 0.075%\n",
      "Epoch 13, Batch 521, LR 2.482395 Loss 19.064133, Accuracy 0.075%\n",
      "Epoch 13, Batch 522, LR 2.482373 Loss 19.064086, Accuracy 0.075%\n",
      "Epoch 13, Batch 523, LR 2.482350 Loss 19.063975, Accuracy 0.076%\n",
      "Epoch 13, Batch 524, LR 2.482328 Loss 19.063977, Accuracy 0.076%\n",
      "Epoch 13, Batch 525, LR 2.482305 Loss 19.063906, Accuracy 0.076%\n",
      "Epoch 13, Batch 526, LR 2.482283 Loss 19.063933, Accuracy 0.076%\n",
      "Epoch 13, Batch 527, LR 2.482260 Loss 19.064091, Accuracy 0.076%\n",
      "Epoch 13, Batch 528, LR 2.482238 Loss 19.064367, Accuracy 0.075%\n",
      "Epoch 13, Batch 529, LR 2.482215 Loss 19.064397, Accuracy 0.075%\n",
      "Epoch 13, Batch 530, LR 2.482193 Loss 19.064012, Accuracy 0.075%\n",
      "Epoch 13, Batch 531, LR 2.482170 Loss 19.063957, Accuracy 0.075%\n",
      "Epoch 13, Batch 532, LR 2.482148 Loss 19.063842, Accuracy 0.075%\n",
      "Epoch 13, Batch 533, LR 2.482125 Loss 19.063781, Accuracy 0.075%\n",
      "Epoch 13, Batch 534, LR 2.482102 Loss 19.063758, Accuracy 0.075%\n",
      "Epoch 13, Batch 535, LR 2.482080 Loss 19.063695, Accuracy 0.074%\n",
      "Epoch 13, Batch 536, LR 2.482057 Loss 19.063649, Accuracy 0.074%\n",
      "Epoch 13, Batch 537, LR 2.482035 Loss 19.063186, Accuracy 0.074%\n",
      "Epoch 13, Batch 538, LR 2.482012 Loss 19.063507, Accuracy 0.074%\n",
      "Epoch 13, Batch 539, LR 2.481989 Loss 19.063624, Accuracy 0.074%\n",
      "Epoch 13, Batch 540, LR 2.481967 Loss 19.063580, Accuracy 0.074%\n",
      "Epoch 13, Batch 541, LR 2.481944 Loss 19.063998, Accuracy 0.075%\n",
      "Epoch 13, Batch 542, LR 2.481921 Loss 19.063829, Accuracy 0.075%\n",
      "Epoch 13, Batch 543, LR 2.481899 Loss 19.063323, Accuracy 0.075%\n",
      "Epoch 13, Batch 544, LR 2.481876 Loss 19.063391, Accuracy 0.075%\n",
      "Epoch 13, Batch 545, LR 2.481853 Loss 19.063805, Accuracy 0.075%\n",
      "Epoch 13, Batch 546, LR 2.481830 Loss 19.063581, Accuracy 0.074%\n",
      "Epoch 13, Batch 547, LR 2.481808 Loss 19.063754, Accuracy 0.074%\n",
      "Epoch 13, Batch 548, LR 2.481785 Loss 19.063544, Accuracy 0.074%\n",
      "Epoch 13, Batch 549, LR 2.481762 Loss 19.063816, Accuracy 0.074%\n",
      "Epoch 13, Batch 550, LR 2.481739 Loss 19.063597, Accuracy 0.074%\n",
      "Epoch 13, Batch 551, LR 2.481716 Loss 19.063595, Accuracy 0.074%\n",
      "Epoch 13, Batch 552, LR 2.481694 Loss 19.063780, Accuracy 0.074%\n",
      "Epoch 13, Batch 553, LR 2.481671 Loss 19.064069, Accuracy 0.073%\n",
      "Epoch 13, Batch 554, LR 2.481648 Loss 19.063819, Accuracy 0.073%\n",
      "Epoch 13, Batch 555, LR 2.481625 Loss 19.064009, Accuracy 0.073%\n",
      "Epoch 13, Batch 556, LR 2.481602 Loss 19.064050, Accuracy 0.073%\n",
      "Epoch 13, Batch 557, LR 2.481579 Loss 19.063807, Accuracy 0.073%\n",
      "Epoch 13, Batch 558, LR 2.481556 Loss 19.064158, Accuracy 0.073%\n",
      "Epoch 13, Batch 559, LR 2.481533 Loss 19.064205, Accuracy 0.073%\n",
      "Epoch 13, Batch 560, LR 2.481510 Loss 19.064131, Accuracy 0.073%\n",
      "Epoch 13, Batch 561, LR 2.481487 Loss 19.064146, Accuracy 0.072%\n",
      "Epoch 13, Batch 562, LR 2.481465 Loss 19.064023, Accuracy 0.072%\n",
      "Epoch 13, Batch 563, LR 2.481442 Loss 19.063965, Accuracy 0.072%\n",
      "Epoch 13, Batch 564, LR 2.481419 Loss 19.064066, Accuracy 0.072%\n",
      "Epoch 13, Batch 565, LR 2.481396 Loss 19.063896, Accuracy 0.072%\n",
      "Epoch 13, Batch 566, LR 2.481372 Loss 19.063945, Accuracy 0.072%\n",
      "Epoch 13, Batch 567, LR 2.481349 Loss 19.063759, Accuracy 0.072%\n",
      "Epoch 13, Batch 568, LR 2.481326 Loss 19.063481, Accuracy 0.072%\n",
      "Epoch 13, Batch 569, LR 2.481303 Loss 19.063641, Accuracy 0.071%\n",
      "Epoch 13, Batch 570, LR 2.481280 Loss 19.063737, Accuracy 0.071%\n",
      "Epoch 13, Batch 571, LR 2.481257 Loss 19.063660, Accuracy 0.071%\n",
      "Epoch 13, Batch 572, LR 2.481234 Loss 19.063699, Accuracy 0.072%\n",
      "Epoch 13, Batch 573, LR 2.481211 Loss 19.063594, Accuracy 0.072%\n",
      "Epoch 13, Batch 574, LR 2.481188 Loss 19.063524, Accuracy 0.073%\n",
      "Epoch 13, Batch 575, LR 2.481165 Loss 19.063539, Accuracy 0.073%\n",
      "Epoch 13, Batch 576, LR 2.481141 Loss 19.063937, Accuracy 0.075%\n",
      "Epoch 13, Batch 577, LR 2.481118 Loss 19.063536, Accuracy 0.074%\n",
      "Epoch 13, Batch 578, LR 2.481095 Loss 19.063779, Accuracy 0.074%\n",
      "Epoch 13, Batch 579, LR 2.481072 Loss 19.063553, Accuracy 0.074%\n",
      "Epoch 13, Batch 580, LR 2.481049 Loss 19.064054, Accuracy 0.074%\n",
      "Epoch 13, Batch 581, LR 2.481025 Loss 19.063978, Accuracy 0.074%\n",
      "Epoch 13, Batch 582, LR 2.481002 Loss 19.064053, Accuracy 0.074%\n",
      "Epoch 13, Batch 583, LR 2.480979 Loss 19.063842, Accuracy 0.075%\n",
      "Epoch 13, Batch 584, LR 2.480955 Loss 19.064407, Accuracy 0.075%\n",
      "Epoch 13, Batch 585, LR 2.480932 Loss 19.064156, Accuracy 0.075%\n",
      "Epoch 13, Batch 586, LR 2.480909 Loss 19.064206, Accuracy 0.075%\n",
      "Epoch 13, Batch 587, LR 2.480886 Loss 19.064025, Accuracy 0.075%\n",
      "Epoch 13, Batch 588, LR 2.480862 Loss 19.064006, Accuracy 0.074%\n",
      "Epoch 13, Batch 589, LR 2.480839 Loss 19.064457, Accuracy 0.074%\n",
      "Epoch 13, Batch 590, LR 2.480815 Loss 19.064701, Accuracy 0.074%\n",
      "Epoch 13, Batch 591, LR 2.480792 Loss 19.064512, Accuracy 0.074%\n",
      "Epoch 13, Batch 592, LR 2.480769 Loss 19.064436, Accuracy 0.075%\n",
      "Epoch 13, Batch 593, LR 2.480745 Loss 19.064458, Accuracy 0.075%\n",
      "Epoch 13, Batch 594, LR 2.480722 Loss 19.064476, Accuracy 0.076%\n",
      "Epoch 13, Batch 595, LR 2.480698 Loss 19.064608, Accuracy 0.076%\n",
      "Epoch 13, Batch 596, LR 2.480675 Loss 19.064792, Accuracy 0.076%\n",
      "Epoch 13, Batch 597, LR 2.480651 Loss 19.064514, Accuracy 0.076%\n",
      "Epoch 13, Batch 598, LR 2.480628 Loss 19.064324, Accuracy 0.077%\n",
      "Epoch 13, Batch 599, LR 2.480604 Loss 19.063983, Accuracy 0.078%\n",
      "Epoch 13, Batch 600, LR 2.480581 Loss 19.064125, Accuracy 0.078%\n",
      "Epoch 13, Batch 601, LR 2.480557 Loss 19.063886, Accuracy 0.078%\n",
      "Epoch 13, Batch 602, LR 2.480534 Loss 19.064053, Accuracy 0.078%\n",
      "Epoch 13, Batch 603, LR 2.480510 Loss 19.063883, Accuracy 0.078%\n",
      "Epoch 13, Batch 604, LR 2.480487 Loss 19.063634, Accuracy 0.078%\n",
      "Epoch 13, Batch 605, LR 2.480463 Loss 19.063686, Accuracy 0.077%\n",
      "Epoch 13, Batch 606, LR 2.480440 Loss 19.063662, Accuracy 0.077%\n",
      "Epoch 13, Batch 607, LR 2.480416 Loss 19.064181, Accuracy 0.077%\n",
      "Epoch 13, Batch 608, LR 2.480392 Loss 19.064069, Accuracy 0.077%\n",
      "Epoch 13, Batch 609, LR 2.480369 Loss 19.064571, Accuracy 0.077%\n",
      "Epoch 13, Batch 610, LR 2.480345 Loss 19.064487, Accuracy 0.078%\n",
      "Epoch 13, Batch 611, LR 2.480321 Loss 19.064453, Accuracy 0.078%\n",
      "Epoch 13, Batch 612, LR 2.480298 Loss 19.064405, Accuracy 0.078%\n",
      "Epoch 13, Batch 613, LR 2.480274 Loss 19.064625, Accuracy 0.078%\n",
      "Epoch 13, Batch 614, LR 2.480250 Loss 19.064737, Accuracy 0.078%\n",
      "Epoch 13, Batch 615, LR 2.480227 Loss 19.064704, Accuracy 0.077%\n",
      "Epoch 13, Batch 616, LR 2.480203 Loss 19.064753, Accuracy 0.077%\n",
      "Epoch 13, Batch 617, LR 2.480179 Loss 19.064534, Accuracy 0.077%\n",
      "Epoch 13, Batch 618, LR 2.480155 Loss 19.064394, Accuracy 0.077%\n",
      "Epoch 13, Batch 619, LR 2.480132 Loss 19.064170, Accuracy 0.077%\n",
      "Epoch 13, Batch 620, LR 2.480108 Loss 19.064225, Accuracy 0.077%\n",
      "Epoch 13, Batch 621, LR 2.480084 Loss 19.064044, Accuracy 0.078%\n",
      "Epoch 13, Batch 622, LR 2.480060 Loss 19.064081, Accuracy 0.079%\n",
      "Epoch 13, Batch 623, LR 2.480036 Loss 19.064318, Accuracy 0.079%\n",
      "Epoch 13, Batch 624, LR 2.480012 Loss 19.064460, Accuracy 0.079%\n",
      "Epoch 13, Batch 625, LR 2.479989 Loss 19.064302, Accuracy 0.079%\n",
      "Epoch 13, Batch 626, LR 2.479965 Loss 19.064815, Accuracy 0.080%\n",
      "Epoch 13, Batch 627, LR 2.479941 Loss 19.064986, Accuracy 0.080%\n",
      "Epoch 13, Batch 628, LR 2.479917 Loss 19.064921, Accuracy 0.080%\n",
      "Epoch 13, Batch 629, LR 2.479893 Loss 19.064920, Accuracy 0.079%\n",
      "Epoch 13, Batch 630, LR 2.479869 Loss 19.064925, Accuracy 0.079%\n",
      "Epoch 13, Batch 631, LR 2.479845 Loss 19.064875, Accuracy 0.079%\n",
      "Epoch 13, Batch 632, LR 2.479821 Loss 19.065111, Accuracy 0.079%\n",
      "Epoch 13, Batch 633, LR 2.479797 Loss 19.064957, Accuracy 0.081%\n",
      "Epoch 13, Batch 634, LR 2.479773 Loss 19.064946, Accuracy 0.081%\n",
      "Epoch 13, Batch 635, LR 2.479749 Loss 19.064920, Accuracy 0.081%\n",
      "Epoch 13, Batch 636, LR 2.479725 Loss 19.064854, Accuracy 0.081%\n",
      "Epoch 13, Batch 637, LR 2.479701 Loss 19.065032, Accuracy 0.081%\n",
      "Epoch 13, Batch 638, LR 2.479677 Loss 19.064269, Accuracy 0.081%\n",
      "Epoch 13, Batch 639, LR 2.479653 Loss 19.064356, Accuracy 0.081%\n",
      "Epoch 13, Batch 640, LR 2.479629 Loss 19.064113, Accuracy 0.081%\n",
      "Epoch 13, Batch 641, LR 2.479605 Loss 19.064412, Accuracy 0.080%\n",
      "Epoch 13, Batch 642, LR 2.479581 Loss 19.064285, Accuracy 0.080%\n",
      "Epoch 13, Batch 643, LR 2.479557 Loss 19.064201, Accuracy 0.080%\n",
      "Epoch 13, Batch 644, LR 2.479532 Loss 19.064418, Accuracy 0.080%\n",
      "Epoch 13, Batch 645, LR 2.479508 Loss 19.064542, Accuracy 0.080%\n",
      "Epoch 13, Batch 646, LR 2.479484 Loss 19.064279, Accuracy 0.081%\n",
      "Epoch 13, Batch 647, LR 2.479460 Loss 19.063912, Accuracy 0.081%\n",
      "Epoch 13, Batch 648, LR 2.479436 Loss 19.063957, Accuracy 0.082%\n",
      "Epoch 13, Batch 649, LR 2.479412 Loss 19.063820, Accuracy 0.082%\n",
      "Epoch 13, Batch 650, LR 2.479387 Loss 19.063978, Accuracy 0.083%\n",
      "Epoch 13, Batch 651, LR 2.479363 Loss 19.063903, Accuracy 0.083%\n",
      "Epoch 13, Batch 652, LR 2.479339 Loss 19.063821, Accuracy 0.083%\n",
      "Epoch 13, Batch 653, LR 2.479315 Loss 19.063719, Accuracy 0.083%\n",
      "Epoch 13, Batch 654, LR 2.479290 Loss 19.063609, Accuracy 0.082%\n",
      "Epoch 13, Batch 655, LR 2.479266 Loss 19.063629, Accuracy 0.082%\n",
      "Epoch 13, Batch 656, LR 2.479242 Loss 19.063963, Accuracy 0.082%\n",
      "Epoch 13, Batch 657, LR 2.479217 Loss 19.064042, Accuracy 0.082%\n",
      "Epoch 13, Batch 658, LR 2.479193 Loss 19.063719, Accuracy 0.082%\n",
      "Epoch 13, Batch 659, LR 2.479169 Loss 19.063566, Accuracy 0.083%\n",
      "Epoch 13, Batch 660, LR 2.479144 Loss 19.063552, Accuracy 0.083%\n",
      "Epoch 13, Batch 661, LR 2.479120 Loss 19.063558, Accuracy 0.083%\n",
      "Epoch 13, Batch 662, LR 2.479096 Loss 19.063389, Accuracy 0.084%\n",
      "Epoch 13, Batch 663, LR 2.479071 Loss 19.063366, Accuracy 0.084%\n",
      "Epoch 13, Batch 664, LR 2.479047 Loss 19.062842, Accuracy 0.084%\n",
      "Epoch 13, Batch 665, LR 2.479022 Loss 19.063018, Accuracy 0.085%\n",
      "Epoch 13, Batch 666, LR 2.478998 Loss 19.062827, Accuracy 0.084%\n",
      "Epoch 13, Batch 667, LR 2.478973 Loss 19.063008, Accuracy 0.084%\n",
      "Epoch 13, Batch 668, LR 2.478949 Loss 19.062970, Accuracy 0.084%\n",
      "Epoch 13, Batch 669, LR 2.478924 Loss 19.063233, Accuracy 0.084%\n",
      "Epoch 13, Batch 670, LR 2.478900 Loss 19.063291, Accuracy 0.084%\n",
      "Epoch 13, Batch 671, LR 2.478875 Loss 19.063165, Accuracy 0.085%\n",
      "Epoch 13, Batch 672, LR 2.478851 Loss 19.063162, Accuracy 0.085%\n",
      "Epoch 13, Batch 673, LR 2.478826 Loss 19.063303, Accuracy 0.085%\n",
      "Epoch 13, Batch 674, LR 2.478802 Loss 19.063260, Accuracy 0.085%\n",
      "Epoch 13, Batch 675, LR 2.478777 Loss 19.063387, Accuracy 0.084%\n",
      "Epoch 13, Batch 676, LR 2.478753 Loss 19.063672, Accuracy 0.084%\n",
      "Epoch 13, Batch 677, LR 2.478728 Loss 19.063724, Accuracy 0.084%\n",
      "Epoch 13, Batch 678, LR 2.478703 Loss 19.063565, Accuracy 0.084%\n",
      "Epoch 13, Batch 679, LR 2.478679 Loss 19.063383, Accuracy 0.084%\n",
      "Epoch 13, Batch 680, LR 2.478654 Loss 19.063736, Accuracy 0.084%\n",
      "Epoch 13, Batch 681, LR 2.478630 Loss 19.063511, Accuracy 0.084%\n",
      "Epoch 13, Batch 682, LR 2.478605 Loss 19.063739, Accuracy 0.084%\n",
      "Epoch 13, Batch 683, LR 2.478580 Loss 19.063915, Accuracy 0.084%\n",
      "Epoch 13, Batch 684, LR 2.478555 Loss 19.063961, Accuracy 0.083%\n",
      "Epoch 13, Batch 685, LR 2.478531 Loss 19.064035, Accuracy 0.083%\n",
      "Epoch 13, Batch 686, LR 2.478506 Loss 19.064271, Accuracy 0.083%\n",
      "Epoch 13, Batch 687, LR 2.478481 Loss 19.064251, Accuracy 0.083%\n",
      "Epoch 13, Batch 688, LR 2.478457 Loss 19.064166, Accuracy 0.083%\n",
      "Epoch 13, Batch 689, LR 2.478432 Loss 19.064197, Accuracy 0.083%\n",
      "Epoch 13, Batch 690, LR 2.478407 Loss 19.064175, Accuracy 0.083%\n",
      "Epoch 13, Batch 691, LR 2.478382 Loss 19.063837, Accuracy 0.083%\n",
      "Epoch 13, Batch 692, LR 2.478357 Loss 19.063766, Accuracy 0.082%\n",
      "Epoch 13, Batch 693, LR 2.478333 Loss 19.063966, Accuracy 0.082%\n",
      "Epoch 13, Batch 694, LR 2.478308 Loss 19.064052, Accuracy 0.082%\n",
      "Epoch 13, Batch 695, LR 2.478283 Loss 19.063947, Accuracy 0.082%\n",
      "Epoch 13, Batch 696, LR 2.478258 Loss 19.063963, Accuracy 0.082%\n",
      "Epoch 13, Batch 697, LR 2.478233 Loss 19.064048, Accuracy 0.082%\n",
      "Epoch 13, Batch 698, LR 2.478208 Loss 19.064340, Accuracy 0.082%\n",
      "Epoch 13, Batch 699, LR 2.478183 Loss 19.064411, Accuracy 0.082%\n",
      "Epoch 13, Batch 700, LR 2.478158 Loss 19.064285, Accuracy 0.083%\n",
      "Epoch 13, Batch 701, LR 2.478133 Loss 19.064123, Accuracy 0.082%\n",
      "Epoch 13, Batch 702, LR 2.478108 Loss 19.064161, Accuracy 0.082%\n",
      "Epoch 13, Batch 703, LR 2.478084 Loss 19.064219, Accuracy 0.082%\n",
      "Epoch 13, Batch 704, LR 2.478059 Loss 19.064564, Accuracy 0.082%\n",
      "Epoch 13, Batch 705, LR 2.478034 Loss 19.064468, Accuracy 0.082%\n",
      "Epoch 13, Batch 706, LR 2.478009 Loss 19.064342, Accuracy 0.082%\n",
      "Epoch 13, Batch 707, LR 2.477983 Loss 19.064430, Accuracy 0.082%\n",
      "Epoch 13, Batch 708, LR 2.477958 Loss 19.064510, Accuracy 0.082%\n",
      "Epoch 13, Batch 709, LR 2.477933 Loss 19.064470, Accuracy 0.082%\n",
      "Epoch 13, Batch 710, LR 2.477908 Loss 19.064553, Accuracy 0.081%\n",
      "Epoch 13, Batch 711, LR 2.477883 Loss 19.064512, Accuracy 0.082%\n",
      "Epoch 13, Batch 712, LR 2.477858 Loss 19.064529, Accuracy 0.082%\n",
      "Epoch 13, Batch 713, LR 2.477833 Loss 19.064280, Accuracy 0.082%\n",
      "Epoch 13, Batch 714, LR 2.477808 Loss 19.064232, Accuracy 0.082%\n",
      "Epoch 13, Batch 715, LR 2.477783 Loss 19.064277, Accuracy 0.082%\n",
      "Epoch 13, Batch 716, LR 2.477758 Loss 19.064412, Accuracy 0.082%\n",
      "Epoch 13, Batch 717, LR 2.477732 Loss 19.064378, Accuracy 0.082%\n",
      "Epoch 13, Batch 718, LR 2.477707 Loss 19.064271, Accuracy 0.083%\n",
      "Epoch 13, Batch 719, LR 2.477682 Loss 19.064365, Accuracy 0.083%\n",
      "Epoch 13, Batch 720, LR 2.477657 Loss 19.064278, Accuracy 0.082%\n",
      "Epoch 13, Batch 721, LR 2.477632 Loss 19.064430, Accuracy 0.082%\n",
      "Epoch 13, Batch 722, LR 2.477606 Loss 19.064575, Accuracy 0.082%\n",
      "Epoch 13, Batch 723, LR 2.477581 Loss 19.064789, Accuracy 0.082%\n",
      "Epoch 13, Batch 724, LR 2.477556 Loss 19.064927, Accuracy 0.082%\n",
      "Epoch 13, Batch 725, LR 2.477531 Loss 19.064992, Accuracy 0.082%\n",
      "Epoch 13, Batch 726, LR 2.477505 Loss 19.064950, Accuracy 0.082%\n",
      "Epoch 13, Batch 727, LR 2.477480 Loss 19.065110, Accuracy 0.082%\n",
      "Epoch 13, Batch 728, LR 2.477455 Loss 19.065125, Accuracy 0.083%\n",
      "Epoch 13, Batch 729, LR 2.477429 Loss 19.065281, Accuracy 0.083%\n",
      "Epoch 13, Batch 730, LR 2.477404 Loss 19.065033, Accuracy 0.083%\n",
      "Epoch 13, Batch 731, LR 2.477379 Loss 19.065191, Accuracy 0.083%\n",
      "Epoch 13, Batch 732, LR 2.477353 Loss 19.065269, Accuracy 0.083%\n",
      "Epoch 13, Batch 733, LR 2.477328 Loss 19.065415, Accuracy 0.084%\n",
      "Epoch 13, Batch 734, LR 2.477303 Loss 19.065435, Accuracy 0.084%\n",
      "Epoch 13, Batch 735, LR 2.477277 Loss 19.065084, Accuracy 0.084%\n",
      "Epoch 13, Batch 736, LR 2.477252 Loss 19.065370, Accuracy 0.084%\n",
      "Epoch 13, Batch 737, LR 2.477226 Loss 19.065361, Accuracy 0.084%\n",
      "Epoch 13, Batch 738, LR 2.477201 Loss 19.065113, Accuracy 0.084%\n",
      "Epoch 13, Batch 739, LR 2.477175 Loss 19.065272, Accuracy 0.084%\n",
      "Epoch 13, Batch 740, LR 2.477150 Loss 19.064966, Accuracy 0.083%\n",
      "Epoch 13, Batch 741, LR 2.477124 Loss 19.064931, Accuracy 0.084%\n",
      "Epoch 13, Batch 742, LR 2.477099 Loss 19.064912, Accuracy 0.084%\n",
      "Epoch 13, Batch 743, LR 2.477073 Loss 19.064704, Accuracy 0.084%\n",
      "Epoch 13, Batch 744, LR 2.477048 Loss 19.064709, Accuracy 0.084%\n",
      "Epoch 13, Batch 745, LR 2.477022 Loss 19.064778, Accuracy 0.084%\n",
      "Epoch 13, Batch 746, LR 2.476997 Loss 19.064925, Accuracy 0.084%\n",
      "Epoch 13, Batch 747, LR 2.476971 Loss 19.065048, Accuracy 0.084%\n",
      "Epoch 13, Batch 748, LR 2.476945 Loss 19.064952, Accuracy 0.084%\n",
      "Epoch 13, Batch 749, LR 2.476920 Loss 19.065048, Accuracy 0.084%\n",
      "Epoch 13, Batch 750, LR 2.476894 Loss 19.064952, Accuracy 0.084%\n",
      "Epoch 13, Batch 751, LR 2.476869 Loss 19.065400, Accuracy 0.084%\n",
      "Epoch 13, Batch 752, LR 2.476843 Loss 19.065400, Accuracy 0.085%\n",
      "Epoch 13, Batch 753, LR 2.476817 Loss 19.065335, Accuracy 0.085%\n",
      "Epoch 13, Batch 754, LR 2.476791 Loss 19.065365, Accuracy 0.085%\n",
      "Epoch 13, Batch 755, LR 2.476766 Loss 19.065440, Accuracy 0.086%\n",
      "Epoch 13, Batch 756, LR 2.476740 Loss 19.065460, Accuracy 0.086%\n",
      "Epoch 13, Batch 757, LR 2.476714 Loss 19.065309, Accuracy 0.086%\n",
      "Epoch 13, Batch 758, LR 2.476689 Loss 19.065436, Accuracy 0.086%\n",
      "Epoch 13, Batch 759, LR 2.476663 Loss 19.065429, Accuracy 0.085%\n",
      "Epoch 13, Batch 760, LR 2.476637 Loss 19.065646, Accuracy 0.085%\n",
      "Epoch 13, Batch 761, LR 2.476611 Loss 19.065513, Accuracy 0.085%\n",
      "Epoch 13, Batch 762, LR 2.476586 Loss 19.065264, Accuracy 0.085%\n",
      "Epoch 13, Batch 763, LR 2.476560 Loss 19.065185, Accuracy 0.085%\n",
      "Epoch 13, Batch 764, LR 2.476534 Loss 19.065173, Accuracy 0.085%\n",
      "Epoch 13, Batch 765, LR 2.476508 Loss 19.065175, Accuracy 0.085%\n",
      "Epoch 13, Batch 766, LR 2.476482 Loss 19.064990, Accuracy 0.085%\n",
      "Epoch 13, Batch 767, LR 2.476456 Loss 19.064967, Accuracy 0.085%\n",
      "Epoch 13, Batch 768, LR 2.476430 Loss 19.065110, Accuracy 0.084%\n",
      "Epoch 13, Batch 769, LR 2.476405 Loss 19.064972, Accuracy 0.084%\n",
      "Epoch 13, Batch 770, LR 2.476379 Loss 19.065063, Accuracy 0.084%\n",
      "Epoch 13, Batch 771, LR 2.476353 Loss 19.064942, Accuracy 0.084%\n",
      "Epoch 13, Batch 772, LR 2.476327 Loss 19.064805, Accuracy 0.084%\n",
      "Epoch 13, Batch 773, LR 2.476301 Loss 19.064847, Accuracy 0.084%\n",
      "Epoch 13, Batch 774, LR 2.476275 Loss 19.064751, Accuracy 0.084%\n",
      "Epoch 13, Batch 775, LR 2.476249 Loss 19.064823, Accuracy 0.084%\n",
      "Epoch 13, Batch 776, LR 2.476223 Loss 19.065048, Accuracy 0.084%\n",
      "Epoch 13, Batch 777, LR 2.476197 Loss 19.065263, Accuracy 0.083%\n",
      "Epoch 13, Batch 778, LR 2.476171 Loss 19.065148, Accuracy 0.083%\n",
      "Epoch 13, Batch 779, LR 2.476145 Loss 19.065056, Accuracy 0.083%\n",
      "Epoch 13, Batch 780, LR 2.476119 Loss 19.065126, Accuracy 0.083%\n",
      "Epoch 13, Batch 781, LR 2.476093 Loss 19.064722, Accuracy 0.083%\n",
      "Epoch 13, Batch 782, LR 2.476067 Loss 19.064744, Accuracy 0.083%\n",
      "Epoch 13, Batch 783, LR 2.476040 Loss 19.064595, Accuracy 0.084%\n",
      "Epoch 13, Batch 784, LR 2.476014 Loss 19.064897, Accuracy 0.084%\n",
      "Epoch 13, Batch 785, LR 2.475988 Loss 19.064726, Accuracy 0.084%\n",
      "Epoch 13, Batch 786, LR 2.475962 Loss 19.064916, Accuracy 0.083%\n",
      "Epoch 13, Batch 787, LR 2.475936 Loss 19.065071, Accuracy 0.083%\n",
      "Epoch 13, Batch 788, LR 2.475910 Loss 19.065028, Accuracy 0.083%\n",
      "Epoch 13, Batch 789, LR 2.475884 Loss 19.065149, Accuracy 0.083%\n",
      "Epoch 13, Batch 790, LR 2.475857 Loss 19.064879, Accuracy 0.083%\n",
      "Epoch 13, Batch 791, LR 2.475831 Loss 19.064707, Accuracy 0.083%\n",
      "Epoch 13, Batch 792, LR 2.475805 Loss 19.064777, Accuracy 0.083%\n",
      "Epoch 13, Batch 793, LR 2.475779 Loss 19.064429, Accuracy 0.083%\n",
      "Epoch 13, Batch 794, LR 2.475753 Loss 19.064374, Accuracy 0.083%\n",
      "Epoch 13, Batch 795, LR 2.475726 Loss 19.064618, Accuracy 0.084%\n",
      "Epoch 13, Batch 796, LR 2.475700 Loss 19.064758, Accuracy 0.083%\n",
      "Epoch 13, Batch 797, LR 2.475674 Loss 19.064733, Accuracy 0.083%\n",
      "Epoch 13, Batch 798, LR 2.475647 Loss 19.064631, Accuracy 0.083%\n",
      "Epoch 13, Batch 799, LR 2.475621 Loss 19.064387, Accuracy 0.083%\n",
      "Epoch 13, Batch 800, LR 2.475595 Loss 19.064356, Accuracy 0.084%\n",
      "Epoch 13, Batch 801, LR 2.475568 Loss 19.064419, Accuracy 0.084%\n",
      "Epoch 13, Batch 802, LR 2.475542 Loss 19.064360, Accuracy 0.085%\n",
      "Epoch 13, Batch 803, LR 2.475516 Loss 19.064233, Accuracy 0.085%\n",
      "Epoch 13, Batch 804, LR 2.475489 Loss 19.064107, Accuracy 0.086%\n",
      "Epoch 13, Batch 805, LR 2.475463 Loss 19.064264, Accuracy 0.085%\n",
      "Epoch 13, Batch 806, LR 2.475436 Loss 19.064251, Accuracy 0.085%\n",
      "Epoch 13, Batch 807, LR 2.475410 Loss 19.064129, Accuracy 0.085%\n",
      "Epoch 13, Batch 808, LR 2.475384 Loss 19.064175, Accuracy 0.085%\n",
      "Epoch 13, Batch 809, LR 2.475357 Loss 19.064446, Accuracy 0.085%\n",
      "Epoch 13, Batch 810, LR 2.475331 Loss 19.064445, Accuracy 0.085%\n",
      "Epoch 13, Batch 811, LR 2.475304 Loss 19.064413, Accuracy 0.085%\n",
      "Epoch 13, Batch 812, LR 2.475278 Loss 19.064221, Accuracy 0.085%\n",
      "Epoch 13, Batch 813, LR 2.475251 Loss 19.063969, Accuracy 0.085%\n",
      "Epoch 13, Batch 814, LR 2.475225 Loss 19.064031, Accuracy 0.084%\n",
      "Epoch 13, Batch 815, LR 2.475198 Loss 19.064063, Accuracy 0.084%\n",
      "Epoch 13, Batch 816, LR 2.475171 Loss 19.064230, Accuracy 0.084%\n",
      "Epoch 13, Batch 817, LR 2.475145 Loss 19.064269, Accuracy 0.084%\n",
      "Epoch 13, Batch 818, LR 2.475118 Loss 19.064003, Accuracy 0.084%\n",
      "Epoch 13, Batch 819, LR 2.475092 Loss 19.064028, Accuracy 0.084%\n",
      "Epoch 13, Batch 820, LR 2.475065 Loss 19.063983, Accuracy 0.084%\n",
      "Epoch 13, Batch 821, LR 2.475038 Loss 19.063946, Accuracy 0.084%\n",
      "Epoch 13, Batch 822, LR 2.475012 Loss 19.064090, Accuracy 0.084%\n",
      "Epoch 13, Batch 823, LR 2.474985 Loss 19.064088, Accuracy 0.084%\n",
      "Epoch 13, Batch 824, LR 2.474959 Loss 19.064249, Accuracy 0.083%\n",
      "Epoch 13, Batch 825, LR 2.474932 Loss 19.064407, Accuracy 0.083%\n",
      "Epoch 13, Batch 826, LR 2.474905 Loss 19.064376, Accuracy 0.083%\n",
      "Epoch 13, Batch 827, LR 2.474878 Loss 19.064139, Accuracy 0.083%\n",
      "Epoch 13, Batch 828, LR 2.474852 Loss 19.064124, Accuracy 0.083%\n",
      "Epoch 13, Batch 829, LR 2.474825 Loss 19.064200, Accuracy 0.083%\n",
      "Epoch 13, Batch 830, LR 2.474798 Loss 19.064300, Accuracy 0.083%\n",
      "Epoch 13, Batch 831, LR 2.474771 Loss 19.064352, Accuracy 0.083%\n",
      "Epoch 13, Batch 832, LR 2.474745 Loss 19.064471, Accuracy 0.083%\n",
      "Epoch 13, Batch 833, LR 2.474718 Loss 19.064560, Accuracy 0.083%\n",
      "Epoch 13, Batch 834, LR 2.474691 Loss 19.064621, Accuracy 0.082%\n",
      "Epoch 13, Batch 835, LR 2.474664 Loss 19.064633, Accuracy 0.082%\n",
      "Epoch 13, Batch 836, LR 2.474637 Loss 19.064817, Accuracy 0.083%\n",
      "Epoch 13, Batch 837, LR 2.474610 Loss 19.064963, Accuracy 0.083%\n",
      "Epoch 13, Batch 838, LR 2.474584 Loss 19.065016, Accuracy 0.084%\n",
      "Epoch 13, Batch 839, LR 2.474557 Loss 19.065123, Accuracy 0.084%\n",
      "Epoch 13, Batch 840, LR 2.474530 Loss 19.065268, Accuracy 0.084%\n",
      "Epoch 13, Batch 841, LR 2.474503 Loss 19.065171, Accuracy 0.084%\n",
      "Epoch 13, Batch 842, LR 2.474476 Loss 19.065211, Accuracy 0.084%\n",
      "Epoch 13, Batch 843, LR 2.474449 Loss 19.065438, Accuracy 0.083%\n",
      "Epoch 13, Batch 844, LR 2.474422 Loss 19.065366, Accuracy 0.083%\n",
      "Epoch 13, Batch 845, LR 2.474395 Loss 19.065373, Accuracy 0.083%\n",
      "Epoch 13, Batch 846, LR 2.474368 Loss 19.065286, Accuracy 0.083%\n",
      "Epoch 13, Batch 847, LR 2.474341 Loss 19.065326, Accuracy 0.083%\n",
      "Epoch 13, Batch 848, LR 2.474314 Loss 19.065291, Accuracy 0.083%\n",
      "Epoch 13, Batch 849, LR 2.474287 Loss 19.065057, Accuracy 0.084%\n",
      "Epoch 13, Batch 850, LR 2.474260 Loss 19.065055, Accuracy 0.085%\n",
      "Epoch 13, Batch 851, LR 2.474233 Loss 19.065230, Accuracy 0.084%\n",
      "Epoch 13, Batch 852, LR 2.474206 Loss 19.065381, Accuracy 0.084%\n",
      "Epoch 13, Batch 853, LR 2.474179 Loss 19.065557, Accuracy 0.084%\n",
      "Epoch 13, Batch 854, LR 2.474152 Loss 19.065570, Accuracy 0.084%\n",
      "Epoch 13, Batch 855, LR 2.474125 Loss 19.065549, Accuracy 0.084%\n",
      "Epoch 13, Batch 856, LR 2.474098 Loss 19.065508, Accuracy 0.085%\n",
      "Epoch 13, Batch 857, LR 2.474070 Loss 19.065448, Accuracy 0.085%\n",
      "Epoch 13, Batch 858, LR 2.474043 Loss 19.065417, Accuracy 0.085%\n",
      "Epoch 13, Batch 859, LR 2.474016 Loss 19.065463, Accuracy 0.085%\n",
      "Epoch 13, Batch 860, LR 2.473989 Loss 19.065314, Accuracy 0.084%\n",
      "Epoch 13, Batch 861, LR 2.473962 Loss 19.065402, Accuracy 0.084%\n",
      "Epoch 13, Batch 862, LR 2.473935 Loss 19.065305, Accuracy 0.084%\n",
      "Epoch 13, Batch 863, LR 2.473907 Loss 19.065017, Accuracy 0.084%\n",
      "Epoch 13, Batch 864, LR 2.473880 Loss 19.065001, Accuracy 0.084%\n",
      "Epoch 13, Batch 865, LR 2.473853 Loss 19.064938, Accuracy 0.084%\n",
      "Epoch 13, Batch 866, LR 2.473826 Loss 19.065192, Accuracy 0.084%\n",
      "Epoch 13, Batch 867, LR 2.473798 Loss 19.065124, Accuracy 0.084%\n",
      "Epoch 13, Batch 868, LR 2.473771 Loss 19.065318, Accuracy 0.084%\n",
      "Epoch 13, Batch 869, LR 2.473744 Loss 19.065361, Accuracy 0.084%\n",
      "Epoch 13, Batch 870, LR 2.473716 Loss 19.065386, Accuracy 0.084%\n",
      "Epoch 13, Batch 871, LR 2.473689 Loss 19.065332, Accuracy 0.084%\n",
      "Epoch 13, Batch 872, LR 2.473662 Loss 19.065289, Accuracy 0.084%\n",
      "Epoch 13, Batch 873, LR 2.473634 Loss 19.065228, Accuracy 0.084%\n",
      "Epoch 13, Batch 874, LR 2.473607 Loss 19.065182, Accuracy 0.084%\n",
      "Epoch 13, Batch 875, LR 2.473580 Loss 19.065140, Accuracy 0.084%\n",
      "Epoch 13, Batch 876, LR 2.473552 Loss 19.065268, Accuracy 0.084%\n",
      "Epoch 13, Batch 877, LR 2.473525 Loss 19.065074, Accuracy 0.084%\n",
      "Epoch 13, Batch 878, LR 2.473497 Loss 19.065122, Accuracy 0.084%\n",
      "Epoch 13, Batch 879, LR 2.473470 Loss 19.064961, Accuracy 0.084%\n",
      "Epoch 13, Batch 880, LR 2.473442 Loss 19.065074, Accuracy 0.083%\n",
      "Epoch 13, Batch 881, LR 2.473415 Loss 19.065248, Accuracy 0.083%\n",
      "Epoch 13, Batch 882, LR 2.473388 Loss 19.065546, Accuracy 0.083%\n",
      "Epoch 13, Batch 883, LR 2.473360 Loss 19.065732, Accuracy 0.083%\n",
      "Epoch 13, Batch 884, LR 2.473332 Loss 19.065849, Accuracy 0.083%\n",
      "Epoch 13, Batch 885, LR 2.473305 Loss 19.066077, Accuracy 0.083%\n",
      "Epoch 13, Batch 886, LR 2.473277 Loss 19.065838, Accuracy 0.083%\n",
      "Epoch 13, Batch 887, LR 2.473250 Loss 19.065713, Accuracy 0.084%\n",
      "Epoch 13, Batch 888, LR 2.473222 Loss 19.065778, Accuracy 0.084%\n",
      "Epoch 13, Batch 889, LR 2.473195 Loss 19.065772, Accuracy 0.083%\n",
      "Epoch 13, Batch 890, LR 2.473167 Loss 19.065887, Accuracy 0.083%\n",
      "Epoch 13, Batch 891, LR 2.473139 Loss 19.065934, Accuracy 0.083%\n",
      "Epoch 13, Batch 892, LR 2.473112 Loss 19.065766, Accuracy 0.084%\n",
      "Epoch 13, Batch 893, LR 2.473084 Loss 19.065868, Accuracy 0.084%\n",
      "Epoch 13, Batch 894, LR 2.473057 Loss 19.065711, Accuracy 0.085%\n",
      "Epoch 13, Batch 895, LR 2.473029 Loss 19.065646, Accuracy 0.086%\n",
      "Epoch 13, Batch 896, LR 2.473001 Loss 19.065673, Accuracy 0.085%\n",
      "Epoch 13, Batch 897, LR 2.472974 Loss 19.065640, Accuracy 0.086%\n",
      "Epoch 13, Batch 898, LR 2.472946 Loss 19.065744, Accuracy 0.086%\n",
      "Epoch 13, Batch 899, LR 2.472918 Loss 19.065734, Accuracy 0.086%\n",
      "Epoch 13, Batch 900, LR 2.472890 Loss 19.065675, Accuracy 0.086%\n",
      "Epoch 13, Batch 901, LR 2.472863 Loss 19.065540, Accuracy 0.086%\n",
      "Epoch 13, Batch 902, LR 2.472835 Loss 19.065774, Accuracy 0.086%\n",
      "Epoch 13, Batch 903, LR 2.472807 Loss 19.065742, Accuracy 0.086%\n",
      "Epoch 13, Batch 904, LR 2.472779 Loss 19.065818, Accuracy 0.086%\n",
      "Epoch 13, Batch 905, LR 2.472751 Loss 19.065821, Accuracy 0.085%\n",
      "Epoch 13, Batch 906, LR 2.472724 Loss 19.065952, Accuracy 0.085%\n",
      "Epoch 13, Batch 907, LR 2.472696 Loss 19.065824, Accuracy 0.085%\n",
      "Epoch 13, Batch 908, LR 2.472668 Loss 19.065749, Accuracy 0.085%\n",
      "Epoch 13, Batch 909, LR 2.472640 Loss 19.065709, Accuracy 0.085%\n",
      "Epoch 13, Batch 910, LR 2.472612 Loss 19.065662, Accuracy 0.085%\n",
      "Epoch 13, Batch 911, LR 2.472584 Loss 19.065584, Accuracy 0.085%\n",
      "Epoch 13, Batch 912, LR 2.472556 Loss 19.065522, Accuracy 0.085%\n",
      "Epoch 13, Batch 913, LR 2.472528 Loss 19.065631, Accuracy 0.085%\n",
      "Epoch 13, Batch 914, LR 2.472501 Loss 19.065757, Accuracy 0.085%\n",
      "Epoch 13, Batch 915, LR 2.472473 Loss 19.065846, Accuracy 0.085%\n",
      "Epoch 13, Batch 916, LR 2.472445 Loss 19.066042, Accuracy 0.084%\n",
      "Epoch 13, Batch 917, LR 2.472417 Loss 19.066344, Accuracy 0.084%\n",
      "Epoch 13, Batch 918, LR 2.472389 Loss 19.066497, Accuracy 0.084%\n",
      "Epoch 13, Batch 919, LR 2.472361 Loss 19.066503, Accuracy 0.084%\n",
      "Epoch 13, Batch 920, LR 2.472333 Loss 19.066591, Accuracy 0.085%\n",
      "Epoch 13, Batch 921, LR 2.472305 Loss 19.066563, Accuracy 0.085%\n",
      "Epoch 13, Batch 922, LR 2.472277 Loss 19.066759, Accuracy 0.085%\n",
      "Epoch 13, Batch 923, LR 2.472248 Loss 19.066750, Accuracy 0.085%\n",
      "Epoch 13, Batch 924, LR 2.472220 Loss 19.066778, Accuracy 0.085%\n",
      "Epoch 13, Batch 925, LR 2.472192 Loss 19.066681, Accuracy 0.084%\n",
      "Epoch 13, Batch 926, LR 2.472164 Loss 19.066697, Accuracy 0.084%\n",
      "Epoch 13, Batch 927, LR 2.472136 Loss 19.066699, Accuracy 0.084%\n",
      "Epoch 13, Batch 928, LR 2.472108 Loss 19.066431, Accuracy 0.084%\n",
      "Epoch 13, Batch 929, LR 2.472080 Loss 19.066458, Accuracy 0.084%\n",
      "Epoch 13, Batch 930, LR 2.472052 Loss 19.066634, Accuracy 0.084%\n",
      "Epoch 13, Batch 931, LR 2.472023 Loss 19.066670, Accuracy 0.084%\n",
      "Epoch 13, Batch 932, LR 2.471995 Loss 19.066769, Accuracy 0.084%\n",
      "Epoch 13, Batch 933, LR 2.471967 Loss 19.066811, Accuracy 0.084%\n",
      "Epoch 13, Batch 934, LR 2.471939 Loss 19.066866, Accuracy 0.084%\n",
      "Epoch 13, Batch 935, LR 2.471911 Loss 19.066696, Accuracy 0.084%\n",
      "Epoch 13, Batch 936, LR 2.471882 Loss 19.066554, Accuracy 0.083%\n",
      "Epoch 13, Batch 937, LR 2.471854 Loss 19.066624, Accuracy 0.083%\n",
      "Epoch 13, Batch 938, LR 2.471826 Loss 19.066655, Accuracy 0.083%\n",
      "Epoch 13, Batch 939, LR 2.471798 Loss 19.066832, Accuracy 0.083%\n",
      "Epoch 13, Batch 940, LR 2.471769 Loss 19.066689, Accuracy 0.083%\n",
      "Epoch 13, Batch 941, LR 2.471741 Loss 19.066700, Accuracy 0.083%\n",
      "Epoch 13, Batch 942, LR 2.471713 Loss 19.066714, Accuracy 0.083%\n",
      "Epoch 13, Batch 943, LR 2.471684 Loss 19.066854, Accuracy 0.083%\n",
      "Epoch 13, Batch 944, LR 2.471656 Loss 19.066970, Accuracy 0.083%\n",
      "Epoch 13, Batch 945, LR 2.471628 Loss 19.067114, Accuracy 0.083%\n",
      "Epoch 13, Batch 946, LR 2.471599 Loss 19.067104, Accuracy 0.083%\n",
      "Epoch 13, Batch 947, LR 2.471571 Loss 19.067003, Accuracy 0.082%\n",
      "Epoch 13, Batch 948, LR 2.471542 Loss 19.066969, Accuracy 0.082%\n",
      "Epoch 13, Batch 949, LR 2.471514 Loss 19.067050, Accuracy 0.082%\n",
      "Epoch 13, Batch 950, LR 2.471485 Loss 19.067167, Accuracy 0.082%\n",
      "Epoch 13, Batch 951, LR 2.471457 Loss 19.067144, Accuracy 0.082%\n",
      "Epoch 13, Batch 952, LR 2.471429 Loss 19.067263, Accuracy 0.082%\n",
      "Epoch 13, Batch 953, LR 2.471400 Loss 19.067421, Accuracy 0.082%\n",
      "Epoch 13, Batch 954, LR 2.471372 Loss 19.067338, Accuracy 0.082%\n",
      "Epoch 13, Batch 955, LR 2.471343 Loss 19.067358, Accuracy 0.082%\n",
      "Epoch 13, Batch 956, LR 2.471315 Loss 19.067230, Accuracy 0.082%\n",
      "Epoch 13, Batch 957, LR 2.471286 Loss 19.067204, Accuracy 0.082%\n",
      "Epoch 13, Batch 958, LR 2.471257 Loss 19.067417, Accuracy 0.082%\n",
      "Epoch 13, Batch 959, LR 2.471229 Loss 19.067263, Accuracy 0.081%\n",
      "Epoch 13, Batch 960, LR 2.471200 Loss 19.067269, Accuracy 0.081%\n",
      "Epoch 13, Batch 961, LR 2.471172 Loss 19.067304, Accuracy 0.081%\n",
      "Epoch 13, Batch 962, LR 2.471143 Loss 19.067359, Accuracy 0.081%\n",
      "Epoch 13, Batch 963, LR 2.471114 Loss 19.067329, Accuracy 0.081%\n",
      "Epoch 13, Batch 964, LR 2.471086 Loss 19.067352, Accuracy 0.081%\n",
      "Epoch 13, Batch 965, LR 2.471057 Loss 19.067369, Accuracy 0.081%\n",
      "Epoch 13, Batch 966, LR 2.471029 Loss 19.067301, Accuracy 0.081%\n",
      "Epoch 13, Batch 967, LR 2.471000 Loss 19.067077, Accuracy 0.081%\n",
      "Epoch 13, Batch 968, LR 2.470971 Loss 19.067184, Accuracy 0.081%\n",
      "Epoch 13, Batch 969, LR 2.470942 Loss 19.067136, Accuracy 0.081%\n",
      "Epoch 13, Batch 970, LR 2.470914 Loss 19.067046, Accuracy 0.081%\n",
      "Epoch 13, Batch 971, LR 2.470885 Loss 19.066858, Accuracy 0.080%\n",
      "Epoch 13, Batch 972, LR 2.470856 Loss 19.066952, Accuracy 0.081%\n",
      "Epoch 13, Batch 973, LR 2.470827 Loss 19.066967, Accuracy 0.081%\n",
      "Epoch 13, Batch 974, LR 2.470799 Loss 19.066958, Accuracy 0.081%\n",
      "Epoch 13, Batch 975, LR 2.470770 Loss 19.067074, Accuracy 0.081%\n",
      "Epoch 13, Batch 976, LR 2.470741 Loss 19.066934, Accuracy 0.081%\n",
      "Epoch 13, Batch 977, LR 2.470712 Loss 19.066830, Accuracy 0.081%\n",
      "Epoch 13, Batch 978, LR 2.470683 Loss 19.066742, Accuracy 0.081%\n",
      "Epoch 13, Batch 979, LR 2.470655 Loss 19.066522, Accuracy 0.081%\n",
      "Epoch 13, Batch 980, LR 2.470626 Loss 19.066712, Accuracy 0.081%\n",
      "Epoch 13, Batch 981, LR 2.470597 Loss 19.066697, Accuracy 0.080%\n",
      "Epoch 13, Batch 982, LR 2.470568 Loss 19.066978, Accuracy 0.080%\n",
      "Epoch 13, Batch 983, LR 2.470539 Loss 19.066980, Accuracy 0.080%\n",
      "Epoch 13, Batch 984, LR 2.470510 Loss 19.066993, Accuracy 0.080%\n",
      "Epoch 13, Batch 985, LR 2.470481 Loss 19.067157, Accuracy 0.080%\n",
      "Epoch 13, Batch 986, LR 2.470452 Loss 19.067166, Accuracy 0.080%\n",
      "Epoch 13, Batch 987, LR 2.470423 Loss 19.067074, Accuracy 0.080%\n",
      "Epoch 13, Batch 988, LR 2.470394 Loss 19.066913, Accuracy 0.081%\n",
      "Epoch 13, Batch 989, LR 2.470365 Loss 19.067114, Accuracy 0.081%\n",
      "Epoch 13, Batch 990, LR 2.470336 Loss 19.067076, Accuracy 0.080%\n",
      "Epoch 13, Batch 991, LR 2.470307 Loss 19.067055, Accuracy 0.080%\n",
      "Epoch 13, Batch 992, LR 2.470278 Loss 19.067141, Accuracy 0.080%\n",
      "Epoch 13, Batch 993, LR 2.470249 Loss 19.067224, Accuracy 0.080%\n",
      "Epoch 13, Batch 994, LR 2.470220 Loss 19.067207, Accuracy 0.080%\n",
      "Epoch 13, Batch 995, LR 2.470191 Loss 19.067163, Accuracy 0.080%\n",
      "Epoch 13, Batch 996, LR 2.470162 Loss 19.067361, Accuracy 0.080%\n",
      "Epoch 13, Batch 997, LR 2.470133 Loss 19.067222, Accuracy 0.080%\n",
      "Epoch 13, Batch 998, LR 2.470104 Loss 19.067065, Accuracy 0.081%\n",
      "Epoch 13, Batch 999, LR 2.470075 Loss 19.066976, Accuracy 0.081%\n",
      "Epoch 13, Batch 1000, LR 2.470046 Loss 19.066984, Accuracy 0.080%\n",
      "Epoch 13, Batch 1001, LR 2.470016 Loss 19.066918, Accuracy 0.080%\n",
      "Epoch 13, Batch 1002, LR 2.469987 Loss 19.066934, Accuracy 0.080%\n",
      "Epoch 13, Batch 1003, LR 2.469958 Loss 19.067101, Accuracy 0.080%\n",
      "Epoch 13, Batch 1004, LR 2.469929 Loss 19.067285, Accuracy 0.080%\n",
      "Epoch 13, Batch 1005, LR 2.469900 Loss 19.067203, Accuracy 0.080%\n",
      "Epoch 13, Batch 1006, LR 2.469870 Loss 19.067049, Accuracy 0.080%\n",
      "Epoch 13, Batch 1007, LR 2.469841 Loss 19.067049, Accuracy 0.080%\n",
      "Epoch 13, Batch 1008, LR 2.469812 Loss 19.067090, Accuracy 0.080%\n",
      "Epoch 13, Batch 1009, LR 2.469783 Loss 19.066951, Accuracy 0.080%\n",
      "Epoch 13, Batch 1010, LR 2.469753 Loss 19.066950, Accuracy 0.080%\n",
      "Epoch 13, Batch 1011, LR 2.469724 Loss 19.066984, Accuracy 0.080%\n",
      "Epoch 13, Batch 1012, LR 2.469695 Loss 19.067119, Accuracy 0.080%\n",
      "Epoch 13, Batch 1013, LR 2.469665 Loss 19.067212, Accuracy 0.079%\n",
      "Epoch 13, Batch 1014, LR 2.469636 Loss 19.067267, Accuracy 0.079%\n",
      "Epoch 13, Batch 1015, LR 2.469607 Loss 19.067132, Accuracy 0.079%\n",
      "Epoch 13, Batch 1016, LR 2.469577 Loss 19.067111, Accuracy 0.079%\n",
      "Epoch 13, Batch 1017, LR 2.469548 Loss 19.067191, Accuracy 0.079%\n",
      "Epoch 13, Batch 1018, LR 2.469519 Loss 19.067161, Accuracy 0.079%\n",
      "Epoch 13, Batch 1019, LR 2.469489 Loss 19.067133, Accuracy 0.079%\n",
      "Epoch 13, Batch 1020, LR 2.469460 Loss 19.067086, Accuracy 0.079%\n",
      "Epoch 13, Batch 1021, LR 2.469430 Loss 19.067133, Accuracy 0.079%\n",
      "Epoch 13, Batch 1022, LR 2.469401 Loss 19.066974, Accuracy 0.080%\n",
      "Epoch 13, Batch 1023, LR 2.469371 Loss 19.066922, Accuracy 0.080%\n",
      "Epoch 13, Batch 1024, LR 2.469342 Loss 19.066752, Accuracy 0.080%\n",
      "Epoch 13, Batch 1025, LR 2.469312 Loss 19.066570, Accuracy 0.080%\n",
      "Epoch 13, Batch 1026, LR 2.469283 Loss 19.066582, Accuracy 0.080%\n",
      "Epoch 13, Batch 1027, LR 2.469253 Loss 19.066766, Accuracy 0.080%\n",
      "Epoch 13, Batch 1028, LR 2.469224 Loss 19.066563, Accuracy 0.080%\n",
      "Epoch 13, Batch 1029, LR 2.469194 Loss 19.066671, Accuracy 0.080%\n",
      "Epoch 13, Batch 1030, LR 2.469165 Loss 19.066443, Accuracy 0.080%\n",
      "Epoch 13, Batch 1031, LR 2.469135 Loss 19.066387, Accuracy 0.080%\n",
      "Epoch 13, Batch 1032, LR 2.469106 Loss 19.066316, Accuracy 0.079%\n",
      "Epoch 13, Batch 1033, LR 2.469076 Loss 19.066402, Accuracy 0.079%\n",
      "Epoch 13, Batch 1034, LR 2.469046 Loss 19.066210, Accuracy 0.079%\n",
      "Epoch 13, Batch 1035, LR 2.469017 Loss 19.066123, Accuracy 0.079%\n",
      "Epoch 13, Batch 1036, LR 2.468987 Loss 19.066174, Accuracy 0.079%\n",
      "Epoch 13, Batch 1037, LR 2.468957 Loss 19.066155, Accuracy 0.079%\n",
      "Epoch 13, Batch 1038, LR 2.468928 Loss 19.066047, Accuracy 0.079%\n",
      "Epoch 13, Batch 1039, LR 2.468898 Loss 19.066083, Accuracy 0.079%\n",
      "Epoch 13, Batch 1040, LR 2.468868 Loss 19.065994, Accuracy 0.079%\n",
      "Epoch 13, Batch 1041, LR 2.468839 Loss 19.066164, Accuracy 0.079%\n",
      "Epoch 13, Batch 1042, LR 2.468809 Loss 19.066075, Accuracy 0.079%\n",
      "Epoch 13, Batch 1043, LR 2.468779 Loss 19.065774, Accuracy 0.079%\n",
      "Epoch 13, Batch 1044, LR 2.468749 Loss 19.065684, Accuracy 0.079%\n",
      "Epoch 13, Batch 1045, LR 2.468720 Loss 19.065565, Accuracy 0.078%\n",
      "Epoch 13, Batch 1046, LR 2.468690 Loss 19.065478, Accuracy 0.078%\n",
      "Epoch 13, Batch 1047, LR 2.468660 Loss 19.065413, Accuracy 0.079%\n",
      "Epoch 13, Loss (train set) 19.065413, Accuracy (train set) 0.079%\n",
      "Epoch 14, Batch 1, LR 2.468630 Loss 19.198631, Accuracy 0.000%\n",
      "Epoch 14, Batch 2, LR 2.468600 Loss 19.059717, Accuracy 0.000%\n",
      "Epoch 14, Batch 3, LR 2.468571 Loss 19.126928, Accuracy 0.000%\n",
      "Epoch 14, Batch 4, LR 2.468541 Loss 19.135232, Accuracy 0.000%\n",
      "Epoch 14, Batch 5, LR 2.468511 Loss 19.119809, Accuracy 0.000%\n",
      "Epoch 14, Batch 6, LR 2.468481 Loss 19.118566, Accuracy 0.000%\n",
      "Epoch 14, Batch 7, LR 2.468451 Loss 19.076131, Accuracy 0.000%\n",
      "Epoch 14, Batch 8, LR 2.468421 Loss 19.088372, Accuracy 0.000%\n",
      "Epoch 14, Batch 9, LR 2.468391 Loss 19.083874, Accuracy 0.000%\n",
      "Epoch 14, Batch 10, LR 2.468361 Loss 19.078861, Accuracy 0.000%\n",
      "Epoch 14, Batch 11, LR 2.468331 Loss 19.068177, Accuracy 0.000%\n",
      "Epoch 14, Batch 12, LR 2.468301 Loss 19.077850, Accuracy 0.065%\n",
      "Epoch 14, Batch 13, LR 2.468271 Loss 19.064697, Accuracy 0.060%\n",
      "Epoch 14, Batch 14, LR 2.468241 Loss 19.056355, Accuracy 0.056%\n",
      "Epoch 14, Batch 15, LR 2.468211 Loss 19.055116, Accuracy 0.052%\n",
      "Epoch 14, Batch 16, LR 2.468181 Loss 19.051800, Accuracy 0.049%\n",
      "Epoch 14, Batch 17, LR 2.468151 Loss 19.050623, Accuracy 0.046%\n",
      "Epoch 14, Batch 18, LR 2.468121 Loss 19.054703, Accuracy 0.043%\n",
      "Epoch 14, Batch 19, LR 2.468091 Loss 19.050778, Accuracy 0.041%\n",
      "Epoch 14, Batch 20, LR 2.468061 Loss 19.052265, Accuracy 0.039%\n",
      "Epoch 14, Batch 21, LR 2.468031 Loss 19.057556, Accuracy 0.037%\n",
      "Epoch 14, Batch 22, LR 2.468001 Loss 19.059752, Accuracy 0.036%\n",
      "Epoch 14, Batch 23, LR 2.467971 Loss 19.058829, Accuracy 0.034%\n",
      "Epoch 14, Batch 24, LR 2.467941 Loss 19.059726, Accuracy 0.033%\n",
      "Epoch 14, Batch 25, LR 2.467910 Loss 19.064163, Accuracy 0.031%\n",
      "Epoch 14, Batch 26, LR 2.467880 Loss 19.070052, Accuracy 0.030%\n",
      "Epoch 14, Batch 27, LR 2.467850 Loss 19.072299, Accuracy 0.029%\n",
      "Epoch 14, Batch 28, LR 2.467820 Loss 19.076153, Accuracy 0.028%\n",
      "Epoch 14, Batch 29, LR 2.467790 Loss 19.077785, Accuracy 0.027%\n",
      "Epoch 14, Batch 30, LR 2.467760 Loss 19.082787, Accuracy 0.026%\n",
      "Epoch 14, Batch 31, LR 2.467729 Loss 19.071658, Accuracy 0.025%\n",
      "Epoch 14, Batch 32, LR 2.467699 Loss 19.071642, Accuracy 0.024%\n",
      "Epoch 14, Batch 33, LR 2.467669 Loss 19.060504, Accuracy 0.024%\n",
      "Epoch 14, Batch 34, LR 2.467639 Loss 19.061734, Accuracy 0.023%\n",
      "Epoch 14, Batch 35, LR 2.467608 Loss 19.065320, Accuracy 0.022%\n",
      "Epoch 14, Batch 36, LR 2.467578 Loss 19.065327, Accuracy 0.022%\n",
      "Epoch 14, Batch 37, LR 2.467548 Loss 19.067524, Accuracy 0.042%\n",
      "Epoch 14, Batch 38, LR 2.467517 Loss 19.073175, Accuracy 0.041%\n",
      "Epoch 14, Batch 39, LR 2.467487 Loss 19.074354, Accuracy 0.040%\n",
      "Epoch 14, Batch 40, LR 2.467457 Loss 19.072949, Accuracy 0.039%\n",
      "Epoch 14, Batch 41, LR 2.467426 Loss 19.074685, Accuracy 0.038%\n",
      "Epoch 14, Batch 42, LR 2.467396 Loss 19.074346, Accuracy 0.037%\n",
      "Epoch 14, Batch 43, LR 2.467365 Loss 19.073684, Accuracy 0.036%\n",
      "Epoch 14, Batch 44, LR 2.467335 Loss 19.072468, Accuracy 0.036%\n",
      "Epoch 14, Batch 45, LR 2.467305 Loss 19.071473, Accuracy 0.035%\n",
      "Epoch 14, Batch 46, LR 2.467274 Loss 19.072688, Accuracy 0.034%\n",
      "Epoch 14, Batch 47, LR 2.467244 Loss 19.072479, Accuracy 0.033%\n",
      "Epoch 14, Batch 48, LR 2.467213 Loss 19.067613, Accuracy 0.033%\n",
      "Epoch 14, Batch 49, LR 2.467183 Loss 19.064854, Accuracy 0.032%\n",
      "Epoch 14, Batch 50, LR 2.467152 Loss 19.071587, Accuracy 0.031%\n",
      "Epoch 14, Batch 51, LR 2.467122 Loss 19.074200, Accuracy 0.031%\n",
      "Epoch 14, Batch 52, LR 2.467091 Loss 19.073519, Accuracy 0.030%\n",
      "Epoch 14, Batch 53, LR 2.467061 Loss 19.071176, Accuracy 0.029%\n",
      "Epoch 14, Batch 54, LR 2.467030 Loss 19.072882, Accuracy 0.029%\n",
      "Epoch 14, Batch 55, LR 2.466999 Loss 19.072012, Accuracy 0.028%\n",
      "Epoch 14, Batch 56, LR 2.466969 Loss 19.073653, Accuracy 0.028%\n",
      "Epoch 14, Batch 57, LR 2.466938 Loss 19.071146, Accuracy 0.041%\n",
      "Epoch 14, Batch 58, LR 2.466908 Loss 19.070725, Accuracy 0.040%\n",
      "Epoch 14, Batch 59, LR 2.466877 Loss 19.073933, Accuracy 0.040%\n",
      "Epoch 14, Batch 60, LR 2.466846 Loss 19.074594, Accuracy 0.039%\n",
      "Epoch 14, Batch 61, LR 2.466816 Loss 19.072229, Accuracy 0.038%\n",
      "Epoch 14, Batch 62, LR 2.466785 Loss 19.072964, Accuracy 0.038%\n",
      "Epoch 14, Batch 63, LR 2.466754 Loss 19.072313, Accuracy 0.037%\n",
      "Epoch 14, Batch 64, LR 2.466724 Loss 19.075072, Accuracy 0.037%\n",
      "Epoch 14, Batch 65, LR 2.466693 Loss 19.071679, Accuracy 0.036%\n",
      "Epoch 14, Batch 66, LR 2.466662 Loss 19.072327, Accuracy 0.047%\n",
      "Epoch 14, Batch 67, LR 2.466632 Loss 19.073202, Accuracy 0.047%\n",
      "Epoch 14, Batch 68, LR 2.466601 Loss 19.070526, Accuracy 0.046%\n",
      "Epoch 14, Batch 69, LR 2.466570 Loss 19.072163, Accuracy 0.045%\n",
      "Epoch 14, Batch 70, LR 2.466539 Loss 19.073233, Accuracy 0.045%\n",
      "Epoch 14, Batch 71, LR 2.466508 Loss 19.070626, Accuracy 0.044%\n",
      "Epoch 14, Batch 72, LR 2.466478 Loss 19.073045, Accuracy 0.043%\n",
      "Epoch 14, Batch 73, LR 2.466447 Loss 19.073786, Accuracy 0.043%\n",
      "Epoch 14, Batch 74, LR 2.466416 Loss 19.072086, Accuracy 0.053%\n",
      "Epoch 14, Batch 75, LR 2.466385 Loss 19.072522, Accuracy 0.052%\n",
      "Epoch 14, Batch 76, LR 2.466354 Loss 19.069755, Accuracy 0.051%\n",
      "Epoch 14, Batch 77, LR 2.466323 Loss 19.067923, Accuracy 0.051%\n",
      "Epoch 14, Batch 78, LR 2.466293 Loss 19.069744, Accuracy 0.050%\n",
      "Epoch 14, Batch 79, LR 2.466262 Loss 19.068325, Accuracy 0.049%\n",
      "Epoch 14, Batch 80, LR 2.466231 Loss 19.067580, Accuracy 0.049%\n",
      "Epoch 14, Batch 81, LR 2.466200 Loss 19.067100, Accuracy 0.048%\n",
      "Epoch 14, Batch 82, LR 2.466169 Loss 19.066082, Accuracy 0.048%\n",
      "Epoch 14, Batch 83, LR 2.466138 Loss 19.065793, Accuracy 0.047%\n",
      "Epoch 14, Batch 84, LR 2.466107 Loss 19.066950, Accuracy 0.047%\n",
      "Epoch 14, Batch 85, LR 2.466076 Loss 19.066796, Accuracy 0.046%\n",
      "Epoch 14, Batch 86, LR 2.466045 Loss 19.068136, Accuracy 0.045%\n",
      "Epoch 14, Batch 87, LR 2.466014 Loss 19.067765, Accuracy 0.045%\n",
      "Epoch 14, Batch 88, LR 2.465983 Loss 19.067231, Accuracy 0.044%\n",
      "Epoch 14, Batch 89, LR 2.465952 Loss 19.067283, Accuracy 0.044%\n",
      "Epoch 14, Batch 90, LR 2.465921 Loss 19.067761, Accuracy 0.043%\n",
      "Epoch 14, Batch 91, LR 2.465890 Loss 19.067539, Accuracy 0.043%\n",
      "Epoch 14, Batch 92, LR 2.465859 Loss 19.068043, Accuracy 0.042%\n",
      "Epoch 14, Batch 93, LR 2.465827 Loss 19.068789, Accuracy 0.042%\n",
      "Epoch 14, Batch 94, LR 2.465796 Loss 19.069017, Accuracy 0.042%\n",
      "Epoch 14, Batch 95, LR 2.465765 Loss 19.069633, Accuracy 0.041%\n",
      "Epoch 14, Batch 96, LR 2.465734 Loss 19.071245, Accuracy 0.041%\n",
      "Epoch 14, Batch 97, LR 2.465703 Loss 19.073396, Accuracy 0.040%\n",
      "Epoch 14, Batch 98, LR 2.465672 Loss 19.072055, Accuracy 0.048%\n",
      "Epoch 14, Batch 99, LR 2.465641 Loss 19.071722, Accuracy 0.047%\n",
      "Epoch 14, Batch 100, LR 2.465609 Loss 19.072598, Accuracy 0.047%\n",
      "Epoch 14, Batch 101, LR 2.465578 Loss 19.073081, Accuracy 0.046%\n",
      "Epoch 14, Batch 102, LR 2.465547 Loss 19.072664, Accuracy 0.046%\n",
      "Epoch 14, Batch 103, LR 2.465516 Loss 19.072598, Accuracy 0.046%\n",
      "Epoch 14, Batch 104, LR 2.465484 Loss 19.072697, Accuracy 0.045%\n",
      "Epoch 14, Batch 105, LR 2.465453 Loss 19.071332, Accuracy 0.045%\n",
      "Epoch 14, Batch 106, LR 2.465422 Loss 19.071818, Accuracy 0.044%\n",
      "Epoch 14, Batch 107, LR 2.465391 Loss 19.071016, Accuracy 0.044%\n",
      "Epoch 14, Batch 108, LR 2.465359 Loss 19.071427, Accuracy 0.043%\n",
      "Epoch 14, Batch 109, LR 2.465328 Loss 19.072158, Accuracy 0.043%\n",
      "Epoch 14, Batch 110, LR 2.465297 Loss 19.071367, Accuracy 0.043%\n",
      "Epoch 14, Batch 111, LR 2.465265 Loss 19.071392, Accuracy 0.042%\n",
      "Epoch 14, Batch 112, LR 2.465234 Loss 19.072846, Accuracy 0.042%\n",
      "Epoch 14, Batch 113, LR 2.465203 Loss 19.074135, Accuracy 0.041%\n",
      "Epoch 14, Batch 114, LR 2.465171 Loss 19.076356, Accuracy 0.041%\n",
      "Epoch 14, Batch 115, LR 2.465140 Loss 19.076439, Accuracy 0.041%\n",
      "Epoch 14, Batch 116, LR 2.465108 Loss 19.077454, Accuracy 0.040%\n",
      "Epoch 14, Batch 117, LR 2.465077 Loss 19.077634, Accuracy 0.040%\n",
      "Epoch 14, Batch 118, LR 2.465045 Loss 19.076367, Accuracy 0.040%\n",
      "Epoch 14, Batch 119, LR 2.465014 Loss 19.075767, Accuracy 0.039%\n",
      "Epoch 14, Batch 120, LR 2.464982 Loss 19.077439, Accuracy 0.039%\n",
      "Epoch 14, Batch 121, LR 2.464951 Loss 19.077392, Accuracy 0.045%\n",
      "Epoch 14, Batch 122, LR 2.464920 Loss 19.077799, Accuracy 0.045%\n",
      "Epoch 14, Batch 123, LR 2.464888 Loss 19.078773, Accuracy 0.044%\n",
      "Epoch 14, Batch 124, LR 2.464856 Loss 19.079325, Accuracy 0.044%\n",
      "Epoch 14, Batch 125, LR 2.464825 Loss 19.080298, Accuracy 0.044%\n",
      "Epoch 14, Batch 126, LR 2.464793 Loss 19.078243, Accuracy 0.043%\n",
      "Epoch 14, Batch 127, LR 2.464762 Loss 19.077810, Accuracy 0.043%\n",
      "Epoch 14, Batch 128, LR 2.464730 Loss 19.077990, Accuracy 0.043%\n",
      "Epoch 14, Batch 129, LR 2.464699 Loss 19.077685, Accuracy 0.055%\n",
      "Epoch 14, Batch 130, LR 2.464667 Loss 19.077945, Accuracy 0.054%\n",
      "Epoch 14, Batch 131, LR 2.464635 Loss 19.076673, Accuracy 0.060%\n",
      "Epoch 14, Batch 132, LR 2.464604 Loss 19.076375, Accuracy 0.059%\n",
      "Epoch 14, Batch 133, LR 2.464572 Loss 19.075112, Accuracy 0.059%\n",
      "Epoch 14, Batch 134, LR 2.464540 Loss 19.073542, Accuracy 0.064%\n",
      "Epoch 14, Batch 135, LR 2.464509 Loss 19.073892, Accuracy 0.064%\n",
      "Epoch 14, Batch 136, LR 2.464477 Loss 19.074219, Accuracy 0.063%\n",
      "Epoch 14, Batch 137, LR 2.464445 Loss 19.075028, Accuracy 0.063%\n",
      "Epoch 14, Batch 138, LR 2.464414 Loss 19.073312, Accuracy 0.062%\n",
      "Epoch 14, Batch 139, LR 2.464382 Loss 19.073028, Accuracy 0.073%\n",
      "Epoch 14, Batch 140, LR 2.464350 Loss 19.071647, Accuracy 0.073%\n",
      "Epoch 14, Batch 141, LR 2.464318 Loss 19.070878, Accuracy 0.072%\n",
      "Epoch 14, Batch 142, LR 2.464286 Loss 19.070417, Accuracy 0.072%\n",
      "Epoch 14, Batch 143, LR 2.464255 Loss 19.071726, Accuracy 0.071%\n",
      "Epoch 14, Batch 144, LR 2.464223 Loss 19.072732, Accuracy 0.071%\n",
      "Epoch 14, Batch 145, LR 2.464191 Loss 19.072208, Accuracy 0.070%\n",
      "Epoch 14, Batch 146, LR 2.464159 Loss 19.072422, Accuracy 0.070%\n",
      "Epoch 14, Batch 147, LR 2.464127 Loss 19.072075, Accuracy 0.069%\n",
      "Epoch 14, Batch 148, LR 2.464095 Loss 19.072288, Accuracy 0.069%\n",
      "Epoch 14, Batch 149, LR 2.464064 Loss 19.072078, Accuracy 0.068%\n",
      "Epoch 14, Batch 150, LR 2.464032 Loss 19.071944, Accuracy 0.073%\n",
      "Epoch 14, Batch 151, LR 2.464000 Loss 19.072433, Accuracy 0.072%\n",
      "Epoch 14, Batch 152, LR 2.463968 Loss 19.071666, Accuracy 0.072%\n",
      "Epoch 14, Batch 153, LR 2.463936 Loss 19.071090, Accuracy 0.071%\n",
      "Epoch 14, Batch 154, LR 2.463904 Loss 19.070791, Accuracy 0.071%\n",
      "Epoch 14, Batch 155, LR 2.463872 Loss 19.070878, Accuracy 0.076%\n",
      "Epoch 14, Batch 156, LR 2.463840 Loss 19.071583, Accuracy 0.075%\n",
      "Epoch 14, Batch 157, LR 2.463808 Loss 19.071379, Accuracy 0.075%\n",
      "Epoch 14, Batch 158, LR 2.463776 Loss 19.071552, Accuracy 0.074%\n",
      "Epoch 14, Batch 159, LR 2.463744 Loss 19.072028, Accuracy 0.074%\n",
      "Epoch 14, Batch 160, LR 2.463712 Loss 19.071868, Accuracy 0.073%\n",
      "Epoch 14, Batch 161, LR 2.463680 Loss 19.071366, Accuracy 0.073%\n",
      "Epoch 14, Batch 162, LR 2.463648 Loss 19.070710, Accuracy 0.077%\n",
      "Epoch 14, Batch 163, LR 2.463616 Loss 19.071541, Accuracy 0.077%\n",
      "Epoch 14, Batch 164, LR 2.463584 Loss 19.070460, Accuracy 0.076%\n",
      "Epoch 14, Batch 165, LR 2.463552 Loss 19.069907, Accuracy 0.076%\n",
      "Epoch 14, Batch 166, LR 2.463519 Loss 19.070259, Accuracy 0.075%\n",
      "Epoch 14, Batch 167, LR 2.463487 Loss 19.070598, Accuracy 0.080%\n",
      "Epoch 14, Batch 168, LR 2.463455 Loss 19.070824, Accuracy 0.079%\n",
      "Epoch 14, Batch 169, LR 2.463423 Loss 19.070668, Accuracy 0.079%\n",
      "Epoch 14, Batch 170, LR 2.463391 Loss 19.070040, Accuracy 0.078%\n",
      "Epoch 14, Batch 171, LR 2.463359 Loss 19.071011, Accuracy 0.078%\n",
      "Epoch 14, Batch 172, LR 2.463326 Loss 19.070002, Accuracy 0.077%\n",
      "Epoch 14, Batch 173, LR 2.463294 Loss 19.069522, Accuracy 0.077%\n",
      "Epoch 14, Batch 174, LR 2.463262 Loss 19.071411, Accuracy 0.081%\n",
      "Epoch 14, Batch 175, LR 2.463230 Loss 19.070290, Accuracy 0.080%\n",
      "Epoch 14, Batch 176, LR 2.463198 Loss 19.069895, Accuracy 0.080%\n",
      "Epoch 14, Batch 177, LR 2.463165 Loss 19.069942, Accuracy 0.079%\n",
      "Epoch 14, Batch 178, LR 2.463133 Loss 19.070794, Accuracy 0.083%\n",
      "Epoch 14, Batch 179, LR 2.463101 Loss 19.070121, Accuracy 0.083%\n",
      "Epoch 14, Batch 180, LR 2.463068 Loss 19.070014, Accuracy 0.082%\n",
      "Epoch 14, Batch 181, LR 2.463036 Loss 19.069115, Accuracy 0.082%\n",
      "Epoch 14, Batch 182, LR 2.463004 Loss 19.068253, Accuracy 0.082%\n",
      "Epoch 14, Batch 183, LR 2.462971 Loss 19.067872, Accuracy 0.081%\n",
      "Epoch 14, Batch 184, LR 2.462939 Loss 19.067745, Accuracy 0.081%\n",
      "Epoch 14, Batch 185, LR 2.462907 Loss 19.068090, Accuracy 0.080%\n",
      "Epoch 14, Batch 186, LR 2.462874 Loss 19.068367, Accuracy 0.080%\n",
      "Epoch 14, Batch 187, LR 2.462842 Loss 19.068311, Accuracy 0.079%\n",
      "Epoch 14, Batch 188, LR 2.462809 Loss 19.068699, Accuracy 0.079%\n",
      "Epoch 14, Batch 189, LR 2.462777 Loss 19.069496, Accuracy 0.079%\n",
      "Epoch 14, Batch 190, LR 2.462744 Loss 19.069748, Accuracy 0.078%\n",
      "Epoch 14, Batch 191, LR 2.462712 Loss 19.070119, Accuracy 0.078%\n",
      "Epoch 14, Batch 192, LR 2.462679 Loss 19.070303, Accuracy 0.077%\n",
      "Epoch 14, Batch 193, LR 2.462647 Loss 19.069828, Accuracy 0.077%\n",
      "Epoch 14, Batch 194, LR 2.462614 Loss 19.069992, Accuracy 0.077%\n",
      "Epoch 14, Batch 195, LR 2.462582 Loss 19.069229, Accuracy 0.076%\n",
      "Epoch 14, Batch 196, LR 2.462549 Loss 19.069498, Accuracy 0.076%\n",
      "Epoch 14, Batch 197, LR 2.462517 Loss 19.069101, Accuracy 0.075%\n",
      "Epoch 14, Batch 198, LR 2.462484 Loss 19.067831, Accuracy 0.075%\n",
      "Epoch 14, Batch 199, LR 2.462452 Loss 19.067065, Accuracy 0.075%\n",
      "Epoch 14, Batch 200, LR 2.462419 Loss 19.067586, Accuracy 0.074%\n",
      "Epoch 14, Batch 201, LR 2.462387 Loss 19.067053, Accuracy 0.074%\n",
      "Epoch 14, Batch 202, LR 2.462354 Loss 19.066956, Accuracy 0.077%\n",
      "Epoch 14, Batch 203, LR 2.462321 Loss 19.066460, Accuracy 0.077%\n",
      "Epoch 14, Batch 204, LR 2.462289 Loss 19.067843, Accuracy 0.077%\n",
      "Epoch 14, Batch 205, LR 2.462256 Loss 19.068398, Accuracy 0.076%\n",
      "Epoch 14, Batch 206, LR 2.462223 Loss 19.069231, Accuracy 0.076%\n",
      "Epoch 14, Batch 207, LR 2.462191 Loss 19.069519, Accuracy 0.075%\n",
      "Epoch 14, Batch 208, LR 2.462158 Loss 19.069350, Accuracy 0.075%\n",
      "Epoch 14, Batch 209, LR 2.462125 Loss 19.069181, Accuracy 0.075%\n",
      "Epoch 14, Batch 210, LR 2.462092 Loss 19.069235, Accuracy 0.074%\n",
      "Epoch 14, Batch 211, LR 2.462060 Loss 19.069629, Accuracy 0.074%\n",
      "Epoch 14, Batch 212, LR 2.462027 Loss 19.069183, Accuracy 0.074%\n",
      "Epoch 14, Batch 213, LR 2.461994 Loss 19.069926, Accuracy 0.073%\n",
      "Epoch 14, Batch 214, LR 2.461961 Loss 19.069880, Accuracy 0.077%\n",
      "Epoch 14, Batch 215, LR 2.461929 Loss 19.070127, Accuracy 0.076%\n",
      "Epoch 14, Batch 216, LR 2.461896 Loss 19.070834, Accuracy 0.076%\n",
      "Epoch 14, Batch 217, LR 2.461863 Loss 19.070455, Accuracy 0.076%\n",
      "Epoch 14, Batch 218, LR 2.461830 Loss 19.070260, Accuracy 0.075%\n",
      "Epoch 14, Batch 219, LR 2.461797 Loss 19.069532, Accuracy 0.075%\n",
      "Epoch 14, Batch 220, LR 2.461764 Loss 19.068781, Accuracy 0.075%\n",
      "Epoch 14, Batch 221, LR 2.461731 Loss 19.068089, Accuracy 0.074%\n",
      "Epoch 14, Batch 222, LR 2.461699 Loss 19.068113, Accuracy 0.074%\n",
      "Epoch 14, Batch 223, LR 2.461666 Loss 19.067476, Accuracy 0.074%\n",
      "Epoch 14, Batch 224, LR 2.461633 Loss 19.067937, Accuracy 0.073%\n",
      "Epoch 14, Batch 225, LR 2.461600 Loss 19.068506, Accuracy 0.073%\n",
      "Epoch 14, Batch 226, LR 2.461567 Loss 19.067898, Accuracy 0.076%\n",
      "Epoch 14, Batch 227, LR 2.461534 Loss 19.068510, Accuracy 0.076%\n",
      "Epoch 14, Batch 228, LR 2.461501 Loss 19.069097, Accuracy 0.075%\n",
      "Epoch 14, Batch 229, LR 2.461468 Loss 19.068683, Accuracy 0.075%\n",
      "Epoch 14, Batch 230, LR 2.461435 Loss 19.068049, Accuracy 0.075%\n",
      "Epoch 14, Batch 231, LR 2.461402 Loss 19.067919, Accuracy 0.074%\n",
      "Epoch 14, Batch 232, LR 2.461369 Loss 19.068016, Accuracy 0.074%\n",
      "Epoch 14, Batch 233, LR 2.461336 Loss 19.067240, Accuracy 0.074%\n",
      "Epoch 14, Batch 234, LR 2.461303 Loss 19.067251, Accuracy 0.073%\n",
      "Epoch 14, Batch 235, LR 2.461270 Loss 19.067559, Accuracy 0.073%\n",
      "Epoch 14, Batch 236, LR 2.461237 Loss 19.067678, Accuracy 0.073%\n",
      "Epoch 14, Batch 237, LR 2.461203 Loss 19.067192, Accuracy 0.073%\n",
      "Epoch 14, Batch 238, LR 2.461170 Loss 19.067006, Accuracy 0.072%\n",
      "Epoch 14, Batch 239, LR 2.461137 Loss 19.066946, Accuracy 0.075%\n",
      "Epoch 14, Batch 240, LR 2.461104 Loss 19.067720, Accuracy 0.075%\n",
      "Epoch 14, Batch 241, LR 2.461071 Loss 19.067796, Accuracy 0.075%\n",
      "Epoch 14, Batch 242, LR 2.461038 Loss 19.067909, Accuracy 0.074%\n",
      "Epoch 14, Batch 243, LR 2.461005 Loss 19.067353, Accuracy 0.074%\n",
      "Epoch 14, Batch 244, LR 2.460971 Loss 19.067407, Accuracy 0.074%\n",
      "Epoch 14, Batch 245, LR 2.460938 Loss 19.067162, Accuracy 0.077%\n",
      "Epoch 14, Batch 246, LR 2.460905 Loss 19.067654, Accuracy 0.076%\n",
      "Epoch 14, Batch 247, LR 2.460872 Loss 19.067806, Accuracy 0.076%\n",
      "Epoch 14, Batch 248, LR 2.460838 Loss 19.067723, Accuracy 0.076%\n",
      "Epoch 14, Batch 249, LR 2.460805 Loss 19.067294, Accuracy 0.075%\n",
      "Epoch 14, Batch 250, LR 2.460772 Loss 19.067581, Accuracy 0.075%\n",
      "Epoch 14, Batch 251, LR 2.460738 Loss 19.067686, Accuracy 0.075%\n",
      "Epoch 14, Batch 252, LR 2.460705 Loss 19.067585, Accuracy 0.074%\n",
      "Epoch 14, Batch 253, LR 2.460672 Loss 19.067673, Accuracy 0.074%\n",
      "Epoch 14, Batch 254, LR 2.460638 Loss 19.068031, Accuracy 0.074%\n",
      "Epoch 14, Batch 255, LR 2.460605 Loss 19.068215, Accuracy 0.074%\n",
      "Epoch 14, Batch 256, LR 2.460572 Loss 19.068556, Accuracy 0.073%\n",
      "Epoch 14, Batch 257, LR 2.460538 Loss 19.068268, Accuracy 0.076%\n",
      "Epoch 14, Batch 258, LR 2.460505 Loss 19.067769, Accuracy 0.076%\n",
      "Epoch 14, Batch 259, LR 2.460472 Loss 19.067975, Accuracy 0.075%\n",
      "Epoch 14, Batch 260, LR 2.460438 Loss 19.067973, Accuracy 0.075%\n",
      "Epoch 14, Batch 261, LR 2.460405 Loss 19.067884, Accuracy 0.075%\n",
      "Epoch 14, Batch 262, LR 2.460371 Loss 19.067477, Accuracy 0.075%\n",
      "Epoch 14, Batch 263, LR 2.460338 Loss 19.067205, Accuracy 0.074%\n",
      "Epoch 14, Batch 264, LR 2.460304 Loss 19.067047, Accuracy 0.074%\n",
      "Epoch 14, Batch 265, LR 2.460271 Loss 19.066813, Accuracy 0.077%\n",
      "Epoch 14, Batch 266, LR 2.460237 Loss 19.066971, Accuracy 0.076%\n",
      "Epoch 14, Batch 267, LR 2.460204 Loss 19.067195, Accuracy 0.076%\n",
      "Epoch 14, Batch 268, LR 2.460170 Loss 19.067024, Accuracy 0.076%\n",
      "Epoch 14, Batch 269, LR 2.460137 Loss 19.067212, Accuracy 0.076%\n",
      "Epoch 14, Batch 270, LR 2.460103 Loss 19.066665, Accuracy 0.081%\n",
      "Epoch 14, Batch 271, LR 2.460070 Loss 19.066645, Accuracy 0.084%\n",
      "Epoch 14, Batch 272, LR 2.460036 Loss 19.066727, Accuracy 0.083%\n",
      "Epoch 14, Batch 273, LR 2.460002 Loss 19.066486, Accuracy 0.086%\n",
      "Epoch 14, Batch 274, LR 2.459969 Loss 19.066602, Accuracy 0.086%\n",
      "Epoch 14, Batch 275, LR 2.459935 Loss 19.066755, Accuracy 0.085%\n",
      "Epoch 14, Batch 276, LR 2.459901 Loss 19.066843, Accuracy 0.085%\n",
      "Epoch 14, Batch 277, LR 2.459868 Loss 19.066972, Accuracy 0.087%\n",
      "Epoch 14, Batch 278, LR 2.459834 Loss 19.066945, Accuracy 0.087%\n",
      "Epoch 14, Batch 279, LR 2.459800 Loss 19.067301, Accuracy 0.087%\n",
      "Epoch 14, Batch 280, LR 2.459767 Loss 19.067820, Accuracy 0.086%\n",
      "Epoch 14, Batch 281, LR 2.459733 Loss 19.067193, Accuracy 0.089%\n",
      "Epoch 14, Batch 282, LR 2.459699 Loss 19.066903, Accuracy 0.089%\n",
      "Epoch 14, Batch 283, LR 2.459665 Loss 19.066842, Accuracy 0.088%\n",
      "Epoch 14, Batch 284, LR 2.459632 Loss 19.066471, Accuracy 0.088%\n",
      "Epoch 14, Batch 285, LR 2.459598 Loss 19.066317, Accuracy 0.088%\n",
      "Epoch 14, Batch 286, LR 2.459564 Loss 19.066151, Accuracy 0.087%\n",
      "Epoch 14, Batch 287, LR 2.459530 Loss 19.065814, Accuracy 0.087%\n",
      "Epoch 14, Batch 288, LR 2.459497 Loss 19.065995, Accuracy 0.087%\n",
      "Epoch 14, Batch 289, LR 2.459463 Loss 19.066023, Accuracy 0.089%\n",
      "Epoch 14, Batch 290, LR 2.459429 Loss 19.066109, Accuracy 0.092%\n",
      "Epoch 14, Batch 291, LR 2.459395 Loss 19.066777, Accuracy 0.091%\n",
      "Epoch 14, Batch 292, LR 2.459361 Loss 19.066371, Accuracy 0.091%\n",
      "Epoch 14, Batch 293, LR 2.459327 Loss 19.067104, Accuracy 0.091%\n",
      "Epoch 14, Batch 294, LR 2.459293 Loss 19.066989, Accuracy 0.090%\n",
      "Epoch 14, Batch 295, LR 2.459259 Loss 19.066916, Accuracy 0.090%\n",
      "Epoch 14, Batch 296, LR 2.459226 Loss 19.066545, Accuracy 0.090%\n",
      "Epoch 14, Batch 297, LR 2.459192 Loss 19.066795, Accuracy 0.089%\n",
      "Epoch 14, Batch 298, LR 2.459158 Loss 19.066743, Accuracy 0.089%\n",
      "Epoch 14, Batch 299, LR 2.459124 Loss 19.066718, Accuracy 0.089%\n",
      "Epoch 14, Batch 300, LR 2.459090 Loss 19.067249, Accuracy 0.089%\n",
      "Epoch 14, Batch 301, LR 2.459056 Loss 19.066916, Accuracy 0.088%\n",
      "Epoch 14, Batch 302, LR 2.459022 Loss 19.066918, Accuracy 0.088%\n",
      "Epoch 14, Batch 303, LR 2.458988 Loss 19.067058, Accuracy 0.088%\n",
      "Epoch 14, Batch 304, LR 2.458954 Loss 19.066891, Accuracy 0.087%\n",
      "Epoch 14, Batch 305, LR 2.458920 Loss 19.067151, Accuracy 0.087%\n",
      "Epoch 14, Batch 306, LR 2.458885 Loss 19.067272, Accuracy 0.087%\n",
      "Epoch 14, Batch 307, LR 2.458851 Loss 19.067318, Accuracy 0.087%\n",
      "Epoch 14, Batch 308, LR 2.458817 Loss 19.067516, Accuracy 0.086%\n",
      "Epoch 14, Batch 309, LR 2.458783 Loss 19.067241, Accuracy 0.086%\n",
      "Epoch 14, Batch 310, LR 2.458749 Loss 19.067916, Accuracy 0.088%\n",
      "Epoch 14, Batch 311, LR 2.458715 Loss 19.067911, Accuracy 0.088%\n",
      "Epoch 14, Batch 312, LR 2.458681 Loss 19.068053, Accuracy 0.088%\n",
      "Epoch 14, Batch 313, LR 2.458647 Loss 19.068659, Accuracy 0.087%\n",
      "Epoch 14, Batch 314, LR 2.458612 Loss 19.069399, Accuracy 0.087%\n",
      "Epoch 14, Batch 315, LR 2.458578 Loss 19.069600, Accuracy 0.087%\n",
      "Epoch 14, Batch 316, LR 2.458544 Loss 19.069268, Accuracy 0.087%\n",
      "Epoch 14, Batch 317, LR 2.458510 Loss 19.069698, Accuracy 0.086%\n",
      "Epoch 14, Batch 318, LR 2.458476 Loss 19.069654, Accuracy 0.086%\n",
      "Epoch 14, Batch 319, LR 2.458441 Loss 19.070220, Accuracy 0.086%\n",
      "Epoch 14, Batch 320, LR 2.458407 Loss 19.070314, Accuracy 0.085%\n",
      "Epoch 14, Batch 321, LR 2.458373 Loss 19.070688, Accuracy 0.085%\n",
      "Epoch 14, Batch 322, LR 2.458339 Loss 19.071017, Accuracy 0.085%\n",
      "Epoch 14, Batch 323, LR 2.458304 Loss 19.071298, Accuracy 0.085%\n",
      "Epoch 14, Batch 324, LR 2.458270 Loss 19.071207, Accuracy 0.084%\n",
      "Epoch 14, Batch 325, LR 2.458236 Loss 19.070488, Accuracy 0.084%\n",
      "Epoch 14, Batch 326, LR 2.458201 Loss 19.070300, Accuracy 0.084%\n",
      "Epoch 14, Batch 327, LR 2.458167 Loss 19.070156, Accuracy 0.084%\n",
      "Epoch 14, Batch 328, LR 2.458133 Loss 19.070400, Accuracy 0.083%\n",
      "Epoch 14, Batch 329, LR 2.458098 Loss 19.070952, Accuracy 0.083%\n",
      "Epoch 14, Batch 330, LR 2.458064 Loss 19.071256, Accuracy 0.083%\n",
      "Epoch 14, Batch 331, LR 2.458029 Loss 19.071263, Accuracy 0.083%\n",
      "Epoch 14, Batch 332, LR 2.457995 Loss 19.071761, Accuracy 0.082%\n",
      "Epoch 14, Batch 333, LR 2.457960 Loss 19.071260, Accuracy 0.082%\n",
      "Epoch 14, Batch 334, LR 2.457926 Loss 19.071561, Accuracy 0.082%\n",
      "Epoch 14, Batch 335, LR 2.457892 Loss 19.071616, Accuracy 0.082%\n",
      "Epoch 14, Batch 336, LR 2.457857 Loss 19.070732, Accuracy 0.081%\n",
      "Epoch 14, Batch 337, LR 2.457823 Loss 19.070513, Accuracy 0.081%\n",
      "Epoch 14, Batch 338, LR 2.457788 Loss 19.070864, Accuracy 0.081%\n",
      "Epoch 14, Batch 339, LR 2.457753 Loss 19.070359, Accuracy 0.081%\n",
      "Epoch 14, Batch 340, LR 2.457719 Loss 19.070038, Accuracy 0.080%\n",
      "Epoch 14, Batch 341, LR 2.457684 Loss 19.070492, Accuracy 0.080%\n",
      "Epoch 14, Batch 342, LR 2.457650 Loss 19.069902, Accuracy 0.082%\n",
      "Epoch 14, Batch 343, LR 2.457615 Loss 19.070076, Accuracy 0.082%\n",
      "Epoch 14, Batch 344, LR 2.457581 Loss 19.069934, Accuracy 0.082%\n",
      "Epoch 14, Batch 345, LR 2.457546 Loss 19.069574, Accuracy 0.082%\n",
      "Epoch 14, Batch 346, LR 2.457511 Loss 19.069388, Accuracy 0.081%\n",
      "Epoch 14, Batch 347, LR 2.457477 Loss 19.069172, Accuracy 0.081%\n",
      "Epoch 14, Batch 348, LR 2.457442 Loss 19.069693, Accuracy 0.083%\n",
      "Epoch 14, Batch 349, LR 2.457407 Loss 19.069262, Accuracy 0.083%\n",
      "Epoch 14, Batch 350, LR 2.457373 Loss 19.069350, Accuracy 0.083%\n",
      "Epoch 14, Batch 351, LR 2.457338 Loss 19.069522, Accuracy 0.082%\n",
      "Epoch 14, Batch 352, LR 2.457303 Loss 19.069078, Accuracy 0.082%\n",
      "Epoch 14, Batch 353, LR 2.457269 Loss 19.068536, Accuracy 0.084%\n",
      "Epoch 14, Batch 354, LR 2.457234 Loss 19.068086, Accuracy 0.084%\n",
      "Epoch 14, Batch 355, LR 2.457199 Loss 19.068116, Accuracy 0.084%\n",
      "Epoch 14, Batch 356, LR 2.457164 Loss 19.067772, Accuracy 0.083%\n",
      "Epoch 14, Batch 357, LR 2.457130 Loss 19.067315, Accuracy 0.083%\n",
      "Epoch 14, Batch 358, LR 2.457095 Loss 19.067293, Accuracy 0.083%\n",
      "Epoch 14, Batch 359, LR 2.457060 Loss 19.067764, Accuracy 0.083%\n",
      "Epoch 14, Batch 360, LR 2.457025 Loss 19.067941, Accuracy 0.082%\n",
      "Epoch 14, Batch 361, LR 2.456990 Loss 19.067762, Accuracy 0.082%\n",
      "Epoch 14, Batch 362, LR 2.456956 Loss 19.067847, Accuracy 0.082%\n",
      "Epoch 14, Batch 363, LR 2.456921 Loss 19.067760, Accuracy 0.082%\n",
      "Epoch 14, Batch 364, LR 2.456886 Loss 19.067071, Accuracy 0.082%\n",
      "Epoch 14, Batch 365, LR 2.456851 Loss 19.067025, Accuracy 0.081%\n",
      "Epoch 14, Batch 366, LR 2.456816 Loss 19.067290, Accuracy 0.081%\n",
      "Epoch 14, Batch 367, LR 2.456781 Loss 19.067234, Accuracy 0.081%\n",
      "Epoch 14, Batch 368, LR 2.456746 Loss 19.067407, Accuracy 0.083%\n",
      "Epoch 14, Batch 369, LR 2.456711 Loss 19.066997, Accuracy 0.085%\n",
      "Epoch 14, Batch 370, LR 2.456676 Loss 19.067122, Accuracy 0.084%\n",
      "Epoch 14, Batch 371, LR 2.456641 Loss 19.067099, Accuracy 0.084%\n",
      "Epoch 14, Batch 372, LR 2.456606 Loss 19.067919, Accuracy 0.084%\n",
      "Epoch 14, Batch 373, LR 2.456571 Loss 19.067836, Accuracy 0.084%\n",
      "Epoch 14, Batch 374, LR 2.456536 Loss 19.067660, Accuracy 0.084%\n",
      "Epoch 14, Batch 375, LR 2.456501 Loss 19.067333, Accuracy 0.083%\n",
      "Epoch 14, Batch 376, LR 2.456466 Loss 19.067172, Accuracy 0.083%\n",
      "Epoch 14, Batch 377, LR 2.456431 Loss 19.066574, Accuracy 0.083%\n",
      "Epoch 14, Batch 378, LR 2.456396 Loss 19.066926, Accuracy 0.083%\n",
      "Epoch 14, Batch 379, LR 2.456361 Loss 19.066897, Accuracy 0.082%\n",
      "Epoch 14, Batch 380, LR 2.456326 Loss 19.066718, Accuracy 0.082%\n",
      "Epoch 14, Batch 381, LR 2.456291 Loss 19.066785, Accuracy 0.082%\n",
      "Epoch 14, Batch 382, LR 2.456256 Loss 19.066649, Accuracy 0.082%\n",
      "Epoch 14, Batch 383, LR 2.456221 Loss 19.066617, Accuracy 0.082%\n",
      "Epoch 14, Batch 384, LR 2.456186 Loss 19.066790, Accuracy 0.081%\n",
      "Epoch 14, Batch 385, LR 2.456150 Loss 19.066857, Accuracy 0.081%\n",
      "Epoch 14, Batch 386, LR 2.456115 Loss 19.067081, Accuracy 0.081%\n",
      "Epoch 14, Batch 387, LR 2.456080 Loss 19.066974, Accuracy 0.081%\n",
      "Epoch 14, Batch 388, LR 2.456045 Loss 19.067312, Accuracy 0.081%\n",
      "Epoch 14, Batch 389, LR 2.456010 Loss 19.067600, Accuracy 0.080%\n",
      "Epoch 14, Batch 390, LR 2.455974 Loss 19.067549, Accuracy 0.080%\n",
      "Epoch 14, Batch 391, LR 2.455939 Loss 19.067628, Accuracy 0.080%\n",
      "Epoch 14, Batch 392, LR 2.455904 Loss 19.067600, Accuracy 0.080%\n",
      "Epoch 14, Batch 393, LR 2.455869 Loss 19.067838, Accuracy 0.080%\n",
      "Epoch 14, Batch 394, LR 2.455833 Loss 19.067203, Accuracy 0.079%\n",
      "Epoch 14, Batch 395, LR 2.455798 Loss 19.067440, Accuracy 0.079%\n",
      "Epoch 14, Batch 396, LR 2.455763 Loss 19.067412, Accuracy 0.079%\n",
      "Epoch 14, Batch 397, LR 2.455727 Loss 19.067081, Accuracy 0.079%\n",
      "Epoch 14, Batch 398, LR 2.455692 Loss 19.067441, Accuracy 0.079%\n",
      "Epoch 14, Batch 399, LR 2.455657 Loss 19.067528, Accuracy 0.078%\n",
      "Epoch 14, Batch 400, LR 2.455621 Loss 19.067720, Accuracy 0.078%\n",
      "Epoch 14, Batch 401, LR 2.455586 Loss 19.067890, Accuracy 0.080%\n",
      "Epoch 14, Batch 402, LR 2.455551 Loss 19.067757, Accuracy 0.080%\n",
      "Epoch 14, Batch 403, LR 2.455515 Loss 19.067761, Accuracy 0.079%\n",
      "Epoch 14, Batch 404, LR 2.455480 Loss 19.068001, Accuracy 0.079%\n",
      "Epoch 14, Batch 405, LR 2.455444 Loss 19.068024, Accuracy 0.079%\n",
      "Epoch 14, Batch 406, LR 2.455409 Loss 19.068103, Accuracy 0.079%\n",
      "Epoch 14, Batch 407, LR 2.455373 Loss 19.068178, Accuracy 0.079%\n",
      "Epoch 14, Batch 408, LR 2.455338 Loss 19.068415, Accuracy 0.079%\n",
      "Epoch 14, Batch 409, LR 2.455302 Loss 19.069061, Accuracy 0.078%\n",
      "Epoch 14, Batch 410, LR 2.455267 Loss 19.069006, Accuracy 0.078%\n",
      "Epoch 14, Batch 411, LR 2.455231 Loss 19.068876, Accuracy 0.078%\n",
      "Epoch 14, Batch 412, LR 2.455196 Loss 19.068520, Accuracy 0.080%\n",
      "Epoch 14, Batch 413, LR 2.455160 Loss 19.068379, Accuracy 0.079%\n",
      "Epoch 14, Batch 414, LR 2.455125 Loss 19.068472, Accuracy 0.079%\n",
      "Epoch 14, Batch 415, LR 2.455089 Loss 19.068466, Accuracy 0.079%\n",
      "Epoch 14, Batch 416, LR 2.455054 Loss 19.068418, Accuracy 0.079%\n",
      "Epoch 14, Batch 417, LR 2.455018 Loss 19.067745, Accuracy 0.079%\n",
      "Epoch 14, Batch 418, LR 2.454982 Loss 19.067075, Accuracy 0.080%\n",
      "Epoch 14, Batch 419, LR 2.454947 Loss 19.066701, Accuracy 0.080%\n",
      "Epoch 14, Batch 420, LR 2.454911 Loss 19.066406, Accuracy 0.080%\n",
      "Epoch 14, Batch 421, LR 2.454875 Loss 19.066224, Accuracy 0.080%\n",
      "Epoch 14, Batch 422, LR 2.454840 Loss 19.066452, Accuracy 0.080%\n",
      "Epoch 14, Batch 423, LR 2.454804 Loss 19.066443, Accuracy 0.079%\n",
      "Epoch 14, Batch 424, LR 2.454768 Loss 19.066020, Accuracy 0.081%\n",
      "Epoch 14, Batch 425, LR 2.454733 Loss 19.066225, Accuracy 0.081%\n",
      "Epoch 14, Batch 426, LR 2.454697 Loss 19.066268, Accuracy 0.083%\n",
      "Epoch 14, Batch 427, LR 2.454661 Loss 19.066045, Accuracy 0.082%\n",
      "Epoch 14, Batch 428, LR 2.454625 Loss 19.066649, Accuracy 0.082%\n",
      "Epoch 14, Batch 429, LR 2.454590 Loss 19.066653, Accuracy 0.082%\n",
      "Epoch 14, Batch 430, LR 2.454554 Loss 19.067058, Accuracy 0.082%\n",
      "Epoch 14, Batch 431, LR 2.454518 Loss 19.066945, Accuracy 0.082%\n",
      "Epoch 14, Batch 432, LR 2.454482 Loss 19.067178, Accuracy 0.081%\n",
      "Epoch 14, Batch 433, LR 2.454446 Loss 19.067044, Accuracy 0.081%\n",
      "Epoch 14, Batch 434, LR 2.454411 Loss 19.066995, Accuracy 0.081%\n",
      "Epoch 14, Batch 435, LR 2.454375 Loss 19.067402, Accuracy 0.084%\n",
      "Epoch 14, Batch 436, LR 2.454339 Loss 19.067672, Accuracy 0.084%\n",
      "Epoch 14, Batch 437, LR 2.454303 Loss 19.067051, Accuracy 0.084%\n",
      "Epoch 14, Batch 438, LR 2.454267 Loss 19.067020, Accuracy 0.084%\n",
      "Epoch 14, Batch 439, LR 2.454231 Loss 19.066938, Accuracy 0.084%\n",
      "Epoch 14, Batch 440, LR 2.454195 Loss 19.066847, Accuracy 0.083%\n",
      "Epoch 14, Batch 441, LR 2.454159 Loss 19.067177, Accuracy 0.083%\n",
      "Epoch 14, Batch 442, LR 2.454123 Loss 19.067110, Accuracy 0.083%\n",
      "Epoch 14, Batch 443, LR 2.454087 Loss 19.067221, Accuracy 0.083%\n",
      "Epoch 14, Batch 444, LR 2.454051 Loss 19.067505, Accuracy 0.084%\n",
      "Epoch 14, Batch 445, LR 2.454015 Loss 19.067628, Accuracy 0.084%\n",
      "Epoch 14, Batch 446, LR 2.453979 Loss 19.067606, Accuracy 0.084%\n",
      "Epoch 14, Batch 447, LR 2.453943 Loss 19.067491, Accuracy 0.084%\n",
      "Epoch 14, Batch 448, LR 2.453907 Loss 19.067495, Accuracy 0.084%\n",
      "Epoch 14, Batch 449, LR 2.453871 Loss 19.067808, Accuracy 0.084%\n",
      "Epoch 14, Batch 450, LR 2.453835 Loss 19.068100, Accuracy 0.083%\n",
      "Epoch 14, Batch 451, LR 2.453799 Loss 19.068260, Accuracy 0.083%\n",
      "Epoch 14, Batch 452, LR 2.453763 Loss 19.068091, Accuracy 0.083%\n",
      "Epoch 14, Batch 453, LR 2.453727 Loss 19.068271, Accuracy 0.083%\n",
      "Epoch 14, Batch 454, LR 2.453691 Loss 19.067826, Accuracy 0.083%\n",
      "Epoch 14, Batch 455, LR 2.453655 Loss 19.067921, Accuracy 0.082%\n",
      "Epoch 14, Batch 456, LR 2.453619 Loss 19.067615, Accuracy 0.082%\n",
      "Epoch 14, Batch 457, LR 2.453582 Loss 19.067771, Accuracy 0.082%\n",
      "Epoch 14, Batch 458, LR 2.453546 Loss 19.067504, Accuracy 0.084%\n",
      "Epoch 14, Batch 459, LR 2.453510 Loss 19.067906, Accuracy 0.085%\n",
      "Epoch 14, Batch 460, LR 2.453474 Loss 19.067597, Accuracy 0.087%\n",
      "Epoch 14, Batch 461, LR 2.453438 Loss 19.067388, Accuracy 0.086%\n",
      "Epoch 14, Batch 462, LR 2.453401 Loss 19.066953, Accuracy 0.086%\n",
      "Epoch 14, Batch 463, LR 2.453365 Loss 19.066584, Accuracy 0.086%\n",
      "Epoch 14, Batch 464, LR 2.453329 Loss 19.066141, Accuracy 0.088%\n",
      "Epoch 14, Batch 465, LR 2.453293 Loss 19.066296, Accuracy 0.087%\n",
      "Epoch 14, Batch 466, LR 2.453256 Loss 19.065892, Accuracy 0.087%\n",
      "Epoch 14, Batch 467, LR 2.453220 Loss 19.066225, Accuracy 0.089%\n",
      "Epoch 14, Batch 468, LR 2.453184 Loss 19.065907, Accuracy 0.088%\n",
      "Epoch 14, Batch 469, LR 2.453147 Loss 19.066105, Accuracy 0.088%\n",
      "Epoch 14, Batch 470, LR 2.453111 Loss 19.065800, Accuracy 0.088%\n",
      "Epoch 14, Batch 471, LR 2.453075 Loss 19.065859, Accuracy 0.088%\n",
      "Epoch 14, Batch 472, LR 2.453038 Loss 19.065683, Accuracy 0.088%\n",
      "Epoch 14, Batch 473, LR 2.453002 Loss 19.065698, Accuracy 0.088%\n",
      "Epoch 14, Batch 474, LR 2.452966 Loss 19.065432, Accuracy 0.087%\n",
      "Epoch 14, Batch 475, LR 2.452929 Loss 19.065700, Accuracy 0.087%\n",
      "Epoch 14, Batch 476, LR 2.452893 Loss 19.065318, Accuracy 0.087%\n",
      "Epoch 14, Batch 477, LR 2.452856 Loss 19.064950, Accuracy 0.087%\n",
      "Epoch 14, Batch 478, LR 2.452820 Loss 19.065208, Accuracy 0.087%\n",
      "Epoch 14, Batch 479, LR 2.452783 Loss 19.065398, Accuracy 0.088%\n",
      "Epoch 14, Batch 480, LR 2.452747 Loss 19.065839, Accuracy 0.088%\n",
      "Epoch 14, Batch 481, LR 2.452710 Loss 19.065392, Accuracy 0.088%\n",
      "Epoch 14, Batch 482, LR 2.452674 Loss 19.065559, Accuracy 0.088%\n",
      "Epoch 14, Batch 483, LR 2.452637 Loss 19.065537, Accuracy 0.087%\n",
      "Epoch 14, Batch 484, LR 2.452601 Loss 19.065322, Accuracy 0.087%\n",
      "Epoch 14, Batch 485, LR 2.452564 Loss 19.065122, Accuracy 0.087%\n",
      "Epoch 14, Batch 486, LR 2.452528 Loss 19.065159, Accuracy 0.087%\n",
      "Epoch 14, Batch 487, LR 2.452491 Loss 19.064852, Accuracy 0.087%\n",
      "Epoch 14, Batch 488, LR 2.452455 Loss 19.064849, Accuracy 0.086%\n",
      "Epoch 14, Batch 489, LR 2.452418 Loss 19.064733, Accuracy 0.086%\n",
      "Epoch 14, Batch 490, LR 2.452381 Loss 19.064820, Accuracy 0.086%\n",
      "Epoch 14, Batch 491, LR 2.452345 Loss 19.064883, Accuracy 0.086%\n",
      "Epoch 14, Batch 492, LR 2.452308 Loss 19.064981, Accuracy 0.086%\n",
      "Epoch 14, Batch 493, LR 2.452272 Loss 19.065762, Accuracy 0.086%\n",
      "Epoch 14, Batch 494, LR 2.452235 Loss 19.065962, Accuracy 0.085%\n",
      "Epoch 14, Batch 495, LR 2.452198 Loss 19.065655, Accuracy 0.085%\n",
      "Epoch 14, Batch 496, LR 2.452161 Loss 19.065906, Accuracy 0.085%\n",
      "Epoch 14, Batch 497, LR 2.452125 Loss 19.066076, Accuracy 0.085%\n",
      "Epoch 14, Batch 498, LR 2.452088 Loss 19.066111, Accuracy 0.085%\n",
      "Epoch 14, Batch 499, LR 2.452051 Loss 19.065920, Accuracy 0.085%\n",
      "Epoch 14, Batch 500, LR 2.452015 Loss 19.065974, Accuracy 0.084%\n",
      "Epoch 14, Batch 501, LR 2.451978 Loss 19.065891, Accuracy 0.084%\n",
      "Epoch 14, Batch 502, LR 2.451941 Loss 19.066007, Accuracy 0.084%\n",
      "Epoch 14, Batch 503, LR 2.451904 Loss 19.065999, Accuracy 0.085%\n",
      "Epoch 14, Batch 504, LR 2.451867 Loss 19.065795, Accuracy 0.085%\n",
      "Epoch 14, Batch 505, LR 2.451831 Loss 19.065679, Accuracy 0.085%\n",
      "Epoch 14, Batch 506, LR 2.451794 Loss 19.066017, Accuracy 0.085%\n",
      "Epoch 14, Batch 507, LR 2.451757 Loss 19.065687, Accuracy 0.086%\n",
      "Epoch 14, Batch 508, LR 2.451720 Loss 19.065398, Accuracy 0.089%\n",
      "Epoch 14, Batch 509, LR 2.451683 Loss 19.065335, Accuracy 0.091%\n",
      "Epoch 14, Batch 510, LR 2.451646 Loss 19.065399, Accuracy 0.092%\n",
      "Epoch 14, Batch 511, LR 2.451609 Loss 19.065136, Accuracy 0.092%\n",
      "Epoch 14, Batch 512, LR 2.451572 Loss 19.065260, Accuracy 0.092%\n",
      "Epoch 14, Batch 513, LR 2.451536 Loss 19.065479, Accuracy 0.091%\n",
      "Epoch 14, Batch 514, LR 2.451499 Loss 19.065231, Accuracy 0.091%\n",
      "Epoch 14, Batch 515, LR 2.451462 Loss 19.065600, Accuracy 0.091%\n",
      "Epoch 14, Batch 516, LR 2.451425 Loss 19.065343, Accuracy 0.091%\n",
      "Epoch 14, Batch 517, LR 2.451388 Loss 19.065055, Accuracy 0.091%\n",
      "Epoch 14, Batch 518, LR 2.451351 Loss 19.065123, Accuracy 0.090%\n",
      "Epoch 14, Batch 519, LR 2.451314 Loss 19.064857, Accuracy 0.092%\n",
      "Epoch 14, Batch 520, LR 2.451277 Loss 19.064966, Accuracy 0.092%\n",
      "Epoch 14, Batch 521, LR 2.451240 Loss 19.064943, Accuracy 0.091%\n",
      "Epoch 14, Batch 522, LR 2.451203 Loss 19.064856, Accuracy 0.091%\n",
      "Epoch 14, Batch 523, LR 2.451165 Loss 19.065009, Accuracy 0.091%\n",
      "Epoch 14, Batch 524, LR 2.451128 Loss 19.065511, Accuracy 0.091%\n",
      "Epoch 14, Batch 525, LR 2.451091 Loss 19.065221, Accuracy 0.091%\n",
      "Epoch 14, Batch 526, LR 2.451054 Loss 19.065070, Accuracy 0.091%\n",
      "Epoch 14, Batch 527, LR 2.451017 Loss 19.065157, Accuracy 0.092%\n",
      "Epoch 14, Batch 528, LR 2.450980 Loss 19.065284, Accuracy 0.092%\n",
      "Epoch 14, Batch 529, LR 2.450943 Loss 19.065107, Accuracy 0.092%\n",
      "Epoch 14, Batch 530, LR 2.450906 Loss 19.065055, Accuracy 0.091%\n",
      "Epoch 14, Batch 531, LR 2.450868 Loss 19.064898, Accuracy 0.091%\n",
      "Epoch 14, Batch 532, LR 2.450831 Loss 19.065031, Accuracy 0.091%\n",
      "Epoch 14, Batch 533, LR 2.450794 Loss 19.065419, Accuracy 0.091%\n",
      "Epoch 14, Batch 534, LR 2.450757 Loss 19.065518, Accuracy 0.092%\n",
      "Epoch 14, Batch 535, LR 2.450720 Loss 19.065465, Accuracy 0.092%\n",
      "Epoch 14, Batch 536, LR 2.450682 Loss 19.065248, Accuracy 0.092%\n",
      "Epoch 14, Batch 537, LR 2.450645 Loss 19.065284, Accuracy 0.092%\n",
      "Epoch 14, Batch 538, LR 2.450608 Loss 19.065278, Accuracy 0.093%\n",
      "Epoch 14, Batch 539, LR 2.450570 Loss 19.065108, Accuracy 0.093%\n",
      "Epoch 14, Batch 540, LR 2.450533 Loss 19.065275, Accuracy 0.093%\n",
      "Epoch 14, Batch 541, LR 2.450496 Loss 19.065383, Accuracy 0.092%\n",
      "Epoch 14, Batch 542, LR 2.450459 Loss 19.065222, Accuracy 0.092%\n",
      "Epoch 14, Batch 543, LR 2.450421 Loss 19.065131, Accuracy 0.092%\n",
      "Epoch 14, Batch 544, LR 2.450384 Loss 19.064990, Accuracy 0.092%\n",
      "Epoch 14, Batch 545, LR 2.450346 Loss 19.065214, Accuracy 0.092%\n",
      "Epoch 14, Batch 546, LR 2.450309 Loss 19.065552, Accuracy 0.092%\n",
      "Epoch 14, Batch 547, LR 2.450272 Loss 19.065580, Accuracy 0.093%\n",
      "Epoch 14, Batch 548, LR 2.450234 Loss 19.065511, Accuracy 0.093%\n",
      "Epoch 14, Batch 549, LR 2.450197 Loss 19.065442, Accuracy 0.092%\n",
      "Epoch 14, Batch 550, LR 2.450159 Loss 19.065554, Accuracy 0.092%\n",
      "Epoch 14, Batch 551, LR 2.450122 Loss 19.065450, Accuracy 0.092%\n",
      "Epoch 14, Batch 552, LR 2.450084 Loss 19.065939, Accuracy 0.092%\n",
      "Epoch 14, Batch 553, LR 2.450047 Loss 19.065701, Accuracy 0.092%\n",
      "Epoch 14, Batch 554, LR 2.450009 Loss 19.065576, Accuracy 0.092%\n",
      "Epoch 14, Batch 555, LR 2.449972 Loss 19.065494, Accuracy 0.091%\n",
      "Epoch 14, Batch 556, LR 2.449934 Loss 19.065345, Accuracy 0.091%\n",
      "Epoch 14, Batch 557, LR 2.449897 Loss 19.065358, Accuracy 0.091%\n",
      "Epoch 14, Batch 558, LR 2.449859 Loss 19.065198, Accuracy 0.092%\n",
      "Epoch 14, Batch 559, LR 2.449822 Loss 19.065386, Accuracy 0.092%\n",
      "Epoch 14, Batch 560, LR 2.449784 Loss 19.065055, Accuracy 0.092%\n",
      "Epoch 14, Batch 561, LR 2.449747 Loss 19.065130, Accuracy 0.092%\n",
      "Epoch 14, Batch 562, LR 2.449709 Loss 19.065300, Accuracy 0.092%\n",
      "Epoch 14, Batch 563, LR 2.449671 Loss 19.065429, Accuracy 0.092%\n",
      "Epoch 14, Batch 564, LR 2.449634 Loss 19.065379, Accuracy 0.091%\n",
      "Epoch 14, Batch 565, LR 2.449596 Loss 19.064951, Accuracy 0.093%\n",
      "Epoch 14, Batch 566, LR 2.449558 Loss 19.065288, Accuracy 0.092%\n",
      "Epoch 14, Batch 567, LR 2.449521 Loss 19.065088, Accuracy 0.092%\n",
      "Epoch 14, Batch 568, LR 2.449483 Loss 19.065018, Accuracy 0.092%\n",
      "Epoch 14, Batch 569, LR 2.449445 Loss 19.064702, Accuracy 0.092%\n",
      "Epoch 14, Batch 570, LR 2.449408 Loss 19.064632, Accuracy 0.092%\n",
      "Epoch 14, Batch 571, LR 2.449370 Loss 19.064953, Accuracy 0.092%\n",
      "Epoch 14, Batch 572, LR 2.449332 Loss 19.064821, Accuracy 0.092%\n",
      "Epoch 14, Batch 573, LR 2.449294 Loss 19.064879, Accuracy 0.091%\n",
      "Epoch 14, Batch 574, LR 2.449257 Loss 19.064649, Accuracy 0.091%\n",
      "Epoch 14, Batch 575, LR 2.449219 Loss 19.064533, Accuracy 0.091%\n",
      "Epoch 14, Batch 576, LR 2.449181 Loss 19.064422, Accuracy 0.091%\n",
      "Epoch 14, Batch 577, LR 2.449143 Loss 19.064224, Accuracy 0.091%\n",
      "Epoch 14, Batch 578, LR 2.449105 Loss 19.064278, Accuracy 0.091%\n",
      "Epoch 14, Batch 579, LR 2.449068 Loss 19.064020, Accuracy 0.090%\n",
      "Epoch 14, Batch 580, LR 2.449030 Loss 19.064259, Accuracy 0.090%\n",
      "Epoch 14, Batch 581, LR 2.448992 Loss 19.064160, Accuracy 0.090%\n",
      "Epoch 14, Batch 582, LR 2.448954 Loss 19.064298, Accuracy 0.090%\n",
      "Epoch 14, Batch 583, LR 2.448916 Loss 19.064288, Accuracy 0.090%\n",
      "Epoch 14, Batch 584, LR 2.448878 Loss 19.064650, Accuracy 0.090%\n",
      "Epoch 14, Batch 585, LR 2.448840 Loss 19.064456, Accuracy 0.089%\n",
      "Epoch 14, Batch 586, LR 2.448802 Loss 19.064604, Accuracy 0.089%\n",
      "Epoch 14, Batch 587, LR 2.448764 Loss 19.064798, Accuracy 0.089%\n",
      "Epoch 14, Batch 588, LR 2.448726 Loss 19.064858, Accuracy 0.089%\n",
      "Epoch 14, Batch 589, LR 2.448688 Loss 19.064935, Accuracy 0.089%\n",
      "Epoch 14, Batch 590, LR 2.448650 Loss 19.064935, Accuracy 0.089%\n",
      "Epoch 14, Batch 591, LR 2.448612 Loss 19.065007, Accuracy 0.089%\n",
      "Epoch 14, Batch 592, LR 2.448574 Loss 19.065359, Accuracy 0.088%\n",
      "Epoch 14, Batch 593, LR 2.448536 Loss 19.065513, Accuracy 0.088%\n",
      "Epoch 14, Batch 594, LR 2.448498 Loss 19.065852, Accuracy 0.088%\n",
      "Epoch 14, Batch 595, LR 2.448460 Loss 19.065952, Accuracy 0.088%\n",
      "Epoch 14, Batch 596, LR 2.448422 Loss 19.065872, Accuracy 0.088%\n",
      "Epoch 14, Batch 597, LR 2.448384 Loss 19.065876, Accuracy 0.088%\n",
      "Epoch 14, Batch 598, LR 2.448346 Loss 19.065924, Accuracy 0.088%\n",
      "Epoch 14, Batch 599, LR 2.448308 Loss 19.065878, Accuracy 0.087%\n",
      "Epoch 14, Batch 600, LR 2.448270 Loss 19.065750, Accuracy 0.087%\n",
      "Epoch 14, Batch 601, LR 2.448232 Loss 19.065776, Accuracy 0.087%\n",
      "Epoch 14, Batch 602, LR 2.448193 Loss 19.065712, Accuracy 0.087%\n",
      "Epoch 14, Batch 603, LR 2.448155 Loss 19.065689, Accuracy 0.087%\n",
      "Epoch 14, Batch 604, LR 2.448117 Loss 19.065818, Accuracy 0.087%\n",
      "Epoch 14, Batch 605, LR 2.448079 Loss 19.065928, Accuracy 0.088%\n",
      "Epoch 14, Batch 606, LR 2.448041 Loss 19.065877, Accuracy 0.088%\n",
      "Epoch 14, Batch 607, LR 2.448002 Loss 19.066254, Accuracy 0.088%\n",
      "Epoch 14, Batch 608, LR 2.447964 Loss 19.066304, Accuracy 0.087%\n",
      "Epoch 14, Batch 609, LR 2.447926 Loss 19.066321, Accuracy 0.087%\n",
      "Epoch 14, Batch 610, LR 2.447888 Loss 19.066425, Accuracy 0.087%\n",
      "Epoch 14, Batch 611, LR 2.447849 Loss 19.066700, Accuracy 0.087%\n",
      "Epoch 14, Batch 612, LR 2.447811 Loss 19.066581, Accuracy 0.087%\n",
      "Epoch 14, Batch 613, LR 2.447773 Loss 19.066546, Accuracy 0.087%\n",
      "Epoch 14, Batch 614, LR 2.447734 Loss 19.066382, Accuracy 0.087%\n",
      "Epoch 14, Batch 615, LR 2.447696 Loss 19.066033, Accuracy 0.086%\n",
      "Epoch 14, Batch 616, LR 2.447658 Loss 19.065952, Accuracy 0.086%\n",
      "Epoch 14, Batch 617, LR 2.447619 Loss 19.065985, Accuracy 0.086%\n",
      "Epoch 14, Batch 618, LR 2.447581 Loss 19.066298, Accuracy 0.086%\n",
      "Epoch 14, Batch 619, LR 2.447543 Loss 19.065995, Accuracy 0.086%\n",
      "Epoch 14, Batch 620, LR 2.447504 Loss 19.066280, Accuracy 0.086%\n",
      "Epoch 14, Batch 621, LR 2.447466 Loss 19.066002, Accuracy 0.086%\n",
      "Epoch 14, Batch 622, LR 2.447427 Loss 19.066030, Accuracy 0.085%\n",
      "Epoch 14, Batch 623, LR 2.447389 Loss 19.066111, Accuracy 0.085%\n",
      "Epoch 14, Batch 624, LR 2.447350 Loss 19.065918, Accuracy 0.085%\n",
      "Epoch 14, Batch 625, LR 2.447312 Loss 19.065752, Accuracy 0.085%\n",
      "Epoch 14, Batch 626, LR 2.447274 Loss 19.066190, Accuracy 0.085%\n",
      "Epoch 14, Batch 627, LR 2.447235 Loss 19.066429, Accuracy 0.085%\n",
      "Epoch 14, Batch 628, LR 2.447196 Loss 19.066521, Accuracy 0.085%\n",
      "Epoch 14, Batch 629, LR 2.447158 Loss 19.066140, Accuracy 0.084%\n",
      "Epoch 14, Batch 630, LR 2.447119 Loss 19.065638, Accuracy 0.084%\n",
      "Epoch 14, Batch 631, LR 2.447081 Loss 19.065777, Accuracy 0.084%\n",
      "Epoch 14, Batch 632, LR 2.447042 Loss 19.065718, Accuracy 0.084%\n",
      "Epoch 14, Batch 633, LR 2.447004 Loss 19.065405, Accuracy 0.084%\n",
      "Epoch 14, Batch 634, LR 2.446965 Loss 19.065822, Accuracy 0.084%\n",
      "Epoch 14, Batch 635, LR 2.446926 Loss 19.065744, Accuracy 0.085%\n",
      "Epoch 14, Batch 636, LR 2.446888 Loss 19.065756, Accuracy 0.085%\n",
      "Epoch 14, Batch 637, LR 2.446849 Loss 19.065703, Accuracy 0.085%\n",
      "Epoch 14, Batch 638, LR 2.446811 Loss 19.065918, Accuracy 0.084%\n",
      "Epoch 14, Batch 639, LR 2.446772 Loss 19.066076, Accuracy 0.084%\n",
      "Epoch 14, Batch 640, LR 2.446733 Loss 19.066051, Accuracy 0.084%\n",
      "Epoch 14, Batch 641, LR 2.446695 Loss 19.066208, Accuracy 0.084%\n",
      "Epoch 14, Batch 642, LR 2.446656 Loss 19.066266, Accuracy 0.084%\n",
      "Epoch 14, Batch 643, LR 2.446617 Loss 19.066254, Accuracy 0.084%\n",
      "Epoch 14, Batch 644, LR 2.446578 Loss 19.066347, Accuracy 0.084%\n",
      "Epoch 14, Batch 645, LR 2.446540 Loss 19.066597, Accuracy 0.084%\n",
      "Epoch 14, Batch 646, LR 2.446501 Loss 19.067033, Accuracy 0.083%\n",
      "Epoch 14, Batch 647, LR 2.446462 Loss 19.066872, Accuracy 0.083%\n",
      "Epoch 14, Batch 648, LR 2.446423 Loss 19.066848, Accuracy 0.083%\n",
      "Epoch 14, Batch 649, LR 2.446384 Loss 19.066958, Accuracy 0.084%\n",
      "Epoch 14, Batch 650, LR 2.446346 Loss 19.067355, Accuracy 0.084%\n",
      "Epoch 14, Batch 651, LR 2.446307 Loss 19.067678, Accuracy 0.084%\n",
      "Epoch 14, Batch 652, LR 2.446268 Loss 19.067714, Accuracy 0.084%\n",
      "Epoch 14, Batch 653, LR 2.446229 Loss 19.067853, Accuracy 0.084%\n",
      "Epoch 14, Batch 654, LR 2.446190 Loss 19.068194, Accuracy 0.084%\n",
      "Epoch 14, Batch 655, LR 2.446151 Loss 19.068153, Accuracy 0.083%\n",
      "Epoch 14, Batch 656, LR 2.446112 Loss 19.068442, Accuracy 0.083%\n",
      "Epoch 14, Batch 657, LR 2.446074 Loss 19.068398, Accuracy 0.083%\n",
      "Epoch 14, Batch 658, LR 2.446035 Loss 19.068268, Accuracy 0.083%\n",
      "Epoch 14, Batch 659, LR 2.445996 Loss 19.068344, Accuracy 0.083%\n",
      "Epoch 14, Batch 660, LR 2.445957 Loss 19.068425, Accuracy 0.083%\n",
      "Epoch 14, Batch 661, LR 2.445918 Loss 19.068157, Accuracy 0.083%\n",
      "Epoch 14, Batch 662, LR 2.445879 Loss 19.067931, Accuracy 0.083%\n",
      "Epoch 14, Batch 663, LR 2.445840 Loss 19.067761, Accuracy 0.082%\n",
      "Epoch 14, Batch 664, LR 2.445801 Loss 19.067754, Accuracy 0.082%\n",
      "Epoch 14, Batch 665, LR 2.445762 Loss 19.067974, Accuracy 0.082%\n",
      "Epoch 14, Batch 666, LR 2.445723 Loss 19.067862, Accuracy 0.082%\n",
      "Epoch 14, Batch 667, LR 2.445684 Loss 19.067952, Accuracy 0.082%\n",
      "Epoch 14, Batch 668, LR 2.445645 Loss 19.067914, Accuracy 0.082%\n",
      "Epoch 14, Batch 669, LR 2.445606 Loss 19.067836, Accuracy 0.082%\n",
      "Epoch 14, Batch 670, LR 2.445566 Loss 19.067887, Accuracy 0.082%\n",
      "Epoch 14, Batch 671, LR 2.445527 Loss 19.067963, Accuracy 0.082%\n",
      "Epoch 14, Batch 672, LR 2.445488 Loss 19.068235, Accuracy 0.081%\n",
      "Epoch 14, Batch 673, LR 2.445449 Loss 19.068210, Accuracy 0.081%\n",
      "Epoch 14, Batch 674, LR 2.445410 Loss 19.068500, Accuracy 0.081%\n",
      "Epoch 14, Batch 675, LR 2.445371 Loss 19.068275, Accuracy 0.081%\n",
      "Epoch 14, Batch 676, LR 2.445332 Loss 19.068211, Accuracy 0.081%\n",
      "Epoch 14, Batch 677, LR 2.445292 Loss 19.068233, Accuracy 0.082%\n",
      "Epoch 14, Batch 678, LR 2.445253 Loss 19.068429, Accuracy 0.082%\n",
      "Epoch 14, Batch 679, LR 2.445214 Loss 19.068265, Accuracy 0.082%\n",
      "Epoch 14, Batch 680, LR 2.445175 Loss 19.068173, Accuracy 0.082%\n",
      "Epoch 14, Batch 681, LR 2.445136 Loss 19.068343, Accuracy 0.081%\n",
      "Epoch 14, Batch 682, LR 2.445096 Loss 19.068307, Accuracy 0.081%\n",
      "Epoch 14, Batch 683, LR 2.445057 Loss 19.068182, Accuracy 0.081%\n",
      "Epoch 14, Batch 684, LR 2.445018 Loss 19.068327, Accuracy 0.081%\n",
      "Epoch 14, Batch 685, LR 2.444978 Loss 19.068315, Accuracy 0.081%\n",
      "Epoch 14, Batch 686, LR 2.444939 Loss 19.068569, Accuracy 0.081%\n",
      "Epoch 14, Batch 687, LR 2.444900 Loss 19.068545, Accuracy 0.081%\n",
      "Epoch 14, Batch 688, LR 2.444860 Loss 19.068497, Accuracy 0.081%\n",
      "Epoch 14, Batch 689, LR 2.444821 Loss 19.068322, Accuracy 0.081%\n",
      "Epoch 14, Batch 690, LR 2.444782 Loss 19.068249, Accuracy 0.080%\n",
      "Epoch 14, Batch 691, LR 2.444742 Loss 19.068221, Accuracy 0.080%\n",
      "Epoch 14, Batch 692, LR 2.444703 Loss 19.068153, Accuracy 0.080%\n",
      "Epoch 14, Batch 693, LR 2.444664 Loss 19.067879, Accuracy 0.080%\n",
      "Epoch 14, Batch 694, LR 2.444624 Loss 19.068024, Accuracy 0.081%\n",
      "Epoch 14, Batch 695, LR 2.444585 Loss 19.068170, Accuracy 0.082%\n",
      "Epoch 14, Batch 696, LR 2.444545 Loss 19.068049, Accuracy 0.082%\n",
      "Epoch 14, Batch 697, LR 2.444506 Loss 19.068142, Accuracy 0.082%\n",
      "Epoch 14, Batch 698, LR 2.444466 Loss 19.067933, Accuracy 0.082%\n",
      "Epoch 14, Batch 699, LR 2.444427 Loss 19.068387, Accuracy 0.082%\n",
      "Epoch 14, Batch 700, LR 2.444387 Loss 19.068390, Accuracy 0.081%\n",
      "Epoch 14, Batch 701, LR 2.444348 Loss 19.068842, Accuracy 0.081%\n",
      "Epoch 14, Batch 702, LR 2.444308 Loss 19.068473, Accuracy 0.081%\n",
      "Epoch 14, Batch 703, LR 2.444269 Loss 19.068553, Accuracy 0.081%\n",
      "Epoch 14, Batch 704, LR 2.444229 Loss 19.068548, Accuracy 0.081%\n",
      "Epoch 14, Batch 705, LR 2.444190 Loss 19.068366, Accuracy 0.081%\n",
      "Epoch 14, Batch 706, LR 2.444150 Loss 19.068414, Accuracy 0.081%\n",
      "Epoch 14, Batch 707, LR 2.444110 Loss 19.068397, Accuracy 0.081%\n",
      "Epoch 14, Batch 708, LR 2.444071 Loss 19.068218, Accuracy 0.081%\n",
      "Epoch 14, Batch 709, LR 2.444031 Loss 19.068141, Accuracy 0.080%\n",
      "Epoch 14, Batch 710, LR 2.443991 Loss 19.068228, Accuracy 0.080%\n",
      "Epoch 14, Batch 711, LR 2.443952 Loss 19.068394, Accuracy 0.080%\n",
      "Epoch 14, Batch 712, LR 2.443912 Loss 19.068482, Accuracy 0.080%\n",
      "Epoch 14, Batch 713, LR 2.443872 Loss 19.068339, Accuracy 0.080%\n",
      "Epoch 14, Batch 714, LR 2.443833 Loss 19.068355, Accuracy 0.081%\n",
      "Epoch 14, Batch 715, LR 2.443793 Loss 19.068111, Accuracy 0.081%\n",
      "Epoch 14, Batch 716, LR 2.443753 Loss 19.068002, Accuracy 0.081%\n",
      "Epoch 14, Batch 717, LR 2.443714 Loss 19.067834, Accuracy 0.082%\n",
      "Epoch 14, Batch 718, LR 2.443674 Loss 19.067849, Accuracy 0.082%\n",
      "Epoch 14, Batch 719, LR 2.443634 Loss 19.067817, Accuracy 0.081%\n",
      "Epoch 14, Batch 720, LR 2.443594 Loss 19.067617, Accuracy 0.081%\n",
      "Epoch 14, Batch 721, LR 2.443555 Loss 19.067563, Accuracy 0.081%\n",
      "Epoch 14, Batch 722, LR 2.443515 Loss 19.067628, Accuracy 0.081%\n",
      "Epoch 14, Batch 723, LR 2.443475 Loss 19.067922, Accuracy 0.081%\n",
      "Epoch 14, Batch 724, LR 2.443435 Loss 19.067975, Accuracy 0.081%\n",
      "Epoch 14, Batch 725, LR 2.443395 Loss 19.067959, Accuracy 0.082%\n",
      "Epoch 14, Batch 726, LR 2.443355 Loss 19.067959, Accuracy 0.082%\n",
      "Epoch 14, Batch 727, LR 2.443316 Loss 19.068217, Accuracy 0.082%\n",
      "Epoch 14, Batch 728, LR 2.443276 Loss 19.068176, Accuracy 0.082%\n",
      "Epoch 14, Batch 729, LR 2.443236 Loss 19.067737, Accuracy 0.081%\n",
      "Epoch 14, Batch 730, LR 2.443196 Loss 19.067452, Accuracy 0.081%\n",
      "Epoch 14, Batch 731, LR 2.443156 Loss 19.067248, Accuracy 0.081%\n",
      "Epoch 14, Batch 732, LR 2.443116 Loss 19.066982, Accuracy 0.081%\n",
      "Epoch 14, Batch 733, LR 2.443076 Loss 19.066815, Accuracy 0.081%\n",
      "Epoch 14, Batch 734, LR 2.443036 Loss 19.066953, Accuracy 0.081%\n",
      "Epoch 14, Batch 735, LR 2.442996 Loss 19.066963, Accuracy 0.082%\n",
      "Epoch 14, Batch 736, LR 2.442956 Loss 19.067108, Accuracy 0.083%\n",
      "Epoch 14, Batch 737, LR 2.442916 Loss 19.067048, Accuracy 0.083%\n",
      "Epoch 14, Batch 738, LR 2.442876 Loss 19.066802, Accuracy 0.084%\n",
      "Epoch 14, Batch 739, LR 2.442836 Loss 19.066596, Accuracy 0.084%\n",
      "Epoch 14, Batch 740, LR 2.442796 Loss 19.066737, Accuracy 0.083%\n",
      "Epoch 14, Batch 741, LR 2.442756 Loss 19.066617, Accuracy 0.083%\n",
      "Epoch 14, Batch 742, LR 2.442716 Loss 19.066507, Accuracy 0.084%\n",
      "Epoch 14, Batch 743, LR 2.442676 Loss 19.066493, Accuracy 0.084%\n",
      "Epoch 14, Batch 744, LR 2.442636 Loss 19.066672, Accuracy 0.084%\n",
      "Epoch 14, Batch 745, LR 2.442595 Loss 19.066528, Accuracy 0.084%\n",
      "Epoch 14, Batch 746, LR 2.442555 Loss 19.066343, Accuracy 0.085%\n",
      "Epoch 14, Batch 747, LR 2.442515 Loss 19.066626, Accuracy 0.085%\n",
      "Epoch 14, Batch 748, LR 2.442475 Loss 19.066705, Accuracy 0.085%\n",
      "Epoch 14, Batch 749, LR 2.442435 Loss 19.066721, Accuracy 0.084%\n",
      "Epoch 14, Batch 750, LR 2.442395 Loss 19.066750, Accuracy 0.084%\n",
      "Epoch 14, Batch 751, LR 2.442354 Loss 19.066889, Accuracy 0.085%\n",
      "Epoch 14, Batch 752, LR 2.442314 Loss 19.066826, Accuracy 0.085%\n",
      "Epoch 14, Batch 753, LR 2.442274 Loss 19.067005, Accuracy 0.085%\n",
      "Epoch 14, Batch 754, LR 2.442234 Loss 19.066931, Accuracy 0.085%\n",
      "Epoch 14, Batch 755, LR 2.442193 Loss 19.066970, Accuracy 0.085%\n",
      "Epoch 14, Batch 756, LR 2.442153 Loss 19.067245, Accuracy 0.085%\n",
      "Epoch 14, Batch 757, LR 2.442113 Loss 19.067433, Accuracy 0.085%\n",
      "Epoch 14, Batch 758, LR 2.442073 Loss 19.067561, Accuracy 0.085%\n",
      "Epoch 14, Batch 759, LR 2.442032 Loss 19.067664, Accuracy 0.084%\n",
      "Epoch 14, Batch 760, LR 2.441992 Loss 19.067847, Accuracy 0.084%\n",
      "Epoch 14, Batch 761, LR 2.441952 Loss 19.068059, Accuracy 0.084%\n",
      "Epoch 14, Batch 762, LR 2.441911 Loss 19.068217, Accuracy 0.084%\n",
      "Epoch 14, Batch 763, LR 2.441871 Loss 19.068323, Accuracy 0.084%\n",
      "Epoch 14, Batch 764, LR 2.441831 Loss 19.068393, Accuracy 0.084%\n",
      "Epoch 14, Batch 765, LR 2.441790 Loss 19.068346, Accuracy 0.084%\n",
      "Epoch 14, Batch 766, LR 2.441750 Loss 19.068340, Accuracy 0.084%\n",
      "Epoch 14, Batch 767, LR 2.441709 Loss 19.068214, Accuracy 0.084%\n",
      "Epoch 14, Batch 768, LR 2.441669 Loss 19.068053, Accuracy 0.084%\n",
      "Epoch 14, Batch 769, LR 2.441628 Loss 19.067918, Accuracy 0.084%\n",
      "Epoch 14, Batch 770, LR 2.441588 Loss 19.068130, Accuracy 0.084%\n",
      "Epoch 14, Batch 771, LR 2.441547 Loss 19.068244, Accuracy 0.084%\n",
      "Epoch 14, Batch 772, LR 2.441507 Loss 19.068212, Accuracy 0.084%\n",
      "Epoch 14, Batch 773, LR 2.441466 Loss 19.068288, Accuracy 0.084%\n",
      "Epoch 14, Batch 774, LR 2.441426 Loss 19.068491, Accuracy 0.084%\n",
      "Epoch 14, Batch 775, LR 2.441385 Loss 19.068423, Accuracy 0.084%\n",
      "Epoch 14, Batch 776, LR 2.441345 Loss 19.068459, Accuracy 0.084%\n",
      "Epoch 14, Batch 777, LR 2.441304 Loss 19.068278, Accuracy 0.084%\n",
      "Epoch 14, Batch 778, LR 2.441264 Loss 19.068287, Accuracy 0.084%\n",
      "Epoch 14, Batch 779, LR 2.441223 Loss 19.068049, Accuracy 0.084%\n",
      "Epoch 14, Batch 780, LR 2.441183 Loss 19.067996, Accuracy 0.084%\n",
      "Epoch 14, Batch 781, LR 2.441142 Loss 19.067836, Accuracy 0.084%\n",
      "Epoch 14, Batch 782, LR 2.441101 Loss 19.067765, Accuracy 0.084%\n",
      "Epoch 14, Batch 783, LR 2.441061 Loss 19.067467, Accuracy 0.085%\n",
      "Epoch 14, Batch 784, LR 2.441020 Loss 19.067617, Accuracy 0.085%\n",
      "Epoch 14, Batch 785, LR 2.440979 Loss 19.067545, Accuracy 0.086%\n",
      "Epoch 14, Batch 786, LR 2.440939 Loss 19.067513, Accuracy 0.085%\n",
      "Epoch 14, Batch 787, LR 2.440898 Loss 19.067532, Accuracy 0.085%\n",
      "Epoch 14, Batch 788, LR 2.440857 Loss 19.067523, Accuracy 0.086%\n",
      "Epoch 14, Batch 789, LR 2.440817 Loss 19.067508, Accuracy 0.087%\n",
      "Epoch 14, Batch 790, LR 2.440776 Loss 19.067687, Accuracy 0.087%\n",
      "Epoch 14, Batch 791, LR 2.440735 Loss 19.067557, Accuracy 0.087%\n",
      "Epoch 14, Batch 792, LR 2.440694 Loss 19.067222, Accuracy 0.087%\n",
      "Epoch 14, Batch 793, LR 2.440654 Loss 19.067181, Accuracy 0.087%\n",
      "Epoch 14, Batch 794, LR 2.440613 Loss 19.067102, Accuracy 0.087%\n",
      "Epoch 14, Batch 795, LR 2.440572 Loss 19.067076, Accuracy 0.086%\n",
      "Epoch 14, Batch 796, LR 2.440531 Loss 19.067022, Accuracy 0.086%\n",
      "Epoch 14, Batch 797, LR 2.440490 Loss 19.066917, Accuracy 0.086%\n",
      "Epoch 14, Batch 798, LR 2.440449 Loss 19.066876, Accuracy 0.086%\n",
      "Epoch 14, Batch 799, LR 2.440409 Loss 19.066977, Accuracy 0.086%\n",
      "Epoch 14, Batch 800, LR 2.440368 Loss 19.066878, Accuracy 0.086%\n",
      "Epoch 14, Batch 801, LR 2.440327 Loss 19.066847, Accuracy 0.086%\n",
      "Epoch 14, Batch 802, LR 2.440286 Loss 19.066757, Accuracy 0.086%\n",
      "Epoch 14, Batch 803, LR 2.440245 Loss 19.066411, Accuracy 0.086%\n",
      "Epoch 14, Batch 804, LR 2.440204 Loss 19.066193, Accuracy 0.086%\n",
      "Epoch 14, Batch 805, LR 2.440163 Loss 19.066104, Accuracy 0.086%\n",
      "Epoch 14, Batch 806, LR 2.440122 Loss 19.066098, Accuracy 0.087%\n",
      "Epoch 14, Batch 807, LR 2.440081 Loss 19.065998, Accuracy 0.087%\n",
      "Epoch 14, Batch 808, LR 2.440040 Loss 19.065739, Accuracy 0.087%\n",
      "Epoch 14, Batch 809, LR 2.439999 Loss 19.065850, Accuracy 0.087%\n",
      "Epoch 14, Batch 810, LR 2.439958 Loss 19.066172, Accuracy 0.087%\n",
      "Epoch 14, Batch 811, LR 2.439917 Loss 19.065994, Accuracy 0.087%\n",
      "Epoch 14, Batch 812, LR 2.439876 Loss 19.066090, Accuracy 0.087%\n",
      "Epoch 14, Batch 813, LR 2.439835 Loss 19.066185, Accuracy 0.086%\n",
      "Epoch 14, Batch 814, LR 2.439794 Loss 19.066274, Accuracy 0.086%\n",
      "Epoch 14, Batch 815, LR 2.439753 Loss 19.066408, Accuracy 0.086%\n",
      "Epoch 14, Batch 816, LR 2.439712 Loss 19.066377, Accuracy 0.086%\n",
      "Epoch 14, Batch 817, LR 2.439671 Loss 19.066298, Accuracy 0.086%\n",
      "Epoch 14, Batch 818, LR 2.439630 Loss 19.066238, Accuracy 0.086%\n",
      "Epoch 14, Batch 819, LR 2.439589 Loss 19.066217, Accuracy 0.086%\n",
      "Epoch 14, Batch 820, LR 2.439547 Loss 19.066458, Accuracy 0.086%\n",
      "Epoch 14, Batch 821, LR 2.439506 Loss 19.066514, Accuracy 0.086%\n",
      "Epoch 14, Batch 822, LR 2.439465 Loss 19.066515, Accuracy 0.086%\n",
      "Epoch 14, Batch 823, LR 2.439424 Loss 19.066586, Accuracy 0.085%\n",
      "Epoch 14, Batch 824, LR 2.439383 Loss 19.066780, Accuracy 0.085%\n",
      "Epoch 14, Batch 825, LR 2.439341 Loss 19.066974, Accuracy 0.085%\n",
      "Epoch 14, Batch 826, LR 2.439300 Loss 19.067125, Accuracy 0.085%\n",
      "Epoch 14, Batch 827, LR 2.439259 Loss 19.067126, Accuracy 0.085%\n",
      "Epoch 14, Batch 828, LR 2.439218 Loss 19.067166, Accuracy 0.086%\n",
      "Epoch 14, Batch 829, LR 2.439176 Loss 19.067374, Accuracy 0.086%\n",
      "Epoch 14, Batch 830, LR 2.439135 Loss 19.067670, Accuracy 0.086%\n",
      "Epoch 14, Batch 831, LR 2.439094 Loss 19.067475, Accuracy 0.086%\n",
      "Epoch 14, Batch 832, LR 2.439053 Loss 19.067415, Accuracy 0.085%\n",
      "Epoch 14, Batch 833, LR 2.439011 Loss 19.067510, Accuracy 0.085%\n",
      "Epoch 14, Batch 834, LR 2.438970 Loss 19.067534, Accuracy 0.085%\n",
      "Epoch 14, Batch 835, LR 2.438929 Loss 19.067796, Accuracy 0.085%\n",
      "Epoch 14, Batch 836, LR 2.438887 Loss 19.067857, Accuracy 0.085%\n",
      "Epoch 14, Batch 837, LR 2.438846 Loss 19.067673, Accuracy 0.085%\n",
      "Epoch 14, Batch 838, LR 2.438804 Loss 19.067625, Accuracy 0.085%\n",
      "Epoch 14, Batch 839, LR 2.438763 Loss 19.067547, Accuracy 0.086%\n",
      "Epoch 14, Batch 840, LR 2.438722 Loss 19.067564, Accuracy 0.086%\n",
      "Epoch 14, Batch 841, LR 2.438680 Loss 19.067650, Accuracy 0.085%\n",
      "Epoch 14, Batch 842, LR 2.438639 Loss 19.067776, Accuracy 0.085%\n",
      "Epoch 14, Batch 843, LR 2.438597 Loss 19.067916, Accuracy 0.085%\n",
      "Epoch 14, Batch 844, LR 2.438556 Loss 19.067954, Accuracy 0.085%\n",
      "Epoch 14, Batch 845, LR 2.438514 Loss 19.067760, Accuracy 0.085%\n",
      "Epoch 14, Batch 846, LR 2.438473 Loss 19.067433, Accuracy 0.085%\n",
      "Epoch 14, Batch 847, LR 2.438431 Loss 19.067503, Accuracy 0.085%\n",
      "Epoch 14, Batch 848, LR 2.438390 Loss 19.067640, Accuracy 0.085%\n",
      "Epoch 14, Batch 849, LR 2.438348 Loss 19.067539, Accuracy 0.085%\n",
      "Epoch 14, Batch 850, LR 2.438307 Loss 19.067341, Accuracy 0.085%\n",
      "Epoch 14, Batch 851, LR 2.438265 Loss 19.067184, Accuracy 0.084%\n",
      "Epoch 14, Batch 852, LR 2.438223 Loss 19.067398, Accuracy 0.084%\n",
      "Epoch 14, Batch 853, LR 2.438182 Loss 19.067167, Accuracy 0.084%\n",
      "Epoch 14, Batch 854, LR 2.438140 Loss 19.067265, Accuracy 0.084%\n",
      "Epoch 14, Batch 855, LR 2.438099 Loss 19.067094, Accuracy 0.084%\n",
      "Epoch 14, Batch 856, LR 2.438057 Loss 19.067218, Accuracy 0.085%\n",
      "Epoch 14, Batch 857, LR 2.438015 Loss 19.067328, Accuracy 0.085%\n",
      "Epoch 14, Batch 858, LR 2.437974 Loss 19.067133, Accuracy 0.085%\n",
      "Epoch 14, Batch 859, LR 2.437932 Loss 19.066923, Accuracy 0.085%\n",
      "Epoch 14, Batch 860, LR 2.437890 Loss 19.067059, Accuracy 0.084%\n",
      "Epoch 14, Batch 861, LR 2.437849 Loss 19.067067, Accuracy 0.084%\n",
      "Epoch 14, Batch 862, LR 2.437807 Loss 19.067386, Accuracy 0.084%\n",
      "Epoch 14, Batch 863, LR 2.437765 Loss 19.067442, Accuracy 0.085%\n",
      "Epoch 14, Batch 864, LR 2.437723 Loss 19.067533, Accuracy 0.085%\n",
      "Epoch 14, Batch 865, LR 2.437682 Loss 19.067732, Accuracy 0.085%\n",
      "Epoch 14, Batch 866, LR 2.437640 Loss 19.067778, Accuracy 0.086%\n",
      "Epoch 14, Batch 867, LR 2.437598 Loss 19.067561, Accuracy 0.086%\n",
      "Epoch 14, Batch 868, LR 2.437556 Loss 19.067458, Accuracy 0.086%\n",
      "Epoch 14, Batch 869, LR 2.437514 Loss 19.067421, Accuracy 0.085%\n",
      "Epoch 14, Batch 870, LR 2.437473 Loss 19.067553, Accuracy 0.085%\n",
      "Epoch 14, Batch 871, LR 2.437431 Loss 19.067483, Accuracy 0.085%\n",
      "Epoch 14, Batch 872, LR 2.437389 Loss 19.067410, Accuracy 0.086%\n",
      "Epoch 14, Batch 873, LR 2.437347 Loss 19.067249, Accuracy 0.086%\n",
      "Epoch 14, Batch 874, LR 2.437305 Loss 19.067071, Accuracy 0.086%\n",
      "Epoch 14, Batch 875, LR 2.437263 Loss 19.067132, Accuracy 0.086%\n",
      "Epoch 14, Batch 876, LR 2.437221 Loss 19.067034, Accuracy 0.086%\n",
      "Epoch 14, Batch 877, LR 2.437179 Loss 19.066830, Accuracy 0.086%\n",
      "Epoch 14, Batch 878, LR 2.437138 Loss 19.066889, Accuracy 0.085%\n",
      "Epoch 14, Batch 879, LR 2.437096 Loss 19.066902, Accuracy 0.085%\n",
      "Epoch 14, Batch 880, LR 2.437054 Loss 19.066929, Accuracy 0.085%\n",
      "Epoch 14, Batch 881, LR 2.437012 Loss 19.066931, Accuracy 0.085%\n",
      "Epoch 14, Batch 882, LR 2.436970 Loss 19.066868, Accuracy 0.085%\n",
      "Epoch 14, Batch 883, LR 2.436928 Loss 19.066926, Accuracy 0.085%\n",
      "Epoch 14, Batch 884, LR 2.436886 Loss 19.066665, Accuracy 0.086%\n",
      "Epoch 14, Batch 885, LR 2.436844 Loss 19.066700, Accuracy 0.087%\n",
      "Epoch 14, Batch 886, LR 2.436802 Loss 19.066513, Accuracy 0.086%\n",
      "Epoch 14, Batch 887, LR 2.436759 Loss 19.066481, Accuracy 0.086%\n",
      "Epoch 14, Batch 888, LR 2.436717 Loss 19.066327, Accuracy 0.086%\n",
      "Epoch 14, Batch 889, LR 2.436675 Loss 19.066256, Accuracy 0.086%\n",
      "Epoch 14, Batch 890, LR 2.436633 Loss 19.066424, Accuracy 0.086%\n",
      "Epoch 14, Batch 891, LR 2.436591 Loss 19.066395, Accuracy 0.086%\n",
      "Epoch 14, Batch 892, LR 2.436549 Loss 19.066239, Accuracy 0.086%\n",
      "Epoch 14, Batch 893, LR 2.436507 Loss 19.066293, Accuracy 0.086%\n",
      "Epoch 14, Batch 894, LR 2.436465 Loss 19.066428, Accuracy 0.086%\n",
      "Epoch 14, Batch 895, LR 2.436422 Loss 19.066351, Accuracy 0.086%\n",
      "Epoch 14, Batch 896, LR 2.436380 Loss 19.066388, Accuracy 0.085%\n",
      "Epoch 14, Batch 897, LR 2.436338 Loss 19.066278, Accuracy 0.086%\n",
      "Epoch 14, Batch 898, LR 2.436296 Loss 19.066168, Accuracy 0.087%\n",
      "Epoch 14, Batch 899, LR 2.436254 Loss 19.066252, Accuracy 0.087%\n",
      "Epoch 14, Batch 900, LR 2.436211 Loss 19.066208, Accuracy 0.087%\n",
      "Epoch 14, Batch 901, LR 2.436169 Loss 19.066481, Accuracy 0.087%\n",
      "Epoch 14, Batch 902, LR 2.436127 Loss 19.066615, Accuracy 0.087%\n",
      "Epoch 14, Batch 903, LR 2.436085 Loss 19.066535, Accuracy 0.087%\n",
      "Epoch 14, Batch 904, LR 2.436042 Loss 19.066541, Accuracy 0.088%\n",
      "Epoch 14, Batch 905, LR 2.436000 Loss 19.066387, Accuracy 0.088%\n",
      "Epoch 14, Batch 906, LR 2.435958 Loss 19.066336, Accuracy 0.088%\n",
      "Epoch 14, Batch 907, LR 2.435915 Loss 19.066328, Accuracy 0.088%\n",
      "Epoch 14, Batch 908, LR 2.435873 Loss 19.066385, Accuracy 0.088%\n",
      "Epoch 14, Batch 909, LR 2.435831 Loss 19.066412, Accuracy 0.088%\n",
      "Epoch 14, Batch 910, LR 2.435788 Loss 19.066391, Accuracy 0.088%\n",
      "Epoch 14, Batch 911, LR 2.435746 Loss 19.066588, Accuracy 0.087%\n",
      "Epoch 14, Batch 912, LR 2.435703 Loss 19.066817, Accuracy 0.087%\n",
      "Epoch 14, Batch 913, LR 2.435661 Loss 19.066829, Accuracy 0.087%\n",
      "Epoch 14, Batch 914, LR 2.435619 Loss 19.066699, Accuracy 0.087%\n",
      "Epoch 14, Batch 915, LR 2.435576 Loss 19.066476, Accuracy 0.087%\n",
      "Epoch 14, Batch 916, LR 2.435534 Loss 19.066844, Accuracy 0.087%\n",
      "Epoch 14, Batch 917, LR 2.435491 Loss 19.066796, Accuracy 0.087%\n",
      "Epoch 14, Batch 918, LR 2.435449 Loss 19.066813, Accuracy 0.087%\n",
      "Epoch 14, Batch 919, LR 2.435406 Loss 19.066870, Accuracy 0.087%\n",
      "Epoch 14, Batch 920, LR 2.435364 Loss 19.066670, Accuracy 0.087%\n",
      "Epoch 14, Batch 921, LR 2.435321 Loss 19.066953, Accuracy 0.087%\n",
      "Epoch 14, Batch 922, LR 2.435279 Loss 19.067076, Accuracy 0.086%\n",
      "Epoch 14, Batch 923, LR 2.435236 Loss 19.067171, Accuracy 0.086%\n",
      "Epoch 14, Batch 924, LR 2.435194 Loss 19.066965, Accuracy 0.086%\n",
      "Epoch 14, Batch 925, LR 2.435151 Loss 19.067071, Accuracy 0.086%\n",
      "Epoch 14, Batch 926, LR 2.435108 Loss 19.067020, Accuracy 0.086%\n",
      "Epoch 14, Batch 927, LR 2.435066 Loss 19.066982, Accuracy 0.086%\n",
      "Epoch 14, Batch 928, LR 2.435023 Loss 19.067047, Accuracy 0.086%\n",
      "Epoch 14, Batch 929, LR 2.434981 Loss 19.066898, Accuracy 0.086%\n",
      "Epoch 14, Batch 930, LR 2.434938 Loss 19.067319, Accuracy 0.086%\n",
      "Epoch 14, Batch 931, LR 2.434895 Loss 19.067357, Accuracy 0.086%\n",
      "Epoch 14, Batch 932, LR 2.434853 Loss 19.067457, Accuracy 0.086%\n",
      "Epoch 14, Batch 933, LR 2.434810 Loss 19.067695, Accuracy 0.085%\n",
      "Epoch 14, Batch 934, LR 2.434767 Loss 19.067919, Accuracy 0.085%\n",
      "Epoch 14, Batch 935, LR 2.434725 Loss 19.067996, Accuracy 0.085%\n",
      "Epoch 14, Batch 936, LR 2.434682 Loss 19.067722, Accuracy 0.085%\n",
      "Epoch 14, Batch 937, LR 2.434639 Loss 19.067771, Accuracy 0.085%\n",
      "Epoch 14, Batch 938, LR 2.434596 Loss 19.067774, Accuracy 0.085%\n",
      "Epoch 14, Batch 939, LR 2.434554 Loss 19.067669, Accuracy 0.086%\n",
      "Epoch 14, Batch 940, LR 2.434511 Loss 19.067808, Accuracy 0.086%\n",
      "Epoch 14, Batch 941, LR 2.434468 Loss 19.067894, Accuracy 0.086%\n",
      "Epoch 14, Batch 942, LR 2.434425 Loss 19.067913, Accuracy 0.085%\n",
      "Epoch 14, Batch 943, LR 2.434382 Loss 19.067867, Accuracy 0.085%\n",
      "Epoch 14, Batch 944, LR 2.434339 Loss 19.067767, Accuracy 0.085%\n",
      "Epoch 14, Batch 945, LR 2.434297 Loss 19.067651, Accuracy 0.085%\n",
      "Epoch 14, Batch 946, LR 2.434254 Loss 19.067653, Accuracy 0.085%\n",
      "Epoch 14, Batch 947, LR 2.434211 Loss 19.067731, Accuracy 0.085%\n",
      "Epoch 14, Batch 948, LR 2.434168 Loss 19.067937, Accuracy 0.085%\n",
      "Epoch 14, Batch 949, LR 2.434125 Loss 19.067933, Accuracy 0.085%\n",
      "Epoch 14, Batch 950, LR 2.434082 Loss 19.067894, Accuracy 0.085%\n",
      "Epoch 14, Batch 951, LR 2.434039 Loss 19.067874, Accuracy 0.085%\n",
      "Epoch 14, Batch 952, LR 2.433996 Loss 19.067921, Accuracy 0.085%\n",
      "Epoch 14, Batch 953, LR 2.433953 Loss 19.067845, Accuracy 0.084%\n",
      "Epoch 14, Batch 954, LR 2.433910 Loss 19.067677, Accuracy 0.084%\n",
      "Epoch 14, Batch 955, LR 2.433867 Loss 19.067586, Accuracy 0.084%\n",
      "Epoch 14, Batch 956, LR 2.433824 Loss 19.067560, Accuracy 0.084%\n",
      "Epoch 14, Batch 957, LR 2.433781 Loss 19.067602, Accuracy 0.084%\n",
      "Epoch 14, Batch 958, LR 2.433738 Loss 19.067451, Accuracy 0.086%\n",
      "Epoch 14, Batch 959, LR 2.433695 Loss 19.067540, Accuracy 0.086%\n",
      "Epoch 14, Batch 960, LR 2.433652 Loss 19.067613, Accuracy 0.085%\n",
      "Epoch 14, Batch 961, LR 2.433609 Loss 19.067653, Accuracy 0.085%\n",
      "Epoch 14, Batch 962, LR 2.433566 Loss 19.067648, Accuracy 0.086%\n",
      "Epoch 14, Batch 963, LR 2.433523 Loss 19.067559, Accuracy 0.087%\n",
      "Epoch 14, Batch 964, LR 2.433480 Loss 19.067533, Accuracy 0.087%\n",
      "Epoch 14, Batch 965, LR 2.433437 Loss 19.067772, Accuracy 0.087%\n",
      "Epoch 14, Batch 966, LR 2.433394 Loss 19.067843, Accuracy 0.087%\n",
      "Epoch 14, Batch 967, LR 2.433350 Loss 19.067784, Accuracy 0.086%\n",
      "Epoch 14, Batch 968, LR 2.433307 Loss 19.067830, Accuracy 0.086%\n",
      "Epoch 14, Batch 969, LR 2.433264 Loss 19.067829, Accuracy 0.086%\n",
      "Epoch 14, Batch 970, LR 2.433221 Loss 19.067826, Accuracy 0.086%\n",
      "Epoch 14, Batch 971, LR 2.433178 Loss 19.067922, Accuracy 0.086%\n",
      "Epoch 14, Batch 972, LR 2.433135 Loss 19.068158, Accuracy 0.086%\n",
      "Epoch 14, Batch 973, LR 2.433091 Loss 19.068034, Accuracy 0.086%\n",
      "Epoch 14, Batch 974, LR 2.433048 Loss 19.067924, Accuracy 0.086%\n",
      "Epoch 14, Batch 975, LR 2.433005 Loss 19.067922, Accuracy 0.086%\n",
      "Epoch 14, Batch 976, LR 2.432962 Loss 19.067793, Accuracy 0.086%\n",
      "Epoch 14, Batch 977, LR 2.432918 Loss 19.067823, Accuracy 0.086%\n",
      "Epoch 14, Batch 978, LR 2.432875 Loss 19.067947, Accuracy 0.086%\n",
      "Epoch 14, Batch 979, LR 2.432832 Loss 19.067883, Accuracy 0.086%\n",
      "Epoch 14, Batch 980, LR 2.432788 Loss 19.067921, Accuracy 0.086%\n",
      "Epoch 14, Batch 981, LR 2.432745 Loss 19.068073, Accuracy 0.086%\n",
      "Epoch 14, Batch 982, LR 2.432702 Loss 19.068227, Accuracy 0.086%\n",
      "Epoch 14, Batch 983, LR 2.432658 Loss 19.068183, Accuracy 0.086%\n",
      "Epoch 14, Batch 984, LR 2.432615 Loss 19.068020, Accuracy 0.087%\n",
      "Epoch 14, Batch 985, LR 2.432571 Loss 19.067859, Accuracy 0.086%\n",
      "Epoch 14, Batch 986, LR 2.432528 Loss 19.067833, Accuracy 0.086%\n",
      "Epoch 14, Batch 987, LR 2.432485 Loss 19.067786, Accuracy 0.086%\n",
      "Epoch 14, Batch 988, LR 2.432441 Loss 19.067842, Accuracy 0.086%\n",
      "Epoch 14, Batch 989, LR 2.432398 Loss 19.067681, Accuracy 0.086%\n",
      "Epoch 14, Batch 990, LR 2.432354 Loss 19.067712, Accuracy 0.087%\n",
      "Epoch 14, Batch 991, LR 2.432311 Loss 19.067670, Accuracy 0.087%\n",
      "Epoch 14, Batch 992, LR 2.432267 Loss 19.067535, Accuracy 0.087%\n",
      "Epoch 14, Batch 993, LR 2.432224 Loss 19.067464, Accuracy 0.087%\n",
      "Epoch 14, Batch 994, LR 2.432180 Loss 19.067590, Accuracy 0.086%\n",
      "Epoch 14, Batch 995, LR 2.432137 Loss 19.067563, Accuracy 0.086%\n",
      "Epoch 14, Batch 996, LR 2.432093 Loss 19.067615, Accuracy 0.086%\n",
      "Epoch 14, Batch 997, LR 2.432050 Loss 19.067568, Accuracy 0.086%\n",
      "Epoch 14, Batch 998, LR 2.432006 Loss 19.067558, Accuracy 0.086%\n",
      "Epoch 14, Batch 999, LR 2.431963 Loss 19.067331, Accuracy 0.086%\n",
      "Epoch 14, Batch 1000, LR 2.431919 Loss 19.067557, Accuracy 0.086%\n",
      "Epoch 14, Batch 1001, LR 2.431875 Loss 19.067600, Accuracy 0.086%\n",
      "Epoch 14, Batch 1002, LR 2.431832 Loss 19.067579, Accuracy 0.086%\n",
      "Epoch 14, Batch 1003, LR 2.431788 Loss 19.067668, Accuracy 0.086%\n",
      "Epoch 14, Batch 1004, LR 2.431744 Loss 19.067522, Accuracy 0.086%\n",
      "Epoch 14, Batch 1005, LR 2.431701 Loss 19.067663, Accuracy 0.086%\n",
      "Epoch 14, Batch 1006, LR 2.431657 Loss 19.067598, Accuracy 0.086%\n",
      "Epoch 14, Batch 1007, LR 2.431613 Loss 19.067549, Accuracy 0.086%\n",
      "Epoch 14, Batch 1008, LR 2.431570 Loss 19.067429, Accuracy 0.086%\n",
      "Epoch 14, Batch 1009, LR 2.431526 Loss 19.067506, Accuracy 0.086%\n",
      "Epoch 14, Batch 1010, LR 2.431482 Loss 19.067236, Accuracy 0.087%\n",
      "Epoch 14, Batch 1011, LR 2.431438 Loss 19.067184, Accuracy 0.087%\n",
      "Epoch 14, Batch 1012, LR 2.431395 Loss 19.067048, Accuracy 0.086%\n",
      "Epoch 14, Batch 1013, LR 2.431351 Loss 19.066937, Accuracy 0.087%\n",
      "Epoch 14, Batch 1014, LR 2.431307 Loss 19.066800, Accuracy 0.087%\n",
      "Epoch 14, Batch 1015, LR 2.431263 Loss 19.066853, Accuracy 0.087%\n",
      "Epoch 14, Batch 1016, LR 2.431220 Loss 19.066665, Accuracy 0.087%\n",
      "Epoch 14, Batch 1017, LR 2.431176 Loss 19.066603, Accuracy 0.087%\n",
      "Epoch 14, Batch 1018, LR 2.431132 Loss 19.066590, Accuracy 0.087%\n",
      "Epoch 14, Batch 1019, LR 2.431088 Loss 19.066736, Accuracy 0.087%\n",
      "Epoch 14, Batch 1020, LR 2.431044 Loss 19.066575, Accuracy 0.087%\n",
      "Epoch 14, Batch 1021, LR 2.431000 Loss 19.066574, Accuracy 0.086%\n",
      "Epoch 14, Batch 1022, LR 2.430956 Loss 19.066488, Accuracy 0.086%\n",
      "Epoch 14, Batch 1023, LR 2.430912 Loss 19.066439, Accuracy 0.086%\n",
      "Epoch 14, Batch 1024, LR 2.430868 Loss 19.066649, Accuracy 0.086%\n",
      "Epoch 14, Batch 1025, LR 2.430825 Loss 19.066796, Accuracy 0.086%\n",
      "Epoch 14, Batch 1026, LR 2.430781 Loss 19.066846, Accuracy 0.087%\n",
      "Epoch 14, Batch 1027, LR 2.430737 Loss 19.066804, Accuracy 0.087%\n",
      "Epoch 14, Batch 1028, LR 2.430693 Loss 19.066808, Accuracy 0.087%\n",
      "Epoch 14, Batch 1029, LR 2.430649 Loss 19.066873, Accuracy 0.087%\n",
      "Epoch 14, Batch 1030, LR 2.430605 Loss 19.066887, Accuracy 0.086%\n",
      "Epoch 14, Batch 1031, LR 2.430561 Loss 19.066898, Accuracy 0.086%\n",
      "Epoch 14, Batch 1032, LR 2.430517 Loss 19.066964, Accuracy 0.086%\n",
      "Epoch 14, Batch 1033, LR 2.430473 Loss 19.067043, Accuracy 0.086%\n",
      "Epoch 14, Batch 1034, LR 2.430429 Loss 19.067021, Accuracy 0.086%\n",
      "Epoch 14, Batch 1035, LR 2.430384 Loss 19.066960, Accuracy 0.086%\n",
      "Epoch 14, Batch 1036, LR 2.430340 Loss 19.067053, Accuracy 0.086%\n",
      "Epoch 14, Batch 1037, LR 2.430296 Loss 19.066978, Accuracy 0.086%\n",
      "Epoch 14, Batch 1038, LR 2.430252 Loss 19.067209, Accuracy 0.086%\n",
      "Epoch 14, Batch 1039, LR 2.430208 Loss 19.067335, Accuracy 0.086%\n",
      "Epoch 14, Batch 1040, LR 2.430164 Loss 19.067163, Accuracy 0.086%\n",
      "Epoch 14, Batch 1041, LR 2.430120 Loss 19.066929, Accuracy 0.086%\n",
      "Epoch 14, Batch 1042, LR 2.430076 Loss 19.066987, Accuracy 0.085%\n",
      "Epoch 14, Batch 1043, LR 2.430031 Loss 19.067049, Accuracy 0.085%\n",
      "Epoch 14, Batch 1044, LR 2.429987 Loss 19.066935, Accuracy 0.085%\n",
      "Epoch 14, Batch 1045, LR 2.429943 Loss 19.067116, Accuracy 0.085%\n",
      "Epoch 14, Batch 1046, LR 2.429899 Loss 19.067314, Accuracy 0.085%\n",
      "Epoch 14, Batch 1047, LR 2.429855 Loss 19.067188, Accuracy 0.086%\n",
      "Epoch 14, Loss (train set) 19.067188, Accuracy (train set) 0.086%\n",
      "Epoch 14, Accuracy (validation set) 0.103%\n",
      "Epoch 15, Batch 1, LR 2.429810 Loss 19.251522, Accuracy 0.000%\n",
      "Epoch 15, Batch 2, LR 2.429766 Loss 19.178538, Accuracy 0.000%\n",
      "Epoch 15, Batch 3, LR 2.429722 Loss 19.238637, Accuracy 0.000%\n",
      "Epoch 15, Batch 4, LR 2.429677 Loss 19.199391, Accuracy 0.000%\n",
      "Epoch 15, Batch 5, LR 2.429633 Loss 19.182183, Accuracy 0.000%\n",
      "Epoch 15, Batch 6, LR 2.429589 Loss 19.145759, Accuracy 0.130%\n",
      "Epoch 15, Batch 7, LR 2.429544 Loss 19.111697, Accuracy 0.112%\n",
      "Epoch 15, Batch 8, LR 2.429500 Loss 19.113325, Accuracy 0.098%\n",
      "Epoch 15, Batch 9, LR 2.429456 Loss 19.094848, Accuracy 0.087%\n",
      "Epoch 15, Batch 10, LR 2.429411 Loss 19.075959, Accuracy 0.078%\n",
      "Epoch 15, Batch 11, LR 2.429367 Loss 19.076596, Accuracy 0.142%\n",
      "Epoch 15, Batch 12, LR 2.429323 Loss 19.077816, Accuracy 0.130%\n",
      "Epoch 15, Batch 13, LR 2.429278 Loss 19.084114, Accuracy 0.120%\n",
      "Epoch 15, Batch 14, LR 2.429234 Loss 19.074777, Accuracy 0.112%\n",
      "Epoch 15, Batch 15, LR 2.429189 Loss 19.071794, Accuracy 0.104%\n",
      "Epoch 15, Batch 16, LR 2.429145 Loss 19.076112, Accuracy 0.146%\n",
      "Epoch 15, Batch 17, LR 2.429100 Loss 19.076713, Accuracy 0.138%\n",
      "Epoch 15, Batch 18, LR 2.429056 Loss 19.074254, Accuracy 0.130%\n",
      "Epoch 15, Batch 19, LR 2.429011 Loss 19.082344, Accuracy 0.123%\n",
      "Epoch 15, Batch 20, LR 2.428967 Loss 19.093276, Accuracy 0.117%\n",
      "Epoch 15, Batch 21, LR 2.428922 Loss 19.092178, Accuracy 0.112%\n",
      "Epoch 15, Batch 22, LR 2.428878 Loss 19.095034, Accuracy 0.107%\n",
      "Epoch 15, Batch 23, LR 2.428833 Loss 19.100090, Accuracy 0.136%\n",
      "Epoch 15, Batch 24, LR 2.428789 Loss 19.095813, Accuracy 0.130%\n",
      "Epoch 15, Batch 25, LR 2.428744 Loss 19.091165, Accuracy 0.125%\n",
      "Epoch 15, Batch 26, LR 2.428700 Loss 19.091472, Accuracy 0.120%\n",
      "Epoch 15, Batch 27, LR 2.428655 Loss 19.086873, Accuracy 0.116%\n",
      "Epoch 15, Batch 28, LR 2.428610 Loss 19.080609, Accuracy 0.112%\n",
      "Epoch 15, Batch 29, LR 2.428566 Loss 19.072090, Accuracy 0.108%\n",
      "Epoch 15, Batch 30, LR 2.428521 Loss 19.073457, Accuracy 0.104%\n",
      "Epoch 15, Batch 31, LR 2.428477 Loss 19.072538, Accuracy 0.101%\n",
      "Epoch 15, Batch 32, LR 2.428432 Loss 19.073524, Accuracy 0.098%\n",
      "Epoch 15, Batch 33, LR 2.428387 Loss 19.067179, Accuracy 0.095%\n",
      "Epoch 15, Batch 34, LR 2.428342 Loss 19.068316, Accuracy 0.115%\n",
      "Epoch 15, Batch 35, LR 2.428298 Loss 19.065494, Accuracy 0.112%\n",
      "Epoch 15, Batch 36, LR 2.428253 Loss 19.063382, Accuracy 0.109%\n",
      "Epoch 15, Batch 37, LR 2.428208 Loss 19.059903, Accuracy 0.106%\n",
      "Epoch 15, Batch 38, LR 2.428164 Loss 19.063956, Accuracy 0.123%\n",
      "Epoch 15, Batch 39, LR 2.428119 Loss 19.061266, Accuracy 0.120%\n",
      "Epoch 15, Batch 40, LR 2.428074 Loss 19.060697, Accuracy 0.117%\n",
      "Epoch 15, Batch 41, LR 2.428029 Loss 19.060906, Accuracy 0.114%\n",
      "Epoch 15, Batch 42, LR 2.427984 Loss 19.060946, Accuracy 0.112%\n",
      "Epoch 15, Batch 43, LR 2.427940 Loss 19.057118, Accuracy 0.109%\n",
      "Epoch 15, Batch 44, LR 2.427895 Loss 19.057076, Accuracy 0.107%\n",
      "Epoch 15, Batch 45, LR 2.427850 Loss 19.056817, Accuracy 0.104%\n",
      "Epoch 15, Batch 46, LR 2.427805 Loss 19.056580, Accuracy 0.102%\n",
      "Epoch 15, Batch 47, LR 2.427760 Loss 19.054579, Accuracy 0.100%\n",
      "Epoch 15, Batch 48, LR 2.427715 Loss 19.059278, Accuracy 0.114%\n",
      "Epoch 15, Batch 49, LR 2.427670 Loss 19.054729, Accuracy 0.112%\n",
      "Epoch 15, Batch 50, LR 2.427625 Loss 19.053253, Accuracy 0.109%\n",
      "Epoch 15, Batch 51, LR 2.427581 Loss 19.050709, Accuracy 0.107%\n",
      "Epoch 15, Batch 52, LR 2.427536 Loss 19.048429, Accuracy 0.105%\n",
      "Epoch 15, Batch 53, LR 2.427491 Loss 19.051551, Accuracy 0.118%\n",
      "Epoch 15, Batch 54, LR 2.427446 Loss 19.045187, Accuracy 0.145%\n",
      "Epoch 15, Batch 55, LR 2.427401 Loss 19.044101, Accuracy 0.142%\n",
      "Epoch 15, Batch 56, LR 2.427356 Loss 19.046172, Accuracy 0.140%\n",
      "Epoch 15, Batch 57, LR 2.427311 Loss 19.046398, Accuracy 0.137%\n",
      "Epoch 15, Batch 58, LR 2.427266 Loss 19.047539, Accuracy 0.135%\n",
      "Epoch 15, Batch 59, LR 2.427221 Loss 19.050597, Accuracy 0.132%\n",
      "Epoch 15, Batch 60, LR 2.427176 Loss 19.054034, Accuracy 0.130%\n",
      "Epoch 15, Batch 61, LR 2.427131 Loss 19.054579, Accuracy 0.128%\n",
      "Epoch 15, Batch 62, LR 2.427085 Loss 19.052969, Accuracy 0.139%\n",
      "Epoch 15, Batch 63, LR 2.427040 Loss 19.053611, Accuracy 0.136%\n",
      "Epoch 15, Batch 64, LR 2.426995 Loss 19.054658, Accuracy 0.134%\n",
      "Epoch 15, Batch 65, LR 2.426950 Loss 19.052468, Accuracy 0.132%\n",
      "Epoch 15, Batch 66, LR 2.426905 Loss 19.052282, Accuracy 0.130%\n",
      "Epoch 15, Batch 67, LR 2.426860 Loss 19.051975, Accuracy 0.152%\n",
      "Epoch 15, Batch 68, LR 2.426815 Loss 19.049390, Accuracy 0.149%\n",
      "Epoch 15, Batch 69, LR 2.426770 Loss 19.050141, Accuracy 0.147%\n",
      "Epoch 15, Batch 70, LR 2.426724 Loss 19.052992, Accuracy 0.145%\n",
      "Epoch 15, Batch 71, LR 2.426679 Loss 19.055839, Accuracy 0.143%\n",
      "Epoch 15, Batch 72, LR 2.426634 Loss 19.056170, Accuracy 0.141%\n",
      "Epoch 15, Batch 73, LR 2.426589 Loss 19.056022, Accuracy 0.139%\n",
      "Epoch 15, Batch 74, LR 2.426544 Loss 19.058888, Accuracy 0.148%\n",
      "Epoch 15, Batch 75, LR 2.426498 Loss 19.059896, Accuracy 0.146%\n",
      "Epoch 15, Batch 76, LR 2.426453 Loss 19.061265, Accuracy 0.144%\n",
      "Epoch 15, Batch 77, LR 2.426408 Loss 19.062630, Accuracy 0.142%\n",
      "Epoch 15, Batch 78, LR 2.426362 Loss 19.061641, Accuracy 0.140%\n",
      "Epoch 15, Batch 79, LR 2.426317 Loss 19.060505, Accuracy 0.138%\n",
      "Epoch 15, Batch 80, LR 2.426272 Loss 19.061002, Accuracy 0.137%\n",
      "Epoch 15, Batch 81, LR 2.426227 Loss 19.063448, Accuracy 0.135%\n",
      "Epoch 15, Batch 82, LR 2.426181 Loss 19.064731, Accuracy 0.133%\n",
      "Epoch 15, Batch 83, LR 2.426136 Loss 19.062478, Accuracy 0.132%\n",
      "Epoch 15, Batch 84, LR 2.426090 Loss 19.061722, Accuracy 0.140%\n",
      "Epoch 15, Batch 85, LR 2.426045 Loss 19.062272, Accuracy 0.138%\n",
      "Epoch 15, Batch 86, LR 2.426000 Loss 19.062742, Accuracy 0.136%\n",
      "Epoch 15, Batch 87, LR 2.425954 Loss 19.062920, Accuracy 0.135%\n",
      "Epoch 15, Batch 88, LR 2.425909 Loss 19.064025, Accuracy 0.133%\n",
      "Epoch 15, Batch 89, LR 2.425863 Loss 19.066432, Accuracy 0.132%\n",
      "Epoch 15, Batch 90, LR 2.425818 Loss 19.066575, Accuracy 0.130%\n",
      "Epoch 15, Batch 91, LR 2.425772 Loss 19.066878, Accuracy 0.129%\n",
      "Epoch 15, Batch 92, LR 2.425727 Loss 19.068406, Accuracy 0.127%\n",
      "Epoch 15, Batch 93, LR 2.425681 Loss 19.069870, Accuracy 0.126%\n",
      "Epoch 15, Batch 94, LR 2.425636 Loss 19.071118, Accuracy 0.125%\n",
      "Epoch 15, Batch 95, LR 2.425590 Loss 19.068652, Accuracy 0.123%\n",
      "Epoch 15, Batch 96, LR 2.425545 Loss 19.067040, Accuracy 0.122%\n",
      "Epoch 15, Batch 97, LR 2.425499 Loss 19.066242, Accuracy 0.129%\n",
      "Epoch 15, Batch 98, LR 2.425454 Loss 19.065788, Accuracy 0.128%\n",
      "Epoch 15, Batch 99, LR 2.425408 Loss 19.065616, Accuracy 0.126%\n",
      "Epoch 15, Batch 100, LR 2.425363 Loss 19.065426, Accuracy 0.125%\n",
      "Epoch 15, Batch 101, LR 2.425317 Loss 19.065393, Accuracy 0.124%\n",
      "Epoch 15, Batch 102, LR 2.425271 Loss 19.064236, Accuracy 0.123%\n",
      "Epoch 15, Batch 103, LR 2.425226 Loss 19.063501, Accuracy 0.121%\n",
      "Epoch 15, Batch 104, LR 2.425180 Loss 19.063701, Accuracy 0.120%\n",
      "Epoch 15, Batch 105, LR 2.425135 Loss 19.063050, Accuracy 0.119%\n",
      "Epoch 15, Batch 106, LR 2.425089 Loss 19.061915, Accuracy 0.118%\n",
      "Epoch 15, Batch 107, LR 2.425043 Loss 19.061727, Accuracy 0.117%\n",
      "Epoch 15, Batch 108, LR 2.424997 Loss 19.062912, Accuracy 0.116%\n",
      "Epoch 15, Batch 109, LR 2.424952 Loss 19.062113, Accuracy 0.122%\n",
      "Epoch 15, Batch 110, LR 2.424906 Loss 19.062883, Accuracy 0.121%\n",
      "Epoch 15, Batch 111, LR 2.424860 Loss 19.061934, Accuracy 0.127%\n",
      "Epoch 15, Batch 112, LR 2.424815 Loss 19.061498, Accuracy 0.126%\n",
      "Epoch 15, Batch 113, LR 2.424769 Loss 19.061824, Accuracy 0.124%\n",
      "Epoch 15, Batch 114, LR 2.424723 Loss 19.060704, Accuracy 0.123%\n",
      "Epoch 15, Batch 115, LR 2.424677 Loss 19.060657, Accuracy 0.122%\n",
      "Epoch 15, Batch 116, LR 2.424631 Loss 19.061568, Accuracy 0.121%\n",
      "Epoch 15, Batch 117, LR 2.424586 Loss 19.061684, Accuracy 0.120%\n",
      "Epoch 15, Batch 118, LR 2.424540 Loss 19.062136, Accuracy 0.119%\n",
      "Epoch 15, Batch 119, LR 2.424494 Loss 19.064126, Accuracy 0.118%\n",
      "Epoch 15, Batch 120, LR 2.424448 Loss 19.063376, Accuracy 0.117%\n",
      "Epoch 15, Batch 121, LR 2.424402 Loss 19.063054, Accuracy 0.123%\n",
      "Epoch 15, Batch 122, LR 2.424356 Loss 19.062993, Accuracy 0.122%\n",
      "Epoch 15, Batch 123, LR 2.424310 Loss 19.061693, Accuracy 0.121%\n",
      "Epoch 15, Batch 124, LR 2.424265 Loss 19.060488, Accuracy 0.120%\n",
      "Epoch 15, Batch 125, LR 2.424219 Loss 19.060308, Accuracy 0.119%\n",
      "Epoch 15, Batch 126, LR 2.424173 Loss 19.060146, Accuracy 0.118%\n",
      "Epoch 15, Batch 127, LR 2.424127 Loss 19.060835, Accuracy 0.117%\n",
      "Epoch 15, Batch 128, LR 2.424081 Loss 19.061613, Accuracy 0.116%\n",
      "Epoch 15, Batch 129, LR 2.424035 Loss 19.062330, Accuracy 0.115%\n",
      "Epoch 15, Batch 130, LR 2.423989 Loss 19.061247, Accuracy 0.114%\n",
      "Epoch 15, Batch 131, LR 2.423943 Loss 19.060379, Accuracy 0.113%\n",
      "Epoch 15, Batch 132, LR 2.423897 Loss 19.059648, Accuracy 0.112%\n",
      "Epoch 15, Batch 133, LR 2.423851 Loss 19.060596, Accuracy 0.112%\n",
      "Epoch 15, Batch 134, LR 2.423805 Loss 19.060259, Accuracy 0.111%\n",
      "Epoch 15, Batch 135, LR 2.423759 Loss 19.061546, Accuracy 0.110%\n",
      "Epoch 15, Batch 136, LR 2.423713 Loss 19.060799, Accuracy 0.109%\n",
      "Epoch 15, Batch 137, LR 2.423666 Loss 19.061762, Accuracy 0.108%\n",
      "Epoch 15, Batch 138, LR 2.423620 Loss 19.062548, Accuracy 0.108%\n",
      "Epoch 15, Batch 139, LR 2.423574 Loss 19.062183, Accuracy 0.107%\n",
      "Epoch 15, Batch 140, LR 2.423528 Loss 19.063101, Accuracy 0.106%\n",
      "Epoch 15, Batch 141, LR 2.423482 Loss 19.063616, Accuracy 0.105%\n",
      "Epoch 15, Batch 142, LR 2.423436 Loss 19.062868, Accuracy 0.105%\n",
      "Epoch 15, Batch 143, LR 2.423390 Loss 19.063648, Accuracy 0.104%\n",
      "Epoch 15, Batch 144, LR 2.423343 Loss 19.063591, Accuracy 0.103%\n",
      "Epoch 15, Batch 145, LR 2.423297 Loss 19.063478, Accuracy 0.102%\n",
      "Epoch 15, Batch 146, LR 2.423251 Loss 19.063812, Accuracy 0.102%\n",
      "Epoch 15, Batch 147, LR 2.423205 Loss 19.064777, Accuracy 0.101%\n",
      "Epoch 15, Batch 148, LR 2.423159 Loss 19.064843, Accuracy 0.100%\n",
      "Epoch 15, Batch 149, LR 2.423112 Loss 19.064354, Accuracy 0.100%\n",
      "Epoch 15, Batch 150, LR 2.423066 Loss 19.064131, Accuracy 0.099%\n",
      "Epoch 15, Batch 151, LR 2.423020 Loss 19.063773, Accuracy 0.098%\n",
      "Epoch 15, Batch 152, LR 2.422974 Loss 19.065382, Accuracy 0.098%\n",
      "Epoch 15, Batch 153, LR 2.422927 Loss 19.065919, Accuracy 0.097%\n",
      "Epoch 15, Batch 154, LR 2.422881 Loss 19.065698, Accuracy 0.101%\n",
      "Epoch 15, Batch 155, LR 2.422835 Loss 19.064961, Accuracy 0.101%\n",
      "Epoch 15, Batch 156, LR 2.422788 Loss 19.064576, Accuracy 0.100%\n",
      "Epoch 15, Batch 157, LR 2.422742 Loss 19.063576, Accuracy 0.100%\n",
      "Epoch 15, Batch 158, LR 2.422696 Loss 19.062522, Accuracy 0.099%\n",
      "Epoch 15, Batch 159, LR 2.422649 Loss 19.062617, Accuracy 0.103%\n",
      "Epoch 15, Batch 160, LR 2.422603 Loss 19.061028, Accuracy 0.103%\n",
      "Epoch 15, Batch 161, LR 2.422556 Loss 19.059903, Accuracy 0.102%\n",
      "Epoch 15, Batch 162, LR 2.422510 Loss 19.059612, Accuracy 0.101%\n",
      "Epoch 15, Batch 163, LR 2.422463 Loss 19.059723, Accuracy 0.105%\n",
      "Epoch 15, Batch 164, LR 2.422417 Loss 19.059637, Accuracy 0.105%\n",
      "Epoch 15, Batch 165, LR 2.422371 Loss 19.058651, Accuracy 0.109%\n",
      "Epoch 15, Batch 166, LR 2.422324 Loss 19.057433, Accuracy 0.108%\n",
      "Epoch 15, Batch 167, LR 2.422278 Loss 19.058058, Accuracy 0.108%\n",
      "Epoch 15, Batch 168, LR 2.422231 Loss 19.058070, Accuracy 0.107%\n",
      "Epoch 15, Batch 169, LR 2.422185 Loss 19.056806, Accuracy 0.106%\n",
      "Epoch 15, Batch 170, LR 2.422138 Loss 19.056772, Accuracy 0.106%\n",
      "Epoch 15, Batch 171, LR 2.422091 Loss 19.056745, Accuracy 0.105%\n",
      "Epoch 15, Batch 172, LR 2.422045 Loss 19.057329, Accuracy 0.104%\n",
      "Epoch 15, Batch 173, LR 2.421998 Loss 19.057136, Accuracy 0.108%\n",
      "Epoch 15, Batch 174, LR 2.421952 Loss 19.056406, Accuracy 0.108%\n",
      "Epoch 15, Batch 175, LR 2.421905 Loss 19.056461, Accuracy 0.107%\n",
      "Epoch 15, Batch 176, LR 2.421859 Loss 19.056107, Accuracy 0.107%\n",
      "Epoch 15, Batch 177, LR 2.421812 Loss 19.056282, Accuracy 0.106%\n",
      "Epoch 15, Batch 178, LR 2.421765 Loss 19.056451, Accuracy 0.105%\n",
      "Epoch 15, Batch 179, LR 2.421719 Loss 19.057322, Accuracy 0.105%\n",
      "Epoch 15, Batch 180, LR 2.421672 Loss 19.056751, Accuracy 0.104%\n",
      "Epoch 15, Batch 181, LR 2.421625 Loss 19.056109, Accuracy 0.104%\n",
      "Epoch 15, Batch 182, LR 2.421579 Loss 19.055574, Accuracy 0.103%\n",
      "Epoch 15, Batch 183, LR 2.421532 Loss 19.055282, Accuracy 0.102%\n",
      "Epoch 15, Batch 184, LR 2.421485 Loss 19.056093, Accuracy 0.102%\n",
      "Epoch 15, Batch 185, LR 2.421438 Loss 19.055317, Accuracy 0.101%\n",
      "Epoch 15, Batch 186, LR 2.421392 Loss 19.054797, Accuracy 0.101%\n",
      "Epoch 15, Batch 187, LR 2.421345 Loss 19.055436, Accuracy 0.100%\n",
      "Epoch 15, Batch 188, LR 2.421298 Loss 19.054926, Accuracy 0.100%\n",
      "Epoch 15, Batch 189, LR 2.421251 Loss 19.054853, Accuracy 0.099%\n",
      "Epoch 15, Batch 190, LR 2.421205 Loss 19.055219, Accuracy 0.103%\n",
      "Epoch 15, Batch 191, LR 2.421158 Loss 19.054650, Accuracy 0.102%\n",
      "Epoch 15, Batch 192, LR 2.421111 Loss 19.054136, Accuracy 0.102%\n",
      "Epoch 15, Batch 193, LR 2.421064 Loss 19.054000, Accuracy 0.101%\n",
      "Epoch 15, Batch 194, LR 2.421017 Loss 19.053156, Accuracy 0.101%\n",
      "Epoch 15, Batch 195, LR 2.420970 Loss 19.052848, Accuracy 0.100%\n",
      "Epoch 15, Batch 196, LR 2.420923 Loss 19.053570, Accuracy 0.100%\n",
      "Epoch 15, Batch 197, LR 2.420877 Loss 19.053960, Accuracy 0.099%\n",
      "Epoch 15, Batch 198, LR 2.420830 Loss 19.053357, Accuracy 0.099%\n",
      "Epoch 15, Batch 199, LR 2.420783 Loss 19.054017, Accuracy 0.098%\n",
      "Epoch 15, Batch 200, LR 2.420736 Loss 19.054737, Accuracy 0.098%\n",
      "Epoch 15, Batch 201, LR 2.420689 Loss 19.055013, Accuracy 0.097%\n",
      "Epoch 15, Batch 202, LR 2.420642 Loss 19.055148, Accuracy 0.097%\n",
      "Epoch 15, Batch 203, LR 2.420595 Loss 19.055919, Accuracy 0.096%\n",
      "Epoch 15, Batch 204, LR 2.420548 Loss 19.056710, Accuracy 0.096%\n",
      "Epoch 15, Batch 205, LR 2.420501 Loss 19.056262, Accuracy 0.095%\n",
      "Epoch 15, Batch 206, LR 2.420454 Loss 19.056484, Accuracy 0.095%\n",
      "Epoch 15, Batch 207, LR 2.420407 Loss 19.056102, Accuracy 0.094%\n",
      "Epoch 15, Batch 208, LR 2.420360 Loss 19.056168, Accuracy 0.094%\n",
      "Epoch 15, Batch 209, LR 2.420313 Loss 19.055703, Accuracy 0.093%\n",
      "Epoch 15, Batch 210, LR 2.420266 Loss 19.055932, Accuracy 0.093%\n",
      "Epoch 15, Batch 211, LR 2.420219 Loss 19.054129, Accuracy 0.093%\n",
      "Epoch 15, Batch 212, LR 2.420172 Loss 19.054076, Accuracy 0.092%\n",
      "Epoch 15, Batch 213, LR 2.420124 Loss 19.055072, Accuracy 0.095%\n",
      "Epoch 15, Batch 214, LR 2.420077 Loss 19.055454, Accuracy 0.095%\n",
      "Epoch 15, Batch 215, LR 2.420030 Loss 19.056222, Accuracy 0.094%\n",
      "Epoch 15, Batch 216, LR 2.419983 Loss 19.055688, Accuracy 0.094%\n",
      "Epoch 15, Batch 217, LR 2.419936 Loss 19.055971, Accuracy 0.094%\n",
      "Epoch 15, Batch 218, LR 2.419889 Loss 19.055720, Accuracy 0.093%\n",
      "Epoch 15, Batch 219, LR 2.419842 Loss 19.055055, Accuracy 0.093%\n",
      "Epoch 15, Batch 220, LR 2.419794 Loss 19.054637, Accuracy 0.092%\n",
      "Epoch 15, Batch 221, LR 2.419747 Loss 19.054488, Accuracy 0.092%\n",
      "Epoch 15, Batch 222, LR 2.419700 Loss 19.054386, Accuracy 0.091%\n",
      "Epoch 15, Batch 223, LR 2.419653 Loss 19.054855, Accuracy 0.091%\n",
      "Epoch 15, Batch 224, LR 2.419605 Loss 19.054582, Accuracy 0.091%\n",
      "Epoch 15, Batch 225, LR 2.419558 Loss 19.054447, Accuracy 0.090%\n",
      "Epoch 15, Batch 226, LR 2.419511 Loss 19.054856, Accuracy 0.090%\n",
      "Epoch 15, Batch 227, LR 2.419464 Loss 19.055384, Accuracy 0.089%\n",
      "Epoch 15, Batch 228, LR 2.419416 Loss 19.055580, Accuracy 0.089%\n",
      "Epoch 15, Batch 229, LR 2.419369 Loss 19.055476, Accuracy 0.089%\n",
      "Epoch 15, Batch 230, LR 2.419322 Loss 19.054836, Accuracy 0.088%\n",
      "Epoch 15, Batch 231, LR 2.419274 Loss 19.055209, Accuracy 0.091%\n",
      "Epoch 15, Batch 232, LR 2.419227 Loss 19.055717, Accuracy 0.091%\n",
      "Epoch 15, Batch 233, LR 2.419179 Loss 19.055841, Accuracy 0.091%\n",
      "Epoch 15, Batch 234, LR 2.419132 Loss 19.055983, Accuracy 0.090%\n",
      "Epoch 15, Batch 235, LR 2.419085 Loss 19.056323, Accuracy 0.090%\n",
      "Epoch 15, Batch 236, LR 2.419037 Loss 19.056776, Accuracy 0.089%\n",
      "Epoch 15, Batch 237, LR 2.418990 Loss 19.057176, Accuracy 0.089%\n",
      "Epoch 15, Batch 238, LR 2.418942 Loss 19.057287, Accuracy 0.089%\n",
      "Epoch 15, Batch 239, LR 2.418895 Loss 19.057425, Accuracy 0.088%\n",
      "Epoch 15, Batch 240, LR 2.418847 Loss 19.057638, Accuracy 0.088%\n",
      "Epoch 15, Batch 241, LR 2.418800 Loss 19.058150, Accuracy 0.088%\n",
      "Epoch 15, Batch 242, LR 2.418752 Loss 19.058066, Accuracy 0.087%\n",
      "Epoch 15, Batch 243, LR 2.418705 Loss 19.058124, Accuracy 0.087%\n",
      "Epoch 15, Batch 244, LR 2.418657 Loss 19.057346, Accuracy 0.090%\n",
      "Epoch 15, Batch 245, LR 2.418610 Loss 19.057290, Accuracy 0.089%\n",
      "Epoch 15, Batch 246, LR 2.418562 Loss 19.057326, Accuracy 0.089%\n",
      "Epoch 15, Batch 247, LR 2.418515 Loss 19.057510, Accuracy 0.089%\n",
      "Epoch 15, Batch 248, LR 2.418467 Loss 19.057433, Accuracy 0.088%\n",
      "Epoch 15, Batch 249, LR 2.418420 Loss 19.057700, Accuracy 0.088%\n",
      "Epoch 15, Batch 250, LR 2.418372 Loss 19.056680, Accuracy 0.087%\n",
      "Epoch 15, Batch 251, LR 2.418324 Loss 19.056417, Accuracy 0.087%\n",
      "Epoch 15, Batch 252, LR 2.418277 Loss 19.057698, Accuracy 0.087%\n",
      "Epoch 15, Batch 253, LR 2.418229 Loss 19.057969, Accuracy 0.086%\n",
      "Epoch 15, Batch 254, LR 2.418181 Loss 19.058599, Accuracy 0.086%\n",
      "Epoch 15, Batch 255, LR 2.418134 Loss 19.057919, Accuracy 0.086%\n",
      "Epoch 15, Batch 256, LR 2.418086 Loss 19.057826, Accuracy 0.085%\n",
      "Epoch 15, Batch 257, LR 2.418038 Loss 19.057478, Accuracy 0.085%\n",
      "Epoch 15, Batch 258, LR 2.417991 Loss 19.057697, Accuracy 0.085%\n",
      "Epoch 15, Batch 259, LR 2.417943 Loss 19.058027, Accuracy 0.084%\n",
      "Epoch 15, Batch 260, LR 2.417895 Loss 19.058003, Accuracy 0.084%\n",
      "Epoch 15, Batch 261, LR 2.417847 Loss 19.057734, Accuracy 0.087%\n",
      "Epoch 15, Batch 262, LR 2.417800 Loss 19.057464, Accuracy 0.086%\n",
      "Epoch 15, Batch 263, LR 2.417752 Loss 19.057681, Accuracy 0.086%\n",
      "Epoch 15, Batch 264, LR 2.417704 Loss 19.057789, Accuracy 0.086%\n",
      "Epoch 15, Batch 265, LR 2.417656 Loss 19.058048, Accuracy 0.085%\n",
      "Epoch 15, Batch 266, LR 2.417608 Loss 19.058367, Accuracy 0.085%\n",
      "Epoch 15, Batch 267, LR 2.417561 Loss 19.058782, Accuracy 0.085%\n",
      "Epoch 15, Batch 268, LR 2.417513 Loss 19.058888, Accuracy 0.085%\n",
      "Epoch 15, Batch 269, LR 2.417465 Loss 19.058977, Accuracy 0.084%\n",
      "Epoch 15, Batch 270, LR 2.417417 Loss 19.059624, Accuracy 0.087%\n",
      "Epoch 15, Batch 271, LR 2.417369 Loss 19.059242, Accuracy 0.086%\n",
      "Epoch 15, Batch 272, LR 2.417321 Loss 19.059199, Accuracy 0.086%\n",
      "Epoch 15, Batch 273, LR 2.417273 Loss 19.059275, Accuracy 0.086%\n",
      "Epoch 15, Batch 274, LR 2.417225 Loss 19.058831, Accuracy 0.086%\n",
      "Epoch 15, Batch 275, LR 2.417177 Loss 19.058748, Accuracy 0.085%\n",
      "Epoch 15, Batch 276, LR 2.417129 Loss 19.058407, Accuracy 0.088%\n",
      "Epoch 15, Batch 277, LR 2.417082 Loss 19.058384, Accuracy 0.087%\n",
      "Epoch 15, Batch 278, LR 2.417034 Loss 19.057933, Accuracy 0.087%\n",
      "Epoch 15, Batch 279, LR 2.416986 Loss 19.057472, Accuracy 0.087%\n",
      "Epoch 15, Batch 280, LR 2.416938 Loss 19.057531, Accuracy 0.086%\n",
      "Epoch 15, Batch 281, LR 2.416890 Loss 19.057819, Accuracy 0.086%\n",
      "Epoch 15, Batch 282, LR 2.416841 Loss 19.058057, Accuracy 0.086%\n",
      "Epoch 15, Batch 283, LR 2.416793 Loss 19.058417, Accuracy 0.086%\n",
      "Epoch 15, Batch 284, LR 2.416745 Loss 19.057713, Accuracy 0.085%\n",
      "Epoch 15, Batch 285, LR 2.416697 Loss 19.057907, Accuracy 0.085%\n",
      "Epoch 15, Batch 286, LR 2.416649 Loss 19.057099, Accuracy 0.085%\n",
      "Epoch 15, Batch 287, LR 2.416601 Loss 19.056911, Accuracy 0.084%\n",
      "Epoch 15, Batch 288, LR 2.416553 Loss 19.057262, Accuracy 0.084%\n",
      "Epoch 15, Batch 289, LR 2.416505 Loss 19.057078, Accuracy 0.084%\n",
      "Epoch 15, Batch 290, LR 2.416457 Loss 19.057409, Accuracy 0.084%\n",
      "Epoch 15, Batch 291, LR 2.416409 Loss 19.057249, Accuracy 0.083%\n",
      "Epoch 15, Batch 292, LR 2.416360 Loss 19.057700, Accuracy 0.083%\n",
      "Epoch 15, Batch 293, LR 2.416312 Loss 19.057261, Accuracy 0.083%\n",
      "Epoch 15, Batch 294, LR 2.416264 Loss 19.057219, Accuracy 0.085%\n",
      "Epoch 15, Batch 295, LR 2.416216 Loss 19.057021, Accuracy 0.085%\n",
      "Epoch 15, Batch 296, LR 2.416168 Loss 19.057006, Accuracy 0.084%\n",
      "Epoch 15, Batch 297, LR 2.416119 Loss 19.056793, Accuracy 0.084%\n",
      "Epoch 15, Batch 298, LR 2.416071 Loss 19.057368, Accuracy 0.084%\n",
      "Epoch 15, Batch 299, LR 2.416023 Loss 19.057272, Accuracy 0.084%\n",
      "Epoch 15, Batch 300, LR 2.415975 Loss 19.057575, Accuracy 0.083%\n",
      "Epoch 15, Batch 301, LR 2.415926 Loss 19.057517, Accuracy 0.083%\n",
      "Epoch 15, Batch 302, LR 2.415878 Loss 19.057499, Accuracy 0.083%\n",
      "Epoch 15, Batch 303, LR 2.415830 Loss 19.056861, Accuracy 0.083%\n",
      "Epoch 15, Batch 304, LR 2.415781 Loss 19.056138, Accuracy 0.082%\n",
      "Epoch 15, Batch 305, LR 2.415733 Loss 19.055557, Accuracy 0.085%\n",
      "Epoch 15, Batch 306, LR 2.415685 Loss 19.055816, Accuracy 0.084%\n",
      "Epoch 15, Batch 307, LR 2.415636 Loss 19.055795, Accuracy 0.084%\n",
      "Epoch 15, Batch 308, LR 2.415588 Loss 19.056089, Accuracy 0.084%\n",
      "Epoch 15, Batch 309, LR 2.415539 Loss 19.055896, Accuracy 0.083%\n",
      "Epoch 15, Batch 310, LR 2.415491 Loss 19.056603, Accuracy 0.083%\n",
      "Epoch 15, Batch 311, LR 2.415443 Loss 19.056884, Accuracy 0.083%\n",
      "Epoch 15, Batch 312, LR 2.415394 Loss 19.056836, Accuracy 0.083%\n",
      "Epoch 15, Batch 313, LR 2.415346 Loss 19.056791, Accuracy 0.082%\n",
      "Epoch 15, Batch 314, LR 2.415297 Loss 19.056737, Accuracy 0.082%\n",
      "Epoch 15, Batch 315, LR 2.415249 Loss 19.057154, Accuracy 0.082%\n",
      "Epoch 15, Batch 316, LR 2.415200 Loss 19.057536, Accuracy 0.082%\n",
      "Epoch 15, Batch 317, LR 2.415152 Loss 19.058178, Accuracy 0.081%\n",
      "Epoch 15, Batch 318, LR 2.415103 Loss 19.058428, Accuracy 0.081%\n",
      "Epoch 15, Batch 319, LR 2.415055 Loss 19.058775, Accuracy 0.081%\n",
      "Epoch 15, Batch 320, LR 2.415006 Loss 19.058997, Accuracy 0.083%\n",
      "Epoch 15, Batch 321, LR 2.414958 Loss 19.059128, Accuracy 0.083%\n",
      "Epoch 15, Batch 322, LR 2.414909 Loss 19.059083, Accuracy 0.082%\n",
      "Epoch 15, Batch 323, LR 2.414861 Loss 19.058782, Accuracy 0.082%\n",
      "Epoch 15, Batch 324, LR 2.414812 Loss 19.058712, Accuracy 0.082%\n",
      "Epoch 15, Batch 325, LR 2.414763 Loss 19.058878, Accuracy 0.082%\n",
      "Epoch 15, Batch 326, LR 2.414715 Loss 19.059184, Accuracy 0.081%\n",
      "Epoch 15, Batch 327, LR 2.414666 Loss 19.058919, Accuracy 0.081%\n",
      "Epoch 15, Batch 328, LR 2.414617 Loss 19.058359, Accuracy 0.083%\n",
      "Epoch 15, Batch 329, LR 2.414569 Loss 19.058532, Accuracy 0.083%\n",
      "Epoch 15, Batch 330, LR 2.414520 Loss 19.058482, Accuracy 0.083%\n",
      "Epoch 15, Batch 331, LR 2.414471 Loss 19.058249, Accuracy 0.083%\n",
      "Epoch 15, Batch 332, LR 2.414423 Loss 19.058462, Accuracy 0.082%\n",
      "Epoch 15, Batch 333, LR 2.414374 Loss 19.059052, Accuracy 0.082%\n",
      "Epoch 15, Batch 334, LR 2.414325 Loss 19.059235, Accuracy 0.082%\n",
      "Epoch 15, Batch 335, LR 2.414276 Loss 19.059554, Accuracy 0.082%\n",
      "Epoch 15, Batch 336, LR 2.414228 Loss 19.058845, Accuracy 0.081%\n",
      "Epoch 15, Batch 337, LR 2.414179 Loss 19.058368, Accuracy 0.081%\n",
      "Epoch 15, Batch 338, LR 2.414130 Loss 19.058245, Accuracy 0.081%\n",
      "Epoch 15, Batch 339, LR 2.414081 Loss 19.057644, Accuracy 0.081%\n",
      "Epoch 15, Batch 340, LR 2.414033 Loss 19.057434, Accuracy 0.080%\n",
      "Epoch 15, Batch 341, LR 2.413984 Loss 19.057535, Accuracy 0.080%\n",
      "Epoch 15, Batch 342, LR 2.413935 Loss 19.057520, Accuracy 0.082%\n",
      "Epoch 15, Batch 343, LR 2.413886 Loss 19.057659, Accuracy 0.082%\n",
      "Epoch 15, Batch 344, LR 2.413837 Loss 19.057454, Accuracy 0.082%\n",
      "Epoch 15, Batch 345, LR 2.413788 Loss 19.057386, Accuracy 0.082%\n",
      "Epoch 15, Batch 346, LR 2.413739 Loss 19.057146, Accuracy 0.081%\n",
      "Epoch 15, Batch 347, LR 2.413690 Loss 19.056805, Accuracy 0.081%\n",
      "Epoch 15, Batch 348, LR 2.413642 Loss 19.056587, Accuracy 0.083%\n",
      "Epoch 15, Batch 349, LR 2.413593 Loss 19.056470, Accuracy 0.083%\n",
      "Epoch 15, Batch 350, LR 2.413544 Loss 19.056660, Accuracy 0.083%\n",
      "Epoch 15, Batch 351, LR 2.413495 Loss 19.056493, Accuracy 0.082%\n",
      "Epoch 15, Batch 352, LR 2.413446 Loss 19.056409, Accuracy 0.082%\n",
      "Epoch 15, Batch 353, LR 2.413397 Loss 19.055920, Accuracy 0.082%\n",
      "Epoch 15, Batch 354, LR 2.413348 Loss 19.055551, Accuracy 0.082%\n",
      "Epoch 15, Batch 355, LR 2.413299 Loss 19.055557, Accuracy 0.081%\n",
      "Epoch 15, Batch 356, LR 2.413250 Loss 19.055683, Accuracy 0.081%\n",
      "Epoch 15, Batch 357, LR 2.413201 Loss 19.056418, Accuracy 0.081%\n",
      "Epoch 15, Batch 358, LR 2.413152 Loss 19.056343, Accuracy 0.081%\n",
      "Epoch 15, Batch 359, LR 2.413103 Loss 19.056167, Accuracy 0.081%\n",
      "Epoch 15, Batch 360, LR 2.413053 Loss 19.056143, Accuracy 0.080%\n",
      "Epoch 15, Batch 361, LR 2.413004 Loss 19.055446, Accuracy 0.080%\n",
      "Epoch 15, Batch 362, LR 2.412955 Loss 19.055520, Accuracy 0.080%\n",
      "Epoch 15, Batch 363, LR 2.412906 Loss 19.055067, Accuracy 0.080%\n",
      "Epoch 15, Batch 364, LR 2.412857 Loss 19.055814, Accuracy 0.079%\n",
      "Epoch 15, Batch 365, LR 2.412808 Loss 19.055647, Accuracy 0.079%\n",
      "Epoch 15, Batch 366, LR 2.412759 Loss 19.055724, Accuracy 0.079%\n",
      "Epoch 15, Batch 367, LR 2.412710 Loss 19.056118, Accuracy 0.081%\n",
      "Epoch 15, Batch 368, LR 2.412660 Loss 19.055973, Accuracy 0.081%\n",
      "Epoch 15, Batch 369, LR 2.412611 Loss 19.055845, Accuracy 0.080%\n",
      "Epoch 15, Batch 370, LR 2.412562 Loss 19.056299, Accuracy 0.080%\n",
      "Epoch 15, Batch 371, LR 2.412513 Loss 19.056690, Accuracy 0.080%\n",
      "Epoch 15, Batch 372, LR 2.412463 Loss 19.056986, Accuracy 0.080%\n",
      "Epoch 15, Batch 373, LR 2.412414 Loss 19.056892, Accuracy 0.080%\n",
      "Epoch 15, Batch 374, LR 2.412365 Loss 19.057146, Accuracy 0.079%\n",
      "Epoch 15, Batch 375, LR 2.412316 Loss 19.057584, Accuracy 0.079%\n",
      "Epoch 15, Batch 376, LR 2.412266 Loss 19.057729, Accuracy 0.079%\n",
      "Epoch 15, Batch 377, LR 2.412217 Loss 19.057831, Accuracy 0.081%\n",
      "Epoch 15, Batch 378, LR 2.412168 Loss 19.057887, Accuracy 0.081%\n",
      "Epoch 15, Batch 379, LR 2.412118 Loss 19.057974, Accuracy 0.080%\n",
      "Epoch 15, Batch 380, LR 2.412069 Loss 19.057736, Accuracy 0.080%\n",
      "Epoch 15, Batch 381, LR 2.412020 Loss 19.057407, Accuracy 0.082%\n",
      "Epoch 15, Batch 382, LR 2.411970 Loss 19.057037, Accuracy 0.082%\n",
      "Epoch 15, Batch 383, LR 2.411921 Loss 19.057420, Accuracy 0.082%\n",
      "Epoch 15, Batch 384, LR 2.411872 Loss 19.057880, Accuracy 0.081%\n",
      "Epoch 15, Batch 385, LR 2.411822 Loss 19.057726, Accuracy 0.081%\n",
      "Epoch 15, Batch 386, LR 2.411773 Loss 19.057622, Accuracy 0.081%\n",
      "Epoch 15, Batch 387, LR 2.411723 Loss 19.057414, Accuracy 0.081%\n",
      "Epoch 15, Batch 388, LR 2.411674 Loss 19.057516, Accuracy 0.081%\n",
      "Epoch 15, Batch 389, LR 2.411624 Loss 19.057616, Accuracy 0.080%\n",
      "Epoch 15, Batch 390, LR 2.411575 Loss 19.057416, Accuracy 0.080%\n",
      "Epoch 15, Batch 391, LR 2.411525 Loss 19.057371, Accuracy 0.080%\n",
      "Epoch 15, Batch 392, LR 2.411476 Loss 19.057097, Accuracy 0.082%\n",
      "Epoch 15, Batch 393, LR 2.411426 Loss 19.056604, Accuracy 0.082%\n",
      "Epoch 15, Batch 394, LR 2.411377 Loss 19.057163, Accuracy 0.081%\n",
      "Epoch 15, Batch 395, LR 2.411327 Loss 19.057193, Accuracy 0.081%\n",
      "Epoch 15, Batch 396, LR 2.411278 Loss 19.057147, Accuracy 0.081%\n",
      "Epoch 15, Batch 397, LR 2.411228 Loss 19.056926, Accuracy 0.081%\n",
      "Epoch 15, Batch 398, LR 2.411179 Loss 19.056701, Accuracy 0.080%\n",
      "Epoch 15, Batch 399, LR 2.411129 Loss 19.056634, Accuracy 0.080%\n",
      "Epoch 15, Batch 400, LR 2.411079 Loss 19.056304, Accuracy 0.080%\n",
      "Epoch 15, Batch 401, LR 2.411030 Loss 19.056279, Accuracy 0.080%\n",
      "Epoch 15, Batch 402, LR 2.410980 Loss 19.056863, Accuracy 0.080%\n",
      "Epoch 15, Batch 403, LR 2.410930 Loss 19.057136, Accuracy 0.079%\n",
      "Epoch 15, Batch 404, LR 2.410881 Loss 19.057148, Accuracy 0.079%\n",
      "Epoch 15, Batch 405, LR 2.410831 Loss 19.057164, Accuracy 0.079%\n",
      "Epoch 15, Batch 406, LR 2.410781 Loss 19.057635, Accuracy 0.079%\n",
      "Epoch 15, Batch 407, LR 2.410732 Loss 19.057248, Accuracy 0.079%\n",
      "Epoch 15, Batch 408, LR 2.410682 Loss 19.057673, Accuracy 0.080%\n",
      "Epoch 15, Batch 409, LR 2.410632 Loss 19.057753, Accuracy 0.082%\n",
      "Epoch 15, Batch 410, LR 2.410583 Loss 19.058185, Accuracy 0.082%\n",
      "Epoch 15, Batch 411, LR 2.410533 Loss 19.057779, Accuracy 0.082%\n",
      "Epoch 15, Batch 412, LR 2.410483 Loss 19.057635, Accuracy 0.082%\n",
      "Epoch 15, Batch 413, LR 2.410433 Loss 19.057651, Accuracy 0.081%\n",
      "Epoch 15, Batch 414, LR 2.410383 Loss 19.058209, Accuracy 0.081%\n",
      "Epoch 15, Batch 415, LR 2.410334 Loss 19.058629, Accuracy 0.081%\n",
      "Epoch 15, Batch 416, LR 2.410284 Loss 19.059139, Accuracy 0.081%\n",
      "Epoch 15, Batch 417, LR 2.410234 Loss 19.059559, Accuracy 0.081%\n",
      "Epoch 15, Batch 418, LR 2.410184 Loss 19.059709, Accuracy 0.080%\n",
      "Epoch 15, Batch 419, LR 2.410134 Loss 19.059758, Accuracy 0.080%\n",
      "Epoch 15, Batch 420, LR 2.410084 Loss 19.059614, Accuracy 0.080%\n",
      "Epoch 15, Batch 421, LR 2.410034 Loss 19.059585, Accuracy 0.080%\n",
      "Epoch 15, Batch 422, LR 2.409985 Loss 19.059728, Accuracy 0.080%\n",
      "Epoch 15, Batch 423, LR 2.409935 Loss 19.059248, Accuracy 0.081%\n",
      "Epoch 15, Batch 424, LR 2.409885 Loss 19.059211, Accuracy 0.081%\n",
      "Epoch 15, Batch 425, LR 2.409835 Loss 19.059272, Accuracy 0.081%\n",
      "Epoch 15, Batch 426, LR 2.409785 Loss 19.059577, Accuracy 0.081%\n",
      "Epoch 15, Batch 427, LR 2.409735 Loss 19.059074, Accuracy 0.081%\n",
      "Epoch 15, Batch 428, LR 2.409685 Loss 19.059069, Accuracy 0.080%\n",
      "Epoch 15, Batch 429, LR 2.409635 Loss 19.059224, Accuracy 0.080%\n",
      "Epoch 15, Batch 430, LR 2.409585 Loss 19.058584, Accuracy 0.080%\n",
      "Epoch 15, Batch 431, LR 2.409535 Loss 19.058146, Accuracy 0.080%\n",
      "Epoch 15, Batch 432, LR 2.409485 Loss 19.057878, Accuracy 0.080%\n",
      "Epoch 15, Batch 433, LR 2.409435 Loss 19.058052, Accuracy 0.079%\n",
      "Epoch 15, Batch 434, LR 2.409385 Loss 19.058232, Accuracy 0.079%\n",
      "Epoch 15, Batch 435, LR 2.409335 Loss 19.057749, Accuracy 0.079%\n",
      "Epoch 15, Batch 436, LR 2.409284 Loss 19.058070, Accuracy 0.079%\n",
      "Epoch 15, Batch 437, LR 2.409234 Loss 19.058126, Accuracy 0.079%\n",
      "Epoch 15, Batch 438, LR 2.409184 Loss 19.058084, Accuracy 0.078%\n",
      "Epoch 15, Batch 439, LR 2.409134 Loss 19.058062, Accuracy 0.078%\n",
      "Epoch 15, Batch 440, LR 2.409084 Loss 19.057895, Accuracy 0.078%\n",
      "Epoch 15, Batch 441, LR 2.409034 Loss 19.057920, Accuracy 0.078%\n",
      "Epoch 15, Batch 442, LR 2.408984 Loss 19.057909, Accuracy 0.078%\n",
      "Epoch 15, Batch 443, LR 2.408933 Loss 19.058143, Accuracy 0.078%\n",
      "Epoch 15, Batch 444, LR 2.408883 Loss 19.057702, Accuracy 0.077%\n",
      "Epoch 15, Batch 445, LR 2.408833 Loss 19.057813, Accuracy 0.077%\n",
      "Epoch 15, Batch 446, LR 2.408783 Loss 19.057938, Accuracy 0.077%\n",
      "Epoch 15, Batch 447, LR 2.408733 Loss 19.057958, Accuracy 0.077%\n",
      "Epoch 15, Batch 448, LR 2.408682 Loss 19.058560, Accuracy 0.077%\n",
      "Epoch 15, Batch 449, LR 2.408632 Loss 19.058698, Accuracy 0.077%\n",
      "Epoch 15, Batch 450, LR 2.408582 Loss 19.058525, Accuracy 0.076%\n",
      "Epoch 15, Batch 451, LR 2.408531 Loss 19.058663, Accuracy 0.076%\n",
      "Epoch 15, Batch 452, LR 2.408481 Loss 19.059261, Accuracy 0.076%\n",
      "Epoch 15, Batch 453, LR 2.408431 Loss 19.059707, Accuracy 0.076%\n",
      "Epoch 15, Batch 454, LR 2.408381 Loss 19.059528, Accuracy 0.076%\n",
      "Epoch 15, Batch 455, LR 2.408330 Loss 19.059382, Accuracy 0.076%\n",
      "Epoch 15, Batch 456, LR 2.408280 Loss 19.059542, Accuracy 0.075%\n",
      "Epoch 15, Batch 457, LR 2.408229 Loss 19.059221, Accuracy 0.075%\n",
      "Epoch 15, Batch 458, LR 2.408179 Loss 19.059126, Accuracy 0.075%\n",
      "Epoch 15, Batch 459, LR 2.408129 Loss 19.059329, Accuracy 0.075%\n",
      "Epoch 15, Batch 460, LR 2.408078 Loss 19.059304, Accuracy 0.075%\n",
      "Epoch 15, Batch 461, LR 2.408028 Loss 19.059458, Accuracy 0.075%\n",
      "Epoch 15, Batch 462, LR 2.407977 Loss 19.059336, Accuracy 0.074%\n",
      "Epoch 15, Batch 463, LR 2.407927 Loss 19.059293, Accuracy 0.074%\n",
      "Epoch 15, Batch 464, LR 2.407876 Loss 19.059424, Accuracy 0.074%\n",
      "Epoch 15, Batch 465, LR 2.407826 Loss 19.059498, Accuracy 0.074%\n",
      "Epoch 15, Batch 466, LR 2.407775 Loss 19.059542, Accuracy 0.074%\n",
      "Epoch 15, Batch 467, LR 2.407725 Loss 19.059471, Accuracy 0.075%\n",
      "Epoch 15, Batch 468, LR 2.407674 Loss 19.059424, Accuracy 0.075%\n",
      "Epoch 15, Batch 469, LR 2.407624 Loss 19.059385, Accuracy 0.075%\n",
      "Epoch 15, Batch 470, LR 2.407573 Loss 19.059293, Accuracy 0.075%\n",
      "Epoch 15, Batch 471, LR 2.407523 Loss 19.059720, Accuracy 0.075%\n",
      "Epoch 15, Batch 472, LR 2.407472 Loss 19.059742, Accuracy 0.074%\n",
      "Epoch 15, Batch 473, LR 2.407422 Loss 19.059659, Accuracy 0.074%\n",
      "Epoch 15, Batch 474, LR 2.407371 Loss 19.059716, Accuracy 0.074%\n",
      "Epoch 15, Batch 475, LR 2.407320 Loss 19.059697, Accuracy 0.074%\n",
      "Epoch 15, Batch 476, LR 2.407270 Loss 19.059849, Accuracy 0.074%\n",
      "Epoch 15, Batch 477, LR 2.407219 Loss 19.059705, Accuracy 0.074%\n",
      "Epoch 15, Batch 478, LR 2.407169 Loss 19.059664, Accuracy 0.074%\n",
      "Epoch 15, Batch 479, LR 2.407118 Loss 19.059506, Accuracy 0.073%\n",
      "Epoch 15, Batch 480, LR 2.407067 Loss 19.059734, Accuracy 0.073%\n",
      "Epoch 15, Batch 481, LR 2.407017 Loss 19.059890, Accuracy 0.073%\n",
      "Epoch 15, Batch 482, LR 2.406966 Loss 19.059848, Accuracy 0.073%\n",
      "Epoch 15, Batch 483, LR 2.406915 Loss 19.059890, Accuracy 0.073%\n",
      "Epoch 15, Batch 484, LR 2.406864 Loss 19.060091, Accuracy 0.073%\n",
      "Epoch 15, Batch 485, LR 2.406814 Loss 19.059856, Accuracy 0.072%\n",
      "Epoch 15, Batch 486, LR 2.406763 Loss 19.060045, Accuracy 0.072%\n",
      "Epoch 15, Batch 487, LR 2.406712 Loss 19.059600, Accuracy 0.074%\n",
      "Epoch 15, Batch 488, LR 2.406661 Loss 19.059540, Accuracy 0.074%\n",
      "Epoch 15, Batch 489, LR 2.406611 Loss 19.059573, Accuracy 0.073%\n",
      "Epoch 15, Batch 490, LR 2.406560 Loss 19.058977, Accuracy 0.075%\n",
      "Epoch 15, Batch 491, LR 2.406509 Loss 19.059311, Accuracy 0.075%\n",
      "Epoch 15, Batch 492, LR 2.406458 Loss 19.059213, Accuracy 0.075%\n",
      "Epoch 15, Batch 493, LR 2.406407 Loss 19.059119, Accuracy 0.074%\n",
      "Epoch 15, Batch 494, LR 2.406356 Loss 19.059256, Accuracy 0.074%\n",
      "Epoch 15, Batch 495, LR 2.406305 Loss 19.059457, Accuracy 0.076%\n",
      "Epoch 15, Batch 496, LR 2.406255 Loss 19.059356, Accuracy 0.077%\n",
      "Epoch 15, Batch 497, LR 2.406204 Loss 19.059616, Accuracy 0.077%\n",
      "Epoch 15, Batch 498, LR 2.406153 Loss 19.060088, Accuracy 0.077%\n",
      "Epoch 15, Batch 499, LR 2.406102 Loss 19.060049, Accuracy 0.077%\n",
      "Epoch 15, Batch 500, LR 2.406051 Loss 19.059930, Accuracy 0.077%\n",
      "Epoch 15, Batch 501, LR 2.406000 Loss 19.059387, Accuracy 0.080%\n",
      "Epoch 15, Batch 502, LR 2.405949 Loss 19.059370, Accuracy 0.079%\n",
      "Epoch 15, Batch 503, LR 2.405898 Loss 19.059565, Accuracy 0.079%\n",
      "Epoch 15, Batch 504, LR 2.405847 Loss 19.059455, Accuracy 0.079%\n",
      "Epoch 15, Batch 505, LR 2.405796 Loss 19.059624, Accuracy 0.079%\n",
      "Epoch 15, Batch 506, LR 2.405745 Loss 19.059728, Accuracy 0.079%\n",
      "Epoch 15, Batch 507, LR 2.405694 Loss 19.059580, Accuracy 0.079%\n",
      "Epoch 15, Batch 508, LR 2.405643 Loss 19.059510, Accuracy 0.078%\n",
      "Epoch 15, Batch 509, LR 2.405592 Loss 19.059665, Accuracy 0.078%\n",
      "Epoch 15, Batch 510, LR 2.405541 Loss 19.059854, Accuracy 0.078%\n",
      "Epoch 15, Batch 511, LR 2.405490 Loss 19.060022, Accuracy 0.078%\n",
      "Epoch 15, Batch 512, LR 2.405439 Loss 19.060095, Accuracy 0.078%\n",
      "Epoch 15, Batch 513, LR 2.405387 Loss 19.059849, Accuracy 0.078%\n",
      "Epoch 15, Batch 514, LR 2.405336 Loss 19.059549, Accuracy 0.078%\n",
      "Epoch 15, Batch 515, LR 2.405285 Loss 19.059732, Accuracy 0.077%\n",
      "Epoch 15, Batch 516, LR 2.405234 Loss 19.059790, Accuracy 0.077%\n",
      "Epoch 15, Batch 517, LR 2.405183 Loss 19.059900, Accuracy 0.077%\n",
      "Epoch 15, Batch 518, LR 2.405132 Loss 19.059938, Accuracy 0.077%\n",
      "Epoch 15, Batch 519, LR 2.405080 Loss 19.059762, Accuracy 0.078%\n",
      "Epoch 15, Batch 520, LR 2.405029 Loss 19.059602, Accuracy 0.078%\n",
      "Epoch 15, Batch 521, LR 2.404978 Loss 19.059210, Accuracy 0.078%\n",
      "Epoch 15, Batch 522, LR 2.404927 Loss 19.059520, Accuracy 0.078%\n",
      "Epoch 15, Batch 523, LR 2.404876 Loss 19.059578, Accuracy 0.078%\n",
      "Epoch 15, Batch 524, LR 2.404824 Loss 19.059779, Accuracy 0.078%\n",
      "Epoch 15, Batch 525, LR 2.404773 Loss 19.059685, Accuracy 0.077%\n",
      "Epoch 15, Batch 526, LR 2.404722 Loss 19.059905, Accuracy 0.077%\n",
      "Epoch 15, Batch 527, LR 2.404670 Loss 19.059822, Accuracy 0.077%\n",
      "Epoch 15, Batch 528, LR 2.404619 Loss 19.060185, Accuracy 0.077%\n",
      "Epoch 15, Batch 529, LR 2.404568 Loss 19.060561, Accuracy 0.077%\n",
      "Epoch 15, Batch 530, LR 2.404516 Loss 19.060454, Accuracy 0.077%\n",
      "Epoch 15, Batch 531, LR 2.404465 Loss 19.060653, Accuracy 0.077%\n",
      "Epoch 15, Batch 532, LR 2.404414 Loss 19.060424, Accuracy 0.076%\n",
      "Epoch 15, Batch 533, LR 2.404362 Loss 19.060487, Accuracy 0.076%\n",
      "Epoch 15, Batch 534, LR 2.404311 Loss 19.060669, Accuracy 0.076%\n",
      "Epoch 15, Batch 535, LR 2.404260 Loss 19.060641, Accuracy 0.077%\n",
      "Epoch 15, Batch 536, LR 2.404208 Loss 19.060624, Accuracy 0.077%\n",
      "Epoch 15, Batch 537, LR 2.404157 Loss 19.060630, Accuracy 0.077%\n",
      "Epoch 15, Batch 538, LR 2.404105 Loss 19.060634, Accuracy 0.078%\n",
      "Epoch 15, Batch 539, LR 2.404054 Loss 19.060713, Accuracy 0.078%\n",
      "Epoch 15, Batch 540, LR 2.404002 Loss 19.060726, Accuracy 0.078%\n",
      "Epoch 15, Batch 541, LR 2.403951 Loss 19.060495, Accuracy 0.079%\n",
      "Epoch 15, Batch 542, LR 2.403899 Loss 19.060980, Accuracy 0.079%\n",
      "Epoch 15, Batch 543, LR 2.403848 Loss 19.061207, Accuracy 0.079%\n",
      "Epoch 15, Batch 544, LR 2.403796 Loss 19.061247, Accuracy 0.079%\n",
      "Epoch 15, Batch 545, LR 2.403745 Loss 19.061086, Accuracy 0.079%\n",
      "Epoch 15, Batch 546, LR 2.403693 Loss 19.060983, Accuracy 0.079%\n",
      "Epoch 15, Batch 547, LR 2.403642 Loss 19.061116, Accuracy 0.079%\n",
      "Epoch 15, Batch 548, LR 2.403590 Loss 19.061092, Accuracy 0.078%\n",
      "Epoch 15, Batch 549, LR 2.403538 Loss 19.061030, Accuracy 0.078%\n",
      "Epoch 15, Batch 550, LR 2.403487 Loss 19.061275, Accuracy 0.078%\n",
      "Epoch 15, Batch 551, LR 2.403435 Loss 19.061406, Accuracy 0.078%\n",
      "Epoch 15, Batch 552, LR 2.403384 Loss 19.061403, Accuracy 0.078%\n",
      "Epoch 15, Batch 553, LR 2.403332 Loss 19.061366, Accuracy 0.078%\n",
      "Epoch 15, Batch 554, LR 2.403280 Loss 19.061567, Accuracy 0.078%\n",
      "Epoch 15, Batch 555, LR 2.403229 Loss 19.061133, Accuracy 0.077%\n",
      "Epoch 15, Batch 556, LR 2.403177 Loss 19.061015, Accuracy 0.079%\n",
      "Epoch 15, Batch 557, LR 2.403125 Loss 19.060909, Accuracy 0.079%\n",
      "Epoch 15, Batch 558, LR 2.403073 Loss 19.061021, Accuracy 0.080%\n",
      "Epoch 15, Batch 559, LR 2.403022 Loss 19.060961, Accuracy 0.080%\n",
      "Epoch 15, Batch 560, LR 2.402970 Loss 19.060990, Accuracy 0.080%\n",
      "Epoch 15, Batch 561, LR 2.402918 Loss 19.060953, Accuracy 0.079%\n",
      "Epoch 15, Batch 562, LR 2.402866 Loss 19.061104, Accuracy 0.079%\n",
      "Epoch 15, Batch 563, LR 2.402815 Loss 19.061477, Accuracy 0.079%\n",
      "Epoch 15, Batch 564, LR 2.402763 Loss 19.061763, Accuracy 0.079%\n",
      "Epoch 15, Batch 565, LR 2.402711 Loss 19.061898, Accuracy 0.079%\n",
      "Epoch 15, Batch 566, LR 2.402659 Loss 19.061968, Accuracy 0.079%\n",
      "Epoch 15, Batch 567, LR 2.402607 Loss 19.062076, Accuracy 0.079%\n",
      "Epoch 15, Batch 568, LR 2.402556 Loss 19.062397, Accuracy 0.078%\n",
      "Epoch 15, Batch 569, LR 2.402504 Loss 19.062877, Accuracy 0.078%\n",
      "Epoch 15, Batch 570, LR 2.402452 Loss 19.062993, Accuracy 0.078%\n",
      "Epoch 15, Batch 571, LR 2.402400 Loss 19.062943, Accuracy 0.078%\n",
      "Epoch 15, Batch 572, LR 2.402348 Loss 19.062900, Accuracy 0.078%\n",
      "Epoch 15, Batch 573, LR 2.402296 Loss 19.062840, Accuracy 0.078%\n",
      "Epoch 15, Batch 574, LR 2.402244 Loss 19.062913, Accuracy 0.078%\n",
      "Epoch 15, Batch 575, LR 2.402192 Loss 19.062815, Accuracy 0.077%\n",
      "Epoch 15, Batch 576, LR 2.402140 Loss 19.062934, Accuracy 0.077%\n",
      "Epoch 15, Batch 577, LR 2.402088 Loss 19.062589, Accuracy 0.077%\n",
      "Epoch 15, Batch 578, LR 2.402036 Loss 19.062847, Accuracy 0.077%\n",
      "Epoch 15, Batch 579, LR 2.401984 Loss 19.062480, Accuracy 0.077%\n",
      "Epoch 15, Batch 580, LR 2.401932 Loss 19.062359, Accuracy 0.077%\n",
      "Epoch 15, Batch 581, LR 2.401880 Loss 19.062018, Accuracy 0.077%\n",
      "Epoch 15, Batch 582, LR 2.401828 Loss 19.062341, Accuracy 0.077%\n",
      "Epoch 15, Batch 583, LR 2.401776 Loss 19.062311, Accuracy 0.076%\n",
      "Epoch 15, Batch 584, LR 2.401724 Loss 19.062603, Accuracy 0.078%\n",
      "Epoch 15, Batch 585, LR 2.401672 Loss 19.062678, Accuracy 0.077%\n",
      "Epoch 15, Batch 586, LR 2.401620 Loss 19.062378, Accuracy 0.077%\n",
      "Epoch 15, Batch 587, LR 2.401568 Loss 19.062432, Accuracy 0.077%\n",
      "Epoch 15, Batch 588, LR 2.401516 Loss 19.062375, Accuracy 0.078%\n",
      "Epoch 15, Batch 589, LR 2.401464 Loss 19.062172, Accuracy 0.078%\n",
      "Epoch 15, Batch 590, LR 2.401412 Loss 19.062012, Accuracy 0.078%\n",
      "Epoch 15, Batch 591, LR 2.401360 Loss 19.061962, Accuracy 0.078%\n",
      "Epoch 15, Batch 592, LR 2.401307 Loss 19.061741, Accuracy 0.078%\n",
      "Epoch 15, Batch 593, LR 2.401255 Loss 19.061392, Accuracy 0.078%\n",
      "Epoch 15, Batch 594, LR 2.401203 Loss 19.061292, Accuracy 0.078%\n",
      "Epoch 15, Batch 595, LR 2.401151 Loss 19.061033, Accuracy 0.079%\n",
      "Epoch 15, Batch 596, LR 2.401099 Loss 19.061191, Accuracy 0.079%\n",
      "Epoch 15, Batch 597, LR 2.401046 Loss 19.061442, Accuracy 0.079%\n",
      "Epoch 15, Batch 598, LR 2.400994 Loss 19.061350, Accuracy 0.078%\n",
      "Epoch 15, Batch 599, LR 2.400942 Loss 19.061051, Accuracy 0.078%\n",
      "Epoch 15, Batch 600, LR 2.400890 Loss 19.061050, Accuracy 0.078%\n",
      "Epoch 15, Batch 601, LR 2.400837 Loss 19.060985, Accuracy 0.078%\n",
      "Epoch 15, Batch 602, LR 2.400785 Loss 19.060844, Accuracy 0.079%\n",
      "Epoch 15, Batch 603, LR 2.400733 Loss 19.060858, Accuracy 0.079%\n",
      "Epoch 15, Batch 604, LR 2.400680 Loss 19.061031, Accuracy 0.080%\n",
      "Epoch 15, Batch 605, LR 2.400628 Loss 19.060979, Accuracy 0.080%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m num_epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, max_epoch):\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     train_loss, train_top1 = \u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmain_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{:1.0f}\u001b[39;00m\u001b[33m, Loss (train set) \u001b[39m\u001b[38;5;132;01m{:f}\u001b[39;00m\u001b[33m, Accuracy (train set) \u001b[39m\u001b[38;5;132;01m{:2.3f}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m.format(num_epoch, train_loss, train_top1))\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (num_epoch + \u001b[32m1\u001b[39m)%val_interval == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/lab3/exercises_blank.py:15\u001b[39m, in \u001b[36mtrain_network\u001b[39m\u001b[34m(train_loader, main_model, optimizer, scheduler, num_epoch, verbose)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Parameter\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m loadWAV, AugmentWAV\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mResNetBlocks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/develop/sr_labs_book/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# В ячейке инициализации модели:\n",
    "model      = ResNet(BasicBlock, layers=layers, activation=activation, num_filters=num_filters, nOut=nOut, encoder_type=encoder_type, n_mels=n_mels, log_input=log_input)\n",
    "trainfunc  = AAMSoftmaxLoss(nOut=nOut, nClasses=num_train_spk, margin=margin, scale=scale)\n",
    "main_model = MainModel(model, trainfunc)  # Без .cuda()\n",
    "\n",
    "# В ячейке обучения уберите .cuda() из данных:\n",
    "start_epoch = 0\n",
    "checkpoint_flag = False\n",
    "\n",
    "if checkpoint_flag:\n",
    "    start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models/lab3_model_0004.pth')\n",
    "    start_epoch = start_epoch + 1\n",
    "\n",
    "# Train model\n",
    "for num_epoch in range(start_epoch, max_epoch):\n",
    "    train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "    \n",
    "    print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "    if (num_epoch + 1)%val_interval == 0:\n",
    "        _, val_top1 = test_network(val_loader, main_model)\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "        saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab3_models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc9d796",
   "metadata": {},
   "source": [
    "**3. Обучение параметров блока построения дикторских моделей с учётом процедуры аугментации данных**\n",
    "\n",
    "Известно, что рроцедуры формирования и передачи речевого сигнала могут сопровождаться воздействием шумов и помех, приводящих к искажению сигнала. В качестве примеров искажающих факторов, влияющих на ухудшение качестве речевого сигнала можно привести: импульсный отклик помещения (реверберация), фоновый шум голосов группы нецелевых дикторов, звук телевизора или радиоприёмника и т.п. Разработка конвейера системы голосовой биометрии требует учёта воздействия искажающих факторов на качество её работы. Поскольку процедура построения современных дикторских моделей основана на обучении глубоких нейронных сетей, требующих большие объёмы данных для обучения их параметров, возможным вариантом увеличения тренировочной выборки может являться использование методов аугментации статистических данных. *Аугментация* – методика создания дополнительных обучающих примеров из имеющихся данных путём внесения в них искажений, которые могут потенциально возникнуть на этапе итогового тестирования системы.\n",
    "\n",
    "Как правило, при решении задачи аугментации данных в речевой обработке используются дополнительные базы шумов и помех. В качестве примеров можно привести базы [SLR17](https://openslr.org/17/) (корпус музыкальных, речевых и шумовых звукозаписей) и [SLR28](https://openslr.org/28/) (база данных реальных и симулированных импульсных откликов комнат, а также изотропных и точечных шумов). Важно отметить, что перед применением с использованием методов аугментации подобных баз к имеющимся данным, требуется убедиться, что частоты дискретизации искажающих баз и оригинальных данных являются одинаковыми. Применительно к рассматриваемому лабораторному практикуму частоты дискретизации всех используемых звукозаписей должны быть равными 16кГц.\n",
    "\n",
    "Как известно, можно выделить два режима аугментации данных: *онлайн* (применяется в ходе процедуры обучения) и *оффлайн* (применяется до процедуры обучения) аугментацию. В рамках настоящей лабораторной работы предлагается использовать онлайн аугментацию в силу не очень большого набора тренировочных данных и большей гибкости экспериментов, чем вс случае онлайн аугментации. Необходимо отметить, что применение онлайн аугментации на практике замедляет процедуру обучения, по сравнению с оффлайн аугментацией, так как наложение искажений, извлечение акустических признаков и их возможная предобработка требует определённого машинного времени.\n",
    "\n",
    "В рамках настоящего пункта предлагается сделать следующее:\n",
    "\n",
    "1. Загрузить и извлечь данные из базы SLR17 (MUSAN). Частота дискретизации данных в рассматриваемой базе равна 16кГц по умолчанию. Поскольку звукозаписи рассматриваемой базы являются достаточно длинными, рекомендуется предварительно разбить эту базу на более маленькие фрагменты (например, длительностью 5 секунд с шагом 3 секунды), сохранив их на диск. \n",
    "\n",
    "2. Загрузить и извлечь данные из базы SLR28 (MUSAN). Частота дискретизации данных в рассматриваемой базе равна 16кГц по умолчанию.\n",
    "\n",
    "3. Модернизировать загрузчик тренировочных данных под возможность случайного наложения (искажаем исходные звукозаписи) и не наложения (не искажаем исходные звукозаписи) одного из четырёх типов искажений (реверберация, музыкальный шум, фоновый шум голосов нескольких дикторов, неструктурированный шум), описанных внутри класса **AugmentWAV** следующего программного кода: **../common/DatasetLoader.py**.\n",
    "\n",
    "4. Используя процедуру обучения из предыдущего пункта с идентичными настройками выполнить тренировку параметров блока генерации дикторских моделей на исходных данных при наличии их аугментирвоанных копий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557d1ef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Download SLR17 (MUSAN) and SLR28 (RIR noises) datasets\n",
    "with open('../data/lists/augment_datasets.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "download_dataset(lines, user=None, password=None, save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f83ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract SLR17 (MUSAN)\n",
    "extract_dataset(save_path='../data', fname='../data/musan.tar.gz')\n",
    "\n",
    "# Extract SLR28 (RIR noises)\n",
    "part_extract(save_path='../data', fname='../data/rirs_noises.zip', target=['RIRS_NOISES/simulated_rirs/mediumroom', 'RIRS_NOISES/simulated_rirs/smallroom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a403f38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split MUSAN (SLR17) dataset for faster random access\n",
    "split_musan(save_path='../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c002fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model      = ResNet(BasicBlock, layers=layers, activation=activation, num_filters=num_filters, nOut=nOut, encoder_type=encoder_type, n_mels=n_mels, log_input=log_input)\n",
    "trainfunc  = AAMSoftmaxLoss(nOut=nOut, nClasses=num_train_spk, margin=margin, scale=scale)\n",
    "main_model = MainModel(model, trainfunc).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2730a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize train dataloader (without augmentation)\n",
    "train_dataset = train_dataset_loader(train_list=train_list, \n",
    "                                     max_frames=max_frames_train, \n",
    "                                     train_path=train_path, \n",
    "                                     augment=True, \n",
    "                                     musan_path=musan_path, \n",
    "                                     rir_path=rir_path)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=batch_size_train, pin_memory=pin_memory, num_workers=num_workers_train, shuffle=shuffle)\n",
    "\n",
    "# Initialize validation dataloader\n",
    "val_dataset = test_dataset_loader(test_list=val_list, max_frames=max_frames_val, test_path=val_path)\n",
    "val_loader  = DataLoader(val_dataset, batch_size=batch_size_val, num_workers=num_workers_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e990b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize optimizer and scheduler\n",
    "optimizer = SGDOptimizer(main_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = OneCycleLRScheduler(optimizer, \n",
    "                                pct_start=0.30, \n",
    "                                cycle_momentum=False, \n",
    "                                max_lr=lr, \n",
    "                                div_factor=20, \n",
    "                                final_div_factor=10000, \n",
    "                                total_steps=max_epoch*len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748a8beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "checkpoint_flag = False\n",
    "\n",
    "if checkpoint_flag:\n",
    "    start_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models_aug/lab3_model_0004.pth')\n",
    "    start_epoch = start_epoch + 1\n",
    "\n",
    "# Train model\n",
    "for num_epoch in range(start_epoch, max_epoch):\n",
    "    train_loss, train_top1 = train_network(train_loader, main_model, optimizer, scheduler, num_epoch, verbose=True)\n",
    "    \n",
    "    print(\"Epoch {:1.0f}, Loss (train set) {:f}, Accuracy (train set) {:2.3f}%\".format(num_epoch, train_loss, train_top1))\n",
    "\n",
    "    if (num_epoch + 1)%val_interval == 0:\n",
    "        _, val_top1 = test_network(val_loader, main_model)\n",
    "        \n",
    "        print(\"Epoch {:1.0f}, Accuracy (validation set) {:2.3f}%\".format(num_epoch, val_top1))\n",
    "        \n",
    "        saveParameters(main_model, optimizer, scheduler, num_epoch, path='../data/lab3_models_aug')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f235a",
   "metadata": {},
   "source": [
    "**4. Тестирование блока построения дикторских моделей**\n",
    "\n",
    "Из литературы известно, что применение алгоритмов машинного обучения на практике требует использования трёх наборов данных: *тренировочное множество* (используется для обучения параметров модели), *валидационное множество* (используется для настройки гиперпараметров), *тестовое множество* (используется для итогового тестирования).\n",
    "\n",
    "В рамках настоящего пункта предлагается выполнить итоговое тестирования блоков генерации дикторских моделей, обученных без аугментации и с аугментацией тренировочных данных, и сравнить полученные результаты. При проведении процедуры тестирования рекомендуется выбрать различное количество фреймов для тестовых звукозаписей, чтобы грубо понять то, как длительность фонограммы влияет на качество распознавания диктора.\n",
    "\n",
    "В качестве целевой метрики предлагается использовать *долю правильных ответов*, то есть количество верно классифицированных объектов по отношению к общему количеству объектов тестового множества. Как и при проведении процедуры обучения и валидации, рассматриваемая процедура тестирования предполагает решение задачи идентификации диктора на закрытом множестве."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7080f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize test dataloader\n",
    "test_dataset = test_dataset_loader(test_list=test_list, max_frames=max_frames_test, test_path=test_path)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size_test, num_workers=num_workers_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a9cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model without augmentation\n",
    "num_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models/lab3_model_0039.pth')\n",
    "\n",
    "# Test model\n",
    "_, test_top1 = test_network(test_loader, main_model)\n",
    "\n",
    "print(\"Epoch {:1.0f}, Accuracy (test set) {:2.3f}%\".format(num_epoch, test_top1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752082b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with augmentation\n",
    "num_epoch = loadParameters(main_model, optimizer, scheduler, path='../data/lab3_models_aug/lab3_model_0039.pth')\n",
    "\n",
    "# Test model\n",
    "_, test_top1 = test_network(test_loader, main_model)\n",
    "\n",
    "print(\"Epoch {:1.0f}, Accuracy (test set) {:2.3f}%\".format(num_epoch, test_top1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4189778",
   "metadata": {},
   "source": [
    "**5. Контрольные вопросы**\n",
    "\n",
    "1. Что такое верификация и идентицикация диктора?\n",
    "\n",
    "2. Что такое распознавание диктора на закрытом и открытом множестве?\n",
    "\n",
    "3. Что такое текстозависимое и текстонезависимое распознавание диктора?\n",
    "\n",
    "4. Описать схему обучения блока генерации дикторских моделей на основе нейронных сетей.\n",
    "\n",
    "5. Описать основные компоненты, из которых состоит нейросетевой блок генерации дикторских моделей (фреймовый уровень, слой статистического пулинга, сегментный уровень, выходной слой).\n",
    "\n",
    "6. Как устроены нейросетевые архитектуры на основе ResNet-блоков?\n",
    "\n",
    "7. Что такое полносвязная нейронная сеть прямого распространения?\n",
    "\n",
    "8. Как устроена стоимостная функция для обучения нейросетевого блока генерации дикторских моделей?\n",
    "\n",
    "9. Что такое аугментация данных?\n",
    "\n",
    "10. Что такое дикторский эмбеддинг и на каком уровне блока построения дикторских моделей он генерируется?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d110ea",
   "metadata": {},
   "source": [
    "**6. Список литературы**\n",
    "\n",
    "1. Bai Z., Zhang X.-L., Chen J. Speaker recognition based on deep learning: an overview // \tarXiv:2012.00931 [eess.AS] ([ссылка](https://arxiv.org/pdf/2012.00931.pdf)).\n",
    "\n",
    "2. Hansen J.H.L., Hasan T. Speaker recognition by machines and humans: a tutorial review // IEEE Signal Processing Magazine, 2015. V. 32. № 6. P. 74–99 ([ссылка](https://www.researchgate.net/publication/282940395_Speaker_Recognition_by_Machines_and_Humans_A_tutorial_review))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f050aef4-f918-40e6-b4fa-8f868e25bffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae572718-2beb-4f90-a4b3-f460e96eb06c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
